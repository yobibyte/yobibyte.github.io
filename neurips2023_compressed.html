
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<link rel="stylesheet" href="style.css"/>
		<title>Welcome to yobihome</title>
		<a href="https://yobibyte.github.io/"><img src="./pics/socrat.png" class="center"></a>
		<h1>NeurIPS 2023 compressed</h1>
		<p>This page contains summaries of all Neurips 2023 accepted papers generated by the compressor, my personal LLM-based project.</p>
	<hr><h3>What is Flagged in Uncertainty Quantification?  Latent Density Models for Uncertainty Categorization</h3>
<p>Hao Sun, Boris van Breugel, Jonathan Crabbé, Nabeel Seedat, Mihaela van der Schaar</p>
<p><a href='https://openreview.net/forum?id=zyhxRc9bew'>https://openreview.net/forum?id=zyhxRc9bew</a></p>
<p><b>Keywords</b>: Uncertainty Explaination, Uncertainty Quantification, Interpretability
</p><p><b>Compressor summary</b>: This paper introduces a framework to classify uncertain examples from machine learning models using the confusion density matrix, which helps compare different uncertainty quantification methods.</p><hr><h3>Don’t blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy</h3>
<p>Aahlad Manas Puli, Lily H Zhang, Yoav Wald, Rajesh Ranganath</p>
<p><a href='https://openreview.net/forum?id=zyZkaqNnpa'>https://openreview.net/forum?id=zyZkaqNnpa</a></p>
<p><b>Keywords</b>: shortcut learning, spurious correlations, perfect stable feature, perception tasks, implicit bias in optimization, improving inductive biases
</p><p><b>Compressor summary</b>: The paragraph discusses how default-ERM models tend to use shortcuts in perception tasks due to their preference for maximizing margins, and proposes alternative loss functions called margin control (MARG-CTRL) that encourage uniform-margin solutions and mitigate shortcut learning.</p><hr><h3>Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization</h3>
<p>Xilie Xu, Jingfeng Zhang, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli</p>
<p><a href='https://openreview.net/forum?id=zuXyQsXVLF'>https://openreview.net/forum?id=zuXyQsXVLF</a></p>
<p><b>Keywords</b>: robust pre-training, adversarial contrastive learning
</p><p><b>Compressor summary</b>: Adversarial contrastive learning (ACL) enhances standard contrastive learning by using adversarial data and introduces adversarial invariant regularization (AIR) to improve robustness against style factors, as shown by causal reasoning and empirical results.</p><hr><h3>An Optimization-based Approach To Node Role Discovery in Networks: Approximating Equitable Partitions</h3>
<p>Michael Scholkemper, Michael T Schaub</p>
<p><a href='https://openreview.net/forum?id=ztDxO15N7f'>https://openreview.net/forum?id=ztDxO15N7f</a></p>
<p><b>Keywords</b>: Role Extraction, Graph Learning, Node Embeddings, Weisfeiler Lehman, Equitable Partition
</p><p><b>Compressor summary</b>: The paragraph discusses a new method for identifying structural roles of nodes in complex networks, based on graph-isomorphism tests and the Weisfeiler-Leman algorithm, and presents a benchmark to test the approach.</p><hr><h3>Generator Identification for Linear SDEs with Additive and Multiplicative Noise</h3>
<p>Yuanyuan Wang, Xi Geng, Wei Huang, Biwei Huang, Mingming Gong</p>
<p><a href='https://openreview.net/forum?id=zsOOqjaj2z'>https://openreview.net/forum?id=zsOOqjaj2z</a></p>
<p><b>Keywords</b>: Linear SDE, Identification, Causal inference
</p><p><b>Compressor summary</b>: The paper proposes identifiability conditions for determining the generator of linear SDEs from its solution process distribution, which is useful for causal inference, and provides sufficient and necessary conditions for additive noise and sufficient ones for multiplicative noise, along with geometric interpretations and simulations.</p><hr><h3>Algorithm Selection for Deep Active Learning with Imbalanced Datasets</h3>
<p>Jifan Zhang, Shuai Shao, saurabh verma, Robert D Nowak</p>
<p><a href='https://openreview.net/forum?id=zrUEHZ6s9C'>https://openreview.net/forum?id=zrUEHZ6s9C</a></p>
<p><b>Keywords</b>: Deep Learning, Active Learning
</p><p><b>Compressor summary</b>: TAILOR is a meta algorithm that adaptively selects among various active learning strategies to efficiently reduce labeling efforts for deep learning applications using class-balanced rewards.</p><hr><h3>Discover and Align Taxonomic Context Priors  for Open-world Semi-Supervised Learning</h3>
<p>Yu Wang, Zhun Zhong, Pengchong Qiao, Xuxin Cheng, Xiawu Zheng, Chang Liu, Nicu Sebe, Rongrong Ji, Jie Chen</p>
<p><a href='https://openreview.net/forum?id=zrLxHYvIFL'>https://openreview.net/forum?id=zrLxHYvIFL</a></p>
<p><b>Keywords</b>: open-world semi-supervised learning; novel class discovery;
</p><p><b>Compressor summary</b>: The paper proposes TIDA, a framework for open-world semi-supervised learning that leverages multi-granularity taxonomic context priors to enhance representation learning and improve pseudo label quality.</p><hr><h3>Learning Invariant Representations of Graph Neural Networks via Cluster Generalization</h3>
<p>Donglin Xia, Xiao Wang, Nian Liu, Chuan Shi</p>
<p><a href='https://openreview.net/forum?id=zrCmeqV3Sz'>https://openreview.net/forum?id=zrCmeqV3Sz</a></p>
<p><b>Keywords</b>: Graph neural networks, network representation learning, deep learning
</p><p><b>Compressor summary</b>: The paper proposes CIT, a mechanism that improves GNNs' generalization ability by transferring cluster information and preserving node diversity when the test graph structure differs from the training graph structure.</p><hr><h3>The expressive power of pooling in Graph Neural Networks</h3>
<p>Filippo Maria Bianchi, Veronica Lachi</p>
<p><a href='https://openreview.net/forum?id=zqyVjCjhYD'>https://openreview.net/forum?id=zqyVjCjhYD</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Graph pooling, Expressive power
</p><p><b>Compressor summary</b>: This paper studies how graph pooling affects the expressiveness of Graph Neural Networks (GNNs) and provides a criterion for choosing or designing pooling operators based on their ability to preserve the GNN's message-passing power.</p><hr><h3>Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples</h3>
<p>Shaokui Wei, Mingda Zhang, Hongyuan Zha, Baoyuan Wu</p>
<p><a href='https://openreview.net/forum?id=zqOcW3R9rd'>https://openreview.net/forum?id=zqOcW3R9rd</a></p>
<p><b>Keywords</b>: Backdoor Attack, Trustworthy AI, Backdoor Learning
</p><p><b>Compressor summary</b>: The paper proposes a method called Shared Adversarial Unlearning (SAU) to purify backdoored machine learning models using a small clean dataset and adversarial training techniques, achieving state-of-the-art performance.</p><hr><h3>The Crucial Role of Normalization in Sharpness-Aware Minimization</h3>
<p>Yan Dai, Kwangjun Ahn, Suvrit Sra</p>
<p><a href='https://openreview.net/forum?id=zq4vFneRiA'>https://openreview.net/forum?id=zq4vFneRiA</a></p>
<p><b>Keywords</b>: Sharpness-Aware Minimization, Normalization, Deep Learning Theory
</p><p><b>Compressor summary</b>: The paragraph discusses a paper that explores how normalization, a key component of the SAM optimizer, improves deep neural network performance by stabilizing the algorithm and enabling it to drift along a manifold of minima.</p><hr><h3>Towards Personalized Federated Learning via Heterogeneous Model Reassembly</h3>
<p>Jiaqi Wang, Xingyi Yang, Suhan Cui, Liwei Che, Lingjuan Lyu, Dongkuan Xu, Fenglong Ma</p>
<p><a href='https://openreview.net/forum?id=zpVCITHknd'>https://openreview.net/forum?id=zpVCITHknd</a></p>
<p><b>Keywords</b>: Federated Learning
</p><p><b>Compressor summary</b>: The paper introduces pFedHR, a framework for personalized federated learning that handles model heterogeneity by reassembling diverse models using server-side optimization and minimizing the impact of different data distributions.</p><hr><h3>Active Learning for Semantic Segmentation with Multi-class Label Query</h3>
<p>Sehyun Hwang, Sohyun Lee, Hoyoung Kim, Minhyeon Oh, Jungseul Ok, Suha Kwak</p>
<p><a href='https://openreview.net/forum?id=znudaK78u8'>https://openreview.net/forum?id=znudaK78u8</a></p>
<p><b>Keywords</b>: semantic segmentation; active learning; partial label learning
</p><p><b>Compressor summary</b>: The paper presents a new active learning method for semantic segmentation that uses multi-class labeling of local image regions and disambiguates partial labels with two loss functions and pseudo labels, achieving better performance and annotation efficiency.</p><hr><h3>Time-Reversed Dissipation Induces Duality Between Minimizing Gradient Norm and Function Value</h3>
<p>Jaeyeon Kim, Asuman E. Ozdaglar, Chanwoo Park, Ernest K. Ryu</p>
<p><a href='https://openreview.net/forum?id=znY173SCxu'>https://openreview.net/forum?id=znY173SCxu</a></p>
<p><b>Keywords</b>: Convex Optimization, Acceleration, First-Order methods
</p><p><b>Compressor summary</b>: The paper introduces H-duality, a novel concept in convex optimization that reveals symmetries between different first-order optimization methods and leads to new efficient techniques for minimizing function values and gradient magnitudes.</p><hr><h3>Optimizing over trained GNNs via symmetry breaking</h3>
<p>Shiqiang Zhang, Juan S Campos, Christian Wolfgang Feldmann, David Walz, Frederik Sandfort, Miriam Mathea, Calvin Tsay, Ruth Misener</p>
<p><a href='https://openreview.net/forum?id=znW5jNIOED'>https://openreview.net/forum?id=znW5jNIOED</a></p>
<p><b>Keywords</b>: Mixed-integer optimization, Graph neural network, Symmetry-breaking, Molecular design
</p><p><b>Compressor summary</b>: The paper proposes methods to optimize machine learning models constrained by trained graph neural networks (GNNs) using symmetry-breaking constraints and applies them to molecular design.</p><hr><h3>An Alternating Optimization Method for Bilevel Problems under the Polyak-Łojasiewicz Condition</h3>
<p>Quan Xiao, Songtao Lu, Tianyi Chen</p>
<p><a href='https://openreview.net/forum?id=zn5ihqknGj'>https://openreview.net/forum?id=zn5ihqknGj</a></p>
<p><b>Keywords</b>: Bilevel optimization, nonconvex constrained optimization, convergence analysis
</p><p><b>Compressor summary</b>: This paper introduces a new method (GALET) for solving bilevel optimization problems with nonconvex lower-level objectives and shows that it has the same convergence rate as gradient descent.</p><hr><h3>Scalable Fair Influence Maximization</h3>
<p>Xiaobin Rui, Zhixiao Wang, Jiayu Zhao, Lichao Sun, Wei Chen</p>
<p><a href='https://openreview.net/forum?id=zmWNe1V6jg'>https://openreview.net/forum?id=zmWNe1V6jg</a></p>
<p><b>Keywords</b>: influence maximization, approximation algorithm, social fairness
</p><p><b>Compressor summary</b>: The paper proposes an efficient algorithm for maximizing welfare fairness in community structures using a weighted maximum coverage problem approach.</p><hr><h3>Curriculum Learning With Infant Egocentric Videos</h3>
<p>Saber Sheybani, Himanshu Hansaria, Justin Newell Wood, Linda B. Smith, Zoran Tiganj</p>
<p><a href='https://openreview.net/forum?id=zkfyOkBVpz'>https://openreview.net/forum?id=zkfyOkBVpz</a></p>
<p><b>Keywords</b>: Curriculum learning, Self-supervised learning, Slow changes, Infant development
</p><p><b>Compressor summary</b>: Infant visual experiences affect the development of their visual system, and starting with data from younger infants improves learning outcomes for AI models trained on their visual inputs.</p><hr><h3>Comparing Apples to Oranges: Learning Similarity Functions for Data Produced by Different Distributions</h3>
<p>Leonidas Tsepenekas, Ivan Brugere, Freddy Lecue, Daniele Magazzeni</p>
<p><a href='https://openreview.net/forum?id=zjpjsJeVJZ'>https://openreview.net/forum?id=zjpjsJeVJZ</a></p>
<p><b>Keywords</b>: individual fairness; similarity learning; active learning
</p><p><b>Compressor summary</b>: The paper proposes an efficient sampling framework to learn similarity functions between different groups using limited expert feedback, and provides theoretical and empirical evidence for its effectiveness.</p><hr><h3>Temporal Causal Mediation through a Point Process: Direct and Indirect Effects of Healthcare Interventions</h3>
<p>Çağlar Hızlı, S. T. John, Anne Tuulikki Juuti, Tuure Tapani Saarinen, Kirsi Hannele Pietiläinen, Pekka Marttinen</p>
<p><a href='https://openreview.net/forum?id=zfHCKDzzC8'>https://openreview.net/forum?id=zfHCKDzzC8</a></p>
<p><b>Keywords</b>: Machine learning for healthcare, Causal mediation, Gaussian process, Point Process
</p><p><b>Compressor summary</b>: The authors propose a new method to analyze the causal effects of an intervention on outcomes and mediators in dynamic systems using temporal point processes.</p><hr><h3>Interpreting Unsupervised Anomaly Detection in Security via Rule Extraction</h3>
<p>Ruoyu Li, Qing Li, Yu Zhang, Dan Zhao, Yong Jiang, Yong Yang</p>
<p><a href='https://openreview.net/forum?id=zfCNwRQ569'>https://openreview.net/forum?id=zfCNwRQ569</a></p>
<p><b>Keywords</b>: unsupervised anomaly detection, global explanation, rule extraction
</p><p><b>Compressor summary</b>: The paper proposes a post-hoc method to explain black-box unsupervised anomaly detection models using distribution decomposition rules and boundary inference rules, making them more interpretable and trustworthy.</p><hr><h3>Counting Distinct Elements Under Person-Level Differential Privacy</h3>
<p>Thomas Steinke, Alexander Knop</p>
<p><a href='https://openreview.net/forum?id=zdli6OxpWd'>https://openreview.net/forum?id=zdli6OxpWd</a></p>
<p><b>Keywords</b>: differential privacy, user-level privacy, person-level privacy, sensitivity
</p><p><b>Compressor summary</b>: The authors propose a method to count distinct elements in a dataset with person-level differential privacy, by approximating the problem with a max-flow solution and optimizing the sensitivity bound.</p><hr><h3>Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework  for Online RL</h3>
<p>Qinghua Liu, Gellért Weisz, András György, Chi Jin, Csaba Szepesvari</p>
<p><a href='https://openreview.net/forum?id=zaQ7wV9NOg'>https://openreview.net/forum?id=zaQ7wV9NOg</a></p>
<p><b>Keywords</b>: Theory of reinforcement learning, policy optimization
</p><p><b>Compressor summary</b>: This paper introduces Optimistic NPG, a simple and efficient policy optimization framework for online RL with linear MDPs, which improves over existing algorithms in terms of computation and exploration.</p><hr><h3>Statistical Limits of Adaptive Linear Models: Low-Dimensional Estimation and Inference</h3>
<p>Licong Lin, Mufang Ying, Suvrojit Ghosh, Koulik Khamaru, Cun-Hui Zhang</p>
<p><a href='https://openreview.net/forum?id=zXckveawHa'>https://openreview.net/forum?id=zXckveawHa</a></p>
<p><b>Keywords</b>: Adaptive linear regression, bandit algorithms, high dimensional statistics, statistical inference
</p><p><b>Compressor summary</b>: The paper explores how adaptive data collection affects estimation and inference in high-dimensional linear models and proposes a new estimator for single coordinate inference with better performance than OLS.</p><hr><h3>Universality and Limitations of Prompt Tuning</h3>
<p>Yihan Wang, Jatin Chauhan, Wei Wang, Cho-Jui Hsieh</p>
<p><a href='https://openreview.net/forum?id=zWxKYyW9ik'>https://openreview.net/forum?id=zWxKYyW9ik</a></p>
<p><b>Keywords</b>: prompt-tuning; language model; expressive power
</p><p><b>Compressor summary</b>: The paper analyzes the theoretical and practical aspects of prompt tuning for transformer-based models, showing its universality for approximating sequence-to-sequence functions and its limitations for limited-depth models and non-invertible datasets.</p><hr><h3>Unpaired Multi-Domain Causal Representation Learning</h3>
<p>Nils Sturma, Chandler Squires, Mathias Drton, Caroline Uhler</p>
<p><a href='https://openreview.net/forum?id=zW1uVN6Mbv'>https://openreview.net/forum?id=zW1uVN6Mbv</a></p>
<p><b>Keywords</b>: linear structural equation models, causality, representation learning, independent component analysis, structure identifiability, multiple views, graphical model
</p><p><b>Compressor summary</b>: The paper proposes a method to identify a shared causal representation of unpaired data from multiple domains using linear models.</p><hr><h3>$S^3$: Increasing GPU Utilization during Generative Inference for Higher Throughput</h3>
<p>Yunho Jin, Chun-Feng Wu, David Brooks, Gu-Yeon Wei</p>
<p><a href='https://openreview.net/forum?id=zUYfbdNl1m'>https://openreview.net/forum?id=zUYfbdNl1m</a></p>
<p><b>Keywords</b>: Throughput, GPU utilization, Sequence length prediction
</p><p><b>Compressor summary</b>: The paper proposes $S^3$, a system that predicts the output sequence length of large language models to increase memory and GPU utilization, and handle mispredictions.</p><hr><h3>Beta Diffusion</h3>
<p>Mingyuan Zhou, Tianqi Chen, Zhendong Wang, Huangjie Zheng</p>
<p><a href='https://openreview.net/forum?id=zTSlm4nmlH'>https://openreview.net/forum?id=zTSlm4nmlH</a></p>
<p><b>Keywords</b>: Diffusion models, KL-divergence upper bounds, multiplicative transitions, scaled and shifted beta distributions
</p><p><b>Compressor summary</b>: Beta diffusion is a new method for generating data within limited ranges using multiplicative transitions based on beta distributions and optimized with KL-divergence upper bounds, which outperform negative ELBOs in experiments with synthetic and natural images.</p><hr><h3>Sounding Bodies: Modeling 3D Spatial Sound of Humans Using Body Pose and Audio</h3>
<p>Xudong XU, Dejan Markovic, Jacob Sandakly, Todd Keebler, Steven Krenn, Alexander Richard</p>
<p><a href='https://openreview.net/forum?id=zQTi3pziFp'>https://openreview.net/forum?id=zQTi3pziFp</a></p>
<p><b>Keywords</b>: sound field, spatial audio, virtual humans, human body, body modeling
</p><p><b>Compressor summary</b>: The authors present a system that generates realistic 3D spatial audio for human bodies using input from microphones and body pose, and introduce a new multimodal dataset for this task.</p><hr><h3>Optimized Covariance Design for AB Test on Social Network under Interference</h3>
<p>Qianyi Chen, Bo Li, LU DENG, Yong Wang</p>
<p><a href='https://openreview.net/forum?id=zQOYGDc9pu'>https://openreview.net/forum?id=zQOYGDc9pu</a></p>
<p><b>Keywords</b>: AB test, interference, causal inference, optimization, social network
</p><p><b>Compressor summary</b>: The paper proposes a new method for designing randomized network experiments that balances bias and variance by optimizing the covariance matrix of the treatment assignment vector using projected gradient descent.</p><hr><h3>Multi-scale Diffusion Denoised Smoothing</h3>
<p>Jongheon Jeong, Jinwoo Shin</p>
<p><a href='https://openreview.net/forum?id=zQ4yraDiRe'>https://openreview.net/forum?id=zQ4yraDiRe</a></p>
<p><b>Keywords</b>: adversarial robustness, certified robustness, randomized smoothing, denoised smoothing, diffusion models
</p><p><b>Compressor summary</b>: The paper presents a method called multi-scale smoothing that improves the trade-off between robustness and accuracy in denoised smoothing by selectively applying randomized smoothing among multiple noise scales, and proposes diffusion fine-tuning to enhance the performance of diffusion models.</p><hr><h3>Conformal PID Control for Time Series Prediction</h3>
<p>Anastasios Nikolas Angelopoulos, Emmanuel Candes, Ryan Tibshirani</p>
<p><a href='https://openreview.net/forum?id=zPYeYv6YYs'>https://openreview.net/forum?id=zPYeYv6YYs</a></p>
<p><b>Keywords</b>: conformal prediction, time series, uncertainty quantification, distribution shift
</p><p><b>Compressor summary</b>: The paper presents uncertainty quantification algorithms for time series prediction that use conformal prediction and control theory, improve coverage on COVID-19 forecasts, and provide an extendable codebase.</p><hr><h3>Residual Alignment: Uncovering the Mechanisms of Residual Networks</h3>
<p>Jianing Li, Vardan Papyan</p>
<p><a href='https://openreview.net/forum?id=zOCIKYVaF5'>https://openreview.net/forum?id=zOCIKYVaF5</a></p>
<p><b>Keywords</b>: Deep Learning, Residual Networks, Neural Networks, Generalization, Spectral Analysis
</p><p><b>Compressor summary</b>: This paper investigates Residual Alignment in ResNet architectures and shows how it aligns intermediate representations linearly across layers and affects generalization.</p><hr><h3>Stabilized Neural Differential Equations for Learning Dynamics with Explicit Constraints</h3>
<p>Alistair White, Niki Kilbertus, Maximilian Gelbrecht, Niklas Boers</p>
<p><a href='https://openreview.net/forum?id=zO2dAQfvHf'>https://openreview.net/forum?id=zO2dAQfvHf</a></p>
<p><b>Keywords</b>: neural differential equations, neural ordinary differential equations, constraints, conservation laws, stabilization, dynamical systems, dynamics, scientific machine learning, physics-informed machine learning
</p><p><b>Compressor summary</b>: SNDEs are a method to enforce constraints on neural differential equations by adding a stabilization term, which makes them compatible with various models and improves performance.</p><hr><h3>P-Flow: A Fast and Data-Efficient Zero-Shot TTS through Speech Prompting</h3>
<p>Sungwon Kim, Kevin J. Shih, Rohan Badlani, Joao Felipe Santos, Evelina Bakhturina, Mikyas T. Desta, Rafael Valle, Sungroh Yoon, Bryan Catanzaro</p>
<p><a href='https://openreview.net/forum?id=zNA7u7wtIN'>https://openreview.net/forum?id=zNA7u7wtIN</a></p>
<p><b>Keywords</b>: text-to-speech, zero-shot TTS, flow matching generative model
</p><p><b>Compressor summary</b>: P-Flow is a fast and data-efficient zero-shot text-to-speech model that uses speech prompts for speaker adaptation and achieves high quality and pronunciation with less training data and faster sampling speed than existing models.</p><hr><h3>FAMO: Fast Adaptive Multitask Optimization</h3>
<p>Bo Liu, Yihao Feng, Peter Stone, qiang liu</p>
<p><a href='https://openreview.net/forum?id=zMeemcUeXL'>https://openreview.net/forum?id=zMeemcUeXL</a></p>
<p><b>Keywords</b>: multitask learning, multitask optimization, conflicting gradients, knowledge transfer
</p><p><b>Compressor summary</b>: The paper introduces FAMO, a method that balances task losses in multitask learning using minimal space and time, outperforming existing techniques.</p><hr><h3>Implicit variance regularization in non-contrastive SSL</h3>
<p>Manu Srinath Halvagal, Axel Laborieux, Friedemann Zenke</p>
<p><a href='https://openreview.net/forum?id=zMNUNd9zs1'>https://openreview.net/forum?id=zMNUNd9zs1</a></p>
<p><b>Keywords</b>: Self-supervised learning, Non-contrastive learning, Learning dynamics
</p><p><b>Compressor summary</b>: Non-contrastive self-supervised learning (SSL) methods use different losses to stabilize learning and avoid representational collapse, and a new family of loss functions called IsoLoss can further improve their performance.</p><hr><h3>New Complexity-Theoretic Frontiers of Tractability for Neural Network Training</h3>
<p>Cornelius Brand, Robert Ganian, Mathis Rocton</p>
<p><a href='https://openreview.net/forum?id=zIEaOZ0saA'>https://openreview.net/forum?id=zIEaOZ0saA</a></p>
<p><b>Keywords</b>: neural network training, computational complexity, ReLU networks, Linear networks
</p><p><b>Compressor summary</b>: The article presents new algorithms that improve the efficiency of training neural networks with linear or ReLU activation functions.</p><hr><h3>Optimal Treatment Regimes for Proximal Causal Learning</h3>
<p>Tao Shen, Yifan Cui</p>
<p><a href='https://openreview.net/forum?id=zGdH4tKtOW'>https://openreview.net/forum?id=zGdH4tKtOW</a></p>
<p><b>Keywords</b>: Optimal treatment regimes, Policy-making, Proximal causal inference, Unmeasured confounding, Value function
</p><p><b>Compressor summary</b>: The paragraph discusses a new framework for causal inference using proxy variables, and proposes an optimal individualized treatment regime based on outcome and treatment confounding bridges, with theoretical guarantees and applications shown.</p><hr><h3>Multi-Swap k-Means++</h3>
<p>Lorenzo Beretta, Vincent Cohen-Addad, Silvio Lattanzi, Nikos Parotsidis</p>
<p><a href='https://openreview.net/forum?id=zGRWp7yRqd'>https://openreview.net/forum?id=zGRWp7yRqd</a></p>
<p><b>Keywords</b>: Clustering, k-means, approximation algorithms
</p><p><b>Compressor summary</b>: The paper proposes an improved $k$-means clustering algorithm by combining $k$-means++ sampling with larger local-search neighborhoods, achieving a 9 + ε approximation ratio and practical efficiency.</p><hr><h3>Automated Classification of Model Errors on ImageNet</h3>
<p>Momchil Peychev, Mark Niklas Mueller, Marc Fischer, Martin Vechev</p>
<p><a href='https://openreview.net/forum?id=zEoP4vzFKy'>https://openreview.net/forum?id=zEoP4vzFKy</a></p>
<p><b>Keywords</b>: ImageNet, evaluation, error classification, error analysis
</p><p><b>Compressor summary</b>: The paragraph discusses how automated error classification can help evaluate computer vision models and finds that top-1 accuracy is still useful for measuring model performance despite its limitations.</p><hr><h3>(Amplified) Banded Matrix Factorization: A unified approach to private training</h3>
<p>Christopher A. Choquette-Choo, Arun Ganesh, Ryan McKenna, Hugh Brendan McMahan, J Keith Rush, Abhradeep Guha Thakurta, Zheng Xu</p>
<p><a href='https://openreview.net/forum?id=zEm6hF97Pz'>https://openreview.net/forum?id=zEm6hF97Pz</a></p>
<p><b>Keywords</b>: Machine Learning, Differential Privacy, Optimization, Private Machine Learning, Federated Learning, Privacy Amplification, Matrix Factorization
</p><p><b>Compressor summary</b>: The paper presents a method to improve privacy-utility tradeoffs for machine learning using matrix factorization with banded matrices, which can be applied in both centralized and federated settings.</p><hr><h3>Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning</h3>
<p>Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati</p>
<p><a href='https://openreview.net/forum?id=zDbsSscmuj'>https://openreview.net/forum?id=zDbsSscmuj</a></p>
<p><b>Keywords</b>: LLMs, Planning, Domain Model, LLMs for Planning, LLMs for Heuristic Guidance
</p><p><b>Compressor summary</b>: The authors propose a method that uses large language models to construct PDDL domain models and then plan with sound planners, reducing human involvement and improving planning efficiency.</p><hr><h3>A Novel Framework for Policy Mirror Descent with General Parameterization and Linear Convergence</h3>
<p>Carlo Alfano, Rui Yuan, Patrick Rebeschini</p>
<p><a href='https://openreview.net/forum?id=zD6lXmTPPh'>https://openreview.net/forum?id=zD6lXmTPPh</a></p>
<p><b>Keywords</b>: Theory for Reinforcement Learning, Policy Optimization, Policy Gradient, Mirror Descent.
</p><p><b>Compressor summary</b>: The paper introduces a policy optimization framework based on mirror descent that allows general parameterizations and shows linear convergence, sample complexity improvements, and experimental validation.</p><hr><h3>Quasi-Monte Carlo Graph Random Features</h3>
<p>Isaac Reid, Adrian Weller, Krzysztof Marcin Choromanski</p>
<p><a href='https://openreview.net/forum?id=zCFfv49MjE'>https://openreview.net/forum?id=zCFfv49MjE</a></p>
<p><b>Keywords</b>: Graph, discrete mathematics, quasi-Monte Carlo, kernel, scalability, Laplacian, clustering, random walks
</p><p><b>Compressor summary</b>: The paper introduces a novel method to improve graph random features by inducing negative correlations through antithetic termination, leading to lower-variance estimators and better performance on various tasks.</p><hr><h3>One-Line-of-Code Data Mollification Improves Optimization of Likelihood-based Generative Models</h3>
<p>Ba-Hien Tran, Giulio Franzese, Pietro Michiardi, Maurizio Filippone</p>
<p><a href='https://openreview.net/forum?id=zAXg8dW8ZO'>https://openreview.net/forum?id=zAXg8dW8ZO</a></p>
<p><b>Keywords</b>: Generative Models, Normalizing Flows, Variational Autoencoders
</p><p><b>Compressor summary</b>: The paper introduces a simple technique to improve the quality of likelihood-based generative models by using data mollification, which enhances density estimation and low-density region handling without additional computational costs.</p><hr><h3>Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models</h3>
<p>Siyan Zhao, Aditya Grover</p>
<p><a href='https://openreview.net/forum?id=zAQK5r1enm'>https://openreview.net/forum?id=zAQK5r1enm</a></p>
<p><b>Keywords</b>: reinforcement learning, generative models, offline RL, sequential decision making
</p><p><b>Compressor summary</b>: Decision Stacks is a framework that splits goal-conditioned policy agents into three independent modules for efficient learning and flexible generative decision making in sequential decision making problems.</p><hr><h3>Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior</h3>
<p>Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy Morozov, Michael W. Mahoney, Amir Gholami</p>
<p><a href='https://openreview.net/forum?id=zANxvzflMl'>https://openreview.net/forum?id=zANxvzflMl</a></p>
<p><b>Keywords</b>: scientific machine learning, scaling, transfer learning, neural operators, foundation models
</p><p><b>Compressor summary</b>: The study shows that pre-training machine learning models can significantly improve performance in scientific applications by adapting them to different tasks with fewer examples and across various physics problems.</p><hr><h3>CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation</h3>
<p>Sihan Xu, Ziqiao Ma, Yidong Huang, Honglak Lee, Joyce Chai</p>
<p><a href='https://openreview.net/forum?id=z9d9DsjAPH'>https://openreview.net/forum?id=z9d9DsjAPH</a></p>
<p><b>Keywords</b>: Image to image translation, latent diffusion models, conditional diffusion models
</p><p><b>Compressor summary</b>: Cyclenet is a method that uses cycle consistency to improve image synthesis with diffusion models, enabling consistent unpaired image-to-image translation and robust generation of high-quality images.</p><hr><h3>A Regularized Conditional GAN for Posterior Sampling in Image Recovery Problems</h3>
<p>Matthew C Bendel, Rizwan Ahmad, Philip Schniter</p>
<p><a href='https://openreview.net/forum?id=z4vKRmq7UO'>https://openreview.net/forum?id=z4vKRmq7UO</a></p>
<p><b>Keywords</b>: Generative adversarial network, inverse problems, posterior sampling, cGAN, GAN
</p><p><b>Compressor summary</b>: The paper proposes a fast method to generate multiple high-quality images from noisy measurements using a conditional Wasserstein GAN with regularization and shows its effectiveness in MRI and inpainting tasks.</p><hr><h3>Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization</h3>
<p>Shurui Gui, Meng Liu, Xiner Li, Youzhi Luo, Shuiwang Ji</p>
<p><a href='https://openreview.net/forum?id=z3HACY5CMa'>https://openreview.net/forum?id=z3HACY5CMa</a></p>
<p><b>Keywords</b>: deep learning, graph neural network, out-of-distribution generalization, distribution shift
</p><p><b>Compressor summary</b>: The paper introduces LECI, a method to improve graph out-of-distribution generalization by using label and environment information and developing an adversarial training strategy.</p><hr><h3>Online List Labeling with Predictions</h3>
<p>Samuel McCauley, Benjamin Moseley, Aidin Niaparast, Shikha Singh</p>
<p><a href='https://openreview.net/forum?id=z37ki6nqAY'>https://openreview.net/forum?id=z37ki6nqAY</a></p>
<p><b>Keywords</b>: Algorithms with Predictions, Data Structures, Learned Indices, Online List Labeling, Resource Allocation, Beyond Worst Case Analysis
</p><p><b>Compressor summary</b>: The paper proposes a new list labeling data structure that leverages predictions to improve performance and provides theoretical guarantees for both worst-case and stochastic error models.</p><hr><h3>Thin and deep Gaussian processes</h3>
<p>Daniel Augusto de Souza, Alexander V Nikitin, S. T. John, Magnus Ross, Mauricio A Álvarez, Marc Peter Deisenroth, João Paulo Pordeus Gomes, Diego Mesquita, César Lincoln Mattos</p>
<p><a href='https://openreview.net/forum?id=z2BHMLA8pM'>https://openreview.net/forum?id=z2BHMLA8pM</a></p>
<p><b>Keywords</b>: Gaussian Processes, Deep Gaussian Processes, non-stationary kernels
</p><p><b>Compressor summary</b>: TDGP combines the benefits of deep GPs and shallow GPs by learning interpretable low-dimensional embeddings while maintaining the flexibility to adjust kernel hyperparameters, making it better suited for uncertainty quantification than previous models.</p><hr><h3>Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers</h3>
<p>Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, David Z. Pan</p>
<p><a href='https://openreview.net/forum?id=z06npyCwDq'>https://openreview.net/forum?id=z06npyCwDq</a></p>
<p><b>Keywords</b>: Transformer, Normalization, Layer Normalization, RMSNorm, Efficient Machine Learning
</p><p><b>Compressor summary</b>: The paragraph discusses different normalization techniques for Transformers and proposes a solution to unify and improve their efficiency without compromising performance.</p><hr><h3>SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks</h3>
<p>Rainer Engelken</p>
<p><a href='https://openreview.net/forum?id=yzZbwQPkmP'>https://openreview.net/forum?id=yzZbwQPkmP</a></p>
<p><b>Keywords</b>: spiking networks, event-based simulation, sparse networks, backpropagation, algorithm, neuroscience
</p><p><b>Compressor summary</b>: SparseProp is a novel event-based algorithm that simulates and trains sparse SNNs with reduced computational cost, enabling efficient and exact simulations of large spiking networks.</p><hr><h3>What Distributions are Robust to Indiscriminate Poisoning Attacks for Linear Learners?</h3>
<p>Fnu Suya, Xiao Zhang, Yuan Tian, David Evans</p>
<p><a href='https://openreview.net/forum?id=yyLFUPNEiT'>https://openreview.net/forum?id=yyLFUPNEiT</a></p>
<p><b>Keywords</b>: poisoning attacks; adversarial machine learning; machine learning security
</p><p><b>Compressor summary</b>: The study explores how linear learners can resist indiscriminate poisoning attacks on some datasets with well-separated and low-variance data distributions.</p><hr><h3>Revisiting Adversarial Robustness Distillation from the Perspective of Robust Fairness</h3>
<p>Xinli Yue, Ningping Mou, Qian Wang, Lingchen Zhao</p>
<p><a href='https://openreview.net/forum?id=ywrPcBEXdC'>https://openreview.net/forum?id=ywrPcBEXdC</a></p>
<p><b>Keywords</b>: Deep Learning, Knowledge Distillation, Adversarial Training, Fairness
</p><p><b>Compressor summary</b>: This paper introduces Fair-ARD, a novel framework to enhance robust fairness in student models by adjusting the weights of difficult classes during adversarial robustness distillation, and shows its effectiveness in improving both overall and class-wise robustness.</p><hr><h3>Computing Optimal Equilibria and Mechanisms via Learning in Zero-Sum Extensive-Form Games</h3>
<p>Brian Hu Zhang, Gabriele Farina, Ioannis Anagnostides, Federico Cacciamani, Stephen Marcus McAleer, Andreas Alexander Haupt, Andrea Celli, Nicola Gatti, Vincent Conitzer, Tuomas Sandholm</p>
<p><a href='https://openreview.net/forum?id=yw1v4RqvPk'>https://openreview.net/forum?id=yw1v4RqvPk</a></p>
<p><b>Keywords</b>: extensive-form games, deep reinforcement learning, mechanism design, correlated equilibria
</p><p><b>Compressor summary</b>: A new method for finding optimal equilibria in extensive-form games is presented, which applies existing zero-sum game learning techniques and achieves state-of-the-art results in tabular games and auction design.</p><hr><h3>Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis</h3>
<p>Alexander Meulemans, Simon Schug, Seijin Kobayashi, Nathaniel Daw, Greg Wayne</p>
<p><a href='https://openreview.net/forum?id=yvqqkOn9Pi'>https://openreview.net/forum?id=yvqqkOn9Pi</a></p>
<p><b>Keywords</b>: Reinforcement learning, Long-term credit assignment, contribution analysis, hindsight credit assignment, policy gradient methods
</p><p><b>Compressor summary</b>: COCOA is a new model-based credit assignment algorithm that measures the contribution of actions to future rewards by answering counterfactual questions, improving sample efficiency in reinforcement learning.</p><hr><h3>Canonical normalizing flows for manifold learning</h3>
<p>Kyriakos Flouris, Ender Konukoglu</p>
<p><a href='https://openreview.net/forum?id=yubwSWol6K'>https://openreview.net/forum?id=yubwSWol6K</a></p>
<p><b>Keywords</b>: manifold learning flows, normalizing flows, optimization, orthogonalization, sparsity, sparse learning, generative modeling, Riemannian manifold, geometry, metric tensor, orthogonal basis
</p><p><b>Compressor summary</b>: The proposed canonical manifold learning flow method generates a sparse and orthogonal low-dimensional representation of the data that improves the efficiency and accuracy of generative modeling techniques compared to other methods.</p><hr><h3>Epidemic Learning: Boosting Decentralized Learning with Randomized Communication</h3>
<p>Martijn De Vos, Sadegh Farhadkhani, Rachid Guerraoui, Anne-marie Kermarrec, Rafael Pires, Rishi Sharma</p>
<p><a href='https://openreview.net/forum?id=ytrhsvGP0r'>https://openreview.net/forum?id=ytrhsvGP0r</a></p>
<p><b>Keywords</b>: Epidemic, Decentralized Learning, Randomized Communication, Peer sampling
</p><p><b>Compressor summary</b>: Epidemic Learning (EL) is a decentralized learning algorithm that uses changing communication topologies to converge faster than conventional approaches, achieving better performance with less communication volume.</p><hr><h3>Expressivity-Preserving GNN Simulation</h3>
<p>Fabian Jogl, Maximilian Thiessen, Thomas Gärtner</p>
<p><a href='https://openreview.net/forum?id=ytTfonl9Wd'>https://openreview.net/forum?id=ytTfonl9Wd</a></p>
<p><b>Keywords</b>: Graph Neural Networks, GNNs, Graphs, Message Passing, Expressiveness, Graph Transformations, Message Passing Graph Neural Networks
</p><p><b>Compressor summary</b>: The paragraph discusses how graph transformations enable message passing operations for simulating graph neural networks (GNNs) without losing expressivity and improving code optimization, while distinguishing between weak and strong simulation methods.</p><hr><h3>A Unifying Perspective on Multi-Calibration: Game Dynamics for Multi-Objective Learning</h3>
<p>Nika Haghtalab, Michael Jordan, Eric Zhao</p>
<p><a href='https://openreview.net/forum?id=ysqlhW0v26'>https://openreview.net/forum?id=ysqlhW0v26</a></p>
<p><b>Keywords</b>: multicalibration, multi-objective learning, learning theory, calibration, fairness, games
</p><p><b>Compressor summary</b>: The authors present a unified framework for designing and analyzing multi-calibrated predictors by using game dynamics and achieving better guarantees than existing methods.</p><hr><h3>Emergent Correspondence from Image Diffusion</h3>
<p>Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, Bharath Hariharan</p>
<p><a href='https://openreview.net/forum?id=ypOiXjdfnU'>https://openreview.net/forum?id=ypOiXjdfnU</a></p>
<p><b>Keywords</b>: Correspondence, Diffusion Model
</p><p><b>Compressor summary</b>: The paper introduces DIFT, a method to establish correspondences between images using diffusion models without explicit supervision, and shows that it can outperform weakly-supervised methods and off-the-shelf features on various tasks.</p><hr><h3>CaMP: Causal Multi-policy Planning for Interactive Navigation in  Multi-room Scenes</h3>
<p>Xiaohan Wang, Yuehu Liu, Xinhang Song, Beibei Wang, Shuqiang Jiang</p>
<p><a href='https://openreview.net/forum?id=yoZTVn0T50'>https://openreview.net/forum?id=yoZTVn0T50</a></p>
<p><b>Keywords</b>: Embodied AI, Interactive Navigation, Causal Reinforcement Learning, Hierarchical Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper introduces a causal diagram for visual navigation in complex scenes, proposes a multi-policy model to explore counterfactual interactions and reduce exploration, and presents a large-scale dataset for evaluation.</p><hr><h3>TOA: Task-oriented Active VQA</h3>
<p>Xiaoying Xing, Mingfu Liang, Ying Wu</p>
<p><a href='https://openreview.net/forum?id=yoAmURKDJi'>https://openreview.net/forum?id=yoAmURKDJi</a></p>
<p><b>Keywords</b>: knowledge-based visual question answering, task-oriented, active image understanding, large language model, visual reasoning, multi-round dialogue
</p><p><b>Compressor summary</b>: The authors propose a new method for knowledge-based visual question answering using large language models that actively collects relevant visual evidence to verify their hypotheses, improving performance and interpretability on open-ended datasets.</p><hr><h3>Covariance-adaptive best arm identification</h3>
<p>El Mehdi Saad, Gilles Blanchard, Nicolas Verzelen</p>
<p><a href='https://openreview.net/forum?id=ymHM1qRUeb'>https://openreview.net/forum?id=ymHM1qRUeb</a></p>
<p><b>Keywords</b>: Multi-armed bandits, Best-arm identification, Adaptive identification
</p><p><b>Compressor summary</b>: The paper proposes new algorithms for identifying the best arm in a multi-armed bandit model with dependent arms and covariance, improving on standard methods in applications like clinical trials.</p><hr><h3>Model-Based Control with Sparse Neural Dynamics</h3>
<p>Ziang Liu, Genggeng Zhou, Jeff He, Tobia Marcucci, Li Fei-Fei, Jiajun Wu, Yunzhu Li</p>
<p><a href='https://openreview.net/forum?id=ymBG2xs9Zf'>https://openreview.net/forum?id=ymBG2xs9Zf</a></p>
<p><b>Keywords</b>: model learning, model-based control, neural network sparsification, mixed-integer programming, trajectory optimization
</p><p><b>Compressor summary</b>: The paper proposes a new framework for learning and controlling with deep neural networks by sparsifying them and using efficient optimization algorithms, achieving better performance in tasks involving contact dynamics.</p><hr><h3>Understanding How Consistency Works in Federated Learning via Stage-wise Relaxed Initialization</h3>
<p>Yan Sun, Li Shen, Dacheng Tao</p>
<p><a href='https://openreview.net/forum?id=ylPX5D7It7'>https://openreview.net/forum?id=ylPX5D7It7</a></p>
<p><b>Keywords</b>: federated learning, local consistency, personalized initialization, excess risk
</p><p><b>Compressor summary</b>: The paper proposes FedInit, an efficient federated learning algorithm that uses personalized relaxed initialization to reduce the "client drift" problem and improve performance in distributed training of global models.</p><hr><h3>Deep Momentum Multi-Marginal Schrödinger Bridge</h3>
<p>Tianrong Chen, Guan-Horng Liu, Molei Tao, Evangelos Theodorou</p>
<p><a href='https://openreview.net/forum?id=ykvvv0gc4R'>https://openreview.net/forum?id=ykvvv0gc4R</a></p>
<p><b>Keywords</b>: Schrödinger Bridge, Trajectory Inference, Optimal Transport
</p><p><b>Compressor summary</b>: The paper presents DMSB, a novel computational framework for inferring high-dimensional multi-marginal trajectories from unlabeled samples at coarse time intervals, which outperforms baselines and can reconstruct velocity distributions from position snapshots.</p><hr><h3>DiffTraj: Generating GPS Trajectory with Diffusion Probabilistic Model</h3>
<p>Yuanshao Zhu, Yongchao Ye, Shiyao Zhang, Xiangyu Zhao, James Yu</p>
<p><a href='https://openreview.net/forum?id=ykMdzevPkJ'>https://openreview.net/forum?id=ykMdzevPkJ</a></p>
<p><b>Keywords</b>: Trajectory Generation, Diffusion Model, Urban Computing, Spatial-temporal Data Mining
</p><p><b>Compressor summary</b>: The authors propose a model called DiffTraj that generates realistic and privacy-preserving GPS trajectories by denoising white noise using spatial-temporal features and a deep neural network.</p><hr><h3>Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension</h3>
<p>Moritz Haas, David Holzmüller, Ulrike von Luxburg, Ingo Steinwart</p>
<p><a href='https://openreview.net/forum?id=yjYwbZBJyl'>https://openreview.net/forum?id=yjYwbZBJyl</a></p>
<p><b>Keywords</b>: benign overfitting, kernels, neural tangent kernel, consistency, learning theory
</p><p><b>Compressor summary</b>: The key to benign overfitting is not high dimension but large derivatives in smooth estimators; this can be achieved by adding small fluctuations to the activation function of wide neural networks.</p><hr><h3>OBJECT 3DIT: Language-guided 3D-aware Image Editing</h3>
<p>Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, Tanmay Gupta</p>
<p><a href='https://openreview.net/forum?id=yjWVd8Fhqt'>https://openreview.net/forum?id=yjWVd8Fhqt</a></p>
<p><b>Keywords</b>: computer vision, image editing, generative modeling, diffusion models, 3D
</p><p><b>Compressor summary</b>: The authors propose a new task called language-guided 3D-aware editing, where edits are done according to language instructions while preserving the 3D scene, and release a large dataset for this task along with models that can perform well on it.</p><hr><h3>E2PNet: Event to Point Cloud Registration with Spatio-Temporal Representation Learning</h3>
<p>Xiuhong Lin, Changjie Qiu, zhipeng cai, Siqi Shen, Yu Zang, Weiquan Liu, Xuesheng Bian, Matthias Müller, Cheng Wang</p>
<p><a href='https://openreview.net/forum?id=yiehppUCO2'>https://openreview.net/forum?id=yiehppUCO2</a></p>
<p><b>Keywords</b>: event camera, 2D-3D registration, representation learning
</p><p><b>Compressor summary</b>: E2PNet is a novel method that registers 2D RGB images to 3D point clouds using event cameras, which improves robustness and enables other vision tasks.</p><hr><h3>Finite-Time Analysis of Whittle Index based Q-Learning for Restless Multi-Armed Bandits with Neural Network Function Approximation</h3>
<p>GUOJUN XIONG, Jian Li</p>
<p><a href='https://openreview.net/forum?id=yhNHpLWJDl'>https://openreview.net/forum?id=yhNHpLWJDl</a></p>
<p><b>Keywords</b>: Restless bandits, Whittle index policy, Q-learning, Two-timescale stochastic approximation, Neural network function approximation
</p><p><b>Compressor summary</b>: Neural-Q-Whittle is a Q-learning algorithm that uses neural networks and Whittle indices to solve intractable restless multi-armed bandits problems with an empirical convergence rate of $\mathcal{O}(1/k^{2/3})$.</p><hr><h3>Visual Programming for Step-by-Step Text-to-Image Generation and Evaluation</h3>
<p>Jaemin Cho, Abhay Zala, Mohit Bansal</p>
<p><a href='https://openreview.net/forum?id=yhBFG9Y85R'>https://openreview.net/forum?id=yhBFG9Y85R</a></p>
<p><b>Keywords</b>: text-to-image generation; visual programming; text-to-image evaluation; step-by-step generation; interpretability; explainability
</p><p><b>Compressor summary</b>: The paragraph introduces two novel visual programming frameworks, VPGen and VPEval, for text-to-image tasks that improve spatial control, handle predefined object classes, and provide interpretable and explainable evaluations with experts in different skills.</p><hr><h3>FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations</h3>
<p>Chanakya Ekbote, Ajinkya Deshpande, Arun Iyer, SUNDARARAJAN SELLAMANICKAM, Ramakrishna B Bairi</p>
<p><a href='https://openreview.net/forum?id=yh0OkiUk5h'>https://openreview.net/forum?id=yh0OkiUk5h</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Unsupervised Representation Learning, Graph Filters
</p><p><b>Compressor summary</b>: The paper introduces FiGURe, a method to improve node representations using filter-based augmentations and random Fourier feature projections, achieving up to 4.4% better performance on downstream tasks than existing unsupervised models.</p><hr><h3>Uncertainty Quantification over Graph with Conformalized Graph Neural Networks</h3>
<p>Kexin Huang, Ying Jin, Emmanuel Candes, Jure Leskovec</p>
<p><a href='https://openreview.net/forum?id=ygjQCOyNfh'>https://openreview.net/forum?id=ygjQCOyNfh</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Conformal Prediction, Uncertainty Quantification
</p><p><b>Compressor summary</b>: CF-GNN is a graph-based model that provides guaranteed uncertainty estimates by extending conformal prediction to graph data and using a topology-aware output correction model to reduce the prediction set size.</p><hr><h3>A generative model of the hippocampal formation trained with theta driven local learning rules</h3>
<p>Tom George, Kim Stachenfeld, Caswell Barry, Claudia Clopath, Tomoki Fukai</p>
<p><a href='https://openreview.net/forum?id=yft4JlxsRf'>https://openreview.net/forum?id=yft4JlxsRf</a></p>
<p><b>Keywords</b>: hippocampus, path integration, local learning, generative models, oscillations, inference, Helmholtz machine, wake-sleep
</p><p><b>Compressor summary</b>: The authors propose a biologically plausible model of the hippocampus that uses theta-band oscillations and a wake-sleep algorithm to learn and generate realistic sensory predictions and path integration.</p><hr><h3>PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust Generalization</h3>
<p>Jiancong Xiao, Ruoyu Sun, Zhi-Quan Luo</p>
<p><a href='https://openreview.net/forum?id=ydKWoqWZ3t'>https://openreview.net/forum?id=ydKWoqWZ3t</a></p>
<p><b>Keywords</b>: Pac-Bayes, Adversarial Robustness, Generalization
</p><p><b>Compressor summary</b>: The paper proposes a new robust generalization bound for deep neural networks that does not require additional assumptions and is tighter than existing bounds, offering insights into the causes of poor robust generalization.</p><hr><h3>Understanding the detrimental class-level effects of data augmentation</h3>
<p>Polina Kirichenko, Mark Ibrahim, Randall Balestriero, Diane Bouchacourt, Shanmukha Ramakrishna Vedantam, Hamed Firooz, Andrew Gordon Wilson</p>
<p><a href='https://openreview.net/forum?id=yageaKlk7S'>https://openreview.net/forum?id=yageaKlk7S</a></p>
<p><b>Keywords</b>: data augmentation, class-dependent bias
</p><p><b>Compressor summary</b>: The paper studies how data augmentation affects image classification, especially for classes with ambiguous or fine-grained distinctions, and proposes class-conditional augmentation strategies to improve performance.</p><hr><h3>Complexity of Derivative-Free Policy Optimization for Structured $\mathcal{H}_\infty$ Control</h3>
<p>Xingang Guo, Darioush Keivan, Geir Dullerud, Peter Seiler, Bin Hu</p>
<p><a href='https://openreview.net/forum?id=yaJ4vZPnHX'>https://openreview.net/forum?id=yaJ4vZPnHX</a></p>
<p><b>Keywords</b>: Structured $\mathcal{H}_\infty$ Control, Nonsmooth Optimization, Complexity Analysis
</p><p><b>Compressor summary</b>: This paper presents new theoretical results on the difficulty of optimizing policies without derivatives for a class of robust control tasks, and shows how to find approximate solutions using zeroth-order oracles.</p><hr><h3>Star-Shaped Denoising Diffusion Probabilistic Models</h3>
<p>Andrey Okhotin, Dmitry Molchanov, Arkhipkin Sergeevich Vladimir, Grigory Bartosh, Viktor Ohanesian, Aibek Alanov, Dmitry P. Vetrov</p>
<p><a href='https://openreview.net/forum?id=yYUdgbmhh9'>https://openreview.net/forum?id=yYUdgbmhh9</a></p>
<p><b>Keywords</b>: Generative models, Diffusion, Exponential Family
</p><p><b>Compressor summary</b>: The paper introduces Star-Shaped Diffusion Probabilistic Models, which enable generative modeling with non-Gaussian distributions using a simpler and more efficient approach than existing methods.</p><hr><h3>Diverse Shape Completion via Style Modulated Generative Adversarial Networks</h3>
<p>Wesley Khademi, Li Fuxin</p>
<p><a href='https://openreview.net/forum?id=yVMlYSL1Bp'>https://openreview.net/forum?id=yVMlYSL1Bp</a></p>
<p><b>Keywords</b>: multimodal shape completion, point cloud completion, 3d shape generation, generative modeling, generative adversarial networks
</p><p><b>Compressor summary</b>: The paper proposes a new network that can generate diverse, plausible 3D shape completions from partial observations using style modulation and multiple discriminators to prevent mode collapse.</p><hr><h3>PGDiff: Guiding Diffusion Models for Versatile Face Restoration via Partial Guidance</h3>
<p>Peiqing Yang, Shangchen Zhou, Qingyi Tao, Chen Change Loy</p>
<p><a href='https://openreview.net/forum?id=yThjbzhIUP'>https://openreview.net/forum?id=yThjbzhIUP</a></p>
<p><b>Keywords</b>: Face Restoration, Diffusion
</p><p><b>Compressor summary</b>: The paper introduces a new restoration approach called partial guidance that adapts to real-world degradations by modeling desired properties of high-quality images and applying them during the reverse diffusion process, achieving good results across various tasks.</p><hr><h3>Fast Conditional Mixing of MCMC Algorithms for Non-log-concave Distributions</h3>
<p>Xiang Cheng, Bohan Wang, Jingzhao Zhang, Yusong Zhu</p>
<p><a href='https://openreview.net/forum?id=yT0f93CeTw'>https://openreview.net/forum?id=yT0f93CeTw</a></p>
<p><b>Keywords</b>: Sampling, MCMC, Conditional Mixing, Non-log-concave Distributions
</p><p><b>Compressor summary</b>: The paragraph explains that MCMC algorithms can work efficiently when a subset of the state space has Poincaré-style inequality, even if global mixing is slow, and this can help in various applications like sampling from mixtures of Gaussians or parameter estimation.</p><hr><h3>Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance</h3>
<p>Lisha Chen, Heshan Devaka Fernando, Yiming Ying, Tianyi Chen</p>
<p><a href='https://openreview.net/forum?id=yPkbdJxQ0o'>https://openreview.net/forum?id=yPkbdJxQ0o</a></p>
<p><b>Keywords</b>: Generalization, algorithm stability, multi-objective optimization, gradient conflict
</p><p><b>Compressor summary</b>: The MoDo algorithm studies the stability of dynamic weighting methods for multi-objective learning and reveals how updating along conflict-avoidant directions may hinder optimal generalization performance.</p><hr><h3>Generalized Information-theoretic Multi-view Clustering</h3>
<p>Weitian Huang, Sirui Yang, Hongmin Cai</p>
<p><a href='https://openreview.net/forum?id=yN6NHZOXkg'>https://openreview.net/forum?id=yN6NHZOXkg</a></p>
<p><b>Keywords</b>: information bottleneck, multi-view clustering, variational autoencoders
</p><p><b>Compressor summary</b>: The paper proposes an information-based multi-view clustering model that uses deep neural networks and Stochastic Gradient Variational Bayes for representation learning and clustering, achieving better results than existing methods.</p><hr><h3>Mechanism Design for Collaborative Normal Mean Estimation</h3>
<p>Yiding Chen, Jerry Zhu, Kirthevasan Kandasamy</p>
<p><a href='https://openreview.net/forum?id=yKCLfOOIL7'>https://openreview.net/forum?id=yKCLfOOIL7</a></p>
<p><b>Keywords</b>: Mechanism design, statistical minimax estimation, federated learning
</p><p><b>Compressor summary</b>: The paper proposes a novel mechanism for estimating the mean of a normal distribution collaboratively among strategic agents, using corrupted data sharing and minimax optimal estimators to incentivize truthful reporting and reduce estimation errors.</p><hr><h3>Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift</h3>
<p>Xingdong Feng, Xin HE, Caixing Wang, Chao Wang, Jingnan Zhang</p>
<p><a href='https://openreview.net/forum?id=yIcCkMUCtL'>https://openreview.net/forum?id=yIcCkMUCtL</a></p>
<p><b>Keywords</b>: kernel methods, covariate shift, reproducing kernel Hilbert space (RKHS)
</p><p><b>Compressor summary</b>: The paper proposes a general analysis of nonparametric methods under covariate shift in RKHS, with applications to various learning tasks and theoretical and numerical results.</p><hr><h3>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</h3>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang</p>
<p><a href='https://openreview.net/forum?id=yHdTscY6Ci'>https://openreview.net/forum?id=yHdTscY6Ci</a></p>
<p><b>Keywords</b>: LLM, ChatGPT, Hugging Face, Autonomous LLM
</p><p><b>Compressor summary</b>: HuggingGPT is an LLM-powered agent that uses ChatGPT to connect various AI models in Hugging Face to solve complicated AI tasks across different domains and modalities, moving toward artificial general intelligence.</p><hr><h3>What Can We Learn from Unlearnable Datasets?</h3>
<p>Pedro Sandoval-Segura, Vasu Singla, Jonas Geiping, Micah Goldblum, Tom Goldstein</p>
<p><a href='https://openreview.net/forum?id=yGs9vTRjaE'>https://openreview.net/forum?id=yGs9vTRjaE</a></p>
<p><b>Keywords</b>: data poisoning, poisons, unlearnable dataset, data protection, imperceptible perturbations, adversarial machine learning
</p><p><b>Compressor summary</b>: Unlearnable dataset methods may not protect data privacy as they can enable neural networks to learn useful features, and linear separability of perturbations is not a necessary condition for this.</p><hr><h3>Augmented Memory Replay-based Continual Learning Approaches for Network Intrusion Detection</h3>
<p>Suresh kumar Amalapuram, Sumohana S. Channappayya, Bheemarjuna Tamma</p>
<p><a href='https://openreview.net/forum?id=yGLokEhdh9'>https://openreview.net/forum?id=yGLokEhdh9</a></p>
<p><b>Keywords</b>: Continual learning, Class imbalance, scalability, Network intrusion detection, and Cybersecurity
</p><p><b>Compressor summary</b>: The paper proposes two methods to improve continual learning-based intrusion detection: one for class imbalance and another for reducing computations, which perform better than baselines on various benchmarks.</p><hr><h3>Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency</h3>
<p>Owen Queen, Thomas Hartvigsen, Teddy Koker, Huan He, Theodoros Tsiligkaridis, Marinka Zitnik</p>
<p><a href='https://openreview.net/forum?id=yEfmhgwslQ'>https://openreview.net/forum?id=yEfmhgwslQ</a></p>
<p><b>Keywords</b>: Explainability, Interpretability, Time Series, Explanations, Temporal patterns, Model Understanding, Latent space, Self-supervised learning
</p><p><b>Compressor summary</b>: TimeX is a model that trains an interpretable surrogate to mimic a pretrained time series model and provides discrete attribution maps and latent space explanations for interpreting time series predictions.</p><hr><h3>Convergence of Adam Under Relaxed Assumptions</h3>
<p>Haochuan Li, Alexander Rakhlin, Ali Jadbabaie</p>
<p><a href='https://openreview.net/forum?id=yEewbkBNzi'>https://openreview.net/forum?id=yEewbkBNzi</a></p>
<p><b>Keywords</b>: Non-convex optimization, Adam, Convergence, Variance reduction
</p><p><b>Compressor summary</b>: The paper proves that the Adam algorithm for optimization converges to stationary points under realistic conditions and introduces a variance-reduced version with faster convergence.</p><hr><h3>Advancing Bayesian Optimization via Learning Correlated Latent Space</h3>
<p>Seunghun Lee, Jaewon Chu, Sihyeon Kim, Juyeon Ko, Hyunwoo J. Kim</p>
<p><a href='https://openreview.net/forum?id=yE62KM4qsO'>https://openreview.net/forum?id=yE62KM4qsO</a></p>
<p><b>Keywords</b>: Bayesian optimization, smoothness regularization, variational autoencoder
</p><p><b>Compressor summary</b>: Correlated latent space Bayesian Optimization (CoBO) is a method that learns correlated latent spaces to reduce the gap between optimization in the latent space and the input space, achieving high performance on discrete data tasks with limited function evaluations.</p><hr><h3>Closing the gap between the upper bound and lower bound of Adam's iteration complexity</h3>
<p>Bohan Wang, Jingwen Fu, Huishuai Zhang, Nanning Zheng, Wei Chen</p>
<p><a href='https://openreview.net/forum?id=yDvb3mlogA'>https://openreview.net/forum?id=yDvb3mlogA</a></p>
<p><b>Keywords</b>: Adam, Convergence, Upper Bound, Lower Bound
</p><p><b>Compressor summary</b>: This paper derives a new convergence guarantee for Adam optimization with smooth conditions and bounded noise variance, filling a gap in existing literature and providing a tight upper bound.</p><hr><h3>Credal Marginal MAP</h3>
<p>Radu Marinescu, Debarun Bhattacharjya, Junkyu Lee, Fabio Cozman, Alexander G. Gray</p>
<p><a href='https://openreview.net/forum?id=yCBqKTvYe9'>https://openreview.net/forum?id=yCBqKTvYe9</a></p>
<p><b>Keywords</b>: graphical models, credal networks, probabilistic inference
</p><p><b>Compressor summary</b>: Credal Marginal MAP inference is explored in this paper, with new exact and approximation methods proposed and evaluated on various problems.</p><hr><h3>Siamese Masked Autoencoders</h3>
<p>Agrim Gupta, Jiajun Wu, Jia Deng, Li Fei-Fei</p>
<p><a href='https://openreview.net/forum?id=yC3q7vInux'>https://openreview.net/forum?id=yC3q7vInux</a></p>
<p><b>Keywords</b>: Representation Learning, Visual Correspondence, Self-supervised learning, Videos
</p><p><b>Compressor summary</b>: Siamese Masked Autoencoders (SiamMAE) is a method for learning visual correspondence from videos by predicting missing patches in one frame based on another frame with masked patches, focusing on object motion and achieving competitive results on various tasks.</p><hr><h3>Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective</h3>
<p>Yifei Wang, Liangchen Li, Jiansheng Yang, Zhouchen Lin, Yisen Wang</p>
<p><a href='https://openreview.net/forum?id=yBoVwpGa5E'>https://openreview.net/forum?id=yBoVwpGa5E</a></p>
<p><b>Keywords</b>: Adversarial Training
</p><p><b>Compressor summary</b>: Adversarial Training (AT) has become arguably the state-of-the-art algorithm for extracting robust features. However, researchers recently notice that AT suffers from severe robust overfitting problems, particularly after learning rate (LR) decay. In this paper, we explain this phenomenon by viewing adversarial training as a dynamic minimax game between the model trainer and the attacker. Specifically, we analyze how LR decay breaks the balance between the minimax game by empowering the trainer with a stronger memorization ability, and show such imbalance induces robust overfitting as a result of memorizing non-robust features. We validate this understanding with extensive experiments, and provide a holistic view of robust overfitting from the dynamics of both the two game players. This understanding further inspires us to alleviate robust overfitting by rebalancing the two players by either regularizing the trainer's capacity or improving the attack strength. Experiments show that the proposed ReBalanced Adversarial Training (ReBAT) can attain good robustness and does not suffer from robust overfitting even after very long training. Code is available at https://github.com/PKU-ML/ReBAT.</p><hr><h3>Error Discovery By Clustering Influence Embeddings</h3>
<p>Fulton Wang, Julius Adebayo, Sarah Tan, Diego Garcia-Olano, Narine Kokhlikyan</p>
<p><a href='https://openreview.net/forum?id=yBVLXvJ1sb'>https://openreview.net/forum?id=yBVLXvJ1sb</a></p>
<p><b>Keywords</b>: Debugging, interpretability, influence functions
</p><p><b>Compressor summary</b>: The authors introduce InfEmden, a slice discovery method that finds coherent groups of test examples where a model under-performs by using influence embeddings and K-Means clustering.</p><hr><h3>Operation-Level Early Stopping for Robustifying Differentiable NAS</h3>
<p>Shen Jiang, Zipeng Ji, Guanghui Zhu, Chunfeng Yuan, Yihua Huang</p>
<p><a href='https://openreview.net/forum?id=yAOwkf4FyL'>https://openreview.net/forum?id=yAOwkf4FyL</a></p>
<p><b>Keywords</b>: Differentiable neural architecture search; Image classification; Failure of DARTS
</p><p><b>Compressor summary</b>: The paper proposes a method called operation-level early stopping (OLES) to improve Differentiable NAS (DARTS) by addressing the overfitting issue caused by skip connections in neural architecture search.</p><hr><h3>SNEkhorn: Dimension Reduction with Symmetric Entropic Affinities</h3>
<p>Hugues Van Assel, Titouan Vayer, Rémi Flamary, Nicolas Courty</p>
<p><a href='https://openreview.net/forum?id=y9U0IJ2uFr'>https://openreview.net/forum?id=y9U0IJ2uFr</a></p>
<p><b>Keywords</b>: Dimension Reduction, Optimal Transport, Affinities
</p><p><b>Compressor summary</b>: The paragraph discusses a novel way of symmetrizing entropic affinities using optimal transport, which improves clustering performance and robustness in dimensionality reduction algorithms like t-SNE.</p><hr><h3>Private Everlasting Prediction</h3>
<p>Moni Naor, Kobbi Nissim, Uri Stemmer, Chao Yan</p>
<p><a href='https://openreview.net/forum?id=y8UAQQHVTX'>https://openreview.net/forum?id=y8UAQQHVTX</a></p>
<p><b>Keywords</b>: Differential privacy, private learning, private prediction
</p><p><b>Compressor summary</b>: The paper proposes private everlasting prediction, a method to protect the privacy of both training data and queries in answering a stream of classification questions, with lower sample complexity than existing methods.</p><hr><h3>On the Importance of Exploration for Generalization in Reinforcement Learning</h3>
<p>Yiding Jiang, J Zico Kolter, Roberta Raileanu</p>
<p><a href='https://openreview.net/forum?id=y5duN2j9s6'>https://openreview.net/forum?id=y5duN2j9s6</a></p>
<p><b>Keywords</b>: reinforcement learning, generalization, procgen, crafter
</p><p><b>Compressor summary</b>: The paper proposes EDE, a method that encourages exploration in deep RL by using an ensemble of Q-value distributions to handle epistemic uncertainty and improve generalization to new environments.</p><hr><h3>CSOT: Curriculum and Structure-Aware Optimal Transport for Learning with Noisy Labels</h3>
<p>Wanxing Chang, Ye Shi, Jingya Wang</p>
<p><a href='https://openreview.net/forum?id=y50AnAbKp1'>https://openreview.net/forum?id=y50AnAbKp1</a></p>
<p><b>Keywords</b>: Learning with Noisy Labels, Optimal Transport, Curriculum Learning
</p><p><b>Compressor summary</b>: The paper proposes a novel optimal transport method, CSOT, to address challenges in learning with noisy labels by considering sample distribution structure and assigning reliable labels incrementally.</p><hr><h3>Learning Causal Models under Independent Changes</h3>
<p>Sarah Mameche, David Kaltenpoth, Jilles Vreeken</p>
<p><a href='https://openreview.net/forum?id=y0OlQSZsyp'>https://openreview.net/forum?id=y0OlQSZsyp</a></p>
<p><b>Keywords</b>: independent mechanisms, causal discovery, information theory, gaussian processes
</p><p><b>Compressor summary</b>: The paper proposes a method to discover causal relationships in systems with changing components using Gaussian Process models and independence assumptions.</p><hr><h3>WITRAN: Water-wave Information Transmission and Recurrent Acceleration Network for Long-range Time Series Forecasting</h3>
<p>Yuxin Jia, Youfang Lin, Xinyan Hao, Yan Lin, Shengnan Guo, Huaiyu Wan</p>
<p><a href='https://openreview.net/forum?id=y08bkEtNBK'>https://openreview.net/forum?id=y08bkEtNBK</a></p>
<p><b>Keywords</b>: long-range time series forecasting, information transmission, long- and short-term repetitive patterns, global and local correlations
</p><p><b>Compressor summary</b>: The WITRAN framework captures semantic information for long-range time series forecasting using a bi-granular information transmission and horizontal vertical gated selective unit, while reducing time and memory complexity compared to previous methods.</p><hr><h3>Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D Diffusion</h3>
<p>weitao Du, Jiujiu Chen, Xuecang Zhang, Zhi-Ming Ma, Shengchao Liu</p>
<p><a href='https://openreview.net/forum?id=xzmaFfw6oh'>https://openreview.net/forum?id=xzmaFfw6oh</a></p>
<p><b>Keywords</b>: Molecule Joint Auto-encoding, Molecule Joint Self-supervised Learning, Markov processes, contrastive learning, molecule representation learning
</p><p><b>Compressor summary</b>: The proposed method, MoleculeJAE, learns both 2D and 3D molecular information using a pretraining technique, achieving state-of-the-art results on various tasks.</p><hr><h3>Color Equivariant Convolutional Networks</h3>
<p>Attila Lengyel, Ombretta Strafforello, Robert-Jan Bruintjes, Alexander Gielisse, Jan van Gemert</p>
<p><a href='https://openreview.net/forum?id=xz8j3r3oUA'>https://openreview.net/forum?id=xz8j3r3oUA</a></p>
<p><b>Keywords</b>: color equivariance, equivariance, color robustness, equivariant convolutions
</p><p><b>Compressor summary</b>: The paper introduces CEConvs, a novel deep learning building block that allows shape feature sharing across different colors while preserving important color information, improving robustness to color changes and performance on various tasks.</p><hr><h3>Towards Hybrid-grained Feature Interaction Selection for Deep Sparse Network</h3>
<p>Fuyuan Lyu, Xing Tang, Dugang Liu, Chen Ma, Weihong Luo, Liang Chen, xiuqiang He, Xue Liu</p>
<p><a href='https://openreview.net/forum?id=xxfHMqNcum'>https://openreview.net/forum?id=xxfHMqNcum</a></p>
<p><b>Keywords</b>: Feature Interaction Search, Deep Sparse Network
</p><p><b>Compressor summary</b>: The paper introduces a hybrid-grained feature interaction selection approach for deep sparse networks that targets both feature field and feature value, and shows its effectiveness on three benchmark datasets.</p><hr><h3>BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis</h3>
<p>Zelin Ni, Hang Yu, Shizhan Liu, Jianguo Li, Weiyao Lin</p>
<p><a href='https://openreview.net/forum?id=xx3qRKvG0T'>https://openreview.net/forum?id=xx3qRKvG0T</a></p>
<p><b>Keywords</b>: time series forecasting, basis learning, self-supervised learning
</p><p><b>Compressor summary</b>: BasisFormer is a time series forecasting architecture that uses adaptive self-supervised learning, cross-attention, and similarity coefficients to create tailored bases for accurate predictions.</p><hr><h3>Unsupervised Polychromatic Neural Representation for CT Metal Artifact Reduction</h3>
<p>Qing Wu, Lixuan Chen, Ce Wang, Hongjiang Wei, S Kevin Zhou, Jingyi Yu, Yuyao Zhang</p>
<p><a href='https://openreview.net/forum?id=xx3QgKyghS'>https://openreview.net/forum?id=xx3QgKyghS</a></p>
<p><b>Keywords</b>: Medical Image, Computed Tomography, Metal Arftiacts, Implicit Neural Representation, Unsupervised Learning
</p><p><b>Compressor summary</b>: The paragraph describes a novel polychromatic neural representation (Polyner) method to improve computed tomography (CT) imaging with metallic implants by modeling the metal artifact reduction problem from a nonlinear inverse perspective and achieving better performance than supervised methods.</p><hr><h3>Personalized Dictionary Learning for Heterogeneous Datasets</h3>
<p>Geyu Liang, Naichen Shi, Raed Al Kontar, Salar Fattahi</p>
<p><a href='https://openreview.net/forum?id=xw6Szwu4xz'>https://openreview.net/forum?id=xw6Szwu4xz</a></p>
<p><b>Keywords</b>: Dictionary Learning, Data Heterogeneity, Personalization
</p><p><b>Compressor summary</b>: The paper introduces PerDL, a problem where sparse linear representations are learned from heterogeneous datasets with shared and unique features, and proposes PerMA, an efficient meta-algorithm that can recover the global and local dictionaries for various learning tasks.</p><hr><h3>TIES-Merging: Resolving Interference When Merging Models</h3>
<p>Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, Mohit Bansal</p>
<p><a href='https://openreview.net/forum?id=xtaX3WyCj1'>https://openreview.net/forum?id=xtaX3WyCj1</a></p>
<p><b>Keywords</b>: Model Merging, Fusing, Collaborative Training, Robust Fine-tuning, Federated Learning
</p><p><b>Compressor summary</b>: TIES-Merging is a novel method to combine task-specific models into a multitask model by resolving parameter interference and exploiting sign information, which leads to better performance in various settings.</p><hr><h3>Faster Discrete Convex Function Minimization with Predictions: The M-Convex Case</h3>
<p>Taihei Oki, Shinsaku Sakaue</p>
<p><a href='https://openreview.net/forum?id=xtQ9IGRzIW'>https://openreview.net/forum?id=xtQ9IGRzIW</a></p>
<p><b>Keywords</b>: algorithms with predictions, beyond the worst-case analysis of algorithms, time complexity, combinatorial optimization, discrete convex analysis, submodular functions
</p><p><b>Compressor summary</b>: The paper presents a framework to accelerate discrete optimization algorithms, especially laminar convex minimization, by using machine-learned predictions.</p><hr><h3>Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach</h3>
<p>Kai Zhao, Qiyu Kang, Yang Song, Rui She, Sijie Wang, Wee Peng Tay</p>
<p><a href='https://openreview.net/forum?id=xtADRDRsM2'>https://openreview.net/forum?id=xtADRDRsM2</a></p>
<p><b>Keywords</b>: adversarial robustness, graph neural networks
</p><p><b>Compressor summary</b>: This paper explores how using conservative Hamiltonian neural flows can improve the adversarial robustness of graph neural networks, which are otherwise vulnerable to perturbations affecting both node features and graph topology.</p><hr><h3>QuIP: 2-Bit Quantization of Large Language Models With Guarantees</h3>
<p>Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa</p>
<p><a href='https://openreview.net/forum?id=xrk9g5vcXR'>https://openreview.net/forum?id=xrk9g5vcXR</a></p>
<p><b>Keywords</b>: Quantization, Large Language Models, Adaptive Rounding, Theoretical Guarantees
</p><p><b>Compressor summary</b>: The paper introduces QuIP, a new method for post-training parameter quantization in large language models that uses incoherence processing to improve results and reduce bits per weight needed.</p><hr><h3>FaceComposer: A Unified Model for Versatile Facial Content Creation</h3>
<p>Jiayu Wang, Kang Zhao, Yifeng Ma, Shiwei Zhang, Yingya Zhang, Yujun Shen, Deli Zhao, Jingren Zhou</p>
<p><a href='https://openreview.net/forum?id=xrK3QA9mLo'>https://openreview.net/forum?id=xrK3QA9mLo</a></p>
<p><b>Keywords</b>: diffusion model; talking face generation; face generation
</p><p><b>Compressor summary</b>: FaceComposer is a versatile generative model for creating facial content with text control and animation, based on the latent diffusion framework, and it includes a large-scale multi-modal face database and an easy-to-use interface.</p><hr><h3>Calibrating “Cheap Signals” in Peer Review without a Prior</h3>
<p>Yuxuan Lu, Yuqing Kong</p>
<p><a href='https://openreview.net/forum?id=xr3KAzboHY'>https://openreview.net/forum?id=xr3KAzboHY</a></p>
<p><b>Keywords</b>: Peer prediction, Peer review, Calibration
</p><p><b>Compressor summary</b>: The paper proposes a one-shot noise calibration process for ranking papers fairly by predicting others' scores, which reduces error probability and outperforms average ratings.</p><hr><h3>Beyond Unimodal: Generalising Neural Processes for Multimodal Uncertainty Estimation</h3>
<p>Myong Chol Jung, He Zhao, Joanna Dipnall, Lan Du</p>
<p><a href='https://openreview.net/forum?id=xq1QvViDdW'>https://openreview.net/forum?id=xq1QvViDdW</a></p>
<p><b>Keywords</b>: Uncertainty estimation, multimodality, neural processes
</p><p><b>Compressor summary</b>: MNPs are a novel approach for estimating uncertainty in multimodal data that adapts Neural Processes, providing robustness and reliability with faster computation.</p><hr><h3>StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners</h3>
<p>Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, Dilip Krishnan</p>
<p><a href='https://openreview.net/forum?id=xpjsOQtKqx'>https://openreview.net/forum?id=xpjsOQtKqx</a></p>
<p><b>Keywords</b>: representation learning, synthetic images, text-to-image models
</p><p><b>Compressor summary</b>: The paper explores using text-to-image models to generate synthetic images for learning visual representations, and shows that their method outperforms existing methods on large scale datasets.</p><hr><h3>Fitting trees to $\ell_1$-hyperbolic distances</h3>
<p>Joon-Hyeok Yim, Anna Gilbert</p>
<p><a href='https://openreview.net/forum?id=xo2lbfQE8I'>https://openreview.net/forum?id=xo2lbfQE8I</a></p>
<p><b>Keywords</b>: tree metric fitting, ultrametric fitting, $\ell_1$-hyperbolicity
</p><p><b>Compressor summary</b>: The paper proposes an algorithm to fit trees based on hyperbolicity values, which improves upon previous algorithms and reveals differences between real and synthetic data sets.</p><hr><h3>Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation</h3>
<p>Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, BIN FU, Tao Chen, Gang YU, Shenghua Gao</p>
<p><a href='https://openreview.net/forum?id=xmxgMij3LY'>https://openreview.net/forum?id=xmxgMij3LY</a></p>
<p><b>Keywords</b>: Conditional 3D Shape Generation, Neural 3D Representation, 3D Reconstruction
</p><p><b>Compressor summary</b>: The paper proposes a new method for generating 3D shapes from 2D images or texts by aligning the representations of 3D shapes in different modalities and using two models to encode and decode them.</p><hr><h3>Text Alignment Is An Efficient Unified Model for Massive NLP Tasks</h3>
<p>Yuheng Zha, Yichi Yang, Ruichen Li, Zhiting Hu</p>
<p><a href='https://openreview.net/forum?id=xkkBFePoFn'>https://openreview.net/forum?id=xkkBFePoFn</a></p>
<p><b>Keywords</b>: Text Alignment, Efficient Unified Model, NLU Tasks, Factual Consistency Evaluation, QA with Unanswerable Question
</p><p><b>Compressor summary</b>: The paper proposes a small and efficient text alignment model that performs well on various NLP tasks, including text entailment, similarity, question answering, and factual consistency, outperforming larger LLMs in some cases.</p><hr><h3>Asymptotics of Bayesian Uncertainty Estimation in Random Features Regression</h3>
<p>Youngsoo Baek, Samuel Berchuck, Sayan Mukherjee</p>
<p><a href='https://openreview.net/forum?id=xgzkuTGBTx'>https://openreview.net/forum?id=xgzkuTGBTx</a></p>
<p><b>Keywords</b>: asymptotics, random features model, Bayesian inference
</p><p><b>Compressor summary</b>: This paper studies how the uncertainty of Bayesian models relates to the performance of their maximum likelihood estimates in high-dimensional settings, finding agreement in certain regimes and similarity in others.</p><hr><h3>Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit Bias for Correlated Inputs</h3>
<p>Dmitry Chistikov, Matthias Englert, Ranko Lazic</p>
<p><a href='https://openreview.net/forum?id=xgY4QcOiEZ'>https://openreview.net/forum?id=xgY4QcOiEZ</a></p>
<p><b>Keywords</b>: implicit bias, implicit regularization, training dynamics, ReLU networks, gradient flow, theoretical analysis
</p><p><b>Compressor summary</b>: The paper investigates how one-hidden layer ReLU networks learn a single neuron task by gradient flow and shows that they implicitly minimize the rank of network parameters while converging to zero loss.</p><hr><h3>Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted Networks</h3>
<p>Steven Adriaensen, Herilalaina Rakotoarison, Samuel Müller, Frank Hutter</p>
<p><a href='https://openreview.net/forum?id=xgTV6rmH6n'>https://openreview.net/forum?id=xgTV6rmH6n</a></p>
<p><b>Keywords</b>: learning curve extrapolation, prior-data fitted networks, transformers, Bayesian inference, uncertainty estimation, model selection
</p><p><b>Compressor summary</b>: The authors propose LC-PFN, a fast and accurate Bayesian method for learning curve extrapolation using prior-data fitted neural networks, which can improve model selection efficiency by enabling predictive early stopping.</p><hr><h3>Physics-Informed Bayesian Optimization of Variational Quantum Circuits</h3>
<p>Kim Andrea Nicoli, Christopher J. Anders, Lena Funcke, Tobias Hartung, Karl Jansen, Stefan Kuhn, Klaus Robert Muller, Paolo Stornati, Pan Kessel, Shinichi Nakajima</p>
<p><a href='https://openreview.net/forum?id=xfBeVGJwyL'>https://openreview.net/forum?id=xfBeVGJwyL</a></p>
<p><b>Keywords</b>: Bayesian optimization, Expected improvement, Quantum computing, Variational Quantum Eigensolvers
</p><p><b>Compressor summary</b>: The paper presents a new Bayesian optimization method for VQEs that uses a novel kernel and acquisition function to reduce uncertainty and improve performance.</p><hr><h3>Efficient Symbolic Policy Learning with Differentiable Symbolic Expression</h3>
<p>Jiaming Guo, Rui Zhang, Shaohui Peng, Qi Yi, Xing Hu, Ruizhi Chen, Zidong Du, Xishan Zhang, Ling Li, Qi Guo, Yunji Chen</p>
<p><a href='https://openreview.net/forum?id=xdQpmUPNHC'>https://openreview.net/forum?id=xdQpmUPNHC</a></p>
<p><b>Keywords</b>: reinforcement learning, context variables, symbolic policy
</p><p><b>Compressor summary</b>: The paper proposes ESPL, an efficient gradient-based method to learn simple and interpretable symbolic policies from scratch for sequential decision-making tasks using a differentiable symbolic expression and a path selector.</p><hr><h3>Towards Data-Agnostic Pruning At Initialization: What Makes a Good Sparse Mask?</h3>
<p>Hoang Pham, The-Anh Ta, Shiwei Liu, Lichuan Xiang, Dung D. Le, Hongkai Wen, Long Tran-Thanh</p>
<p><a href='https://openreview.net/forum?id=xdOoCWCYaY'>https://openreview.net/forum?id=xdOoCWCYaY</a></p>
<p><b>Keywords</b>: Pruning Neural Network, Sparsity, Neural Architecture Search
</p><p><b>Compressor summary</b>: The paper proposes a new framework and method for pruning neural networks at initialization that balances effective nodes and paths, improving accuracy and computational efficiency compared to existing methods.</p><hr><h3>Adversarial Resilience in Sequential Prediction via Abstention</h3>
<p>Surbhi Goel, Steve Hanneke, Shay Moran, Abhishek Shetty</p>
<p><a href='https://openreview.net/forum?id=xcGhx9FdxM'>https://openreview.net/forum?id=xcGhx9FdxM</a></p>
<p><b>Keywords</b>: Sequential prediction, adversarial examples, abstention, out-of-distribution, VC Classes
</p><p><b>Compressor summary</b>: The paper proposes a new model for sequential prediction that allows learners to abstain from making predictions on adversarial examples and designs learners with better error guarantees than existing methods.</p><hr><h3>On Evaluating Adversarial Robustness of Large Vision-Language Models</h3>
<p>Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-man Cheung, Min Lin</p>
<p><a href='https://openreview.net/forum?id=xbbknN9QFs'>https://openreview.net/forum?id=xbbknN9QFs</a></p>
<p><b>Keywords</b>: Large Vision-Language Models, Adversarial Robustness
</p><p><b>Compressor summary</b>: The paragraph discusses evaluating the robustness of vision-language models (VLMs) to targeted adversarial examples and their potential security flaws before deployment.</p><hr><h3>Practical Equivariances via Relational Conditional Neural Processes</h3>
<p>Daolang Huang, Manuel Haussmann, Ulpu Remes, S. T. John, Grégoire Clarté, Kevin Sebastian Luck, Samuel Kaski, Luigi Acerbi</p>
<p><a href='https://openreview.net/forum?id=xax5eWeObb'>https://openreview.net/forum?id=xax5eWeObb</a></p>
<p><b>Keywords</b>: neural processes, equivariance, Gaussian processes
</p><p><b>Compressor summary</b>: Relational Conditional Neural Processes (RCNPs) are a new model that improves the performance of equivariant neural processes in higher dimensions for various machine learning tasks.</p><hr><h3>For SALE: State-Action Representation Learning for Deep Reinforcement Learning</h3>
<p>Scott Fujimoto, Wei-Di Chang, Edward J. Smith, Shixiang Shane Gu, Doina Precup, David Meger</p>
<p><a href='https://openreview.net/forum?id=xZvGrzRq17'>https://openreview.net/forum?id=xZvGrzRq17</a></p>
<p><b>Keywords</b>: Deep reinforcement learning, representation learning
</p><p><b>Compressor summary</b>: The paper introduces SALE, a method for learning embeddings from low-level states that improve representation learning in reinforcement learning, and combines it with TD3 to create TD7, which performs better than existing continuous control algorithms on OpenAI gym benchmark tasks.</p><hr><h3>Learning Curves for Noisy Heterogeneous Feature-Subsampled Ridge Ensembles</h3>
<p>Benjamin Samuel Ruben, Cengiz Pehlevan</p>
<p><a href='https://openreview.net/forum?id=xXfDB8kJUs'>https://openreview.net/forum?id=xXfDB8kJUs</a></p>
<p><b>Keywords</b>: ridge regression, ensembling methods
</p><p><b>Compressor summary</b>: Feature bagging reduces prediction variance by combining predictions of many estimators trained on subsets or projections of features, and can mitigate double-descent in noisy least-squares ridge ensembles.</p><hr><h3>Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy</h3>
<p>Dongmin Park, Seola Choi, Doyoung Kim, Hwanjun Song, Jae-Gil Lee</p>
<p><a href='https://openreview.net/forum?id=xWCp0uLcpG'>https://openreview.net/forum?id=xWCp0uLcpG</a></p>
<p><b>Keywords</b>: Data Pruning, Data Subset Selection, Noisy Labels, Relabeling, Self-training
</p><p><b>Compressor summary</b>: Prune4Rel is a novel data pruning algorithm that maximizes re-labeling accuracy and generalization performance by finding a subset of training examples with high prediction confidence of their neighbors in the subset.</p><hr><h3>Fair Graph Distillation</h3>
<p>Qizhang Feng, Zhimeng Jiang, Ruiquan Li, Yicheng Wang, Na Zou, Jiang Bian, Xia Hu</p>
<p><a href='https://openreview.net/forum?id=xW0ayZxPWs'>https://openreview.net/forum?id=xW0ayZxPWs</a></p>
<p><b>Keywords</b>: Graph Distillation, Algorithmic Fairness
</p><p><b>Compressor summary</b>: The paper proposes a method to create smaller graphs that are both accurate and fair for graph neural networks, by developing a new metric called coherence and using a bi-level optimization algorithm.</p><hr><h3>Rethinking Incentives in Recommender Systems: Are Monotone Rewards Always Beneficial?</h3>
<p>Fan Yao, Chuanhao Li, Karthik Abinav Sankararaman, Yiming Liao, Yan Zhu, Qifan Wang, Hongning Wang, Haifeng Xu</p>
<p><a href='https://openreview.net/forum?id=xUyBP16Q5J'>https://openreview.net/forum?id=xUyBP16Q5J</a></p>
<p><b>Keywords</b>: Recommender system, Mechanism design, Potential function, Optimization
</p><p><b>Compressor summary</b>: The text discusses how online content recommendation platforms affect media creators' choices and welfare, and proposes a new reward mechanism called Backward Rewarding Mechanisms to improve the platform's design.</p><hr><h3>Compact Neural Volumetric Video Representations with Dynamic Codebooks</h3>
<p>Haoyu Guo, Sida Peng, Yunzhi Yan, Linzhan Mou, Yujun Shen, Hujun Bao, Xiaowei Zhou</p>
<p><a href='https://openreview.net/forum?id=xTgM7XLN9P'>https://openreview.net/forum?id=xTgM7XLN9P</a></p>
<p><b>Keywords</b>: Computer Vision, 3D Vision, Volumetric Video
</p><p><b>Compressor summary</b>: The paper introduces a novel neural representation called dynamic codebook that compresses volumetric videos by merging similar features and compensating with dynamic codes, achieving better quality and storage efficiency.</p><hr><h3>Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs</h3>
<p>Peng Jin, Yang Wu, Yanbo Fan, Zhongqian Sun, Yang Wei, Li Yuan</p>
<p><a href='https://openreview.net/forum?id=xSEhb2j3TK'>https://openreview.net/forum?id=xSEhb2j3TK</a></p>
<p><b>Keywords</b>: Text-driven Motion Synthesis, Diffusion Models, Graph networks
</p><p><b>Compressor summary</b>: The paper proposes using hierarchical semantic graphs to generate fine-grained human motion from text descriptions, improving over existing methods that may focus too much on action names and lack detail.</p><hr><h3>Robust Model Reasoning and Fitting via Dual Sparsity Pursuit</h3>
<p>Xingyu Jiang, Jiayi Ma</p>
<p><a href='https://openreview.net/forum?id=xRfTcZdQxq'>https://openreview.net/forum?id=xRfTcZdQxq</a></p>
<p><b>Keywords</b>: Model reasoning; Model fitting; Outliers; Sparse subspace learning; Feature matching
</p><p><b>Compressor summary</b>: The paper proposes a unified optimization model for outlier rejection, true model reasoning and parameter estimation, which uses sparse subspace recovery to search for independent bases in an over-embedded data space.</p><hr><h3>The Tunnel Effect: Building Data Representations in Deep Neural Networks</h3>
<p>Wojciech Masarczyk, Mateusz Ostaszewski, Ehsan Imani, Razvan Pascanu, Piotr Miłoś, Tomasz Trzcinski</p>
<p><a href='https://openreview.net/forum?id=xQOHOpe1Fv'>https://openreview.net/forum?id=xQOHOpe1Fv</a></p>
<p><b>Keywords</b>: representation learning, continual learning, training dynamics
</p><p><b>Compressor summary</b>: The paper investigates how deep neural networks split into two parts during image classification training, where the initial layers create separable representations and the latter layers, called "the tunnel," compress them without significantly affecting performance.</p><hr><h3>Stability of Random Forests and Coverage of Random-Forest Prediction Intervals</h3>
<p>Yan Wang, Huaiqing Wu, Dan Nettleton</p>
<p><a href='https://openreview.net/forum?id=xPqINp0Eu1'>https://openreview.net/forum?id=xPqINp0Eu1</a></p>
<p><b>Keywords</b>: Stability, Prediction Intervals, Random Forests
</p><p><b>Compressor summary</b>: The paper shows how random forests are stable and can provide good interval predictions using out-of-bag error and weak assumptions.</p><hr><h3>Learning DAGs from Data with Few Root Causes</h3>
<p>Panagiotis Misiakos, Chris Wendler, Markus Püschel</p>
<p><a href='https://openreview.net/forum?id=xPLaXSuSvQ'>https://openreview.net/forum?id=xPLaXSuSvQ</a></p>
<p><b>Keywords</b>: directed acyclic graph, few root causes, structural equation models, linear SEMs, additive noise
</p><p><b>Compressor summary</b>: The paper proposes a new algorithm for learning directed acyclic graphs (DAGs) from linear structural equation models (SEMs) with fewer root causes and noise, which improves upon previous methods.</p><hr><h3>CrossGNN: Confronting Noisy Multivariate Time Series Via Cross Interaction Refinement</h3>
<p>Qihe Huang, Lei Shen, Ruixin Zhang, Shouhong Ding, Binwu Wang, Zhengyang Zhou, Yang Wang</p>
<p><a href='https://openreview.net/forum?id=xOzlW2vUYc'>https://openreview.net/forum?id=xOzlW2vUYc</a></p>
<p><b>Keywords</b>: Time Series Forecasting;
</p><p><b>Compressor summary</b>: CrossGNN is a linear complexity GNN model that improves multivariate time series forecasting by refining cross-scale and cross-variable interactions using an adaptive multi-scale identifier and two specialized GNN components.</p><hr><h3>Proximity-Informed Calibration for Deep Neural Networks</h3>
<p>Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, Bryan Hooi</p>
<p><a href='https://openreview.net/forum?id=xOJUmwwlJc'>https://openreview.net/forum?id=xOJUmwwlJc</a></p>
<p><b>Keywords</b>: Calibration, Uncertainty Estimation, Trustworthiness, Fairness, Multicalibration
</p><p><b>Compressor summary</b>: The text discusses a common issue in calibration algorithms called proximity bias, which causes models to be more overconfident in low proximity data, and proposes a new algorithm (ProCal) to address it.</p><hr><h3>RL-based Stateful Neural Adaptive Sampling and Denoising for Real-Time Path Tracing</h3>
<p>Antoine Scardigli, Lukas Cavigelli, Lorenz K Muller</p>
<p><a href='https://openreview.net/forum?id=xNyR7DXUzJ'>https://openreview.net/forum?id=xNyR7DXUzJ</a></p>
<p><b>Keywords</b>: computer graphics, rendering, ray tracing, GPU acceleration, RL, spatiotemporal latent space
</p><p><b>Compressor summary</b>: The paper proposes a framework that uses reinforcement learning and neural networks to improve Monte-Carlo path tracing, reducing noise and rendering times for realistic image synthesis in real-time applications.</p><hr><h3>Optimal Algorithms for the Inhomogeneous Spiked Wigner Model</h3>
<p>Alexander Pak, Justin Ko, Florent Krzakala</p>
<p><a href='https://openreview.net/forum?id=xNUmTRYtV1'>https://openreview.net/forum?id=xNUmTRYtV1</a></p>
<p><b>Keywords</b>: Spectral Method, Community detection, Wigner Spike model, Random Matrix, BBP transition, Approximate Message Passing, Spin glasses, Statistical Physics
</p><p><b>Compressor summary</b>: The paper proposes an AMP algorithm for a spiked Wigner problem with inhomogeneous noise and compares it with a spectral method, showing good performance and matching information-theoretic results.</p><hr><h3>Hierarchical Multi-Agent Skill Discovery</h3>
<p>Mingyu Yang, Yaodong Yang, Zhenbo Lu, Wengang Zhou, Houqiang Li</p>
<p><a href='https://openreview.net/forum?id=xMgO04HDOS'>https://openreview.net/forum?id=xMgO04HDOS</a></p>
<p><b>Keywords</b>: Multi-Agent Reinforcement Learning, Hierarchical Skill Discovery, Probabilistic Graphical Model
</p><p><b>Compressor summary</b>: The text describes a hierarchical algorithm called Hierarchical Multi-Agent Skill Discovery (HMASD) that learns team and individual skills for multi-agent tasks without extrinsic rewards, outperforming existing methods.</p><hr><h3>Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks</h3>
<p>Minki Kang, Seanie Lee, Jinheon Baek, Kenji Kawaguchi, Sung Ju Hwang</p>
<p><a href='https://openreview.net/forum?id=xJLEQQrFia'>https://openreview.net/forum?id=xJLEQQrFia</a></p>
<p><b>Keywords</b>: language model, distillation, reasoning, knowledge augmentation
</p><p><b>Compressor summary</b>: KARD is a novel method that enhances small LMs with rationales from large and external sources, achieving better results on knowledge-intensive reasoning tasks.</p><hr><h3>Robust Bayesian Satisficing</h3>
<p>Artun Saday, Y. Cahit Yıldırım, Cem Tekin</p>
<p><a href='https://openreview.net/forum?id=xINPCvgULc'>https://openreview.net/forum?id=xINPCvgULc</a></p>
<p><b>Keywords</b>: robust satisficing, regret minimization, Gaussian processes
</p><p><b>Compressor summary</b>: The paper introduces RoBOS, a novel robust Bayesian satisficing algorithm for noisy black-box optimization that performs well under distributional shifts.</p><hr><h3>Towards Better Dynamic Graph Learning: New Architecture and Unified Library</h3>
<p>Le Yu, Leilei Sun, Bowen Du, Weifeng Lv</p>
<p><a href='https://openreview.net/forum?id=xHNzWHbklj'>https://openreview.net/forum?id=xHNzWHbklj</a></p>
<p><b>Keywords</b>: dynamic graph learning, Transformer-based architecture, dynamic graph library
</p><p><b>Compressor summary</b>: DyGFormer is a new Transformer-based architecture for dynamic graph learning that leverages historical interactions and uses DyGLib, a unified library for rigorous evaluations.</p><hr><h3>State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding</h3>
<p>Devleena Das, Sonia Chernova, Been Kim</p>
<p><a href='https://openreview.net/forum?id=xGz0wAIJrS'>https://openreview.net/forum?id=xGz0wAIJrS</a></p>
<p><b>Keywords</b>: Concept-Based Explanations, Reinforcement Learning, Human-AI Interaction
</p><p><b>Compressor summary</b>: The paper proposes State2Explanation (S2E), a framework that learns to embed state-action pairs and concept-based explanations, which can improve both an RL agent's learning and users' understanding of the agent's decisions in sequential decision making tasks.</p><hr><h3>Boosting Spectral Clustering on Incomplete Data via Kernel Correction and Affinity Learning</h3>
<p>Fangchen Yu, Runze Zhao, Zhan Shi, Yiwen Lu, Jicong Fan, Yicheng Zeng, Jianfeng Mao, Wenye Li</p>
<p><a href='https://openreview.net/forum?id=xFtuNq23D5'>https://openreview.net/forum?id=xFtuNq23D5</a></p>
<p><b>Keywords</b>: Spectral Clustering, Incomplete Data, Kernel Correction, Self-expressive Affinity Learning
</p><p><b>Compressor summary</b>: The paper proposes two new methods to improve spectral clustering on incomplete data by enhancing the kernel matrix and learning adaptive affinity matrices.</p><hr><h3>Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards</h3>
<p>Hao Qin, Kwang-Sung Jun, Chicheng Zhang</p>
<p><a href='https://openreview.net/forum?id=xF89MjFbWp'>https://openreview.net/forum?id=xF89MjFbWp</a></p>
<p><b>Keywords</b>: multi-armed bandits, bounded rewards
</p><p><b>Compressor summary</b>: The paper analyzes KL-Maillard Sampling, a variant of Maillard Sampling that achieves finite-time gap-dependent regret bounds and asymptotic optimality in bandit problems with Bernoulli rewards.</p><hr><h3>Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning</h3>
<p>Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, Samet Oymak</p>
<p><a href='https://openreview.net/forum?id=xEhKwsqxMa'>https://openreview.net/forum?id=xEhKwsqxMa</a></p>
<p><b>Keywords</b>: chain-of-thought, in-context learning, attention, compositional learning, approximation, length generalization
</p><p><b>Compressor summary</b>: The study examines how chain-of-thought (CoT) helps transformers learn compositional functions more efficiently by breaking down the learning process into two phases and reducing sample complexity.</p><hr><h3>LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching</h3>
<p>Duy Minh Ho Nguyen, Hoang Nguyen, Nghiem Tuong Diep, Tan Ngoc Pham, Tri Cao, Binh T. Nguyen, Paul Swoboda, Nhat Ho, Shadi Albarqouni, Pengtao Xie, Daniel Sonntag, Mathias Niepert</p>
<p><a href='https://openreview.net/forum?id=xE7oH5iVGK'>https://openreview.net/forum?id=xE7oH5iVGK</a></p>
<p><b>Keywords</b>: medical imaging; self-supervised learning; graph matching; large-vision model
</p><p><b>Compressor summary</b>: LVM-Med is a family of deep networks trained on large-scale medical datasets that uses a novel self-supervised contrastive learning algorithm to improve performance on various medical tasks.</p><hr><h3>Probabilistic inverse optimal control for non-linear partially observable systems disentangles perceptual uncertainty and behavioral costs</h3>
<p>Dominik Straub, Matthias Schultheis, Heinz Koeppl, Constantin A. Rothkopf</p>
<p><a href='https://openreview.net/forum?id=xDHzQQ4lnC'>https://openreview.net/forum?id=xDHzQQ4lnC</a></p>
<p><b>Keywords</b>: inverse optimal control, probabilistic modeling, motor control, cognitive science
</p><p><b>Compressor summary</b>: The paper presents a probabilistic approach to inverse optimal control for partially observable stochastic non-linear systems with unobserved action signals, which can characterize behavior in sequential decision-making tasks under uncertainty and disentangle perceptual factors and behavioral costs.</p><hr><h3>SODA: Robust Training of Test-Time Data Adaptors</h3>
<p>Zige Wang, Yonggang Zhang, Zhen Fang, Long Lan, Wenjing Yang, Bo Han</p>
<p><a href='https://openreview.net/forum?id=xBqjoG0NxM'>https://openreview.net/forum?id=xBqjoG0NxM</a></p>
<p><b>Keywords</b>: test-time data adaptation, zeroth-order optimization, out-of-distribution generalization
</p><p><b>Compressor summary</b>: The paper proposes a method called SODA, which uses reliable labels and preserves data information, to improve test-time data adaptation and adapt deployed models to distribution shifts while maintaining privacy.</p><hr><h3>Do Not Marginalize Mechanisms, Rather Consolidate!</h3>
<p>Moritz Willig, Matej Zečević, Devendra Singh Dhami, Kristian Kersting</p>
<p><a href='https://openreview.net/forum?id=xBhvMu4J03'>https://openreview.net/forum?id=xBhvMu4J03</a></p>
<p><b>Keywords</b>: Structural Causal Models, Marginalization, Consolidation, Compression
</p><p><b>Compressor summary</b>: Consolidating causal mechanisms helps simplify large-scale structural causal models while maintaining their causality and interventional behavior.</p><hr><h3>Thrust: Adaptively Propels Large Language Models with External Knowledge</h3>
<p>Xinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu, Jianshu Chen</p>
<p><a href='https://openreview.net/forum?id=x9FOu3W6iy'>https://openreview.net/forum?id=x9FOu3W6iy</a></p>
<p><b>Keywords</b>: knowledge-intensive natural language processing, pre-trained language models, instance-level adaptive knowledge usage
</p><p><b>Compressor summary</b>: The paper proposes IAPEK, a technique that adaptively retrieves external knowledge for PTLMs using Thrust, a metric that measures models' instance-level knowledgeability, achieving higher cost-efficiency and performance improvement on 26% of the tasks.</p><hr><h3>Recasting Continual Learning as Sequence Modeling</h3>
<p>Soochan Lee, Jaehyeon Son, Gunhee Kim</p>
<p><a href='https://openreview.net/forum?id=x816mCbWpR'>https://openreview.net/forum?id=x816mCbWpR</a></p>
<p><b>Keywords</b>: meta-continual learning, sequence modeling, Transformers, efficient Transformers
</p><p><b>Compressor summary</b>: The authors propose a novel approach to continual learning by framing it as a sequence modeling problem and demonstrate its effectiveness using Transformers and their efficient variants on various benchmarks.</p><hr><h3>Lending Interaction Wings to Recommender Systems with Conversational Agents</h3>
<p>Jiarui Jin, Xianyu Chen, Fanghua Ye, Mengyue Yang, Yue Feng, Weinan Zhang, Yong Yu, Jun Wang</p>
<p><a href='https://openreview.net/forum?id=x7q7w07r6Y'>https://openreview.net/forum?id=x7q7w07r6Y</a></p>
<p><b>Keywords</b>: Conversational Agent, Recommender System, Conversational Recommendation
</p><p><b>Compressor summary</b>: CORE is a framework for chatbots to improve online recommendations by minimizing uncertainty through querying attributes or items.</p><hr><h3>Neural Ideal Large Eddy Simulation: Modeling Turbulence with Neural Stochastic Differential Equations</h3>
<p>Anudhyan Boral, Zhong Yi Wan, Leonardo Zepeda-Nunez, James Lottes, Qing Wang, Yi-Fan Chen, John Roberts Anderson, Fei Sha</p>
<p><a href='https://openreview.net/forum?id=x6cOcxRnxG'>https://openreview.net/forum?id=x6cOcxRnxG</a></p>
<p><b>Keywords</b>: partial differential equations, physics, turbulence, stochastic differential equations, physical simulation, neural differential equations
</p><p><b>Compressor summary</b>: The authors propose a neural network approach that combines ideal large eddy simulation and stochastic differential equations to model turbulent flows, achieving better accuracy and stability than traditional methods.</p><hr><h3>FedNAR: Federated Optimization with Normalized Annealing Regularization</h3>
<p>Junbo Li, Ang Li, Chong Tian, Qirong Ho, Eric Xing, Hongyi Wang</p>
<p><a href='https://openreview.net/forum?id=x5fs7TXKDc'>https://openreview.net/forum?id=x5fs7TXKDc</a></p>
<p><b>Keywords</b>: Federated learning, weight decay, adaptive hyperparameters
</p><p><b>Compressor summary</b>: The paper proposes FedNAR, a method to improve federated learning by adjusting weight decay and gradient co-clipping, which enhances convergence speed and model accuracy.</p><hr><h3>Improving *day-ahead* Solar Irradiance Time Series Forecasting by Leveraging Spatio-Temporal Context</h3>
<p>Oussama Boussif, Ghait Boukachab, Dan Assouline, Stefano Massaroli, Tianle Yuan, Loubna Benabbou, Yoshua Bengio</p>
<p><a href='https://openreview.net/forum?id=x5ZruOa4ax'>https://openreview.net/forum?id=x5ZruOa4ax</a></p>
<p><b>Keywords</b>: Time series forecasting, multi-modal learning, solar irradiance, context-enriched learning
</p><p><b>Compressor summary</b>: The paper proposes a deep learning architecture using satellite data to improve solar irradiance forecasting accuracy and uncertainty estimation, as well as introducing a new multi-modal dataset for training and testing.</p><hr><h3>Stochastic Distributed Optimization under Average Second-order Similarity: Algorithms and Analysis</h3>
<p>Dachao Lin, Yuze Han, Haishan Ye, Zhihua Zhang</p>
<p><a href='https://openreview.net/forum?id=x5JCDCvR4b'>https://openreview.net/forum?id=x5JCDCvR4b</a></p>
<p><b>Keywords</b>: distributed optimization, convex optimization, second-order similarity, client sampling
</p><p><b>Compressor summary</b>: The paper proposes two new algorithms for distributed optimization problems with improved communication complexity and shows their effectiveness in smoothness-free and ill-conditioned scenarios.</p><hr><h3>Online Constrained Meta-Learning: Provable Guarantees for Generalization</h3>
<p>Siyuan Xu, Minghui Zhu</p>
<p><a href='https://openreview.net/forum?id=x2xQEszznV'>https://openreview.net/forum?id=x2xQEszznV</a></p>
<p><b>Keywords</b>: meta-learning; generalization
</p><p><b>Compressor summary</b>: The paper introduces an online meta-learning framework that learns from constrained tasks and analyzes its optimality gaps and constraint violations, with a practical algorithm and experiments on imitation learning and few-shot classification.</p><hr><h3>Taming Local Effects in Graph-based Spatiotemporal Forecasting</h3>
<p>Andrea Cini, Ivan Marisca, Daniele Zambon, Cesare Alippi</p>
<p><a href='https://openreview.net/forum?id=x2PH6q32LR'>https://openreview.net/forum?id=x2PH6q32LR</a></p>
<p><b>Keywords</b>: time series forecasting, spatiotemporal forecasting, graph-based spatiotemporal forecasting, graph neural networks
</p><p><b>Compressor summary</b>: The paper explores the trade-off between global and local models in spatiotemporal forecasting using graph neural networks, and proposes a method to incorporate trainable node embeddings for better adaptation to different time series dynamics.</p><hr><h3>FAST: a Fused and Accurate Shrinkage Tree for Heterogeneous Treatment Effects Estimation</h3>
<p>Jia Gu, Caizhi Tang, Han Yan, Qing Cui, Longfei Li, JUN ZHOU</p>
<p><a href='https://openreview.net/forum?id=wzg0BsV8rQ'>https://openreview.net/forum?id=wzg0BsV8rQ</a></p>
<p><b>Keywords</b>: Data fusion, heterogeneous treatment effects estimation, shrinkage estimation, tree-based method
</p><p><b>Compressor summary</b>: The paper introduces FAST, a method for estimating treatment effects that uses both trial and observational data to improve accuracy and robustness by combining optimal weighting and tree-based techniques.</p><hr><h3>Searching for Optimal Per-Coordinate Step-sizes with Multidimensional Backtracking</h3>
<p>Frederik Kunstner, Victor S. Portella, Mark Schmidt, Nick Harvey</p>
<p><a href='https://openreview.net/forum?id=wzPcffMZ3b'>https://openreview.net/forum?id=wzPcffMZ3b</a></p>
<p><b>Keywords</b>: line-search, gradient descent, hypergradient, adaptive methods, smooth, convex, optimization, preconditioning
</p><p><b>Compressor summary</b>: The authors introduce multidimensional backtracking, a technique that finds optimal per-coordinate step-sizes for smooth convex problems by using hyper-gradients and cutting-plane methods.</p><hr><h3>Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation</h3>
<p>Ruida Zhou, Tao Liu, Min Cheng, Dileep Kalathil, Panganamala Kumar, Chao Tian</p>
<p><a href='https://openreview.net/forum?id=wxkBdtDbmH'>https://openreview.net/forum?id=wxkBdtDbmH</a></p>
<p><b>Keywords</b>: robust reinforcement learning, policy-based approach, function approximation, actor-critic
</p><p><b>Compressor summary</b>: The authors propose two new uncertainty sets for large-scale robust reinforcement learning and a robust natural actor-critic algorithm that converges to the optimal policy in finite time.</p><hr><h3>Federated Learning with Bilateral Curation for Partially Class-Disjoint Data</h3>
<p>Ziqing Fan, Ruipeng Zhang, Jiangchao Yao, Bo Han, Ya Zhang, Yanfeng Wang</p>
<p><a href='https://openreview.net/forum?id=wwmKVO8bsR'>https://openreview.net/forum?id=wwmKVO8bsR</a></p>
<p><b>Keywords</b>: federated learning, data heterogeneity, partially class-disjoint data
</p><p><b>Compressor summary</b>: FedGELA is a novel approach for federated learning with partially class-disjoint data that uses a simplex ETF classifier to achieve fair and equal discrimination for all classes, improving performance and providing convergence guarantees.</p><hr><h3>Adapting Fairness Interventions to Missing Values</h3>
<p>Raymond Feng, Flavio Calmon, Hao Wang</p>
<p><a href='https://openreview.net/forum?id=wwkQUiaKbo'>https://openreview.net/forum?id=wwkQUiaKbo</a></p>
<p><b>Keywords</b>: algorithmic fairness, discrimination, missing values, machine learning
</p><p><b>Compressor summary</b>: The paper analyzes how missing values in data affect algorithmic fairness, showing that the standard "impute-then-classify" approach can worsen fairness and accuracy, and presents new algorithms that handle missing patterns while preserving their information for better fairness and accuracy.</p><hr><h3>Subject-driven Text-to-Image Generation via Apprenticeship Learning</h3>
<p>Wenhu Chen, Hexiang Hu, YANDONG LI, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, William W. Cohen</p>
<p><a href='https://openreview.net/forum?id=wv3bHyQbX7'>https://openreview.net/forum?id=wv3bHyQbX7</a></p>
<p><b>Keywords</b>: Diffusion Model, Image Generation, Image Editing, In-Context Learning
</p><p><b>Compressor summary</b>: SuTI is a fast and high-quality text-to-image generator that learns from millions of online image clusters to produce customized images without fine-tuning for each subject.</p><hr><h3>Granger Components Analysis: Unsupervised learning of latent temporal dependencies</h3>
<p>Jacek Dmochowski</p>
<p><a href='https://openreview.net/forum?id=wqIm0Qsgy0'>https://openreview.net/forum?id=wqIm0Qsgy0</a></p>
<p><b>Keywords</b>: components analysis, unsupervised learning, Granger Causality
</p><p><b>Compressor summary</b>: The paper introduces a new unsupervised learning technique for time series data that uses Granger causality to identify driving and driven signals in multivariate data sets.</p><hr><h3>ContinuAR: Continuous Autoregression For Infinite-Fidelity Fusion</h3>
<p>WEI W. XING, Yuxin Wang, Zheng Xing</p>
<p><a href='https://openreview.net/forum?id=wpfsnu5syT'>https://openreview.net/forum?id=wpfsnu5syT</a></p>
<p><b>Keywords</b>: Gaussian process, autoregression, multi fidelity, nonparametric Bayesian
</p><p><b>Compressor summary</b>: The authors propose ContinuAR, a novel method for multi-fidelity fusion that improves efficiency and performance compared to existing techniques.</p><hr><h3>BayesDAG: Gradient-Based Posterior Inference for Causal Discovery</h3>
<p>Yashas Annadani, Nick Pawlowski, Joel Jennings, Stefan Bauer, Cheng Zhang, Wenbo Gong</p>
<p><a href='https://openreview.net/forum?id=woptnU6fh1'>https://openreview.net/forum?id=woptnU6fh1</a></p>
<p><b>Keywords</b>: Causal Discovery, Structure Learning, Bayesian Inference, Variational Inference, MCMC, Generative Model
</p><p><b>Compressor summary</b>: The text introduces a new Bayesian causal discovery method that samples DAGs from the posterior without regularization, using a combination of SG-MCMC and VI, and applies gradient-based MCMC sampling for the first time in this field.</p><hr><h3>Maximization of Average Precision for Deep Learning with Adversarial Ranking Robustness</h3>
<p>Gang Li, Wei Tong, Tianbao Yang</p>
<p><a href='https://openreview.net/forum?id=wm5Ane9VRO'>https://openreview.net/forum?id=wm5Ane9VRO</a></p>
<p><b>Keywords</b>: Adversarial Average Precision Maximization, Robust Average Precision, Adversarial Ranking Robustness, Adversarial Training
</p><p><b>Compressor summary</b>: The paper proposes a new method to optimize both average precision and adversarial robustness, outperforming current methods by a significant margin.</p><hr><h3>VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models</h3>
<p>Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho</p>
<p><a href='https://openreview.net/forum?id=wkIBfnGPTA'>https://openreview.net/forum?id=wkIBfnGPTA</a></p>
<p><b>Keywords</b>: backdoor, diffusion model, trustworthy
</p><p><b>Compressor summary</b>: The paper introduces a backdoor attack framework (VillanDiffusion) for diffusion models, which are generative AI models that can be vulnerable to output manipulation attacks when exposed to malicious patterns at the input.</p><hr><h3>Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models</h3>
<p>Lin Li, Jun Xiao, Guikun Chen, Jian Shao, Yueting Zhuang, Long Chen</p>
<p><a href='https://openreview.net/forum?id=wiv21EJ0Vd'>https://openreview.net/forum?id=wiv21EJ0Vd</a></p>
<p><b>Keywords</b>: Visual relation detection, Zero-short learning, Scene graph generation
</p><p><b>Compressor summary</b>: RECODE is a novel zero-shot VRD method that uses composite description prompts to improve relation detection by leveraging LLMs and spatial information.</p><hr><h3>Stein $\Pi$-Importance Sampling</h3>
<p>Congye Wang, Wilson Ye Chen, Heishiro Kanagawa, Chris J. Oates</p>
<p><a href='https://openreview.net/forum?id=wiidCRA3at'>https://openreview.net/forum?id=wiidCRA3at</a></p>
<p><b>Keywords</b>: Bayesian, discrepancy, kernel, sampling, Stein's method
</p><p><b>Compressor summary</b>: The paper proposes a new method for improving Monte Carlo output using Stein importance sampling and provides a novel variational construction and convergence conditions for it.</p><hr><h3>Outlier-Robust Wasserstein DRO</h3>
<p>Sloan Nietert, Ziv Goldfeld, Soroosh Shafiee</p>
<p><a href='https://openreview.net/forum?id=wg3d2FKAm8'>https://openreview.net/forum?id=wg3d2FKAm8</a></p>
<p><b>Keywords</b>: distributionally robust optimization, robust statistics, optimal transport, Wasserstein distance
</p><p><b>Compressor summary</b>: The paper proposes a new robust optimization approach that handles both geometric and non-geometric data perturbations and shows improved performance on regression and classification tasks.</p><hr><h3>Consistent Aggregation of Objectives with Diverse Time Preferences Requires Non-Markovian Rewards</h3>
<p>Silviu Pitis</p>
<p><a href='https://openreview.net/forum?id=wcdF6jR0Sp'>https://openreview.net/forum?id=wcdF6jR0Sp</a></p>
<p><b>Keywords</b>: Normative Agency Design, Reward Design, Sequential Decision Making, Reinforcement Learning, Intertemporal Fairness, Multi-Objective Decision Making
</p><p><b>Compressor summary</b>: The paper shows that optimal AI agents must use non-Markovian rewards when serving multiple objectives with different discount factors, and proposes a practical aggregation scheme for this purpose.</p><hr><h3>Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: FTRL with General Regularizers and Multiple Optimal Arms</h3>
<p>Tiancheng Jin, Junyan Liu, Haipeng Luo</p>
<p><a href='https://openreview.net/forum?id=wbg4JEM5Jp'>https://openreview.net/forum?id=wbg4JEM5Jp</a></p>
<p><b>Keywords</b>: multi-armed bandit, best of both worlds, Follow-the-Regularized-Leader, Tsallis entropy, Shannon entropy, Log-barrier
</p><p><b>Compressor summary</b>: The paper studies adaptive multi-armed bandit algorithms and improves the performance of FTRL with various regularizers and a new learning rate schedule, removing the uniqueness assumption and achieving better regret bounds.</p><hr><h3>High-dimensional Asymptotics of Denoising Autoencoders</h3>
<p>Hugo Cui, Lenka Zdeborova</p>
<p><a href='https://openreview.net/forum?id=wbbTqsiKzl'>https://openreview.net/forum?id=wbbTqsiKzl</a></p>
<p><b>Keywords</b>: statistical physics, replica method, autoencoder, exact asymptotics
</p><p><b>Compressor summary</b>: The paper proposes a two-layer non-linear autoencoder with tied weights and a skip connection for denoising high-dimensional data from a Gaussian mixture, and compares it to other methods.</p><hr><h3>Provable benefits of score matching</h3>
<p>Chirag Pabbaraju, Dhruv Rohatgi, Anish Sevekari, Holden Lee, Ankur Moitra, Andrej Risteski</p>
<p><a href='https://openreview.net/forum?id=waXoG35kbb'>https://openreview.net/forum?id=waXoG35kbb</a></p>
<p><b>Keywords</b>: theory, score matching, exponential families, sample complexity, computational hardness
</p><p><b>Compressor summary</b>: The paper presents a new exponential family of distributions where score matching is as easy to optimize as maximum likelihood, unlike other cases where score matching is harder or intractable.</p><hr><h3>Collaboratively Learning Linear Models with Structured Missing Data</h3>
<p>Chen Cheng, Gary Cheng, John Duchi</p>
<p><a href='https://openreview.net/forum?id=waDF0oACu2'>https://openreview.net/forum?id=waDF0oACu2</a></p>
<p><b>Keywords</b>: Collaborative Learning, Missing Data, Sensors, Linear Regression
</p><p><b>Compressor summary</b>: The paper proposes Collab, a distributed, semi-supervised algorithm for learning least squares estimates for m agents with different features, without communicating the labeled data, achieving near-optimal performance and generalizability.</p><hr><h3>Optimal Block-wise Asymmetric Graph Construction for Graph-based Semi-supervised Learning</h3>
<p>Zixing Song, Yifei Zhang, Irwin King</p>
<p><a href='https://openreview.net/forum?id=wYkfog48Bq'>https://openreview.net/forum?id=wYkfog48Bq</a></p>
<p><b>Keywords</b>: Graph-based Semi-supervised Learning, Affinity Graph Construction
</p><p><b>Compressor summary</b>: This paper proposes an optimal asymmetric graph structure for label inference in graph-based semi-supervised learning that differentiates the roles of labeled and unlabeled nodes, leading to improved performance on synthetic and real datasets.</p><hr><h3>Language-driven Scene Synthesis using Multi-conditional Diffusion Model</h3>
<p>An Dinh Vuong, Minh Nhat VU, Toan Tien Nguyen, Baoru Huang, Dzung Nguyen, Thieu Vo, Anh Nguyen</p>
<p><a href='https://openreview.net/forum?id=wYKU1C77sa'>https://openreview.net/forum?id=wYKU1C77sa</a></p>
<p><b>Keywords</b>: scene synthesis, language-driven, diffusion models, multi-conditional generation, 3D point cloud
</p><p><b>Compressor summary</b>: The paper proposes a new language-driven scene synthesis task that combines text, human motion, and objects, and presents a multi-conditional diffusion model to solve it effectively.</p><hr><h3>What can a Single Attention Layer Learn? A Study Through the Random Features Lens</h3>
<p>Hengyu Fu, Tianyu Guo, Yu Bai, Song Mei</p>
<p><a href='https://openreview.net/forum?id=wX8GuzDSJR'>https://openreview.net/forum?id=wX8GuzDSJR</a></p>
<p><b>Keywords</b>: transformers, attention, deep learning theory, random features
</p><p><b>Compressor summary</b>: The paper analyzes how attention layers in Transformers learn and generalize, showing they can efficiently learn certain functions with random features and have advantages over standard neural networks.</p><hr><h3>Transitivity Recovering Decompositions: Interpretable and Robust Fine-Grained Relationships</h3>
<p>Abhra Chaudhuri, Massimiliano Mancini, Zeynep Akata, Anjan Dutta</p>
<p><a href='https://openreview.net/forum?id=wUNPmdE273'>https://openreview.net/forum?id=wUNPmdE273</a></p>
<p><b>Keywords</b>: Interpretability, Robustness, Fine-Grained Representation Learning, Graph Theory, Information Theory
</p><p><b>Compressor summary</b>: The paper presents a method called TRD that converts abstract relational representations in image analysis into interpretable graphs and shows its effectiveness and robustness to noisy views.</p><hr><h3>Improving Diffusion-Based Image Synthesis with Context Prediction</h3>
<p>Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming Cai, Wentao Zhang, Bin CUI</p>
<p><a href='https://openreview.net/forum?id=wRhLd65bDt'>https://openreview.net/forum?id=wRhLd65bDt</a></p>
<p><b>Keywords</b>: Diffusion Model, Image Generation
</p><p><b>Compressor summary</b>: ConPreDiff is a novel diffusion model that predicts neighborhood context for better image synthesis and achieves state-of-the-art results in text-to-image generation.</p><hr><h3>Critical Initialization of Wide and Deep Neural Networks using Partial Jacobians: General Theory and Applications</h3>
<p>Darshil Doshi, Tianyu He, Andrey Gromov</p>
<p><a href='https://openreview.net/forum?id=wRJqZRxDEX'>https://openreview.net/forum?id=wRJqZRxDEX</a></p>
<p><b>Keywords</b>: Criticality, Gaussian Process, Jacobian, LayerNorm, Residual connections, ResNet
</p><p><b>Compressor summary</b>: The authors propose a method to diagnose criticality in deep neural networks using partial Jacobians, which helps select optimal initialization and show that LayerNorm and residual connections lead to a critical architecture for any initialization.</p><hr><h3>Small batch deep reinforcement learning</h3>
<p>Johan Samir Obando Ceron, Marc G Bellemare, Pablo Samuel Castro</p>
<p><a href='https://openreview.net/forum?id=wPqEvmwFEh'>https://openreview.net/forum?id=wPqEvmwFEh</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Deep Reinforcement Learning, Value based, Batch Size
</p><p><b>Compressor summary</b>: Reducing the batch size in value-based deep reinforcement learning can improve performance, contrary to the common belief that larger batches are better.</p><hr><h3>Progressive Ensemble Distillation: Building Ensembles for Efficient Inference</h3>
<p>Don Dennis, Abhishek Shetty, Anish Sevekari, Kazuhito Koishida, Virginia Smith</p>
<p><a href='https://openreview.net/forum?id=wNxyDofh74'>https://openreview.net/forum?id=wNxyDofh74</a></p>
<p><b>Keywords</b>: Edge computing, compression, efficient inference, distillation and inference, run-time tradeoff, inference-time tradeoff, on-device, user-side, client-side
</p><p><b>Compressor summary</b>: B-DISTIL is a method to create ensembles of smaller student models from a large pretrained teacher model using function composition based aggregation rules, achieving similar performance while allowing flexibility in accuracy vs. inference cost trade-offs.</p><hr><h3>Leveraging Early-Stage Robustness in Diffusion Models for Efficient and High-Quality Image Synthesis</h3>
<p>Yulhwa Kim, Dongwon Jo, Hyesung Jeon, Taesu Kim, Daehyun Ahn, Hyungjun Kim, jae-joon kim</p>
<p><a href='https://openreview.net/forum?id=wNpsGwixjG'>https://openreview.net/forum?id=wNpsGwixjG</a></p>
<p><b>Keywords</b>: diffusion models, post-training quantization
</p><p><b>Compressor summary</b>: The paper proposes a faster noise estimation method for diffusion models by using low-bit activations in the early reverse diffusion process, while keeping high-bit activations for later stages to maintain image quality.</p><hr><h3>A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)</h3>
<p>Weijie Tu, Weijian Deng, Tom Gedeon</p>
<p><a href='https://openreview.net/forum?id=wMNpMe0vp3'>https://openreview.net/forum?id=wMNpMe0vp3</a></p>
<p><b>Keywords</b>: CLIP
</p><p><b>Compressor summary</b>: This paper investigates the safety measures of CLIP models, focusing on their resilience to visual factor variations, uncertainty estimations, and anomaly detection, by testing them on various datasets and conditions.</p><hr><h3>Calibrating Neural Simulation-Based Inference with Differentiable Coverage Probability</h3>
<p>Maciej Falkiewicz, Naoya Takeishi, Imahn Shekhzadeh, Antoine Wehenkel, Arnaud Delaunoy, Gilles Louppe, Alexandros Kalousis</p>
<p><a href='https://openreview.net/forum?id=wLiMhVJ7fx'>https://openreview.net/forum?id=wLiMhVJ7fx</a></p>
<p><b>Keywords</b>: simulation-based inference, inverse problem, bayesian inference, uncertainty quantification, generative modeling
</p><p><b>Compressor summary</b>: Bayesian inference uses probability to express uncertainty, but existing methods can be overconfident; a new method adds calibration error to neural models for more accurate uncertainty quantification.</p><hr><h3>An Efficient and Robust Framework for Approximate Nearest Neighbor Search with Attribute Constraint</h3>
<p>Mengzhao Wang, Lingwei Lv, Xiaoliang Xu, Yuxiang Wang, Qiang Yue, Jiongkang Ni</p>
<p><a href='https://openreview.net/forum?id=wLFXTAWa5V'>https://openreview.net/forum?id=wLFXTAWa5V</a></p>
<p><b>Keywords</b>: approximate nearest neighbor search, attribute filtering, high-dimensional vector, proximity graph
</p><p><b>Compressor summary</b>: The paper presents a fast and accurate framework for hybrid query processing using composite indexes based on proximity graphs, improving both approximate nearest neighbor search and attribute filtering.</p><hr><h3>Learning a 1-layer conditional generative model in total variation</h3>
<p>Ajil Jalal, Justin Kang, Ananya Uppal, Kannan Ramchandran, Eric Price</p>
<p><a href='https://openreview.net/forum?id=wImYhdu4VF'>https://openreview.net/forum?id=wImYhdu4VF</a></p>
<p><b>Keywords</b>: Generative models, distribution learning, maximum likelihood estimation
</p><p><b>Compressor summary</b>: A conditional generative model is a method to sample from a conditional distribution without assumptions on the input distribution, and it can learn deep models using few samples.</p><hr><h3>A Single-Loop Accelerated Extra-Gradient Difference Algorithm with Improved Complexity Bounds for Constrained Minimax Optimization</h3>
<p>Yuanyuan Liu, Fanhua Shang, Weixin An, Junhao Liu, Hongying Liu, Zhouchen Lin</p>
<p><a href='https://openreview.net/forum?id=wIlmx4bHrO'>https://openreview.net/forum?id=wIlmx4bHrO</a></p>
<p><b>Keywords</b>: Constrained Minimax Optimization; nonconvex- nonconcave
</p><p><b>Compressor summary</b>: The paper presents a new algorithm for solving constrained nonconvex-nonconcave problems that improves convergence rates and complexity bounds compared to existing methods.</p><hr><h3>Online Corrupted User Detection and Regret Minimization</h3>
<p>Zhiyong Wang, Jize Xie, Tong Yu, Shuai Li, John C.S. Lui</p>
<p><a href='https://openreview.net/forum?id=wHhPIv5G8Q'>https://openreview.net/forum?id=wHhPIv5G8Q</a></p>
<p><b>Keywords</b>: online learning, online corrupted user detection, clustering of bandits
</p><p><b>Compressor summary</b>: The paper proposes an online learning problem and two novel algorithms to learn from user relations and detect corrupted users in web systems.</p><hr><h3>Restart Sampling for Improving Generative Processes</h3>
<p>Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian, Ziming Liu, Tommi S. Jaakkola</p>
<p><a href='https://openreview.net/forum?id=wFuemocyHZ'>https://openreview.net/forum?id=wFuemocyHZ</a></p>
<p><b>Keywords</b>: Generative models, diffusion models, PFGM, sampling
</p><p><b>Compressor summary</b>: Restart is a novel sampling algorithm that combines ODE and SDE methods to achieve faster and higher quality results in generative models.</p><hr><h3>Sharp Calibrated Gaussian Processes</h3>
<p>Alexandre Capone, Sandra Hirche, Geoff Pleiss</p>
<p><a href='https://openreview.net/forum?id=wFH5hZAwYz'>https://openreview.net/forum?id=wFH5hZAwYz</a></p>
<p><b>Keywords</b>: Gaussian Processes, Frequentist Statistics, Kernel Methods, Model Selection and Structure Learning, Regression
</p><p><b>Compressor summary</b>: The paragraph describes a new method for improving the uncertainty estimates of Gaussian processes by using a different set of hyperparameters that better satisfy calibration constraints and yield tighter predictive quantiles.</p><hr><h3>Improving Self-supervised Molecular Representation Learning using Persistent Homology</h3>
<p>Yuankai Luo, Lei Shi, Veronika Thost</p>
<p><a href='https://openreview.net/forum?id=wEiUGpcr0M'>https://openreview.net/forum?id=wEiUGpcr0M</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Molecular Representation Learning, Persistent Homology, Contrastive Learning, Self-supervised Learning
</p><p><b>Compressor summary</b>: The paper proposes a self-supervised learning method for molecular representation learning using persistent homology, which offers unique features and improves predictive power, especially in small datasets.</p><hr><h3>Ambient Diffusion: Learning Clean Distributions from Corrupted Data</h3>
<p>Giannis Daras, Kulin Shah, Yuval Dagan, Aravind Gollakota, Alex Dimakis, Adam Klivans</p>
<p><a href='https://openreview.net/forum?id=wBJBLy9kBY'>https://openreview.net/forum?id=wBJBLy9kBY</a></p>
<p><b>Keywords</b>: corrupted data, generative models, ambient gan, inverse problems, learning from measurements
</p><p><b>Compressor summary</b>: The paper proposes a diffusion-based method to learn an unknown distribution using highly-corrupted samples, which can train generative models that are less likely to memorize individual samples and work on standard benchmarks with significant pixel loss or corrupted datasets.</p><hr><h3>Interpretable Reward Redistribution in Reinforcement Learning: A Causal Approach</h3>
<p>Yudi Zhang, Yali Du, Biwei Huang, Ziyan Wang, Jun Wang, Meng Fang, Mykola Pechenizkiy</p>
<p><a href='https://openreview.net/forum?id=w7TyuWhGZP'>https://openreview.net/forum?id=w7TyuWhGZP</a></p>
<p><b>Keywords</b>: Reinforcement learning, sparse reward, return decomposition, causal modeling
</p><p><b>Compressor summary</b>: This paper proposes GRD, a framework that uses causal generative models to redistribute rewards in reinforcement learning for delayed reward scenarios and improves policy optimization by training over the most favorable state subspace.</p><hr><h3>InfoCD: A Contrastive Chamfer Distance Loss for Point Cloud Completion</h3>
<p>Fangzhou Lin, Yun Yue, Ziming Zhang, Songlin Hou, Kazunori Yamada, Vijaya B Kolachalama, Venkatesh Saligrama</p>
<p><a href='https://openreview.net/forum?id=w7LxAZfDfv'>https://openreview.net/forum?id=w7LxAZfDfv</a></p>
<p><b>Keywords</b>: Contrastive learning; Point cloud completion
</p><p><b>Compressor summary</b>: The paper introduces InfoCD, a contrastive Chamfer distance loss that improves point cloud completion and surface similarity estimation by maximizing mutual information between underlying surfaces, achieving state-of-the-art results on benchmark datasets.</p><hr><h3>Compositional Sculpting of Iterative Generative Processes</h3>
<p>Timur Garipov, Sebastiaan De Peuter, Ge Yang, Vikas Garg, Samuel Kaski, Tommi S. Jaakkola</p>
<p><a href='https://openreview.net/forum?id=w79RtqIyoM'>https://openreview.net/forum?id=w79RtqIyoM</a></p>
<p><b>Keywords</b>: generative model composition, GFlowNets, diffusion models, classifier guidance, probabilistic methods
</p><p><b>Compressor summary</b>: The paper proposes Compositional Sculpting, a method for combining and sampling from iterative generative models like GFlowNets and diffusion models using classifier guidance.</p><hr><h3>Hyper-HMM: aligning human brains and semantic features in a common latent event space</h3>
<p>Caroline Lee, Jane Han, Ma Feilong, Guo Jiahui, James Haxby, Christopher Baldassano</p>
<p><a href='https://openreview.net/forum?id=w6krZiUa7t'>https://openreview.net/forum?id=w6krZiUa7t</a></p>
<p><b>Keywords</b>: Brain Imaging, Other Cognitive Science, Other Neuroscience
</p><p><b>Compressor summary</b>: The Hyper-HMM is a hybrid model that aligns both temporal and spatial features across brains when processing naturalistic stimuli, allowing for mapping of individual cognitive dynamics and semantic content.</p><hr><h3>Minimax Optimal Rate for Parameter Estimation in Multivariate Deviated Models</h3>
<p>Dat Do, Huy Nguyen, Khai Nguyen, Nhat Ho</p>
<p><a href='https://openreview.net/forum?id=w3ghbKBJg4'>https://openreview.net/forum?id=w3ghbKBJg4</a></p>
<p><b>Keywords</b>: Mixture Model, Minimax Rate, Maximum Likelihood Estimation
</p><p><b>Compressor summary</b>: The paper studies how to estimate parameters in a multivariate model with a mixed density function, using a new condition called \emph{distinguishability} to deal with challenges in convergence rates.</p><hr><h3>Balanced Training for Sparse GANs</h3>
<p>Yite Wang, Jing Wu, Naira Hovakimyan, Ruoyu Sun</p>
<p><a href='https://openreview.net/forum?id=w2F8Fm6Sg3'>https://openreview.net/forum?id=w2F8Fm6Sg3</a></p>
<p><b>Keywords</b>: Dynamics sparse training; pruning; neural network pruning; empirical deep learning
</p><p><b>Compressor summary</b>: The paper introduces a new metric and method for balancing sparse training in generative adversarial networks (GANs) to improve efficiency without sacrificing performance.</p><hr><h3>Optimal Learners for Realizable Regression: PAC Learning and Online Learning</h3>
<p>Idan Attias, Steve Hanneke, Alkis Kalavasis, Amin Karbasi, Grigoris Velegkas</p>
<p><a href='https://openreview.net/forum?id=w116w62fxH'>https://openreview.net/forum?id=w116w62fxH</a></p>
<p><b>Keywords</b>: Learning Theory, Regression, PAC Learning, Online Learning
</p><p><b>Compressor summary</b>: The authors study the complexity of learning realizable regression in different settings, introduce new dimensions to characterize learnability, and design optimal learners for these settings.</p><hr><h3>Visual Instruction Tuning</h3>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee</p>
<p><a href='https://openreview.net/forum?id=w0H2xGHlkw'>https://openreview.net/forum?id=w0H2xGHlkw</a></p>
<p><b>Keywords</b>: visual instruction tuning, instruction tuning, multimodal, LLM, GPT
</p><p><b>Compressor summary</b>: The paper introduces LLaVA, a multimodal model that uses language to generate image instructions for GPT-4, improving its performance on various tasks, including Science QA.</p><hr><h3>GAUCHE: A Library for Gaussian Processes in Chemistry</h3>
<p>Ryan-Rhys Griffiths, Leo Klarner, Henry Moss, Aditya Ravuri, Sang T. Truong, Yuanqi Du, Samuel Don Stanton, Gary Tom, Bojana Ranković, Arian Rokkum Jamasb, Aryan Deshwal, Julius Schwartz, Austin Tripp, Gregory Kell, Simon Frieder, Anthony Bourached, Alex James Chan, Jacob Moss, Chengzhi Guo, Johannes P. Dürholt, Saudamini Chaurasia, Ji Won Park, Felix Strieth-Kalthoff, Alpha Lee, Bingqing Cheng, Alan Aspuru-Guzik, Philippe Schwaller, Jian Tang</p>
<p><a href='https://openreview.net/forum?id=vzrA6uqOis'>https://openreview.net/forum?id=vzrA6uqOis</a></p>
<p><b>Keywords</b>: Gaussian processes, Bayesian optimization, Chemistry, Molecular Machine Learning, Applications, Software
</p><p><b>Compressor summary</b>: GAUCHE is an open-source library that uses Gaussian processes to optimize molecular representations for applications like molecular discovery, chemical reaction optimization, and protein design.</p><hr><h3>Adaptive whitening with fast gain modulation and slow synaptic plasticity</h3>
<p>Lyndon Duong, Eero P Simoncelli, Dmitri Chklovskii, David Lipshutz</p>
<p><a href='https://openreview.net/forum?id=vz7SdRqWGM'>https://openreview.net/forum?id=vz7SdRqWGM</a></p>
<p><b>Keywords</b>: neuroscience, adaptation, whitening, efficient coding, recurrent neural network, gain modulation, synaptic plasticity, local learning rules
</p><p><b>Compressor summary</b>: The paragraph describes a new model that combines synaptic plasticity and gain modulation to rapidly adapt sensory neurons to changing input statistics.</p><hr><h3>Learning from Rich Semantics and Coarse Locations for Long-tailed Object Detection</h3>
<p>Lingchen Meng, Xiyang Dai, Jianwei Yang, Dongdong Chen, Yinpeng Chen, Mengchen Liu, Yi-Ling Chen, Zuxuan Wu, Lu Yuan, Yu-Gang Jiang</p>
<p><a href='https://openreview.net/forum?id=vybQs1Gbuk'>https://openreview.net/forum?id=vybQs1Gbuk</a></p>
<p><b>Keywords</b>: Long-tail object detection, visual semantics, soft supervision
</p><p><b>Compressor summary</b>: RichSem is a simple and effective method for long-tailed object detection that uses semantic branch to learn rich semantics from coarse locations as additional soft supervision without needing accurate bounding boxes.</p><hr><h3>Optimal Regret Is Achievable with Bounded Approximate Inference Error: An Enhanced Bayesian Upper Confidence Bound Framework</h3>
<p>Ziyi Huang, Henry Lam, Amirhossein Meisami, Haofeng Zhang</p>
<p><a href='https://openreview.net/forum?id=vwr4bHHsRT'>https://openreview.net/forum?id=vwr4bHHsRT</a></p>
<p><b>Keywords</b>: Bayesian bandits, approximate Bayesian inference, Bayesian Upper Confidence Bound, optimal regret order, bounded inference error
</p><p><b>Compressor summary</b>: The EBUCB framework improves on existing Bayesian bandit algorithms by achieving optimal regret bounds under constant approximate inference error using two different $\alpha$-divergences, while previous methods only had worst-case linear regret with one $\alpha$-divergence.</p><hr><h3>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</h3>
<p>Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi</p>
<p><a href='https://openreview.net/forum?id=vvoWPYqZJA'>https://openreview.net/forum?id=vvoWPYqZJA</a></p>
<p><b>Keywords</b>: Vision-Language Models, Instruction Tuning, Zero-shot
</p><p><b>Compressor summary</b>: The paper explores vision-language instruction tuning using pretrained BLIP-2 models and introduces a Query Transformer, achieving state-of-the-art results on various tasks.</p><hr><h3>Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning</h3>
<p>Shenzhi Wang, Qisen Yang, Jiawei Gao, Matthieu Gaetan Lin, HAO CHEN, Liwei Wu, Ning Jia, Shiji Song, Gao Huang</p>
<p><a href='https://openreview.net/forum?id=vtoY8qJjTR'>https://openreview.net/forum?id=vtoY8qJjTR</a></p>
<p><b>Keywords</b>: reinforcement learning, offline-to-online reinforcement learning, offline reinforcement learning, policy improvement, policy constraint
</p><p><b>Compressor summary</b>: FamO2O is a framework that adapts improvement and constraint balances for offline-to-online reinforcement learning based on each state, improving performance over existing methods.</p><hr><h3>The Geometry of Neural Nets' Parameter Spaces Under Reparametrization</h3>
<p>Agustinus Kristiadi, Felix Dangel, Philipp Hennig</p>
<p><a href='https://openreview.net/forum?id=vtLNwa6uX0'>https://openreview.net/forum?id=vtLNwa6uX0</a></p>
<p><b>Keywords</b>: neural network, invariance, equivariance, reparametrization, riemannian geometry, parameter space
</p><p><b>Compressor summary</b>: The paper studies how to represent the metric of neural nets under reparametrization using Riemannian geometry, which has implications for flatness analysis and optimization.</p><hr><h3>Revisiting the Minimalist Approach to Offline Reinforcement Learning</h3>
<p>Denis Tarasov, Vladislav Kurenkov, Alexander Nikulin, Sergey Kolesnikov</p>
<p><a href='https://openreview.net/forum?id=vqGWslLeEw'>https://openreview.net/forum?id=vqGWslLeEw</a></p>
<p><b>Keywords</b>: Offline Reinforcement Learning
</p><p><b>Compressor summary</b>: The authors analyze recent offline reinforcement learning algorithms, propose ReBRAC that integrates several design elements, and show its state-of-the-art performance in various settings and datasets.</p><hr><h3>Online PCA in Converging Self-consistent Field Equations</h3>
<p>Xihan Li, Xiang Chen, Rasul Tutunov, Haitham Bou Ammar, Lei Wang, Jun Wang</p>
<p><a href='https://openreview.net/forum?id=vq11gurmUY'>https://openreview.net/forum?id=vq11gurmUY</a></p>
<p><b>Keywords</b>: Self-consistent Field Equation, Computational Science, Online PCA
</p><p><b>Compressor summary</b>: The authors propose a novel approach to solve the SCF equation using online PCA techniques, which improves convergence and applies to electronic structure problems.</p><hr><h3>TransHP: Image Classification with Hierarchical Prompting</h3>
<p>Wenhao Wang, Yifan Sun, Wei Li, Yi Yang</p>
<p><a href='https://openreview.net/forum?id=vpQuCsZXz2'>https://openreview.net/forum?id=vpQuCsZXz2</a></p>
<p><b>Keywords</b>: hierarchical image classification, hierarchical prompting, vision transformer
</p><p><b>Compressor summary</b>: The paper proposes a novel hierarchical image classification method that uses tokenized ancestor-class hints to improve accuracy, efficiency, and explainability of image classification models.</p><hr><h3>Combinatorial Optimization with Policy Adaptation using Latent Space Search</h3>
<p>Felix Chalumeau, Shikha Surana, Clément Bonnet, Nathan Grinsztajn, Arnu Pretorius, Alexandre Laterre, Thomas D Barrett</p>
<p><a href='https://openreview.net/forum?id=vpMBqdt9Hl'>https://openreview.net/forum?id=vpMBqdt9Hl</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Combinatorial Optimization, TSP, CVRP, JSSP
</p><p><b>Compressor summary</b>: COMPASS is a novel RL approach that pre-trains diverse policies in a latent space to improve search performance for combinatorial optimization problems.</p><hr><h3>Conditional score-based diffusion models for Bayesian inference in infinite dimensions</h3>
<p>Lorenzo Baldassari, Ali Siahkoohi, Josselin Garnier, Knut Solna, Maarten V. de Hoop</p>
<p><a href='https://openreview.net/forum?id=voG6nEW9BV'>https://openreview.net/forum?id=voG6nEW9BV</a></p>
<p><b>Keywords</b>: score-based generative models, diffusion models, inverse problems, bayesian inference, infinite dimensions
</p><p><b>Compressor summary</b>: The paper proposes a new method for solving infinite-dimensional Bayesian linear inverse problems using amortized conditional score-based diffusion models, which improves upon previous heuristic approaches by being more theoretically grounded and computationally efficient.</p><hr><h3>Higher-Order Uncoupled Dynamics Do Not Lead to Nash Equilibrium - Except When They Do</h3>
<p>Sarah Asad Toonsi, Jeff S Shamma</p>
<p><a href='https://openreview.net/forum?id=vnTUuecp2v'>https://openreview.net/forum?id=vnTUuecp2v</a></p>
<p><b>Keywords</b>: Learning in games, Nash equilibrium, Uncoupled Dynamics
</p><p><b>Compressor summary</b>: The paper introduces higher-order gradient play dynamics for multi-agent learning that can lead to Nash Equilibrium or fail depending on the game and dynamics.</p><hr><h3>On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective</h3>
<p>Zeke Xie, zhiqiang xu, Jingzhao Zhang, Issei Sato, Masashi Sugiyama</p>
<p><a href='https://openreview.net/forum?id=vnGcubtzR1'>https://openreview.net/forum?id=vnGcubtzR1</a></p>
<p><b>Keywords</b>: Weight Decay, Regularization, Optimization, Deep Learning
</p><p><b>Compressor summary</b>: The paper proposes a new method called Scheduled Weight Decay (SWD) to reduce large gradient norms caused by weight decay, which can improve convergence and generalization in deep neural networks.</p><hr><h3>A Novel Approach for Effective Multi-View Clustering with Information-Theoretic Perspective</h3>
<p>Chenhang Cui, Yazhou Ren, Jingyu Pu, Jiawei Li, Xiaorong Pu, Tianyi Wu, Yutao Shi, Lifang He</p>
<p><a href='https://openreview.net/forum?id=vlDbqzwczj'>https://openreview.net/forum?id=vlDbqzwczj</a></p>
<p><b>Keywords</b>: multi-view learning， clustering
</p><p><b>Compressor summary</b>: SUMVC is a new information-theoretic approach for multi-view clustering that improves performance by reducing redundancy and enhancing consistent information across views.</p><hr><h3>Structured Voronoi Sampling</h3>
<p>Afra Amini, Li Du, Ryan Cotterell</p>
<p><a href='https://openreview.net/forum?id=vf77fTbgG3'>https://openreview.net/forum?id=vf77fTbgG3</a></p>
<p><b>Keywords</b>: Natural Language Processing, Text Generation, Controlled Generation, MCMC, HMC, Langevin Dynamics
</p><p><b>Compressor summary</b>: The paper introduces Structured Voronoi Sampling (SVS), a principled gradient-based technique for sampling from language models that generates fluent and diverse texts while adhering to control targets.</p><hr><h3>Extremal Domain Translation with Neural Optimal Transport</h3>
<p>Milena Gazdieva, Alexander Korotin, Daniil Selikhanovych, Evgeny Burnaev</p>
<p><a href='https://openreview.net/forum?id=vZRiMjo826'>https://openreview.net/forum?id=vZRiMjo826</a></p>
<p><b>Keywords</b>: optimal transport, partial optimal transport, neural networks, domain translation
</p><p><b>Compressor summary</b>: The paper introduces extremal transport, a method for unpaired image domain translation based on neural optimal transport and inspired by theoretical limits of possible translations.</p><hr><h3>ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning</h3>
<p>Junguang Jiang, Baixu Chen, Junwei Pan, Ximei Wang, Dapeng Liu, jie jiang, Mingsheng Long</p>
<p><a href='https://openreview.net/forum?id=vZHk1QlBQW'>https://openreview.net/forum?id=vZHk1QlBQW</a></p>
<p><b>Keywords</b>: Auxiliary-Task Learning, Negative Transfer
</p><p><b>Compressor summary</b>: ForkMerge is a novel approach that helps mitigate negative transfer in auxiliary-task learning by periodically forking the model into multiple branches and dynamically merging them based on task weights.</p><hr><h3>A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing</h3>
<p>Junren Chen, Jonathan Scarlett, Michael Ng, Zhaoqiang Liu</p>
<p><a href='https://openreview.net/forum?id=vUXNNLatFv'>https://openreview.net/forum?id=vUXNNLatFv</a></p>
<p><b>Keywords</b>: Compressed sensing, generative models, nonlinearity, uniform recovery
</p><p><b>Compressor summary</b>: The paper presents a unified framework for uniform recovery guarantees in nonlinear generative compressed sensing, handling discontinuous or unknown observation models, and achieves similar performance to existing non-uniform results with little additional cost.</p><hr><h3>Faster Margin Maximization Rates for Generic Optimization Methods</h3>
<p>Guanghui Wang, Zihao Hu, Vidya Muthukumar, Jacob Abernethy</p>
<p><a href='https://openreview.net/forum?id=vTug54Uunq'>https://openreview.net/forum?id=vTug54Uunq</a></p>
<p><b>Keywords</b>: Implicit bias, margin maximization, zero-sum game, online learning
</p><p><b>Compressor summary</b>: The paper proposes new techniques to improve the implicit bias rates of generic optimization methods like mirror descent and steepest descent in binary classification problems, using a unified framework based on regularized bilinear games.</p><hr><h3>Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity</h3>
<p>Zhanpeng Zhou, Yongyi Yang, Xiaojiang Yang, Junchi Yan, Wei Hu</p>
<p><a href='https://openreview.net/forum?id=vORUHrVEnH'>https://openreview.net/forum?id=vORUHrVEnH</a></p>
<p><b>Keywords</b>: Linear Mode Connectivity, Permutation Invariance, Optimization Landscape, Science of Deep Learning
</p><p><b>Compressor summary</b>: This paper introduces a concept called Layerwise Linear Feature Connectivity (LLFC), which shows how different neural networks can be connected by linear paths in their feature maps, based on the Linear Mode Connectivity phenomenon.</p><hr><h3>Data Pruning via Moving-one-Sample-out</h3>
<p>Haoru Tan, Sitong Wu, Fei Du, Yukang Chen, Zhibin Wang, Fan Wang, XIAOJUAN QI</p>
<p><a href='https://openreview.net/forum?id=vO6ZdPWaHc'>https://openreview.net/forum?id=vO6ZdPWaHc</a></p>
<p><b>Keywords</b>: Data Valuation, Deep Learning, Data Pruning, Coreset Selection.
</p><p><b>Compressor summary</b>: The paper presents MoSo, a data-pruning method that uses gradient information to identify and remove uninformative samples from the training set, improving performance at high pruning ratios.</p><hr><h3>Imitation Learning from Imperfection: Theoretical Justifications and Algorithms</h3>
<p>Ziniu Li, Tian Xu, Zeyu Qin, Yang Yu, Zhi-Quan Luo</p>
<p><a href='https://openreview.net/forum?id=vO04AzsB49'>https://openreview.net/forum?id=vO04AzsB49</a></p>
<p><b>Keywords</b>: imitation learning, distribution shift, policy optimization, data selection
</p><p><b>Compressor summary</b>: The paper presents a framework for imitation learning with supplementary data that addresses distribution shift issues using importance sampling to improve performance in various tasks.</p><hr><h3>Suggesting Variable Order for Cylindrical Algebraic Decomposition via Reinforcement Learning</h3>
<p>Fuqi Jia, Yuhang Dong, Minghao Liu, Pei Huang, Feifei Ma, Jian Zhang</p>
<p><a href='https://openreview.net/forum?id=vNsdFwjPtL'>https://openreview.net/forum?id=vNsdFwjPtL</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Graph Neural Network, Cylindrical Algebraic Decomposition.
</p><p><b>Compressor summary</b>: The paper introduces two reinforcement learning methods with graph neural networks for suggesting variable order in cylindrical algebraic decomposition, improving efficiency and generalizing well to different datasets.</p><hr><h3>Exploiting Correlated Auxiliary Feedback in Parameterized Bandits</h3>
<p>Arun Verma, Zhongxiang Dai, Yao Shu, Bryan Kian Hsiang Low</p>
<p><a href='https://openreview.net/forum?id=vM5VnNQ4n7'>https://openreview.net/forum?id=vM5VnNQ4n7</a></p>
<p><b>Keywords</b>: Parameterized Bandits, Auxiliary Feedback, Control Variate, Regret Minimization
</p><p><b>Compressor summary</b>: The paper proposes a method that uses auxiliary feedback to improve reward estimation, leading to reduced regret in parameterized bandits problems.</p><hr><h3>Transformer as a hippocampal memory consolidation model based on NMDAR-inspired nonlinearity</h3>
<p>Dong-Kyum Kim, Jea Kwon, Meeyoung Cha, C. Justin Lee</p>
<p><a href='https://openreview.net/forum?id=vKpVJxplmB'>https://openreview.net/forum?id=vKpVJxplmB</a></p>
<p><b>Keywords</b>: Transformer, NMDA, long-term memory, reference memory, memory consolidation
</p><p><b>Compressor summary</b>: The paper proposes a new activation function for deep learning models that mimics the hippocampus's NMDA receptor dynamics, improving working memory and spatial representation in transformers.</p><hr><h3>Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization</h3>
<p>Ruichen Jiang, Aryan Mokhtari</p>
<p><a href='https://openreview.net/forum?id=vIGNYQ4Alv'>https://openreview.net/forum?id=vIGNYQ4Alv</a></p>
<p><b>Keywords</b>: convex optimization, quasi-Newton methods, Monteiro-Svaiter acceleration, Nesterov's accelerated gradient, online learning
</p><p><b>Compressor summary</b>: The paper presents a new optimization method that beats Nesterov's accelerated gradient in certain regimes and achieves optimal or faster convergence rates based on an online learning approach.</p><hr><h3>Accelerated Zeroth-order Method for Non-Smooth Stochastic Convex Optimization Problem with Infinite Variance</h3>
<p>Nikita Kornilov, Ohad Shamir, Aleksandr Lobanov, Darina Dvinskikh, Alexander Gasnikov, Innokentiy Andreevich Shibaev, Eduard Gorbunov, Samuel Horváth</p>
<p><a href='https://openreview.net/forum?id=vHSQTEIFkp'>https://openreview.net/forum?id=vHSQTEIFkp</a></p>
<p><b>Keywords</b>: stochastic optimization, gradient-free optimization, zero-order oracle, gradient clipping, infinite variance
</p><p><b>Compressor summary</b>: The paper presents an optimal algorithm for non-smooth stochastic convex optimization with infinite noise variance, adapted from a clipped accelerated gradient method.</p><hr><h3>Generalized Weighted Path Consistency for Mastering Atari Games</h3>
<p>Dengwei Zhao, Shikui Tu, Lei Xu</p>
<p><a href='https://openreview.net/forum?id=vHRLS8HhK1'>https://openreview.net/forum?id=vHRLS8HhK1</a></p>
<p><b>Keywords</b>: Monte Carlo Tree Search, Reinforcement learning, Path consistency.
</p><p><b>Compressor summary</b>: The paper proposes GW-PCZero, a generalized version of PCZero that uses a weighting mechanism to reduce variance and improve learning efficiency in reinforcement learning with neural-guided MCTS for real applications with non-zero immediate reward.</p><hr><h3>Self-supervised video pretraining yields robust and more human-aligned visual representations</h3>
<p>Nikhil Parthasarathy, S. M. Ali Eslami, Joao Carreira, Olivier J Henaff</p>
<p><a href='https://openreview.net/forum?id=vF8ukt5l1R'>https://openreview.net/forum?id=vF8ukt5l1R</a></p>
<p><b>Keywords</b>: self-supervised learning, contrastive, video pretraining, representation learning, visual representation, human alignment, robustness, shape-bias, saliency
</p><p><b>Compressor summary</b>: Video pretraining using VITO can learn visually general, robust, and human-like representations from complex transformations in videos.</p><hr><h3>On Class Distributions Induced by Nearest Neighbor Graphs for Node Classification of Tabular Data</h3>
<p>Federico Errica</p>
<p><a href='https://openreview.net/forum?id=vEzcRdiTkP'>https://openreview.net/forum?id=vEzcRdiTkP</a></p>
<p><b>Keywords</b>: Deep Graph Networks, Graph Neural Networks, Graph Representation Learning, Nearest Neighbors, Node Classification, Tabular Data
</p><p><b>Compressor summary</b>: The paper questions the benefits of using nearest neighbor graphs as a way to represent missing graph structures in machine learning problems and suggests exploring other techniques instead.</p><hr><h3>Neural Algorithmic Reasoning Without Intermediate Supervision</h3>
<p>Gleb Rodionov, Liudmila Prokhorenkova</p>
<p><a href='https://openreview.net/forum?id=vBwSACOB3x'>https://openreview.net/forum?id=vBwSACOB3x</a></p>
<p><b>Keywords</b>: neural algorithmic reasoning, graph neural networks, self-supervised regularization
</p><p><b>Compressor summary</b>: This paper proposes an unsupervised method to learn neural algorithms that can generalize to larger input sizes and achieve state-of-the-art results on classic algorithmic problems like sorting.</p><hr><h3>An Exploration-by-Optimization Approach to Best of Both Worlds in Linear Bandits</h3>
<p>Shinji Ito, Kei Takemura</p>
<p><a href='https://openreview.net/forum?id=vBHKSTgcYQ'>https://openreview.net/forum?id=vBHKSTgcYQ</a></p>
<p><b>Keywords</b>: bandit, linear bandit, best of both worlds, exploration by optimization
</p><p><b>Compressor summary</b>: The paper presents a linear bandit algorithm that performs near-optimally in both stochastic and adversarial settings, using exploration by optimization with specific regret bounds depending on dimension, time horizon, and sub-optimality gap.</p><hr><h3>Reflexion: language agents with verbal reinforcement learning</h3>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, Shunyu Yao</p>
<p><a href='https://openreview.net/forum?id=vAElhFcKW6'>https://openreview.net/forum?id=vAElhFcKW6</a></p>
<p><b>Keywords</b>: language model, reasoning, decision making, programming
</p><p><b>Compressor summary</b>: Reflexion is a framework that helps language agents learn from feedback by reflecting on it verbally and storing it in memory for better decision-making in future trials, improving performance across various tasks.</p><hr><h3>MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion</h3>
<p>Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, Yasutaka Furukawa</p>
<p><a href='https://openreview.net/forum?id=vA0vj1mY77'>https://openreview.net/forum?id=vA0vj1mY77</a></p>
<p><b>Keywords</b>: multiview; image generation; generative model; diffusion models
</p><p><b>Compressor summary</b>: MVDiffusion is a method for generating multi-view images with high resolution and rich content by concurrently interacting with all views using a correspondence-aware attention mechanism, which includes modules for generation, interpolation, and super-resolution.</p><hr><h3>Deep Neural Collapse Is Provably Optimal for the Deep Unconstrained Features Model</h3>
<p>Peter Súkeník, Marco Mondelli, Christoph H Lampert</p>
<p><a href='https://openreview.net/forum?id=v9yC7sSXf3'>https://openreview.net/forum?id=v9yC7sSXf3</a></p>
<p><b>Keywords</b>: neural collapse, unconstrained features model, deep learning
</p><p><b>Compressor summary</b>: The paper studies deep neural collapse, a phenomenon where neural networks lose structure in earlier layers, and proposes a generalized analytical framework to explain it.</p><hr><h3>Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion</h3>
<p>Taehyun Cho, Seungyub Han, Heesoo Lee, Kyungjae Lee, Jungwoo Lee</p>
<p><a href='https://openreview.net/forum?id=v8u3EFAyW9'>https://openreview.net/forum?id=v8u3EFAyW9</a></p>
<p><b>Keywords</b>: distributional reinforcement learning, risk
</p><p><b>Compressor summary</b>: The paper proposes a new distributional reinforcement learning method that uses randomized risk criterion for exploration and proves its convergence, optimality, and effectiveness in various environments.</p><hr><h3>End-To-End Latent Variational Diffusion Models for Inverse Problems in High Energy Physics</h3>
<p>Alexander Shmakov, Kevin Greif, Michael James Fenton, Aishik Ghosh, Pierre Baldi, Daniel Whiteson</p>
<p><a href='https://openreview.net/forum?id=v7WWesSiOu'>https://openreview.net/forum?id=v7WWesSiOu</a></p>
<p><b>Keywords</b>: Diffusion, Variational, VAE, LDM, Physics, Unfolding
</p><p><b>Compressor summary</b>: The paragraph discusses a new deep learning method for correcting detector effects in particle physics measurements at the Large Hadron Collider, which outperforms existing methods in accuracy and efficiency.</p><hr><h3>Adaptive Algorithms for Relaxed Pareto Set Identification</h3>
<p>Cyrille Kone, Emilie Kaufmann, Laura Richert</p>
<p><a href='https://openreview.net/forum?id=v6jIxRRDyD'>https://openreview.net/forum?id=v6jIxRRDyD</a></p>
<p><b>Keywords</b>: bandit, pure-exploration, pareto front, pareto set
</p><p><b>Compressor summary</b>: The paper proposes a new sampling strategy called Adaptive Pareto Exploration that can identify a relevant subset of the Pareto optimal set in multi-objective bandit problems, and demonstrates its effectiveness on vaccination strategies against Covid-19.</p><hr><h3>Deep Equilibrium Based Neural Operators for Steady-State PDEs</h3>
<p>Tanya Marwah, Ashwini Pokle, J Zico Kolter, Zachary Chase Lipton, Jianfeng Lu, Andrej Risteski</p>
<p><a href='https://openreview.net/forum?id=v6YzxwJlQn'>https://openreview.net/forum?id=v6YzxwJlQn</a></p>
<p><b>Keywords</b>: Deep Equilibrium Models, Partial Differential Equations, Neural Operators
</p><p><b>Compressor summary</b>: The authors propose FNO-DEQ, a deep learning architecture that solves steady-state PDEs more efficiently and robustly than existing methods by using weight-tied neural networks and exploiting the fixed point nature of their solutions.</p><hr><h3>Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization</h3>
<p>Nathan Grinsztajn, Daniel Furelos-Blanco, Shikha Surana, Clément Bonnet, Thomas D Barrett</p>
<p><a href='https://openreview.net/forum?id=v6VpqGcGAR'>https://openreview.net/forum?id=v6VpqGcGAR</a></p>
<p><b>Keywords</b>: Combinatorial Optimization, Reinforcement Learning, TSP, CVRP, JSSP
</p><p><b>Compressor summary</b>: The paper introduces Poppy, a training method for populations of reinforcement learning policies that induce unsupervised specialization to solve hard combinatorial optimization problems effectively.</p><hr><h3>Improving Adversarial Robustness via Information Bottleneck Distillation</h3>
<p>Huafeng Kuang, Hong Liu, YONGJIAN WU, Shin'ichi Satoh, Rongrong Ji</p>
<p><a href='https://openreview.net/forum?id=v5Aaxk4sSy'>https://openreview.net/forum?id=v5Aaxk4sSy</a></p>
<p><b>Keywords</b>: Information Bottleneck, Adversarial training, Adversarial  robustness, Knowledge distillation
</p><p><b>Compressor summary</b>: The study proposes an Information Bottleneck Distillation approach that uses prior knowledge from a pre-trained model to improve the information bottlenecks and robustness of deep neural networks against adversarial attacks.</p><hr><h3>UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild</h3>
<p>Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, Ran Xu</p>
<p><a href='https://openreview.net/forum?id=v54eUIayFh'>https://openreview.net/forum?id=v54eUIayFh</a></p>
<p><b>Keywords</b>: Image Generation, Multi-modal, HyperNet
</p><p><b>Compressor summary</b>: UniControl is a new generative foundation model that can handle various visual conditions and generate images with precise controls, while still allowing for arbitrary language prompts.</p><hr><h3>MonoUNI: A Unified Vehicle and Infrastructure-side Monocular 3D Object Detection Network with Sufficient Depth Clues</h3>
<p>Jinrang Jia, Zhenjia Li, Yifeng Shi</p>
<p><a href='https://openreview.net/forum?id=v2oGdhbKxi'>https://openreview.net/forum?id=v2oGdhbKxi</a></p>
<p><b>Keywords</b>: 3D detection, deep learning, autonomous driving
</p><p><b>Compressor summary</b>: The paper proposes a unified optimization target called normalized depth for monocular 3D detection of vehicle and infrastructure sides in autonomous driving, and introduces 3D normalized cube depth to improve accuracy.</p><hr><h3>H2RBox-v2: Incorporating Symmetry for Boosting Horizontal Box Supervised Oriented Object Detection</h3>
<p>Yi Yu, Xue Yang, Qingyun Li, Yue Zhou, Feipeng Da, Junchi Yan</p>
<p><a href='https://openreview.net/forum?id=v1VVKaMYbk'>https://openreview.net/forum?id=v1VVKaMYbk</a></p>
<p><b>Keywords</b>: Oriented object detection, self-supervised learning
</p><p><b>Compressor summary</b>: H2RBox-v2 is a new method for oriented object detection that uses reflection symmetry and self-supervision to improve performance and reduce the need for high-quality annotations.</p><hr><h3>Egocentric Planning for Scalable Embodied Task Achievement</h3>
<p>Xiaotian Liu, Hector Palacios, Christian Muise</p>
<p><a href='https://openreview.net/forum?id=v0lkbp66Uw'>https://openreview.net/forum?id=v0lkbp66Uw</a></p>
<p><b>Keywords</b>: Embodied AI, High-Level Actions, Symbolic Reasoning, Replanning, ALFRED Challenge, Flexible Task Achievement, User-Goal Understanding, Object Types and Actions, Perception Grounding
</p><p><b>Compressor summary</b>: Egocentric Planning combines symbolic planning and Object-oriented POMDPs to solve complex domestic tasks with visual perception and natural language processing, achieving a high success rate in ALFRED benchmark and winning the CVPR Embodied AI workshop challenge.</p><hr><h3>Temporal Continual Learning with Prior Compensation for Human Motion Prediction</h3>
<p>Jianwei Tang, Jiangxin Sun, Xiaotong Lin, lifang zhang, Wei-Shi Zheng, Jian-Fang Hu</p>
<p><a href='https://openreview.net/forum?id=v0GzRLvVp3'>https://openreview.net/forum?id=v0GzRLvVp3</a></p>
<p><b>Keywords</b>: Human Motion Prediction; Temporal Continual Learning; Prior Compensation Factor
</p><p><b>Compressor summary</b>: The paper proposes a new framework called Temporal Continual Learning (TCL) for human motion prediction that better preserves prior information and adapts to different models, datasets, and applications.</p><hr><h3>Online robust non-stationary estimation</h3>
<p>Abishek Sankararaman, Murali Balakrishnan</p>
<p><a href='https://openreview.net/forum?id=uzOBDerK1j'>https://openreview.net/forum?id=uzOBDerK1j</a></p>
<p><b>Keywords</b>: Estimation, heavy-tails, distribution shifts, regret
</p><p><b>Compressor summary</b>: The text describes a new algorithm for estimating time-varying parameters from noisy and corrupted high-dimensional data-streams that is adaptive to drift, robust to heavy-tails and corruptions, requires no distributional knowledge, and can be implemented online.</p><hr><h3>Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers</h3>
<p>Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, Thomas Hofmann</p>
<p><a href='https://openreview.net/forum?id=uvdJgFFzby'>https://openreview.net/forum?id=uvdJgFFzby</a></p>
<p><b>Keywords</b>: Transformers, Context-pruning, Efficient Transformer
</p><p><b>Compressor summary</b>: The study proposes a method that prunes uninformative tokens from context during generation, reducing computational and memory requirements while maintaining model expressiveness and interpretability.</p><hr><h3>Training Your Image Restoration Network Better with  Random Weight Network as Optimization Function</h3>
<p>Man Zhou, Naishan Zheng, Yuan Xu, Chun-Le Guo, Chongyi Li</p>
<p><a href='https://openreview.net/forum?id=uv3ge0goPa'>https://openreview.net/forum?id=uv3ge0goPa</a></p>
<p><b>Keywords</b>: Image restoration, low-light image enhancement, image de-noising
</p><p><b>Compressor summary</b>: The study proposes using random weight networks as constraints for training better image restoration networks, and explores four prototypes with different strategies and variants.</p><hr><h3>Can semi-supervised learning use all the data effectively? A lower bound perspective</h3>
<p>Alexandru Tifrea, Gizem Yüce, Amartya Sanyal, Fanny Yang</p>
<p><a href='https://openreview.net/forum?id=utreNaM1VY'>https://openreview.net/forum?id=utreNaM1VY</a></p>
<p><b>Keywords</b>: semi-supervised learning, statistical lower bound
</p><p><b>Compressor summary</b>: The paragraph discusses the limitations of existing theory on semi-supervised learning (SSL) algorithms, which may not always improve upon unsupervised and supervised learning algorithms, but can still perform well in real-world scenarios.</p><hr><h3>All Points Matter: Entropy-Regularized Distribution Alignment for Weakly-supervised 3D Segmentation</h3>
<p>Liyao Tang, Zhe Chen, Shanshan Zhao, Chaoyue Wang, Dacheng Tao</p>
<p><a href='https://openreview.net/forum?id=utQms7PPx5'>https://openreview.net/forum?id=utQms7PPx5</a></p>
<p><b>Keywords</b>: point cloud segmentation, weak supervision
</p><p><b>Compressor summary</b>: The paper proposes ERDA, a learning strategy that uses entropy regularization and distribution alignment to improve weakly supervised 3D segmentation tasks with pseudo-labels, achieving state-of-the-art performance with minimal annotations.</p><hr><h3>Fused Gromov-Wasserstein Graph Mixup for Graph-level Classifications</h3>
<p>Xinyu Ma, Xu Chu, Yasha Wang, Yang Lin, Junfeng Zhao, Liantao Ma, Wenwu Zhu</p>
<p><a href='https://openreview.net/forum?id=uqkUguNu40'>https://openreview.net/forum?id=uqkUguNu40</a></p>
<p><b>Keywords</b>: Graph Data Augmentation, Graph Mixup, Fused Gromov Wasserstein
</p><p><b>Compressor summary</b>: The paper introduces FGWMixup, a graph data augmentation method that uses optimal transport to find an optimal inter-graph node matching strategy, improving the generalizability and robustness of GNNs.</p><hr><h3>Optimal approximation using complex-valued neural networks</h3>
<p>Paul Geuchen, Felix Voigtlaender</p>
<p><a href='https://openreview.net/forum?id=uotGmrcooz'>https://openreview.net/forum?id=uotGmrcooz</a></p>
<p><b>Keywords</b>: complex-valued neural networks, approximation rates
</p><p><b>Compressor summary</b>: The paper analyzes the expressivity and approximation properties of complex-valued neural networks (CVNNs) with various activation functions, showing that their error scales as $m^{-k/(2n)}$ for smooth target functions and is optimal under a continuity assumption.</p><hr><h3>PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning</h3>
<p>Neeratyoy Mallik, Eddie Bergman, Carl Hvarfner, Danny Stoll, Maciej Janowski, Marius Lindauer, Luigi Nardi, Frank Hutter</p>
<p><a href='https://openreview.net/forum?id=uoiwugtpCH'>https://openreview.net/forum?id=uoiwugtpCH</a></p>
<p><b>Keywords</b>: Hyperparameter Optimization, Deep Learning
</p><p><b>Compressor summary</b>: PriorBand is an HPO algorithm for DL that combines expert knowledge and cheap proxy tasks to efficiently optimize hyperparameters.</p><hr><h3>Minimax Forward and Backward Learning of Evolving Tasks with Performance Guarantees</h3>
<p>Veronica Alvarez, Santiago Mazuelas, Jose A. Lozano</p>
<p><a href='https://openreview.net/forum?id=uoRiO855Sj'>https://openreview.net/forum?id=uoRiO855Sj</a></p>
<p><b>Keywords</b>: Concept drift, Continual learning, Minimax classification, Performance guarantees
</p><p><b>Compressor summary</b>: IMRCs use forward and backward learning to improve classification accuracy on evolving tasks with few samples per task.</p><hr><h3>Sample-efficient Multi-objective Molecular Optimization with GFlowNets</h3>
<p>Yiheng Zhu, Jialu Wu, Chaowen Hu, Jiahuan Yan, Chang-Yu Hsieh, Tingjun Hou, Jian Wu</p>
<p><a href='https://openreview.net/forum?id=uoG1fLIK2s'>https://openreview.net/forum?id=uoG1fLIK2s</a></p>
<p><b>Keywords</b>: drug discovery, multi-objective molecular optimization, Bayesian optimization, generative flow networks
</p><p><b>Compressor summary</b>: The paper introduces HN-GFN, a novel algorithm that uses a single hypernetwork to optimize multiple objectives and diversity in molecule design, and a hindsight-like strategy to improve learning and sharing of high-quality molecules.</p><hr><h3>Energy-Based Sliced Wasserstein Distance</h3>
<p>Khai Nguyen, Nhat Ho</p>
<p><a href='https://openreview.net/forum?id=umvV3yvo4N'>https://openreview.net/forum?id=umvV3yvo4N</a></p>
<p><b>Keywords</b>: Sliced Wasserstein, Monte Carlo Methods, Point-Cloud, Optimal Transport
</p><p><b>Compressor summary</b>: The paper introduces a new way to choose the slicing distribution for the Wasserstein distance, called energy-based sliced Wasserstein (EBSW), which improves on previous approaches in terms of discrimination, cost, and stability.</p><hr><h3>Enhancing Knowledge Transfer for Task Incremental Learning with Data-free Subnetwork</h3>
<p>Qiang Gao, Xiaojun Shan, Yuchen Zhang, Fan Zhou</p>
<p><a href='https://openreview.net/forum?id=uj9PxVTVqq'>https://openreview.net/forum?id=uj9PxVTVqq</a></p>
<p><b>Keywords</b>: data-free subnetwork, task-incremental learning, knowledge transfer, mask
</p><p><b>Compressor summary</b>: DSN is a novel method for neuron-wise incremental learning that transfers knowledge across tasks using masks and data-free replay, avoiding forgetting and privacy issues.</p><hr><h3>Annotator: A Generic Active Learning Baseline for LiDAR Semantic Segmentation</h3>
<p>Binhui Xie, Shuang Li, qingju guo, Chi Harold Liu, Xinjing Cheng</p>
<p><a href='https://openreview.net/forum?id=uiiVSVADDc'>https://openreview.net/forum?id=uiiVSVADDc</a></p>
<p><b>Keywords</b>: Active Learning, LiDAR Semantic Segmentation, Domain Adaptation
</p><p><b>Compressor summary</b>: Annotator is an active learning method that uses voxel-centric online selection to efficiently annotate LiDAR point clouds for semantic segmentation under various distribution shifts.</p><hr><h3>Mechanic: A Learning Rate Tuner</h3>
<p>Ashok Cutkosky, Aaron Defazio, Harsh Mehta</p>
<p><a href='https://openreview.net/forum?id=uhKtQMn21D'>https://openreview.net/forum?id=uhKtQMn21D</a></p>
<p><b>Keywords</b>: optimization, deep learning, online convex optimization
</p><p><b>Compressor summary</b>: Mechanic is a technique for automatically adjusting the learning rate in any optimization algorithm, which performs well in various deep learning tasks and can match or surpass manual tuning.</p><hr><h3>HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution</h3>
<p>Eric Nguyen, Michael Poli, Marjan Faizi, Armin W Thomas, Michael Wornow, Callum Birch-Sykes, Stefano Massaroli, Aman Patel, Clayton M. Rabideau, Yoshua Bengio, Stefano Ermon, Christopher Re, Stephen Baccus</p>
<p><a href='https://openreview.net/forum?id=ubzNoJjOKj'>https://openreview.net/forum?id=ubzNoJjOKj</a></p>
<p><b>Keywords</b>: genomics, hyena, foundation models, large language models, transformers
</p><p><b>Compressor summary</b>: HyenaDNA is a large language model that can process up to 1 million tokens of human genome data, enabling long-range interactions and single nucleotide resolution for genomic tasks, and outperforms previous models on various benchmarks.</p><hr><h3>Uncovering Meanings of Embeddings via Partial Orthogonality</h3>
<p>Yibo Jiang, Bryon Aragam, Victor Veitch</p>
<p><a href='https://openreview.net/forum?id=ubp5s2tgXq'>https://openreview.net/forum?id=ubp5s2tgXq</a></p>
<p><b>Keywords</b>: embedding, representation, graphical models, partial orthogonality, Markov boundary
</p><p><b>Compressor summary</b>: The paper explores how semantic structure is encoded in vector embeddings using partial orthogonality and introduces embeddings that preserve conditional independence structures.</p><hr><h3>SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds</h3>
<p>Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, Jian Ren</p>
<p><a href='https://openreview.net/forum?id=ubgdInLSF9'>https://openreview.net/forum?id=ubgdInLSF9</a></p>
<p><b>Keywords</b>: Text-to-Image, Diffusion model, mobile devices, distillation
</p><p><b>Compressor summary</b>: The authors introduce a generic approach to make text-to-image diffusion models faster and more accessible on mobile devices, improving efficiency by modifying the network architecture and step distillation, and achieving better results than Stable Diffusion v$1.5$.</p><hr><h3>Domain Agnostic Fourier Neural Operators</h3>
<p>Ning Liu, Siavash Jafarzadeh, Yue Yu</p>
<p><a href='https://openreview.net/forum?id=ubap5FKbJs'>https://openreview.net/forum?id=ubap5FKbJs</a></p>
<p><b>Keywords</b>: Operator-Regression Neural Networks, Neural Operators, Data-Driven Physics Modeling, Geometrical and Topological Shape Changes
</p><p><b>Compressor summary</b>: The authors propose a novel neural operator architecture, DAFNO, that can learn nonlinear mappings between function spaces on irregular geometries and evolving domains using FFT, achieving state-of-the-art accuracy in material modeling and airfoil simulation, and demonstrating generalizability to unseen crack patterns.</p><hr><h3>Small Total-Cost Constraints in Contextual Bandits with Knapsacks, with Application to Fairness</h3>
<p>Evgenii E Chzhen, Christophe Giraud, Zhen LI, Gilles Stoltz</p>
<p><a href='https://openreview.net/forum?id=uZvG0HLkOB'>https://openreview.net/forum?id=uZvG0HLkOB</a></p>
<p><b>Keywords</b>: mutli-armed bandits, bandits with knapsacks, primal-dual approaches
</p><p><b>Compressor summary</b>: The paper proposes a new algorithm for contextual bandit problems with knapsacks that can handle cost constraints of the order of $\sqrt{T}$, which is more efficient and simpler than existing approaches.</p><hr><h3>CL-NeRF: Continual Learning of Neural Radiance Fields for Evolving Scene Representation</h3>
<p>Xiuzhe Wu, Peng Dai, Weipeng DENG, Handi Chen, Yang Wu, Yan-Pei Cao, Ying Shan, XIAOJUAN QI</p>
<p><a href='https://openreview.net/forum?id=uZjpSBTPik'>https://openreview.net/forum?id=uZjpSBTPik</a></p>
<p><b>Keywords</b>: Neural Radiance Field; Continual Learning; Scene Representation
</p><p><b>Compressor summary</b>: The paper introduces CL-NeRF, a method for efficiently adapting Neural Radiance Fields to scene changes using continual learning and a new benchmark for evaluating it.</p><hr><h3>Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</h3>
<p>Pengchong Hu, Zhizhong Han</p>
<p><a href='https://openreview.net/forum?id=uWNqy09dFW'>https://openreview.net/forum?id=uWNqy09dFW</a></p>
<p><b>Keywords</b>: 3D Reconstruction, SDF, Neural Rendering, Implicit Representations, SLAM
</p><p><b>Compressor summary</b>: The paragraph introduces a novel method for 3D reconstruction from multi-view images using neural implicit representations, volume rendering with an attentive depth fusion prior, and a Truncated Signed Distance Function (TSDF).</p><hr><h3>Complementary Benefits of Contrastive Learning and Self-Training Under Distribution Shift</h3>
<p>Saurabh Garg, Amrith Setlur, Zachary Chase Lipton, Sivaraman Balakrishnan, Virginia Smith, Aditi Raghunathan</p>
<p><a href='https://openreview.net/forum?id=uWGH6jDTVv'>https://openreview.net/forum?id=uWGH6jDTVv</a></p>
<p><b>Keywords</b>: contrastive learning; self training; distribution shift; semi supervised learning; unsupervised domain adaptation
</p><p><b>Compressor summary</b>: The paper investigates how combining self-training and contrastive learning techniques affects unsupervised domain adaptation and semi-supervised learning, showing improved accuracy in the former but not the latter.</p><hr><h3>Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception</h3>
<p>Hassan Akbari, Dan Kondratyuk, Yin Cui, Rachel Hornung, Huisheng Wang, Hartwig Adam</p>
<p><a href='https://openreview.net/forum?id=uTlKUAm68H'>https://openreview.net/forum?id=uTlKUAm68H</a></p>
<p><b>Keywords</b>: Alternating Gradient Descent, Multimodal, Mixture of Experts, AGD, MoE, Deep Learning, Optimization
</p><p><b>Compressor summary</b>: IMP is a multimodal multi-task training method that uses alternating gradient descent and mixture-of-experts to efficiently improve model performance on various tasks, achieving state-of-the-art results in zero-shot video classification.</p><hr><h3>Self-Supervised Reinforcement Learning that Transfers using Random Features</h3>
<p>Boyuan Chen, Chuning Zhu, Pulkit Agrawal, Kaiqing Zhang, Abhishek Gupta</p>
<p><a href='https://openreview.net/forum?id=uRewSnLJAa'>https://openreview.net/forum?id=uRewSnLJAa</a></p>
<p><b>Keywords</b>: deep reinforcement learning, self-supervised learning
</p><p><b>Compressor summary</b>: The paper proposes a self-supervised RL method that combines model-free and model-based approaches for transferring behaviors across tasks with different rewards in complex environments.</p><hr><h3>Sampling weights of deep neural networks</h3>
<p>Erik Lien Bolager, Iryna Burak, Chinmay Datar, Qing Sun, Felix Dietrich</p>
<p><a href='https://openreview.net/forum?id=uRHpgo6TMR'>https://openreview.net/forum?id=uRHpgo6TMR</a></p>
<p><b>Keywords</b>: random sampling, neural network parameters, iterative optimization
</p><p><b>Compressor summary</b>: The authors propose a method for quickly training fully-connected neural networks without gradient computations by using a data-dependent probability distribution and an efficient sampling algorithm.</p><hr><h3>A Logic for Expressing Log-Precision Transformers</h3>
<p>William Merrill, Ashish Sabharwal</p>
<p><a href='https://openreview.net/forum?id=uR8TtWCIsr'>https://openreview.net/forum?id=uR8TtWCIsr</a></p>
<p><b>Keywords</b>: transformers, logic, reasoning, circuit complexity, mechanistic interpretability
</p><p><b>Compressor summary</b>: The paper investigates whether log-precision transformers, which can attend universally, can be represented by a generalized first-order logic with majority-vote quantifiers.</p><hr><h3>Data Selection for Language Models via Importance Resampling</h3>
<p>Sang Michael Xie, Shibani Santurkar, Tengyu Ma, Percy Liang</p>
<p><a href='https://openreview.net/forum?id=uPSQv0leAu'>https://openreview.net/forum?id=uPSQv0leAu</a></p>
<p><b>Keywords</b>: Language models, pretraining, data selection, fine-tuning
</p><p><b>Compressor summary</b>: DSIR is a data selection method that uses importance resampling in a reduced feature space to match a target distribution, achieving comparable or better results than expert curation or simple heuristics.</p><hr><h3>Breadcrumbs to the Goal: Supervised Goal Selection from Human-in-the-Loop Feedback</h3>
<p>Marcel Torne Villasevil, Max Balsells I Pamies, Zihan Wang, Samedh Desai, Tao Chen, Pulkit Agrawal, Abhishek Gupta</p>
<p><a href='https://openreview.net/forum?id=uOEeui0rL7'>https://openreview.net/forum?id=uOEeui0rL7</a></p>
<p><b>Keywords</b>: Learning from human preferences, self-supervised learning, exploration in reinforcement learning
</p><p><b>Compressor summary</b>: HUGE is a technique that uses low-quality human feedback to guide exploration for reinforcement learning without requiring reward specification or exploration bonuses.</p><hr><h3>Sample Complexity Bounds for Score-Matching: Causal Discovery and Generative Modeling</h3>
<p>Zhenyu Zhu, Francesco Locatello, Volkan Cevher</p>
<p><a href='https://openreview.net/forum?id=uNnPWR66b8'>https://openreview.net/forum?id=uNnPWR66b8</a></p>
<p><b>Keywords</b>: Causal discovery, Score matching, Score-based generative modeling
</p><p><b>Compressor summary</b>: The paper studies how well deep neural networks can estimate a score function for causal discovery using score-matching methods.</p><hr><h3>Adaptive Linear Estimating Equations</h3>
<p>Mufang Ying, Koulik Khamaru, Cun-Hui Zhang</p>
<p><a href='https://openreview.net/forum?id=uNmKBZrRZC'>https://openreview.net/forum?id=uNmKBZrRZC</a></p>
<p><b>Keywords</b>: bandit algorithm, statistical inference, adaptively collected data, asymptotic normality
</p><p><b>Compressor summary</b>: The paper proposes a debiased estimator for sequential data collection that improves statistical inference by achieving both non-asymptotic performance and asymptotic normality.</p><hr><h3>The Graph Pencil Method: Mapping Subgraph Densities to Stochastic Block Models</h3>
<p>Lee M. Gunderson, Gecia Bravo-Hermsdorff, Peter Orbanz</p>
<p><a href='https://openreview.net/forum?id=uN71BdBEG8'>https://openreview.net/forum?id=uN71BdBEG8</a></p>
<p><b>Keywords</b>: Stochastic block model, SBM, graphons, matrix pencil method, method of moments
</p><p><b>Compressor summary</b>: The paper presents a method to find the parameters of a stochastic block model from a finite set of subgraph densities using a unique translation map that is efficient and easy to compute.</p><hr><h3>Fair Allocation of Indivisible Chores: Beyond Additive Costs</h3>
<p>Bo Li, Fangxiao Wang, Yu Zhou</p>
<p><a href='https://openreview.net/forum?id=uJmsYZiu3E'>https://openreview.net/forum?id=uJmsYZiu3E</a></p>
<p><b>Keywords</b>: fair allocation of chores, beyond additive cost functions, bin packing, job scheduling
</p><p><b>Compressor summary</b>: The paper explores the limitations of achieving exact or good approximations of maximin share fairness in allocating indivisible tasks with subadditive costs and studies two specific settings, bin packing and job scheduling, where constant approximate allocations exist.</p><hr><h3>Exploring Geometry of Blind Spots in Vision models</h3>
<p>Sriram Balasubramanian, Gaurang Sriramanan, Vinu Sankar Sadasivan, Soheil Feizi</p>
<p><a href='https://openreview.net/forum?id=uJ3qNIsDGF'>https://openreview.net/forum?id=uJ3qNIsDGF</a></p>
<p><b>Keywords</b>: Neural networks, Vision models, blind spots, undersensitivity, invariance, level set geometry, input connectivity
</p><p><b>Compressor summary</b>: The authors study how vision models like CNNs and Transformers are under-sensitive to input perturbations and propose a method to explore the geometry of "equi-confidence" level sets in these networks, revealing a star-like structure.</p><hr><h3>FairLISA: Fair User Modeling with Limited Sensitive Attributes Information</h3>
<p>Zheng Zhang, Qi Liu, Hao Jiang, Fei Wang, Yan Zhuang, Le Wu, Weibo Gao, Enhong Chen</p>
<p><a href='https://openreview.net/forum?id=uFpjPJMkv6'>https://openreview.net/forum?id=uFpjPJMkv6</a></p>
<p><b>Keywords</b>: fairness, user modeling
</p><p><b>Compressor summary</b>: The paper proposes a new framework called FairLISA to create user models that are fair and accurate even when some sensitive data is missing or unknown.</p><hr><h3>Focus Your Attention when Few-Shot Classification</h3>
<p>Haoqing Wang, Shibo Jie, Zhi-Hong Deng</p>
<p><a href='https://openreview.net/forum?id=uFlE0qgtRO'>https://openreview.net/forum?id=uFlE0qgtRO</a></p>
<p><b>Keywords</b>: few-shot image classification, fine-tuning, vision transformers
</p><p><b>Compressor summary</b>: The paper proposes a method to improve few-shot image classification with pre-trained vision transformers by using attention and gradient information to locate key entities in support images, which helps the model focus on class-related features and generalize better.</p><hr><h3>Static and Sequential Malicious Attacks in the Context of Selective Forgetting</h3>
<p>CHENXU ZHAO, Wei Qian, Zhitao Ying, Mengdi Huai</p>
<p><a href='https://openreview.net/forum?id=uEJfW3OtUm'>https://openreview.net/forum?id=uEJfW3OtUm</a></p>
<p><b>Keywords</b>: Selective forgetting, static setting, sequential setting, security and robustness
</p><p><b>Compressor summary</b>: The paper explores malicious data update attacks on machine unlearning systems and proposes new attack methods and algorithms to exploit them.</p><hr><h3>Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs</h3>
<p>Lin Yang, Junlong Lyu, Wenlong Lyu, Zhitang Chen</p>
<p><a href='https://openreview.net/forum?id=uDV4lA0gZ6'>https://openreview.net/forum?id=uDV4lA0gZ6</a></p>
<p><b>Keywords</b>: bayesian optimization, robust optimization
</p><p><b>Compressor summary</b>: The paper proposes AIRBO, a robust Bayesian Optimization algorithm that models uncertain inputs using Maximum Mean Discrepancy and Nystrom approximation, achieving state-of-the-art performance under various input uncertainties.</p><hr><h3>(S)GD over Diagonal Linear Networks: Implicit bias, Large Stepsizes and Edge of Stability</h3>
<p>Mathieu Even, Scott Pesme, Suriya Gunasekar, Nicolas Flammarion</p>
<p><a href='https://openreview.net/forum?id=uAyElhYKxg'>https://openreview.net/forum?id=uAyElhYKxg</a></p>
<p><b>Keywords</b>: SGD, GD, implicit bias, large stepsizes, edge of stability, diagonal linear networks
</p><p><b>Compressor summary</b>: The paper studies how randomness and big steps affect gradient descent and stochastic gradient descent in linear networks with two layers, proving their convergence and analysing how these factors influence solution quality in regression problems.</p><hr><h3>ReDS: Offline RL With Heteroskedastic Datasets via Support Constraints</h3>
<p>Anikait Singh, Aviral Kumar, Quan Vuong, Yevgen Chebotar, Sergey Levine</p>
<p><a href='https://openreview.net/forum?id=u8srPlinoj'>https://openreview.net/forum?id=u8srPlinoj</a></p>
<p><b>Keywords</b>: offline RL, support constraints, heteroskedastic data
</p><p><b>Compressor summary</b>: Offline reinforcement learning methods struggle to learn from data with non-uniform variability due to distribution constraints; conservative Q-learning (CQL) with reweighting improves performance by allowing the learned policy to choose how closely to follow the behavior policy per state.</p><hr><h3>Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models</h3>
<p>Haonan Duan, Adam Dziedzic, Nicolas Papernot, Franziska Boenisch</p>
<p><a href='https://openreview.net/forum?id=u6Xv3FuF8N'>https://openreview.net/forum?id=u6Xv3FuF8N</a></p>
<p><b>Keywords</b>: differential privacy, in-context learning, trustworthy ML
</p><p><b>Compressor summary</b>: The authors propose a method to privately learn how to prompt large language models without exposing the data used for prompting, which can maintain high accuracy while protecting user privacy.</p><hr><h3>Real-World Image Variation by Aligning Diffusion Inversion Chain</h3>
<p>Yuechen ZHANG, Jinbo Xing, Eric Lo, Jiaya Jia</p>
<p><a href='https://openreview.net/forum?id=u6Ibs4hTJH'>https://openreview.net/forum?id=u6Ibs4hTJH</a></p>
<p><b>Keywords</b>: image variation, diffusion model, image generation, text-driven image editing
</p><p><b>Compressor summary</b>: RIVAL is a novel inference pipeline that generates high-quality image variations by aligning the diffusion process to the source image's inversion chain using cross-image self-attention and step-wise distribution normalization.</p><hr><h3>MADG: Margin-based Adversarial Learning for Domain Generalization</h3>
<p>Aveen Dayal, Vimal K B, Linga Reddy Cenkeramaddi, C Krishna Mohan, Abhinav Kumar, Vineeth N. Balasubramanian</p>
<p><a href='https://openreview.net/forum?id=u6BYyPuD29'>https://openreview.net/forum?id=u6BYyPuD29</a></p>
<p><b>Keywords</b>: Domain Generalization, Margin Loss, Adversarial Learning, Domain Adaptation
</p><p><b>Compressor summary</b>: The paper proposes a new adversarial learning method for domain generalization called MADG, which uses a margin loss-based discrepancy metric to learn domain-invariant features and achieves better generalization to unseen domains.</p><hr><h3>Graph Denoising Diffusion for Inverse Protein Folding</h3>
<p>Kai Yi, Bingxin Zhou, Yiqing Shen, Pietro Lio, Yu Guang Wang</p>
<p><a href='https://openreview.net/forum?id=u4YXKKG5dX'>https://openreview.net/forum?id=u4YXKKG5dX</a></p>
<p><b>Keywords</b>: inverse folding, graph neural networks, roto-translation equivariance, diffusion model
</p><p><b>Compressor summary</b>: The proposed graph denoising diffusion model generates diverse amino acid sequences for a given protein backbone using physiochemical properties, local environment, and prior knowledge of amino acid replacements, outperforming existing methods in inverse protein folding.</p><hr><h3>Uncovering motifs of concurrent signaling across multiple neuronal populations</h3>
<p>Evren Gokcen, Anna Ivic Jasper, Alison Xu, Adam Kohn, Christian K. Machens, Byron M. Yu</p>
<p><a href='https://openreview.net/forum?id=u39QQh5L8Q'>https://openreview.net/forum?id=u39QQh5L8Q</a></p>
<p><b>Keywords</b>: neuroscience, multi-population neural recordings, dimensionality reduction, latent variable models, Gaussian processes
</p><p><b>Compressor summary</b>: The paragraph describes a new framework for analyzing signals among multiple neuronal populations in different brain networks, which reveals how they communicate and evolve over time.</p><hr><h3>Robust Data Valuation with Weighted Banzhaf Values</h3>
<p>Weida Li, Yaoliang Yu</p>
<p><a href='https://openreview.net/forum?id=u359tNBpxF'>https://openreview.net/forum?id=u359tNBpxF</a></p>
<p><b>Keywords</b>: data valuation, robustness, weighted Banzhaf values
</p><p><b>Compressor summary</b>: The paper proposes a new robust data valuation method using weighted Banzhaf values and introduces Kronecker noise to analyze stochasticity.</p><hr><h3>PlanE: Representation Learning over Planar Graphs</h3>
<p>Radoslav Dimitrov, Zeyang Zhao, Ralph Abboud, Ismail Ilkan Ceylan</p>
<p><a href='https://openreview.net/forum?id=u2RJ0I3o3j'>https://openreview.net/forum?id=u2RJ0I3o3j</a></p>
<p><b>Keywords</b>: Graph Representation Learning; Planar Graphs; Graph Property Prediction
</p><p><b>Compressor summary</b>: The paragraph introduces PlanE, a framework for learning complete invariants of planar graphs, which are special graph classes with efficient isomorphism testing algorithms, and reports its successful performance on benchmarks.</p><hr><h3>Knowledge Distillation Performs Partial Variance Reduction</h3>
<p>Mher Safaryan, Alexandra Peste, Dan Alistarh</p>
<p><a href='https://openreview.net/forum?id=tzxP9Rx0LV'>https://openreview.net/forum?id=tzxP9Rx0LV</a></p>
<p><b>Keywords</b>: knowledge distillation, stochastic optimization, variance reduction
</p><p><b>Compressor summary</b>: The paper explores how knowledge distillation works as a stochastic variance reduction technique for enhancing student models with teacher models, and highlights the importance of careful parametrization for optimal performance.</p><hr><h3>GEX: A flexible method for approximating influence via Geometric Ensemble</h3>
<p>SungYub Kim, Kyungsu Kim, Eunho Yang</p>
<p><a href='https://openreview.net/forum?id=tz4ECtAu8e'>https://openreview.net/forum?id=tz4ECtAu8e</a></p>
<p><b>Keywords</b>: Influence Function, Geometric Ensemble, Loss Landscape
</p><p><b>Compressor summary</b>: The paper proposes a new method to improve Influence Function (IF) approximations for neural networks by addressing limitations and enhancing performance in various tasks.</p><hr><h3>InstanT: Semi-supervised Learning with Instance-dependent Thresholds</h3>
<p>Muyang Li, Runze Wu, Haoyu Liu, Jun Yu, Xun Yang, Bo Han, Tongliang Liu</p>
<p><a href='https://openreview.net/forum?id=txv7TnPvOi'>https://openreview.net/forum?id=txv7TnPvOi</a></p>
<p><b>Keywords</b>: semi-supervised learning, pseudo-labeling
</p><p><b>Compressor summary</b>: The paper proposes a new SSL method that uses instance-dependent thresholds to select confident unlabeled instances and assign pseudo-labels with probabilistic guarantees.</p><hr><h3>Fed-FA: Theoretically Modeling Client Data Divergence for Federated Language Backdoor Defense</h3>
<p>Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun</p>
<p><a href='https://openreview.net/forum?id=txPdKZrrZF'>https://openreview.net/forum?id=txPdKZrrZF</a></p>
<p><b>Keywords</b>: federated learning, backdoor learning, robust federated aggregation, data divergence
</p><p><b>Compressor summary</b>: The paper proposes a method called Federated F-Divergence-Based Aggregation (Fed-FA) to detect and exclude suspicious clients in federated NLP systems by modeling data divergence using f-divergence indicators and synthetic datasets.</p><hr><h3>DinoSR: Self-Distillation and Online Clustering for Self-supervised Speech Representation Learning</h3>
<p>Alexander H. Liu, Heng-Jui Chang, Michael Auli, Wei-Ning Hsu, James R. Glass</p>
<p><a href='https://openreview.net/forum?id=twmHKU3Ds4'>https://openreview.net/forum?id=twmHKU3Ds4</a></p>
<p><b>Keywords</b>: speech representation learning, self-supervised learning, self-distillation, discrete representation learning
</p><p><b>Compressor summary</b>: The paper presents a self-supervised speech representation learning model called DinoSR that combines masked language modeling, self-distillation, and online clustering to achieve state-of-the-art performance in various downstream tasks.</p><hr><h3>The Bayesian Stability Zoo</h3>
<p>Shay Moran, Hilla Schefler, Jonathan Shafer</p>
<p><a href='https://openreview.net/forum?id=tw4QaiiJex'>https://openreview.net/forum?id=tw4QaiiJex</a></p>
<p><b>Keywords</b>: Algorithmic stability, Replicability, Differential Privacy, KL Stability, Mutual Information Stability, Global Stability, Perfect Generalization, PAC Learning, Littlestone Dimension, Clique Dimension, PAC Bayes
</p><p><b>Compressor summary</b>: The paper explores the connections between different definitions of stability in learning theory, including various types of privacy, generalization, and divergence-based stability notions.</p><hr><h3>Secure Out-of-Distribution Task Generalization with Energy-Based Models</h3>
<p>Shengzhuang Chen, Long-Kai Huang, Jonathan Richard Schwarz, Yilun Du, Ying Wei</p>
<p><a href='https://openreview.net/forum?id=tt7bQnTdRm'>https://openreview.net/forum?id=tt7bQnTdRm</a></p>
<p><b>Keywords</b>: meta-generalization, out-of-distribution tasks
</p><p><b>Compressor summary</b>: The EBML framework uses two neural networks to characterize any meta-training task distribution, enabling detection and adaptation of out-of-distribution tasks for safe and effective meta-learning.</p><hr><h3>Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning</h3>
<p>Changsheng Lv, Shuai Zhang, Yapeng Tian, Mengshi Qi, Huadong Ma</p>
<p><a href='https://openreview.net/forum?id=trHfuGQyyr'>https://openreview.net/forum?id=trHfuGQyyr</a></p>
<p><b>Keywords</b>: Physical Audiovisual；Commonsense Reasoning
</p><p><b>Compressor summary</b>: The paper proposes a Disentangled Counterfactual Learning (DCL) method for inferring physical commonsense from audiovisual data, which decouples video factors and models causal relationships using a variational autoencoder and a counterfactual learning module.</p><hr><h3>NetHack is Hard to Hack</h3>
<p>Ulyana Piterbarg, Lerrel Pinto, Rob Fergus</p>
<p><a href='https://openreview.net/forum?id=tp2nEZ5zfP'>https://openreview.net/forum?id=tp2nEZ5zfP</a></p>
<p><b>Keywords</b>: imitation learning, NetHack
</p><p><b>Compressor summary</b>: The paper studies why neural policy learning struggles in NetHack and proposes improvements based on a winning symbolic agent's strategy, leading to a better neural agent that still falls behind symbolic and human players.</p><hr><h3>Derandomized novelty detection with FDR control via conformal e-values</h3>
<p>Meshi Bashari, Amir Epstein, Yaniv Romano, Matteo Sesia</p>
<p><a href='https://openreview.net/forum?id=toYvRJ7Zmy'>https://openreview.net/forum?id=toYvRJ7Zmy</a></p>
<p><b>Keywords</b>: Conformal inference, Derandomization, E-values, False discovery rate, Out-of-distribution testing, Testing for outliers, Uncertainty
</p><p><b>Compressor summary</b>: The authors propose a method to make conformal inference more stable for novelty detection by using conformal e-values instead of p-values and weighting them with additional data information.</p><hr><h3>SafeDICE: Offline Safe Imitation Learning with Non-Preferred Demonstrations</h3>
<p>Youngsoo Jang, Geon-Hyeong Kim, Jongmin Lee, Sungryull Sohn, Byoungjip Kim, Honglak Lee, Moontae Lee</p>
<p><a href='https://openreview.net/forum?id=toEGuA9Qfn'>https://openreview.net/forum?id=toEGuA9Qfn</a></p>
<p><b>Keywords</b>: Imitation learning, Preference-based learning, Safe imitation learning
</p><p><b>Compressor summary</b>: The paper introduces SafeDICE, an offline imitation learning algorithm that learns a safe policy from non-preferred demonstrations and unlabeled data by estimating stationary distribution corrections.</p><hr><h3>Dream the Impossible: Outlier Imagination with Diffusion Models</h3>
<p>Xuefeng Du, Yiyou Sun, Jerry Zhu, Yixuan Li</p>
<p><a href='https://openreview.net/forum?id=tnRboxQIec'>https://openreview.net/forum?id=tnRboxQIec</a></p>
<p><b>Keywords</b>: Outlier imagination, machine learning
</p><p><b>Compressor summary</b>: The paper introduces Dream-OOD, a framework that generates realistic outlier images for machine learning models using in-distribution data and classes, without requiring manual data collection or cleaning.</p><hr><h3>Add and Thin: Diffusion for Temporal Point Processes</h3>
<p>David Lüdke, Marin Biloš, Oleksandr Shchur, Marten Lienen, Stephan Günnemann</p>
<p><a href='https://openreview.net/forum?id=tn9Dldam9L'>https://openreview.net/forum?id=tn9Dldam9L</a></p>
<p><b>Keywords</b>: Point Processes, Diffusion, Temporal Data, Generative Model, Forecasting, Density Estimation, Denoising
</p><p><b>Compressor summary</b>: The paper introduces ADD-THIN, a probabilistic denoising diffusion model that improves long-term forecasting for event sequences with discrete and continuous components by using temporal point process frameworks.</p><hr><h3>SPACE: Single-round Participant Amalgamation for Contribution Evaluation in Federated Learning</h3>
<p>Yi-Chung Chen, Hsi-Wen Chen, Shun-Guei Wang, Ming-Syan Chen</p>
<p><a href='https://openreview.net/forum?id=tmxjuIFSEc'>https://openreview.net/forum?id=tmxjuIFSEc</a></p>
<p><b>Keywords</b>: Federated Learning, Contribution Evaluation, Shapley Value, Knowledge Amalgamation
</p><p><b>Compressor summary</b>: The paper introduces SPACE, an efficient method for evaluating participant contributions in federated learning using Federated Knowledge Amalgamation and Prototype-based Model Evaluation.</p><hr><h3>Exponential Lower Bounds for Fictitious Play in Potential Games</h3>
<p>Ioannis Panageas, Nikolas Patris, Stratis Skoulakis, Volkan Cevher</p>
<p><a href='https://openreview.net/forum?id=tkenkPYkxj'>https://openreview.net/forum?id=tkenkPYkxj</a></p>
<p><b>Keywords</b>: fictitious play, convergence rate, potential games
</p><p><b>Compressor summary</b>: This paper studies the convergence rate of Fictitious Play in potential games and shows that it can take exponential time even for two-agent games.</p><hr><h3>Feature learning via mean-field Langevin dynamics: classifying sparse parities and beyond</h3>
<p>Taiji Suzuki, Denny Wu, Kazusato Oko, Atsushi Nitanda</p>
<p><a href='https://openreview.net/forum?id=tj86aGVNb3'>https://openreview.net/forum?id=tj86aGVNb3</a></p>
<p><b>Keywords</b>: mean-field regime, feature learning, Neural network optimization, sparse parity function, classification, sample complexity
</p><p><b>Compressor summary</b>: The paragraph discusses how mean-field Langevin dynamics (MFLD) can optimize and improve generalization performance of neural networks for binary classification problems by learning features, and provides a new analysis of its sample complexity and convergence rate.</p><hr><h3>No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models</h3>
<p>Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, Matt Kusner</p>
<p><a href='https://openreview.net/forum?id=thbXgJ8gNK'>https://openreview.net/forum?id=thbXgJ8gNK</a></p>
<p><b>Keywords</b>: language models, transformers, efficient training
</p><p><b>Compressor summary</b>: The authors investigate various efficient training algorithms for Transformer-based language models, but find that a fully-decayed learning rate outperforms them all.</p><hr><h3>GlyphControl: Glyph Conditional Control for Visual Text Generation</h3>
<p>Yukang Yang, Dongnan Gui, Yuhui Yuan, Weicong Liang, Haisong Ding, Han Hu, Kai Chen</p>
<p><a href='https://openreview.net/forum?id=thPI8hrA4V'>https://openreview.net/forum?id=thPI8hrA4V</a></p>
<p><b>Keywords</b>: Generative Models, Visual Text Generation, Diffusion Models
</p><p><b>Compressor summary</b>: The paper introduces a new method called GlyphControl to improve visual text generation by using glyph instructions with an off-the-shelf Stable-Diffusion model and creates a dataset called LAION-Glyph for further research.</p><hr><h3>Bypassing spike sorting: Density-based decoding using spike localization from dense multielectrode probes</h3>
<p>Yizi Zhang, Tianxiao He, Julien Boussard, Charlie Windolf, Olivier Winter, Eric M. Trautmann, Noam Roth, Hailey Barrel, Mark M Churchland, Nick Steinmetz, Erdem Varol, Cole Lincoln Hurwitz, Liam Paninski</p>
<p><a href='https://openreview.net/forum?id=tgQRMrsxht'>https://openreview.net/forum?id=tgQRMrsxht</a></p>
<p><b>Keywords</b>: neural decoding, brain-computer interfaces, spike sorting, variational inference, generative models
</p><p><b>Compressor summary</b>: The authors propose a new method for decoding neural activity without spike sorting, using mixture of Gaussians to model uncertainty and improve performance in brain-computer interfaces.</p><hr><h3>SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models</h3>
<p>Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, Zhaoxiang Zhang</p>
<p><a href='https://openreview.net/forum?id=tfyr2zRVoK'>https://openreview.net/forum?id=tfyr2zRVoK</a></p>
<p><b>Keywords</b>: Large Language Model; Task Planning; Embodied AI; Robotics; Software Automation
</p><p><b>Compressor summary</b>: The authors propose SheetCopilot, an agent that uses natural language to control spreadsheets and complete various tasks, and present a dataset and evaluation pipeline to benchmark its performance.</p><hr><h3>Stable Diffusion is Unstable</h3>
<p>Chengbin Du, Yanxi Li, Zhongwei Qiu, Chang Xu</p>
<p><a href='https://openreview.net/forum?id=tesBViWnbx'>https://openreview.net/forum?id=tesBViWnbx</a></p>
<p><b>Keywords</b>: Adversarial Attack, Generative Model, Diffusion Model, Latent Diffusion Model, Conditional Latent Diffusion Model
</p><p><b>Compressor summary</b>: Auto-attack on Text-to-image Models (ATM) is a method to make text-to-image models more robust by generating perturbations that can prevent them from blending or losing primary subjects in generated images.</p><hr><h3>FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy</h3>
<p>Zuhao Yang, Yingfang Yuan, Yang Xu, SHUO ZHAN, Huajun Bai, Kefan Chen</p>
<p><a href='https://openreview.net/forum?id=tdyLryDebq'>https://openreview.net/forum?id=tdyLryDebq</a></p>
<p><b>Keywords</b>: natural language generation; evaluation metrics; cross-entropy; language model
</p><p><b>Compressor summary</b>: The proposed FACE metrics use Fourier Analysis of Cross-Entropy to measure similarity between model-generated and human language, effectively identifying the gap between them and reflecting various factors such as model size and sampling methods.</p><hr><h3>CQM: Curriculum Reinforcement Learning with a Quantized World Model</h3>
<p>Seungjae Lee, Daesol Cho, Jonghae Park, H. Jin Kim</p>
<p><a href='https://openreview.net/forum?id=tcotyjon2a'>https://openreview.net/forum?id=tcotyjon2a</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Curriculum Learning, Goal-conditioned RL
</p><p><b>Compressor summary</b>: The authors propose a new method for curriculum RL that automatically defines a semantic goal space from continuous observations and suggests uncertainty and temporal distance-aware goals, improving exploration and performance on goal-reaching tasks.</p><hr><h3>Tracr: Compiled Transformers as a Laboratory for Interpretability</h3>
<p>David Lindner, Janos Kramar, Sebastian Farquhar, Matthew Rahtz, Thomas McGrath, Vladimir Mikulik</p>
<p><a href='https://openreview.net/forum?id=tbbId8u7nP'>https://openreview.net/forum?id=tbbId8u7nP</a></p>
<p><b>Keywords</b>: interpretability, transformers, language models, RASP, Tracr, mechanistic interpretability
</p><p><b>Compressor summary</b>: The authors introduce a compiler called Tracr that generates structured transformer models from human-readable programs, enabling experiments and interpretability evaluations.</p><hr><h3>Ignorance is Bliss: Robust Control via Information Gating</h3>
<p>Manan Tomar, Riashat Islam, Matthew E. Taylor, Sergey Levine, Philip Bachman</p>
<p><a href='https://openreview.net/forum?id=tW2KSph9o8'>https://openreview.net/forum?id=tW2KSph9o8</a></p>
<p><b>Keywords</b>: representation learning, mutual information
</p><p><b>Compressor summary</b>: Information gating is a method for learning efficient and robust representations by selectively revealing or hiding information depending on the task at hand.</p><hr><h3>Language Semantic Graph Guided Data-Efficient Learning</h3>
<p>Wenxuan Ma, Shuang Li, Lincan Cai, Jingxuan Kang</p>
<p><a href='https://openreview.net/forum?id=tUyW68cRqr'>https://openreview.net/forum?id=tUyW68cRqr</a></p>
<p><b>Keywords</b>: Data-Efficient Learning, Language Semantic Graph
</p><p><b>Compressor summary</b>: The paragraph discusses a new approach called Language Semantic Graph (LSG) that uses semantic information from labels to improve data efficiency in machine learning tasks, and demonstrates its effectiveness across different modalities and scenarios.</p><hr><h3>ForecastPFN: Synthetically-Trained Zero-Shot Forecasting</h3>
<p>Samuel Dooley, Gurnoor Singh Khurana, Chirag Mohapatra, Siddartha Venkat Naidu, Colin White</p>
<p><a href='https://openreview.net/forum?id=tScBQRNgjk'>https://openreview.net/forum?id=tScBQRNgjk</a></p>
<p><b>Keywords</b>: Forecasting, Zero-shot, Synthetic Data
</p><p><b>Compressor summary</b>: ForecastPFN is a new zero-shot forecasting model that uses synthetic data and Bayesian inference to make more accurate and faster predictions than existing methods, even with limited real-life observations.</p><hr><h3>Human-Guided Complexity-Controlled Abstractions</h3>
<p>Andi Peng, Mycal Tucker, Eoin M. Kenny, Noga Zaslavsky, Pulkit Agrawal, Julie Shah</p>
<p><a href='https://openreview.net/forum?id=tSEeRl7ACo'>https://openreview.net/forum?id=tSEeRl7ACo</a></p>
<p><b>Keywords</b>: human-in-the-loop, representation learning, interpretability
</p><p><b>Compressor summary</b>: The authors propose training neural networks to generate a spectrum of discrete representations and control their complexity based on the task, which leads to better generalization and can be informed by human intuition.</p><hr><h3>Modeling Human Visual Motion Processing with Trainable Motion Energy Sensing and a Self-attention Network</h3>
<p>Zitang Sun, Yen-Ju Chen, Yung-Hao Yang, Shin'ya Nishida</p>
<p><a href='https://openreview.net/forum?id=tRKimbAk5D'>https://openreview.net/forum?id=tRKimbAk5D</a></p>
<p><b>Keywords</b>: motion perception, optical flow estimation, attention mechanism, psychophysics, In silico neurophysiology, human vision
</p><p><b>Compressor summary</b>: The paper proposes a new image-computable model that combines biological and computer vision approaches to simulate human motion perception in complex scenes.</p><hr><h3>D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts</h3>
<p>Fenggen Yu, Qimin Chen, Maham Tanveer, Ali Mahdavi Amiri, Hao Zhang</p>
<p><a href='https://openreview.net/forum?id=tQYGjnxPOm'>https://openreview.net/forum?id=tQYGjnxPOm</a></p>
<p><b>Keywords</b>: 3D reconstruction, constructive solid geometry, unsupervised learning, compact shape assembly
</p><p><b>Compressor summary</b>: D$^2$CSG is a neural model that learns to reconstruct 3D CAD shapes using quadric primitives and a dedicated residual branch for complex shape complements.</p><hr><h3>Non-Stationary Bandits with Auto-Regressive Temporal Dependency</h3>
<p>Qinyi Chen, Negin Golrezaei, Djallel Bouneffouf</p>
<p><a href='https://openreview.net/forum?id=tP50lLiZIo'>https://openreview.net/forum?id=tP50lLiZIo</a></p>
<p><b>Keywords</b>: non-stationary bandits; autoregressive model; low-regret policy; online learning algorithms
</p><p><b>Compressor summary</b>: The paper introduces a new non-stationary MAB framework that considers temporal dynamics in real-world applications like recommendation systems, and proposes an algorithm with two mechanisms for exploration-exploitation and discarding outdated information.</p><hr><h3>Optimistic Active Exploration of Dynamical Systems</h3>
<p>Bhavya Sukhija, Lenart Treven, Cansu Sancaktar, Sebastian Blaes, Stelian Coros, Andreas Krause</p>
<p><a href='https://openreview.net/forum?id=tLrkjK128n'>https://openreview.net/forum?id=tLrkjK128n</a></p>
<p><b>Keywords</b>: Active Exploration, Reinforcement Learning, Dynamical Systems
</p><p><b>Compressor summary</b>: OPAX is an algorithm for active exploration in reinforcement learning that uses probabilistic models to quantify uncertainty and maximizes information gain, enabling zero-shot solutions for multiple tasks.</p><hr><h3>Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts</h3>
<p>Emanuele Marconato, Stefano Teso, Antonio Vergari, Andrea Passerini</p>
<p><a href='https://openreview.net/forum?id=tLTtqySDFb'>https://openreview.net/forum?id=tLTtqySDFb</a></p>
<p><b>Keywords</b>: Neuro-Symbolic Integration, Trustworthy AI, Concept Learning, Learning Shortcuts, Mitigation Strategies
</p><p><b>Compressor summary</b>: The paragraph discusses how neuro-symbolic predictive models can suffer from reasoning shortcuts that compromise their advantages, and proposes mitigation strategies to address this issue.</p><hr><h3>Emergent Communication in Interactive Sketch Question Answering</h3>
<p>Zixing Lei, Yiming Zhang, Yuxin Xiong, Siheng Chen</p>
<p><a href='https://openreview.net/forum?id=tLEDsaKuDh'>https://openreview.net/forum?id=tLEDsaKuDh</a></p>
<p><b>Keywords</b>: Emergent communication, Interactive, Question Answering
</p><p><b>Compressor summary</b>: The authors propose a new task (ISQA) and system for vision-based emergent communication, which uses sketches to answer questions about images in multiple rounds, improving accuracy, complexity, and interpretability.</p><hr><h3>Parallel-mentoring for Offline Model-based Optimization</h3>
<p>Can Chen, Christopher Beckham, Zixuan Liu, Xue Liu, Christopher Pal</p>
<p><a href='https://openreview.net/forum?id=tJwyg9Zg9G'>https://openreview.net/forum?id=tJwyg9Zg9G</a></p>
<p><b>Keywords</b>: offline model-based optimization, bi-level optimization
</p><p><b>Compressor summary</b>: The authors propose parallel-mentoring, a novel method that uses majority voting and adaptive soft-labeling to improve model-based optimization with multiple proxies, addressing out-of-distribution issues in black-box objective function maximization.</p><hr><h3>Offline RL with Discrete Proxy Representations for Generalizability in POMDPs</h3>
<p>Pengjie Gu, Xinyu Cai, Dong Xing, Xinrun Wang, Mengchen Zhao, Bo An</p>
<p><a href='https://openreview.net/forum?id=tJN664ZNVG'>https://openreview.net/forum?id=tJN664ZNVG</a></p>
<p><b>Keywords</b>: Offline RL, POMDP
</p><p><b>Compressor summary</b>: ORDER is a probabilistic framework for offline RL that uses discrete state representations and proxy representations to improve robustness against diverse masked observabilities.</p><hr><h3>Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data</h3>
<p>Boris van Breugel, Nabeel Seedat, Fergus Imrie, Mihaela van der Schaar</p>
<p><a href='https://openreview.net/forum?id=tJ88RBqupo'>https://openreview.net/forum?id=tJ88RBqupo</a></p>
<p><b>Keywords</b>: model evaluation, tabular, synthetic data
</p><p><b>Compressor summary</b>: The paper proposes 3S Testing, a method that uses generative models to create synthetic test data for small subgroups and simulate distributional shifts, improving the evaluation of machine learning model performance on diverse and underrepresented groups.</p><hr><h3>Pre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction</h3>
<p>Zuobai Zhang, Minghao Xu, Aurelie Lozano, Vijil Chenthamarakshan, Payel Das, Jian Tang</p>
<p><a href='https://openreview.net/forum?id=tIzbNQko3c'>https://openreview.net/forum?id=tIzbNQko3c</a></p>
<p><b>Keywords</b>: Protein representation learning, diffusion models, self-supervised learning
</p><p><b>Compressor summary</b>: The paper proposes a pre-training method for protein encoders using joint sequence-structure diffusion models and enhances it with a technique to capture conformational variations among protein conformers.</p><hr><h3>Latent Graph Inference with Limited Supervision</h3>
<p>Jianglin Lu, Yi Xu, Huan Wang, Yue Bai, Yun Fu</p>
<p><a href='https://openreview.net/forum?id=tGuMwFnRZX'>https://openreview.net/forum?id=tGuMwFnRZX</a></p>
<p><b>Keywords</b>: Latent Graph Inference, CUR Matrix Decomposition, Graph Neural Networks
</p><p><b>Compressor summary</b>: The paper proposes a method to improve latent graph inference by restoring corrupted affinities and replenishing missed supervision for better node representation learning.</p><hr><h3>Distribution-Free Model-Agnostic Regression Calibration via Nonparametric Methods</h3>
<p>Shang Liu, Zhongze Cai, Xiaocheng Li</p>
<p><a href='https://openreview.net/forum?id=tGPx7HdBr4'>https://openreview.net/forum?id=tGPx7HdBr4</a></p>
<p><b>Keywords</b>: Regression calibration, model recalibration, conditional quantile, nonparametric method
</p><p><b>Compressor summary</b>: The paper proposes nonparametric methods for individual calibration of regression models that are efficient, consistent, and provide theoretical guarantees, addressing limitations of existing heuristic methods.</p><hr><h3>Maximum State Entropy Exploration using Predecessor and Successor Representations</h3>
<p>Arnav Kumar Jain, Lucas Lehnert, Irina Rish, Glen Berseth</p>
<p><a href='https://openreview.net/forum?id=tFsxtqGmkn'>https://openreview.net/forum?id=tFsxtqGmkn</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Maximum state entropy exploration, Non-Markovian exploration, Successor Representation
</p><p><b>Compressor summary</b>: The authors propose a new exploration algorithm, $\eta\psi$-Learning, that learns efficient policies by using past experience to predict state visitation entropy and maximize coverage.</p><hr><h3>Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions</h3>
<p>Sayantan Choudhury, Eduard Gorbunov, Nicolas Loizou</p>
<p><a href='https://openreview.net/forum?id=tFeaLw9AWn'>https://openreview.net/forum?id=tFeaLw9AWn</a></p>
<p><b>Keywords</b>: Optimization, Machine Learning, Extragradient Methods, Min-Max Optimization
</p><p><b>Compressor summary</b>: The paper studies single-call stochastic extragradient methods for solving large-scale min-max optimization and variational inequalities problems, improves their convergence guarantees, and applies them to two classes of structured non-monotone VIPs using the expected residual condition.</p><hr><h3>Federated Compositional Deep AUC Maximization</h3>
<p>Xinwen Zhang, Yihan Zhang, Tianbao Yang, Richard Souvenir, Hongchang Gao</p>
<p><a href='https://openreview.net/forum?id=tF7W8ai8J3'>https://openreview.net/forum?id=tF7W8ai8J3</a></p>
<p><b>Keywords</b>: federated learning, compositional optimization, minimax optimization, AUC maximization
</p><p><b>Compressor summary</b>: The paragraph discusses a novel federated learning method for imbalanced data that optimizes the AUC score and provides theoretical and empirical evidence of its effectiveness.</p><hr><h3>PPi: Pretraining Brain Signal Model for Patient-independent Seizure Detection</h3>
<p>Zhizhang Yuan, Daoze Zhang, Yang Yang, Junru Chen, Yafeng Li</p>
<p><a href='https://openreview.net/forum?id=tEmFyqjaJh'>https://openreview.net/forum?id=tEmFyqjaJh</a></p>
<p><b>Keywords</b>: Brain signal, Seizure detection, Pretraining, Domain generalization
</p><p><b>Compressor summary</b>: The study proposes a patient-independent seizure detection model using stereoelectroencephalography (SEEG) data that overcomes challenges like domain shift and pattern evolution with novel self-supervised tasks and domain generalization techniques.</p><hr><h3>Safety Verification of Decision-Tree Policies in Continuous Time</h3>
<p>Christian Schilling, Anna Lukina, Emir Demirović, Kim Guldstrand Larsen</p>
<p><a href='https://openreview.net/forum?id=tEKBU5XOTw'>https://openreview.net/forum?id=tEKBU5XOTw</a></p>
<p><b>Keywords</b>: safety verification, decision tree, reinforcement learning, controller, continuous time
</p><p><b>Compressor summary</b>: The paper introduces an algorithm to verify the safety of decision tree controlled systems in continuous time, overcoming the challenges of existing methods.</p><hr><h3>Doubly Constrained Fair Clustering</h3>
<p>John P Dickerson, Seyed A. Esmaeili, Jamie Heather Morgenstern, Claire Jie Zhang</p>
<p><a href='https://openreview.net/forum?id=tECyQO1QOp'>https://openreview.net/forum?id=tECyQO1QOp</a></p>
<p><b>Keywords</b>: Fairness, Clustering, Approximation Algorithms
</p><p><b>Compressor summary</b>: The paper investigates how two common fairness criteria for clustering, group fairness and diversity in center selection, can be simultaneously satisfied using a constant approximation algorithm, and shows that they are compatible with some but not all distance-based fairness notions.</p><hr><h3>A Robust and Opponent-Aware League Training Method for StarCraft II</h3>
<p>Ruozi Huang, Xipeng Wu, Hongsheng Yu, Zhong Fan, Haobo Fu, QIANG FU, Yang Wei</p>
<p><a href='https://openreview.net/forum?id=tDAu3FPJn9'>https://openreview.net/forum?id=tDAu3FPJn9</a></p>
<p><b>Keywords</b>: StarCraft II, league training, AlphaStar, opponent-modeling, reinforcement learning
</p><p><b>Compressor summary</b>: The paper presents improvements to AlphaStar's league training for StarCraft II, enabling better and more resource-efficient AI agents with advanced exploitation and opponent modeling abilities.</p><hr><h3>On the Power of SVD in the Stochastic Block Model</h3>
<p>Xinyu Mao, Jiapeng Zhang</p>
<p><a href='https://openreview.net/forum?id=tC0r8duG9z'>https://openreview.net/forum?id=tC0r8duG9z</a></p>
<p><b>Keywords</b>: Clustering Algorithms, Stochastic Block Model, Spectral Algorithms
</p><p><b>Compressor summary</b>: The paper investigates how spectral methods like SVD can improve clustering algorithms and shows that it works well for a specific type of network model called the stochastic block model.</p><hr><h3>Replicable Reinforcement Learning</h3>
<p>ERIC EATON, Marcel Hussing, Michael Kearns, Jessica Sorrell</p>
<p><a href='https://openreview.net/forum?id=tBwRbgsol1'>https://openreview.net/forum?id=tBwRbgsol1</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Learning Theory, Replicability, Reproducibility
</p><p><b>Compressor summary</b>: The text discusses the development of algorithm frameworks for replicability in various fields and introduces provably replicable algorithms for parallel value iteration and R-Max in episodic reinforcement learning.</p><hr><h3>Understanding Deep Gradient Leakage via Inversion Influence Functions</h3>
<p>Haobo Zhang, Junyuan Hong, Yuyang Deng, Mehrdad Mahdavi, Jiayu Zhou</p>
<p><a href='https://openreview.net/forum?id=tBib2fWr3r'>https://openreview.net/forum?id=tBib2fWr3r</a></p>
<p><b>Keywords</b>: Deep Learning, Privacy, Federated Learning, Influence Function
</p><p><b>Compressor summary</b>: The paper proposes a method called Inversion Influence Function (I$^2$F) to analyze and understand the privacy leakage caused by Deep Gradient Leakage (DGL) attack in distributed learning settings, and show its effectiveness on various scenarios.</p><hr><h3>A Bounded Ability Estimation for Computerized Adaptive Testing</h3>
<p>Yan Zhuang, Qi Liu, GuanHao Zhao, Zhenya Huang, Weizhe Huang, Zachary Pardos, Enhong Chen, Jinze Wu, Xin Li</p>
<p><a href='https://openreview.net/forum?id=tAwjG5bM7H'>https://openreview.net/forum?id=tAwjG5bM7H</a></p>
<p><b>Keywords</b>: adaptive learning, computerized adaptive testing, educational measurement, cognitive diagnosis
</p><p><b>Compressor summary</b>: The paper proposes a new method (BECAT) for computerized adaptive testing that selects questions to estimate student's ability more accurately and efficiently by matching the gradient of full responses.</p><hr><h3>Uncertainty Estimation for Safety-critical Scene Segmentation via Fine-grained Reward Maximization</h3>
<p>Hongzheng Yang, Cheng Chen, Yueyao Chen, Markus Scheppach, Hon Chi Yip, Qi Dou</p>
<p><a href='https://openreview.net/forum?id=t9Swbo82dB'>https://openreview.net/forum?id=t9Swbo82dB</a></p>
<p><b>Keywords</b>: uncertainty estimation, semantic segmentation, medical application
</p><p><b>Compressor summary</b>: The paper proposes a novel framework that uses reinforcement learning and reward optimization to improve uncertainty estimation in deep segmentation models for safety-critical applications like medical imaging.</p><hr><h3>Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models</h3>
<p>Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, Rongrong Ji</p>
<p><a href='https://openreview.net/forum?id=t877958UGZ'>https://openreview.net/forum?id=t877958UGZ</a></p>
<p><b>Keywords</b>: vision-language instruction tuning, multimodal LLM, efficient training
</p><p><b>Compressor summary</b>: The paper proposes a cost-effective method called Mixture-of-Modality Adaptation (MMA) to improve the multimodal capabilities of large language models for vision-language tasks, achieving competitive performance and efficiency with low expenditure.</p><hr><h3>Rewiring Neurons in Non-Stationary Environments</h3>
<p>Zhicheng Sun, Yadong MU</p>
<p><a href='https://openreview.net/forum?id=t7ozN4AXd0'>https://openreview.net/forum?id=t7ozN4AXd0</a></p>
<p><b>Keywords</b>: continual learning, reinforcement learning, brain-inspired learning
</p><p><b>Compressor summary</b>: The paragraph describes a novel rewiring approach for the human brain in continual reinforcement learning that promotes adaptivity and exploration, with a focus on stability and plasticity-stability tradeoffs.</p><hr><h3>Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network</h3>
<p>Tristan Deleu, Mizu Nishikawa-Toomey, Jithendaraa Subramanian, Nikolay Malkin, Laurent Charlin, Yoshua Bengio</p>
<p><a href='https://openreview.net/forum?id=t7lnhhi7De'>https://openreview.net/forum?id=t7lnhhi7De</a></p>
<p><b>Keywords</b>: bayesian network, bayesian, structure learning, causal discovery, gflownet
</p><p><b>Compressor summary</b>: The paper proposes JSP-GFN, a method that uses GFlowNets to approximate the joint posterior over Bayesian Network structure and parameters, achieving good results on various datasets.</p><hr><h3>Achieving Cross Modal Generalization with Multimodal Unified Representation</h3>
<p>Yan Xia, Hai Huang, Jieming Zhu, Zhou Zhao</p>
<p><a href='https://openreview.net/forum?id=t7ZowrDWVw'>https://openreview.net/forum?id=t7ZowrDWVw</a></p>
<p><b>Keywords</b>: multi-modal, discrete representation, mutual information estimation
</p><p><b>Compressor summary</b>: The paper proposes a novel task called Cross Modal Generalization (CMG) that learns a unified discrete representation from paired multimodal data and introduces Uni-Code, which contains two key contributions for fine-grained unified representation of multimodal sequences.</p><hr><h3>Trans-Dimensional Generative Modeling via Jump Diffusion Models</h3>
<p>Andrew Campbell, William Harvey, Christian Dietrich Weilbach, Valentin De Bortoli, Tom Rainforth, Arnaud Doucet</p>
<p><a href='https://openreview.net/forum?id=t6nA7x3GAC'>https://openreview.net/forum?id=t6nA7x3GAC</a></p>
<p><b>Keywords</b>: diffusion, score-based, score, markov chain, jump diffusion, poisson
</p><p><b>Compressor summary</b>: The proposed generative model can handle data of different dimensions by switching between them during the generation process, leading to better performance on tasks like imputation and interpolation.</p><hr><h3>QuadAttac$K$: A Quadratic Programming Approach to Learning Ordered Top-$K$ Adversarial Attacks</h3>
<p>Thomas Paniagua, Ryan Grainger, Tianfu Wu</p>
<p><a href='https://openreview.net/forum?id=t3vPEjgNtj'>https://openreview.net/forum?id=t3vPEjgNtj</a></p>
<p><b>Keywords</b>: Ordered Top-K Clear-Box Targeted Adversarial Attack, Deep Neural Networks, Quadratic Programming, Robustness
</p><p><b>Compressor summary</b>: The paper introduces QuadAttac$K$, a method to learn more aggressive targeted attacks on DNNs by solving quadratic programming in the feature embedding space.</p><hr><h3>Scalable Membership Inference Attacks via Quantile Regression</h3>
<p>Martin Andres Bertran, Shuai Tang, Aaron Roth, Michael Kearns, Jamie Heather Morgenstern, Steven Wu</p>
<p><a href='https://openreview.net/forum?id=t3WCiGjHqd'>https://openreview.net/forum?id=t3WCiGjHqd</a></p>
<p><b>Keywords</b>: machine learning, privacy, membership inference
</p><p><b>Compressor summary</b>: Our proposed attack estimates the distribution of confidence scores on non-training points using quantile regression, and it is more efficient and black-box than shadow model attacks.</p><hr><h3>Tailoring Self-Attention for Graph via Rooted Subtrees</h3>
<p>Siyuan Huang, Yunchong Song, Jiayue Zhou, Zhouhan Lin</p>
<p><a href='https://openreview.net/forum?id=t2hEZadBBk'>https://openreview.net/forum?id=t2hEZadBBk</a></p>
<p><b>Keywords</b>: Graph Based Learning
</p><p><b>Compressor summary</b>: The paper introduces Subtree Attention, a novel graph attention mechanism that addresses the limitations of local and global attention by allowing computation of attention weights among multi-hop neighbors and has linear time complexity.</p><hr><h3>Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities</h3>
<p>Andrii Zadaianchuk, Maximilian Seitzer, Georg Martius</p>
<p><a href='https://openreview.net/forum?id=t1jLRFvBqm'>https://openreview.net/forum?id=t1jLRFvBqm</a></p>
<p><b>Keywords</b>: object-centric learning, video, representation learning, self-supervised learning, unsupervised learning
</p><p><b>Compressor summary</b>: The paper proposes a new method to learn object-centric representations from videos using pre-trained features and temporal feature similarity loss, achieving state-of-the-art results on synthetic and unconstrained video datasets.</p><hr><h3>A unified framework for information-theoretic generalization bounds</h3>
<p>Yifeng Chu, Maxim Raginsky</p>
<p><a href='https://openreview.net/forum?id=t0fkjO4aZj'>https://openreview.net/forum?id=t0fkjO4aZj</a></p>
<p><b>Keywords</b>: generalization bounds, information theory, chaining, PAC-Bayes, couplings
</p><p><b>Compressor summary</b>: The paper proposes a method to derive generalization bounds for learning algorithms using a probabilistic decorrelation lemma and other techniques.</p><hr><h3>Riemannian Projection-free Online Learning</h3>
<p>Zihao Hu, Guanghui Wang, Jacob Abernethy</p>
<p><a href='https://openreview.net/forum?id=szFqlNRxeS'>https://openreview.net/forum?id=szFqlNRxeS</a></p>
<p><b>Keywords</b>: Online learning, Riemannian optimization, projection-free optimization
</p><p><b>Compressor summary</b>: The paper proposes projection-free optimization methods for online geodesically convex problems on curved spaces with separation or linear optimization oracles, achieving sub-linear regret guarantees in different settings.</p><hr><h3>A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks</h3>
<p>Vignesh Kothapalli, Tom Tirer, Joan Bruna</p>
<p><a href='https://openreview.net/forum?id=sxao2udWXi'>https://openreview.net/forum?id=sxao2udWXi</a></p>
<p><b>Keywords</b>: Neural collapse, Graph neural networks, Community detection
</p><p><b>Compressor summary</b>: The paper investigates how graph topology affects feature evolution in node-wise classification using graph neural networks (GNNs) and compares it to the Neural Collapse phenomenon in instance-wise deep classifiers.</p><hr><h3>Is RLHF More Difficult than Standard RL? A Theoretical Perspective</h3>
<p>Yuanhao Wang, Qinghua Liu, Chi Jin</p>
<p><a href='https://openreview.net/forum?id=sxZLrBqg50'>https://openreview.net/forum?id=sxZLrBqg50</a></p>
<p><b>Keywords</b>: reinforcement learning theory, reinforcement learning from human feedback, preference-based reinforcement learning
</p><p><b>Compressor summary</b>: The paper shows that existing reward-based RL algorithms can be used to solve preference-based RL with minimal modifications, and applies the theory to various models with preferences.</p><hr><h3>Meta-in-context learning in large language models</h3>
<p>Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew Botvinick, Jane X Wang, Eric Schulz</p>
<p><a href='https://openreview.net/forum?id=sx0xpaO0za'>https://openreview.net/forum?id=sx0xpaO0za</a></p>
<p><b>Keywords</b>: Large language models, in-context learning, meta-learning, GPT-3
</p><p><b>Compressor summary</b>: The paper introduces meta-in-context learning, a method to improve large language models' in-context learning abilities by recursively applying them to different tasks, and shows its effectiveness on various domains and benchmarks.</p><hr><h3>The noise level in linear regression with dependent data</h3>
<p>Ingvar Ziemann, Stephen Tu, George J. Pappas, Nikolai Matni</p>
<p><a href='https://openreview.net/forum?id=swNtr6vGqg'>https://openreview.net/forum?id=swNtr6vGqg</a></p>
<p><b>Keywords</b>: Learning Theory, Learning with dependent data, Time-Series
</p><p><b>Compressor summary</b>: We find upper limits for random linear regression with dependent data without assuming realizability, and our analysis shows how the error improves as we reduce misspecification.</p><hr><h3>A Unified, Scalable Framework for Neural Population Decoding</h3>
<p>Mehdi Azabou, Vinam Arora, Venkataramana Ganesh, Ximeng Mao, Santosh B Nachimuthu, Michael Jacob Mendelson, Blake Aaron Richards, Matthew G Perich, Guillaume Lajoie, Eva L Dyer</p>
<p><a href='https://openreview.net/forum?id=sw2Y0sirtM'>https://openreview.net/forum?id=sw2Y0sirtM</a></p>
<p><b>Keywords</b>: neural population, brain decoder, transformer, tokenization, sequence-to-sequence, electrophysiology, brain-computer interfaces
</p><p><b>Compressor summary</b>: The paper introduces a method to model neural activity across diverse recordings using tokenization, cross-attention, and PerceiverIO, enabling few-shot performance with minimal labels in new sessions.</p><hr><h3>CEIL: Generalized Contextual Imitation Learning</h3>
<p>Jinxin Liu, Li He, Yachen Kang, Zifeng Zhuang, Donglin Wang, Huazhe Xu</p>
<p><a href='https://openreview.net/forum?id=suzMI2P1rT'>https://openreview.net/forum?id=suzMI2P1rT</a></p>
<p><b>Keywords</b>: imitation learning, reinforcement learning, offline imitation learning
</p><p><b>Compressor summary</b>: The paper introduces ContExtual Imitation Learning (CEIL), a versatile algorithm for imitation learning that learns from expert behaviors using hindsight embeddings, and demonstrates its effectiveness on various settings and benchmarks.</p><hr><h3>Riemannian SAM: Sharpness-Aware Minimization on Riemannian Manifolds</h3>
<p>Jihun Yun, Eunho Yang</p>
<p><a href='https://openreview.net/forum?id=strvrjSi3C'>https://openreview.net/forum?id=strvrjSi3C</a></p>
<p><b>Keywords</b>: optimization, riemannian, manifolds, sharpness-aware
</p><p><b>Compressor summary</b>: The paper introduces Riemannian SAM, a novel optimization algorithm for training geometric deep learning models, and shows its benefits on knowledge graph completion and machine translation tasks.</p><hr><h3>The Simplicity Bias in Multi-Task RNNs: Shared Attractors, Reuse of Dynamics, and Geometric Representation</h3>
<p>Elia Turner, Omri Barak</p>
<p><a href='https://openreview.net/forum?id=stDm3S0CV7'>https://openreview.net/forum?id=stDm3S0CV7</a></p>
<p><b>Keywords</b>: Computational Neural Models; Recurrent Neural Networks; Multiple Tasks; Geometry;Dynamical Systems;Attractors;Neuroscience
</p><p><b>Compressor summary</b>: The authors study how Recurrent Neural Networks (RNNs) handle multiple tasks and find that they tend to reuse existing dynamics and opt for simple solutions, which they call the "simplicity bias".</p><hr><h3>Stable and low-precision training for large-scale vision-language models</h3>
<p>Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari S. Morcos, Ali Farhadi, Ludwig Schmidt</p>
<p><a href='https://openreview.net/forum?id=sqqASmpA2R'>https://openreview.net/forum?id=sqqASmpA2R</a></p>
<p><b>Keywords</b>: CLIP, int8, stability
</p><p><b>Compressor summary</b>: The authors propose SwitchBack, a fast linear layer for int8 training, and recommend a hybrid of AdamW and Adafactor to prevent loss spikes in large language-vision models.</p><hr><h3>HASSOD: Hierarchical Adaptive Self-Supervised Object Detection</h3>
<p>Shengcao Cao, Dhiraj Joshi, Liangyan Gui, Yu-Xiong Wang</p>
<p><a href='https://openreview.net/forum?id=sqkGJjIRfG'>https://openreview.net/forum?id=sqkGJjIRfG</a></p>
<p><b>Keywords</b>: self-supervised learning, object detection
</p><p><b>Compressor summary</b>: HASSOD is a novel self-supervised object detection method that learns to detect objects and understand their compositions without human supervision, achieving improved performance and interpretability over existing methods.</p><hr><h3>Sparsity-Preserving Differentially Private Training of Large Embedding Models</h3>
<p>Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang</p>
<p><a href='https://openreview.net/forum?id=sqTcCXkG4P'>https://openreview.net/forum?id=sqTcCXkG4P</a></p>
<p><b>Keywords</b>: Differential Privacy, Recommendation Systems, Embedding Models, Efficient Machine Learning
</p><p><b>Compressor summary</b>: The paper introduces DP-FEST and DP-AdaFEST, two new privacy-preserving algorithms for training large embedding models that reduce gradient size and maintain accuracy.</p><hr><h3>What’s Left? Concept Grounding with Logic-Enhanced Foundation Models</h3>
<p>Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Jiajun Wu</p>
<p><a href='https://openreview.net/forum?id=sq4o3tjWaj'>https://openreview.net/forum?id=sq4o3tjWaj</a></p>
<p><b>Keywords</b>: concept learning, visual reasoning, large language models, neuro-symbolic learning
</p><p><b>Compressor summary</b>: The paper proposes LEFT, a framework that learns to ground and reason with concepts across domains using a logic-based program executor and an LLM interpreter, outperforming previous methods in reasoning tasks.</p><hr><h3>Belief Projection-Based Reinforcement Learning for Environments with Delayed Feedback</h3>
<p>Jangwon Kim, Hangyeol Kim, Jiwook Kang, Jongchan Baek, Soohee Han</p>
<p><a href='https://openreview.net/forum?id=sq0m11cUMV'>https://openreview.net/forum?id=sq0m11cUMV</a></p>
<p><b>Keywords</b>: time-delay system, reinforcement learning
</p><p><b>Compressor summary</b>: The paragraph describes a new actor-critic algorithm called BPQL that solves delayed feedback problems in reinforcement learning by reducing the state space size and outperforming traditional methods in continuous control tasks.</p><hr><h3>Unsupervised Semantic Correspondence Using Stable Diffusion</h3>
<p>Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi</p>
<p><a href='https://openreview.net/forum?id=sovxUzPzLN'>https://openreview.net/forum?id=sovxUzPzLN</a></p>
<p><b>Keywords</b>: Semantic Correspondence, Stable Diffusion, Optimization-based Inference
</p><p><b>Compressor summary</b>: The authors propose a method to find semantic correspondences between images using diffusion models' semantic knowledge without any training, achieving results comparable to strongly supervised methods.</p><hr><h3>Dynamically Masked Discriminator for GANs</h3>
<p>Wentian Zhang, Haozhe Liu, Bing Li, Jinheng Xie, Yawen Huang, Yuexiang Li, Yefeng Zheng, Bernard Ghanem</p>
<p><a href='https://openreview.net/forum?id=sodl2c3aTM'>https://openreview.net/forum?id=sodl2c3aTM</a></p>
<p><b>Keywords</b>: Generative model, Generative Adversarial Network
</p><p><b>Compressor summary</b>: The paper proposes a new GAN method that helps the discriminator adapt faster to changes in generated data, improving the quality of generated results.</p><hr><h3>AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</h3>
<p>Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, Chenliang Xu</p>
<p><a href='https://openreview.net/forum?id=snY3FOnlQi'>https://openreview.net/forum?id=snY3FOnlQi</a></p>
<p><b>Keywords</b>: Scene synthesis, audio-visual, NeRF
</p><p><b>Compressor summary</b>: The authors propose a novel approach to synthesize realistic audio-visual scenes using NeRF, integrating knowledge of sound propagation and source-centric acoustic fields, and present a new dataset and demo videos.</p><hr><h3>Beyond MLE: Convex Learning for Text Generation</h3>
<p>Chenze Shao, Zhengrui Ma, Min Zhang, Yang Feng</p>
<p><a href='https://openreview.net/forum?id=sla7V80uWA'>https://openreview.net/forum?id=sla7V80uWA</a></p>
<p><b>Keywords</b>: Maximum likelihood estimation, Convex function, Text generation
</p><p><b>Compressor summary</b>: The paper proposes convex functions as an alternative to maximum likelihood estimation (MLE) for training text generation models, which can improve performance and efficiency on various tasks.</p><hr><h3>Survival Instinct in Offline Reinforcement Learning</h3>
<p>Anqi Li, Dipendra Misra, Andrey Kolobov, Ching-An Cheng</p>
<p><a href='https://openreview.net/forum?id=shePL2nbwl'>https://openreview.net/forum?id=shePL2nbwl</a></p>
<p><b>Keywords</b>: Offline RL, safe RL
</p><p><b>Compressor summary</b>: Offline reinforcement learning can produce good and safe policies even with wrong reward labels due to a survival instinct from pessimism and implicit biases in data collection.</p><hr><h3>FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning</h3>
<p>Kun Song, Huimin Ma, Bochao Zou, Huishuai Zhang, Weiran Huang</p>
<p><a href='https://openreview.net/forum?id=shXnfALjuH'>https://openreview.net/forum?id=shXnfALjuH</a></p>
<p><b>Keywords</b>: few-shot learning, CLIP, fine-tuning
</p><p><b>Compressor summary</b>: The paper introduces FD-Align, a method that improves few-shot learning by preserving spurious feature consistency and generalizing better to ID and OOD tasks.</p><hr><h3>Data Market Design through Deep Learning</h3>
<p>Sai Srivatsa Ravindranath, Yanchen Jiang, David C. Parkes</p>
<p><a href='https://openreview.net/forum?id=sgCrNMOuXp'>https://openreview.net/forum?id=sgCrNMOuXp</a></p>
<p><b>Keywords</b>: Data Markets, Information Design, Differentiable Economics, Economics, Deep Learning, Mechanism Design, Algorithmic Game Theory
</p><p><b>Compressor summary</b>: The paper proposes a deep learning approach to design revenue-optimal data markets by learning signaling schemes and handling obedience and incentive constraints.</p><hr><h3>On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling and Beyond</h3>
<p>Thanh Nguyen-Tang, Raman Arora</p>
<p><a href='https://openreview.net/forum?id=sdlh4gVOj8'>https://openreview.net/forum?id=sdlh4gVOj8</a></p>
<p><b>Keywords</b>: reinforcement learning, offline reinforcement learning
</p><p><b>Compressor summary</b>: The paper investigates what makes offline reinforcement learning sample-efficient and unifies three classes of algorithms using a new concept of data diversity.</p><hr><h3>Retrieval-Augmented Multiple Instance Learning</h3>
<p>Yufei CUI, Ziquan Liu, Yixin CHEN, Yuchen Lu, Xinyue Yu, Xue Liu, Tei-Wei Kuo, Miguel R. D. Rodrigues, Chun Jason Xue, Antoni B. Chan</p>
<p><a href='https://openreview.net/forum?id=scaKiAtbI3'>https://openreview.net/forum?id=scaKiAtbI3</a></p>
<p><b>Keywords</b>: Multiple Instance Learning, Whole Slide Imaging, Nearest Neighbor Retrieval
</p><p><b>Compressor summary</b>: The paper introduces RAM-MIL, a framework that uses Optimal Transport to improve MIL performance on out-of-domain data in medical diagnosis tasks based on whole slide images.</p><hr><h3>VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset</h3>
<p>Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, Jing Liu</p>
<p><a href='https://openreview.net/forum?id=scYa9DYUAy'>https://openreview.net/forum?id=scYa9DYUAy</a></p>
<p><b>Keywords</b>: Cross-Modality Foundation Model, Cross-Modality Pretraining Dataset
</p><p><b>Compressor summary</b>: This paper introduces a large dataset (VAST-27M) for multi-modal video captioning and a model (VAST) that can process vision, audio, and subtitle modalities for various tasks.</p><hr><h3>Unleashing the Full Potential of Product Quantization for Large-Scale Image Retrieval</h3>
<p>Yu Liang, Shiliang Zhang, Kenli Li, Xiaoyu Wang</p>
<p><a href='https://openreview.net/forum?id=scG0cwftEe'>https://openreview.net/forum?id=scG0cwftEe</a></p>
<p><b>Keywords</b>: Deep Hash, Image Retrieval, Product Quantization
</p><p><b>Compressor summary</b>: The paragraph introduces a novel deep hashing framework using product quantization that addresses the limitations of current methods on large-scale datasets and achieves better performance.</p><hr><h3>Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing</h3>
<p>Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort</p>
<p><a href='https://openreview.net/forum?id=sbusw6LD41'>https://openreview.net/forum?id=sbusw6LD41</a></p>
<p><b>Keywords</b>: transformers, LLM, softmax, attention, outliers, quantization, post-training quantization
</p><p><b>Compressor summary</b>: The paper proposes two modifications to the attention mechanism in transformer models to reduce outliers and enable efficient quantization without compromising performance.</p><hr><h3>DreamSparse: Escaping from Plato’s Cave with 2D Diffusion Model Given Sparse Views</h3>
<p>Paul Yoo, Jiaxian Guo, Yutaka Matsuo, Shixiang Shane Gu</p>
<p><a href='https://openreview.net/forum?id=sZNBYvunEr'>https://openreview.net/forum?id=sZNBYvunEr</a></p>
<p><b>Keywords</b>: Novel View Synthesis, Diffusion Model
</p><p><b>Compressor summary</b>: DreamSparse is a framework that uses a pre-trained diffusion model to generate high-quality novel view images from sparse views by incorporating 3D geometry information as a prior.</p><hr><h3>DiffPack: A Torsional Diffusion Model for Autoregressive Protein Side-Chain Packing</h3>
<p>Yangtian Zhang, Zuobai Zhang, Bozitao Zhong, Sanchit Misra, Jian Tang</p>
<p><a href='https://openreview.net/forum?id=sXMQPKbLXf'>https://openreview.net/forum?id=sXMQPKbLXf</a></p>
<p><b>Keywords</b>: protein side-chain packing, diffusion models, autoregressive models, geometric deep learning
</p><p><b>Compressor summary</b>: The authors propose DiffPack, a torsional diffusion model that predicts protein side-chain packing by learning joint distributions of side-chain angles and improves accuracy while reducing model size.</p><hr><h3>DISCOVER: Making Vision Networks Interpretable via Competition and Dissection</h3>
<p>Konstantinos P. Panousis, Sotirios Chatzis</p>
<p><a href='https://openreview.net/forum?id=sWNOvNXGLP'>https://openreview.net/forum?id=sWNOvNXGLP</a></p>
<p><b>Keywords</b>: Interpretability, Explainability, Network Dissection, Competitive Networks, Sparsity, Multimodal Models
</p><p><b>Compressor summary</b>: The authors present a method to generate textual descriptions of individual neuron functions in deep vision networks, enabling easier interpretation and exploration of their decision processes.</p><hr><h3>Ordering-based Conditions for Global Convergence of Policy Gradient Methods</h3>
<p>Jincheng Mei, Bo Dai, Alekh Agarwal, Mohammad Ghavamzadeh, Csaba Szepesvari, Dale Schuurmans</p>
<p><a href='https://openreview.net/forum?id=sW8yGZ4uVJ'>https://openreview.net/forum?id=sW8yGZ4uVJ</a></p>
<p><b>Keywords</b>: reinforcement learning, policy gradient, policy optimization, function approximation, global convergence
</p><p><b>Compressor summary</b>: The paper shows that policy gradient methods can converge globally for finite-arm bandits with linear function approximation, depending on properties between the policy update and the representation, without relying heavily on approximation error as a key quantity.</p><hr><h3>Weakly-Supervised Audio-Visual Segmentation</h3>
<p>Shentong Mo, Bhiksha Raj</p>
<p><a href='https://openreview.net/forum?id=sUqG96QqZM'>https://openreview.net/forum?id=sUqG96QqZM</a></p>
<p><b>Keywords</b>: audio-visual learning, visual sound localization, audio-visual segmentation
</p><p><b>Compressor summary</b>: The paragraph describes a novel framework called WS-AVS that simplifies the supervision for audio-visual segmentation by using instance-level annotations and multi-scale contrastive learning, and shows its effectiveness on various scenarios.</p><hr><h3>D-Separation for Causal Self-Explanation</h3>
<p>Wei Liu, Jun Wang, Haozhao Wang, Ruixuan Li, Zhiying Deng, YuanKai Zhang, Yang Qiu</p>
<p><a href='https://openreview.net/forum?id=sUFGPYS25Q'>https://openreview.net/forum?id=sUFGPYS25Q</a></p>
<p><b>Keywords</b>: interpretability, causal inference, rationalization, self-explaining
</p><p><b>Compressor summary</b>: The Minimum Conditional Dependence (MCD) criterion is a novel way to uncover the causal rationale of NLP models by minimizing the dependence between non-selected input parts and the target label, which improves interpretability and performance compared to previous MMI-based methods.</p><hr><h3>Let the Flows Tell:  Solving Graph Combinatorial Problems with GFlowNets</h3>
<p>Dinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron Courville, Yoshua Bengio, Ling Pan</p>
<p><a href='https://openreview.net/forum?id=sTjW3JHs2V'>https://openreview.net/forum?id=sTjW3JHs2V</a></p>
<p><b>Keywords</b>: graph; combinatorial optimization; sampling; gflownets
</p><p><b>Compressor summary</b>: The paper presents a method to train conditional GFlowNets on MDPs for solving combinatorial optimization problems by sampling from the solution space and demonstrates their effectiveness in finding high-quality solutions.</p><hr><h3>Understanding and Addressing the Pitfalls of Bisimulation-based Representations in Offline Reinforcement Learning</h3>
<p>Hongyu Zang, Xin Li, Leiji Zhang, Yang Liu, Baigui Sun, Riashat Islam, Remi Tachet des Combes, Romain Laroche</p>
<p><a href='https://openreview.net/forum?id=sQyRQjun46'>https://openreview.net/forum?id=sQyRQjun46</a></p>
<p><b>Keywords</b>: Bisimulation metrics, Reinforcement Learning, Representation Learning, Offline RL
</p><p><b>Compressor summary</b>: The paper analyzes why bisimulation methods work well in online RL but not in offline RL, and proposes using the expectile operator and reward scaling to improve performance on two benchmarks.</p><hr><h3>Cocktail: Mixing Multi-Modality Control for Text-Conditional Image Generation</h3>
<p>Minghui Hu, Jianbin Zheng, Daqing Liu, Chuanxia Zheng, Chaoyue Wang, Dacheng Tao, Tat-Jen Cham</p>
<p><a href='https://openreview.net/forum?id=sQBGVw5qH9'>https://openreview.net/forum?id=sQBGVw5qH9</a></p>
<p><b>Keywords</b>: Multi-modality, Image Generation, Diffusion
</p><p><b>Compressor summary</b>: Cocktail is a pipeline that mixes different modalities into one embedding and uses a hyper-network called gControlNet to align and infuse control signals from various sources into a pre-trained diffusion model, resulting in high-quality image synthesis with diverse contents.</p><hr><h3>A Measure-Theoretic Axiomatisation of Causality</h3>
<p>Junhyung Park, Simon Buchholz, Bernhard Schölkopf, Krikamol Muandet</p>
<p><a href='https://openreview.net/forum?id=sPLTQSf6GI'>https://openreview.net/forum?id=sPLTQSf6GI</a></p>
<p><b>Keywords</b>: Causality, probability theory, causal models
</p><p><b>Compressor summary</b>: The text introduces a new concept of causal space that combines probability theory with causality to overcome some limitations of current approaches.</p><hr><h3>Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series</h3>
<p>Yihe Wang, Yu Han, Haishuai Wang, Xiang Zhang</p>
<p><a href='https://openreview.net/forum?id=sOQBHlCmzp'>https://openreview.net/forum?id=sOQBHlCmzp</a></p>
<p><b>Keywords</b>: Deep Learning, Contrastive Learning, Self-supervised Learning, Time Series, Healthcare
</p><p><b>Compressor summary</b>: COMET is a hierarchical framework that leverages data consistencies at multiple levels in medical time series to improve contrastive representation learning and outperforms existing methods on diverse datasets.</p><hr><h3>Projection-Free Online Convex Optimization via Efficient Newton Iterations</h3>
<p>Khashayar Gatmiry, Zakaria Mhammedi</p>
<p><a href='https://openreview.net/forum?id=sOOg1xJADA'>https://openreview.net/forum?id=sOOg1xJADA</a></p>
<p><b>Keywords</b>: Online Learning, Online convex optimization, projection-free, Newton method
</p><p><b>Compressor summary</b>: The paper proposes a new online convex optimization algorithm that uses self-concordant barriers to avoid projections and achieve low regret without computing matrix inverses often.</p><hr><h3>Error Bounds for Learning with Vector-Valued Random Features</h3>
<p>Samuel Lanthaler, Nicholas H. Nelsen</p>
<p><a href='https://openreview.net/forum?id=sLr1sohnmo'>https://openreview.net/forum?id=sLr1sohnmo</a></p>
<p><b>Keywords</b>: random features, random feature model, operator learning, vector-valued
</p><p><b>Compressor summary</b>: The paper analyzes vector-valued random features ridge regression in an infinite-dimensional setting, providing strong consistency and minimax optimal convergence rates without relying on random matrix theory.</p><hr><h3>On skip connections and normalisation layers in deep optimisation</h3>
<p>Lachlan Ewen MacDonald, Jack Valmadre, Hemanth Saratchandran, Simon Lucey</p>
<p><a href='https://openreview.net/forum?id=sLhXMkI0kx'>https://openreview.net/forum?id=sLhXMkI0kx</a></p>
<p><b>Keywords</b>: optimisation, optimization, skip, connection, normalisation, normalization, deep, learning, polyak, lojasiewicz, lipschitz
</p><p><b>Compressor summary</b>: The paper presents a theory for gradient optimisation of deep neural networks that explains the roles of normalisation layers and skip connections, and shows how they enable training to global optima and acceleration of training.</p><hr><h3>ELDEN: Exploration via Local Dependencies</h3>
<p>Zizhao Wang, Jiaheng Hu, Peter Stone, Roberto Martín-Martín</p>
<p><a href='https://openreview.net/forum?id=sL4pJBXkxu'>https://openreview.net/forum?id=sL4pJBXkxu</a></p>
<p><b>Keywords</b>: reinforcement learning; intrinsic motivation; exploration
</p><p><b>Compressor summary</b>: ELDEN is a novel intrinsic reward function that encourages reinforcement learning agents to explore environments with complex dependencies by exploiting uncertainty in the learned dynamics.</p><hr><h3>Cross-links Matter for Link Prediction: Rethinking the Debiased GNN from a Data Perspective</h3>
<p>Zihan Luo, Hong Huang, Jianxun Lian, Xiran Song, Xing Xie, Hai Jin</p>
<p><a href='https://openreview.net/forum?id=sJDkwMVqb9'>https://openreview.net/forum?id=sJDkwMVqb9</a></p>
<p><b>Keywords</b>: Cross-links, Debias, Graph Neural Networks, Link Prediction
</p><p><b>Compressor summary</b>: The paper proposes a twin-structure framework to reduce data bias between internal-links and cross-links in GNN-based link prediction, enhancing both bias mitigation and utility.</p><hr><h3>VOCE: Variational Optimization with Conservative Estimation for Offline Safe Reinforcement Learning</h3>
<p>Jiayi Guan, Guang Chen, Jiaming Ji, Long Yang, Ao Zhou, Zhijun Li, changjun jiang</p>
<p><a href='https://openreview.net/forum?id=sIU3WujeSl'>https://openreview.net/forum?id=sIU3WujeSl</a></p>
<p><b>Keywords</b>: Offline safe reinforcement learning, Pessimistic conservative estimation, Variational optimization, Reinforcement Learning
</p><p><b>Compressor summary</b>: VOCE is a new offline safe RL algorithm that uses probabilistic inference and pessimistic estimation to learn policies that satisfy safety constraints while optimizing rewards.</p><hr><h3>Q-DM: An Efficient Low-bit Quantized Diffusion Model</h3>
<p>Yanjing Li, Sheng Xu, Xianbin Cao, Xiao Sun, Baochang Zhang</p>
<p><a href='https://openreview.net/forum?id=sFGkL5BsPi'>https://openreview.net/forum?id=sFGkL5BsPi</a></p>
<p><b>Keywords</b>: network quantization, diffusion model, image synthesize
</p><p><b>Compressor summary</b>: The paper proposes TaQ and NeM methods to improve low-bit quantized diffusion models, reducing computation and memory costs while maintaining high-quality data generation.</p><hr><h3>SutraNets: Sub-series Autoregressive Networks for Long-Sequence, Probabilistic Forecasting</h3>
<p>Shane Bergsma, Tim Zeyl, Lei Guo</p>
<p><a href='https://openreview.net/forum?id=sC4RbbVKbu'>https://openreview.net/forum?id=sC4RbbVKbu</a></p>
<p><b>Keywords</b>: time series, probabilistic forecasting, autoregressive generative models, neural networks
</p><p><b>Compressor summary</b>: SutraNets are a new method for predicting long time series by factoring likelihood into conditional probabilities and generating multivariate outputs to reduce error accumulation.</p><hr><h3>Doubly Robust Augmented Transfer for Meta-Reinforcement Learning</h3>
<p>Yuankun Jiang, Nuowen Kan, Chenglin Li, Wenrui Dai, Junni Zou, Hongkai Xiong</p>
<p><a href='https://openreview.net/forum?id=sABYNWKcwK'>https://openreview.net/forum?id=sABYNWKcwK</a></p>
<p><b>Keywords</b>: Meta-reinforcement learning, doubly robust (DR), sample transfer
</p><p><b>Compressor summary</b>: The paper proposes a new Meta-RL method called DRaT that addresses dynamics and reward function variations across tasks by using a doubly robust estimator to transfer informative samples from other tasks.</p><hr><h3>RH-BrainFS: Regional Heterogeneous Multimodal Brain Networks Fusion Strategy</h3>
<p>Hongting Ye, Yalu Zheng, Yueying Li, Ke Zhang, Youyong Kong, Yonggui Yuan</p>
<p><a href='https://openreview.net/forum?id=s97ezbqoDZ'>https://openreview.net/forum?id=s97ezbqoDZ</a></p>
<p><b>Keywords</b>: Multimodal, Neuroscience, Subgraph, Transformer
</p><p><b>Compressor summary</b>: The authors present RH-BrainFS, a novel multimodal brain network fusion strategy that addresses regional heterogeneity between structural and functional connectivity, and shows its effectiveness in various neuroscience tasks.</p><hr><h3>AND: Adversarial Neural Degradation for Learning Blind Image Super-Resolution</h3>
<p>Fangzhou Luo, Xiaolin Wu, Yanhui Guo</p>
<p><a href='https://openreview.net/forum?id=s8QsYV1VZ2'>https://openreview.net/forum?id=s8QsYV1VZ2</a></p>
<p><b>Keywords</b>: Blind Image Super-Resolution
</p><p><b>Compressor summary</b>: The paper proposes an adversarial neural degradation (AND) model that improves image super-resolution by generating a wide range of complex degradation effects without explicit supervision, resulting in better generalization and performance on real-world images.</p><hr><h3>Soft-Unification in Deep Probabilistic Logic</h3>
<p>Jaron Maene, Luc De Raedt</p>
<p><a href='https://openreview.net/forum?id=s86M8naPSv'>https://openreview.net/forum?id=s86M8naPSv</a></p>
<p><b>Keywords</b>: neuro-symbolic AI, probabilistic logic, embeddings
</p><p><b>Compressor summary</b>: The paragraph discusses soft-unification, a technique for combining logic and neural concepts in AI, and proposes a new framework called DeepSoftLog that improves upon previous systems by introducing probabilistic semantics and achieving better performance on neuro-symbolic benchmarks.</p><hr><h3>Don’t Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner</h3>
<p>Zhengxiang Shi, Aldo Lipani</p>
<p><a href='https://openreview.net/forum?id=s7xWeJQACI'>https://openreview.net/forum?id=s7xWeJQACI</a></p>
<p><b>Keywords</b>: Continued Pre-training, Prompt-based Fine-tuning, Language Models
</p><p><b>Compressor summary</b>: The study proposes Prompt-based Continued Pre-training (PCP), which improves prompt-based fine-tuning in natural language processing tasks by combining instruction tuning with conventional continued pre-training using unsupervised objectives.</p><hr><h3>On Certified Generalization in Structured Prediction</h3>
<p>Bastian Boll, Christoph Schnoerr</p>
<p><a href='https://openreview.net/forum?id=s1jQ91yFAb'>https://openreview.net/forum?id=s1jQ91yFAb</a></p>
<p><b>Keywords</b>: Structured Prediction, PAC-Bayes, Concentration Inequalities, Statistical Learning Theory, Knothe-Rosenblatt Rearrangement
</p><p><b>Compressor summary</b>: The paper proposes a new way to measure how well structured prediction models generalize using generative models and Wasserstein dependency matrices.</p><hr><h3>Focused Transformer: Contrastive Training for Context Scaling</h3>
<p>Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Miłoś</p>
<p><a href='https://openreview.net/forum?id=s1FjXzJ0jy'>https://openreview.net/forum?id=s1FjXzJ0jy</a></p>
<p><b>Keywords</b>: Transformers, Language Models, Natural Language Processing
</p><p><b>Compressor summary</b>: The Focused Transformer (FoT) technique addresses the distraction issue in large language models by enhancing the structure of the (key, value) space and extending the effective context length through contrastive learning.</p><hr><h3>GALOPA: Graph Transport Learning with Optimal Plan Alignment</h3>
<p>Yejiang Wang, Yuhai Zhao, Daniel Zhengkui Wang, Ling Li</p>
<p><a href='https://openreview.net/forum?id=rzlqOVExUA'>https://openreview.net/forum?id=rzlqOVExUA</a></p>
<p><b>Keywords</b>: graph neural network; self-supervised learning; optimal transport;
</p><p><b>Compressor summary</b>: The paper proposes a self-supervised graph learning method that preserves both structural and matching information between graphs using optimal transport plans, and shows its advantages over existing methods in different scenarios.</p><hr><h3>Private Federated Frequency Estimation: Adapting to the Hardness of the Instance</h3>
<p>Jingfeng Wu, Wennan Zhu, Peter Kairouz, Vladimir Braverman</p>
<p><a href='https://openreview.net/forum?id=rzDBoh1tBh'>https://openreview.net/forum?id=rzDBoh1tBh</a></p>
<p><b>Keywords</b>: sketch, federated analytics, privacy
</p><p><b>Compressor summary</b>: The paper proposes an improved sketch algorithm for federated frequency estimation with multiple communication rounds, and a two-phase approach that adapts the sketch size based on problem complexity and ensures differential privacy.</p><hr><h3>EgoEnv: Human-centric environment representations from egocentric video</h3>
<p>Tushar Nagarajan, Santhosh Kumar Ramakrishnan, Ruta Desai, James Hillis, Kristen Grauman</p>
<p><a href='https://openreview.net/forum?id=rybsHQ4DXy'>https://openreview.net/forum?id=rybsHQ4DXy</a></p>
<p><b>Keywords</b>: egocentric video, 3D environment, sim2real, sim-to-real, episodic memory
</p><p><b>Compressor summary</b>: The paper proposes an approach to understand human activities in their environment using egocentric video and 3D simulation, and shows its effectiveness on real-world videos.</p><hr><h3>Matrix Compression via Randomized Low Rank and Low Precision Factorization</h3>
<p>Rajarshi Saha, Varun Srivastava, Mert Pilanci</p>
<p><a href='https://openreview.net/forum?id=rxsCTtkqA9'>https://openreview.net/forum?id=rxsCTtkqA9</a></p>
<p><b>Keywords</b>: Matrix compression, Randomized low rank factorization, Randomized SVD, Sketching, Quantized embeddings, Random matrices
</p><p><b>Compressor summary</b>: The paper proposes an algorithm to decompose large matrices into low rank factors and quantize them for efficient storage and processing.</p><hr><h3>Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors</h3>
<p>Paul Steven Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Cohen Ethan, Aidan James Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth Norman, Tanishq Mathew Abraham</p>
<p><a href='https://openreview.net/forum?id=rwrblCYb2A'>https://openreview.net/forum?id=rwrblCYb2A</a></p>
<p><b>Keywords</b>: fMRI, computational neuroscience, mind reading, diffusion models
</p><p><b>Compressor summary</b>: MindEye is a novel method to reconstruct and retrieve images from brain activity using fMRI and specialized submodules for retrieval and reconstruction.</p><hr><h3>Break It Down:  Evidence for Structural Compositionality in Neural Networks</h3>
<p>Michael A. Lepori, Thomas Serre, Ellie Pavlick</p>
<p><a href='https://openreview.net/forum?id=rwbzMiuFQl'>https://openreview.net/forum?id=rwbzMiuFQl</a></p>
<p><b>Keywords</b>: Deep Learning, Compositionality, Cognitive Science
</p><p><b>Compressor summary</b>: The authors investigate whether neural networks use modular subnetworks or template matching to solve complex tasks, and find evidence that they can learn structural compositionality without relying on symbolic mechanisms.</p><hr><h3>DäRF: Boosting Radiance Fields from Sparse Input Views with Monocular Depth Adaptation</h3>
<p>Jiuhn Song, Seonghoon Park, Honggyu An, Seokju Cho, Min-Seop Kwak, Sungjin Cho, Seungryong Kim</p>
<p><a href='https://openreview.net/forum?id=rsrfEIdawr'>https://openreview.net/forum?id=rsrfEIdawr</a></p>
<p><b>Keywords</b>: Neural Radiance Fields, 3D Reconstruction, Few-shot NeRF, Monocular Priors
</p><p><b>Compressor summary</b>: The DäRF framework combines NeRF and monocular depth estimation to improve novel view synthesis and 3D geometry reconstruction with a small number of real-world images.</p><hr><h3>PointGPT: Auto-regressively Generative Pre-training from Point Clouds</h3>
<p>Guangyan Chen, Meiling Wang, Yi Yang, Kai Yu, Li Yuan, Yufeng Yue</p>
<p><a href='https://openreview.net/forum?id=rqE0fEQDqs'>https://openreview.net/forum?id=rqE0fEQDqs</a></p>
<p><b>Keywords</b>: Generative Pre-training Transformer; GPT; Auto-regressively Generative Pre-training; Self-supervised Learning; Point clouds
</p><p><b>Compressor summary</b>: PointGPT extends the GPT language model to point clouds, using a novel auto-regressive generation task and a dual masking strategy, achieving state-of-the-art results on various downstream tasks.</p><hr><h3>On the Role of Noise in the Sample Complexity of Learning Recurrent Neural Networks: Exponential Gaps for Long Sequences</h3>
<p>Alireza Fathollah Pour, Hassan Ashtiani</p>
<p><a href='https://openreview.net/forum?id=rpuEARqB54'>https://openreview.net/forum?id=rpuEARqB54</a></p>
<p><b>Keywords</b>: PAC Learning, Recurrent Neural Networks, Noise, Sample Complexity
</p><p><b>Compressor summary</b>: The paper studies how adding noise to multi-layered sigmoid networks affects their sample complexity for classifying sequences, and shows an exponential gap between noisy and non-noisy networks.</p><hr><h3>ConRad: Image Constrained Radiance Fields for 3D Generation from a Single Image</h3>
<p>Senthil Purushwalkam, Nikhil Naik</p>
<p><a href='https://openreview.net/forum?id=roGYQvarnC'>https://openreview.net/forum?id=roGYQvarnC</a></p>
<p><b>Keywords</b>: 3D, generation, diffusion, viewpoint
</p><p><b>Compressor summary</b>: The paper introduces ConRad, a novel method that uses pretrained image generation models to reconstruct 3D objects from a single RGB image, capturing the appearance and preserving details while producing realistic 3D reconstructions.</p><hr><h3>AdaPlanner: Adaptive Planning from Feedback with Language Models</h3>
<p>Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang</p>
<p><a href='https://openreview.net/forum?id=rnKgbKmelt'>https://openreview.net/forum?id=rnKgbKmelt</a></p>
<p><b>Keywords</b>: Large language models, decision making, closed-loop planning
</p><p><b>Compressor summary</b>: AdaPlanner is a closed-loop approach that allows large language models to adaptively refine their plans in response to environmental feedback and outperforms state-of-the-art methods with fewer samples.</p><hr><h3>Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off</h3>
<p>Zichen Zhang, Johannes Kirschner, Junxi Zhang, Francesco Zanini, Alex Ayoub, Masood Dehghan, Dale Schuurmans</p>
<p><a href='https://openreview.net/forum?id=rmQgQCZWiP'>https://openreview.net/forum?id=rmQgQCZWiP</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Policy Evaluation, Temporal Discretization, Continuous Time, LQR
</p><p><b>Compressor summary</b>: The paper analyzes how time discretization affects Monte-Carlo policy evaluation for LQR systems and finds a trade-off between approximation and statistical error, leading to an optimal choice of temporal resolution for data efficiency.</p><hr><h3>False Discovery Proportion control for aggregated Knockoffs</h3>
<p>Alexandre Blain, Bertrand Thirion, Olivier Grisel, Pierre Neuvial</p>
<p><a href='https://openreview.net/forum?id=rlPUJ60bwM'>https://openreview.net/forum?id=rlPUJ60bwM</a></p>
<p><b>Keywords</b>: Knockoffs, Derandomization of Knockoffs, False Discoveries Proportion control, Controlled variable selection, Statistical inference, High-dimensional inference
</p><p><b>Compressor summary</b>: KOPI is a new method that improves Knockoff-based inference by controlling the actual proportion of false discoveries and using a novel aggregation technique.</p><hr><h3>Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks</h3>
<p>Andong Wang, Chao Li, Mingyuan Bai, Zhong Jin, Guoxu Zhou, Qibin Zhao</p>
<p><a href='https://openreview.net/forum?id=rih3hsSWx8'>https://openreview.net/forum?id=rih3hsSWx8</a></p>
<p><b>Keywords</b>: Tensor SVD; Tensor Neural Networks; Transformed Low-rankness; Adversarial Generalization; Implicit Bias.
</p><p><b>Compressor summary</b>: The paper analyzes the theoretical generalization error of neural networks with t-product layers (t-NNs) and shows that transformed low-rank parameterization can improve their adversarial robustness.</p><hr><h3>DreamHuman: Animatable 3D Avatars from Text</h3>
<p>Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Fieraru, Cristian Sminchisescu</p>
<p><a href='https://openreview.net/forum?id=rheCTpRrxI'>https://openreview.net/forum?id=rheCTpRrxI</a></p>
<p><b>Keywords</b>: text to 3d; 3d avatars
</p><p><b>Compressor summary</b>: DreamHuman is a method that creates realistic 3D human avatars from text descriptions by combining different models and optimization techniques, resulting in diverse and high-quality animatable models.</p><hr><h3>Counterfactual Evaluation of Peer-Review Assignment Policies</h3>
<p>Martin Saveski, Steven Jecmen, Nihar B Shah, Johan Ugander</p>
<p><a href='https://openreview.net/forum?id=rhIfzCZoXG'>https://openreview.net/forum?id=rhIfzCZoXG</a></p>
<p><b>Keywords</b>: peer review, causal inference, counterfactual policy evaluation
</p><p><b>Compressor summary</b>: This paper proposes and applies novel methods to evaluate how changes in peer-review assignment algorithms affect review quality using randomness and data from two computer science venues.</p><hr><h3>Policy Optimization for Continuous Reinforcement Learning</h3>
<p>Hanyang Zhao, Wenpin Tang, David Yao</p>
<p><a href='https://openreview.net/forum?id=rfcak9EV99'>https://openreview.net/forum?id=rfcak9EV99</a></p>
<p><b>Keywords</b>: exploratory stochastic control, occupation time, performance difference, policy optimization
</p><p><b>Compressor summary</b>: The authors develop a new concept called occupation time for continuous reinforcement learning and show how it can improve performance and optimization methods in this setting.</p><hr><h3>Parallel Spiking Neurons with High Efficiency and Ability to Learn Long-term Dependencies</h3>
<p>Wei Fang, Zhaofei Yu, Zhaokun Zhou, Ding Chen, Yanqi Chen, Zhengyu Ma, Timothée Masquelier, Yonghong Tian</p>
<p><a href='https://openreview.net/forum?id=rfTFJvTkr2'>https://openreview.net/forum?id=rfTFJvTkr2</a></p>
<p><b>Keywords</b>: Spiking Neural Network, SNN, deep learning, spiking neuron, neuromorphic computing
</p><p><b>Compressor summary</b>: The authors propose Parallel Spiking Neurons, which improve simulation speed and accuracy in spiking neural networks by reformulating neuronal dynamics without reset and enabling parallel processing of inputs.</p><hr><h3>Why think step by step? Reasoning emerges from the locality of experience</h3>
<p>Ben Prystawski, Michael Y. Li, Noah Goodman</p>
<p><a href='https://openreview.net/forum?id=rcXXNFVlEn'>https://openreview.net/forum?id=rcXXNFVlEn</a></p>
<p><b>Keywords</b>: chain-of-thought; language models; reasoning
</p><p><b>Compressor summary</b>: The text discusses how chain-of-thought reasoning helps language models make better inferences when trained on locally structured data with strong variable dependencies, reducing bias and improving efficiency.</p><hr><h3>Adaptive Test-Time Personalization for Federated Learning</h3>
<p>Wenxuan Bao, Tianxin Wei, Haohan Wang, Jingrui He</p>
<p><a href='https://openreview.net/forum?id=rbw9xCU6Ci'>https://openreview.net/forum?id=rbw9xCU6Ci</a></p>
<p><b>Keywords</b>: federated learning, personalized federated learning, test-time adaptation
</p><p><b>Compressor summary</b>: The paper introduces ATP, a novel test-time personalized federated learning algorithm that adapts global models in an unsupervised way without labeled data from multiple clients with different distributions, achieving strong generalization and outperforming existing methods.</p><hr><h3>Swarm Reinforcement Learning for Adaptive Mesh Refinement</h3>
<p>Niklas Freymuth, Philipp Dahlinger, Tobias Daniel Würth, Simon Reisch, Luise Kärger, Gerhard Neumann</p>
<p><a href='https://openreview.net/forum?id=rZqRu8e4uc'>https://openreview.net/forum?id=rZqRu8e4uc</a></p>
<p><b>Keywords</b>: Adaptive Mesh Refinement, Finite Element Method, Swarm Reinforcement Learning, Graph Neural Networks
</p><p><b>Compressor summary</b>: The paper proposes ASMR, a novel Adaptive Swarm Markov Decision Process for adaptive mesh refinement in engineering simulations, which achieves significant speedup and quality improvements compared to existing methods.</p><hr><h3>On the impact of activation  and normalization in obtaining  isometric embeddings at initialization</h3>
<p>Amir Joudaki, Hadi Daneshmand, Francis Bach</p>
<p><a href='https://openreview.net/forum?id=rY4sA9qYKy'>https://openreview.net/forum?id=rY4sA9qYKy</a></p>
<p><b>Keywords</b>: dynamical isometry, Lyapunov analysis, random neural networks
</p><p><b>Compressor summary</b>: In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results do not extend to layer normalization, which is widely used in transformers, and can not quantitatively characterize the role of non-linear activations.  To bridge this gap, we prove that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards the identity matrix at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function.</p><hr><h3>Wasserstein Gradient Flows for Optimizing Gaussian Mixture Policies</h3>
<p>Hanna Ziesche, Leonel Rozo</p>
<p><a href='https://openreview.net/forum?id=rW4mNcDxpS'>https://openreview.net/forum?id=rW4mNcDxpS</a></p>
<p><b>Keywords</b>: Policy optimization, robot learning, reinforcement learning, Gaussian mixture models, optimal transport, robotics
</p><p><b>Compressor summary</b>: The paper presents a novel policy optimization method for robots based on optimal transport and Riemannian optimization of GMMs, which enhances performance in different robot tasks.</p><hr><h3>PRED: Pre-training via Semantic Rendering on LiDAR Point Clouds</h3>
<p>Hao Yang, Haiyang Wang, Di Dai, Liwei Wang</p>
<p><a href='https://openreview.net/forum?id=rUldfB4SPT'>https://openreview.net/forum?id=rUldfB4SPT</a></p>
<p><b>Keywords</b>: Pre-Train, Autonomous Driving, LiDAR, 3D Object Detection
</p><p><b>Compressor summary</b>: The paper proposes a new method, PRED, that uses images and neural rendering to help train point cloud encoders for 3D perception tasks in autonomous driving, addressing the challenges of incompleteness and occlusions.</p><hr><h3>Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search</h3>
<p>Indradyumna Roy, Rishi Agarwal, Soumen Chakrabarti, Anirban Dasgupta, Abir De</p>
<p><a href='https://openreview.net/forum?id=rUf0GV5CuU'>https://openreview.net/forum?id=rUf0GV5CuU</a></p>
<p><b>Keywords</b>: Locality sensitive hashing, Fourier transform, Order embeddings
</p><p><b>Compressor summary</b>: The paper proposes FourierHashNet, a data-sensitive and trainable index for fast retrieval of relevant documents using hinge distance transformed into dominance similarity in the frequency domain.</p><hr><h3>Homotopy-based training of NeuralODEs for accurate dynamics discovery</h3>
<p>Joon-Hyuk Ko, Hankyul Koh, Nojun Park, Wonho Jhe</p>
<p><a href='https://openreview.net/forum?id=rUFckPrzXR'>https://openreview.net/forum?id=rUFckPrzXR</a></p>
<p><b>Keywords</b>: neural ordinary differential equations, synchronization, homotopy optimization, loss landscape, dynamical systems
</p><p><b>Compressor summary</b>: This paper introduces a new training method for NeuralODEs that improves their performance and speed by using synchronization and homotopy optimization without changing the model architecture.</p><hr><h3>Efficient Learning of Linear Graph Neural Networks via Node Subsampling</h3>
<p>Seiyun Shin, Ilan Shomorony, Han Zhao</p>
<p><a href='https://openreview.net/forum?id=rQI3FOzo1f'>https://openreview.net/forum?id=rQI3FOzo1f</a></p>
<p><b>Keywords</b>: Graph neural networks, Random sampling, Regression
</p><p><b>Compressor summary</b>: Graph Neural Networks (GNNs) aim to learn from large graphs, but their computational cost can be high; this paper proposes an efficient training method for a specific type of GNN by subsampling nodes and using leverage score sampling.</p><hr><h3>Continuous-time Analysis of Anchor Acceleration</h3>
<p>Jaewook J. Suh, Jisun Park, Ernest K. Ryu</p>
<p><a href='https://openreview.net/forum?id=rN99gLCBe4'>https://openreview.net/forum?id=rN99gLCBe4</a></p>
<p><b>Keywords</b>: acceleration, convex optimization, continuous-time analysis, monotone operator, monotone inclusion, minimax optimization, fixed-point problem, anchor acceleration
</p><p><b>Compressor summary</b>: This paper analyzes the continuous-time models of anchor acceleration, a minimization technique with an unclear mechanism, and proposes an adaptive method based on the findings.</p><hr><h3>Preconditioning Matters: Fast Global Convergence of Non-convex Matrix Factorization via Scaled Gradient Descent</h3>
<p>Xixi Jia, Hailin Wang, Jiangjun Peng, Xiangchu Feng, Deyu Meng</p>
<p><a href='https://openreview.net/forum?id=rLpLjCBW4J'>https://openreview.net/forum?id=rLpLjCBW4J</a></p>
<p><b>Keywords</b>: Non-convex optimization, matrix factorization, low rank, scaled gradient descent
</p><p><b>Compressor summary</b>: The paper proposes two preconditioned gradient descent methods for low-rank matrix factorization that converge faster and do not require small learning rates or initializations.</p><hr><h3>ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image Collections</h3>
<p>Chun-Han Yao, Amit Raj, Wei-Chih Hung, Michael Rubinstein, Yuanzhen Li, Ming-Hsuan Yang, Varun Jampani</p>
<p><a href='https://openreview.net/forum?id=rJc5Lsn5QU'>https://openreview.net/forum?id=rJc5Lsn5QU</a></p>
<p><b>Keywords</b>: 3D articulated shape, animal body estimation, diffusion for 3D
</p><p><b>Compressor summary</b>: ARTIC3D is a self-supervised framework that reconstructs 3D shapes from sparse images using skeleton-based surface representation and 2D diffusion priors, achieving high-quality results even with noisy and occluded inputs.</p><hr><h3>Adversarial Counterfactual Environment Model Learning</h3>
<p>Xiong-Hui Chen, Yang Yu, Zhengmao Zhu, ZhiHua Yu, Chen Zhenjun, Chenghe Wang, Yinan Wu, Rong-Jun Qin, Hongqiu Wu, Ruijin Ding, Huang Fangsheng</p>
<p><a href='https://openreview.net/forum?id=rHAX0LRwk8'>https://openreview.net/forum?id=rHAX0LRwk8</a></p>
<p><b>Keywords</b>: environment model learning, offline reinforcement learning, off-policy evaluation, individual treatment effects estimation, causal inference, adversarial learning
</p><p><b>Compressor summary</b>: The paper proposes a new model-learning objective called AWRM to address the selection bias of behavior policies in environment dynamics models and shows that it improves counterfactual prediction and downstream tasks using the GALILEO algorithm.</p><hr><h3>Formalizing locality for normative synaptic plasticity models</h3>
<p>Colin Bredenberg, Ezekiel Williams, Cristina Savin, Blake Aaron Richards, Guillaume Lajoie</p>
<p><a href='https://openreview.net/forum?id=rGN3X9jnEg'>https://openreview.net/forum?id=rGN3X9jnEg</a></p>
<p><b>Keywords</b>: synaptic plasticity, computational neuroscience
</p><p><b>Compressor summary</b>: The authors propose formal definitions of locality for machine learning algorithms that aim to be biologically plausible, in order to make clear what quantities cannot be included in a learning rule and test their predictions against them.</p><hr><h3>FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout</h3>
<p>Irene Wang, Prashant J. Nair, Divya Mahajan</p>
<p><a href='https://openreview.net/forum?id=rG1M3kOVba'>https://openreview.net/forum?id=rG1M3kOVba</a></p>
<p><b>Keywords</b>: Federated Learning
</p><p><b>Compressor summary</b>: FLuID is a method to balance the training load in Federated Learning by creating sub-models for low-performance devices without sacrificing accuracy.</p><hr><h3>Learning better with Dale’s Law: A Spectral Perspective</h3>
<p>Pingsheng Li, Jonathan Cornford, Arna Ghosh, Blake Aaron Richards</p>
<p><a href='https://openreview.net/forum?id=rDiMgZulwi'>https://openreview.net/forum?id=rDiMgZulwi</a></p>
<p><b>Keywords</b>: Dale's Law, RNNs, brain-inspired neural networks, DANNs, computational neuroscience, spectral properties, inhibition
</p><p><b>Compressor summary</b>: The authors extend Dale's ANNs to RNNs, showing that good performance is possible while respecting Dale's Law, which states that neurons must be excitatory or inhibitory. They also find that the spectral properties of the recurrent weight matrix at initialisation are more impactful on network performance than sign constraints.</p><hr><h3>Learning Invariant Molecular Representation in Latent Discrete Space</h3>
<p>Xiang Zhuang, Qiang Zhang, Keyan Ding, Yatao Bian, Xiao Wang, Jingsong Lv, Hongyang Chen, Huajun Chen</p>
<p><a href='https://openreview.net/forum?id=r9fzp8eyhZ'>https://openreview.net/forum?id=r9fzp8eyhZ</a></p>
<p><b>Keywords</b>: molecular representation learning, out-of-distribution
</p><p><b>Compressor summary</b>: The authors propose a new molecular representation learning framework that improves generalization and robustness against distribution shifts, using a novel strategy of first encoding and then separating features in the latent space.</p><hr><h3>Learning to Group Auxiliary Datasets for Molecule</h3>
<p>Tinglin Huang, Ziniu Hu, Zhitao Ying</p>
<p><a href='https://openreview.net/forum?id=r9eZH6WNm2'>https://openreview.net/forum?id=r9eZH6WNm2</a></p>
<p><b>Keywords</b>: molecule, routing mechanism, meta gradient
</p><p><b>Compressor summary</b>: MolGroup is a method that uses graph structure similarity and task similarity to select the best auxiliary datasets for small molecule machine learning models, improving their performance.</p><hr><h3>Grounding Neural Inference with Satisfiability Modulo Theories</h3>
<p>Zifan Wang, Saranya Vijayakumar, Kaiji Lu, Vijay Ganesh, Somesh Jha, Matt Fredrikson</p>
<p><a href='https://openreview.net/forum?id=r8snfquzs3'>https://openreview.net/forum?id=r8snfquzs3</a></p>
<p><b>Keywords</b>: Satisfiability Modulo Theories, Solver Layer, Combinatorial Problem, MAXSAT, SAT
</p><p><b>Compressor summary</b>: The paper proposes SMTLayer, a technique that integrates SMT solvers into DNNs to encode domain knowledge as mathematical formulas, enabling more efficient, robust, and interpretable learning.</p><hr><h3>TexQ: Zero-shot Network Quantization with Texture Feature Distribution Calibration</h3>
<p>Xinrui Chen, Yizhi Wang, Renao Yan, Yiqing Liu, Tian Guan, Yonghong He</p>
<p><a href='https://openreview.net/forum?id=r8LYNleLf9'>https://openreview.net/forum?id=r8LYNleLf9</a></p>
<p><b>Keywords</b>: Zero-shot quantization, Texture feature calibration, Post-training quantization, low bit width, Neural network compression
</p><p><b>Compressor summary</b>: The paper proposes TexQ, a novel zero-shot quantization method for neural networks that synthesizes calibration images and uses mixup knowledge distillation to generate diverse samples, improving performance in ultra-low bit width quantization.</p><hr><h3>Learning Adaptive Tensorial Density Fields for Clean Cryo-ET Reconstruction</h3>
<p>YUANHAO WANG, Ramzi Idoughi, Wolfgang Heidrich</p>
<p><a href='https://openreview.net/forum?id=r7g9nFsulw'>https://openreview.net/forum?id=r7g9nFsulw</a></p>
<p><b>Keywords</b>: Neural density fields, Coordinate-based representations, Quadtree structure, Cryo-electron microscope
</p><p><b>Compressor summary</b>: The authors propose a new learning-based method for improving 3D structure reconstruction from cryo-ET data, which addresses challenges such as missing data and high noise levels by using an adaptive tensor representation and a novel loss function.</p><hr><h3>Meta-Learning Adversarial Bandit Algorithms</h3>
<p>Mikhail Khodak, Ilya Osadchiy, Keegan Harris, Nina Balcan, Kfir Yehuda Levy, Ron Meir, Steven Wu</p>
<p><a href='https://openreview.net/forum?id=r6xGZ0XL2g'>https://openreview.net/forum?id=r6xGZ0XL2g</a></p>
<p><b>Keywords</b>: online learning, multi-armed bandits, meta-learning, multi-task learning, bandit linear optimization
</p><p><b>Compressor summary</b>: The authors propose meta-algorithms that improve performance across multiple similar tasks using bandit feedback, by tuning hyperparameters of inner learners in online-within-online settings for MAB and BLO.</p><hr><h3>Global Update Tracking: A Decentralized Learning Algorithm for Heterogeneous Data</h3>
<p>Sai Aparna Aketi, Abolfazl Hashemi, Kaushik Roy</p>
<p><a href='https://openreview.net/forum?id=qyixBZl8Ph'>https://openreview.net/forum?id=qyixBZl8Ph</a></p>
<p><b>Keywords</b>: Federated Learning, Decentralized Learning, Non-IID Data, Heterogeneous data distribution, Peer-to-peer connectivity
</p><p><b>Compressor summary</b>: The paper proposes Global Update Tracking (GUT), a decentralized learning algorithm that reduces the impact of heterogeneous data distribution and achieves state-of-the-art performance on various Computer Vision datasets.</p><hr><h3>Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information</h3>
<p>Arman Zharmagambetov, Brandon Amos, Aaron M Ferber, Taoan Huang, Bistra Dilkina, Yuandong Tian</p>
<p><a href='https://openreview.net/forum?id=qyEm4tF2p1'>https://openreview.net/forum?id=qyEm4tF2p1</a></p>
<p><b>Keywords</b>: learning surrogates, predict+optimize framework, combinatorial nonlinear optimization, argmin differentiation
</p><p><b>Compressor summary</b>: The paper proposes a learnable landscape surrogate to improve learning-integrated optimization by addressing challenges such as problem uncertainty and sparse gradients.</p><hr><h3>Diffusion Schrödinger Bridge Matching</h3>
<p>Yuyang Shi, Valentin De Bortoli, Andrew Campbell, Arnaud Doucet</p>
<p><a href='https://openreview.net/forum?id=qy07OHsJT5'>https://openreview.net/forum?id=qy07OHsJT5</a></p>
<p><b>Keywords</b>: diffusion Schrödinger bridge, bridge matching, optimal transport
</p><p><b>Compressor summary</b>: The paragraph discusses a new method, Iterative Markovian Fitting (IMF), for solving Schrödinger bridges (SBs) problems and a novel algorithm, Diffusion Schrödinger Bridge Matching (DSBM), which significantly improves SB numerics and recovers various recent transport methods.</p><hr><h3>Reinforcement Learning with Simple Sequence Priors</h3>
<p>Tankred Saanum, Noemi Elteto, Peter Dayan, Marcel Binz, Eric Schulz</p>
<p><a href='https://openreview.net/forum?id=qxF8Pge6vM'>https://openreview.net/forum?id=qxF8Pge6vM</a></p>
<p><b>Keywords</b>: Deep Reinforcement Learning, Compression, Sequence learning, Information bottleneck, Mutual information
</p><p><b>Compressor summary</b>: The paper proposes an RL algorithm that learns simple action sequences, either learned by autoregressive models or compressed by data algorithms, and shows it outperforms model-free approaches in continuous control tasks.</p><hr><h3>Multimodal Deep Learning Model Unveils Behavioral Dynamics of V1 Activity in Freely Moving Mice</h3>
<p>Aiwen Xu, Yuchen Hou, Cris M. Niell, Michael Beyeler</p>
<p><a href='https://openreview.net/forum?id=qv5UZJTNda'>https://openreview.net/forum?id=qv5UZJTNda</a></p>
<p><b>Keywords</b>: neuroscience, cognitive science, multimodal learning, representation learning, network architecture, computational biology, visual perception
</p><p><b>Compressor summary</b>: The authors introduce a multimodal recurrent neural network that can predict visual cortex activity in free-moving mice by integrating visual input, behavior, and temporal dynamics, revealing new insights into cortical function.</p><hr><h3>On the Robustness of Mechanism Design under Total Variation Distance</h3>
<p>Anuran Makur, Marios Mertzanidis, Alexandros Psomas, Athina Terzoglou</p>
<p><a href='https://openreview.net/forum?id=qumBHr77ht'>https://openreview.net/forum?id=qumBHr77ht</a></p>
<p><b>Keywords</b>: mechanism design, revenue maximization, correlated distributions, total variation distance
</p><p><b>Compressor summary</b>: The paper studies how to design truthful mechanisms for agents with unknown and correlated valuation functions, showing that some existing mechanisms are robust to variations in the distribution, and providing various applications and extensions.</p><hr><h3>Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources</h3>
<p>Feiyang Kang, Hoang Anh Just, Anit Kumar Sahu, Ruoxi Jia</p>
<p><a href='https://openreview.net/forum?id=quMBEd27x9'>https://openreview.net/forum?id=quMBEd27x9</a></p>
<p><b>Keywords</b>: data-centric AI, data acquisition, data valuation, performance prediction, data markets, optimal transport, scaling laws
</p><p><b>Compressor summary</b>: The paper introduces <projektor>, a framework that predicts model performance and supports data selection decisions using partial samples of prospective data sources, improving over existing methods in accuracy and computation costs.</p><hr><h3>TabMT: Generating tabular data with masked transformers</h3>
<p>Manbir S Gulati, Paul F Roysdon</p>
<p><a href='https://openreview.net/forum?id=qs4swxtIAQ'>https://openreview.net/forum?id=qs4swxtIAQ</a></p>
<p><b>Keywords</b>: Tabular Data, Deep Learning, Generative Modeling, Transformers, Masked Transformers, Synthetic data
</p><p><b>Compressor summary</b>: TabMT is a new Masked Transformer model that generates synthetic tabular data with strong performance, handling missing data and various data types, and providing good privacy tradeoffs.</p><hr><h3>PHOTOSWAP: Personalized Subject Swapping in Images</h3>
<p>Jing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong, Qing Liu, Zhifei Zhang, HE Zhang, Jianming Zhang, HyunJoon Jung, Xin Eric Wang</p>
<p><a href='https://openreview.net/forum?id=qqcIM8NiiB'>https://openreview.net/forum?id=qqcIM8NiiB</a></p>
<p><b>Keywords</b>: image editing, diffusion model, text to image generation
</p><p><b>Compressor summary</b>: The text introduces Photoswap, a novel approach for personalized subject swapping in images without training, which preserves the pose and coherence of the image while swapping subjects.</p><hr><h3>Lower Bounds on Adaptive Sensing for Matrix Recovery</h3>
<p>Praneeth Kacham, David Woodruff</p>
<p><a href='https://openreview.net/forum?id=qptO6YDZEP'>https://openreview.net/forum?id=qptO6YDZEP</a></p>
<p><b>Keywords</b>: Compressed Sensing, Matrix Recovery, Low rank approximation
</p><p><b>Compressor summary</b>: The authors study the lower bounds on adaptive algorithms for recovering low rank matrices using linear measurements, showing that any such algorithm must perform a certain number of rounds to achieve a satisfactory approximation.</p><hr><h3>Aligning Synthetic Medical Images with Clinical Knowledge using Human Feedback</h3>
<p>Shenghuan Sun, Gregory Goldgof, Atul Butte, Ahmed Alaa</p>
<p><a href='https://openreview.net/forum?id=qlnlamFQEa'>https://openreview.net/forum?id=qlnlamFQEa</a></p>
<p><b>Keywords</b>: Synthetic clinical data, Machine learning for healthcare
</p><p><b>Compressor summary</b>: The paper introduces a pathologist-in-the-loop framework that uses human feedback to improve the quality and plausibility of synthetic medical images generated by conditional diffusion models, addressing challenges in assessing clinical sensibility and incorporating domain knowledge.</p><hr><h3>Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability</h3>
<p>David Liu, Máté Lengyel</p>
<p><a href='https://openreview.net/forum?id=qlJoo2y3gY'>https://openreview.net/forum?id=qlJoo2y3gY</a></p>
<p><b>Keywords</b>: Gaussian processes, renewal processes, point processes, neural data analysis, Bayesian machine learning, non-stationary time series
</p><p><b>Compressor summary</b>: The authors propose a Bayesian approach to model neural spiking activity that captures instantaneous variability in response to covariates, and apply it to two datasets of animal navigation, showing improved predictive power and richer patterns of modulation.</p><hr><h3>Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm</h3>
<p>Miaoxi Zhu, Li Shen, Bo Du, Dacheng Tao</p>
<p><a href='https://openreview.net/forum?id=ql6LVyi2Dg'>https://openreview.net/forum?id=ql6LVyi2Dg</a></p>
<p><b>Keywords</b>: decentralized algorithm, minimax problem, algorithmic stability, generalization analysis
</p><p><b>Compressor summary</b>: The paper studies how decentralized algorithms like D-SGDA can generalize well despite their structure, by analyzing their primal-dual generalization bound using algorithmic stability.</p><hr><h3>Rank-1 Matrix Completion with Gradient Descent and Small Random Initialization</h3>
<p>Daesung Kim, Hye Won Chung</p>
<p><a href='https://openreview.net/forum?id=qjqJL2lfkH'>https://openreview.net/forum?id=qjqJL2lfkH</a></p>
<p><b>Keywords</b>: Matrix completion, gradient descent, random initialization
</p><p><b>Compressor summary</b>: The paper analyzes gradient descent algorithm's convergence behavior on rank-1 symmetric matrix completion problem with small random initialization, and shows that it converges to the ground truth when more samples are available.</p><hr><h3>High-Fidelity Audio Compression with Improved RVQGAN</h3>
<p>Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, Kundan Kumar</p>
<p><a href='https://openreview.net/forum?id=qjnl1QUnFA'>https://openreview.net/forum?id=qjnl1QUnFA</a></p>
<p><b>Keywords</b>: audio generation, audio compression, GAN, audio, speech
</p><p><b>Compressor summary</b>: The authors introduce a universal neural audio compression algorithm that achieves 90x compression of various audio domains with high fidelity, outperforming existing methods and providing open-source code and weights.</p><hr><h3>Transformers learn through gradual rank increase</h3>
<p>Emmanuel Abbe, Samy Bengio, Enric Boix-Adserà, Etai Littwin, Joshua M. Susskind</p>
<p><a href='https://openreview.net/forum?id=qieeNlO3C7'>https://openreview.net/forum?id=qieeNlO3C7</a></p>
<p><b>Keywords</b>: transformers, low-rank bias, incremental learning
</p><p><b>Compressor summary</b>: The text describes how transformers learn incrementally, leading to a growing difference between trained and initial weights, and provides theoretical and empirical evidence for this behavior.</p><hr><h3>Diffusion Self-Guidance for Controllable Image Generation</h3>
<p>Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, Aleksander Holynski</p>
<p><a href='https://openreview.net/forum?id=qgv56R2YJ7'>https://openreview.net/forum?id=qgv56R2YJ7</a></p>
<p><b>Keywords</b>: generative models, image editing, diffusion, guidance
</p><p><b>Compressor summary</b>: Self-guidance is a technique that uses internal representations of diffusion models to control various aspects of image generation and manipulation without additional training or models.</p><hr><h3>Convolutional Visual Prompt for Robust Visual Perception</h3>
<p>Yun-Yun Tsai, Chengzhi Mao, Junfeng Yang</p>
<p><a href='https://openreview.net/forum?id=qgmrC8jhCo'>https://openreview.net/forum?id=qgmrC8jhCo</a></p>
<p><b>Keywords</b>: self-supervised learning, representation learning, visual prompts, domain generalization, input adaptation
</p><p><b>Compressor summary</b>: Convolutional visual prompts (CVP) are a lightweight method for adapting vision models in test-time settings without labels, improving their robustness against out-of-distribution samples.</p><hr><h3>Affinity-Aware Graph Networks</h3>
<p>Ameya Velingker, Ali Kemal Sinop, Ira Ktena, Petar Veličković, Sreenivas Gollapudi</p>
<p><a href='https://openreview.net/forum?id=qgiG7WZohZ'>https://openreview.net/forum?id=qgiG7WZohZ</a></p>
<p><b>Keywords</b>: graph neural networks, message passing, effective resistance, hitting time
</p><p><b>Compressor summary</b>: The paper explores using random walk measures as features in graph neural networks to improve performance on various node and graph property prediction tasks with low computational complexity.</p><hr><h3>ANTN: Bridging Autoregressive Neural Networks and Tensor Networks for Quantum Many-Body Simulation</h3>
<p>Zhuo Chen, Laker Newhouse, Eddie Chen, Di Luo, Marin Soljacic</p>
<p><a href='https://openreview.net/forum?id=qdsDy0zbn4'>https://openreview.net/forum?id=qdsDy0zbn4</a></p>
<p><b>Keywords</b>: Autoregressive neural network, tensor network, quantum many-body physics, variational Monte Carlo
</p><p><b>Compressor summary</b>: The Autoregressive Neural TensorNet (ANTN) is a novel architecture that combines tensor networks and autoregressive neural networks to improve quantum many-body physics simulation, outperforming existing methods.</p><hr><h3>Cross-Domain Policy Adaptation via Value-Guided Data Filtering</h3>
<p>Kang Xu, Chenjia Bai, Xiaoteng Ma, Dong Wang, Bin Zhao, Zhen Wang, Xuelong Li, Wei Li</p>
<p><a href='https://openreview.net/forum?id=qdM260dXsa'>https://openreview.net/forum?id=qdM260dXsa</a></p>
<p><b>Keywords</b>: Reinforcement Learning; Domain Adaptation; Online Dynamics Adaptation
</p><p><b>Compressor summary</b>: The paper proposes a new reinforcement learning method that adapts policies across domains with different dynamics by using value consistency to selectively share data, and shows its superior performance.</p><hr><h3>Parsel🐍: Algorithmic Reasoning with Language Models by Composing Decompositions</h3>
<p>Eric Zelikman, Qian Huang, Gabriel Poesia, Noah Goodman, Nick Haber</p>
<p><a href='https://openreview.net/forum?id=qd9qcbVAwQ'>https://openreview.net/forum?id=qd9qcbVAwQ</a></p>
<p><b>Keywords</b>: reasoning, language models, code synthesis, decomposition
</p><p><b>Compressor summary</b>: Parsel is a framework that helps large language models generate complex algorithms with code by breaking down tasks into hierarchical natural language function descriptions and searching over combinations of possible function implementations using tests.</p><hr><h3>Multi-Head Adapter Routing for Cross-Task Generalization</h3>
<p>Lucas Caccia, Edoardo Ponti, Zhan Su, Matheus Pereira, Nicolas Le Roux, Alessandro Sordoni</p>
<p><a href='https://openreview.net/forum?id=qcQhBli5Ho'>https://openreview.net/forum?id=qcQhBli5Ho</a></p>
<p><b>Keywords</b>: Parameter Efficient Finetuning, Multitask Learning, Transfer Learning, Natural Language Processing
</p><p><b>Compressor summary</b>: The paper explores how adapter routing affects PEFT and proposes $\texttt{MHR}$, a new routing method that improves performance by combining subsets of adapter parameters, and $\texttt{MHR}$-$\mu$, a variant that discards routing for single-adapter fine-tuning and zero-shot transfer.</p><hr><h3>One Risk to Rule Them All: A Risk-Sensitive Perspective on Model-Based Offline Reinforcement Learning</h3>
<p>Marc Rigter, Bruno Lacerda, Nick Hawes</p>
<p><a href='https://openreview.net/forum?id=qZjl2TKvUY'>https://openreview.net/forum?id=qZjl2TKvUY</a></p>
<p><b>Keywords</b>: offline reinforcement learning, model-based reinforcement learning, risk, uncertainty
</p><p><b>Compressor summary</b>: The paper proposes a model-based offline reinforcement learning algorithm that combines risk-aversion to aleatoric and epistemic uncertainty to address distributional shift and safety in safety-critical domains.</p><hr><h3>Multi-Objective Intrinsic Reward Learning for Conversational Recommender Systems</h3>
<p>Zhendong Chu, Nan Wang, Hongning Wang</p>
<p><a href='https://openreview.net/forum?id=qYAp31KwU2'>https://openreview.net/forum?id=qYAp31KwU2</a></p>
<p><b>Keywords</b>: Conversational Recommendation, Reinforcement Learning, Meta Learning
</p><p><b>Compressor summary</b>: The paper proposes a new method to improve Conversational Recommender Systems by learning user preferences and designing task-specific rewards from user interactions, leading to better recommendations with fewer turns.</p><hr><h3>Are GATs Out of Balance?</h3>
<p>Nimrah Mustafa, Aleksandar Bojchevski, Rebekka Burkholz</p>
<p><a href='https://openreview.net/forum?id=qY7UqLoora'>https://openreview.net/forum?id=qY7UqLoora</a></p>
<p><b>Keywords</b>: graph attention networks, gradient flow, conservation law
</p><p><b>Compressor summary</b>: The paper explores the optimization and learning dynamics of Graph Attention Networks (GATs), derives a conservation law for their gradient flow dynamics, proposes an initialization scheme to improve trainability and speedup, and serves as a foundation for future studies on positive homogeneous models with attention.</p><hr><h3>CoPriv: Network/Protocol Co-Optimization for Communication-Efficient Private Inference</h3>
<p>Wenxuan Zeng, Meng Li, Haichuan Yang, Wen-jie Lu, Runsheng Wang, Ru Huang</p>
<p><a href='https://openreview.net/forum?id=qVeDwgYsho'>https://openreview.net/forum?id=qVeDwgYsho</a></p>
<p><b>Keywords</b>: Private Inference, Network/Protocol Co-Optimization, Winograd Convolution, Structural Re-parameterization
</p><p><b>Compressor summary</b>: CoPriv is a framework that optimizes both the DNN architecture and the 2PC inference protocol to significantly reduce communication overhead while maintaining accuracy for secure privacy protection in DNN inference.</p><hr><h3>LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas</h3>
<p>Kensen Shi, Hanjun Dai, Wen-Ding Li, Kevin Ellis, Charles Sutton</p>
<p><a href='https://openreview.net/forum?id=qVMPXrX4FR'>https://openreview.net/forum?id=qVMPXrX4FR</a></p>
<p><b>Keywords</b>: Program Synthesis, Programming By Example, Lambdas, Functional Programming
</p><p><b>Compressor summary</b>: LambdaBeam is a search algorithm that uses neural networks to synthesize lambda functions for longer and more general programs, overcoming the limitations of prior approaches in handling iterative loops and higher-order functions.</p><hr><h3>Multi-resolution Spectral Coherence for Graph Generation with Score-based Diffusion</h3>
<p>Hyuna Cho, Minjae Jeong, Sooyeon Jeon, Sungsoo Ahn, Won Hwa Kim</p>
<p><a href='https://openreview.net/forum?id=qUlpDjYnsp'>https://openreview.net/forum?id=qUlpDjYnsp</a></p>
<p><b>Keywords</b>: graph wavelet transform, multi-scale wavelet filtering, graph generation, diffusion model
</p><p><b>Compressor summary</b>: The paper introduces a new method called Wavelet Graph Diffusion Model (Wave-GD) that generates realistic graphs with accurate node and edge frequency characteristics by modeling their joint distribution in the spectral space.</p><hr><h3>Alleviating the Semantic Gap for Generalized fMRI-to-Image Reconstruction</h3>
<p>Tao Fang, Qian Zheng, Gang Pan</p>
<p><a href='https://openreview.net/forum?id=qSS9izTOpo'>https://openreview.net/forum?id=qSS9izTOpo</a></p>
<p><b>Keywords</b>: fMRI, image reconstruction, brain decoding
</p><p><b>Compressor summary</b>: The paper proposes a method (GESS) to improve fMRI-to-image reconstruction by using CLIP features to bridge the semantic gap between training and testing data, as well as leveraging structural information and uncertainty estimation.</p><hr><h3>The Double-Edged Sword of Implicit Bias: Generalization vs. Robustness in ReLU Networks</h3>
<p>Spencer Frei, Gal Vardi, Peter Bartlett, Nathan Srebro</p>
<p><a href='https://openreview.net/forum?id=qSCziWQBPD'>https://openreview.net/forum?id=qSCziWQBPD</a></p>
<p><b>Keywords</b>: adversarial robustness, neural networks, implicit bias, generalization
</p><p><b>Compressor summary</b>: The paper investigates how the biased gradient flow in two-layer ReLU networks affects generalization and adversarial robustness, showing that it leads to solutions that are good at fitting data but vulnerable to small perturbations.</p><hr><h3>Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability</h3>
<p>Hanlin Zhu, Amy Zhang</p>
<p><a href='https://openreview.net/forum?id=qS9aHF8bXz'>https://openreview.net/forum?id=qS9aHF8bXz</a></p>
<p><b>Keywords</b>: offline goal-conditioned RL, provably efficient algorithm, single-policy concentrability, general function approximation
</p><p><b>Compressor summary</b>: The paper provides a rigorous theoretical analysis of an existing offline goal-conditioned reinforcement learning algorithm, showing its efficiency, stability, and effectiveness in various real-world environments.</p><hr><h3>Complex Query Answering on Eventuality Knowledge Graph with Implicit Logical Constraints</h3>
<p>Jiaxin Bai, Xin Liu, Weiqi Wang, Chen Luo, Yangqiu Song</p>
<p><a href='https://openreview.net/forum?id=qQnO1HLQHe'>https://openreview.net/forum?id=qQnO1HLQHe</a></p>
<p><b>Keywords</b>: Knowledge Graph, Complex Query Answering, Eventuality Graph
</p><p><b>Compressor summary</b>: The paper proposes a new framework, CEQA, that uses implicit logical constraints and theorem provers to answer complex queries on eventuality-centric knowledge graphs, improving neural query encoders' performance.</p><hr><h3>Statistical Insights into HSIC in High Dimensions</h3>
<p>Tao Zhang, Yaowu Zhang, Tingyou Zhou</p>
<p><a href='https://openreview.net/forum?id=qPyvuFT0U9'>https://openreview.net/forum?id=qPyvuFT0U9</a></p>
<p><b>Keywords</b>: High dimensionality; Independence test; Kernel method; Nonlinear dependency.
</p><p><b>Compressor summary</b>: The paper studies how a popular measure of dependence, HSIC, behaves when the dimensions of the variables it measures grow at different rates and provides conditions for its performance in high-dimensional settings.</p><hr><h3>Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context</h3>
<p>Lakshya Agrawal, Aditya Kanade, Navin Goyal, Shuvendu K Lahiri, Sriram Rajamani</p>
<p><a href='https://openreview.net/forum?id=qPUbKxKvXq'>https://openreview.net/forum?id=qPUbKxKvXq</a></p>
<p><b>Keywords</b>: Language models, code generation, correctness, program analysis
</p><p><b>Compressor summary</b>: Monitor-guided decoding (MGD) helps language models of code achieve better compilation rates and agreement with ground truth by using static analysis to provide context, especially when dealing with global information.</p><hr><h3>Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals</h3>
<p>Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, Tom Mitchell</p>
<p><a href='https://openreview.net/forum?id=qP0Drg2HuH'>https://openreview.net/forum?id=qP0Drg2HuH</a></p>
<p><b>Keywords</b>: Games, Instruction Manual, Atari Games, Large Language Models, Language Models, Zero-shot, In-context prompting
</p><p><b>Compressor summary</b>: The Read and Reward framework uses instruction manuals to enhance the performance and efficiency of reinforcement learning agents on Atari games.</p><hr><h3>TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion</h3>
<p>Preetha Vijayan, Prashant Shivaram Bhat, Bahram Zonooz, Elahe Arani</p>
<p><a href='https://openreview.net/forum?id=qL3zPoWJda'>https://openreview.net/forum?id=qL3zPoWJda</a></p>
<p><b>Keywords</b>: Continual Learning, Catastrophic Forgetting, Experience Replay, Lifelong Learning, Bio-Inspired, Active Forgetting, Scalable Neurogenesis
</p><p><b>Compressor summary</b>: The paragraph introduces TriRE, a novel continual learning paradigm that combines multiple neurophysiological processes to reduce catastrophic forgetting and promote knowledge transfer in deep neural networks.</p><hr><h3>VaRT: Variational Regression Trees</h3>
<p>Sebastian Salazar</p>
<p><a href='https://openreview.net/forum?id=qJRlz3SucN'>https://openreview.net/forum?id=qJRlz3SucN</a></p>
<p><b>Keywords</b>: Probabilistic Machine Learning, Variational Inference, Bayesian Inference, Bayesian Nonparametrics
</p><p><b>Compressor summary</b>: The paper presents a new Bayesian model for decision trees that uses variational inference and shows its effectiveness in regression and causal inference problems, with a PyTorch implementation available.</p><hr><h3>Is Heterogeneity Notorious? Taming Heterogeneity to Handle Test-Time Shift in Federated Learning</h3>
<p>Yue Tan, Chen Chen, Weiming Zhuang, Xin Dong, Lingjuan Lyu, Guodong Long</p>
<p><a href='https://openreview.net/forum?id=qJJmu4qsLO'>https://openreview.net/forum?id=qJJmu4qsLO</a></p>
<p><b>Keywords</b>: Federated Learning, Test-Time Shift, Contrastive Learning
</p><p><b>Compressor summary</b>: FedICON is a framework that uses contrastive learning to capture and adapt to heterogeneous data in federated learning, improving performance under test-time shifts.</p><hr><h3>Sketching Algorithms for Sparse Dictionary Learning: PTAS and Turnstile Streaming</h3>
<p>Gregory Dexter, Petros Drineas, David Woodruff, Taisuke Yasuda</p>
<p><a href='https://openreview.net/forum?id=qHzEFxtheD'>https://openreview.net/forum?id=qHzEFxtheD</a></p>
<p><b>Keywords</b>: dictionary learning, k means clustering, sketching, ptas, streaming
</p><p><b>Compressor summary</b>: The paper develops new techniques to extend sketching-based approaches to sparse dictionary learning and Euclidean k-means clustering problems, and obtains new upper and lower bounds for these problems in various settings.</p><hr><h3>Don't be so Monotone: Relaxing Stochastic Line Search in Over-Parameterized Models</h3>
<p>Leonardo Galli, Holger Rauhut, Mark Schmidt</p>
<p><a href='https://openreview.net/forum?id=qHrZszJSXj'>https://openreview.net/forum?id=qHrZszJSXj</a></p>
<p><b>Keywords</b>: line search, nonmonotone, stochastic gradient descent, over-parametrized models, Polyak step size, optimization
</p><p><b>Compressor summary</b>: Nonmonotone line search methods can improve the speed and generalization of SGD/Adam by allowing larger step sizes without compromising convergence rates, as demonstrated by the PoNoS method and a new resetting technique.</p><hr><h3>Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective</h3>
<p>Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, Liwei Wang</p>
<p><a href='https://openreview.net/forum?id=qHrADgAdYu'>https://openreview.net/forum?id=qHrADgAdYu</a></p>
<p><b>Keywords</b>: Chain-of-Thought Prompting, Large Language Models, Theory, Circuit Complexity, Dynamic Programming
</p><p><b>Compressor summary</b>: The paper investigates how Chain-of-Thought prompting enhances Large Language Models' performance in solving mathematical and decision-making problems, using circuit complexity theory and experiments.</p><hr><h3>Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy</h3>
<p>Anastasia Koloskova, Ryan McKenna, Zachary Charles, J Keith Rush, Hugh Brendan McMahan</p>
<p><a href='https://openreview.net/forum?id=qCglMj6A4z'>https://openreview.net/forum?id=qCglMj6A4z</a></p>
<p><b>Keywords</b>: optimization, machine learning, differential privacy
</p><p><b>Compressor summary</b>: The paper analyzes how gradient descent works with linearly correlated noise, which arises in some privacy-preserving optimization methods, and proposes improved matrix factorizations for better performance.</p><hr><h3>VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models</h3>
<p>Ziyi Yin, Muchao Ye, Tianrong Zhang, Tianyu Du, Jinguo Zhu, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma</p>
<p><a href='https://openreview.net/forum?id=qBAED3u1XZ'>https://openreview.net/forum?id=qBAED3u1XZ</a></p>
<p><b>Keywords</b>: vision-language, adversarial attacks, pre-trained model, fine-tuned model
</p><p><b>Compressor summary</b>: This paper proposes VLATTACK, a method to craft image and text perturbations that can attack black-box fine-tuned Vision-Language models on different tasks by using block-wise similarity attacks for images and existing methods for texts.</p><hr><h3>Complex-valued Neurons Can Learn More but Slower than Real-valued Neurons via Gradient Descent</h3>
<p>Jin-Hui Wu, Shao-Qun Zhang, Yuan Jiang, Zhi-Hua Zhou</p>
<p><a href='https://openreview.net/forum?id=qA0uHmaVKk'>https://openreview.net/forum?id=qA0uHmaVKk</a></p>
<p><b>Keywords</b>: Complex-valued Neural Networks; Learning Neurons; Real-valued Neural Networks; Convergence Rate
</p><p><b>Compressor summary</b>: Complex-valued neural networks can efficiently learn functions of real-valued neurons and complex-valued neurons faster than real-valued neurons can learn each other or be learned by complex-valued neurons.</p><hr><h3>Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints</h3>
<p>Dohyeong Kim, Kyungjae Lee, Songhwai Oh</p>
<p><a href='https://openreview.net/forum?id=q9WMXjUxxT'>https://openreview.net/forum?id=q9WMXjUxxT</a></p>
<p><b>Keywords</b>: Reinforcement learning, Safety, Multiple Constraints, Distributional Critic
</p><p><b>Compressor summary</b>: The paper proposes a trust region-based safe reinforcement learning algorithm for robotic tasks with multiple constraints, and shows its effectiveness in experiments.</p><hr><h3>Deep Contract Design via Discontinuous Networks</h3>
<p>Tonghan Wang, Paul Duetting, Dmitry Ivanov, Inbal Talgam-Cohen, David C. Parkes</p>
<p><a href='https://openreview.net/forum?id=q8mH2d6uw2'>https://openreview.net/forum?id=q8mH2d6uw2</a></p>
<p><b>Keywords</b>: Automated contract design, discontinuous neural networks
</p><p><b>Compressor summary</b>: The paper proposes using deep learning and a new type of network (DeLU) to automate the design of optimal contracts, considering both agent's incentives and principal's utility.</p><hr><h3>Learning from Active Human Involvement through Proxy Value Propagation</h3>
<p>Zhenghao Peng, Wenjie Mo, Chenda Duan, Quanyi Li, Bolei Zhou</p>
<p><a href='https://openreview.net/forum?id=q8SukwaEBy'>https://openreview.net/forum?id=q8SukwaEBy</a></p>
<p><b>Keywords</b>: Machine Learning, Human-in-the-loop Reinforcement Learning, Safety, Sample Efficiency, Reward-free
</p><p><b>Compressor summary</b>: The proposed method uses a proxy value function to express human intents and label state-action pairs for policy optimization, allowing the AI agent to learn from active human involvement in various control tasks.</p><hr><h3>Triple Eagle: Simple, Fast and Practical Budget-Feasible Mechanisms</h3>
<p>Kai Han, You Wu, He Huang, Shuang Cui</p>
<p><a href='https://openreview.net/forum?id=q6bVqOgGxP'>https://openreview.net/forum?id=q6bVqOgGxP</a></p>
<p><b>Keywords</b>: mechanism design, budget-feasible, truthful
</p><p><b>Compressor summary</b>: The paper introduces TripleEagle, a new algorithmic framework for designing Budget-Feasible Mechanisms with better approximation ratios, linear complexities, and strategyproofness for submodular valuation functions.</p><hr><h3>Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting</h3>
<p>Marcel Kollovieh, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao Wang, Bernie Wang</p>
<p><a href='https://openreview.net/forum?id=q6X038vKgU'>https://openreview.net/forum?id=q6X038vKgU</a></p>
<p><b>Keywords</b>: diffusion models, time series forecasting, generative modeling, deep learning
</p><p><b>Compressor summary</b>: The authors propose a task-agnostic diffusion model for time series applications and show its effectiveness in forecasting, refinement, and synthetic data generation.</p><hr><h3>Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models</h3>
<p>Simian Luo, Chuanhao Yan, Chenxu Hu, Hang Zhao</p>
<p><a href='https://openreview.net/forum?id=q5FAZAIooz'>https://openreview.net/forum?id=q5FAZAIooz</a></p>
<p><b>Keywords</b>: Video-to-Audio Generation; Contrastive Audio-Visual Pretraining; Latent Diffusion Model;
</p><p><b>Compressor summary</b>: Diff-Foley is a new method for generating high-quality audio from silent videos with improved synchronization and relevance, using a latent diffusion model and contrastive pretraining.</p><hr><h3>Discriminative Feature Attributions: Bridging Post Hoc Explainability and Inherent Interpretability</h3>
<p>Usha Bhalla, Suraj Srinivas, Himabindu Lakkaraju</p>
<p><a href='https://openreview.net/forum?id=q4HlFS7B7Y'>https://openreview.net/forum?id=q4HlFS7B7Y</a></p>
<p><b>Keywords</b>: Machine Learning Explainability, Machine Learning Interpretability
</p><p><b>Compressor summary</b>: The paragraph discusses a method called Distractor Erasure Tuning (DiET), which improves the faithfulness of post hoc explanations for machine learning models by making them robust to distractor erasure in inputs.</p><hr><h3>Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks</h3>
<p>Eeshaan Jain, Tushar Nandy, Gaurav Aggarwal, Ashish V. Tendulkar, Rishabh K Iyer, Abir De</p>
<p><a href='https://openreview.net/forum?id=q3fCWoC9l0'>https://openreview.net/forum?id=q3fCWoC9l0</a></p>
<p><b>Keywords</b>: Data Subset Selection, Efficient Learning
</p><p><b>Compressor summary</b>: The paragraph describes $\texttt{SubSelNet}$, a non-adaptive framework for subset selection in efficient learning that uses an attention-based neural gadget and can be either transductive or inductive, achieving better results than other methods on various real datasets.</p><hr><h3>Brain Dissection: fMRI-trained Networks Reveal Spatial Selectivity in the Processing of Natural Images</h3>
<p>Gabriel Herbert Sarch, Michael J. Tarr, Katerina Fragkiadaki, Leila Wehbe</p>
<p><a href='https://openreview.net/forum?id=q3fA5tTod3'>https://openreview.net/forum?id=q3fA5tTod3</a></p>
<p><b>Keywords</b>: Computational Neuroscience, Deep Neural Networks, Visual Neuroscience, Visual Streams, Scene Perception, Brain Imaging
</p><p><b>Compressor summary</b>: The paper trains neural networks to predict brain responses to images and uses "network dissection" to explore how different regions of the visual cortex interpret various features of natural scenes.</p><hr><h3>Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt</h3>
<p>Yining Ma, Zhiguang Cao, Yeow Meng Chee</p>
<p><a href='https://openreview.net/forum?id=q1JukwH2yP'>https://openreview.net/forum?id=q1JukwH2yP</a></p>
<p><b>Keywords</b>: learning to optimize, vehicle routing problem, combinatorial optimization
</p><p><b>Compressor summary</b>: The paper introduces NeuOpt, a learning-to-search solver for routing problems that learns flexible k-opt exchanges and uses GIRE, D2A, and reward shaping to explore both feasible and infeasible regions effectively, achieving superior performance on TSP and CVRP compared to existing methods.</p><hr><h3>Learning Linear Causal Representations from Interventions under General Nonlinear Mixing</h3>
<p>Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Schölkopf, Pradeep Kumar Ravikumar</p>
<p><a href='https://openreview.net/forum?id=q131tA7HCT'>https://openreview.net/forum?id=q131tA7HCT</a></p>
<p><b>Keywords</b>: Causal Representation Learning, Interventional data, Gaussian Structural Causal models
</p><p><b>Compressor summary</b>: The authors study how to learn causal representations from unknown interventions using Gaussian latent distributions and a general mixing function, and they prove identifiability results without access to intervention targets.</p><hr><h3>SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning</h3>
<p>Dohyeok Lee, Seungyub Han, Taehyun Cho, Jungwoo Lee</p>
<p><a href='https://openreview.net/forum?id=q0sdoFIfNg'>https://openreview.net/forum?id=q0sdoFIfNg</a></p>
<p><b>Keywords</b>: Deep Reinforcement Learning, Ensemble Q-learning
</p><p><b>Compressor summary</b>: The paper proposes a new method, spiked Wishart Q-ensemble independence regularization (SPQR), to improve deep reinforcement learning by ensuring diversity of multiple Q-functions using random matrix theory.</p><hr><h3>On the Consistency of Maximum Likelihood Estimation of Probabilistic Principal Component Analysis</h3>
<p>Arghya Datta, Sayak Chakrabarty</p>
<p><a href='https://openreview.net/forum?id=q0RfX96un8'>https://openreview.net/forum?id=q0RfX96un8</a></p>
<p><b>Keywords</b>: maximum likelihood estimate, non-identifiability, Redner approach, quotient topological spaces, consistency
</p><p><b>Compressor summary</b>: The text describes PPCA, a widely used statistical tool with applications in various fields, and proposes a novel approach to resolve the identifiability issue of its maximum likelihood solution using quotient topological spaces.</p><hr><h3>StateMask: Explaining Deep Reinforcement Learning through State Mask</h3>
<p>Zelei Cheng, Xian Wu, Jiahao Yu, Wenhai Sun, Wenbo Guo, Xinyu Xing</p>
<p><a href='https://openreview.net/forum?id=pzc6LnUxYN'>https://openreview.net/forum?id=pzc6LnUxYN</a></p>
<p><b>Keywords</b>: deep reinforcement learning, interpretation, explanation
</p><p><b>Compressor summary</b>: StateMask is a novel method to identify critical states for an agent's final reward by learning a mask net that temporarily blinds the agent without affecting its performance.</p><hr><h3>Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy</h3>
<p>Chendi Wang, Buxin Su, Jiayuan Ye, Reza Shokri, Weijie J Su</p>
<p><a href='https://openreview.net/forum?id=pw5hEuEroL'>https://openreview.net/forum?id=pw5hEuEroL</a></p>
<p><b>Keywords</b>: Differential privacy, $f$-DP, mixture mechanisms, shuffling, differentially private gradient descent
</p><p><b>Compressor summary</b>: The paper proposes an improved analysis of differential privacy for shuffling models and one-iteration DP-GD using $f$-DP, with applications to random initialization and mixture mechanisms.</p><hr><h3>Flow-Attention-based Spatio-Temporal Aggregation Network for 3D Mask Detection</h3>
<p>Yuxin Cao, Yian Li, Yumeng Zhu, Derui Wang, Minhui Xue</p>
<p><a href='https://openreview.net/forum?id=pvSKVt3EsM'>https://openreview.net/forum?id=pvSKVt3EsM</a></p>
<p><b>Keywords</b>: 3D mask detection, spatio-temporal aggregation, optical flow, deep learning
</p><p><b>Compressor summary</b>: FASTEN is a novel 3D mask detection framework that uses flow attention and spatio-temporal aggregation to quickly and accurately detect spoofing attacks in face recognition systems with low computational overhead and only five frames of input.</p><hr><h3>Most Neural Networks Are Almost Learnable</h3>
<p>Amit Daniely, Nathan Srebro, Gal Vardi</p>
<p><a href='https://openreview.net/forum?id=pvPujuvjQd'>https://openreview.net/forum?id=pvPujuvjQd</a></p>
<p><b>Keywords</b>: learning neural networks, computational complexity, random networks
</p><p><b>Compressor summary</b>: The paper proposes a polynomial-time approximation scheme (PTAS) for learning random constant-depth neural networks with an additive error of epsilon under certain conditions on the network size and activation functions.</p><hr><h3>GraphPatcher: Mitigating Degree Bias for Graph Neural Networks via Test-time Augmentation</h3>
<p>Mingxuan Ju, Tong Zhao, Wenhao Yu, Neil Shah, Yanfang Ye</p>
<p><a href='https://openreview.net/forum?id=puupdGOWUp'>https://openreview.net/forum?id=puupdGOWUp</a></p>
<p><b>Keywords</b>: Graph neural network, Test-time Augmentation
</p><p><b>Compressor summary</b>: The paragraph discusses a test-time augmentation framework, GraphPatcher, that improves the performance of graph neural networks on low-degree nodes without compromising their ability to handle high-degree nodes.</p><hr><h3>Self-Predictive Universal AI</h3>
<p>Elliot Catt, Jordi Grau-Moya, Marcus Hutter, Matthew Aitchison, Tim Genewein, Gregoire Deletang, Li Kevin Wenliang, Joel Veness</p>
<p><a href='https://openreview.net/forum?id=psXVkKO9No'>https://openreview.net/forum?id=psXVkKO9No</a></p>
<p><b>Keywords</b>: General Reinforcement Learning, Reinforcement Learning, Self-Modeling, Bayes-optimality, Policy Distillation, Uncertainty, Universal AI
</p><p><b>Compressor summary</b>: Self-AIXI is a universal agent that uses learning to obtain good policies by self-predicting its own action data, converging to AIXI and having maximal intelligence and self-optimization properties.</p><hr><h3>Label Poisoning is All You Need</h3>
<p>Rishi Dev Jha, Jonathan Hayase, Sewoong Oh</p>
<p><a href='https://openreview.net/forum?id=prftZp6mDH'>https://openreview.net/forum?id=prftZp6mDH</a></p>
<p><b>Keywords</b>: security, backdoor attack
</p><p><b>Compressor summary</b>: The paper introduces FLIP, a novel label-only backdoor attack method that can successfully manipulate image predictions with corrupted training labels, while maintaining high clean test accuracy.</p><hr><h3>Distributional Pareto-Optimal Multi-Objective Reinforcement Learning</h3>
<p>Xin-Qiang Cai, Pushi Zhang, Li Zhao, Jiang Bian, Masashi Sugiyama, Ashley Juan Llorens</p>
<p><a href='https://openreview.net/forum?id=prIwYTU9PV'>https://openreview.net/forum?id=prIwYTU9PV</a></p>
<p><b>Keywords</b>: Multi-Objective Reinforcement Learning
</p><p><b>Compressor summary</b>: DPMORL is a novel approach that learns policies balancing multiple objectives while considering return uncertainty and satisfying diverse distributional preferences.</p><hr><h3>ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation</h3>
<p>Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu</p>
<p><a href='https://openreview.net/forum?id=ppJuFSOAnM'>https://openreview.net/forum?id=ppJuFSOAnM</a></p>
<p><b>Keywords</b>: diffusion models, text to 3D
</p><p><b>Compressor summary</b>: VSD is a particle-based variational framework that improves text-to-3D generation by modeling the 3D parameter as a random variable and addressing over-saturation, over-smoothing, and low-diversity issues.</p><hr><h3>Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schrödinger Equation</h3>
<p>Kirill Neklyudov, Jannes Nys, Luca Thiede, Juan Felipe Carrasquilla Alvarez, qiang liu, Max Welling, Alireza Makhzani</p>
<p><a href='https://openreview.net/forum?id=pjSzKhSrfs'>https://openreview.net/forum?id=pjSzKhSrfs</a></p>
<p><b>Keywords</b>: Quantum Monte Carlo, Schrödinger equation, Wasserstein Fisher-Rao gradient flow
</p><p><b>Compressor summary</b>: The paper introduces Wasserstein Quantum Monte Carlo, a new method for solving quantum many-body problems that uses transportation-based gradients and converges faster than traditional methods.</p><hr><h3>Kernelized Reinforcement Learning with Order Optimal Regret Bounds</h3>
<p>Sattar Vakili, Julia Olkhovskaya</p>
<p><a href='https://openreview.net/forum?id=pirH9ycaNg'>https://openreview.net/forum?id=pirH9ycaNg</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Kernel ridge regression, Gaussian processes, LSVI
</p><p><b>Compressor summary</b>: The paper proposes $\pi$-KRVI, an optimistic modification of least-squares value iteration for reinforcement learning with nonlinear function approximation using kernel ridge regression, and proves its sublinear regret bound under a general setting.</p><hr><h3>Learning Probabilistic Symmetrization for Architecture Agnostic Equivariance</h3>
<p>Jinwoo Kim, Dat Tien Nguyen, Ayhan Suleymanzade, Hyeokjun An, Seunghoon Hong</p>
<p><a href='https://openreview.net/forum?id=phnN1eu5AX'>https://openreview.net/forum?id=phnN1eu5AX</a></p>
<p><b>Keywords</b>: equivariant machine learning, transformers, graphs, general-purpose architectures
</p><p><b>Compressor summary</b>: The authors propose a framework that uses a non-equivariant backbone and an equivariant network to learn functions with group symmetries, achieving competitive results against tailored equivariant architectures.</p><hr><h3>FedFed: Feature Distillation against Data Heterogeneity in Federated Learning</h3>
<p>Zhiqin Yang, Yonggang Zhang, Yu Zheng, Xinmei Tian, Hao Peng, Tongliang Liu, Bo Han</p>
<p><a href='https://openreview.net/forum?id=phnGilhPH8'>https://openreview.net/forum?id=phnGilhPH8</a></p>
<p><b>Keywords</b>: Federated Learning, Data Heterogeneity
</p><p><b>Compressor summary</b>: The paper proposes a method called FedFed that shares some features of data to improve federated learning while preserving privacy.</p><hr><h3>Recurrent Hypernetworks are Surprisingly Strong in Meta-RL</h3>
<p>Jacob Beck, Risto Vuorio, Zheng Xiong, Shimon Whiteson</p>
<p><a href='https://openreview.net/forum?id=pefAAzu8an'>https://openreview.net/forum?id=pefAAzu8an</a></p>
<p><b>Keywords</b>: meta-RL, RL, reinforcement learning, memory, rnn, recurrent, hypernetwork, few-shot
</p><p><b>Compressor summary</b>: The paper investigates how using hypernetworks can improve the performance of simple recurrent networks in meta-RL tasks.</p><hr><h3>Reducing Blackwell and Average Optimality to Discounted MDPs via the Blackwell Discount Factor</h3>
<p>Julien Grand-Clément, Marek Petrik</p>
<p><a href='https://openreview.net/forum?id=pcuC65JWAa'>https://openreview.net/forum?id=pcuC65JWAa</a></p>
<p><b>Keywords</b>: Markov Decision Process, Blackwell optimality, average optimality, robust optimization
</p><p><b>Compressor summary</b>: The paper introduces a discount factor for Markov Decision Processes called the Blackwell discount factor and shows how it can be used to find optimal policies without strict assumptions on the MDP structure.</p><hr><h3>Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks</h3>
<p>Roey Magen, Ohad Shamir</p>
<p><a href='https://openreview.net/forum?id=pcpjtYNJCH'>https://openreview.net/forum?id=pcpjtYNJCH</a></p>
<p><b>Keywords</b>: sample complexity; learning theory; neural networks; linear predictors
</p><p><b>Compressor summary</b>: The text discusses new findings on how many examples are needed to learn vector-valued linear predictors and neural networks, showing different sample complexity behavior compared to scalar-valued linear predictors.</p><hr><h3>Binarized Spectral Compressive Imaging</h3>
<p>Yuanhao Cai, Yuxin Zheng, Jing Lin, Xin Yuan, Yulun Zhang, Haoqian Wang</p>
<p><a href='https://openreview.net/forum?id=pcKwgdVAlq'>https://openreview.net/forum?id=pcKwgdVAlq</a></p>
<p><b>Keywords</b>: Applications, Computer Vision, Low-level Vision, Image Restoration, Snapshot Compressive Imaging, Hyperspectral Image Reconstruction
</p><p><b>Compressor summary</b>: The paper proposes a novel method, BiSRNet, for efficient HSI restoration from compressed measurements in SCI systems using binarized convolutional modules that outperform existing binarization algorithms.</p><hr><h3>Learning Generalizable Agents via Saliency-guided Features Decorrelation</h3>
<p>Sili Huang, Yanchao Sun, Jifeng Hu, Siyuan Guo, Hechang Chen, Yi Chang, Lichao Sun, Bo Yang</p>
<p><a href='https://openreview.net/forum?id=pb1OwZNgr2'>https://openreview.net/forum?id=pb1OwZNgr2</a></p>
<p><b>Keywords</b>: reinforcement learning, generalization
</p><p><b>Compressor summary</b>: SGFD is a method to improve generalization in visual RL by decorrelating features using saliency maps and random Fourier functions.</p><hr><h3>Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator</h3>
<p>Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, Sibei Yang</p>
<p><a href='https://openreview.net/forum?id=paa2OU5jN8'>https://openreview.net/forum?id=paa2OU5jN8</a></p>
<p><b>Keywords</b>: Text-to-Video, Zero-Shot Generation, Large Language Model, Latent Diffusion Models
</p><p><b>Compressor summary</b>: Text-to-video generation using large language models and latent diffusion models to create vivid and high-quality videos without video data or training.</p><hr><h3>Kernel-Based Tests for Likelihood-Free Hypothesis Testing</h3>
<p>Patrik Robert Gerber, Tianze Jiang, Yury Polyanskiy, Rui Sun</p>
<p><a href='https://openreview.net/forum?id=paTESG8iSE'>https://openreview.net/forum?id=paTESG8iSE</a></p>
<p><b>Keywords</b>: Kernel methods, Maximum mean discrepancy, Likelihood-free inference, Hypothesis testing, Minimax statistics
</p><p><b>Compressor summary</b>: The paper studies the trade-off between labeling more inputs and using less training data in two applications: detecting the Higgs boson and identifying planted images in CIFAR-10 dataset.</p><hr><h3>Enhancing Robot Program Synthesis Through Environmental Context</h3>
<p>Tianyi Chen, Qidi Wang, Zhen Dong, Liwei Shen, Xin Peng</p>
<p><a href='https://openreview.net/forum?id=pZ2Ww45GkL'>https://openreview.net/forum?id=pZ2Ww45GkL</a></p>
<p><b>Keywords</b>: program synthesis, partial envrionment, robotic programming, domain-specific language
</p><p><b>Compressor summary</b>: The paper presents a framework for synthesizing programs for robots in partially observed environments by learning an embedding space and using graph structure to rectify errors.</p><hr><h3>The Best of Both Worlds in Network Population Games: Reaching Consensus and Convergence to Equilibrium</h3>
<p>Shuyue Hu, Harold Soh, Georgios Piliouras</p>
<p><a href='https://openreview.net/forum?id=pXtVyj4R33'>https://openreview.net/forum?id=pXtVyj4R33</a></p>
<p><b>Keywords</b>: Multi-Agent Learning, Consensus Formation, Smooth Fictitious Play, Network Game, Population Game
</p><p><b>Compressor summary</b>: The paper explores how consensus and equilibrium are related in multi-agent systems with multiple interacting sub-populations and shows that smooth fictitious play can achieve both goals.</p><hr><h3>Principled Weight Initialisation for Input-Convex Neural Networks</h3>
<p>Pieter-Jan Hoedt, Günter Klambauer</p>
<p><a href='https://openreview.net/forum?id=pWZ97hUQtQ'>https://openreview.net/forum?id=pWZ97hUQtQ</a></p>
<p><b>Keywords</b>: initialization, signal propagation, input-convex networks
</p><p><b>Compressor summary</b>: ICNNs are networks with guaranteed convexity that require a new weight initialization method for better learning and generalization, which can also be trained without skip-connections and applied to drug discovery tasks.</p><hr><h3>RETVec: Resilient and Efficient Text Vectorizer</h3>
<p>Elie Bursztein, Marina Zhang, Owen Skipper Vallis, Xinyu Jia, Alexey Kurakin</p>
<p><a href='https://openreview.net/forum?id=pVlC0reMKq'>https://openreview.net/forum?id=pVlC0reMKq</a></p>
<p><b>Keywords</b>: language modeling, text embedding, adversarial text attack, text vectorization
</p><p><b>Compressor summary</b>: RETVec is a robust, multilingual text vectorizer with a character encoding and an optional embedding model that can handle typos and adversarial attacks.</p><hr><h3>CorresNeRF: Image Correspondence Priors for Neural Radiance Fields</h3>
<p>Yixing Lao, Xiaogang Xu, zhipeng cai, Xihui Liu, Hengshuang Zhao</p>
<p><a href='https://openreview.net/forum?id=pTCZWSDltG'>https://openreview.net/forum?id=pTCZWSDltG</a></p>
<p><b>Keywords</b>: Neural Radiance Fields, 3D Reconstruction, Few View
</p><p><b>Compressor summary</b>: CorresNeRF uses image correspondence priors to supervise NeRF training, improving performance under sparse-view settings for novel view synthesis and surface reconstruction tasks.</p><hr><h3>Parameter-efficient Tuning of Large-scale Multimodal Foundation Model</h3>
<p>Haixin Wang, Xinlong Yang, Jianlong Chang, Dian Jin, Jinan Sun, Shikun Zhang, Xiao Luo, Qi Tian</p>
<p><a href='https://openreview.net/forum?id=pT8DIhsJCw'>https://openreview.net/forum?id=pT8DIhsJCw</a></p>
<p><b>Keywords</b>: parameter-efficient transfer learning; multi-modal learning; prompt learning
</p><p><b>Compressor summary</b>: The paper introduces AURORA, a framework for multimodal transfer learning that uses mode approximation and Informative Context Enhancement to achieve high performance with few parameters.</p><hr><h3>Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models</h3>
<p>Yichao Cao, Qingfei Tang, Xiu Su, Song Chen, Shan You, Xiaobo Lu, Chang Xu</p>
<p><a href='https://openreview.net/forum?id=pQvAL40Cdj'>https://openreview.net/forum?id=pQvAL40Cdj</a></p>
<p><b>Keywords</b>: Human-object interaction, Commonsense Knowledge, Foundation Models
</p><p><b>Compressor summary</b>: UniHOI is a method for recognizing complex human-object interactions in open world settings using vision-language foundation models and large language models to guide decoding, interpret interactions, and handle various input types.</p><hr><h3>Leveraging Vision-Centric Multi-Modal Expertise for 3D Object Detection</h3>
<p>Linyan Huang, Zhiqi Li, Chonghao Sima, Wenhai Wang, Jingdong Wang, Yu Qiao, Hongyang Li</p>
<p><a href='https://openreview.net/forum?id=pQF9kbM8Ea'>https://openreview.net/forum?id=pQF9kbM8Ea</a></p>
<p><b>Keywords</b>: camera-only detection, multi-modal distillation, multi-view object detection
</p><p><b>Compressor summary</b>: The paper proposes VCD, a framework that improves camera-only 3D object detection by using a multi-modal expert with LiDAR input and trajectory-based distillation supervision, achieving state-of-the-art results on nuScenes.</p><hr><h3>ESSEN: Improving Evolution State Estimation for Temporal Networks using Von Neumann Entropy</h3>
<p>Qiyao Huang, Yingyue Zhang, Zhihong Zhang, Edwin Hancock</p>
<p><a href='https://openreview.net/forum?id=pO7d6iFdnc'>https://openreview.net/forum?id=pO7d6iFdnc</a></p>
<p><b>Keywords</b>: Temporal Network, Graph Neural Network, Von Neumann Entropy
</p><p><b>Compressor summary</b>: ESSEN is a novel framework that uses von Neumann entropy and thermodynamic temperature to measure temporal network evolution and improve link prediction performance.</p><hr><h3>Statistical Knowledge Assessment for Large Language Models</h3>
<p>Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Zhifang Sui, Lei Li</p>
<p><a href='https://openreview.net/forum?id=pNtG6NAmx0'>https://openreview.net/forum?id=pNtG6NAmx0</a></p>
<p><b>Keywords</b>: Large Language Models, Knowledge Assessment, Evaluation
</p><p><b>Compressor summary</b>: The paper introduces KaRR, a statistical approach that quantifies how well large language models (LLMs) can generate factually correct answers for given facts, based on the ratio of LLM text to random chances. The paper tests 20 LLMs and finds strong agreement with human judgments.</p><hr><h3>PRODIGY: Enabling In-context Learning Over Graphs</h3>
<p>Qian Huang, Hongyu Ren, Peng Chen, Gregor Kržmanc, Daniel Zeng, Percy Liang, Jure Leskovec</p>
<p><a href='https://openreview.net/forum?id=pLwYhNNnoR'>https://openreview.net/forum?id=pLwYhNNnoR</a></p>
<p><b>Keywords</b>: Graph Neural Network, in-context learning, pretraining
</p><p><b>Compressor summary</b>: PRODIGY is a novel framework that enables in-context learning over graphs using a prompt graph representation and a graph neural network architecture, achieving strong performance on citation networks and knowledge graphs tasks.</p><hr><h3>Kernelized Cumulants: Beyond Kernel Mean Embeddings</h3>
<p>Patric Bonnier, Harald Oberhauser, Zoltán Szabó</p>
<p><a href='https://openreview.net/forum?id=pLsPFxqn7J'>https://openreview.net/forum?id=pLsPFxqn7J</a></p>
<p><b>Keywords</b>: kernel, cumulant, mean embedding, Hilbert-Schmidt independence criterion, maximum mean discrepancy
</p><p><b>Compressor summary</b>: The paper introduces kernelized cumulants for reproducing kernel Hilbert spaces, which are computationally efficient and provide new all-purpose statistics for data analysis.</p><hr><h3>An active learning framework for multi-group mean estimation</h3>
<p>Abdellah Aznag, Rachel Cummings, Adam N. Elmachtoub</p>
<p><a href='https://openreview.net/forum?id=pLcSrn8NpJ'>https://openreview.net/forum?id=pLcSrn8NpJ</a></p>
<p><b>Keywords</b>: Active learning, mean estimation, bandit feedback, data acquisition
</p><p><b>Compressor summary</b>: The paper proposes an active learning algorithm called Variance-UCB to learn the means of multiple unknown groups by minimizing the $p$-norm of variances, with optimal regret bounds for finite and infinite $p$.</p><hr><h3>Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization</h3>
<p>Quanqi Hu, Dixian Zhu, Tianbao Yang</p>
<p><a href='https://openreview.net/forum?id=pLOWV1UGF6'>https://openreview.net/forum?id=pLOWV1UGF6</a></p>
<p><b>Keywords</b>: non-smooth optimization, weakly-convex optimization, compositional optimization, AUC maximization
</p><p><b>Compressor summary</b>: The paper studies non-smooth weakly-convex finite-sum coupled compositional optimization problems and proposes algorithms for solving them in machine learning applications.</p><hr><h3>PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks</h3>
<p>Ian Char, Jeff Schneider</p>
<p><a href='https://openreview.net/forum?id=pKnhUWqZTJ'>https://openreview.net/forum?id=pKnhUWqZTJ</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Control, POMDP
</p><p><b>Compressor summary</b>: The paragraph discusses using PID features for history encoding in deep reinforcement learning to improve robustness and performance on various control tasks.</p><hr><h3>Adversarial Examples Exist in Two-Layer ReLU Networks for Low Dimensional Linear Subspaces</h3>
<p>Odelia Melamed, Gilad Yehudai, Gal Vardi</p>
<p><a href='https://openreview.net/forum?id=pJbEXBBN88'>https://openreview.net/forum?id=pJbEXBBN88</a></p>
<p><b>Keywords</b>: Adversarial Examples, Robustness, Neural Networks, Classification
</p><p><b>Compressor summary</b>: This paper studies how gradient methods affect the robustness of two-layer neural networks trained on low dimensional linear subspaces and suggests ways to improve their resistance to adversarial examples in certain directions.</p><hr><h3>Optimal Exploration for Model-Based RL in Nonlinear Systems</h3>
<p>Andrew Wagenmaker, Guanya Shi, Kevin Jamieson</p>
<p><a href='https://openreview.net/forum?id=pJQu0zpKCS'>https://openreview.net/forum?id=pJQu0zpKCS</a></p>
<p><b>Keywords</b>: reinforcement learning, control theory, system identification, experiment design, active learning
</p><p><b>Compressor summary</b>: The paper proposes an algorithm for exploring nonlinear dynamical systems to learn relevant parameters for controller optimization more efficiently.</p><hr><h3>What Makes Good Examples for Visual In-Context Learning?</h3>
<p>Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu</p>
<p><a href='https://openreview.net/forum?id=pIXTMrBe7f'>https://openreview.net/forum?id=pIXTMrBe7f</a></p>
<p><b>Keywords</b>: computer vision, visual in-context learning, prompt learning
</p><p><b>Compressor summary</b>: This paper proposes prompt retrieval frameworks for large vision models to improve their performance in in-context learning by selecting appropriate input-output pairs without updating any internal model parameters.</p><hr><h3>Causal Effect Identification in Uncertain Causal Networks</h3>
<p>Sina Akbari, Fateme Jamshidi, Ehsan Mokhtarian, Matthew James Vowels, Jalal Etesami, Negar Kiyavash</p>
<p><a href='https://openreview.net/forum?id=pH4Fv7C3yC'>https://openreview.net/forum?id=pH4Fv7C3yC</a></p>
<p><b>Keywords</b>: Causal effect, identifiability, causal DAGs, probabilistic graphs
</p><p><b>Compressor summary</b>: The paper studies how to find the most plausible probabilistic causal graph that can identify a specific causal effect, using algorithms that solve an NP-hard optimization problem called edge ID.</p><hr><h3>FlatMatch: Bridging Labeled Data and Unlabeled Data with Cross-Sharpness for Semi-Supervised Learning</h3>
<p>Zhuo Huang, Li Shen, Jun Yu, Bo Han, Tongliang Liu</p>
<p><a href='https://openreview.net/forum?id=pE3yaP0Eqg'>https://openreview.net/forum?id=pE3yaP0Eqg</a></p>
<p><b>Keywords</b>: Semi-Supervised Learning
</p><p><b>Compressor summary</b>: FlatMatch is a new SSL method that balances learning on labeled and unlabeled data by penalizing the prediction difference between worst-case and original models, improving generalization performance.</p><hr><h3>Latent Space Translation via Semantic Alignment</h3>
<p>Valentino Maiorca, Luca Moschella, Antonio Norelli, Marco Fumero, Francesco Locatello, Emanuele Rodolà</p>
<p><a href='https://openreview.net/forum?id=pBa70rGHlr'>https://openreview.net/forum?id=pBa70rGHlr</a></p>
<p><b>Keywords</b>: latent space translation, relative representation, Procrustes analysis, zero-shot, stitching, latent communication, representation learning, manifold alignment, multimodal
</p><p><b>Compressor summary</b>: The paper proposes a method to translate and stitch representations learned from different neural networks, enabling better understanding of their intrinsic similarity and improving performance in various tasks and domains.</p><hr><h3>Demystifying the Optimal Performance of Multi-Class Classification</h3>
<p>Minoh Jeong, Martina Cardone, Alex Dytso</p>
<p><a href='https://openreview.net/forum?id=p9k5MS0JAL'>https://openreview.net/forum?id=p9k5MS0JAL</a></p>
<p><b>Keywords</b>: Bayes error, estimation, classification, minimum error probability
</p><p><b>Compressor summary</b>: The authors propose a new method to estimate the Bayes error rate for multi-class classification problems and analyze its properties and performance on synthetic and real data.</p><hr><h3>From Trainable Negative Depth to Edge Heterophily in Graphs</h3>
<p>Yuchen Yan, Yuzhong Chen, Huiyuan Chen, Minghua Xu, Mahashweta Das, Hao Yang, Hanghang Tong</p>
<p><a href='https://openreview.net/forum?id=p8lowHbuv8'>https://openreview.net/forum?id=p8lowHbuv8</a></p>
<p><b>Keywords</b>: graph convolutional network
</p><p><b>Compressor summary</b>: The paper proposes a new GCN model that can adjust its depth continuously to handle both homophilic and heterophilic graph topologies, and shows improved performance on node classification tasks.</p><hr><h3>Modality-Independent Teachers Meet Weakly-Supervised Audio-Visual Event Parser</h3>
<p>Yung-Hsuan Lai, Yen-Chun Chen, Yu-Chiang Frank Wang</p>
<p><a href='https://openreview.net/forum?id=p8gTWkFIvx'>https://openreview.net/forum?id=p8gTWkFIvx</a></p>
<p><b>Keywords</b>: Audio-Visual Video Parsing, Audio-Visual Learning
</p><p><b>Compressor summary</b>: The paper introduces VALOR, a method that uses pre-trained models as teachers to help learn audio and visual labels from weak labels in an unaligned setting, improving performance on the Look, Listen, and Parse dataset and Audio-Visual Event Localization tasks.</p><hr><h3>DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction</h3>
<p>Mohammadreza Pourreza, Davood Rafiei</p>
<p><a href='https://openreview.net/forum?id=p53QDxSIc5'>https://openreview.net/forum?id=p53QDxSIc5</a></p>
<p><b>Keywords</b>: In-Context Learning, Text-to-SQL, Task Decomposition, Spider Challenge, Natural Language Interfaces to Databases
</p><p><b>Compressor summary</b>: The paper proposes decomposing the text-to-SQL task into smaller sub-tasks and using LLMs to solve them, leading to improved performance and setting new benchmarks.</p><hr><h3>Riemannian stochastic optimization methods avoid strict saddle points</h3>
<p>Ya-Ping Hsieh, Mohammad Reza Karimi Jaghargh, Andreas Krause, Panayotis Mertikopoulos</p>
<p><a href='https://openreview.net/forum?id=p4SjKPchJy'>https://openreview.net/forum?id=p4SjKPchJy</a></p>
<p><b>Keywords</b>: Riemannian optimization, saddle points, stochastic approximation
</p><p><b>Compressor summary</b>: The paper investigates if retraction-based stochastic Riemannian optimization algorithms can avoid saddle points with certain assumptions, ensuring the convergence to a local minimizer.</p><hr><h3>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model</h3>
<p>Michael Hanna, Ollie Liu, Alexandre Variengien</p>
<p><a href='https://openreview.net/forum?id=p4PckNQR8k'>https://openreview.net/forum?id=p4PckNQR8k</a></p>
<p><b>Keywords</b>: interpretability, language models, NLP
</p><p><b>Compressor summary</b>: The paper examines how pre-trained language models like GPT-2 small use basic math skills and identifies the specific components and mechanisms involved in solving a simple mathematical task.</p><hr><h3>Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision</h3>
<p>Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Daniel Cox, Yiming Yang, Chuang Gan</p>
<p><a href='https://openreview.net/forum?id=p40XRfBX96'>https://openreview.net/forum?id=p40XRfBX96</a></p>
<p><b>Keywords</b>: AI Alignment, Large Language Models, In Context Learning, Neural Symbolics
</p><p><b>Compressor summary</b>: The proposed SELF-ALIGN approach enables AI assistant agents to align themselves with human intentions using minimal human supervision by combining LLMs and principle-driven reasoning.</p><hr><h3>FlowPG: Action-constrained Policy Gradient with Normalizing Flows</h3>
<p>Janaka Chathuranga Brahmanage, Jiajing Ling, Akshat Kumar</p>
<p><a href='https://openreview.net/forum?id=p1gzxzJ4Y5'>https://openreview.net/forum?id=p1gzxzJ4Y5</a></p>
<p><b>Keywords</b>: action-constrained reinforcement learning, decision making
</p><p><b>Compressor summary</b>: ACRL uses a normalizing flow model to learn an invertible mapping between feasible actions and latent variables, enabling faster and more accurate action sampling and constraint satisfaction in DDPG-based RL problems.</p><hr><h3>Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training</h3>
<p>Yefan Zhou, Tianyu Pang, Keqin Liu, charles h martin, Michael W. Mahoney, Yaoqing Yang</p>
<p><a href='https://openreview.net/forum?id=oyV9FslE3j'>https://openreview.net/forum?id=oyV9FslE3j</a></p>
<p><b>Keywords</b>: Heavy-tail self-regularization, learning rate schedule
</p><p><b>Compressor summary</b>: The paper introduces TempBalance, a layer-wise learning rate method for neural network training, which uses Heavy-Tailed Self-Regularization Theory to balance the temperature across layers and improve performance on various datasets.</p><hr><h3>Language-based Action Concept Spaces Improve Video Self-Supervised Learning</h3>
<p>Kanchana Ranasinghe, Michael S Ryoo</p>
<p><a href='https://openreview.net/forum?id=oyFyOPZUCs'>https://openreview.net/forum?id=oyFyOPZUCs</a></p>
<p><b>Keywords</b>: self-supervised learning for videos, zero-shot action recognition
</p><p><b>Compressor summary</b>: The paper proposes a method to adapt image CLIP models to the video domain using language tied self-supervised learning, which improves action recognition performance on benchmarks.</p><hr><h3>MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition</h3>
<p>Nicolas Menet, Michael Hersche, Geethan Karunaratne, Luca Benini, Abu Sebastian, Abbas Rahimi</p>
<p><a href='https://openreview.net/forum?id=ox7aynitoW'>https://openreview.net/forum?id=ox7aynitoW</a></p>
<p><b>Keywords</b>: Computation in superposition, Vector-symbolic architectures, Convolutional neural networks, Transformers
</p><p><b>Compressor summary</b>: The authors propose MIMONets, which can process multiple inputs at once using variable binding and unbinding. This enables computation in superposition, improving inference efficiency and performance-accuracy trade-offs for CNN and Transformer models. They also provide theoretical bounds on the interference between superposition channels.</p><hr><h3>On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions</h3>
<p>Yufeng Zhang, Jialu Pan, Kenli Li, Wanwei Liu, Zhenbang Chen, Xinwang Liu, J Wang</p>
<p><a href='https://openreview.net/forum?id=ouLe91yibj'>https://openreview.net/forum?id=ouLe91yibj</a></p>
<p><b>Keywords</b>: Kullback-Leibler divergence, statistical divergence, multivariate Gaussian distribution, mathematical optimization, Lambert $W$ function, machine learning, flow-based model, reinforcement learning
</p><p><b>Compressor summary</b>: The paper studies properties of KL divergence between multivariate Gaussian distributions, deriving bounds for different scenarios and showing that it follows a relaxed triangle inequality.</p><hr><h3>Linear Time Algorithms for k-means with Multi-Swap Local Search</h3>
<p>Junyu Huang, Qilong Feng, Ziyun Huang, Jinhui Xu, Jianxin Wang</p>
<p><a href='https://openreview.net/forum?id=oss2jXD1Zs'>https://openreview.net/forum?id=oss2jXD1Zs</a></p>
<p><b>Keywords</b>: Approximation Algorithms, k-means Clustering, Local Search
</p><p><b>Compressor summary</b>: The paper proposes a multi-swap local search algorithm for the $k$-means problem with linear running time and improved approximation ratio, and also introduces a sampling-based method and a recombination mechanism to enhance the performance.</p><hr><h3>Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning</h3>
<p>Cuong Pham, Cuong C. Nguyen, Trung Le, Dinh Phung, Gustavo Carneiro, Thanh-Toan Do</p>
<p><a href='https://openreview.net/forum?id=os2BdbiGwX'>https://openreview.net/forum?id=os2BdbiGwX</a></p>
<p><b>Keywords</b>: Bayesian Neural Networks, Deep Mutual Learning
</p><p><b>Compressor summary</b>: The paper proposes a deep mutual learning approach that increases diversity in parameter and feature distributions of Bayesian Neural Networks, leading to significant performance improvements in classification accuracy, negative log-likelihood, and expected calibration error.</p><hr><h3>Bypassing the Simulator: Near-Optimal Adversarial Linear Contextual Bandits</h3>
<p>Haolin Liu, Chen-Yu Wei, Julian Zimmert</p>
<p><a href='https://openreview.net/forum?id=orh4e0AO9R'>https://openreview.net/forum?id=orh4e0AO9R</a></p>
<p><b>Keywords</b>: adversarial linear contextual bandits, log-determinant barrier
</p><p><b>Compressor summary</b>: The paper proposes a new method for solving the adversarial linear contextual bandit problem that achieves lower regret and maintains computational efficiency in the presence of stochastic arm availability and additive misspecification errors.</p><hr><h3>Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds</h3>
<p>Ziqiao Wang, Yongyi Mao</p>
<p><a href='https://openreview.net/forum?id=oqDSDKLd3S'>https://openreview.net/forum?id=oqDSDKLd3S</a></p>
<p><b>Keywords</b>: generalization, information-theoretic bounds, stability
</p><p><b>Compressor summary</b>: The authors propose a new way to measure generalization in machine learning using a matrix and a stability concept that improve existing bounds, especially for stochastic convex optimization problems.</p><hr><h3>Explore In-Context Learning for 3D Point Cloud Understanding</h3>
<p>Zhongbin Fang, Xiangtai Li, Xia Li, Joachim M. Buhmann, Chen Change Loy, Mengyuan Liu</p>
<p><a href='https://openreview.net/forum?id=ooXpTZYwXa'>https://openreview.net/forum?id=ooXpTZYwXa</a></p>
<p><b>Keywords</b>: In-context learning, Point cloud, Prompt tuning
</p><p><b>Compressor summary</b>: The paper introduces Point-In-Context, a novel framework for in-context learning in 3D point clouds, which addresses technical challenges and outperforms individual models.</p><hr><h3>Multi-Fidelity Multi-Armed Bandits Revisited</h3>
<p>Xuchuang Wang, Qingyun Wu, Wei Chen, John C.S. Lui</p>
<p><a href='https://openreview.net/forum?id=oi45JlpSOT'>https://openreview.net/forum?id=oi45JlpSOT</a></p>
<p><b>Keywords</b>: Multi-fidelity, multi-armed bandits
</p><p><b>Compressor summary</b>: The paper investigates the multi-fidelity multi-armed bandit problem, where each arm has different costs and observation accuracies, and proposes algorithms for best arm identification and regret minimization.</p><hr><h3>Successor-Predecessor Intrinsic Exploration</h3>
<p>Changmin Yu, Neil Burgess, Maneesh Sahani, Samuel Gershman</p>
<p><a href='https://openreview.net/forum?id=ohKbQp0jIY'>https://openreview.net/forum?id=ohKbQp0jIY</a></p>
<p><b>Keywords</b>: Exploration, reinforcement learning
</p><p><b>Compressor summary</b>: The paper introduces SPIE, an exploration algorithm for reinforcement learning that combines prospective and retrospective information to generate more efficient and ethologically plausible behavior in environments with sparse rewards and bottleneck states.</p><hr><h3>Disentangling Cognitive Diagnosis with Limited Exercise Labels</h3>
<p>Xiangzhi Chen, Le Wu, Fei Liu, Lei Chen, Kun Zhang, Richang Hong, Meng Wang</p>
<p><a href='https://openreview.net/forum?id=ogPBujRhiN'>https://openreview.net/forum?id=ogPBujRhiN</a></p>
<p><b>Keywords</b>: Intelligent Education System, Cognitive Diagnosis, Disentangled Representation Learning, Interpretability
</p><p><b>Compressor summary</b>: The paper proposes a new method called Disentanglement based Cognitive Diagnosis (DCD) for measuring students' proficiency in specific knowledge concepts with limited exercise labels, using response records and two novel modules.</p><hr><h3>Learning Efficient Coding of Natural Images with Maximum Manifold Capacity Representations</h3>
<p>Thomas Edward Yerxa, Yilun Kuang, Eero P Simoncelli, SueYeon Chung</p>
<p><a href='https://openreview.net/forum?id=og9V7NgOrQ'>https://openreview.net/forum?id=og9V7NgOrQ</a></p>
<p><b>Keywords</b>: computational neuroscience, theoretical neuroscience, efficient coding, representation geometry, neural manifolds, self-supervised learning, statistical physics of learning
</p><p><b>Compressor summary</b>: The efficient coding hypothesis suggests sensory systems adapt to their inputs for maximal information capture, but measuring this is difficult; researchers have developed Maximum Manifold Capacity Representations (MMCR) which use a novel efficiency metric and are competitive with self-supervised learning methods.</p><hr><h3>Online (Multinomial) Logistic Bandit: Improved Regret and Constant Computation Cost</h3>
<p>Yu-Jie Zhang, Masashi Sugiyama</p>
<p><a href='https://openreview.net/forum?id=ofa1U5BJVJ'>https://openreview.net/forum?id=ofa1U5BJVJ</a></p>
<p><b>Keywords</b>: Logistic Bandit, Generalized Linear Bandit, Regret Bound, Computation Cost
</p><p><b>Compressor summary</b>: The paper proposes a logistic bandit algorithm that is statistically efficient, computationally fast, and adaptable to complex decision-making problems, improving on existing binary and multinomial methods.</p><hr><h3>Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?</h3>
<p>Haitao Mao, Zhikai Chen, Wei Jin, Haoyu Han, Yao Ma, Tong Zhao, Neil Shah, Jiliang Tang</p>
<p><a href='https://openreview.net/forum?id=oef30oScVB'>https://openreview.net/forum?id=oef30oScVB</a></p>
<p><b>Keywords</b>: Graph Neural Network
</p><p><b>Compressor summary</b>: The study shows that Graph Neural Networks perform well on nodes with similar structures in homophilic graphs and heterophilic nodes in heterophilic graphs, but struggle with nodes having different structures in the opposite graph type.</p><hr><h3>FedGCN: Convergence-Communication Tradeoffs in Federated Training of Graph Convolutional Networks</h3>
<p>Yuhang Yao, Weizhao Jin, Srivatsan Ravi, Carlee Joe-Wong</p>
<p><a href='https://openreview.net/forum?id=ody3RBUuJS'>https://openreview.net/forum?id=ody3RBUuJS</a></p>
<p><b>Keywords</b>: Federated Graph Learning;
</p><p><b>Compressor summary</b>: FedGCN is a novel algorithm that trains GCN models on graphs distributed among clients using federated learning, achieving fast convergence, high accuracy, and low communication.</p><hr><h3>SQ Lower Bounds for Learning Mixtures of Linear Classifiers</h3>
<p>Ilias Diakonikolas, Daniel Kane, Yuxin Sun</p>
<p><a href='https://openreview.net/forum?id=obCNIzeSrg'>https://openreview.net/forum?id=obCNIzeSrg</a></p>
<p><b>Keywords</b>: mixtures models, linear classifier, Statistical Query model, spherical designs
</p><p><b>Compressor summary</b>: The paper studies how to learn mixtures of linear classifiers with Gaussian features and shows that current algorithms are nearly optimal using a new construction of spherical designs.</p><hr><h3>FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation</h3>
<p>Xinyu Sun, Peihao Chen, Jugang Fan, Jian Chen, Thomas H. Li, Mingkui Tan</p>
<p><a href='https://openreview.net/forum?id=oaJEB5Qcia'>https://openreview.net/forum?id=oaJEB5Qcia</a></p>
<p><b>Keywords</b>: Visual Navigation, Image-Goal Navigation, Embodied AI
</p><p><b>Compressor summary</b>: The paper proposes a Fine-grained Goal Prompting method to improve image-goal navigation for autonomous systems by using high-resolution feature maps in the goal image as prompts.</p><hr><h3>TFLEX: Temporal Feature-Logic Embedding Framework for Complex Reasoning over Temporal Knowledge Graph</h3>
<p>Xueyuan Lin, Haihong E, Chengjin Xu, Gengxian Zhou, Haoran Luo, Tianyi Hu, Fenglong Su, Ningyuan Li, Mingzhi Sun</p>
<p><a href='https://openreview.net/forum?id=oaGdsgB18L'>https://openreview.net/forum?id=oaGdsgB18L</a></p>
<p><b>Keywords</b>: Temporal Knowledge Graph Reasoning, Temporal Knowledge Graph Embedding, Temporal Knowledge Graph, Temporal Logic, Knowledge Graph Reasoning, Knowledge Graph Embedding, Knowledge Graph, Machine Learning
</p><p><b>Compressor summary</b>: TFLEX is a novel framework for multi-hop logical reasoning over temporal knowledge graphs that uses fuzzy logic to handle complex queries involving entities, timestamps, and set operations.</p><hr><h3>Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts</h3>
<p>Chaoqi Wang, Ziyu Ye, Zhe Feng, Ashwinkumar Badanidiyuru, Haifeng Xu</p>
<p><a href='https://openreview.net/forum?id=oaCDiKoJ2w'>https://openreview.net/forum?id=oaCDiKoJ2w</a></p>
<p><b>Keywords</b>: linear stochastic bandits, online learning, partial information, contextual bandits
</p><p><b>Compressor summary</b>: The paragraph discusses a novel contextual bandit problem with post-serving contexts, where additional valuable information about user rewards can be observed after arm selection, and presents poLinUCB, a new algorithm that achieves tight regret under standard assumptions using a robustified version of the Elliptical Potential Lemma.</p><hr><h3>Learning Cuts via Enumeration Oracles</h3>
<p>Daniel Thuerck, Boro Sofranac, Marc Pfetsch, Sebastian Pokutta</p>
<p><a href='https://openreview.net/forum?id=oU4QHdcIWW'>https://openreview.net/forum?id=oU4QHdcIWW</a></p>
<p><b>Keywords</b>: Integer Programming, Cutting Planes, Optimization
</p><p><b>Compressor summary</b>: The paper presents a novel method for learning facets of polyhedra by embedding an enumeration oracle in a Frank-Wolfe algorithm to generate strong cutting planes.</p><hr><h3>Softmax Output Approximation for Activation Memory-Efficient Training of Attention-based Networks</h3>
<p>Changhyeon Lee, Seulki Lee</p>
<p><a href='https://openreview.net/forum?id=oScaeIibRx'>https://openreview.net/forum?id=oScaeIibRx</a></p>
<p><b>Keywords</b>: Memory efficient, Activation saving memory, NLP, Transformer
</p><p><b>Compressor summary</b>: The paper proposes a method to reduce memory usage in attention-based networks by approximating the softmax output during training and shows that it improves performance with less memory.</p><hr><h3>Neural Harmonics: Bridging Spectral Embedding and Matrix Completion in Self-Supervised Learning</h3>
<p>Marina Munkhoeva, Ivan Oseledets</p>
<p><a href='https://openreview.net/forum?id=oSYjkJKHZx'>https://openreview.net/forum?id=oSYjkJKHZx</a></p>
<p><b>Keywords</b>: unsupervised learning, self-supervised learning, representation learning, matrix completion
</p><p><b>Compressor summary</b>: The paper explores how self-supervised learning methods use a Laplace operator and low-rank matrix completion to learn representations without labels, and analyzes their convergence and performance.</p><hr><h3>Analyzing the Sample Complexity of Self-Supervised Image Reconstruction Methods</h3>
<p>Tobit Klug, Dogukan Atik, Reinhard Heckel</p>
<p><a href='https://openreview.net/forum?id=oRn953uhFq'>https://openreview.net/forum?id=oRn953uhFq</a></p>
<p><b>Keywords</b>: image reconstruction, denoising, accelerated MRI, self-supervised, sample complexity
</p><p><b>Compressor summary</b>: The paper compares the performance and sample complexity of supervised and self-supervised methods for image reconstruction tasks, finding that self-supervised methods require more training data but eventually achieve similar results.</p><hr><h3>Activity Grammars for Temporal Action Segmentation</h3>
<p>Dayoung Gong, Joonseok Lee, Deunsol Jung, Suha Kwak, Minsu Cho</p>
<p><a href='https://openreview.net/forum?id=oOXZ5JEjPb'>https://openreview.net/forum?id=oOXZ5JEjPb</a></p>
<p><b>Keywords</b>: neuro-symbolic approach, Temporal action segmentation, grammar
</p><p><b>Compressor summary</b>: The paper introduces KARI, a grammar induction algorithm for temporal action segmentation, and BEP, a parser that transforms frame-level probabilities into actions according to the induced grammar.</p><hr><h3>Neural Fields with Hard Constraints of Arbitrary Differential Order</h3>
<p>Fangcheng Zhong, Kyle Thomas Fogarty, Param Hanji, Tianhao Walter Wu, Alejandro Sztrajman, Andrew Everett Spielberg, Andrea Tagliasacchi, Petra Bosilj, Cengiz Oztireli</p>
<p><a href='https://openreview.net/forum?id=oO1IreC6Sd'>https://openreview.net/forum?id=oO1IreC6Sd</a></p>
<p><b>Keywords</b>: neural fields, constrained optimization
</p><p><b>Compressor summary</b>: The authors present Constrained Neural Fields (CNF), a method for imposing hard constraints on neural networks during optimization, inspired by meshless interpolation and spectral collocation techniques in scientific computing.</p><hr><h3>Particle-based Variational Inference with Generalized Wasserstein Gradient Flow</h3>
<p>Ziheng Cheng, Shiyue Zhang, Longlin Yu, Cheng Zhang</p>
<p><a href='https://openreview.net/forum?id=oNuam8eFz2'>https://openreview.net/forum?id=oNuam8eFz2</a></p>
<p><b>Keywords</b>: Particle-based VI, generalized Wasserstein gradient flow
</p><p><b>Compressor summary</b>: The paper introduces GWG, a novel ParVI framework that improves on ParVIs by using a generalized Wasserstein gradient flow of the KL divergence, which allows for more flexible kernel design and stronger convergence guarantees.</p><hr><h3>Unbiased constrained sampling with Self-Concordant Barrier Hamiltonian Monte Carlo</h3>
<p>Maxence Noble, Valentin De Bortoli, Alain Durmus</p>
<p><a href='https://openreview.net/forum?id=oMm1dfo3tK'>https://openreview.net/forum?id=oMm1dfo3tK</a></p>
<p><b>Keywords</b>: Hamiltonian Monte Carlo, Riemannian manifold, self-concordant barrier, constrained sampling
</p><p><b>Compressor summary</b>: The paper introduces Barrier Hamiltonian Monte Carlo, a method for sampling from constrained Gibbs distributions using Hamiltonian dynamics and a new filter step, which overcomes bias issues in existing Riemannian HMC methods.</p><hr><h3>When Demonstrations meet Generative World Models: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning</h3>
<p>Siliang Zeng, Chenliang Li, Alfredo Garcia, Mingyi Hong</p>
<p><a href='https://openreview.net/forum?id=oML3v2cFg2'>https://openreview.net/forum?id=oML3v2cFg2</a></p>
<p><b>Keywords</b>: Inverse Reinforcement Learning, Model-based Offline Inverse Reinforcement Learning
</p><p><b>Compressor summary</b>: The paragraph introduces a bi-level optimization approach to improve offline inverse reinforcement learning by incorporating uncertainty in the expert's model of the world.</p><hr><h3>Worst-case Performance of Popular Approximate Nearest Neighbor Search Implementations: Guarantees and Limitations</h3>
<p>Piotr Indyk, Haike Xu</p>
<p><a href='https://openreview.net/forum?id=oKqaWlEfjY'>https://openreview.net/forum?id=oKqaWlEfjY</a></p>
<p><b>Keywords</b>: Nearest neighbor search; graph-based algorithms; worst-case analysis
</p><p><b>Compressor summary</b>: The paper analyzes the theoretical guarantees of graph-based approximate nearest neighbor search algorithms, finding that some variants have slow preprocessing but good performance, while others have fast preprocessing but high query time.</p><hr><h3>UniT: A Unified Look at Certified Robust Training against Text Adversarial Perturbation</h3>
<p>Muchao Ye, Ziyi Yin, Tianrong Zhang, Tianyu Du, Jinghui Chen, Ting Wang, Fenglong Ma</p>
<p><a href='https://openreview.net/forum?id=oGxE2Nvlda'>https://openreview.net/forum?id=oGxE2Nvlda</a></p>
<p><b>Keywords</b>: certified robust training, text adversarial defense
</p><p><b>Compressor summary</b>: The paragraph describes a new framework called UniT that unifies and improves existing certified robust training pipelines for text classification by working in the word embedding space and using decoupled regularization loss to enhance the base model's robustness.</p><hr><h3>Generate What You Prefer: Reshaping Sequential Recommendation via Guided Diffusion</h3>
<p>Zhengyi Yang, Jiancan Wu, Zhicai Wang, Xiang Wang, Yancheng Yuan, Xiangnan He</p>
<p><a href='https://openreview.net/forum?id=oFpBnt6bgC'>https://openreview.net/forum?id=oFpBnt6bgC</a></p>
<p><b>Keywords</b>: Sequential Recommendation, Recommendation System, Generative Model, Diffusion Model
</p><p><b>Compressor summary</b>: The text describes a new sequential recommendation method, DreamRec, that generates an oracle item based on the user's historical interactions, rather than relying on negative sampling and classification.</p><hr><h3>Gradient Informed Proximal Policy Optimization</h3>
<p>Sanghyun Son, Laura Yu Zheng, Ryan Sullivan, Yi-Ling Qiao, Ming Lin</p>
<p><a href='https://openreview.net/forum?id=oFaLc6fHSt'>https://openreview.net/forum?id=oFaLc6fHSt</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Analytic Gradient-Based Policy Learning, Proximal Policy Optimization, Differentiable Programming
</p><p><b>Compressor summary</b>: The paper introduces a new policy learning method that combines analytical gradients with PPO, using an α-policy to adjust the influence of these gradients and improve performance in different environments.</p><hr><h3>Directional diffusion models for graph representation learning</h3>
<p>Run Yang, Yuling Yang, Fan Zhou, Qiang Sun</p>
<p><a href='https://openreview.net/forum?id=oDtyJt5JLk'>https://openreview.net/forum?id=oDtyJt5JLk</a></p>
<p><b>Keywords</b>: diffusion models, graph representation learning, unsupervised learning
</p><p><b>Compressor summary</b>: The paper proposes directional diffusion models for unsupervised graph representation learning, which use data-dependent, anisotropic, and directional noises to better capture graph structures and outperform existing methods on 12 datasets.</p><hr><h3>Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective</h3>
<p>Zeyu Zhang, Yi Su, Hui Yuan, Yiran Wu, Rishab Balasubramanian, Qingyun Wu, Huazheng Wang, Mengdi Wang</p>
<p><a href='https://openreview.net/forum?id=oDcWnfZyZW'>https://openreview.net/forum?id=oDcWnfZyZW</a></p>
<p><b>Keywords</b>: learning to rank, off-policy learning, reinforcement learning, click model
</p><p><b>Compressor summary</b>: The paper proposes a method (CUOLR) for off-policy Learning to Rank that adapts to various click models using offline reinforcement learning without complex debiasing techniques or prior knowledge.</p><hr><h3>Distributional Policy Evaluation: a Maximum Entropy approach to Representation Learning</h3>
<p>Riccardo Zamboni, Alberto Maria Metelli, Marcello Restelli</p>
<p><a href='https://openreview.net/forum?id=o91in9tDEs'>https://openreview.net/forum?id=o91in9tDEs</a></p>
<p><b>Keywords</b>: reinforcement learning, distributional reinforcement learning, maximum entropy estimation, representation learning
</p><p><b>Compressor summary</b>: The paper proposes a new Max-Ent framework for evaluating policies in distributional RL, which can consider the state representation complexity and guide state space learning using progressive factorization.</p><hr><h3>Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle</h3>
<p>Chandra Sekhar Mukherjee, Pan Peng, Jiapeng Zhang</p>
<p><a href='https://openreview.net/forum?id=o7W0Zet6p3'>https://openreview.net/forum?id=o7W0Zet6p3</a></p>
<p><b>Keywords</b>: SBM, Unbalanced SBM, Spectral algorithms, Small cluster barrier
</p><p><b>Compressor summary</b>: The paper proposes an SVD-based algorithm to detect communities of varying sizes in the stochastic block model and improves upon previous results, also providing an efficient clustering algorithm with sublinear query complexity.</p><hr><h3>Regression with Cost-based Rejection</h3>
<p>Xin Cheng, Yuzhou Cao, Haobo Wang, Hongxin Wei, Bo An, Lei Feng</p>
<p><a href='https://openreview.net/forum?id=o7HckkxOZH'>https://openreview.net/forum?id=o7HckkxOZH</a></p>
<p><b>Keywords</b>: regression, rejection costs, surrogate loss
</p><p><b>Compressor summary</b>: The paper proposes a regression problem with cost-based rejection, where models reject examples based on their variance and mean squared error, and shows that this approach can improve prediction performance.</p><hr><h3>Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels</h3>
<p>Jian Chen, Ruiyi Zhang, Tong Yu, Rohan Sharma, zhiqiang xu, Tong Sun, Changyou Chen</p>
<p><a href='https://openreview.net/forum?id=o778eWSr1S'>https://openreview.net/forum?id=o778eWSr1S</a></p>
<p><b>Keywords</b>: diffusion model, label noise, retrieval augmented learning
</p><p><b>Compressor summary</b>: The paper proposes a new diffusion model that uses generative uncertainty and neighbor consistency to learn from noisy labels and achieve state-of-the-art results on real-world datasets.</p><hr><h3>Recursion in Recursion: Two-Level Nested Recursion for Length Generalization with Scalability</h3>
<p>Jishnu Ray Chowdhury, Cornelia Caragea</p>
<p><a href='https://openreview.net/forum?id=o6yTKfdnbA'>https://openreview.net/forum?id=o6yTKfdnbA</a></p>
<p><b>Keywords</b>: Recursive Neural Networks, Long Range Arena, RvNN, Long Range Sequence Modeling, Length Generalization, LRA, Structured Encoding, Inductive Bias, Hierarchical Model, Recursive Models
</p><p><b>Compressor summary</b>: RIR combines a balanced tree model with a Beam Tree RvNN inner recursion to achieve length-generalization on ListOps and scalability on long sequence tasks like LRA, outperforming Transformers.</p><hr><h3>Arbitrarily Scalable Environment Generators via Neural Cellular Automata</h3>
<p>Yulun Zhang, Matthew Christopher Fontaine, Varun Bhatt, Stefanos Nikolaidis, Jiaoyang Li</p>
<p><a href='https://openreview.net/forum?id=o6Dnt1uEyZ'>https://openreview.net/forum?id=o6Dnt1uEyZ</a></p>
<p><b>Keywords</b>: Multi-robot systems, quality diversity, automatic environment generation, neural cellular automata
</p><p><b>Compressor summary</b>: The paper proposes optimizing Neural Cellular Automata (NCA) environment generators via Quality Diversity (QD) algorithms to create arbitrarily large environments for multi-robot systems, improving their scalability and throughput.</p><hr><h3>Certifiably Robust Graph Contrastive Learning</h3>
<p>Minhua Lin, Teng Xiao, Enyan Dai, Xiang Zhang, Suhang Wang</p>
<p><a href='https://openreview.net/forum?id=o50nH0sV9x'>https://openreview.net/forum?id=o50nH0sV9x</a></p>
<p><b>Keywords</b>: Certifiable Robustness, Graph Contrastive Learning
</p><p><b>Compressor summary</b>: The paper proposes a unified criterion and a novel technique called RES to certify and enhance the robustness of Graph Contrastive Learning models against adversarial attacks, which can be proven to be effective in downstream tasks.</p><hr><h3>Causal de Finetti: On the Identification of Invariant Causal Structure in Exchangeable Data</h3>
<p>Siyuan Guo, Viktor Tóth, Bernhard Schölkopf, Ferenc Huszár</p>
<p><a href='https://openreview.net/forum?id=o4RtDFMSNL'>https://openreview.net/forum?id=o4RtDFMSNL</a></p>
<p><b>Keywords</b>: Independent Causal Mechanism, Causal Discovery, Exchangeable, Bayesian Statistics
</p><p><b>Compressor summary</b>: The text discusses how exchangeable data can help constraint-based causal discovery methods identify more accurate causal structures than i.i.d. data by using independent causal mechanism generative processes.</p><hr><h3>Identifiability Guarantees for Causal Disentanglement from Soft Interventions</h3>
<p>Jiaqi Zhang, Kristjan Greenewald, Chandler Squires, Akash Srivastava, Karthikeyan Shanmugam, Caroline Uhler</p>
<p><a href='https://openreview.net/forum?id=o16sYKHk3S'>https://openreview.net/forum?id=o16sYKHk3S</a></p>
<p><b>Keywords</b>: Causality, Identifiability, Disentanglement
</p><p><b>Compressor summary</b>: The paper proposes a method for causal disentanglement using latent variables, which can identify the causal model from unpaired observational and interventional data even when some causal variables are unobserved, and demonstrates its application to predicting combinatorial perturbation effects in genomics.</p><hr><h3>Active Observing in Continuous-time Control</h3>
<p>Samuel Holt, Alihan Hüyük, Mihaela van der Schaar</p>
<p><a href='https://openreview.net/forum?id=o0ggjFD24U'>https://openreview.net/forum?id=o0ggjFD24U</a></p>
<p><b>Keywords</b>: Sensing, Model-Based Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper introduces a new continuous-time control problem with costly observations, where irregular observation policies can improve expected utility, and presents a simple initial method to solve it.</p><hr><h3>Individual Arbitrariness and Group Fairness</h3>
<p>Carol Xuan Long, Hsiang Hsu, Wael Alghamdi, Flavio Calmon</p>
<p><a href='https://openreview.net/forum?id=nzkWhoXUpv'>https://openreview.net/forum?id=nzkWhoXUpv</a></p>
<p><b>Keywords</b>: predictive multiplicity, fairness in machine learning, Rashomon effect
</p><p><b>Compressor summary</b>: The paper discusses how group fairness and accuracy optimizations can increase predictive multiplicity, a phenomenon where similar models produce different outputs for individual samples, and proposes an ensemble algorithm to improve consistency in individual-level decision-making.</p><hr><h3>Variational Gaussian Processes with Decoupled Conditionals</h3>
<p>Xinran Zhu, Kaiwen Wu, Natalie Maus, Jacob R. Gardner, David Bindel</p>
<p><a href='https://openreview.net/forum?id=nwK8UkK3uB'>https://openreview.net/forum?id=nwK8UkK3uB</a></p>
<p><b>Keywords</b>: Gaussian processes, variational inference, variational Gaussian processes, Bayesian optimization
</p><p><b>Compressor summary</b>: The paper proposes a new way to improve variational Gaussian processes by making the predictive mean and covariance more flexible, leading to better performance on regression tasks and Bayesian optimization.</p><hr><h3>State-Action Similarity-Based Representations for Off-Policy Evaluation</h3>
<p>Brahma S Pavse, Josiah P. Hanna</p>
<p><a href='https://openreview.net/forum?id=nvX3MiQM0G'>https://openreview.net/forum?id=nvX3MiQM0G</a></p>
<p><b>Keywords</b>: reinforcement learning, off-policy evaluation, off-policy RL, representation learning, behavioral similarity metrics
</p><p><b>Compressor summary</b>: The paper proposes a new method to improve off-policy evaluation in reinforcement learning by using a learned encoder to transform the dataset before applying fitted q-evaluation, and introduces an OPE-specific state-action similarity metric to learn the encoder.</p><hr><h3>A Scale-Invariant Sorting Criterion to Find a Causal Order in Additive Noise Models</h3>
<p>Alexander Gilbert Reisach, Myriam Tami, Christof Seiler, Antoine Chambaz, Sebastian Weichwald</p>
<p><a href='https://openreview.net/forum?id=nrbR2F29vU'>https://openreview.net/forum?id=nrbR2F29vU</a></p>
<p><b>Keywords</b>: Causal Discovery, Directed Acyclic Graph, Varsortability, Additive Noise Model, Structural Causal Model, Simulation, Benchmark
</p><p><b>Compressor summary</b>: The paper studies how different parameter choices in additive noise models affect causal discovery and introduces a new method called $R^2$-SortnRegress that leverages high $R^2$ values to find causal orders.</p><hr><h3>Leave No Stone Unturned: Mine Extra Knowledge for Imbalanced Facial Expression Recognition</h3>
<p>Yuhang Zhang, Yaqi Li, lixiong Qin, Xuannan Liu, Weihong Deng</p>
<p><a href='https://openreview.net/forum?id=nrQif5tH7O'>https://openreview.net/forum?id=nrQif5tH7O</a></p>
<p><b>Keywords</b>: Facial expression recognition, imbalanced learning
</p><p><b>Compressor summary</b>: The paper proposes a novel method for facial expression recognition that extracts extra knowledge from both major and minor class samples using re-balanced attention maps and smooth labels to handle the imbalance problem.</p><hr><h3>CLIP4HOI: Towards Adapting CLIP for Practical Zero-Shot HOI Detection</h3>
<p>Yunyao Mao, Jiajun Deng, Wengang Zhou, Li Li, Yao Fang, Houqiang Li</p>
<p><a href='https://openreview.net/forum?id=nqIIWnwe73'>https://openreview.net/forum?id=nqIIWnwe73</a></p>
<p><b>Keywords</b>: human-object interaction detection, zero-shot learning, CLIP model adaptatiion
</p><p><b>Compressor summary</b>: CLIP4HOI is a novel framework for zero-shot human-object interaction detection that uses CLIP to independently identify humans and objects and adapts the model into a fine-grained classifier for proposal discrimination, improving transferability and performance on rare and unseen categories.</p><hr><h3>Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection</h3>
<p>Hezhe Qiao, Guansong Pang</p>
<p><a href='https://openreview.net/forum?id=nq4OhifyEe'>https://openreview.net/forum?id=nq4OhifyEe</a></p>
<p><b>Keywords</b>: Anomaly Detection, Graph Neural Network, Graph Anomaly Detection, One-Class Homophily, Local Node Affinity
</p><p><b>Compressor summary</b>: The paper introduces a novel anomaly scoring method for graph anomaly detection, based on local node affinity, and a tailored representation learning approach called Truncated Affinity Maximization (TAM) that leverages the one-class homophily phenomenon.</p><hr><h3>Marginal Density Ratio for Off-Policy Evaluation in Contextual Bandits</h3>
<p>Muhammad Faaiz Taufiq, Arnaud Doucet, Rob Cornish, Jean-Francois Ton</p>
<p><a href='https://openreview.net/forum?id=noyleECBam'>https://openreview.net/forum?id=noyleECBam</a></p>
<p><b>Keywords</b>: contextual bandits, variance reduction, off-policy evaluation
</p><p><b>Compressor summary</b>: The Marginal Ratio (MR) estimator is a new off-policy evaluation method for contextual bandits that reduces variance compared to existing methods like Inverse Probability Weighting and Doubly Robust, and performs well in causal inference settings.</p><hr><h3>Joint Feature and Differentiable $ k $-NN Graph Learning using Dirichlet Energy</h3>
<p>Lei Xu, Lei Chen, Rong Wang, Feiping Nie, Xuelong Li</p>
<p><a href='https://openreview.net/forum?id=noMktb4ait'>https://openreview.net/forum?id=noMktb4ait</a></p>
<p><b>Keywords</b>: Feature Selection, Differential k-NN Graph, Dirichlet Energy
</p><p><b>Compressor summary</b>: The paper proposes a deep feature selection method that uses differentiable k-NN graph learning based on Dirichlet Energy to identify important features and learn dynamic graphs, and applies Optimal Transport theory to address non-differentiability issues.</p><hr><h3>Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima</h3>
<p>Dongkuk Si, Chulhee Yun</p>
<p><a href='https://openreview.net/forum?id=nijJN0LHqM'>https://openreview.net/forum?id=nijJN0LHqM</a></p>
<p><b>Keywords</b>: Sharpness-Aware Minimization, convex optimization
</p><p><b>Compressor summary</b>: The paper studies how varying the parameters in an optimizer called SAM affects its convergence properties on smooth and non-smooth functions.</p><hr><h3>Beyond Pretrained Features: Noisy Image Modeling Provides Adversarial Defense</h3>
<p>Zunzhi You, Daochang Liu, Bohyung Han, Chang Xu</p>
<p><a href='https://openreview.net/forum?id=niHkj9ixUZ'>https://openreview.net/forum?id=niHkj9ixUZ</a></p>
<p><b>Keywords</b>: self-supervised learning, adversarial robustness
</p><p><b>Compressor summary</b>: The paper proposes De^3, a method that uses noisy image modeling to improve adversarial robustness of deep neural networks by exploiting the pretrained decoder for denoising and sampling the noise scale hyperparameter from random distributions.</p><hr><h3>Post-processing Private Synthetic Data for Improving Utility on Selected Measures</h3>
<p>Hao Wang, Shivchander Sudalairaj, John Henning, Kristjan Greenewald, Akash Srivastava</p>
<p><a href='https://openreview.net/forum?id=neu9JlNweE'>https://openreview.net/forum?id=neu9JlNweE</a></p>
<p><b>Keywords</b>: differential privacy, synthetic data
</p><p><b>Compressor summary</b>: The paper introduces a post-processing technique to improve the utility of synthetic data for specific end-user requirements while maintaining privacy and quality, by resampling and optimizing weights using efficient algorithms.</p><hr><h3>Symbolic Discovery of Optimization Algorithms</h3>
<p>Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V Le</p>
<p><a href='https://openreview.net/forum?id=ne6zeqLFCZ'>https://openreview.net/forum?id=ne6zeqLFCZ</a></p>
<p><b>Keywords</b>: AutoML
</p><p><b>Compressor summary</b>: The authors propose a method to discover optimization algorithms for deep neural network training and introduce Lion, an efficient and effective optimizer that performs well on various tasks.</p><hr><h3>Learning Curves for Deep Structured Gaussian Feature Models</h3>
<p>Jacob A Zavatone-Veth, Cengiz Pehlevan</p>
<p><a href='https://openreview.net/forum?id=nbG6zfJtIe'>https://openreview.net/forum?id=nbG6zfJtIe</a></p>
<p><b>Keywords</b>: random feature models, generalization, deep networks, ridge regression
</p><p><b>Compressor summary</b>: The study examines how weight anisotropy and structured Gaussian features impact generalization in deep learning models using the replica trick from statistical physics.</p><hr><h3>Generating Behaviorally Diverse Policies with Latent Diffusion Models</h3>
<p>Shashank Hegde, Sumeet Batra, K.R. Zentner, Gaurav S. Sukhatme</p>
<p><a href='https://openreview.net/forum?id=nafgeYknRT'>https://openreview.net/forum?id=nafgeYknRT</a></p>
<p><b>Keywords</b>: Latent Diffusion, Quality Diversity, Reinforcement Learning, Graph Neural Networks
</p><p><b>Compressor summary</b>: The paper proposes using diffusion models to compress a collection of diverse policies into one generative model that can select and sequence behaviors with language.</p><hr><h3>Uncertainty Quantification via Neural Posterior Principal Components</h3>
<p>Elias Nehme, Omer Yair, Tomer Michaeli</p>
<p><a href='https://openreview.net/forum?id=nZ0jnXizyR'>https://openreview.net/forum?id=nZ0jnXizyR</a></p>
<p><b>Keywords</b>: Uncertainty Quantification, Inverse Problems, Probabilistic Modelling, Principal Components Analysis, Deep Learning
</p><p><b>Compressor summary</b>: This paper proposes a fast method to predict the principal components of the posterior distribution for image restoration models, enabling better uncertainty quantification and visualization.</p><hr><h3>Regret Matching+: (In)Stability and Fast Convergence in Games</h3>
<p>Gabriele Farina, Julien Grand-Clément, Christian Kroer, Chung-Wei Lee, Haipeng Luo</p>
<p><a href='https://openreview.net/forum?id=nYgs0qZJ97'>https://openreview.net/forum?id=nYgs0qZJ97</a></p>
<p><b>Keywords</b>: Regret Matching, Predictive algorithms, Extensive-Form Games
</p><p><b>Compressor summary</b>: The paper investigates the instability of regret matching algorithms, proposes fixes, and demonstrates their effectiveness in various game scenarios using theory and experiments.</p><hr><h3>Mitigating Source Bias for Fairer Weak Supervision</h3>
<p>Changho Shin, Sonia Cromp, Dyah Adila, Frederic Sala</p>
<p><a href='https://openreview.net/forum?id=nXPqMyWUnx'>https://openreview.net/forum?id=nXPqMyWUnx</a></p>
<p><b>Keywords</b>: Weak supervision, fairness
</p><p><b>Compressor summary</b>: Weak supervision can produce biased pseudolabels, and a fairness-based technique can improve accuracy and reduce bias in weak supervision without tradeoffs.</p><hr><h3>Aggregating Capacity in FL through Successive Layer Training for Computationally-Constrained Devices</h3>
<p>Kilian Pfeiffer, Ramin Khalili, Joerg Henkel</p>
<p><a href='https://openreview.net/forum?id=nXNsqB4Yr1'>https://openreview.net/forum?id=nXNsqB4Yr1</a></p>
<p><b>Keywords</b>: Federated Learning, Memory, Resource Constraints
</p><p><b>Compressor summary</b>: The paper proposes a new method for federated learning on edge devices that reduces resource requirements and improves model accuracy by enabling successive freezing and training of model parameters.</p><hr><h3>FedGame: A Game-Theoretic Defense against Backdoor Attacks in Federated Learning</h3>
<p>Jinyuan Jia, Zhuowen Yuan, Dinuka Sahabandu, Luyao Niu, Arezoo Rajabi, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran</p>
<p><a href='https://openreview.net/forum?id=nX0zYBGEka'>https://openreview.net/forum?id=nX0zYBGEka</a></p>
<p><b>Keywords</b>: backdoor defense, federated learning, game theory
</p><p><b>Compressor summary</b>: Federated learning faces backdoor attacks from dynamic attackers, and the paper proposes FedGame, a minimax game-based interactive defense mechanism that enhances robustness against such attacks.</p><hr><h3>RegBN: Batch Normalization of Multimodal Data with Regularization</h3>
<p>MORTEZA GHAHREMANI, Christian Wachinger</p>
<p><a href='https://openreview.net/forum?id=nUbdkXqC8R'>https://openreview.net/forum?id=nUbdkXqC8R</a></p>
<p><b>Keywords</b>: Multimodal Data, Multimodality, Batch Normalization, Heterogeneous data, Regularization, Confounder, Confounding Effect Removal, Data Dependency
</p><p><b>Compressor summary</b>: The paper introduces RegBN, a novel approach for multimodal Batch Normalization with REGularization, which uses the Frobenius norm as a regularizer term to address confounders and dependencies among different data sources, simplifying training and inference, and improving performance across diverse modalities and architectures.</p><hr><h3>Robust Matrix Sensing in the Semi-Random Model</h3>
<p>Xing Gao, Yu Cheng</p>
<p><a href='https://openreview.net/forum?id=nSr2epejn2'>https://openreview.net/forum?id=nSr2epejn2</a></p>
<p><b>Keywords</b>: Matrix sensing, Optimization, Low-rank matrix recovery, Semi-random, Adversarial input, Robustness
</p><p><b>Compressor summary</b>: The paper proposes a new non-convex optimization algorithm for semi-random matrix sensing that avoids bad local optima by reweighting the input matrices based on the current solution and takes weighted gradient steps.</p><hr><h3>Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape Matching via Unsupervised Functional Map Regularized Reconstruction</h3>
<p>Souhaib Attaiki, Maks Ovsjanikov</p>
<p><a href='https://openreview.net/forum?id=nSgMh5v5Ne'>https://openreview.net/forum?id=nSgMh5v5Ne</a></p>
<p><b>Keywords</b>: shape matching
</p><p><b>Compressor summary</b>: Shape Non-rigid Kinematics (SNK) is a new method for matching shapes that requires no training or ground truth data, and uses an encoder-decoder architecture to deform the source shape to match the target shape.</p><hr><h3>Differentiable Clustering with Perturbed Spanning Forests</h3>
<p>Lawrence Stewart, Francis Bach, Felipe Llinares-López, Quentin Berthet</p>
<p><a href='https://openreview.net/forum?id=nRfcVBsF9n'>https://openreview.net/forum?id=nRfcVBsF9n</a></p>
<p><b>Keywords</b>: Structured learning, Clustering, Differentiable, weakly supervised, semi-supervised, representation learning
</p><p><b>Compressor summary</b>: A new method for clustering data that can be trained end-to-end with efficient gradients and works well in noisy and complex settings is presented, along with a custom loss function for learning from partial clustering data.</p><hr><h3>Interpretability at Scale: Identifying Causal Mechanisms in Alpaca</h3>
<p>Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, Noah Goodman</p>
<p><a href='https://openreview.net/forum?id=nRfClnMhVX'>https://openreview.net/forum?id=nRfClnMhVX</a></p>
<p><b>Keywords</b>: Mechanistic Interpretability
</p><p><b>Compressor summary</b>: The paper introduces Boundless DAS, an improved method to find interpretable causal structures in large language models, and applies it to the Alpaca model, revealing its use of two boolean variables for numerical reasoning.</p><hr><h3>Multiclass Boosting: Simple and Intuitive Weak Learning Criteria</h3>
<p>Nataly Brukhim, Amit Daniely, Yishay Mansour, Shay Moran</p>
<p><a href='https://openreview.net/forum?id=nQ84YY9Iut'>https://openreview.net/forum?id=nQ84YY9Iut</a></p>
<p><b>Keywords</b>: Boosting, Multiclass classification, PAC Learning, List PAC Learning
</p><p><b>Compressor summary</b>: The paper presents a new boosting method for multiclass classification that generalizes weak learnability, has low sample complexity, and simplifies theoretical applications in list learning.</p><hr><h3>Eliminating Domain Bias for Federated Learning in Representation Space</h3>
<p>Jianqing Zhang, Yang Hua, Jian Cao, Hao Wang, Tao Song, Zhengui XUE, Ruhui Ma, Haibing Guan</p>
<p><a href='https://openreview.net/forum?id=nO5i1XdUS0'>https://openreview.net/forum?id=nO5i1XdUS0</a></p>
<p><b>Keywords</b>: Federated Learning, Personalized Federated Learning, Representation, Knowledge Transfer
</p><p><b>Compressor summary</b>: The paper introduces a framework called Domain Bias Eliminator (DBE) to address representation degeneration in federated learning and improve generalization and personalization abilities.</p><hr><h3>Memory Efficient Optimizers with 4-bit States</h3>
<p>Bingrui Li, Jianfei Chen, Jun Zhu</p>
<p><a href='https://openreview.net/forum?id=nN8TnHB5nw'>https://openreview.net/forum?id=nN8TnHB5nw</a></p>
<p><b>Keywords</b>: memory efficiency, optimizer, Adam, quantization
</p><p><b>Compressor summary</b>: The paper proposes 4-bit quantization of optimizer states for neural networks, achieving memory efficiency without sacrificing accuracy on various tasks.</p><hr><h3>GPT-ST: Generative Pre-Training of Spatio-Temporal Graph Neural Networks</h3>
<p>Zhonghang Li, Lianghao Xia, Yong Xu, Chao Huang</p>
<p><a href='https://openreview.net/forum?id=nMH5cUaSj8'>https://openreview.net/forum?id=nMH5cUaSj8</a></p>
<p><b>Keywords</b>: Spatial Temporal Prediction, Deep Neural Networks, Pre-training Model
</p><p><b>Compressor summary</b>: The authors propose a spatio-temporal pre-training framework that enhances the performance of downstream models for traffic management and travel planning by learning customized representations and adaptive mask strategies.</p><hr><h3>Provably Efficient Algorithm for Nonstationary Low-Rank MDPs</h3>
<p>Yuan Cheng, Jing Yang, Yingbin Liang</p>
<p><a href='https://openreview.net/forum?id=nMB41QjLDY'>https://openreview.net/forum?id=nMB41QjLDY</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Nonstationary Environment, Representation Learning, Policy Optimization, Statistical Complexity
</p><p><b>Compressor summary</b>: The paper proposes two algorithms, PORTAL and Ada-PORTAL, for reinforcement learning in nonstationary low-rank Markov Decision Processes, with theoretical guarantees on their sample efficiency and suboptimality gap.</p><hr><h3>A Framework for Fast and Stable Representations of Multiparameter Persistent Homology Decompositions</h3>
<p>David Loiseaux, Mathieu Carrière, Andrew Blumberg</p>
<p><a href='https://openreview.net/forum?id=nKCUDd9GYu'>https://openreview.net/forum?id=nKCUDd9GYu</a></p>
<p><b>Keywords</b>: Topological Data Analysis, Multiparameter Persistent Homology, Kernel Methods, Convergence Rate, Statistical Learning
</p><p><b>Compressor summary</b>: The article proposes a new representation framework for multiparameter persistent homology in topological data analysis, which is stable, efficient, and informative, and shows its effectiveness on geometric and point cloud data.</p><hr><h3>Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts</h3>
<p>Gleb Bazhenov, Denis Kuznedelev, Andrey Malinin, Artem Babenko, Liudmila Prokhorenkova</p>
<p><a href='https://openreview.net/forum?id=nJFJcgjnGo'>https://openreview.net/forum?id=nJFJcgjnGo</a></p>
<p><b>Keywords</b>: graph, distributional shift, structural shift, uncertainty, robustness, graph neural networks
</p><p><b>Compressor summary</b>: The authors propose a method to create diverse distributional shifts in graph learning based on node structure, evaluate its difficulty, and show that simple models often perform better than complex ones.</p><hr><h3>PromptRestorer: A Prompting Image Restoration Method with Degradation Perception</h3>
<p>Cong Wang, Jinshan Pan, Wei Wang, Jiangxin Dong, Mengzhu Wang, Yakun Ju, Junyang Chen</p>
<p><a href='https://openreview.net/forum?id=nIaNgaQvsV'>https://openreview.net/forum?id=nIaNgaQvsV</a></p>
<p><b>Keywords</b>: Degradation Vanishing, Prompting Learning, Image Restoration
</p><p><b>Compressor summary</b>: PromptRestorer is a novel image restoration model that uses degradation features to guide the restoration process and achieves state-of-the-art results on 4 tasks.</p><hr><h3>$H$-Consistency Bounds: Characterization and Extensions</h3>
<p>Anqi Mao, Mehryar Mohri, Yutao Zhong</p>
<p><a href='https://openreview.net/forum?id=nI7EmXq2PL'>https://openreview.net/forum?id=nI7EmXq2PL</a></p>
<p><b>Keywords</b>: consistency, H-consistency, characterization, learning theory
</p><p><b>Compressor summary</b>: The paper introduces general characterizations and extensions of $H$-consistency bounds for surrogate losses in multi-class classification, covering constrained and comp-sum losses, and providing tighter bounds than previous studies.</p><hr><h3>What Truly Matters in Trajectory Prediction for Autonomous Driving?</h3>
<p>Tran Phong, Haoran Wu, Cunjun Yu, Panpan Cai, Sifa Zheng, David Hsu</p>
<p><a href='https://openreview.net/forum?id=nG35q8pNL9'>https://openreview.net/forum?id=nG35q8pNL9</a></p>
<p><b>Keywords</b>: trajectory prediction; autonomous driving
</p><p><b>Compressor summary</b>: The paper studies the dynamics gap between trajectory prediction accuracy on fixed datasets and real-world driving performance, and emphasizes the trade-off between computational efficiency and prediction accuracy for autonomous driving systems.</p><hr><h3>Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs with Short Burn-In Time</h3>
<p>Xiang Ji, Gen Li</p>
<p><a href='https://openreview.net/forum?id=nFsbQHFmj2'>https://openreview.net/forum?id=nFsbQHFmj2</a></p>
<p><b>Keywords</b>: reinforcement learning theory, regret minimization, minimax optimality
</p><p><b>Compressor summary</b>: The paper presents a new model-free reinforcement learning algorithm that achieves optimal performance with low memory and computational cost, and fast sample efficiency.</p><hr><h3>Label Correction of Crowdsourced Noisy Annotations with an Instance-Dependent Noise Transition Model</h3>
<p>Hui Guo, Boyu Wang, Grace Yi</p>
<p><a href='https://openreview.net/forum?id=nFEQNYsjQO'>https://openreview.net/forum?id=nFEQNYsjQO</a></p>
<p><b>Keywords</b>: Noisy Label, Instance-Dependent Transition Matrix, Label Correction, Crowdsourcing
</p><p><b>Compressor summary</b>: The paper proposes a new label correction algorithm using a Bayesian network to model instance-dependent noise transitions in crowdsourced annotations, with theoretical guarantees and experimental validation.</p><hr><h3>Contrastive Training of Complex-Valued Autoencoders for Object Discovery</h3>
<p>Aleksandar Stanić, Anand Gopalakrishnan, Kazuki Irie, Jürgen Schmidhuber</p>
<p><a href='https://openreview.net/forum?id=nF6X3u0FaA'>https://openreview.net/forum?id=nF6X3u0FaA</a></p>
<p><b>Keywords</b>: object-centric learning, complex-valued networks, unsupervised learning, temporal correlation hypothesis
</p><p><b>Compressor summary</b>: The authors propose a new method for synchrony-based object-centric models that improves their ability to learn from multi-object color datasets and store more than three objects.</p><hr><h3>Private Distribution Learning with Public Data: The View from Sample Compression</h3>
<p>Shai Ben-David, Alex Bie, Clement Louis Canonne, Gautam Kamath, Vikrant Singhal</p>
<p><a href='https://openreview.net/forum?id=nDIrJmKPd5'>https://openreview.net/forum?id=nDIrJmKPd5</a></p>
<p><b>Keywords</b>: differential privacy, distribution learning, gaussians, mixture of gaussians, compression schemes, robust compression schemes, privacy
</p><p><b>Compressor summary</b>: The paper studies how to privately learn a distribution from public and private data, and shows that this depends on compressing the data and learning from lists of examples.</p><hr><h3>FouriDown: Factoring Down-Sampling into Shuffling and Superposing</h3>
<p>Qi Zhu, Man Zhou, Jie Huang, Naishan Zheng, Hongzhi Gao, Chongyi Li, Yuan Xu, Feng Zhao</p>
<p><a href='https://openreview.net/forum?id=nCwStXFDQu'>https://openreview.net/forum?id=nCwStXFDQu</a></p>
<p><b>Keywords</b>: Image restoration, Down-Sampling, Fourier transform
</p><p><b>Compressor summary</b>: The authors propose a new down-sampling method called FouriDown, which uses a learnable and context-adaptive Fourier function to improve image restoration tasks like de-blurring and low-light enhancement.</p><hr><h3>The Equivalence of Dynamic and Strategic Stability under Regularized Learning in Games</h3>
<p>Victor Boone, Panayotis Mertikopoulos</p>
<p><a href='https://openreview.net/forum?id=nCLdsEzZBV'>https://openreview.net/forum?id=nCLdsEzZBV</a></p>
<p><b>Keywords</b>: Regularized learning, dynamic stability, strategic stability, Nash equilibrium
</p><p><b>Compressor summary</b>: The paper studies how no-regret learning in multiplayer games leads to setwise rationality properties and closedness under better replies, and explores the convergence rate of different regularization methods.</p><hr><h3>Predicting Global Label Relationship Matrix for Graph Neural Networks under Heterophily</h3>
<p>Langzhang Liang, Xiangjing Hu, Zenglin Xu, Zixing Song, Irwin King</p>
<p><a href='https://openreview.net/forum?id=nBFMCyEi0j'>https://openreview.net/forum?id=nBFMCyEi0j</a></p>
<p><b>Keywords</b>: graph neural networks, heterophily problem, global label relationship matrix
</p><p><b>Compressor summary</b>: LRGNN is a generic GNN that works on both homophilous and heterophilous graphs by predicting the low-rank label relationship matrix using robust low-rank approximation, which has two advantages for graph modeling.</p><hr><h3>Training Transitive and Commutative Multimodal Transformers with LoReTTa</h3>
<p>Manuel Tran, Yashin Dicente Cid, Amal Lahiani, Fabian J Theis, Tingying Peng, Eldad Klaiman</p>
<p><a href='https://openreview.net/forum?id=nArzDm353Y'>https://openreview.net/forum?id=nArzDm353Y</a></p>
<p><b>Keywords</b>: generative pre-training, causal modeling, masked modeling, commutative modeling, transitive modeling, multimodal learning
</p><p><b>Compressor summary</b>: LoReTTa is a self-supervised framework that links modalities with causal modeling and masked modeling to handle multimodal datasets with missing modalities.</p><hr><h3>Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses</h3>
<p>Gon Buzaglo, Niv Haim, Gilad Yehudai, Gal Vardi, Yakir Oz, Yaniv Nikankin, michal Irani</p>
<p><a href='https://openreview.net/forum?id=nA9Fh3HFHJ'>https://openreview.net/forum?id=nA9Fh3HFHJ</a></p>
<p><b>Keywords</b>: memorization, data reconstruction, implicit bias
</p><p><b>Compressor summary</b>: This paper extends a previous study on reconstructing training data from neural networks and explores factors affecting this process, such as weight decay and neuron count.</p><hr><h3>Environment-Aware Dynamic Graph Learning for Out-of-Distribution Generalization</h3>
<p>Haonan Yuan, Qingyun Sun, Xingcheng Fu, Ziwei Zhang, Cheng Ji, Hao Peng, Jianxin Li</p>
<p><a href='https://openreview.net/forum?id=n8JWIzYPRz'>https://openreview.net/forum?id=n8JWIzYPRz</a></p>
<p><b>Keywords</b>: dynamic graph learning, out-of-distribution generalization, invariant learning, link prediction
</p><p><b>Compressor summary</b>: The EAGLE framework models and exploits spatio-temporal environments for out-of-distribution generalization in dynamic graph neural networks, achieving superior performance against existing methods under distribution shifts.</p><hr><h3>Clifford Group Equivariant Neural Networks</h3>
<p>David Ruhe, Johannes Brandstetter, Patrick Forré</p>
<p><a href='https://openreview.net/forum?id=n84bzMrGUD'>https://openreview.net/forum?id=n84bzMrGUD</a></p>
<p><b>Keywords</b>: Clifford algebras, geometric deep dearning, Clifford group equivariance, E(n)-equivariant neural networks, O(n)-equivariant neural networks
</p><p><b>Compressor summary</b>: The paragraph introduces a new method for building equivariant neural networks using the Clifford group, which has desirable properties and shows state-of-the-art performance on various tasks.</p><hr><h3>Learning with Explanation Constraints</h3>
<p>Rattana Pukdee, Dylan Sam, J Zico Kolter, Nina Balcan, Pradeep Kumar Ravikumar</p>
<p><a href='https://openreview.net/forum?id=n6ztJ3Lrdj'>https://openreview.net/forum?id=n6ztJ3Lrdj</a></p>
<p><b>Keywords</b>: Interpretable ML, Semi-supervised learning, Learning theory
</p><p><b>Compressor summary</b>: The paper proposes a learning framework that uses explanation constraints from prior knowledge to improve the performance and interpretability of deep learning models, especially when using gradient information as explanations.</p><hr><h3>Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity</h3>
<p>Youssef Allouah, Rachid Guerraoui, Nirupam Gupta, Rafael Pinot, Geovani Rizk</p>
<p><a href='https://openreview.net/forum?id=n3fPDW87is'>https://openreview.net/forum?id=n3fPDW87is</a></p>
<p><b>Keywords</b>: Optimization, Byzantine resilience, Distributed machine learning, federated learning
</p><p><b>Compressor summary</b>: The paper proposes a more realistic heterogeneity model for robust distributed learning algorithms, showing lower breakdown points and better matching with empirical observations.</p><hr><h3>DRAUC: An Instance-wise Distributionally Robust AUC Optimization Framework</h3>
<p>Siran Dai, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, Qingming Huang</p>
<p><a href='https://openreview.net/forum?id=n3ZVdny7OH'>https://openreview.net/forum?id=n3ZVdny7OH</a></p>
<p><b>Keywords</b>: Robust Learning AUC
</p><p><b>Compressor summary</b>: The paper proposes a method to optimize AUC in long-tailed classification scenarios using distributionally robust optimization and a surrogate loss, while addressing label bias.</p><hr><h3>Fast Optimal Transport through Sliced Generalized Wasserstein Geodesics</h3>
<p>Guillaume Mahey, Laetitia Chapel, Gilles Gasso, Clément Bonet, Nicolas Courty</p>
<p><a href='https://openreview.net/forum?id=n3XuYdvhNW'>https://openreview.net/forum?id=n3XuYdvhNW</a></p>
<p><b>Keywords</b>: Optimal Transport, Wasserstein distance, Generalized Geodesics, Sliced Wasserstein
</p><p><b>Compressor summary</b>: The paper introduces a new proxy for Wasserstein distance called min-SWGG, which is based on one-dimensional projections and has benefits in various applications such as gradient flows, shape matching, and image colorization.</p><hr><h3>2Direction: Theoretically Faster Distributed Training with Bidirectional Communication Compression</h3>
<p>Alexander Tyurin, Peter Richtárik</p>
<p><a href='https://openreview.net/forum?id=n18MhTsSGb'>https://openreview.net/forum?id=n18MhTsSGb</a></p>
<p><b>Keywords</b>: convex optimization, accelerated method, communication compression, bidirectional compression, distributed optimization
</p><p><b>Compressor summary</b>: The 2Direction method speeds up distributed convex optimization by using bidirectional compressed communication and a new error-feedback mechanism, outperforming previous methods in both communication complexity and acceleration.</p><hr><h3>Optimality in Mean Estimation: Beyond Worst-Case, Beyond Sub-Gaussian, and Beyond $1+\alpha$ Moments</h3>
<p>Trung Dang, Jasper C.H. Lee, Maoyuan Song, Paul Valiant</p>
<p><a href='https://openreview.net/forum?id=mvSDs51eqQ'>https://openreview.net/forum?id=mvSDs51eqQ</a></p>
<p><b>Keywords</b>: mean estimation, instance optimality
</p><p><b>Compressor summary</b>: The paper explores if mean estimation algorithms can benefit from knowing specific features of the input distribution and finds that it's possible in some cases, but generally not, while introducing a new framework for analyzing algorithm optimality.</p><hr><h3>Thinker: Learning to Plan and Act</h3>
<p>Stephen Chung, Ivan Anokhin, David Krueger</p>
<p><a href='https://openreview.net/forum?id=mumEBl0arj'>https://openreview.net/forum?id=mumEBl0arj</a></p>
<p><b>Keywords</b>: Reinforcement learning, model-based reinforcement learning, planning, Monte Carlo Tree Search, Markov Decision Process
</p><p><b>Compressor summary</b>: The Thinker algorithm is a novel reinforcement learning approach that allows agents to interact with and utilize a learned world model for autonomous planning, achieving state-of-the-art performance in Sokoban and competitive results in Atari 2600.</p><hr><h3>Reliable Off-Policy Learning for Dosage Combinations</h3>
<p>Jonas Schweisthal, Dennis Frauen, Valentyn Melnychuk, Stefan Feuerriegel</p>
<p><a href='https://openreview.net/forum?id=muVKSb8gi5'>https://openreview.net/forum?id=muVKSb8gi5</a></p>
<p><b>Keywords</b>: off-policy learning, causal inference, reliable machine learning, medicine, dosaging, normalizing flows
</p><p><b>Compressor summary</b>: The paper proposes a novel method for personalized medicine that estimates individualized dose-response functions and optimal dosage combinations using neural networks, conditional normalizing flows, and gradient-based learning.</p><hr><h3>Efficient Sampling of Stochastic Differential Equations with Positive Semi-Definite Models</h3>
<p>Anant Raj, Umut Simsekli, Alessandro Rudi</p>
<p><a href='https://openreview.net/forum?id=mookk2nLO9'>https://openreview.net/forum?id=mookk2nLO9</a></p>
<p><b>Keywords</b>: Kernel Methods, Sampling, Fokker-Planck Equation, Fractional Fokker-Planck Equation, Stochastic Differential Equations, Partial Differential Equations
</p><p><b>Compressor summary</b>: The paper proposes an efficient method to sample from a stochastic differential equation using a recent probabilistic model and shows that it works well for smooth solutions.</p><hr><h3>Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction</h3>
<p>Anagh Malik, Parsa Mirdehghan, Sotiris Nousias, Kyros Kutulakos, David B. Lindell</p>
<p><a href='https://openreview.net/forum?id=mmmd2vp0n0'>https://openreview.net/forum?id=mmmd2vp0n0</a></p>
<p><b>Keywords</b>: neural radiance fields, 3D reconstruction, single-photon lidar, computational imaging
</p><p><b>Compressor summary</b>: The paper proposes a novel method to render transient NeRFs using single-photon lidar data and captures light transport phenomena at picosecond timescales, showing improved geometry and appearance compared to point cloud-based supervision.</p><hr><h3>Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Method</h3>
<p>Constantine Caramanis, Dimitris Fotakis, Alkis Kalavasis, Vasilis Kontonis, Christos Tzamos</p>
<p><a href='https://openreview.net/forum?id=mmTy1iyU5G'>https://openreview.net/forum?id=mmTy1iyU5G</a></p>
<p><b>Keywords</b>: Policy Gradient, Combinatorial Optimization, Gradient Descent
</p><p><b>Compressor summary</b>: The authors develop a theoretical framework to analyze effective deep neural network and reinforcement learning methods for solving combinatorial problems, showing they can find near-optimal solutions with a limited number of parameters and without getting stuck in sub-optimal states. They also propose a novel regularization process for gradient descent that improves performance.</p><hr><h3>A Causal Framework for Decomposing Spurious Variations</h3>
<p>Drago Plecko, Elias Bareinboim</p>
<p><a href='https://openreview.net/forum?id=mm9svgvwvk'>https://openreview.net/forum?id=mm9svgvwvk</a></p>
<p><b>Keywords</b>: Causal Inference, Confounding, Fair and Explainable AI
</p><p><b>Compressor summary</b>: The authors develop formal tools for decomposing spurious associations in causal mechanisms using Markovian and Semi-Markovian models, providing applications in various fields and demonstrating their approach on a real dataset.</p><hr><h3>Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment</h3>
<p>Hao Liu, Wilson Yan, Pieter Abbeel</p>
<p><a href='https://openreview.net/forum?id=mlxRLIy7kc'>https://openreview.net/forum?id=mlxRLIy7kc</a></p>
<p><b>Keywords</b>: Large Language Model, VQVAE, Vector Quantization, Multimodal
</p><p><b>Compressor summary</b>: The paper proposes LQAE, a method to align images and texts without supervision, using pretrained language models, which improves few-shot learning in vision tasks like image classification and VQA.</p><hr><h3>Unleashing the Power of Randomization in Auditing Differentially Private ML</h3>
<p>Krishna Pillutla, Galen Andrew, Peter Kairouz, Hugh Brendan McMahan, Alina Oprea, Sewoong Oh</p>
<p><a href='https://openreview.net/forum?id=mlbes5TAAg'>https://openreview.net/forum?id=mlbes5TAAg</a></p>
<p><b>Keywords</b>: Differential privacy auditing, multiple canaries, randomization, lifting, adaptive confidence intervals
</p><p><b>Compressor summary</b>: The authors propose a method to audit differentially private machine learning by adding multiple randomized examples (canaries) and use lifted differential privacy, statistical tests, and confidence intervals to improve sample complexity.</p><hr><h3>Robust Mean Estimation Without Moments for Symmetric Distributions</h3>
<p>Gleb Novikov, David Steurer, Stefan Tiegel</p>
<p><a href='https://openreview.net/forum?id=mkve1raJUc'>https://openreview.net/forum?id=mkve1raJUc</a></p>
<p><b>Keywords</b>: Robust Mean Estimation, Unbounded First Moment, Symmetric Distributions (Spherical, Elliptical, Product), Filtering Algorithm, Huber Loss
</p><p><b>Compressor summary</b>: The paper proposes efficient and robust algorithms for estimating the mean of symmetric distributions, including product Cauchy and elliptical distributions, without moment assumptions or strong distributional assumptions.</p><hr><h3>Memory-Constrained Algorithms for Convex Optimization</h3>
<p>Moise Blanchard, Junhui Zhang, Patrick Jaillet</p>
<p><a href='https://openreview.net/forum?id=mkKQr56xdB'>https://openreview.net/forum?id=mkKQr56xdB</a></p>
<p><b>Keywords</b>: Convex optimization, feasibility problem, first-order methods, memory constraints, cutting planes, oracle complexity
</p><p><b>Compressor summary</b>: The paper introduces recursive cutting-plane algorithms for solving feasibility problems and convex optimization with constrained memory, achieving sub-polynomial complexity and optimal memory trade-offs in some regimes.</p><hr><h3>Beyond Average Return in Markov Decision Processes</h3>
<p>Alexandre Marthe, Aurélien Garivier, Claire Vernade</p>
<p><a href='https://openreview.net/forum?id=mgNu8nDFwa'>https://openreview.net/forum?id=mgNu8nDFwa</a></p>
<p><b>Keywords</b>: Markov Decision Process, Dynamic Programming, statistical functionnals, Distributionnal Reinforcement Learning, Policy Evaluation, Planning
</p><p><b>Compressor summary</b>: The paragraph discusses how Dynamic Programming can handle certain operations efficiently in Markov Decision Processes, but only for specific statistics classes, and explores the use of Distributional Reinforcement Learning to evaluate functionals approximately.</p><hr><h3>Large Language Models Are Zero-Shot Time Series Forecasters</h3>
<p>Nate Gruver, Marc Anton Finzi, Shikai Qiu, Andrew Gordon Wilson</p>
<p><a href='https://openreview.net/forum?id=md68e8iZK1'>https://openreview.net/forum?id=md68e8iZK1</a></p>
<p><b>Keywords</b>: large language models, time series, probabilistic forecasting
</p><p><b>Compressor summary</b>: The authors propose using large language models to predict future values in time series by encoding them as text and show their effectiveness in handling various aspects of time series data.</p><hr><h3>Neural Lighting Simulation for Urban Scenes</h3>
<p>Ava Pun, Gary Sun, Jingkang Wang, Yun Chen, Ze Yang, Sivabalan Manivasagam, Wei-Chiu Ma, Raquel Urtasun</p>
<p><a href='https://openreview.net/forum?id=mcx8IGneYw'>https://openreview.net/forum?id=mcx8IGneYw</a></p>
<p><b>Keywords</b>: Scene Relighting, Lighting Estimation, Camera Simulation, Self-Driving, Lighting Simulation, Scene Editing
</p><p><b>Compressor summary</b>: LightSim is a neural lighting camera simulation system that creates realistic and diverse images under different illumination conditions for training image-based robot perception models.</p><hr><h3>SEENN: Towards Temporal Spiking Early Exit Neural Networks</h3>
<p>Yuhang Li, Tamar Geller, Youngeun Kim, Priyadarshini Panda</p>
<p><a href='https://openreview.net/forum?id=mbaN0Y0QTw'>https://openreview.net/forum?id=mbaN0Y0QTw</a></p>
<p><b>Keywords</b>: Spiking Neural Networks, ANN-SNN Conversion, Conditional Computing
</p><p><b>Compressor summary</b>: This paper introduces Spiking Early-Exit Neural Networks (SEENNs), which adjust the number of timesteps in spiking neural networks based on input samples, achieving better accuracy and efficiency tradeoffs.</p><hr><h3>Bridging Discrete and Backpropagation: Straight-Through and Beyond</h3>
<p>Liyuan Liu, Chengyu Dong, Xiaodong Liu, Bin Yu, Jianfeng Gao</p>
<p><a href='https://openreview.net/forum?id=mayAyPrhJI'>https://openreview.net/forum?id=mayAyPrhJI</a></p>
<p><b>Keywords</b>: discrete random variables, back-propagation, straight through
</p><p><b>Compressor summary</b>: ReinMax is a novel method that approximates gradients for discrete latent variables in deep learning using Heun's method, achieving second-order accuracy with negligible computation overheads and outperforming existing approaches.</p><hr><h3>Towards the Difficulty for a Deep Neural Network to Learn Concepts of Different Complexities</h3>
<p>Dongrui Liu, Huiqi Deng, Xu Cheng, Qihan Ren, Kangrui Wang, Quanshi Zhang</p>
<p><a href='https://openreview.net/forum?id=mZ3hnyL9bS'>https://openreview.net/forum?id=mZ3hnyL9bS</a></p>
<p><b>Keywords</b>: representation complexity, deep learning
</p><p><b>Compressor summary</b>: The paper explores why deep neural networks find it harder to learn complex concepts involving many input variables, and identifies the specific factor that increases learning difficulty.</p><hr><h3>Class-Conditional Conformal Prediction with Many Classes</h3>
<p>Tiffany Ding, Anastasios Nikolas Angelopoulos, Stephen Bates, Michael Jordan, Ryan Tibshirani</p>
<p><a href='https://openreview.net/forum?id=mYz6ApeU4J'>https://openreview.net/forum?id=mYz6ApeU4J</a></p>
<p><b>Keywords</b>: conformal prediction, uncertainty quantification, class imbalance
</p><p><b>Compressor summary</b>: Clustered conformal prediction is a method that groups similar classes together and improves the probability of correctly predicting their labels when there is limited labeled data per class.</p><hr><h3>Reining Generalization in Offline Reinforcement Learning via Representation Distinction</h3>
<p>Yi Ma, Hongyao Tang, Dong Li, Zhaopeng Meng</p>
<p><a href='https://openreview.net/forum?id=mVywRIDNIl'>https://openreview.net/forum?id=mVywRIDNIl</a></p>
<p><b>Keywords</b>: Offline Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper proposes Representation Distinction (RD), a technique that improves offline reinforcement learning by differentiating between in-sample and out-of-distribution data representations to reduce overgeneralization.</p><hr><h3>Hierarchical Gaussian Mixture based Task Generative Model for Robust Meta-Learning</h3>
<p>Yizhou Zhang, Jingchao Ni, Wei Cheng, Zhengzhang Chen, Liang Tong, Haifeng Chen, Yan Liu</p>
<p><a href='https://openreview.net/forum?id=mVTyeQIiE4'>https://openreview.net/forum?id=mVTyeQIiE4</a></p>
<p><b>Keywords</b>: Few-Shot Learning, Meta Learning, Task Representation
</p><p><b>Compressor summary</b>: The paper proposes a meta-learning framework that models the density of task instances using a Hierarchical Gaussian Mixture based Task Generative Model (HTGM), which helps adapt to new tasks from different or unseen distributions.</p><hr><h3>InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding</h3>
<p>Junda Wu, Tong Yu, Rui Wang, Zhao Song, Ruiyi Zhang, Handong Zhao, Chaochao Lu, Shuai Li, Ricardo Henao</p>
<p><a href='https://openreview.net/forum?id=mSNfjOcDUv'>https://openreview.net/forum?id=mSNfjOcDUv</a></p>
<p><b>Keywords</b>: soft prompt tuning
</p><p><b>Compressor summary</b>: The paragraph describes a new method called InfoPrompt that improves soft prompt tuning by maximizing the mutual information between prompts and other model parameters, leading to faster convergence and better performance.</p><hr><h3>The Rise of AI Language Pathologists: Exploring Two-level Prompt Learning for Few-shot Weakly-supervised Whole Slide Image Classification</h3>
<p>Linhao Qu, xiaoyuan Luo, Kexue Fu, Manning Wang, Zhijian Song</p>
<p><a href='https://openreview.net/forum?id=mSDfBXr8Py'>https://openreview.net/forum?id=mSDfBXr8Py</a></p>
<p><b>Keywords</b>: multiple instance learning, whole slide image classification, prompt learning, vision-language model, few-shot learning
</p><p><b>Compressor summary</b>: The paper proposes a novel few-shot weakly supervised learning method for pathology Whole Slide Image (WSI) classification using prompt learning and GPT-4 to handle challenges posed by the weak bag labels within the Multiple Instance Learning framework.</p><hr><h3>Scaling Open-Vocabulary Object Detection</h3>
<p>Matthias Minderer, Alexey A. Gritsenko, Neil Houlsby</p>
<p><a href='https://openreview.net/forum?id=mQPNcBWjGc'>https://openreview.net/forum?id=mQPNcBWjGc</a></p>
<p><b>Keywords</b>: object detection, open-vocabulary object detection, vision transformers, vision-language models, scaling, self-training
</p><p><b>Compressor summary</b>: OWLv2 is a model that uses self-training with pseudo-annotations to scale up object detection data, achieving significant improvements in performance on rare classes.</p><hr><h3>Slimmed Asymmetrical Contrastive Learning and Cross Distillation for Lightweight Model Training</h3>
<p>Jian Meng, Li Yang, Kyungmin Lee, Jinwoo Shin, Deliang Fan, Jae-sun Seo</p>
<p><a href='https://openreview.net/forum?id=mOVEJletyD'>https://openreview.net/forum?id=mOVEJletyD</a></p>
<p><b>Keywords</b>: Contrastive Learning, Self-supervised Learning, Energy-efficient contrastive learning
</p><p><b>Compressor summary</b>: SACL-XD is a new self-supervised contrastive learning scheme that improves the performance of lightweight models and reduces energy consumption in AI applications.</p><hr><h3>(Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More</h3>
<p>Jan Schuchardt, Yan Scholten, Stephan Günnemann</p>
<p><a href='https://openreview.net/forum?id=mLe63bAYc7'>https://openreview.net/forum?id=mLe63bAYc7</a></p>
<p><b>Keywords</b>: Adversarial robustness, Geometric machine learning, Equivariances, Robustness Certification, Graph neural networks
</p><p><b>Compressor summary</b>: The paragraph discusses a new approach to adversarial robustness that considers task equivariance in real-world tasks like molecular property prediction, and proposes methods to achieve provable robustness for various models and architectures.</p><hr><h3>Efficient Testable Learning of Halfspaces with Adversarial Label Noise</h3>
<p>Ilias Diakonikolas, Daniel Kane, Vasilis Kontonis, Sihan Liu, Nikos Zarifis</p>
<p><a href='https://openreview.net/forum?id=mIm0hsUUt1'>https://openreview.net/forum?id=mIm0hsUUt1</a></p>
<p><b>Keywords</b>: Machine Learning
</p><p><b>Compressor summary</b>: The paper presents a fast algorithm for learning halfspaces under the Gaussian distribution with adversarial label noise, using iterative soft localization and testers to ensure data quality.</p><hr><h3>Theoretically Guaranteed Bidirectional Data Rectification for Robust Sequential Recommendation</h3>
<p>yatong sun, Bin Wang, Zhu Sun, Xiaochun Yang, Yan Wang</p>
<p><a href='https://openreview.net/forum?id=mHsxsrLl0y'>https://openreview.net/forum?id=mHsxsrLl0y</a></p>
<p><b>Keywords</b>: recommender systems, sequential recommendation
</p><p><b>Compressor summary</b>: The paper proposes BirDRec, a framework that rectifies unreliable data in sequential recommender systems by adjusting both input and target items, and reduces its complexity with sampling and self-ensemble methods.</p><hr><h3>Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games</h3>
<p>Youbang Sun, Tao Liu, Ruida Zhou, Panganamala Kumar, Shahin Shahrampour</p>
<p><a href='https://openreview.net/forum?id=mA7nTGXjD3'>https://openreview.net/forum?id=mA7nTGXjD3</a></p>
<p><b>Keywords</b>: Multi Agent Reinforcement Learning, Markov Potential Games, Natural Policy Gradient, Nash Equilibrium
</p><p><b>Compressor summary</b>: The paper proposes an efficient algorithm for multi-agent reinforcement learning that finds approximate Nash Equilibria using policy gradients, and demonstrates its effectiveness on two examples.</p><hr><h3>Learning Motion Refinement for Unsupervised Face Animation</h3>
<p>Jiale Tao, Shuhang Gu, Wen Li, Lixin Duan</p>
<p><a href='https://openreview.net/forum?id=m9uHv1Pxq7'>https://openreview.net/forum?id=m9uHv1Pxq7</a></p>
<p><b>Keywords</b>: Face animation, Motion refinement, Structure correlation
</p><p><b>Compressor summary</b>: The paper presents a new unsupervised face animation approach that learns both coarse and fine facial motions using a local affine motion model and a novel motion refinement module based on dense correlation between source and driving images.</p><hr><h3>Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond</h3>
<p>Oleg Platonov, Denis Kuznedelev, Artem Babenko, Liudmila Prokhorenkova</p>
<p><a href='https://openreview.net/forum?id=m7PIJWOdlY'>https://openreview.net/forum?id=m7PIJWOdlY</a></p>
<p><b>Keywords</b>: graph characteristics, homophily, heterophily, label informativeness, constant baseline, GNN
</p><p><b>Compressor summary</b>: The paper discusses the drawbacks of common homophily measures in graph neural networks and proposes new measures called adjusted homophily and label informativeness.</p><hr><h3>Equivariant Adaptation of Large Pretrained Models</h3>
<p>Arnab Kumar Mondal, Siba Smarak Panigrahi, Sékou-Oumar Kaba, Sai Rajeswar, Siamak Ravanbakhsh</p>
<p><a href='https://openreview.net/forum?id=m6dRQJw280'>https://openreview.net/forum?id=m6dRQJw280</a></p>
<p><b>Keywords</b>: deep learning, large pretrained models, symmetry, equivariance, group theory, computer vision, point clouds, foundation models
</p><p><b>Compressor summary</b>: Equivariant networks can be made from large pretrained models by using a canonicalization network that transforms input to a canonical form, improving robustness to deterministic data transformations.</p><hr><h3>Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks</h3>
<p>Jiarong Xu, Renhong Huang, XIN JIANG, Yuxuan Cao, Carl Yang, Chunping Wang, Yang Yang</p>
<p><a href='https://openreview.net/forum?id=m2WR1yJ8N9'>https://openreview.net/forum?id=m2WR1yJ8N9</a></p>
<p><b>Keywords</b>: graph neural networks, pre-training
</p><p><b>Compressor summary</b>: The paper proposes a graph pre-training framework (APT) that uses fewer, carefully selected data points to improve downstream tasks, by choosing representative and instructive data based on graph properties and predictive uncertainty.</p><hr><h3>Learning List-Level Domain-Invariant Representations for Ranking</h3>
<p>Ruicheng Xian, Honglei Zhuang, Zhen Qin, Hamed Zamani, Jing Lu, Ji Ma, Kai Hui, Han Zhao, Xuanhui Wang, Michael Bendersky</p>
<p><a href='https://openreview.net/forum?id=m21rQusNgb'>https://openreview.net/forum?id=m21rQusNgb</a></p>
<p><b>Keywords</b>: learning to rank, domain adaptation, text ranking
</p><p><b>Compressor summary</b>: This paper introduces list-level alignment, a new invariant representation learning method for ranking problems that leverages the list structure of data and provides theoretical and empirical improvements over existing methods.</p><hr><h3>Efficient Hyper-parameter Optimization with Cubic Regularization</h3>
<p>Zhenqian Shen, Hansi Yang, Yong Li, James Kwok, quanming yao</p>
<p><a href='https://openreview.net/forum?id=m11TbsaQQI'>https://openreview.net/forum?id=m11TbsaQQI</a></p>
<p><b>Keywords</b>: hyper-parameter optimization, cubic regularization
</p><p><b>Compressor summary</b>: The paper proposes a new hyper-parameter optimization method that uses cubic regularization and stochastic relaxation to avoid local optima and work without hyper-gradients.</p><hr><h3>Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery</h3>
<p>Sarah Rastegar, Hazel Doughty, Cees G. M. Snoek</p>
<p><a href='https://openreview.net/forum?id=m0vfXMrLwF'>https://openreview.net/forum?id=m0vfXMrLwF</a></p>
<p><b>Keywords</b>: Generalized category discovery, Open world learning, Open-set recognition
</p><p><b>Compressor summary</b>: The paper proposes a self-supervised method for discovering unknown categories at test time by viewing them as optimal solutions to well-defined problems and using minimal length codes to represent the hierarchy in real-world data.</p><hr><h3>StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models</h3>
<p>Yinghao Aaron Li, Cong Han, Vinay S Raghavan, Gavin Mischler, Nima Mesgarani</p>
<p><a href='https://openreview.net/forum?id=m0RbqrUM26'>https://openreview.net/forum?id=m0RbqrUM26</a></p>
<p><b>Keywords</b>: Speech Processing, Text-to-Speech, Diffusion Model, Large Language Model, Self-Supervised Speech Model, WavLM
</p><p><b>Compressor summary</b>: StyleTTS 2 is a text-to-speech model that uses style diffusion, adversarial training, and large speech language models to generate high-quality, natural-sounding speech with different styles.</p><hr><h3>DiffComplete: Diffusion-based Generative 3D Shape Completion</h3>
<p>Ruihang Chu, Enze Xie, Shentong Mo, Zhenguo Li, Matthias Nießner, Chi-Wing Fu, Jiaya Jia</p>
<p><a href='https://openreview.net/forum?id=lzqaQRsITh'>https://openreview.net/forum?id=lzqaQRsITh</a></p>
<p><b>Keywords</b>: 3d shape completion, conditional generation, diffusion models
</p><p><b>Compressor summary</b>: The authors present a new diffusion-based method called DiffComplete that balances realism, multi-modality, and high fidelity in 3D shape completion by using hierarchical feature aggregation and occupancy-aware fusion.</p><hr><h3>Spontaneous symmetry breaking in generative diffusion models</h3>
<p>Gabriel Raya, Luca Ambrogioni</p>
<p><a href='https://openreview.net/forum?id=lxGFGMMSVl'>https://openreview.net/forum?id=lxGFGMMSVl</a></p>
<p><b>Keywords</b>: generative models;diffusion models;score-based generative models; symmetry-breaking
</p><p><b>Compressor summary</b>: The paper presents a new understanding of diffusion models' dynamics, leading to improved performance and diversity in generating high-dimensional data.</p><hr><h3>CARE: Modeling Interacting Dynamics Under Temporal Environmental Variation</h3>
<p>Xiao Luo, Haixin Wang, Zijie Huang, Huiyu Jiang, Abhijeet Sadashiv Gangan, Song Jiang, Yizhou Sun</p>
<p><a href='https://openreview.net/forum?id=lwg3ohkFRv'>https://openreview.net/forum?id=lwg3ohkFRv</a></p>
<p><b>Keywords</b>: Dynamical System, Distribution Shift, Neural ODE, Graph Neural Network
</p><p><b>Compressor summary</b>: The paper proposes a probabilistic model called Context-attended Graph ODE (CARE) for time-varying interacting dynamical systems and shows its effectiveness on four datasets.</p><hr><h3>H-InDex: Visual Reinforcement Learning with Hand-Informed Representations for Dexterous Manipulation</h3>
<p>Yanjie Ze, Yuyao Liu, Ruizhe Shi, Jiaxin Qin, Zhecheng Yuan, Jiashun Wang, Huazhe Xu</p>
<p><a href='https://openreview.net/forum?id=lvvaNwnP6M'>https://openreview.net/forum?id=lvvaNwnP6M</a></p>
<p><b>Keywords</b>: Visual Reinforcement Learning, Representation Learning, Dexterous Manipulation
</p><p><b>Compressor summary</b>: The paper proposes a framework called H-InDex that uses human hand poses to improve robotic dexterous manipulation tasks using reinforcement learning.</p><hr><h3>K-Nearest-Neighbor Local Sampling Based Conditional Independence Testing</h3>
<p>Shuai Li, Yingjie Zhang, Hongtu Zhu, Christina Dan Wang, Hai Shu, Ziqi Chen, Zhuoran Sun, Yanfeng Yang</p>
<p><a href='https://openreview.net/forum?id=luyXPdkNSN'>https://openreview.net/forum?id=luyXPdkNSN</a></p>
<p><b>Keywords</b>: Conditional Independence testing, causal inference, conditional mutual information, k-nearest neighbor, conditional randomization test, conditional permutation test
</p><p><b>Compressor summary</b>: The article introduces a novel conditional independence testing method that is efficient, powerful, and robust, without assumptions about distributions or dependencies.</p><hr><h3>SPA: A Graph Spectral Alignment Perspective for Domain Adaptation</h3>
<p>Zhiqing Xiao, Haobo Wang, Ying Jin, Lei Feng, Gang Chen, Fei Huang, Junbo Zhao</p>
<p><a href='https://openreview.net/forum?id=lpx9LZPVtZ'>https://openreview.net/forum?id=lpx9LZPVtZ</a></p>
<p><b>Keywords</b>: Domain Adaptation, Self-training, Graph Spectra
</p><p><b>Compressor summary</b>: The paper proposes a new graph-based method for unsupervised domain adaptation, called SPA, which improves the performance of machine learning models in different target domains by aligning their structures and enhancing discriminability.</p><hr><h3>ProtoDiff: Learning to Learn Prototypical Networks by Task-Guided Diffusion</h3>
<p>Yingjun Du, Zehao Xiao, Shengcai Liao, Cees G. M. Snoek</p>
<p><a href='https://openreview.net/forum?id=lp9GR2t3hn'>https://openreview.net/forum?id=lp9GR2t3hn</a></p>
<p><b>Keywords</b>: Meta-learning, few-shot learning, diffusion model, prototype
</p><p><b>Compressor summary</b>: ProtoDiff is a novel framework that uses task-guided diffusion to generate overfitted prototypes for few-shot learning challenges, improving classification performance.</p><hr><h3>A Computation and Communication Efficient Method for Distributed Nonconvex Problems in the Partial Participation Setting</h3>
<p>Alexander Tyurin, Peter Richtárik</p>
<p><a href='https://openreview.net/forum?id=loxinzXlCx'>https://openreview.net/forum?id=loxinzXlCx</a></p>
<p><b>Keywords</b>: Nonconvex Optimization, Partial Participation, Variance Reduction, Compressed Communication, Distributed Optimization
</p><p><b>Compressor summary</b>: The new method optimizes distributed optimization and federated learning by combining variance reduction, partial participation, and compressed communication with optimal oracle and communication complexities.</p><hr><h3>Robust Learning for Smoothed Online Convex Optimization with Feedback Delay</h3>
<p>Pengfei Li, Jianyi Yang, Adam Wierman, Shaolei Ren</p>
<p><a href='https://openreview.net/forum?id=loixpHDZKj'>https://openreview.net/forum?id=loixpHDZKj</a></p>
<p><b>Keywords</b>: Online optimization, competitive algorithm, switching cost
</p><p><b>Compressor summary</b>: The paper proposes an online optimization method called Robustness-Constrained Learning that uses machine learning predictions to improve performance while accounting for multi-step costs and feedback delay.</p><hr><h3>Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms</h3>
<p>Qian Yu, Yining Wang, Baihe Huang, Qi Lei, Jason D. Lee</p>
<p><a href='https://openreview.net/forum?id=lnTpBUge5G'>https://openreview.net/forum?id=lnTpBUge5G</a></p>
<p><b>Keywords</b>: optimization, quadratic bandits, sample complexity, optimality
</p><p><b>Compressor summary</b>: The paper studies how to efficiently optimize quadratic functions using stochastic zeroth-order methods and introduces the concept of energy allocation to analyze the information-theoretic limitations and proposes a Hessian-independent algorithm that works well for all cases.</p><hr><h3>Flexible Attention-Based Multi-Policy Fusion for Efficient Deep Reinforcement Learning</h3>
<p>Zih-Yun Chiu, Yi-Lin Tuan, William Yang Wang, Michael C. Yip</p>
<p><a href='https://openreview.net/forum?id=lmXNcKhj4c'>https://openreview.net/forum?id=lmXNcKhj4c</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Deep Reinforcement Learning, Sample Efficiency, Generalizability, Multi-Policy Decision Making, Multi-Policy Continuous Control
</p><p><b>Compressor summary</b>: The paper introduces Knowledge-Grounded RL (KGRL), a reinforcement learning paradigm that fuses multiple knowledge policies for efficient and flexible learning, and proposes Knowledge-Inclusive Attention Network (KIAN) as a new actor architecture.</p><hr><h3>A General Framework for Robust G-Invariance in G-Equivariant Networks</h3>
<p>Sophia Sanborn, Nina Miolane</p>
<p><a href='https://openreview.net/forum?id=llP6lmMiXE'>https://openreview.net/forum?id=llP6lmMiXE</a></p>
<p><b>Keywords</b>: equivariance, group-equivariant cnns, invariance, pooling, convolutional neural networks
</p><p><b>Compressor summary</b>: The $G$-TC layer is a robust group-invariance method for $G$-CNNs that uses triple correlation theory to preserve signal structure and improve classification accuracy, working for both commutative and non-commutative groups.</p><hr><h3>Single-Pass Pivot Algorithm for Correlation Clustering. Keep it simple!</h3>
<p>Konstantin Makarychev, Sayak Chakrabarty</p>
<p><a href='https://openreview.net/forum?id=lkEiOZlmPm'>https://openreview.net/forum?id=lkEiOZlmPm</a></p>
<p><b>Keywords</b>: correlation clustering, Pivot algorithm, streaming
</p><p><b>Compressor summary</b>: The paper proposes a simplified and efficient algorithm for correlation clustering with improved memory usage compared to previous approaches.</p><hr><h3>Do SSL Models Have Déjà Vu? A Case of Unintended Memorization in Self-supervised Learning</h3>
<p>Casey Meehan, Florian Bordes, Pascal Vincent, Kamalika Chaudhuri, Chuan Guo</p>
<p><a href='https://openreview.net/forum?id=lkBygTc0SI'>https://openreview.net/forum?id=lkBygTc0SI</a></p>
<p><b>Keywords</b>: self-supervised learning, privacy, data reconstruction, memorization
</p><p><b>Compressor summary</b>: SSL models can accidentally remember specific parts of images they've seen before, which could expose private information and be a security risk.</p><hr><h3>A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression</h3>
<p>Tin Sum Cheng, Aurelien Lucchi, Anastasis Kratsios, Ivan Dokmanić, David Belius</p>
<p><a href='https://openreview.net/forum?id=lk6KDG6qI7'>https://openreview.net/forum?id=lk6KDG6qI7</a></p>
<p><b>Keywords</b>: Kernel, regression, bias-variance, generalization
</p><p><b>Compressor summary</b>: Existing statistical learning guarantees for general kernel regressors often yield loose bounds when used with finite-rank kernels. Yet, finite-rank kernels naturally appear in a number of machine learning problems, e.g. when fine-tuning a pre-trained deep neural network's last layer to adapt it to a novel task when performing transfer learning.  We address this gap for finite-rank kernel ridge regression (KRR) by deriving sharp non-asymptotic upper and lower bounds for the KRR test error of any finite-rank KRR. Our bounds are tighter than previously derived bounds on finite-rank KRR and, unlike comparable results, they also remain valid for any regularization parameters.</p><hr><h3>Nominality Score Conditioned Time Series Anomaly Detection by Point/Sequential Reconstruction</h3>
<p>Chih-Yu Lai, Fan-Keng Sun, Zhengqi Gao, Jeffrey Lang, Duane S Boning</p>
<p><a href='https://openreview.net/forum?id=ljgM3vNqfQ'>https://openreview.net/forum?id=ljgM3vNqfQ</a></p>
<p><b>Keywords</b>: time series, anomaly detection, point anomalies, contextual anomalies, nominality score, induced anomaly score
</p><p><b>Compressor summary</b>: The paper presents a framework for unsupervised time series anomaly detection using point-based and sequence-based models, and shows it beats most existing methods.</p><hr><h3>Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection</h3>
<p>Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, Song Mei</p>
<p><a href='https://openreview.net/forum?id=liMSqUuVg9'>https://openreview.net/forum?id=liMSqUuVg9</a></p>
<p><b>Keywords</b>: in-context learning, transformers, deep learning theory, learning theory
</p><p><b>Compressor summary</b>: The paragraph discusses how neural sequence models based on the transformer architecture can perform in-context learning and adaptively select different algorithms or tasks without explicit prompting, using a comprehensive statistical theory and experimental evidence.</p><hr><h3>Toward Re-Identifying Any Animal</h3>
<p>Bingliang Jiao, Lingqiao Liu, Liying Gao, Ruiqi Wu, Guosheng Lin, PENG WANG, Yanning Zhang</p>
<p><a href='https://openreview.net/forum?id=leS8668NJm'>https://openreview.net/forum?id=leS8668NJm</a></p>
<p><b>Keywords</b>: Re-identification, Category-generalizable
</p><p><b>Compressor summary</b>: The paper introduces a new task called Re-identify Any Animal in the Wild (ReID-AW) and proposes UniReID, a universal re-identification model that can handle any unseen wildlife category using dynamic prompts and semantic knowledge from GPT-4.</p><hr><h3>A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</h3>
<p>Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, Ming-Hsuan Yang</p>
<p><a href='https://openreview.net/forum?id=lds9D17HRd'>https://openreview.net/forum?id=lds9D17HRd</a></p>
<p><b>Keywords</b>: Semantic Correspondence, Diffusion Models, Vision Transformer, Representation
</p><p><b>Compressor summary</b>: The text describes how diffusion models can be used for image processing tasks and explores their properties, performance, and potential applications.</p><hr><h3>A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation</h3>
<p>Qitao Zhao, Ce Zheng, Mengyuan Liu, Chen Chen</p>
<p><a href='https://openreview.net/forum?id=lclQ2RvWYu'>https://openreview.net/forum?id=lclQ2RvWYu</a></p>
<p><b>Keywords</b>: Human Pose Estimation; 2D-to-3D Lifting; Context-Aware
</p><p><b>Compressor summary</b>: The proposed method improves 3D human pose estimation by using spatial context from 2D pose detectors without relying on temporal clues or additional computation.</p><hr><h3>Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory</h3>
<p>Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, Rui Yan</p>
<p><a href='https://openreview.net/forum?id=lYNSvp51a7'>https://openreview.net/forum?id=lYNSvp51a7</a></p>
<p><b>Keywords</b>: natural language processing, retrieval-augmented text generation, self memory
</p><p><b>Compressor summary</b>: The paper proposes a novel framework called selfmem that uses its own output as memory to improve text generation tasks like translation, summarization, and dialogue generation.</p><hr><h3>DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining</h3>
<p>Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V Le, Tengyu Ma, Adams Wei Yu</p>
<p><a href='https://openreview.net/forum?id=lXuByUeHhd'>https://openreview.net/forum?id=lXuByUeHhd</a></p>
<p><b>Keywords</b>: language models, pretraining, domain reweighting, data curation
</p><p><b>Compressor summary</b>: DoReMi is a method that uses a small proxy model to optimize mixture proportions of pretraining data domains and improve language model performance without knowing downstream tasks.</p><hr><h3>Entropy-based Training Methods for Scalable Neural Implicit Samplers</h3>
<p>Weijian Luo, Boya Zhang, Zhihua Zhang</p>
<p><a href='https://openreview.net/forum?id=lXOoR4KYcJ'>https://openreview.net/forum?id=lXOoR4KYcJ</a></p>
<p><b>Keywords</b>: implicit sampler, learning to sample, generative models
</p><p><b>Compressor summary</b>: The paper introduces a neural implicit sampler that efficiently generates samples from un-normalized target distributions using two novel training methods, and shows its effectiveness, efficiency, and scalability on three sampling benchmarks.</p><hr><h3>Not All Out-of-Distribution Data Are Harmful to Open-Set Active Learning</h3>
<p>Yang Yang, Yuxuan Zhang, XIN SONG, Yi Xu</p>
<p><a href='https://openreview.net/forum?id=lV3LIGlc1w'>https://openreview.net/forum?id=lV3LIGlc1w</a></p>
<p><b>Keywords</b>: Out-of-Distribution, Active Learning
</p><p><b>Compressor summary</b>: Progressive Active Learning (PAL) is a sampling scheme that selects valuable out-of-distribution instances and balances pseudo-ID and pseudo-OOD instances to improve both ID classifier and OOD detector performance in open-set active learning.</p><hr><h3>Unconstrained Dynamic Regret via Sparse Coding</h3>
<p>Zhiyu Zhang, Ashok Cutkosky, Ioannis Paschalidis</p>
<p><a href='https://openreview.net/forum?id=lT9n36RH1w'>https://openreview.net/forum?id=lT9n36RH1w</a></p>
<p><b>Keywords</b>: Dynamic online learning, parameter-free online learning, time series forecasting, wavelet
</p><p><b>Compressor summary</b>: The paper proposes adaptive regret bounds for online convex optimization under nonstationary comparator sequences by using sparse coding and energy-sparse complexity measures.</p><hr><h3>Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards</h3>
<p>Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, Matthieu Cord</p>
<p><a href='https://openreview.net/forum?id=lSbbC2VyCu'>https://openreview.net/forum?id=lSbbC2VyCu</a></p>
<p><b>Keywords</b>: Deep learning, Foundation models, Fine-tuning, Reward optimization, Linear mode connectivity, Weight averaging, Model soups, Robustness, Generalization, Alignment, Multi objective learning.
</p><p><b>Compressor summary</b>: The paper introduces rewarded soup, a multi-policy approach that aims for Pareto-optimal generalization across different rewards and preferences, by interpolating specialized networks' weights linearly.</p><hr><h3>Masked Space-Time Hash Encoding for Efficient Dynamic Scene Reconstruction</h3>
<p>Feng Wang, Zilong Chen, Guokang Wang, Yafei Song, Huaping Liu</p>
<p><a href='https://openreview.net/forum?id=lSLYXuLqRQ'>https://openreview.net/forum?id=lSLYXuLqRQ</a></p>
<p><b>Keywords</b>: NeRF, Dynamic Scenes
</p><p><b>Compressor summary</b>: MSTH is a novel method that efficiently reconstructs dynamic 3D scenes from videos by using a learnable mask to guide a weighted combination of 3D and 4D hash encodings, reducing redundancy and achieving better results than previous methods with less training time and memory storage.</p><hr><h3>Extensible Prompts for Language Models on Zero-shot Language Style Customization</h3>
<p>Tao Ge, Jing Hu, Li Dong, Shaoguang Mao, Yan Xia, Xun Wang, Si-Qing Chen, Furu Wei</p>
<p><a href='https://openreview.net/forum?id=lRxpVfDMzz'>https://openreview.net/forum?id=lRxpVfDMzz</a></p>
<p><b>Keywords</b>: large language model, prompt, imaginary words, OOD robustness, natural language, zero-shot
</p><p><b>Compressor summary</b>: X-Prompt uses imaginary words that can be understood by large language models to help them comprehend complex concepts and enable better interactions with humans.</p><hr><h3>Social Motion Prediction with Cognitive Hierarchies</h3>
<p>Wentao Zhu, Jason Qin, Yuke Lou, Hang Ye, Xiaoxuan Ma, Hai Ci, Yizhou Wang</p>
<p><a href='https://openreview.net/forum?id=lRu0dN7BY6'>https://openreview.net/forum?id=lRu0dN7BY6</a></p>
<p><b>Keywords</b>: multi-person motion prediction
</p><p><b>Compressor summary</b>: The study introduces a new benchmark, formulation, and framework for predicting 3D multi-person motions in team sports using behavioral cloning, generative adversarial imitation learning, and cognitive hierarchy.</p><hr><h3>Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration</h3>
<p>Haitao Lin, Yufei Huang, Odin Zhang, Yunfan Liu, Lirong Wu, Siyuan Li, Zhiyuan Chen, Stan Z. Li</p>
<p><a href='https://openreview.net/forum?id=lRG11M91dx'>https://openreview.net/forum?id=lRG11M91dx</a></p>
<p><b>Keywords</b>: sturcture-based drug design; molecule generation; diffusion model
</p><p><b>Compressor summary</b>: \textsc{D3FG} is a functional-group-based diffusion model that generates realistic and interacting molecules for protein pocket-specific drug design using graph neural networks.</p><hr><h3>Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models</h3>
<p>Geon Yeong Park, Jeongsol Kim, Beomsu Kim, Sang Wan Lee, Jong Chul Ye</p>
<p><a href='https://openreview.net/forum?id=lOCHMGO6ow'>https://openreview.net/forum?id=lOCHMGO6ow</a></p>
<p><b>Keywords</b>: Diffusion model, Energy-based model, Text-to-image generation
</p><p><b>Compressor summary</b>: The paper proposes a novel energy-based model framework that adapts context control for text-to-image diffusion models, improving semantic alignment and enabling zero-shot compositional generation for various image generation tasks.</p><hr><h3>Hypothesis Selection with Memory Constraints</h3>
<p>Maryam Aliakbarpour, Mark Bun, Adam Smith</p>
<p><a href='https://openreview.net/forum?id=lM1UnEssuX'>https://openreview.net/forum?id=lM1UnEssuX</a></p>
<p><b>Keywords</b>: Hypothesis selection, memory constrained algorithms, density estimation, limited space
</p><p><b>Compressor summary</b>: The paper studies how to select the best distribution from a set of candidates using limited memory and queries, achieving nearly optimal tradeoffs between memory usage and sample complexity.</p><hr><h3>On the Interplay between Social Welfare and Tractability of Equilibria</h3>
<p>Ioannis Anagnostides, Tuomas Sandholm</p>
<p><a href='https://openreview.net/forum?id=lM0xyViO90'>https://openreview.net/forum?id=lM0xyViO90</a></p>
<p><b>Keywords</b>: learning in games, optimistic gradient descent, Nash equilibrium, price of anarchy, smooth games, social welfare
</p><p><b>Compressor summary</b>: The paper connects efficiency, smoothness, and no-regret learning algorithms to study equilibrium computation in different game types and improve welfare bounds.</p><hr><h3>PDP: Parameter-free Differentiable Pruning is All You Need</h3>
<p>Minsik Cho, Saurabh Adya, Devang Naik</p>
<p><a href='https://openreview.net/forum?id=lLztVBaBVU'>https://openreview.net/forum?id=lLztVBaBVU</a></p>
<p><b>Keywords</b>: pruning, cnn, transformers
</p><p><b>Compressor summary</b>: PDP is an efficient and effective train-time pruning scheme for DNNs that works on various tasks, architectures, and pruning constraints, achieving state-of-the-art results in model size, accuracy, and training cost.</p><hr><h3>Unlimiformer: Long-Range Transformers with Unlimited Length Input</h3>
<p>Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley</p>
<p><a href='https://openreview.net/forum?id=lJWUJWLCJo'>https://openreview.net/forum?id=lJWUJWLCJo</a></p>
<p><b>Keywords</b>: retrieval augmentation, summarization, long-context, generation, long-input, encoder-decoder, transformers, language models, natural language generation, natural language processing, deep learning, neural networks
</p><p><b>Compressor summary</b>: The paper introduces Unlimiformer, a method that allows transformers to handle long inputs by using a kNN index for attention instead of attending to every token.</p><hr><h3>Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism</h3>
<p>Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Yunhe Wang, Kai Han</p>
<p><a href='https://openreview.net/forum?id=lJDoPAjkCV'>https://openreview.net/forum?id=lJDoPAjkCV</a></p>
<p><b>Keywords</b>: YOLO, object detection, computer vision
</p><p><b>Compressor summary</b>: The paper introduces Gold-YOLO, a new real-time object detection model that uses convolution and self-attention to improve feature fusion and achieves state-of-the-art performance while maintaining low latency.</p><hr><h3>The CLIP Model is Secretly an Image-to-Prompt Converter</h3>
<p>Yuxuan Ding, Chunna Tian, Haoxuan Ding, Lingqiao Liu</p>
<p><a href='https://openreview.net/forum?id=lHa7gFbmvS'>https://openreview.net/forum?id=lHa7gFbmvS</a></p>
<p><b>Keywords</b>: Diffusion Model, CLIP model, Image Variation, Customized Generation
</p><p><b>Compressor summary</b>: The paper shows how to use CLIP to convert images into text prompts for text-to-image generation, improving this ability with some training data or online steps, and enabling better image-text interactions for tasks like variation and editing.</p><hr><h3>AutoGO: Automated Computation Graph Optimization for Neural Network Evolution</h3>
<p>Mohammad Salameh, Keith G Mills, Negar Hassanpour, Fred X. Han, Shuting Zhang, Wei Lu, SHANGLING JUI, CHUNHUA ZHOU, Fengyu Sun, Di Niu</p>
<p><a href='https://openreview.net/forum?id=lDI3ZuyzM9'>https://openreview.net/forum?id=lDI3ZuyzM9</a></p>
<p><b>Keywords</b>: Neural Architecture Search, Optimization Framework, Performance Prediction
</p><p><b>Compressor summary</b>: AutoGO is a framework that optimizes neural networks by evolving their low-level computation graphs, improving performance and hardware compatibility for various computer vision tasks.</p><hr><h3>An information-theoretic quantification of the content of communication between brain regions</h3>
<p>Marco Celotto, Jan Bím, Alejandro Tlaie, Vito De Feo, Alessandro Toso, Stefan M Lemke, Daniel Chicharro, Hamed Nili, Malte Bieler, Ileana Livia Hanganu-Opatz, Tobias H. Donner, Andrea Brovelli, Stefano Panzeri</p>
<p><a href='https://openreview.net/forum?id=lD8xaUWw24'>https://openreview.net/forum?id=lD8xaUWw24</a></p>
<p><b>Keywords</b>: Information transmission; Brain data analysis; Sensory processing; Partial information decomposition
</p><p><b>Compressor summary</b>: The authors develop a new method called Feature-specific Information Transfer (FIT) that can measure how much information about specific features is transmitted between brain regions, beyond what traditional methods can reveal.</p><hr><h3>Team-PSRO for Learning Approximate TMECor in Large Team Games via Cooperative Reinforcement Learning</h3>
<p>Stephen Marcus McAleer, Gabriele Farina, Gaoyue Zhou, Mingzhi Wang, Yaodong Yang, Tuomas Sandholm</p>
<p><a href='https://openreview.net/forum?id=lCThtrJxoH'>https://openreview.net/forum?id=lCThtrJxoH</a></p>
<p><b>Keywords</b>: PSRO, team games, TMECor, populations, equilibrium, game theory, RL
</p><p><b>Compressor summary</b>: The paper introduces two algorithms for multi-player team games that balance game-theoretic guarantees with scalability and outperform existing methods on some experiments.</p><hr><h3>Adversarial Learning for Feature Shift Detection and Correction</h3>
<p>Míriam Barrabés, Daniel Mas Montserrat, Margarita Geleta, Xavier Giró-i-Nieto, Alexander G Ioannidis</p>
<p><a href='https://openreview.net/forum?id=lBhRTO2uWf'>https://openreview.net/forum?id=lBhRTO2uWf</a></p>
<p><b>Keywords</b>: feature shift detection, distribution shift, shift, data-centric AI
</p><p><b>Compressor summary</b>: The paper proposes a method to detect and fix feature shifts in data using adversarial learning and simple iterative heuristics, outperforming existing approaches.</p><hr><h3>Normalization Layers Are All That Sharpness-Aware Minimization Needs</h3>
<p>Maximilian Mueller, Tiffany Joyce Vlaar, David Rolnick, Matthias Hein</p>
<p><a href='https://openreview.net/forum?id=lArwl3y9x6'>https://openreview.net/forum?id=lArwl3y9x6</a></p>
<p><b>Keywords</b>: sharpness-aware minimization, flatness, generalization, normalization layers
</p><p><b>Compressor summary</b>: The paper explores how perturbing only the normalization parameters in the adversarial step of Sharpness-aware minimization (SAM) can improve generalization performance compared to perturbing all parameters or using alternative sparse perturbation methods.</p><hr><h3>DrugCLIP: Contrasive Protein-Molecule Representation Learning for Virtual Screening</h3>
<p>Bowen Gao, Bo Qiang, Haichuan Tan, Yinjun Jia, Minsi Ren, Minsi Lu, Jingjing Liu, Wei-Ying Ma, Yanyan Lan</p>
<p><a href='https://openreview.net/forum?id=lAbCgNcxm7'>https://openreview.net/forum?id=lAbCgNcxm7</a></p>
<p><b>Keywords</b>: Application, Drug Discovery, Representation Learning, Dataset Augmentation
</p><p><b>Compressor summary</b>: The paper proposes a new contrastive learning method called DrugCLIP for faster and better virtual screening of potential drugs using protein pocket and molecule representations without explicit binding-affinity scores.</p><hr><h3>Unsupervised Learning for Solving the Travelling Salesman Problem</h3>
<p>Yimeng Min, Yiwei Bai, Carla P Gomes</p>
<p><a href='https://openreview.net/forum?id=lAEc7aIW20'>https://openreview.net/forum?id=lAEc7aIW20</a></p>
<p><b>Keywords</b>: Combinatorial Optimization, Graph Neural Network, Travelling Salesman Problem
</p><p><b>Compressor summary</b>: UTSP is an unsupervised learning framework that uses a graph neural network and local search to solve the TSP, achieving better results than existing data-driven heuristics while being more efficient in terms of parameters and training samples.</p><hr><h3>Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces</h3>
<p>Martin Ryner, Jan Kronqvist, Johan Karlsson</p>
<p><a href='https://openreview.net/forum?id=l9MbuqzlZt'>https://openreview.net/forum?id=l9MbuqzlZt</a></p>
<p><b>Keywords</b>: Gromov-Wasserstein problem, QAP, Global optimization
</p><p><b>Compressor summary</b>: The paper proposes an efficient method for solving the Gromov-Wasserstein problem, which finds the best assignment between two sets of points preserving distances, using low-dimensional optimization.</p><hr><h3>Visual Instruction Inversion: Image Editing via Image Prompting</h3>
<p>Thao Nguyen, Yuheng Li, Utkarsh Ojha, Yong Jae Lee</p>
<p><a href='https://openreview.net/forum?id=l9BsCh8ikK'>https://openreview.net/forum?id=l9BsCh8ikK</a></p>
<p><b>Keywords</b>: image editing, diffusion models, visual prompting
</p><p><b>Compressor summary</b>: The paper proposes a method for image editing using visual prompts and text-to-image diffusion models to generate editing directions from examples.</p><hr><h3>Generative Category-level Object Pose Estimation via Diffusion Models</h3>
<p>Jiyao Zhang, Mingdong Wu, Hao Dong</p>
<p><a href='https://openreview.net/forum?id=l6ypbj6Nv5'>https://openreview.net/forum?id=l6ypbj6Nv5</a></p>
<p><b>Keywords</b>: Category-Level Object Pose Estimation, Diffusion Model
</p><p><b>Compressor summary</b>: The authors propose a novel solution for category-level object pose estimation using conditional generative modeling with score-based diffusion models, achieving state-of-the-art results on the REAL275 dataset and generalizing well to new categories.</p><hr><h3>Practical Contextual Bandits with Feedback Graphs</h3>
<p>Mengxiao Zhang, Yuheng Zhang, Olga Vrousgou, Haipeng Luo, Paul Mineiro</p>
<p><a href='https://openreview.net/forum?id=l6pYRbuHpO'>https://openreview.net/forum?id=l6pYRbuHpO</a></p>
<p><b>Keywords</b>: Online learning with feedback graphs, Contextual Bandits, Practical algorithms
</p><p><b>Compressor summary</b>: The paper proposes a method for contextual bandits with feedback graphs that simplifies learning by reducing it to regression and achieving minimal performance loss.</p><hr><h3>Fine-Grained Visual Prompting</h3>
<p>Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, Jian Yang</p>
<p><a href='https://openreview.net/forum?id=l6R4Go3noz'>https://openreview.net/forum?id=l6R4Go3noz</a></p>
<p><b>Keywords</b>: visual prompting, zero-shot, visual language model, referring expression comprehension
</p><p><b>Compressor summary</b>: The authors propose a fine-grained visual prompting method that uses pixel-level masks to improve zero-shot instance-level recognition of objects in images, achieving significant improvements over existing methods on several benchmarks.</p><hr><h3>Relative Entropic Optimal Transport: a (Prior-aware) Matching Perspective to (Unbalanced) Classification</h3>
<p>Liangliang Shi, Haoyu Zhen, Gu Zhang, Junchi Yan</p>
<p><a href='https://openreview.net/forum?id=l61Kp1zBwC'>https://openreview.net/forum?id=l61Kp1zBwC</a></p>
<p><b>Keywords</b>: Optimal Transport; Unbalanced Classification
</p><p><b>Compressor summary</b>: The paper proposes a new optimal transport method for classification, which connects optimal transport and machine learning concepts like barycentric projection and transfer learning.</p><hr><h3>FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space</h3>
<p>Shengzhong Liu, Tomoyoshi Kimura, Dongxin Liu, Ruijie Wang, Jinyang Li, Suhas Diggavi, Mani Srivastava, Tarek Abdelzaher</p>
<p><a href='https://openreview.net/forum?id=l4CZCKXoSn'>https://openreview.net/forum?id=l4CZCKXoSn</a></p>
<p><b>Keywords</b>: Multimodal Time Series; Contrastive Learning; Factorized Latent Space
</p><p><b>Compressor summary</b>: FOCAL is a novel contrastive learning framework for multimodal time series that considers modality-specific features and temporal information locality, improving performance in downstream tasks.</p><hr><h3>BIRD: Generalizable Backdoor Detection and Removal for Deep Reinforcement Learning</h3>
<p>Xuan Chen, Wenbo Guo, Guanhong Tao, Xiangyu Zhang, Dawn Song</p>
<p><a href='https://openreview.net/forum?id=l3yxZS3QdT'>https://openreview.net/forum?id=l3yxZS3QdT</a></p>
<p><b>Keywords</b>: Backdoor Defense, Deep Reinforcement Learning
</p><p><b>Compressor summary</b>: BIRD is a technique that detects and removes backdoors from pretrained deep reinforcement learning policies without needing information about the attack or access to its training process.</p><hr><h3>Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer</h3>
<p>Yuandong Tian, Yiping Wang, Beidi Chen, Simon Shaolei Du</p>
<p><a href='https://openreview.net/forum?id=l3HUgVHqGQ'>https://openreview.net/forum?id=l3HUgVHqGQ</a></p>
<p><b>Keywords</b>: transformer, training dynamics, theoretical analysis, self-attention, interpretability, neural network understanding
</p><p><b>Compressor summary</b>: The paper analyzes how self-attention in a 1-layer transformer works during training for next token prediction, showing that it acts as a discriminative scanning algorithm with a scan and snap dynamics.</p><hr><h3>Feature Likelihood Score: Evaluating the Generalization of Generative Models Using Samples</h3>
<p>Marco Jiralerspong, Joey Bose, Ian Gemp, Chongli Qin, Yoram Bachrach, Gauthier Gidel</p>
<p><a href='https://openreview.net/forum?id=l2VKZkolT7'>https://openreview.net/forum?id=l2VKZkolT7</a></p>
<p><b>Keywords</b>: Generative model, FID, Evaluation, Precision, Recall, Likelihood
</p><p><b>Compressor summary</b>: FLS is a new metric that assesses the novelty, fidelity, and diversity of generated data by deep generative models, addressing limitations of existing methods.</p><hr><h3>Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL</h3>
<p>Peng Cheng, Xianyuan Zhan, Zhihao Wu, Wenjia Zhang, Youfang Lin, Shou cheng Song, Han Wang, Li Jiang</p>
<p><a href='https://openreview.net/forum?id=kyXMU3H7RB'>https://openreview.net/forum?id=kyXMU3H7RB</a></p>
<p><b>Keywords</b>: sample efficiency; offline reinforcement learning; fundamental symmetry
</p><p><b>Compressor summary</b>: The paper proposes a new offline reinforcement learning algorithm (TSRL) that leverages time-reversal symmetry of system dynamics to improve performance on small datasets and enhance data efficiency and generalizability.</p><hr><h3>Minimum-Risk Recalibration of Classifiers</h3>
<p>Zeyu Sun, Dogyoon Song, Alfred Hero</p>
<p><a href='https://openreview.net/forum?id=kvXcHfBghm'>https://openreview.net/forum?id=kvXcHfBghm</a></p>
<p><b>Keywords</b>: probability calibration, optimal number of bins, label shift adaptation
</p><p><b>Compressor summary</b>: The paper introduces minimum-risk recalibration using MSE decomposition, analyzes UMB recalibration, and proposes a two-stage approach for label shift adaptation.</p><hr><h3>PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning.</h3>
<p>Mingjia Shi, Yuhao Zhou, Kai Wang, Huaizheng Zhang, Shudong Huang, Qing Ye, Jiancheng Lv</p>
<p><a href='https://openreview.net/forum?id=kuxu4lCRr5'>https://openreview.net/forum?id=kuxu4lCRr5</a></p>
<p><b>Keywords</b>: Federated Learning, Personalized Federated Learning, Expectation Maximization, Relaxed Mirror Descent
</p><p><b>Compressor summary</b>: The paper proposes a new method, pFedBreD, for personalized federated learning that injects prior knowledge into the global model to improve performance on various datasets.</p><hr><h3>Computing Optimal Nash Equilibria in Multiplayer Games</h3>
<p>Youzhi Zhang, Bo An, Venkatramanan Siva Subrahmanian</p>
<p><a href='https://openreview.net/forum?id=kupNhxLc6k'>https://openreview.net/forum?id=kupNhxLc6k</a></p>
<p><b>Keywords</b>: Algorithmic game theory, Optimal Nash equilibrium
</p><p><b>Compressor summary</b>: The paper proposes a new algorithm, CRM, to efficiently compute a Nash Equilibrium in multiplayer games by using correlation plans to reduce the solution space and improve the speed.</p><hr><h3>DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models</h3>
<p>Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, Sibei Yang</p>
<p><a href='https://openreview.net/forum?id=ktYjrgOENR'>https://openreview.net/forum?id=ktYjrgOENR</a></p>
<p><b>Keywords</b>: Chain-of-Thought Reasoning, Multimodal Science Question Answering, Vision and Langauge
</p><p><b>Compressor summary</b>: The text discusses challenges in applying chain of thought reasoning to multimodal contexts and proposes a new method called DDCoT that improves reasoning abilities, generalizability, and explainability of AI models.</p><hr><h3>Multi-task learning with summary statistics</h3>
<p>Parker Knight, Rui Duan</p>
<p><a href='https://openreview.net/forum?id=ktTSji9ZIs'>https://openreview.net/forum?id=ktTSji9ZIs</a></p>
<p><b>Keywords</b>: multi-task learning, genetic risk prediction, summary statistics
</p><p><b>Compressor summary</b>: The paper proposes a multi-task learning framework using summary statistics and adaptive parameter selection for applications where data-sharing is restricted, such as healthcare, and analyzes its performance theoretically and empirically.</p><hr><h3>Towards Last-layer Retraining for Group Robustness with Fewer Annotations</h3>
<p>Tyler LaBonte, Vidya Muthukumar, Abhishek Kumar</p>
<p><a href='https://openreview.net/forum?id=kshC3NOP6h'>https://openreview.net/forum?id=kshC3NOP6h</a></p>
<p><b>Keywords</b>: spurious correlations, group robustness, last-layer retraining, distribution shift
</p><p><b>Compressor summary</b>: The paper proposes a method called SELF that improves group robustness of neural networks by retuning the last layer using disagreements or misclassifications, without needing group annotations.</p><hr><h3>SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data</h3>
<p>BANG AN, Xun Zhou, Yongjian Zhong, Tianbao Yang</p>
<p><a href='https://openreview.net/forum?id=ks7Mf5lzSx'>https://openreview.net/forum?id=ks7Mf5lzSx</a></p>
<p><b>Keywords</b>: urban event, NDCG optimization, ranking, traffic accident, crime, spatiotemporal data
</p><p><b>Compressor summary</b>: SpatialRank is a novel method for urban event ranking that uses adaptive graph convolution layers and a hybrid NDCG loss to predict the top-k most risky locations based on spatiotemporal dependencies and neighboring spatial proximity.</p><hr><h3>Easy Learning from Label Proportions</h3>
<p>Robert Istvan Busa-Fekete, Heejin Choi, Travis Dick, Claudio Gentile, Andres Munoz medina</p>
<p><a href='https://openreview.net/forum?id=kqBUgrkm1c'>https://openreview.net/forum?id=kqBUgrkm1c</a></p>
<p><b>Keywords</b>: learning with partial information, unbiased loss, classification, proportion matching
</p><p><b>Compressor summary</b>: EASYLLP is a debiasing technique for weakly supervised classification that uses aggregate labels and works with various loss functions, improving instance level performance on Learning from Label Proportions.</p><hr><h3>Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks</h3>
<p>Mingze Wang, Chao Ma</p>
<p><a href='https://openreview.net/forum?id=konBXvt2iS'>https://openreview.net/forum?id=konBXvt2iS</a></p>
<p><b>Keywords</b>: non-convex optimization, training dynamics, neural network
</p><p><b>Compressor summary</b>: The paper analyzes the training process of a two-layer ReLU network using Gradient Flow and identifies four phases and specific nonlinear behaviors that occur during learning.</p><hr><h3>Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples</h3>
<p>Hao Sun, Alihan Hüyük, Daniel Jarrett, Mihaela van der Schaar</p>
<p><a href='https://openreview.net/forum?id=kmbG9iBRIb'>https://openreview.net/forum?id=kmbG9iBRIb</a></p>
<p><b>Keywords</b>: Accountability, Reinforcement Learning, Batched Control, Accountable Decision-Making, Offline RL, Interpretability in RL
</p><p><b>Compressor summary</b>: The paper presents AOC, a controller that uses an offline dataset as the Decision Corpus and selects a subset of examples for accountable control in low-data scenarios, evaluating it in simulated and real healthcare settings.</p><hr><h3>Offline Minimax Soft-Q-learning Under Realizability and Partial Coverage</h3>
<p>Masatoshi Uehara, Nathan Kallus, Jason D. Lee, Wen Sun</p>
<p><a href='https://openreview.net/forum?id=kjkLJ7NJJZ'>https://openreview.net/forum?id=kjkLJ7NJJZ</a></p>
<p><b>Keywords</b>: Reinforcement learning theory, PAC RL, Offline Reinforcement learning
</p><p><b>Compressor summary</b>: The paper proposes value-based algorithms for offline reinforcement learning that can achieve PAC guarantees even with partial coverage of data, using novel minimax loss functions derived from Lagrange functions in nonlinear convex optimization problems.</p><hr><h3>Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation</h3>
<p>David Brandfonbrener, Ofir Nachum, Joan Bruna</p>
<p><a href='https://openreview.net/forum?id=kjMGHTo8Cs'>https://openreview.net/forum?id=kjMGHTo8Cs</a></p>
<p><b>Keywords</b>: representation learning, imitation learning
</p><p><b>Compressor summary</b>: This paper explores how large datasets with multitask demonstrations can be pretrained using inverse dynamics modeling to learn low dimensional representations for imitation learning in unknown environments.</p><hr><h3>On permutation symmetries in Bayesian neural network posteriors: a variational perspective</h3>
<p>Simone Rossi, Ankit Singh, Thomas Hannagan</p>
<p><a href='https://openreview.net/forum?id=kj33zJ9Vue'>https://openreview.net/forum?id=kj33zJ9Vue</a></p>
<p><b>Keywords</b>: Bayesian deep learning, approximate inference, permutation symmetries
</p><p><b>Compressor summary</b>: The authors extend the formalism of marginalized loss barrier and solution interpolation to Bayesian neural networks, propose an algorithm to search for linearly connected solutions, and experiment with different architectures and datasets.</p><hr><h3>End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes</h3>
<p>Alexandre Max Maraval, Matthieu Zimmer, Antoine Grosnit, Haitham Bou Ammar</p>
<p><a href='https://openreview.net/forum?id=kfWzpZvEUh'>https://openreview.net/forum?id=kfWzpZvEUh</a></p>
<p><b>Keywords</b>: meta-learning, bayesian optimisation, neural process, transformer, end-to-end, reinforcement learning
</p><p><b>Compressor summary</b>: The paper presents a differentiable meta-BO framework that uses neural processes and transformers to learn acquisition functions with RL and an auxiliary task to handle sparse rewards.</p><hr><h3>TextDiffuser: Diffusion Models as Text Painters</h3>
<p>Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei</p>
<p><a href='https://openreview.net/forum?id=ke3RgcDmfO'>https://openreview.net/forum?id=ke3RgcDmfO</a></p>
<p><b>Keywords</b>: Diffusion Model; Text Rendering
</p><p><b>Compressor summary</b>: TextDiffuser is a new method for generating images with readable and coherent text that works by first generating keyword layouts with a Transformer model and then using diffusion models to create images based on the layout and text prompts. It also introduces a large-scale dataset and evaluation tool for text rendering quality.</p><hr><h3>Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition</h3>
<p>Shuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai Zheng, Mu Li, Alex Smola, Xu Sun</p>
<p><a href='https://openreview.net/forum?id=kdFR6IUEW6'>https://openreview.net/forum?id=kdFR6IUEW6</a></p>
<p><b>Keywords</b>: Prompt Pre-Training, CLIP, Open-Vocabulary Visual Recognition
</p><p><b>Compressor summary</b>: POMP is a prompt pre-training method for vision-language models that improves visual recognition tasks with zero-shot learning and achieves state-of-the-art performances on 21 datasets.</p><hr><h3>DESSERT: An Efficient Algorithm for Vector Set Search with Vector Set Queries</h3>
<p>Joshua Engels, Benjamin Coleman, Vihan Lakshman, Anshumali Shrivastava</p>
<p><a href='https://openreview.net/forum?id=kXfrlWXLwH'>https://openreview.net/forum?id=kXfrlWXLwH</a></p>
<p><b>Keywords</b>: Embedding Based Retrieval, Passage Ranking, Locality Sensitive Hashing, Randomized Algorithms
</p><p><b>Compressor summary</b>: The paper introduces DESSERT, a fast approximate search algorithm for vector set queries, which can be integrated into semantic search models like ColBERT to improve their speed and performance.</p><hr><h3>DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model</h3>
<p>Xiuye Gu, Yin Cui, Jonathan Huang, Abdullah Rashwan, Xuan Yang, Xingyi Zhou, Golnaz Ghiasi, Weicheng Kuo, Huizhong Chen, Liang-Chieh Chen, David A Ross</p>
<p><a href='https://openreview.net/forum?id=kXOXrVnwbb'>https://openreview.net/forum?id=kXOXrVnwbb</a></p>
<p><b>Keywords</b>: universal segmentation, multi-task segmentation, multi-dataset segmentation, panoptic segmentation, semantic segmentation, instance segmentation, weakly-supervised segmentation
</p><p><b>Compressor summary</b>: The paper proposes DaTaSeg, a universal multi-dataset multi-task segmentation model that leverages weak-supervision, text embeddings, and merge operations to improve performance across different tasks and datasets, including open-vocabulary segmentation.</p><hr><h3>Towards Efficient Pre-Trained Language Model via Feature Correlation Distillation</h3>
<p>Kun Huang, Xin Guo, Meng Wang</p>
<p><a href='https://openreview.net/forum?id=kVfHQV668B'>https://openreview.net/forum?id=kVfHQV668B</a></p>
<p><b>Keywords</b>: Knowledge Distillation; Pre-Trained Language Model
</p><p><b>Compressor summary</b>: FCD is a new method for compressing PLMs that uses output feature relationships to transfer knowledge more effectively and achieves better results than previous KD approaches.</p><hr><h3>Bayesian Active Causal Discovery with Multi-Fidelity Experiments</h3>
<p>Zeyu Zhang, Chaozhuo Li, Xu Chen, Xing Xie</p>
<p><a href='https://openreview.net/forum?id=kS8rIH43Zc'>https://openreview.net/forum?id=kS8rIH43Zc</a></p>
<p><b>Keywords</b>: Causal Discovery, Active Learning, Multi-fidelity
</p><p><b>Compressor summary</b>: The paper presents a probabilistic model for multi-fidelity active causal discovery, with a mutual-information based acquisition function and a cascading model to capture correlations between different fidelity oracles, and extends it to batch intervention scenarios.</p><hr><h3>A Fractional Graph Laplacian Approach to Oversmoothing</h3>
<p>Sohir Maskey, Raffaele Paolino, Aras Bacho, Gitta Kutyniok</p>
<p><a href='https://openreview.net/forum?id=kS7ED7eE74'>https://openreview.net/forum?id=kS7ED7eE74</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Graph Neural ODE, Fractional Laplacian, Oversmoothing
</p><p><b>Compressor summary</b>: The paper introduces fractional graph Laplacian neural ODEs for directed graphs to capture long-range dependencies and mitigate oversmoothing in graph neural networks.</p><hr><h3>Inferring Hybrid Neural Fluid Fields from Videos</h3>
<p>Hong-Xing Yu, Yang Zheng, Yuan Gao, Yitong Deng, Bo Zhu, Jiajun Wu</p>
<p><a href='https://openreview.net/forum?id=kRdaTkaBwC'>https://openreview.net/forum?id=kRdaTkaBwC</a></p>
<p><b>Keywords</b>: neural scene representations, fluid dynamics, flow reconstruction, physics-based learning
</p><p><b>Compressor summary</b>: The paper presents HyFluid, a neural method to jointly estimate fluid density and velocity from sparse multiview videos, using physics-based losses and a hybrid neural velocity representation that handles visual ambiguities and turbulence in fluid flows.</p><hr><h3>NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA</h3>
<p>Hyeong Kyu Choi, Seunghun Lee, Jaewon Chu, Hyunwoo J. Kim</p>
<p><a href='https://openreview.net/forum?id=kR5ycmBclj'>https://openreview.net/forum?id=kR5ycmBclj</a></p>
<p><b>Keywords</b>: Knowledge Graph Question Answering, Knowledge Graph, Graph Neural Networks
</p><p><b>Compressor summary</b>: NuTrea is a tree search-based GNN model that improves multi-hop KGQA by incorporating broader KG context and handling ambiguous nodes.</p><hr><h3>Subclass-Dominant Label Noise: A Counterexample for the Success of Early Stopping</h3>
<p>Yingbin Bai, Zhongyi Han, Erkun Yang, Jun Yu, Bo Han, Dadong Wang, Tongliang Liu</p>
<p><a href='https://openreview.net/forum?id=kR21XsZeAr'>https://openreview.net/forum?id=kR21XsZeAr</a></p>
<p><b>Keywords</b>: learning with noisy labels, weakly supervised learning
</p><p><b>Compressor summary</b>: The paper studies a type of label noise called subclass-dominant label noise (SDN) and proposes a method called NoiseCluster that uses long-trained representations to identify and correct it, achieving better results than existing methods on synthetic and real data.</p><hr><h3>Online Ad Allocation with Predictions</h3>
<p>Fabian Christian Spaeh, Alina Ene</p>
<p><a href='https://openreview.net/forum?id=kPfd3pcwHV'>https://openreview.net/forum?id=kPfd3pcwHV</a></p>
<p><b>Keywords</b>: Learning Augmented Algorithms, Display Ads, Generalized Assignment Problem
</p><p><b>Compressor summary</b>: The paper presents a learning-augmented algorithm for online packing problems that uses machine-learned predictions to improve performance beyond worst-case algorithms, and shows its effectiveness on synthetic and real data.</p><hr><h3>Abide by the law and follow the flow: conservation laws for gradient flows</h3>
<p>Sibylle Marcotte, Rémi Gribonval, Gabriel Peyré</p>
<p><a href='https://openreview.net/forum?id=kMueEV8Eyy'>https://openreview.net/forum?id=kMueEV8Eyy</a></p>
<p><b>Keywords</b>: Implicit bias, conservation laws, gradient flow, linear neural network, matrix factorization
</p><p><b>Compressor summary</b>: The article explores how to identify and count conservation laws, which describe quantities conserved during gradient flows of a given model, and how these laws relate to optimization initialization properties in over-parameterized models.</p><hr><h3>Point Cloud Completion with Pretrained Text-to-Image Diffusion Models</h3>
<p>Yoni Kasten, Ohad Rahamim, Gal Chechik</p>
<p><a href='https://openreview.net/forum?id=kMmAYbT0VL'>https://openreview.net/forum?id=kMmAYbT0VL</a></p>
<p><b>Keywords</b>: Point Cloud, Text, 3D
</p><p><b>Compressor summary</b>: SDS-Complete uses a text-to-image diffusion model to complete incomplete point clouds of various objects without needing extensive 3D data, improving performance on out-of-distribution objects.</p><hr><h3>On the choice of Perception Loss Function for Learned Video Compression</h3>
<p>Sadaf Salehkalaibar, Truong Buu Phan, Jun Chen, Wei Yu, Ashish J Khisti</p>
<p><a href='https://openreview.net/forum?id=kLIieSS2P3'>https://openreview.net/forum?id=kLIieSS2P3</a></p>
<p><b>Keywords</b>: Video Compression, Information Theory, Neural Compression
</p><p><b>Compressor summary</b>: The choice of perception loss function affects video compression quality, especially at low bit rates, but encoded representations can be near universal and work for either choice of function.</p><hr><h3>ConDaFormer: Disassembled Transformer with Local Structure Enhancement for 3D Point Cloud Understanding</h3>
<p>Lunhao Duan, Shanshan Zhao, Nan Xue, Mingming Gong, Gui-Song Xia, Dacheng Tao</p>
<p><a href='https://openreview.net/forum?id=kKXJkiniOx'>https://openreview.net/forum?id=kKXJkiniOx</a></p>
<p><b>Keywords</b>: Point Cloud, Transformer, 3D Segmentation, 3D object detection
</p><p><b>Compressor summary</b>: The paper proposes ConDaFormer, a new transformer block for 3D point cloud understanding that reduces costs and models local geometry prior by disassembling cubic windows into three orthogonal 2D planes and using depth-wise convolution to enhance local structure.</p><hr><h3>On Learning Necessary and Sufficient Causal Graphs</h3>
<p>Hengrui Cai, Yixin Wang, Michael Jordan, Rui Song</p>
<p><a href='https://openreview.net/forum?id=kKFDMtpeDW'>https://openreview.net/forum?id=kKFDMtpeDW</a></p>
<p><b>Keywords</b>: Causal structural learning, Necessity and sufficiency, Natural causal effects, Probabilities of causation, Variable selection
</p><p><b>Compressor summary</b>: The paper proposes a method to learn a subset of causally relevant variables from a complex graph using probabilities of causation, which can improve causal estimation in various fields.</p><hr><h3>When Do Graph Neural Networks Help with Node Classification? Investigating the Homophily Principle on Node Distinguishability</h3>
<p>Sitao Luan, Chenqing Hua, Minkai Xu, Qincheng Lu, Jiaqi Zhu, Xiao-Wen Chang, Jie Fu, Jure Leskovec, Doina Precup</p>
<p><a href='https://openreview.net/forum?id=kJmYu3Ti2z'>https://openreview.net/forum?id=kJmYu3Ti2z</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Homophily, Heterophily, Low-pass filter, High-pass filter, Node Distinguishability, Metrics
</p><p><b>Compressor summary</b>: The paper studies how the similarity of node neighborhood patterns (intra-class Node Distinguishability) and differences between classes (inter-class ND) affect Graph Neural Networks' performance, proposing new metrics to measure them and a hypothesis-testing based performance metric that goes beyond homophily.</p><hr><h3>On the Identifiability of Sparse ICA without Assuming Non-Gaussianity</h3>
<p>Ignavier Ng, Yujia Zheng, Xinshuai Dong, Kun Zhang</p>
<p><a href='https://openreview.net/forum?id=kJIibP5bq2'>https://openreview.net/forum?id=kJIibP5bq2</a></p>
<p><b>Keywords</b>: independent component analysis, second-order statistics, sparsity
</p><p><b>Compressor summary</b>: The paper presents a new method for identifying Gaussian sources using independent component analysis that does not require non-Gaussian assumptions or restrictive connective structures, and provides experimental validation.</p><hr><h3>Limits, approximation and size transferability for GNNs on sparse graphs via graphops</h3>
<p>Thien Le, Stefanie Jegelka</p>
<p><a href='https://openreview.net/forum?id=kDQwossJuI'>https://openreview.net/forum?id=kDQwossJuI</a></p>
<p><b>Keywords</b>: graph neural networks, convolution, graph limits, size transferability
</p><p><b>Compressor summary</b>: The paper investigates how well graph neural networks (GNNs) can generalize to different graphs, especially sparse ones, using a theoretical approach based on graphops.</p><hr><h3>Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning?</h3>
<p>Jialu Gao, Kaizhe Hu, Guowei Xu, Huazhe Xu</p>
<p><a href='https://openreview.net/forum?id=kChEBODIx9'>https://openreview.net/forum?id=kChEBODIx9</a></p>
<p><b>Keywords</b>: Visual Reinforcement Learning, Large Generative Models, Image Editing, Robotics
</p><p><b>Compressor summary</b>: LfVoid is a method that uses text-to-image models and image editing to guide robots based on natural language instructions, even without in-domain training or true goal observations.</p><hr><h3>Coherent Soft Imitation Learning</h3>
<p>Joe Watson, Sandy Huang, Nicolas Heess</p>
<p><a href='https://openreview.net/forum?id=kCCD8d2aEu'>https://openreview.net/forum?id=kCCD8d2aEu</a></p>
<p><b>Keywords</b>: imitation learning, inverse reinforcement learning, behavioral cloning, learning from demonstration
</p><p><b>Compressor summary</b>: The authors propose a hybrid imitation learning method that combines behavioral cloning and reinforcement learning to learn from expert demonstrations and achieve complex tasks with minimal hyperparameter tuning.</p><hr><h3>SAME: Uncovering GNN Black Box with Structure-aware Shapley-based Multipiece Explanations</h3>
<p>Ziyuan Ye, Rihan Huang, Qilin Wu, Quanying Liu</p>
<p><a href='https://openreview.net/forum?id=kBBsj9KRgh'>https://openreview.net/forum?id=kBBsj9KRgh</a></p>
<p><b>Keywords</b>: GNN explainability, Shapley value, Monte Carlo tree search, structure awareness, multi-grained explanation
</p><p><b>Compressor summary</b>: SAME is a new method for explaining graph neural networks that uses Monte Carlo tree search to explore structure-aware substructures and improves explanation fidelity on various benchmarks.</p><hr><h3>Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design</h3>
<p>Matthew Thomas Jackson, Minqi Jiang, Jack Parker-Holder, Risto Vuorio, Chris Lu, Gregory Farquhar, Shimon Whiteson, Jakob Nicolaus Foerster</p>
<p><a href='https://openreview.net/forum?id=kAU6Cdq1gV'>https://openreview.net/forum?id=kAU6Cdq1gV</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Meta-Learning, Meta-RL, Meta-Optimization, Policy Meta-Optimization, Environment Design, Unsupervised Environment Design, Auto-Curricula
</p><p><b>Compressor summary</b>: This paper explores how meta-learning update rules for deep reinforcement learning can improve generalization and proposes GROOVE, a novel method that generates curricula to maximize the regret of a meta-learned optimizer.</p><hr><h3>Open Compound Domain Adaptation with Object Style Compensation for Semantic Segmentation</h3>
<p>Tingliang Feng, Hao Shi, Xueyang Liu, Wei Feng, Liang Wan, Yanlin Zhou, Di Lin</p>
<p><a href='https://openreview.net/forum?id=k9zSU3pdi4'>https://openreview.net/forum?id=k9zSU3pdi4</a></p>
<p><b>Keywords</b>: Object Style Compensation, Open Compound Domain Adaptation, Semantic Segmentation
</p><p><b>Compressor summary</b>: The paper introduces Object Style Compensation, a method that adapts object styles across domains for semantic image segmentation by using an Object-Level Discrepancy Memory.</p><hr><h3>PDF: Point Diffusion Implicit Function for Large-scale Scene Neural Representation</h3>
<p>Yuhan Ding, Fukun Yin, Jiayuan Fan, Hui Li, Xin Chen, Wen Liu, Chongshan Lu, Gang YU, Tao Chen</p>
<p><a href='https://openreview.net/forum?id=k8U8ZijXHh'>https://openreview.net/forum?id=k8U8ZijXHh</a></p>
<p><b>Keywords</b>: implicit neural representation; diffusion; point cloud; volume rendering
</p><p><b>Compressor summary</b>: The authors propose a new method called Point Diffusion implicit Function (PDF) that uses point cloud super-resolution and background modeling to represent and synthesize large-scale outdoor scenes for novel view synthesis, achieving better results than existing methods.</p><hr><h3>L2T-DLN: Learning to Teach with Dynamic Loss Network</h3>
<p>Zhaoyang Hai, Liyuan Pan, Xiabi Liu, Zhengzheng Liu, Mirna Yunita</p>
<p><a href='https://openreview.net/forum?id=k6yNi6DEqK'>https://openreview.net/forum?id=k6yNi6DEqK</a></p>
<p><b>Keywords</b>: Learning to teach, dynamic loss function, optimization
</p><p><b>Compressor summary</b>: The paper proposes a teacher model with memory units and a Dynamic Loss Network to adjust loss functions adaptively based on both the states of the student model and the loss function, leading to improved deep learning performance on various tasks.</p><hr><h3>Streaming Algorithms and Lower Bounds for Estimating Correlation Clustering Cost</h3>
<p>Sepehr Assadi, Vihan Shah, Chen Wang</p>
<p><a href='https://openreview.net/forum?id=k4ZCORSFEd'>https://openreview.net/forum?id=k4ZCORSFEd</a></p>
<p><b>Keywords</b>: Correlation Clustering, Graph Streaming Algorithms, Large-scale Clustering, Graph Learning
</p><p><b>Compressor summary</b>: This paper studies streaming correlation clustering with very limited memory, proposing two novel algorithms that estimate the optimal cost up to a constant factor and some additional error.</p><hr><h3>L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference</h3>
<p>Julia Linhart, Alexandre Gramfort, Pedro L. C. Rodrigues</p>
<p><a href='https://openreview.net/forum?id=k2UVKezeWn'>https://openreview.net/forum?id=k2UVKezeWn</a></p>
<p><b>Keywords</b>: machine learning, calibration, simulation-based inference, neuroscience, normalizing flows, classifier two-sample tests
</p><p><b>Compressor summary</b>: The paper introduces a new method, $\ell$-C2ST, for evaluating deep generative models' approximations of complex posterior distributions at specific observations, improving interpretability and performance over existing approaches.</p><hr><h3>Lookaround Optimizer: $k$ steps around, 1 step average</h3>
<p>Jiangtao Zhang, Shunyu Liu, Jie Song, Tongtian Zhu, Zhengqi Xu, Mingli Song</p>
<p><a href='https://openreview.net/forum?id=k1Xy5zCNOJ'>https://openreview.net/forum?id=k1Xy5zCNOJ</a></p>
<p><b>Keywords</b>: Deep Learning, Computer Vision, Mode Connectivity, Weight Average
</p><p><b>Compressor summary</b>: Lookaround is a new optimizer that improves diversity and generalization in deep networks by training multiple networks on transformed data and averaging them, leading to flatter minima.</p><hr><h3>Metropolis Sampling for Constrained Diffusion Models</h3>
<p>Nic Fishman, Leo Klarner, Emile Mathieu, Michael John Hutchinson, Valentin De Bortoli</p>
<p><a href='https://openreview.net/forum?id=jzseUq55eP'>https://openreview.net/forum?id=jzseUq55eP</a></p>
<p><b>Keywords</b>: diffusion model, generative modelling, manifold, constraints, proteins, robotics
</p><p><b>Compressor summary</b>: The paper proposes a simple noising scheme based on Metropolis sampling for diffusion models with domain-informed constraints, improving efficiency and performance over existing methods.</p><hr><h3>Generalized test utilities for long-tail performance in extreme multi-label classification</h3>
<p>Erik Schultheis, Marek Wydmuch, Wojciech Kotlowski, Rohit Babbar, Krzysztof Dembczynski</p>
<p><a href='https://openreview.net/forum?id=jze2r6RDFz'>https://openreview.net/forum?id=jze2r6RDFz</a></p>
<p><b>Keywords</b>: extreme multi-label classification, long-tail labels performance, complex performance measures
</p><p><b>Compressor summary</b>: The paper proposes a new metric for extreme multi-label classification that captures the importance of rare labels and optimizes expected test utility using efficient prediction rules.</p><hr><h3>Ess-InfoGAIL: Semi-supervised Imitation Learning from Imbalanced Demonstrations</h3>
<p>Huiqiao Fu, Kaiqiang Tang, Yuanyang Lu, Yiming Qi, Guizhou Deng, Flood Sung, Chunlin Chen</p>
<p><a href='https://openreview.net/forum?id=jxhUNLoi4m'>https://openreview.net/forum?id=jxhUNLoi4m</a></p>
<p><b>Keywords</b>: Generative adversarial imitation learning, semi-supervised learning, multi-modal behaviors, imbalanced data
</p><p><b>Compressor summary</b>: Imitation learning can reproduce expert behaviors without explicit rewards, but real-world demos are challenging; a new semi-supervised method learns disentangled behavior representations from imbalanced data using generative adversarial networks and information maximization.</p><hr><h3>Auxiliary Losses for Learning Generalizable Concept-based Models</h3>
<p>Ivaxi Sheth, Samira Ebrahimi Kahou</p>
<p><a href='https://openreview.net/forum?id=jvYXln6Gzn'>https://openreview.net/forum?id=jvYXln6Gzn</a></p>
<p><b>Keywords</b>: Interpretability, concept bottleneck models, explainability
</p><p><b>Compressor summary</b>: The paper introduces coop-CBM, a transparent neural network model that preserves concept representations and improves performance under different data distributions.</p><hr><h3>Improving Language Plasticity via Pretraining with Active Forgetting</h3>
<p>Yihong Chen, Kelly Marchisio, Roberta Raileanu, David Ifeoluwa Adelani, Pontus Stenetorp, Sebastian Riedel, Mikel Artetxe</p>
<p><a href='https://openreview.net/forum?id=jvEbQBxd8X'>https://openreview.net/forum?id=jvEbQBxd8X</a></p>
<p><b>Keywords</b>: plasticity, continual learning, meta-learning, embeddings, cross-lingual transfer, forgetting
</p><p><b>Compressor summary</b>: The paper proposes using active forgetting during pretraining to create PLMs that can adapt quickly and effectively to new languages, especially when there is little data available.</p><hr><h3>Deep Reinforcement Learning with Plasticity Injection</h3>
<p>Evgenii Nikishin, Junhyuk Oh, Georg Ostrovski, Clare Lyle, Razvan Pascanu, Will Dabney, Andre Barreto</p>
<p><a href='https://openreview.net/forum?id=jucDLW6G9l'>https://openreview.net/forum?id=jucDLW6G9l</a></p>
<p><b>Keywords</b>: deep reinforcement learning, continual learning, loss of plasticity
</p><p><b>Compressor summary</b>: The paper proposes plasticity injection, a method to increase neural network plasticity in deep reinforcement learning without changing parameters or predictions, and demonstrates its usefulness as a diagnostic and improvement tool for RL agents.</p><hr><h3>Simple and Controllable Music Generation</h3>
<p>Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez</p>
<p><a href='https://openreview.net/forum?id=jtiQ26sCJi'>https://openreview.net/forum?id=jtiQ26sCJi</a></p>
<p><b>Keywords</b>: Music generation, Generative AI, Transformer, Language Models
</p><p><b>Compressor summary</b>: MusicGen is a single transformer model that generates high-quality music based on text or melodic features, outperforming previous methods.</p><hr><h3>Fine-grained Expressivity of Graph Neural Networks</h3>
<p>Jan Böker, Ron Levie, Ningyuan Teresa Huang, Soledad Villar, Christopher Morris</p>
<p><a href='https://openreview.net/forum?id=jt10uWlEbc'>https://openreview.net/forum?id=jt10uWlEbc</a></p>
<p><b>Keywords</b>: graphons, universal approximation, weisfeiler-leman, graph metric, tree homomorphisms, tree distance, optimal transport, GNNs
</p><p><b>Compressor summary</b>: The paper presents a continuous extension of the Weisfeiler--Leman test for analyzing the expressive power of message-passing graph neural networks on graphons, providing a theoretical and empirical framework to understand their limitations and performance.</p><hr><h3>Domain Re-Modulation for Few-Shot Generative Domain Adaptation</h3>
<p>Yi Wu, Ziqiang Li, Chaoyue Wang, Heliang Zheng, Shanshan Zhao, Bin Li, Dacheng Tao</p>
<p><a href='https://openreview.net/forum?id=jown9RvYn7'>https://openreview.net/forum?id=jown9RvYn7</a></p>
<p><b>Keywords</b>: StyleGAN, Few-Shot Generative Domain Adaptation
</p><p><b>Compressor summary</b>: The study proposes a new generator structure called Domain Re-Modulation (DoRM) for few-shot Generative Domain Adaptation, which incorporates memory and domain association to achieve high quality, diversity, and consistency across domains.</p><hr><h3>Implicit Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis</h3>
<p>Zhu Wang, Sourav Medya, Sathya N. Ravi</p>
<p><a href='https://openreview.net/forum?id=jooPcatnVF'>https://openreview.net/forum?id=jooPcatnVF</a></p>
<p><b>Keywords</b>: Implicit layer, Out-of-distribution detection, multimodal learning
</p><p><b>Compressor summary</b>: The authors propose a method to combine deep neural networks and semantic knowledge for vision and language tasks, using an implicit out-of-distribution detector to filter irrelevant information.</p><hr><h3>Learning Reliable Logical Rules with SATNet</h3>
<p>Zhaoyu Li, Jinpei Guo, Yuhe Jiang, Xujie Si</p>
<p><a href='https://openreview.net/forum?id=jnIBiP2di1'>https://openreview.net/forum?id=jnIBiP2di1</a></p>
<p><b>Keywords</b>: Logical Reasoning, Rule Learning, Interpretation, SATNet
</p><p><b>Compressor summary</b>: The paper proposes a new framework that generates interpretable and verifiable logical rules for advanced AI systems using differentiable learning and MaxSAT solver.</p><hr><h3>D-CIPHER: Discovery of Closed-form Partial Differential Equations</h3>
<p>Krzysztof Kacprzyk, Zhaozhi Qian, Mihaela van der Schaar</p>
<p><a href='https://openreview.net/forum?id=jnCPN1vpSR'>https://openreview.net/forum?id=jnCPN1vpSR</a></p>
<p><b>Keywords</b>: differential equations, symbolic regression
</p><p><b>Compressor summary</b>: D-CIPHER is a new method for finding differential equations from data that works with noisy and infrequent observations and discovers more phenomena than existing approaches.</p><hr><h3>Energy-based learning algorithms for analog computing: a comparative study</h3>
<p>Benjamin Scellier, Maxence Ernoult, Jack Kendall, Suhas Kumar</p>
<p><a href='https://openreview.net/forum?id=jl5a3t78Uh'>https://openreview.net/forum?id=jl5a3t78Uh</a></p>
<p><b>Keywords</b>: energy-based learning algorithm, contrastive learning, equilibrium propagation, coupled learning, convolutional Hopfield network
</p><p><b>Compressor summary</b>: The paragraph compares seven energy-based learning algorithms on deep convolutional Hopfield networks for five vision tasks and finds that negative perturbations perform better than positive ones.</p><hr><h3>Statistical Guarantees for Variational Autoencoders using PAC-Bayesian Theory</h3>
<p>Sokhna Diarra Mbacke, Florence Clerc, Pascal Germain</p>
<p><a href='https://openreview.net/forum?id=jkPDRHff3s'>https://openreview.net/forum?id=jkPDRHff3s</a></p>
<p><b>Keywords</b>: Variational Autoencoders, PAC-Bayes, Statistical Learning Theory
</p><p><b>Compressor summary</b>: The authors use PAC-Bayesian theory to provide statistical guarantees for Variational Autoencoders (VAEs) and bounds on their performance metrics.</p><hr><h3>Adaptive Online Replanning with Diffusion Models</h3>
<p>Siyuan Zhou, Yilun Du, Shun Zhang, Mengdi Xu, Yikang Shen, Wei Xiao, Dit-Yan Yeung, Chuang Gan</p>
<p><a href='https://openreview.net/forum?id=jhs8F63xI6'>https://openreview.net/forum?id=jhs8F63xI6</a></p>
<p><b>Keywords</b>: Decision making, Robotics, Planning-based
</p><p><b>Compressor summary</b>: The paper proposes a method to improve diffusion planning by deciding when to replan and adjusting trajectories, leading to better performance in robotic control tasks.</p><hr><h3>Finite-Time Analysis of Single-Timescale Actor-Critic</h3>
<p>Xuyang Chen, Lin Zhao</p>
<p><a href='https://openreview.net/forum?id=jh3UNSQK0l'>https://openreview.net/forum?id=jh3UNSQK0l</a></p>
<p><b>Keywords</b>: Finite-time analysis, single-timescale actor-critic
</p><p><b>Compressor summary</b>: The paper analyzes the convergence of online single-timescale actor-critic with linear function approximation on continuous state space, achieving an $\epsilon$-approximate stationary point with improved sample complexity.</p><hr><h3>Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation</h3>
<p>Susung Hong, Donghoon Ahn, Seungryong Kim</p>
<p><a href='https://openreview.net/forum?id=jgIrJeHHlz'>https://openreview.net/forum?id=jgIrJeHHlz</a></p>
<p><b>Keywords</b>: Text-to-3D, Diffusion Models
</p><p><b>Compressor summary</b>: The paper proposes two methods to improve view consistency in text-to-3D generation by debiasing scores from 2D diffusion models and aligning user prompts with object views.</p><hr><h3>From ViT Features to Training-free Video Object Segmentation via Streaming-data Mixture Models</h3>
<p>Roy Uziel, Or Dinari, Oren Freifeld</p>
<p><a href='https://openreview.net/forum?id=jfsjKBDB1z'>https://openreview.net/forum?id=jfsjKBDB1z</a></p>
<p><b>Keywords</b>: Unsupervised, video segmentation, clustering
</p><p><b>Compressor summary</b>: The paper presents a training-free video object segmentation method that uses pre-trained deep features and classical clustering, achieving state-of-the-art results with low memory and fast inference.</p><hr><h3>MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates</h3>
<p>Mohammad Mozaffari, Sikan Li, Zhao Zhang, Maryam Mehri Dehnavi</p>
<p><a href='https://openreview.net/forum?id=jcnvDO96N5'>https://openreview.net/forum?id=jcnvDO96N5</a></p>
<p><b>Keywords</b>: machine learning, deep learning, optimizers, distributed training;second-order optimization;
</p><p><b>Compressor summary</b>: MKOR is a new optimizer that improves training speed and convergence of DNNs by reducing complexity and increasing second-order updates frequency.</p><hr><h3>Interaction Measures, Partition Lattices and Kernel Tests for High-Order Interactions</h3>
<p>Zhaolu Liu, Robert Peach, Pedro A. M. Mediano, Mauricio Barahona</p>
<p><a href='https://openreview.net/forum?id=jcRB6xHdJ2'>https://openreview.net/forum?id=jcRB6xHdJ2</a></p>
<p><b>Keywords</b>: High-order interactions; Lattice theory; Kernel tests
</p><p><b>Compressor summary</b>: The text introduces a method to measure and test interactions between groups of more than two variables in complex systems using kernel-based tests and lattice theory connections.</p><hr><h3>Generator Born from Classifier</h3>
<p>Runpeng Yu, Xinchao Wang</p>
<p><a href='https://openreview.net/forum?id=jcJVgIFY2r'>https://openreview.net/forum?id=jcJVgIFY2r</a></p>
<p><b>Keywords</b>: Generative Model
</p><p><b>Compressor summary</b>: The paper presents a new method to reconstruct an image generator using a pre-trained classifier, by training the generator to satisfy convergence conditions based on maximum-margin bias theory.</p><hr><h3>Design from Policies: Conservative Test-Time Adaptation for Offline Policy Optimization</h3>
<p>Jinxin Liu, Hongyin Zhang, Zifeng Zhuang, Yachen Kang, Donglin Wang, Bin Wang</p>
<p><a href='https://openreview.net/forum?id=jZYf1GxH1V'>https://openreview.net/forum?id=jZYf1GxH1V</a></p>
<p><b>Keywords</b>: offline reinforcement learning, test-time adaptation
</p><p><b>Compressor summary</b>: The paper proposes a non-iterative bi-level offline RL method called DROP that learns an MBO score model and a behavior embedding for safe policy extraction during testing.</p><hr><h3>Moral Responsibility for AI Systems</h3>
<p>Sander Beckers</p>
<p><a href='https://openreview.net/forum?id=jYIknUIgkd'>https://openreview.net/forum?id=jYIknUIgkd</a></p>
<p><b>Keywords</b>: responsibility, causation, causal models
</p><p><b>Compressor summary</b>: The paper defines moral responsibility for AI systems using causal models, compares its approach to existing ones, and extends the concept to a degree of responsibility.</p><hr><h3>Beyond Deep Ensembles: A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift</h3>
<p>Florian Seligmann, Philipp Becker, Michael Volpp, Gerhard Neumann</p>
<p><a href='https://openreview.net/forum?id=jX49iKr6vb'>https://openreview.net/forum?id=jX49iKr6vb</a></p>
<p><b>Keywords</b>: bayesian deep learning, distribution shift, calibration
</p><p><b>Compressor summary</b>: The paper surveys recent BDL methods on realistic tasks, focusing on how they generalize and calibrate under distribution shift. It explores different aspects of BDL, such as signed error, fine-tuning, and ensembles, and compares various algorithms in terms of accuracy and calibration.</p><hr><h3>RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths</h3>
<p>Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, Ping Luo</p>
<p><a href='https://openreview.net/forum?id=jUdZCcoOu3'>https://openreview.net/forum?id=jUdZCcoOu3</a></p>
<p><b>Keywords</b>: Diffusion Model, Text-to-Image Generation
</p><p><b>Compressor summary</b>: RAPHAEL is a text-conditional image diffusion model that generates highly artistic images by stacking tens of mixture-of-experts layers, outperforming other models in image quality and style switching, with a zero-shot FID score of 6.61 on the COCO dataset.</p><hr><h3>SPRING: Studying Papers and Reasoning to play Games</h3>
<p>Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Tom Mitchell, Yuanzhi Li</p>
<p><a href='https://openreview.net/forum?id=jU9qiRMDtR'>https://openreview.net/forum?id=jU9qiRMDtR</a></p>
<p><b>Keywords</b>: Games, Instruction Manual, Crafter, Open-world games, Large Language Models, Language Models, Zero-shot, In-context prompting
</p><p><b>Compressor summary</b>: SPRING is a novel approach that uses a large language model to reason and play open-world survival games like Crafter, outperforming state-of-the-art RL baselines without any training.</p><hr><h3>Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases</h3>
<p>Mazda Moayeri, Wenxiao Wang, Sahil Singla, Soheil Feizi</p>
<p><a href='https://openreview.net/forum?id=jSuhnO9QJv'>https://openreview.net/forum?id=jSuhnO9QJv</a></p>
<p><b>Keywords</b>: spurious correlations, interpretability, bias, distributional robustness
</p><p><b>Compressor summary</b>: The paper proposes a simple and effective way to reduce model biases due to spurious features by ranking images based on how much they rely on such cues, using deep neural features of an interpretable network. The method works on any data and model without major modifications, and shows that class-wise biases are strongly related across models.</p><hr><h3>When are ensembles really effective?</h3>
<p>Ryan Theisen, Hyunsuk Kim, Yaoqing Yang, Liam Hodgkinson, Michael W. Mahoney</p>
<p><a href='https://openreview.net/forum?id=jS4DUGOtBD'>https://openreview.net/forum?id=jS4DUGOtBD</a></p>
<p><b>Keywords</b>: Ensembling, theory, deep learning
</p><p><b>Compressor summary</b>: The paper investigates when ensembling improves classification performance, finding that it depends on the disagreement rate and the average error rate of the models involved, with non-interpolating models benefiting more from ensembling than interpolating ones.</p><hr><h3>Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning</h3>
<p>Guozheng Ma, Linrui Zhang, Haoyu Wang, Lu Li, Zilin Wang, Zhen Wang, Li Shen, Xueqian Wang, Dacheng Tao</p>
<p><a href='https://openreview.net/forum?id=jRL6ErxMVB'>https://openreview.net/forum?id=jRL6ErxMVB</a></p>
<p><b>Keywords</b>: Data Augmentation, Visual Reinforcement Learning, Sample Efficiency
</p><p><b>Compressor summary</b>: This work investigates the attributes of data augmentation (DA) in visual reinforcement learning, introducing new DA operations and fusion schemes to improve sample efficiency.</p><hr><h3>Is Learning in Games Good for the Learners?</h3>
<p>William Brown, Jon Schneider, Kiran Vodrahalli</p>
<p><a href='https://openreview.net/forum?id=jR2FkqW6GB'>https://openreview.net/forum?id=jR2FkqW6GB</a></p>
<p><b>Keywords</b>: learning in games, correlated equilibria, Stackelberg equilibria, swap regret, dynamic regret
</p><p><b>Compressor summary</b>: This text discusses tradeoffs between reward and regret in repeated games with two agents using generalized equilibrium, and shows that different algorithms can lead to different outcomes depending on the opponent's algorithm choice.</p><hr><h3>IEBins: Iterative Elastic Bins for Monocular Depth Estimation</h3>
<p>Shuwei Shao, Zhongcai Pei, Xingming Wu, Zhong Liu, Weihai Chen, Zhengguo Li</p>
<p><a href='https://openreview.net/forum?id=jOuxQGRVoQ'>https://openreview.net/forum?id=jOuxQGRVoQ</a></p>
<p><b>Keywords</b>: Monocular depth estimation, Iterative refinement, Deep learning
</p><p><b>Compressor summary</b>: The paper introduces a novel iterative elastic bins method for monocular depth estimation, which progressively optimizes the search range and utilizes temporal context modeling with GRU-based architecture to achieve state-of-the-art results.</p><hr><h3>Fast Partitioned Learned Bloom Filter</h3>
<p>Atsuki Sato, Yusuke Matsui</p>
<p><a href='https://openreview.net/forum?id=jL2eJxPK88'>https://openreview.net/forum?id=jL2eJxPK88</a></p>
<p><b>Keywords</b>: optimization, data structures, algorithms, theory, learned algorithms
</p><p><b>Compressor summary</b>: This paper presents two methods to construct a learned Bloom filter faster and with similar memory efficiency, while one method can achieve the same data structure as the original.</p><hr><h3>CluB: Cluster Meets BEV for LiDAR-Based 3D Object Detection</h3>
<p>Yingjie Wang, Jiajun Deng, Yuenan Hou, Yao Li, Yu Zhang, Jianmin Ji, Wanli Ouyang, Yanyong Zhang</p>
<p><a href='https://openreview.net/forum?id=jIhX7SpfCz'>https://openreview.net/forum?id=jIhX7SpfCz</a></p>
<p><b>Keywords</b>: 3D object detection ; Point clouds
</p><p><b>Compressor summary</b>: The paper proposes CluB, a 3D object detection framework that combines BEV-based and cluster-based detectors by enriching object representation at feature and query levels.</p><hr><h3>Gradient Flossing: Improving Gradient Descent through Dynamic Control of Jacobians</h3>
<p>Rainer Engelken</p>
<p><a href='https://openreview.net/forum?id=jEQRoJzDx8'>https://openreview.net/forum?id=jEQRoJzDx8</a></p>
<p><b>Keywords</b>: exploding/vanishing gradients, Lyapunov exponents, Lyapunov spectrum, chaos, RNN, condition number, Jacobian
</p><p><b>Compressor summary</b>: Gradient flossing is a new method to stabilize recurrent neural networks by regularizing Lyapunov exponents during learning, leading to improved convergence speed and success rate for tasks with long time horizons.</p><hr><h3>The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation</h3>
<p>Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, David J. Fleet</p>
<p><a href='https://openreview.net/forum?id=jDIlzSU8wJ'>https://openreview.net/forum?id=jDIlzSU8wJ</a></p>
<p><b>Keywords</b>: Monocular depth, optical flow, diffusion, depth, flow
</p><p><b>Compressor summary</b>: Denoising diffusion probabilistic models can estimate optical flow and monocular depth effectively without specialized architectures, providing uncertainty and multimodality information.</p><hr><h3>Learning Repeatable Speech Embeddings Using An Intra-class Correlation Regularizer</h3>
<p>Jianwei Zhang, Suren Jayasuriya, Visar Berisha</p>
<p><a href='https://openreview.net/forum?id=jCPRG3FuHV'>https://openreview.net/forum?id=jCPRG3FuHV</a></p>
<p><b>Keywords</b>: repeatability, embeddings, metric learning, intra-class correlation, intra-class variance
</p><p><b>Compressor summary</b>: The authors propose a method to measure and improve the repeatability of embeddings for machine learning tasks using intra-class correlation coefficients (ICC) and show its effectiveness on speech tasks.</p><hr><h3>Hierarchical Adaptive Value Estimation for Multi-modal Visual Reinforcement Learning</h3>
<p>Yangru Huang, Peixi Peng, Yifan Zhao, Haoran Xu, Mengyue Geng, Yonghong Tian</p>
<p><a href='https://openreview.net/forum?id=jB4wsc1DQW'>https://openreview.net/forum?id=jB4wsc1DQW</a></p>
<p><b>Keywords</b>: vision-based reinforcement learning, multi-modal, event camera
</p><p><b>Compressor summary</b>: The paper proposes a local value estimation framework for vision-based reinforcement learning that adapts to different modalities and improves performance in autonomous driving.</p><hr><h3>Jailbroken: How Does LLM Safety Training Fail?</h3>
<p>Alexander Wei, Nika Haghtalab, Jacob Steinhardt</p>
<p><a href='https://openreview.net/forum?id=jA235JGM09'>https://openreview.net/forum?id=jA235JGM09</a></p>
<p><b>Keywords</b>: red teaming, safety, RLHF, large language models
</p><p><b>Compressor summary</b>: The text discusses how large language models trained for safety still face adversarial misuse, investigates why this happens, and evaluates new attacks on state-of-the-art models like GPT-4 and Claude v1.3.</p><hr><h3>On Masked Pre-training and the Marginal Likelihood</h3>
<p>Pablo Moreno-Muñoz, Pol G. Recasens, Søren Hauberg</p>
<p><a href='https://openreview.net/forum?id=j9wGUcS30B'>https://openreview.net/forum?id=j9wGUcS30B</a></p>
<p><b>Keywords</b>: Marginal likelihood, masked pre-training, Bayesian inference
</p><p><b>Compressor summary</b>: The paper explains how masked pre-training works by maximizing a model's generalization measure and explores its potential for Bayesian models.</p><hr><h3>Learning from Both Structural and Textual Knowledge for Inductive Knowledge Graph Completion</h3>
<p>Kunxun Qi, Jianfeng Du, Hai Wan</p>
<p><a href='https://openreview.net/forum?id=j7x9wW3tCf'>https://openreview.net/forum?id=j7x9wW3tCf</a></p>
<p><b>Keywords</b>: Knowledge graph completion; Neural approximate rule learning; Neural rule-based system
</p><p><b>Compressor summary</b>: The paper proposes a two-stage framework for knowledge graph completion using both structural and textual knowledge, which improves performance over existing rule-based systems.</p><hr><h3>DynPoint: Dynamic Neural Point For View Synthesis</h3>
<p>Kaichen Zhou, Jia-Xing Zhong, Sangyun Shin, Kai Lu, Yiyuan Yang, Andrew Markham, Niki Trigoni</p>
<p><a href='https://openreview.net/forum?id=j7U4pFkCYB'>https://openreview.net/forum?id=j7U4pFkCYB</a></p>
<p><b>Keywords</b>: View Synthesis, Monocular Video
</p><p><b>Compressor summary</b>: DynPoint is an algorithm that accelerates view synthesis for unconstrained monocular videos by predicting 3D correspondence between frames and using hierarchical neural point clouds.</p><hr><h3>Scaling Data-Constrained Language Models</h3>
<p>Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, Colin Raffel</p>
<p><a href='https://openreview.net/forum?id=j5BuTrEj35'>https://openreview.net/forum?id=j5BuTrEj35</a></p>
<p><b>Keywords</b>: large language models, scaling laws, data engineering
</p><p><b>Compressor summary</b>: The paper explores how to scale language models when there is limited text data and proposes a scaling law for compute optimality that takes into account repeated tokens and excess parameters.</p><hr><h3>What You See is What You Read? Improving Text-Image Alignment Evaluation</h3>
<p>Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee Aharoni, Jonathan Herzig, Oran Lang, Eran Ofek, Idan Szpektor</p>
<p><a href='https://openreview.net/forum?id=j5AoleAIru'>https://openreview.net/forum?id=j5AoleAIru</a></p>
<p><b>Keywords</b>: Vision-and-language, Image-text alignment, Text-to-image generation, Image-to-text generation, Multi-modal models, Synthetic images, Meta-evaluation, Visual-question-answering
</p><p><b>Compressor summary</b>: The authors propose two methods for automatic evaluation of text-image alignment and introduce SeeTRUE, a dataset with human judgements for semantically aligned text-image pairs.</p><hr><h3>Resolving the Tug-of-War: A Separation of Communication and Learning in Federated Learning</h3>
<p>Junyi Li, Heng Huang</p>
<p><a href='https://openreview.net/forum?id=j4QVhftpYM'>https://openreview.net/forum?id=j4QVhftpYM</a></p>
<p><b>Keywords</b>: Federated Learning
</p><p><b>Compressor summary</b>: FedSep is a novel two-layer federated learning framework that separates communication and learning layers for better privacy and efficiency, and outperforms existing methods in communication-efficient and heterogeneous-model FL tasks.</p><hr><h3>Active Vision Reinforcement Learning under Limited Visual Observability</h3>
<p>Jinghuan Shang, Michael S Ryoo</p>
<p><a href='https://openreview.net/forum?id=j2oYaFpbrB'>https://openreview.net/forum?id=j2oYaFpbrB</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Active Reinforcement Learning, Visual Reinforcement Learning, Active Vision, Active Perception, Partial Observability, Sensorimotor
</p><p><b>Compressor summary</b>: The paper introduces SUGARL, a framework that learns motor and sensory policies together using an intrinsic sensorimotor reward, which enables agents to control their visual observations in partially observable environments.</p><hr><h3>Versatile Energy-Based Probabilistic Models for High Energy Physics</h3>
<p>Taoli Cheng, Aaron Courville</p>
<p><a href='https://openreview.net/forum?id=j0U6XJubbP'>https://openreview.net/forum?id=j0U6XJubbP</a></p>
<p><b>Keywords</b>: Generative modeling, Energy-based models, Out-of-distribution detection, Sciences, Application, Physics
</p><p><b>Compressor summary</b>: The authors propose a flexible energy-based probabilistic model for High Energy Physics events that can generate simulations, detect anomalies, and identify particles.</p><hr><h3>Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation</h3>
<p>Wei-Ning Chen, Dan Song, Ayfer Ozgur, Peter Kairouz</p>
<p><a href='https://openreview.net/forum?id=izNfcaHJk0'>https://openreview.net/forum?id=izNfcaHJk0</a></p>
<p><b>Keywords</b>: Differential Privacy, Federated Learning, Communication
</p><p><b>Compressor summary</b>: The paper studies how to achieve the best balance of privacy, communication, and accuracy in federated learning and analytics, and proposes two methods that use compression and random data selection to improve this trade-off.</p><hr><h3>Near-Optimal Algorithms for Gaussians with Huber Contamination: Mean Estimation and Linear Regression</h3>
<p>Ilias Diakonikolas, Daniel Kane, Ankit Pensia, Thanasis Pittas</p>
<p><a href='https://openreview.net/forum?id=iyweRIXAeH'>https://openreview.net/forum?id=iyweRIXAeH</a></p>
<p><b>Keywords</b>: robust statistics, high-dimensional inference, regression, nearly linear time algorithms
</p><p><b>Compressor summary</b>: The paper presents novel algorithms for Gaussian mean estimation and linear regression with Huber contamination, achieving near-optimal sample and time complexity with optimal error guarantees.</p><hr><h3>GPEX, A Framework For Interpreting Artificial Neural Networks</h3>
<p>Amir Akbarnejad, Gilbert Bigras, Nilanjan Ray</p>
<p><a href='https://openreview.net/forum?id=iy4Of0w8ML'>https://openreview.net/forum?id=iy4Of0w8ML</a></p>
<p><b>Keywords</b>: Gaussian processes, Explainable AI
</p><p><b>Compressor summary</b>: The authors propose a method that uses Gaussian processes to match and explain the outputs of deep neural networks without strict assumptions, and demonstrate its effectiveness on various datasets using computational techniques for scalability and GPU acceleration.</p><hr><h3>Non-adversarial training of Neural SDEs with signature kernel scores</h3>
<p>Zacharia Issa, Blanka Horvath, Maud Lemercier, Cristopher Salvi</p>
<p><a href='https://openreview.net/forum?id=ixcsBZw5pl'>https://openreview.net/forum?id=ixcsBZw5pl</a></p>
<p><b>Keywords</b>: Neural SDEs, score-based generative models, signature kernels, time series
</p><p><b>Compressor summary</b>: This paper proposes a new way to train Neural SDEs non-adversarially using kernel scores, which improves stability, avoids mode collapse, and allows for generating spatiotemporal data.</p><hr><h3>Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting</h3>
<p>Hejie Cui, Xinyu Fang, Zihan Zhang, Ran Xu, Xuan Kan, Xin Liu, Yue Yu, Manling Li, Yangqiu Song, Carl Yang</p>
<p><a href='https://openreview.net/forum?id=ixVAXsdtJO'>https://openreview.net/forum?id=ixVAXsdtJO</a></p>
<p><b>Keywords</b>: Visual Knowledge Extraction, Multimodality, Large Model Prompting
</p><p><b>Compressor summary</b>: OpenVik is a new method for extracting open visual knowledge from images using an open relational region detector and a multimodal model, with enhanced data techniques to diversify the results.</p><hr><h3>Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models</h3>
<p>Andy Zhou, Jindong Wang, Yu-Xiong Wang, Haohan Wang</p>
<p><a href='https://openreview.net/forum?id=iwp3H8uSeK'>https://openreview.net/forum?id=iwp3H8uSeK</a></p>
<p><b>Keywords</b>: robustness, knowledge distillation, adversarial training, data augmentation, generalization
</p><p><b>Compressor summary</b>: The paper introduces a framework that improves vision models' robustness by combining knowledge distillation and data augmentation using a robust teacher and Discrete Adversarial Distillation (DAD).</p><hr><h3>Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models</h3>
<p>Zhendong Wang, Yifan Jiang, Huangjie Zheng, Peihao Wang, Pengcheng He, Zhangyang Wang, Weizhu Chen, Mingyuan Zhou</p>
<p><a href='https://openreview.net/forum?id=iv2sTQtbst'>https://openreview.net/forum?id=iv2sTQtbst</a></p>
<p><b>Keywords</b>: diffusion models, training efficiency, data efficiency
</p><p><b>Compressor summary</b>: Patch Diffusion is a faster and more efficient way to train diffusion models that uses patches with random sizes and coordinates, improving the quality of generated images even on small datasets.</p><hr><h3>Saddle-to-Saddle Dynamics in Diagonal Linear Networks</h3>
<p>Scott Pesme, Nicolas Flammarion</p>
<p><a href='https://openreview.net/forum?id=iuqCXg1Gng'>https://openreview.net/forum?id=iuqCXg1Gng</a></p>
<p><b>Keywords</b>: gradient flow, saddle-to-saddle, diagonal linear network, incremental learning
</p><p><b>Compressor summary</b>: The paper describes how gradient flow in two-layer linear networks for regression finds the minimum l1-norm solution by jumping between saddles using a recursive algorithm and arc-length time-reparametrization, with no assumptions on data or network size.</p><hr><h3>Improving Compositional Generalization using Iterated Learning and Simplicial Embeddings</h3>
<p>Yi Ren, Samuel Lavoie, Mikhail Galkin, Danica J. Sutherland, Aaron Courville</p>
<p><a href='https://openreview.net/forum?id=irRHgjePdR'>https://openreview.net/forum?id=irRHgjePdR</a></p>
<p><b>Keywords</b>: compositional generalization, systematic generalization, iterated learning, representation learning, graph neural networks
</p><p><b>Compressor summary</b>: The authors propose iterated learning on models with simplicial embeddings to improve deep networks' compositional generalization, inspired by human language development and Kolmogorov complexity analysis.</p><hr><h3>Punctuation-level Attack: Single-shot and Single Punctuation Can Fool Text Models</h3>
<p>Wenqiang Wang, Chongyang Du, Tao Wang, Kaihao Zhang, Wenhan Luo, Lin Ma, Wei Liu, Xiaochun Cao</p>
<p><a href='https://openreview.net/forum?id=ir6WWkFR80'>https://openreview.net/forum?id=ir6WWkFR80</a></p>
<p><b>Keywords</b>: Punctuation-level Attack, Textual Adversarial attack, Natural Language Processing
</p><p><b>Compressor summary</b>: The paper introduces a new way of textual attack using single punctuation changes and proposes a search method to find the best position for the attack without exhaustive search.</p><hr><h3>Adaptive recurrent vision performs zero-shot computation scaling to unseen difficulty levels</h3>
<p>Vijay Veerabadran, Srinivas Ravishankar, Yuan Tang, Ritik Raina, Virginia R. de Sa</p>
<p><a href='https://openreview.net/forum?id=iqezE0EyXq'>https://openreview.net/forum?id=iqezE0EyXq</a></p>
<p><b>Keywords</b>: cognitive science, recurrent neural networks, adaptive computation time, visual reasoning
</p><p><b>Compressor summary</b>: The study investigates how adaptive recurrent neural networks can help vision models solve difficult visual reasoning problems by adjusting their computational resources based on the input's needs.</p><hr><h3>Generalized Belief Transport</h3>
<p>Junqi Wang, PEI WANG, Patrick Shafto</p>
<p><a href='https://openreview.net/forum?id=iohoef1bfM'>https://openreview.net/forum?id=iohoef1bfM</a></p>
<p><b>Keywords</b>: Bayesian theory, Belief transport, Unbalanced optimal transport, parametrization, asymptotic behavior, environment drift detection
</p><p><b>Compressor summary</b>: The paragraph discusses a new mathematical framework called Generalized Belief Transport that unifies different learning models by considering them as points in a broader space of possibilities, and explores its properties and behavior.</p><hr><h3>History Filtering in Imperfect Information Games: Algorithms and Complexity</h3>
<p>Christopher Solinas, Doug Rebstock, Nathan R. Sturtevant, Michael Buro</p>
<p><a href='https://openreview.net/forum?id=inIONNg8Sq'>https://openreview.net/forum?id=inIONNg8Sq</a></p>
<p><b>Keywords</b>: search, game theory, multi-agent, learning, markov chain monte carlo, complexity
</p><p><b>Compressor summary</b>: The authors analyze the computational aspects of filtering histories for subgame decomposition, a key technique for AI in imperfect information games, and introduce a novel Markov Chain Monte Carlo-based generation algorithm for trick-taking card games.</p><hr><h3>AIMS: All-Inclusive Multi-Level Segmentation for Anything</h3>
<p>Lu Qi, Jason Kuen, Weidong Guo, Jiuxiang Gu, Zhe Lin, Bo Du, Yu Xu, Ming-Hsuan Yang</p>
<p><a href='https://openreview.net/forum?id=ikkdTD3hQJ'>https://openreview.net/forum?id=ikkdTD3hQJ</a></p>
<p><b>Keywords</b>: Image Segmentation
</p><p><b>Compressor summary</b>: The paper introduces AIMS, a new task for image segmentation that divides visual regions into three levels, and proposes a unified model to tackle challenges like annotation inconsistency and task correlation.</p><hr><h3>Semi-Supervised Contrastive Learning for Deep Regression with Ordinal Rankings from Spectral Seriation</h3>
<p>Weihang Dai, Yao DU, Hanru Bai, Kwang-Ting Cheng, Xiaomeng Li</p>
<p><a href='https://openreview.net/forum?id=ij3svnPLzG'>https://openreview.net/forum?id=ij3svnPLzG</a></p>
<p><b>Keywords</b>: Semi-supervised learning, deep regression, contrastive learning
</p><p><b>Compressor summary</b>: The authors propose a new semi-supervised contrastive learning method for deep regression that uses unlabeled data and achieves better results than existing methods.</p><hr><h3>Frequency-domain MLPs are More Effective Learners in Time Series Forecasting</h3>
<p>Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian, Longbing Cao, Zhendong Niu</p>
<p><a href='https://openreview.net/forum?id=iif9mGCTfy'>https://openreview.net/forum?id=iif9mGCTfy</a></p>
<p><b>Keywords</b>: time series forecasting, multi-layer perceptrons, frequency domain
</p><p><b>Compressor summary</b>: The paper proposes FreTS, a time series forecasting method that uses frequency-domain MLPs to learn global dependencies and concentrate on key parts of frequency components, outperforming existing methods.</p><hr><h3>GNNEvaluator: Evaluating GNN Performance On Unseen Graphs Without Labels</h3>
<p>Xin Zheng, Miao Zhang, Chunyang Chen, Soheila Molaei, Chuan Zhou, Shirui Pan</p>
<p><a href='https://openreview.net/forum?id=ihlT8yvQ2I'>https://openreview.net/forum?id=ihlT8yvQ2I</a></p>
<p><b>Keywords</b>: graph neural networks, GNN model evaluation, node classification accuracy
</p><p><b>Compressor summary</b>: The paper proposes a two-stage framework to evaluate graph neural network (GNN) models by estimating their performance on unseen graphs without labels using a DiscGraph set and a GNNEvaluator.</p><hr><h3>Maximum Independent Set: Self-Training through Dynamic Programming</h3>
<p>Lorenzo Brusca, Lars C.P.M. Quaedvlieg, Stratis Skoulakis, Grigorios Chrysos, Volkan Cevher</p>
<p><a href='https://openreview.net/forum?id=igE3Zbxvws'>https://openreview.net/forum?id=igE3Zbxvws</a></p>
<p><b>Keywords</b>: Maximum Independent Set, Combinatorial Optimization, Graph Neural Networks, Dynamic Programming
</p><p><b>Compressor summary</b>: The paper introduces a GNN-based algorithm for solving the MIS problem using a DP-like recursive approach and shows its effectiveness on various datasets.</p><hr><h3>EvoPrompting: Language Models for Code-Level Neural Architecture Search</h3>
<p>Angelica Chen, David Dohan, David So</p>
<p><a href='https://openreview.net/forum?id=ifbF4WdT8f'>https://openreview.net/forum?id=ifbF4WdT8f</a></p>
<p><b>Keywords</b>: language models, evolution, prompting, neural architecture search, code generation
</p><p><b>Compressor summary</b>: The paper explores using language models as mutation and crossover operators for evolutionary neural architecture search (NAS), proposing EvoPrompting which combines prompt engineering with soft prompt-tuning, and shows its effectiveness in finding diverse and high performing models on various tasks.</p><hr><h3>Interpretable Prototype-based Graph Information Bottleneck</h3>
<p>Sangwoo Seo, Sungwon Kim, Chanyoung Park</p>
<p><a href='https://openreview.net/forum?id=icWwBKyVMs'>https://openreview.net/forum?id=icWwBKyVMs</a></p>
<p><b>Keywords</b>: Graph neural network, Explainable AI, Interpretability
</p><p><b>Compressor summary</b>: The paper introduces interpretable Prototype-based Graph Information Bottleneck (PGIB), a novel framework for explainable GNNs that learns prototypes from key subgraphs important for predictions, improving both performance and interpretability.</p><hr><h3>DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis</h3>
<p>YoungJoong Kwon, Lingjie Liu, Henry Fuchs, Marc Habermann, Christian Theobalt</p>
<p><a href='https://openreview.net/forum?id=iajxrSgOSX'>https://openreview.net/forum?id=iajxrSgOSX</a></p>
<p><b>Keywords</b>: DELIFFAS: Avatar Modeling, Avatar Synthesis, Animatable Human, Light Fields, Human Performance Capture
</p><p><b>Compressor summary</b>: The DELIFFAS method creates realistic human avatars with fast inference speed by using a surface light field attached to a deformable mesh model, achieving state-of-the-art results in appearance synthesis and speed.</p><hr><h3>Similarity-based cooperative equilibrium</h3>
<p>Caspar Oesterheld, Johannes Treutlein, Roger Baker Grosse, Vincent Conitzer, Jakob Nicolaus Foerster</p>
<p><a href='https://openreview.net/forum?id=ia4AL3QnOv'>https://openreview.net/forum?id=ia4AL3QnOv</a></p>
<p><b>Keywords</b>: Program equilibrium, multi-agent learning, game theory, opponent shaping, superrationality, decision theory, cooperative AI, Newcomb's problem
</p><p><b>Compressor summary</b>: The paper proposes a realistic setting in which machine learning agents can achieve cooperative outcomes in social dilemmas by only observing a single number indicating their similarity, and shows that this can be learned using simple ML methods.</p><hr><h3>PCF-GAN: generating sequential data via the characteristic function of measures on the path space</h3>
<p>Hang Lou, Siran Li, Hao Ni</p>
<p><a href='https://openreview.net/forum?id=iWWLgcUTZU'>https://openreview.net/forum?id=iWWLgcUTZU</a></p>
<p><b>Keywords</b>: Generative adversarial networks, time series generation, rough path theory, Lie group
</p><p><b>Compressor summary</b>: The paragraph introduces PCF-GAN, a novel GAN that uses path characteristic functions to represent time series distributions and improve their generation and reconstruction quality.</p><hr><h3>Provable benefits of annealing for estimating normalizing constants: Importance Sampling, Noise-Contrastive Estimation, and beyond</h3>
<p>Omar Chehab, Aapo Hyvarinen, Andrej Risteski</p>
<p><a href='https://openreview.net/forum?id=iWGC0Nsq9i'>https://openreview.net/forum?id=iWGC0Nsq9i</a></p>
<p><b>Keywords</b>: noise-contrastive estimation, annealed importance sampling
</p><p><b>Compressor summary</b>: The paragraph discusses different design choices for Monte Carlo methods that estimate the normalization constant using annealing and their impact on estimation error, showing that some choices are more efficient than others depending on the context.</p><hr><h3>On the Identifiability and Interpretability of Gaussian Process Models</h3>
<p>Jiawen Chen, Wancen Mu, Yun Li, Didong Li</p>
<p><a href='https://openreview.net/forum?id=iVYInarGXg'>https://openreview.net/forum?id=iVYInarGXg</a></p>
<p><b>Keywords</b>: Gaussian process, Identifiability, Interpretability, Mixture kernel, Separable kernel
</p><p><b>Compressor summary</b>: The paper analyzes additive and multiplicative mixtures of Mat\'ern kernels in single- and multi-output Gaussian process models, showing that the latter are more suitable for multi-output tasks due to better identifiability properties.</p><hr><h3>Adversarial Training from Mean Field Perspective</h3>
<p>Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki</p>
<p><a href='https://openreview.net/forum?id=iT9MOAZqsb'>https://openreview.net/forum?id=iT9MOAZqsb</a></p>
<p><b>Keywords</b>: adversarial training; mean field theory
</p><p><b>Compressor summary</b>: The study analyzes adversarial training in random deep neural networks using a new mean field theory framework, showing its effects on network capacity, width, and dimensions.</p><hr><h3>A Trichotomy for Transductive Online Learning</h3>
<p>Steve Hanneke, Shay Moran, Jonathan Shafer</p>
<p><a href='https://openreview.net/forum?id=iSd8g75QvP'>https://openreview.net/forum?id=iSd8g75QvP</a></p>
<p><b>Keywords</b>: Online Learning, Transductive Online Learning, Offline Learning, Mistake Bound
</p><p><b>Compressor summary</b>: The paper presents new bounds on learner mistakes in transductive online learning, showing a trichotomy of possible mistake rates depending on VC and Littlestone dimensions.</p><hr><h3>Uncertainty-Aware Alignment  Network  for Cross-Domain Video-Text Retrieval</h3>
<p>Xiaoshuai Hao, Wanqian Zhang</p>
<p><a href='https://openreview.net/forum?id=iQlK3VJxV7'>https://openreview.net/forum?id=iQlK3VJxV7</a></p>
<p><b>Keywords</b>: video-text retrieval; cross-domain;Unsupervised Domain Adaptation Video-text Retrieval;
</p><p><b>Compressor summary</b>: The paper presents UAN, a novel method to improve unsupervised domain adaptation video-text retrieval by addressing multimodal mutual information and uncertainty-aware alignment.</p><hr><h3>Learning To Dive In Branch And Bound</h3>
<p>Max B. Paulus, Andreas Krause</p>
<p><a href='https://openreview.net/forum?id=iPTF2hON1C'>https://openreview.net/forum?id=iPTF2hON1C</a></p>
<p><b>Keywords</b>: Generative Modeling, Combinatorial Optimization, Mixed Integer Programming, Graph Neural Networks, Diving Heuristics
</p><p><b>Compressor summary</b>: L2Dive is a diving heuristic that uses graph neural networks to predict variable assignments and improve the performance of mixed integer linear programs on various combinatorial optimization problems.</p><hr><h3>Parallel Submodular Function Minimization</h3>
<p>Deeparnab Chakrabarty, Andrei Graur, Haotian Jiang, Aaron Sidford</p>
<p><a href='https://openreview.net/forum?id=iMfPFPMsZo'>https://openreview.net/forum?id=iMfPFPMsZo</a></p>
<p><b>Keywords</b>: parallel computation, convex optimization, submodular function minimization
</p><p><b>Compressor summary</b>: The paragraph discusses two new methods for parallel submodular function minimization with different trade-offs between depth and query complexity, and also introduces a highly-parallel algorithm for minimizing an $\ell_\infty$-Lipschitz function over the hypercube.</p><hr><h3>A Unified Model and Dimension for Interactive Estimation</h3>
<p>Nataly Brukhim, Miroslav Dudík, Aldo Pacchiano, Robert E. Schapire</p>
<p><a href='https://openreview.net/forum?id=iM0MWWBr4W'>https://openreview.net/forum?id=iM0MWWBr4W</a></p>
<p><b>Keywords</b>: Interactive learning, bandits, statistical queries
</p><p><b>Compressor summary</b>: The paper proposes a new interactive learning framework with a combinatorial measure called Dissimilarity dimension that captures learnability and unifies two classic models, with improved analyses.</p><hr><h3>Bicriteria Approximation Algorithms for the Submodular Cover Problem</h3>
<p>Wenjing Chen, Victoria G. Crawford</p>
<p><a href='https://openreview.net/forum?id=iKarSI2a73'>https://openreview.net/forum?id=iKarSI2a73</a></p>
<p><b>Keywords</b>: submodular, combinatorial optimization, approximation algorithms
</p><p><b>Compressor summary</b>: The paper presents scalable algorithms for solving submodular cover problems with different variants and applications in data summarization and graph cut.</p><hr><h3>Joint Prompt Optimization of Stacked LLMs using Variational Inference</h3>
<p>Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre Côté, Matheus Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, Nicolas Le Roux</p>
<p><a href='https://openreview.net/forum?id=iImnbUVhok'>https://openreview.net/forum?id=iImnbUVhok</a></p>
<p><b>Keywords</b>: deep prompt optimization, llm, variational inference, graphical model, chaining
</p><p><b>Compressor summary</b>: The text describes how to optimize prompts for two-layer deep language networks (DLNs) using latent variable inference and shows their potential to match or surpass GPT-4's performance with smaller models.</p><hr><h3>Simplifying Neural Network Training Under Class Imbalance</h3>
<p>Ravid Shwartz-Ziv, Micah Goldblum, Yucen Lily Li, C. Bayan Bruss, Andrew Gordon Wilson</p>
<p><a href='https://openreview.net/forum?id=iGmDQn4CRj'>https://openreview.net/forum?id=iGmDQn4CRj</a></p>
<p><b>Keywords</b>: Class Imbalance, Hyperparameters, Long-Tailed Distributions
</p><p><b>Compressor summary</b>: The authors show that by adjusting some standard deep learning components, one can achieve excellent results on class-imbalanced datasets without using specialized techniques.</p><hr><h3>Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks</h3>
<p>Feng Chen, Daniel Kunin, Atsushi Yamamura, Surya Ganguli</p>
<p><a href='https://openreview.net/forum?id=iFxWrxDekd'>https://openreview.net/forum?id=iFxWrxDekd</a></p>
<p><b>Keywords</b>: Implicit Bias, SGD Dynamics, Implicit regularization, Learning rate schedule, Stochastic Gradient Descent, Invariant set, Attractive saddle points, Stochastic collapse, Permutation invariance, Simplicity bias, Teacher-student
</p><p><b>Compressor summary</b>: The paper shows that stochastic gradient descent (SGD) often simplifies neural networks during training by converging to simpler subnetworks, improving generalization.</p><hr><h3>MultiMoDN—Multimodal, Multi-Task, Interpretable Modular Networks</h3>
<p>Vinitra Swamy, Malika Satayeva, Jibril Frej, Thierry Bossy, Thijs Vogels, Martin Jaggi, Tanja Käser, Mary-Anne Hartley</p>
<p><a href='https://openreview.net/forum?id=iB3Ew6z4WL'>https://openreview.net/forum?id=iB3Ew6z4WL</a></p>
<p><b>Keywords</b>: Deep Learning, Multimodal Learning, Multi-task learning, Missingness, Interpretability
</p><p><b>Compressor summary</b>: MultiModN is a multimodal network that fuses latent representations in a sequence of any modality type, providing interpretable feedback on real-world tasks while being resistant to missing not-at-random data.</p><hr><h3>Sharp Bounds for Generalized Causal Sensitivity Analysis</h3>
<p>Dennis Frauen, Valentyn Melnychuk, Stefan Feuerriegel</p>
<p><a href='https://openreview.net/forum?id=iAcEmyhwk2'>https://openreview.net/forum?id=iAcEmyhwk2</a></p>
<p><b>Keywords</b>: Causal machine learning, treatment effect estimation, sensitivity analysis, unobserved confounding, uncertainty estimation
</p><p><b>Compressor summary</b>: The paper presents a general framework for causal sensitivity analysis under unobserved confounding, extending the marginal sensitivity model to various types of treatments and outcomes, and providing sharp bounds for different causal effects using observational data.</p><hr><h3>CAST: Cross-Attention in Space and Time for Video Action Recognition</h3>
<p>Dongho Lee, Jongseo Lee, Jinwoo Choi</p>
<p><a href='https://openreview.net/forum?id=iATY9W5Xw7'>https://openreview.net/forum?id=iATY9W5Xw7</a></p>
<p><b>Keywords</b>: action recognition, video understanding, cross-attention, balanced spatio-temporal understanding
</p><p><b>Compressor summary</b>: Recognizing human actions in videos requires spatial and temporal understanding. Most existing action recognition models lack a balanced spatio-temporal understanding of videos. In this work, we propose a novel two-stream architecture, called Cross-Attention in Space and Time (CAST), that achieves a balanced spatio-temporal understanding of videos using only RGB input. Our proposed bottleneck cross-attention mechanism enables the spatial and temporal expert models to exchange information and make synergistic predictions, leading to improved performance. We validate the proposed method with extensive experiments on public benchmarks with different characteristics: EPIC-Kitchens-100, Something-Something-V2, and Kinetics-400. Our method consistently shows favorable performance across these datasets, while the performance of existing methods fluctuates depending on the dataset characteristics. The code is available at https://github.com/KHU-VLL/CAST.</p><hr><h3>Geodesic Multi-Modal Mixup for Robust Fine-Tuning</h3>
<p>Changdae Oh, Junhyuk So, Hoyoon Byun, YongTaek Lim, Minchul Shin, Jong-June Jeon, Kyungwoo Song</p>
<p><a href='https://openreview.net/forum?id=iAAXq60Bw1'>https://openreview.net/forum?id=iAAXq60Bw1</a></p>
<p><b>Keywords</b>: multi-modal learning, robustness, fine-tuning, contrastive learning, CLIP, Mixup
</p><p><b>Compressor summary</b>: The authors study CLIP's multi-modal embeddings and propose a fine-tuning method to improve alignment and uniformity for better transferability and robustness.</p><hr><h3>Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity</h3>
<p>Zijiao Chen, Jiaxin Qing, Juan Helen Zhou</p>
<p><a href='https://openreview.net/forum?id=i913TUOvTK'>https://openreview.net/forum?id=i913TUOvTK</a></p>
<p><b>Keywords</b>: Video Reconstruction from Brain Activities, Diffusion Model, Contrastive Learning
</p><p><b>Compressor summary</b>: The paragraph describes a new method called Mind-Video that can reconstruct high-quality videos from brain activities using various techniques and outperforms previous methods in accuracy and interpretability.</p><hr><h3>ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer</h3>
<p>Haoran You, Huihong Shi, Yipin Guo, Yingyan Celine Lin</p>
<p><a href='https://openreview.net/forum?id=i6mMWNcTfu'>https://openreview.net/forum?id=i6mMWNcTfu</a></p>
<p><b>Keywords</b>: Efficient Vision Transformer; Multiplication-reduced networks; Hardware acceleration
</p><p><b>Compressor summary</b>: The authors propose a new multiplication-reduced model called ShiftAddViT, which uses bitwise shifts and additions instead of dense multiplications to speed up ViTs on GPUs without losing much accuracy.</p><hr><h3>Decentralized Matrix Sensing: Statistical Guarantees and Fast Convergence</h3>
<p>Marie Maros, Gesualdo Scutari</p>
<p><a href='https://openreview.net/forum?id=i5sSWKbF3b'>https://openreview.net/forum?id=i5sSWKbF3b</a></p>
<p><b>Keywords</b>: Distributed non-convex optimization, Low-rank matrix recovery
</p><p><b>Compressor summary</b>: The paper studies how agents in a network can estimate a low-rank matrix using a decentralized gradient algorithm and shows that it converges to the correct solution with good performance guarantees.</p><hr><h3>Segment Any Point Cloud Sequences by Distilling Vision Foundation Models</h3>
<p>Youquan Liu, Lingdong Kong, Jun CEN, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu</p>
<p><a href='https://openreview.net/forum?id=i39yXaUKuF'>https://openreview.net/forum?id=i39yXaUKuF</a></p>
<p><b>Keywords</b>: autonomous driving, point cloud segmentation, self-supervised learning, 3D scene understanding
</p><p><b>Compressor summary</b>: Seal is a novel framework that leverages vision foundation models for efficient and versatile segmentation of diverse automotive point clouds, with properties such as scalability, consistency, and generalizability.</p><hr><h3>A Unified Fast Gradient Clipping Framework for DP-SGD</h3>
<p>Weiwei Kong, Andres Munoz medina</p>
<p><a href='https://openreview.net/forum?id=i2H2sEiq2T'>https://openreview.net/forum?id=i2H2sEiq2T</a></p>
<p><b>Keywords</b>: differential privacy, dp-sgd, gradient clipping, computational complexity
</p><p><b>Compressor summary</b>: The paper proposes a framework that enables fast norm computations for gradients in DP-SGD with arbitrary intermediate operations and shows improvements for specific operations using components of the framework.</p><hr><h3>GloptiNets: Scalable Non-Convex Optimization with Certificates</h3>
<p>Gaspard Beugnot, Julien Mairal, Alessandro Rudi</p>
<p><a href='https://openreview.net/forum?id=i28zCSsQIc'>https://openreview.net/forum?id=i28zCSsQIc</a></p>
<p><b>Keywords</b>: non-convex optimization, polynomial optimization, kernel sum-of-squares
</p><p><b>Compressor summary</b>: The authors propose a novel optimization method that leverages the regularity of smooth functions in the Fourier spectrum and neural network techniques, leading to better performance and scalability.</p><hr><h3>State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory</h3>
<p>Shida Wang, Beichen Xue</p>
<p><a href='https://openreview.net/forum?id=i0OmcF14Kf'>https://openreview.net/forum?id=i0OmcF14Kf</a></p>
<p><b>Keywords</b>: state space, approximation theory, sequence modelling
</p><p><b>Compressor summary</b>: The paper shows how adding nonlinear activation to state-space models improves their ability to learn complex sequence patterns, but does not solve the problem of exponential decaying memory.</p><hr><h3>Learning to Influence Human Behavior with Offline Reinforcement Learning</h3>
<p>Joey Hong, Sergey Levine, Anca Dragan</p>
<p><a href='https://openreview.net/forum?id=hzND3ZEFg2'>https://openreview.net/forum?id=hzND3ZEFg2</a></p>
<p><b>Keywords</b>: offline reinforcement learning, human-aware reinforcement learning, multi-agent influence
</p><p><b>Compressor summary</b>: The text discusses how AI agents can use offline reinforcement learning to influence suboptimal humans towards better performance by learning from a dataset of human-human interactions.</p><hr><h3>CLeAR: Continual Learning on Algorithmic Reasoning for Human-like Intelligence</h3>
<p>Bong Gyun Kang, HyunGi Kim, Dahuin Jung, Sungroh Yoon</p>
<p><a href='https://openreview.net/forum?id=hz33V7Tb2O'>https://openreview.net/forum?id=hz33V7Tb2O</a></p>
<p><b>Keywords</b>: Continual learning, Knowledge transfer, Algorithmic reasoning
</p><p><b>Compressor summary</b>: CLeAR is a novel algorithmic reasoning methodology that enables continual learning of abstract logical concepts with minimal forgetting and backward transfer.</p><hr><h3>CWCL: Cross-Modal Transfer with Continuously Weighted Contrastive Loss</h3>
<p>Rakshith Sharma Srinivasa, Jaejin Cho, Chouchang Yang, Yashas Malur Saidutta, Ching-Hua Lee, Yilin Shen, Hongxia Jin</p>
<p><a href='https://openreview.net/forum?id=hz10oiVMNE'>https://openreview.net/forum?id=hz10oiVMNE</a></p>
<p><b>Keywords</b>: contrastive loss, multimodal representation learning, zero-shot learning, intent classification, pre-trained models, modality alignment, cross-modal transfer
</p><p><b>Compressor summary</b>: The paper introduces CWCL, a new contrastive loss function that improves cross-modal 0-shot transfer by using a continuous measure of similarity, achieving better results than existing methods on various tasks and datasets.</p><hr><h3>A polar prediction model for learning to represent visual transformations</h3>
<p>Pierre-Étienne H Fiquet, Eero P Simoncelli</p>
<p><a href='https://openreview.net/forum?id=hyPUZX03Ks'>https://openreview.net/forum?id=hyPUZX03Ks</a></p>
<p><b>Keywords</b>: video prediction, neural coding, symmetry discovery, self-supervised representation-learning
</p><p><b>Compressor summary</b>: The paper proposes a self-supervised learning method that uses natural video patterns to predict future frames and discover simple transformations, achieving comparable performance to conventional deep networks while being interpretable and fast.</p><hr><h3>Focus on Query: Adversarial Mining Transformer for Few-Shot Segmentation</h3>
<p>Yuan Wang, Naisong Luo, Tianzhu Zhang</p>
<p><a href='https://openreview.net/forum?id=hxJu0386if'>https://openreview.net/forum?id=hxJu0386if</a></p>
<p><b>Keywords</b>: Computer Vision, Few-shot Segmentation
</p><p><b>Compressor summary</b>: The paper introduces AMFormer, a new few-shot segmentation model that focuses on query-centric learning and achieves state-of-the-art results with weak support labels.</p><hr><h3>Analyzing Vision Transformers for Image Classification in Class Embedding Space</h3>
<p>Martina G. Vilas, Timothy Schaumlöffel, Gemma Roig</p>
<p><a href='https://openreview.net/forum?id=hwjmEZ8561'>https://openreview.net/forum?id=hwjmEZ8561</a></p>
<p><b>Keywords</b>: transformers, computer vision, image classification, mechanistic interpretability, explainability
</p><p><b>Compressor summary</b>: This paper presents a method to analyze how Vision Transformers learn image representations for classification tasks and use attention and context to build categorical features.</p><hr><h3>NPCL: Neural Processes for Uncertainty-Aware Continual Learning</h3>
<p>Saurav Jha, Dong Gong, He Zhao, Lina Yao</p>
<p><a href='https://openreview.net/forum?id=huh0XmSdBK'>https://openreview.net/forum?id=huh0XmSdBK</a></p>
<p><b>Keywords</b>: Continual Learning, Neural Process, Uncertainty, Incremental Learning
</p><p><b>Compressor summary</b>: The paper proposes a new approach to continuous learning using neural processes, which can handle tasks with less interference, reduce forgetting, and provide reliable uncertainty estimates for deep neural networks.</p><hr><h3>$p$-value Adjustment for Monotonous, Unbiased, and Fast Clustering Comparison</h3>
<p>Kai Klede, Thomas Altstidl, Dario Zanca, Bjoern Eskofier</p>
<p><a href='https://openreview.net/forum?id=htkdwc6jDB'>https://openreview.net/forum?id=htkdwc6jDB</a></p>
<p><b>Keywords</b>: Clustering, (Other) Machine Learning Topics
</p><p><b>Compressor summary</b>: The paper introduces a new clustering comparison method called p-value adjusted Rand Index (PMI_2) that is unbiased, monotonous, and efficient for selecting better clustering and community detection algorithms.</p><hr><h3>AMDP: An Adaptive Detection Procedure for False Discovery Rate Control in High-Dimensional Mediation Analysis</h3>
<p>Jiarong Ding, Xuehu Zhu</p>
<p><a href='https://openreview.net/forum?id=htM8yp2EwX'>https://openreview.net/forum?id=htM8yp2EwX</a></p>
<p><b>Keywords</b>: Mediation analysis, Composite null hypothesis, Local false discovery rate, Optimal ranking rule, High-dimensional
</p><p><b>Compressor summary</b>: The paper introduces a new method for detecting significant mediators in high-dimensional mediation analysis that controls the false discovery rate by ranking and selecting hypotheses based on p-values and composite null probabilities.</p><hr><h3>UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models</h3>
<p>Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, Jiwen Lu</p>
<p><a href='https://openreview.net/forum?id=hrkmlPhp1u'>https://openreview.net/forum?id=hrkmlPhp1u</a></p>
<p><b>Keywords</b>: diffusion models, fast sampling, predictor-corrector, training-free
</p><p><b>Compressor summary</b>: The paper introduces UniPC, a framework for fast and high-quality sampling of diffusion probabilistic models in image synthesis, improving on previous methods especially with few steps.</p><hr><h3>Boosting Verification of Deep Reinforcement Learning via Piece-Wise Linear Decision Neural Networks</h3>
<p>Jiaxu Tian, Dapeng Zhi, Si Liu, Peixin Wang, Cheng Chen, Min Zhang</p>
<p><a href='https://openreview.net/forum?id=hpYb4eUinX'>https://openreview.net/forum?id=hpYb4eUinX</a></p>
<p><b>Keywords</b>: Deep reinforcement learning, Reachability analysis, Hybrid system, State abstraction
</p><p><b>Compressor summary</b>: The paper proposes a new method to verify deep reinforcement learning systems using piece-wise linear decision neural networks, which can reduce overestimation and speed up verification by up to 438 times.</p><hr><h3>Macro Placement by Wire-Mask-Guided Black-Box Optimization</h3>
<p>Yunqi Shi, Ke Xue, Lei Song, Chao Qian</p>
<p><a href='https://openreview.net/forum?id=hoyL1Ypjoo'>https://openreview.net/forum?id=hoyL1Ypjoo</a></p>
<p><b>Keywords</b>: Black Box Optimization, Macro Placement, Electronic Design Automation, Reinforcement Learning, Application
</p><p><b>Compressor summary</b>: The paper proposes a new optimization framework, WireMask-BBO, for improving chip floorplanning by minimizing wirelength and avoiding overlapping, achieving significant improvements in quality and efficiency over previous methods.</p><hr><h3>Computing Approximate $\ell_p$ Sensitivities</h3>
<p>Swati Padmanabhan, David Woodruff, Qiuyi Zhang</p>
<p><a href='https://openreview.net/forum?id=hn1oJO7lg6'>https://openreview.net/forum?id=hn1oJO7lg6</a></p>
<p><b>Keywords</b>: $\ell_p$ sensitivities, Lewis weights, leverage scores, approximation algorithms, total sensitivity
</p><p><b>Compressor summary</b>: The paper introduces efficient algorithms for computing $\ell_p$ sensitivities and other summary statistics of a matrix, which can help reduce dimensionality in regression tasks and improve data quality after removing low-sensitivity points.</p><hr><h3>Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction</h3>
<p>Zechuan Zhang, Li Sun, Zongxin Yang, Ling Chen, Yi Yang</p>
<p><a href='https://openreview.net/forum?id=hlkhPdhuAO'>https://openreview.net/forum?id=hlkhPdhuAO</a></p>
<p><b>Keywords</b>: 3D human avatar reconstruction, vision transformer, parametric body model, tri-plane representation
</p><p><b>Compressor summary</b>: The GTA model uses a transformer-based architecture to reconstruct 3D clothed human avatars from monocular images, overcoming limitations of previous methods by capturing global-correlated features and using a hybrid prior fusion strategy.</p><hr><h3>Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training</h3>
<p>Aleksandra Nowak, Bram Grooten, Decebal Constantin Mocanu, Jacek Tabor</p>
<p><a href='https://openreview.net/forum?id=hkPn7M9k1W'>https://openreview.net/forum?id=hkPn7M9k1W</a></p>
<p><b>Keywords</b>: Dynamic Sparse Training, Pruning, Deep Learning
</p><p><b>Compressor summary</b>: The paragraph discusses the importance of studying pruning criteria for Dynamic Sparse Training (DST) and how simplicity works best in the low-density regime.</p><hr><h3>Iteratively Learn Diverse Strategies with State Distance Information</h3>
<p>Wei Fu, Weihua Du, Jingwei Li, Sunli Chen, Jingzhao Zhang, Yi Wu</p>
<p><a href='https://openreview.net/forum?id=hiwF7aG1dt'>https://openreview.net/forum?id=hiwF7aG1dt</a></p>
<p><b>Keywords</b>: diverse behavior, multi-agent reinforcement learning, deep reinforcement learning
</p><p><b>Compressor summary</b>: The paper proposes a new diversity-driven reinforcement learning algorithm (SIPO) that incorporates state-space distance information and shows improved performance in discovering diverse and interpretable strategies across various domains.</p><hr><h3>Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification</h3>
<p>Kanishk Jain, Shyamgopal Karthik, Vineet Gandhi</p>
<p><a href='https://openreview.net/forum?id=hiQG8qGxso'>https://openreview.net/forum?id=hiQG8qGxso</a></p>
<p><b>Keywords</b>: Hierarchical Classification, Fine-grained Classification, Ensembles, Mistake Severity
</p><p><b>Compressor summary</b>: The paper proposes HiE, a method that uses label hierarchy to correct coarse-grained predictions and improve fine-grained classification performance, achieving state-of-the-art results and practical applicability.</p><hr><h3>Learning Topology-Agnostic EEG Representations with Geometry-Aware Modeling</h3>
<p>Ke Yi, Yansen Wang, Kan Ren, Dongsheng Li</p>
<p><a href='https://openreview.net/forum?id=hiOUySN0ub'>https://openreview.net/forum?id=hiOUySN0ub</a></p>
<p><b>Keywords</b>: Electroencephalogram, EEG Pre-training, EEG-based emotion recognition
</p><p><b>Compressor summary</b>: The authors propose a new EEG pre-training framework, MMM, which uses a unified channel topology and multi-dimensional position encoding to improve performance on emotional recognition tasks.</p><hr><h3>Path following algorithms for $\ell_2$-regularized $M$-estimation with approximation guarantee</h3>
<p>Yunzhang Zhu, Renxiong Liu</p>
<p><a href='https://openreview.net/forum?id=hgLMht2Z3L'>https://openreview.net/forum?id=hgLMht2Z3L</a></p>
<p><b>Keywords</b>: regularization, optimization, tuning parameter selection
</p><p><b>Compressor summary</b>: The paper proposes a method for choosing grid points and an adaptive stopping criterion to efficiently compute approximation solution paths for regularized M-estimation problems in machine learning.</p><hr><h3>Taylor TD-learning</h3>
<p>Michele Garibbo, Maxime Robeyns, Laurence Aitchison</p>
<p><a href='https://openreview.net/forum?id=hcXDbbzgoh'>https://openreview.net/forum?id=hcXDbbzgoh</a></p>
<p><b>Keywords</b>: Reinforcement learning, TD-learning, model-based, variance reduction
</p><p><b>Compressor summary</b>: Taylor TD is a model-based RL framework that reduces variance in continuous state-action settings by using a first-order Taylor series expansion of TD updates, integrating over stochasticity, and providing stable learning guarantees.</p><hr><h3>Exact Generalization Guarantees for (Regularized) Wasserstein Distributionally Robust Models</h3>
<p>Waïss Azizian, Franck Iutzeler, Jérôme Malick</p>
<p><a href='https://openreview.net/forum?id=haniyY7zm1'>https://openreview.net/forum?id=haniyY7zm1</a></p>
<p><b>Keywords</b>: robust optimization, distributionally robust optimization, optimization under uncertainty, generalization, Wasserstein distance, optimal transport
</p><p><b>Compressor summary</b>: Wasserstein distributionally robust estimators are powerful models for prediction and decision-making under uncertainty with improved generalization guarantees on various classes of models without the curse of dimensionality or distribution shifts.</p><hr><h3>$SE(3)$  Equivariant Convolution and Transformer in Ray Space</h3>
<p>Yinshuang Xu, Jiahui Lei, Kostas Daniilidis</p>
<p><a href='https://openreview.net/forum?id=haHIji0yFt'>https://openreview.net/forum?id=haHIji0yFt</a></p>
<p><b>Keywords</b>: equivariance, light field, equivariant convolution over homogeneous space
</p><p><b>Compressor summary</b>: The paper proposes an SE(3)-equivariant convolution and transformer for learning geometric priors from multiple views, which can improve 3D reconstruction and novel view rendering tasks.</p><hr><h3>InsActor: Instruction-driven Physics-based Characters</h3>
<p>Jiawei Ren, Mingyuan Zhang, Cunjun Yu, Xiao Ma, Liang Pan, Ziwei Liu</p>
<p><a href='https://openreview.net/forum?id=hXevuspQnX'>https://openreview.net/forum?id=hXevuspQnX</a></p>
<p><b>Keywords</b>: Physics-based Animation; Human Motion Generation
</p><p><b>Compressor summary</b>: InsActor is a framework that uses diffusion policies and skill discovery to generate physics-based character animations from high-level human instructions.</p><hr><h3>A new perspective on building efficient and expressive 3D equivariant graph neural networks</h3>
<p>weitao Du, Yuanqi Du, Limei Wang, Dieqiao Feng, Guifeng Wang, Shuiwang Ji, Carla P Gomes, Zhi-Ming Ma</p>
<p><a href='https://openreview.net/forum?id=hWPNYWkYPN'>https://openreview.net/forum?id=hWPNYWkYPN</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Geometric Deep Learning, Equivariance, Symmetry
</p><p><b>Compressor summary</b>: The paper introduces local hierarchy of 3D isomorphism to evaluate and improve the expressiveness of equivariant GNNs for 3D object modeling, and presents LEFTNet that achieves state-of-the-art results on molecular property prediction tasks.</p><hr><h3>Hardware Resilience Properties of Text-Guided Image Classifiers</h3>
<p>Syed Talal Wasim, Kabila Haile Soboka, Abdulrahman Mahmoud, Salman Khan, David Brooks, Gu-Yeon Wei</p>
<p><a href='https://openreview.net/forum?id=hVVp8TXIPs'>https://openreview.net/forum?id=hVVp8TXIPs</a></p>
<p><b>Keywords</b>: Hardware Resilience, Reliability, Image Classification, CLIP, Vision-Language, Multimodal
</p><p><b>Compressor summary</b>: The paper proposes a method to improve image classification model reliability using GPT-3 text embeddings as an initialization for the classification layer, resulting in significant hardware reliability improvements with minimal accuracy loss.</p><hr><h3>A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints</h3>
<p>Kareem Ahmed, Kai-Wei Chang, Guy Van den Broeck</p>
<p><a href='https://openreview.net/forum?id=hVAla2O73O'>https://openreview.net/forum?id=hVAla2O73O</a></p>
<p><b>Keywords</b>: Neuro-Symbolic, Tractable Models, Learning with Constraints
</p><p><b>Compressor summary</b>: The authors propose a neuro-symbolic method for enforcing constraints on auto-regressive distributions that approximates the likelihood with pseudolikelihood, improving performance on Sudoku, shortest-path prediction, and detoxifying language models.</p><hr><h3>A Hierarchical Training Paradigm for Antibody Structure-sequence Co-design</h3>
<p>Fang Wu, Stan Z. Li</p>
<p><a href='https://openreview.net/forum?id=hV52oj0Sik'>https://openreview.net/forum?id=hV52oj0Sik</a></p>
<p><b>Keywords</b>: Antibody Design
</p><p><b>Compressor summary</b>: The paper proposes HTP, a hierarchical training paradigm that uses GNNs and protein language models to design therapeutic antibodies with desired binding specificity by incorporating evolutionary information from various databases.</p><hr><h3>Adversarial Examples Are Not Real Features</h3>
<p>Ang Li, Yifei Wang, Yiwen Guo, Yisen Wang</p>
<p><a href='https://openreview.net/forum?id=hSkEcIFi3o'>https://openreview.net/forum?id=hSkEcIFi3o</a></p>
<p><b>Keywords</b>: Adversarial Examples, Adversarial Training, Generalization, Generative Models
</p><p><b>Compressor summary</b>: The paper re-examines the theory of adversarial examples by showing that non-robust features are not as useful or transferable across different learning paradigms, suggesting they are more like shortcuts and robustness alone is insufficient for model reliability.</p><hr><h3>Imagine That! Abstract-to-Intricate Text-to-Image Synthesis with Scene Graph Hallucination Diffusion</h3>
<p>Shengqiong Wu, Hao Fei, Hanwang Zhang, Tat-Seng Chua</p>
<p><a href='https://openreview.net/forum?id=hSTaTBIUCj'>https://openreview.net/forum?id=hSTaTBIUCj</a></p>
<p><b>Keywords</b>: Image Synthesis, Scene Graph, Diffusion Model
</p><p><b>Compressor summary</b>: The text describes a new method for generating detailed images from simple text prompts using scene graph hallucination and diffusion models.</p><hr><h3>Rubik's Cube: High-Order Channel Interactions with a Hierarchical Receptive Field</h3>
<p>Naishan Zheng, Man Zhou, Chong Zhou, Chen Change Loy</p>
<p><a href='https://openreview.net/forum?id=hOOOvOMok5'>https://openreview.net/forum?id=hOOOvOMok5</a></p>
<p><b>Keywords</b>: Image restoration, low-light image enhancement, image de-noising
</p><p><b>Compressor summary</b>: The paper introduces a new high-order channel-wise convolution operator called Rubik's cube convolution that improves image restoration performance efficiently and simply by expanding first-order interactions to arbitrary high orders.</p><hr><h3>Near Optimal Reconstruction of Spherical Harmonic Expansions</h3>
<p>Amir Zandieh, Insu Han, Haim Avron</p>
<p><a href='https://openreview.net/forum?id=hNpedVWwoe'>https://openreview.net/forum?id=hNpedVWwoe</a></p>
<p><b>Keywords</b>: spherical harmonic expansion, Gegenbauer polynomials, interpolation, leverage score sampling
</p><p><b>Compressor summary</b>: The paragraph describes an efficient algorithm to recover the spherical harmonic expansion of functions defined on the unit sphere using a near-optimal number of function evaluations and kernel regression.</p><hr><h3>Game Solving with Online Fine-Tuning</h3>
<p>Ti-Rong Wu, Hung Guei, Ting Han Wei, Chung-Chin Shih, Jui-Te Chin, I-Chen Wu</p>
<p><a href='https://openreview.net/forum?id=hN4qpvGzWn'>https://openreview.net/forum?id=hN4qpvGzWn</a></p>
<p><b>Keywords</b>: Game Solving, Computer Games, AlphaZero, Online Fine-Tuning, Monte Carlo Tree Search, Deep Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper proposes using online fine-tuning and tailor-designed heuristics to improve the AlphaZero algorithm's performance in solving games, especially those with poor lines of play from the losing side.</p><hr><h3>Parts of Speech–Grounded Subspaces in Vision-Language Models</h3>
<p>James Oldfield, Christos Tzelepis, Yannis Panagakis, Mihalis Nicolaou, Ioannis Patras</p>
<p><a href='https://openreview.net/forum?id=hLoanbRrjM'>https://openreview.net/forum?id=hLoanbRrjM</a></p>
<p><b>Keywords</b>: generative models, text-to-image, vision-language models, interpretability
</p><p><b>Compressor summary</b>: The paper proposes a method to separate different visual properties in CLIP's image representations using part-of-speech associations and apply it to text-to-image synthesis and style transfer tasks.</p><hr><h3>Self-Supervised Motion Magnification by Backpropagating Through Optical Flow</h3>
<p>Zhaoying Pan, Daniel Geng, Andrew Owens</p>
<p><a href='https://openreview.net/forum?id=hLPJ7xLbNF'>https://openreview.net/forum?id=hLPJ7xLbNF</a></p>
<p><b>Keywords</b>: Video Processing, Motion Processing, Motion Magnification, Optical Flow
</p><p><b>Compressor summary</b>: The paper introduces a self-supervised technique to magnify subtle motions in video by adjusting the optical flow based on a given factor and training a model with a loss function that minimizes the deviation from the desired magnification.</p><hr><h3>Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems</h3>
<p>Benjamin Coleman, Wang-Cheng Kang, Matthew Fahrbach, Ruoxi Wang, Lichan Hong, Ed H. Chi, Derek Zhiyuan Cheng</p>
<p><a href='https://openreview.net/forum?id=hJzEoQHfCe'>https://openreview.net/forum?id=hJzEoQHfCe</a></p>
<p><b>Keywords</b>: embedding learning; recommendation systems; representation learning
</p><p><b>Compressor summary</b>: The text introduces Feature Multiplexing, a framework that uses one representation space for multiple features, which improves embedding efficiency and effectiveness for web-scale machine learning systems.</p><hr><h3>Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift</h3>
<p>Yongduo Sui, Qitian Wu, Jiancan Wu, Qing Cui, Longfei Li, JUN ZHOU, Xiang Wang, Xiangnan He</p>
<p><a href='https://openreview.net/forum?id=hIGZujtOQv'>https://openreview.net/forum?id=hIGZujtOQv</a></p>
<p><b>Keywords</b>: Graph Neural Network, Graph Data Augmentation, Distribution Shift
</p><p><b>Compressor summary</b>: The text discusses the problem of distribution shifts in graph representation learning, proposing a new data augmentation strategy called Adversarial Invariant Augmentation (AIA) to handle covariate shift on graphs.</p><hr><h3>Face Reconstruction from Facial Templates by Learning Latent Space of a Generator Network</h3>
<p>Hatef Otroshi Shahreza, Sébastien Marcel</p>
<p><a href='https://openreview.net/forum?id=hI6EPhq70A'>https://openreview.net/forum?id=hI6EPhq70A</a></p>
<p><b>Keywords</b>: Face Recognition (FR), Face reconstruction, Generative Adversarial Network (GAN), Privacy, Security, Template Inversion (TI) attack, Transferability
</p><p><b>Compressor summary</b>: The paper proposes a method to attack face recognition systems by reconstructing realistic face images from facial templates using a GAN-based framework, which can be applied in different scenarios and is transferable between systems.</p><hr><h3>Block Broyden's Methods for Solving Nonlinear Equations</h3>
<p>Chengchang Liu, Cheng Chen, Luo Luo, John C.S. Lui</p>
<p><a href='https://openreview.net/forum?id=hHv3UuffXV'>https://openreview.net/forum?id=hHv3UuffXV</a></p>
<p><b>Keywords</b>: Broyden's method, nonlinear equations
</p><p><b>Compressor summary</b>: The paper introduces block variants of good and bad Broyden's methods for nonlinear equation solving, with improved convergence rates and reduced computational cost compared to existing methods.</p><hr><h3>Equivariant Flow Matching with Hybrid Probability Transport for 3D Molecule Generation</h3>
<p>Yuxuan Song, Jingjing Gong, Minkai Xu, Ziyao Cao, Yanyan Lan, Stefano Ermon, Hao Zhou, Wei-Ying Ma</p>
<p><a href='https://openreview.net/forum?id=hHUZ5V9XFu'>https://openreview.net/forum?id=hHUZ5V9XFu</a></p>
<p><b>Keywords</b>: Drug Design, Molecule Generation, Deep Learning, Computational Biology
</p><p><b>Compressor summary</b>: The paper introduces geometric flow matching, a hybrid probability path method that improves the stability and efficiency of diffusion models for generating 3D molecules.</p><hr><h3>Creating a Public Repository for Joining Private Data</h3>
<p>James Cook, Milind Shyani, Nina Mishra</p>
<p><a href='https://openreview.net/forum?id=hExFOGZTSt'>https://openreview.net/forum?id=hExFOGZTSt</a></p>
<p><b>Keywords</b>: Differential privacy, machine learning, linear queries
</p><p><b>Compressor summary</b>: The paper proposes a non-interactive privacy-preserving method to publish datasets with sensitive attributes and enable joins with other datasets on those attributes, using private sketches and satisfying pure differential privacy.</p><hr><h3>A Finite-Sample Analysis of Payoff-Based Independent Learning in Zero-Sum Stochastic Games</h3>
<p>Zaiwei Chen, Kaiqing Zhang, Eric Mazumdar, Asuman E. Ozdaglar, Adam Wierman</p>
<p><a href='https://openreview.net/forum?id=hElNdYMs8Z'>https://openreview.net/forum?id=hElNdYMs8Z</a></p>
<p><b>Keywords</b>: Zero-sum stochastic games, payoff-based independent learning, best-response-type dynamics, finite-sample analysis
</p><p><b>Compressor summary</b>: The authors propose a novel learning dynamics for two-player stochastic games that combines matrix game and stochastic game methods, and provide theoretical guarantees on its convergence, rationality, and symmetry.</p><hr><h3>UniTSFace: Unified Threshold Integrated Sample-to-Sample Loss for Face Recognition</h3>
<p>Qiufu Li, Xi Jia, Jiancan Zhou, Linlin Shen, Jinming Duan</p>
<p><a href='https://openreview.net/forum?id=hE7PG1lUZx'>https://openreview.net/forum?id=hE7PG1lUZx</a></p>
<p><b>Keywords</b>: Face Recognition, Unified Threshold, USS Loss
</p><p><b>Compressor summary</b>: The paper proposes a new loss function (USS loss) for face recognition that uses a unified threshold to distinguish positive from negative facial pairs and improves performance compared to existing methods.</p><hr><h3>Distributionally Robust Linear Quadratic Control</h3>
<p>Bahar Taskesen, Dan Andrei Iancu, Çağıl Koçyiğit, Daniel Kuhn</p>
<p><a href='https://openreview.net/forum?id=hE5RWzQyvf'>https://openreview.net/forum?id=hE5RWzQyvf</a></p>
<p><b>Keywords</b>: linear quadratic control, distributionally robust optimization, optimal transport, Wasserstein distance
</p><p><b>Compressor summary</b>: The paper proposes an optimal control policy for a generalization of the discrete-time, finite-horizon LQG problem with unknown noise distributions, using the Frank-Wolfe algorithm and Kalman filter estimation.</p><hr><h3>Online Adaptive Policy Selection in Time-Varying Systems: No-Regret via Contractive Perturbations</h3>
<p>Yiheng Lin, James A Preiss, Emile Timothy Anand, Yingying Li, Yisong Yue, Adam Wierman</p>
<p><a href='https://openreview.net/forum?id=hDajsofjRM'>https://openreview.net/forum?id=hDajsofjRM</a></p>
<p><b>Keywords</b>: Online policy selection, online control, online learning
</p><p><b>Compressor summary</b>: The GAPS algorithm optimizes online policy selection in systems with varying costs and dynamics by approximating an ideal gradient descent algorithm, achieving optimal or local regret depending on convexity, and outperforming current methods in adaptability.</p><hr><h3>Knowledge Distillation for High Dimensional Search Index</h3>
<p>Zepu Lu, Jin Chen, Defu Lian, ZAIXI ZHANG, Yong Ge, Enhong Chen</p>
<p><a href='https://openreview.net/forum?id=hCg4w8L8Dt'>https://openreview.net/forum?id=hCg4w8L8Dt</a></p>
<p><b>Keywords</b>: Approximate Nearest Neighbor Search, Knowledge Distillation, Product Quantization, Inverted Index
</p><p><b>Compressor summary</b>: The paper proposes a new learning algorithm, KDindex, that improves retrieval performance of compressed search indices by distilling knowledge from high-precision models and using additional supervision signals between queries and documents.</p><hr><h3>Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization</h3>
<p>Liang Zhang, Junchi YANG, Amin Karbasi, Niao He</p>
<p><a href='https://openreview.net/forum?id=hCdqDkA25J'>https://openreview.net/forum?id=hCdqDkA25J</a></p>
<p><b>Keywords</b>: Reproducibility, Convex Optimization, Minimax Optimization, Saddle-Point Problem
</p><p><b>Compressor summary</b>: The paper shows how to achieve both optimal reproducibility and near-optimal convergence for machine learning algorithms under various error-prone oracles, challenging the previous perception of a trade-off between these factors.</p><hr><h3>On the Generalization Properties of Diffusion Models</h3>
<p>Puheng Li, Zhong Li, Huishuai Zhang, Jiang Bian</p>
<p><a href='https://openreview.net/forum?id=hCUG1MCFk5'>https://openreview.net/forum?id=hCUG1MCFk5</a></p>
<p><b>Keywords</b>: diffusion models, training dynamics, generalization gap, modes shift
</p><p><b>Compressor summary</b>: This paper explores the generalization capabilities of diffusion models and provides theoretical and empirical evidence for their performance on different scenarios, such as early stopping and mode shifts.</p><hr><h3>Learning Modulated Transformation in GANs</h3>
<p>Ceyuan Yang, Qihang Zhang, Yinghao Xu, Jiapeng Zhu, Yujun Shen, Bo Dai</p>
<p><a href='https://openreview.net/forum?id=h8vJVABiBP'>https://openreview.net/forum?id=h8vJVABiBP</a></p>
<p><b>Keywords</b>: generative adversarial network, image synthesis, video synthesis
</p><p><b>Compressor summary</b>: The paper introduces a new module (MTM) for GANs that can handle geometry deformation by predicting spatial offsets based on latent codes, improving performance on various generative tasks.</p><hr><h3>Towards robust and generalizable representations of extracellular data using contrastive learning</h3>
<p>Ankit Vishnubhotla, Charlotte Loh, Akash Srivastava, Liam Paninski, Cole Lincoln Hurwitz</p>
<p><a href='https://openreview.net/forum?id=h6WUKM7PCI'>https://openreview.net/forum?id=h6WUKM7PCI</a></p>
<p><b>Keywords</b>: contrastive learning, representations, extracellular, high-density, spike sorting, cell-type classification, transformers, invariance
</p><p><b>Compressor summary</b>: The authors propose a new contrastive learning framework, CEED, for analyzing high-density extracellular recordings in neuroscience and show that it outperforms current methods.</p><hr><h3>VideoComposer: Compositional Video Synthesis with Motion Controllability</h3>
<p>Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, Jingren Zhou</p>
<p><a href='https://openreview.net/forum?id=h4r00NGkjR'>https://openreview.net/forum?id=h4r00NGkjR</a></p>
<p><b>Keywords</b>: Video Synthesis, Video Diffusion Model, Compositional Synthesis
</p><p><b>Compressor summary</b>: VideoComposer is a tool that enables users to create videos with various temporal conditions by using motion vectors from compressed videos and a Spatio-Temporal Condition encoder.</p><hr><h3>Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language.</h3>
<p>Eghbal A. Hosseini, Evelina Fedorenko</p>
<p><a href='https://openreview.net/forum?id=h3lTrt4Ftb'>https://openreview.net/forum?id=h3lTrt4Ftb</a></p>
<p><b>Keywords</b>: (Cognitive/Neuroscience) Language, Structured Prediction, (Application) Natural Language and Text Processing
</p><p><b>Compressor summary</b>: The authors investigate how predicting the next word in a sentence affects the way transformer models represent language, and find that these models tend to favor straighter trajectories for making predictions.</p><hr><h3>Front-door Adjustment Beyond Markov Equivalence with Limited Graph Knowledge</h3>
<p>Abhin Shah, Karthikeyan Shanmugam, Murat Kocaoglu</p>
<p><a href='https://openreview.net/forum?id=h3kuB4z2G9'>https://openreview.net/forum?id=h3kuB4z2G9</a></p>
<p><b>Keywords</b>: Causal Effect Estimation, Front-door Adjustment, Limited Graph Knowledge
</p><p><b>Compressor summary</b>: The text discusses a new method to estimate causal effects without knowing the graph structure, using testable conditional independence statements and front-door-like adjustment with limited side information.</p><hr><h3>Customizable Image Synthesis with Multiple Subjects</h3>
<p>Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, Yang Cao</p>
<p><a href='https://openreview.net/forum?id=h3QNH3qeC3'>https://openreview.net/forum?id=h3QNH3qeC3</a></p>
<p><b>Keywords</b>: text2img, diffusion model, customization
</p><p><b>Compressor summary</b>: This paper proposes an efficient method to synthesize images with multiple user-specified subjects using text embeddings and layout guidance, achieving better results than existing approaches.</p><hr><h3>Scattering Vision Transformer: Spectral Mixing Matters</h3>
<p>Badri Narayana Patro, Vijay Srinivas Agneeswaran</p>
<p><a href='https://openreview.net/forum?id=h3MShWMxNt'>https://openreview.net/forum?id=h3MShWMxNt</a></p>
<p><b>Keywords</b>: Vision Transformer, Scatter Network, Spectral Transformer, Token Mixing, Channel Mixing, and Einstein Blending Method
</p><p><b>Compressor summary</b>: The Scattering Vision Transformer (SVT) is a novel approach that captures intricate image details and reduces attention complexity in vision transformers using spectrally scattering networks and spectral gating.</p><hr><h3>Multi-modal Queried Object Detection in the Wild</h3>
<p>Yifan Xu, Mengdan Zhang, Chaoyou Fu, Peixian Chen, Xiaoshan Yang, Ke Li, Changsheng Xu</p>
<p><a href='https://openreview.net/forum?id=h3CGHf7457'>https://openreview.net/forum?id=h3CGHf7457</a></p>
<p><b>Keywords</b>: open-world object detection, multi-modal query, vision-language pre-training
</p><p><b>Compressor summary</b>: MQ-Det is a method to improve object detection using both text and visual information, which can be easily applied to existing detectors and boosts performance on various tasks.</p><hr><h3>Faster Differentially Private Convex Optimization via Second-Order Methods</h3>
<p>Arun Ganesh, MAHDI HAGHIFAM, Thomas Steinke, Abhradeep Guha Thakurta</p>
<p><a href='https://openreview.net/forum?id=h2lkx9SQCD'>https://openreview.net/forum?id=h2lkx9SQCD</a></p>
<p><b>Keywords</b>: differential privacy, logistic regression, optimization, Newton's method, second-order methods
</p><p><b>Compressor summary</b>: The paper proposes a private version of Newton's method that can accelerate differentially private optimization, and tests it on logistic regression with promising results.</p><hr><h3>Improved Convergence in High Probability of Clipped Gradient Methods with Heavy Tailed Noise</h3>
<p>Ta Duy Nguyen, Thien Hang Nguyen, Alina Ene, Huy Nguyen</p>
<p><a href='https://openreview.net/forum?id=h1FhXVM0cB'>https://openreview.net/forum?id=h1FhXVM0cB</a></p>
<p><b>Keywords</b>: convex optimization, non-convex optimization, high probability convergence, heavy-tailed noise, clipped stochastic gradient descent, clipped stochastic mirror descent
</p><p><b>Compressor summary</b>: The authors propose a new analysis approach for clipped gradient methods with heavy-tailed noise that improves convergence guarantees, allows time-varying step sizes and clipping parameters, and does not need problem constants.</p><hr><h3>Resilient Constrained Learning</h3>
<p>Ignacio Hounie, Alejandro Ribeiro, Luiz F. O. Chamon</p>
<p><a href='https://openreview.net/forum?id=h0RVoZuUl6'>https://openreview.net/forum?id=h0RVoZuUl6</a></p>
<p><b>Keywords</b>: Constrained Learning, Relaxation, Lagrangian duality, Primal-Dual, Machine Learning, Federated Learning, Invariance
</p><p><b>Compressor summary</b>: The paper proposes a constrained learning approach that adapts requirements while solving the task, balancing performance gains against the cost of relaxing constraints, called resilient constrained learning.</p><hr><h3>Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale</h3>
<p>Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, Wei-Ning Hsu</p>
<p><a href='https://openreview.net/forum?id=gzCS252hCO'>https://openreview.net/forum?id=gzCS252hCO</a></p>
<p><b>Keywords</b>: speech generation, flow-matching, diffusion, in-context learning, text-to-speech
</p><p><b>Compressor summary</b>: Voicebox is a versatile text-guided generative model for speech that can perform various tasks, such as zero-shot TTS, noise removal, and style conversion, with better performance and faster speed than VALL-E.</p><hr><h3>Query-based Temporal Fusion with Explicit Motion for 3D Object Detection</h3>
<p>Jinghua Hou, Zhe Liu, dingkang liang, Zhikang Zou, Xiaoqing Ye, Xiang Bai</p>
<p><a href='https://openreview.net/forum?id=gySmwdmVDF'>https://openreview.net/forum?id=gySmwdmVDF</a></p>
<p><b>Keywords</b>: 3D Object Detection, Temporal, LiDAR-only, Multi-modality, Autonomous Driving
</p><p><b>Compressor summary</b>: The paper introduces QTNet, a simple and effective method for 3D detection in autonomous driving that exploits object queries in previous frames to enhance current ones using Motion-guided Temporal Modeling (MTM).</p><hr><h3>Emergent Communication for Rules Reasoning</h3>
<p>Yuxuan Guo, Yifan Hao, Rui Zhang, Enshuai Zhou, Zidong Du, Xishan Zhang, Xinkai Song, Yuanbo Wen, Yongwei Zhao, Xuehai Zhou, Jiaming Guo, Qi Yi, Shaohui Peng, Di Huang, Ruizhi Chen, Qi Guo, Yunji Chen</p>
<p><a href='https://openreview.net/forum?id=gx20B4ItIw'>https://openreview.net/forum?id=gx20B4ItIw</a></p>
<p><b>Keywords</b>: Emergent communication, Multi-agent communication, Raven's Progressive Matrices, Representation learning
</p><p><b>Compressor summary</b>: The paper introduces a new game for training deep-learning-based agents to reason and communicate high-level rules, and presents a dataset and training method for this purpose.</p><hr><h3>Composable Coresets for Determinant Maximization: Greedy is Almost Optimal</h3>
<p>Siddharth Gollapudi, Sepideh Mahabadi, Varun Sivashankar</p>
<p><a href='https://openreview.net/forum?id=gwvwbsnTps'>https://openreview.net/forum?id=gwvwbsnTps</a></p>
<p><b>Keywords</b>: Determinant Maximization, Composable Coresets, Greedy Algorithm, DPP
</p><p><b>Compressor summary</b>: The paper proposes and analyzes a Greedy algorithm for determinant maximization with an almost optimal approximation factor and shows its practicality and local optimality on real data sets.</p><hr><h3>Use perturbations when learning from explanations</h3>
<p>Juyeon Heo, Vihari Piratla, Matthew Robert Wicker, Adrian Weller</p>
<p><a href='https://openreview.net/forum?id=guyhQMSp2F'>https://openreview.net/forum?id=guyhQMSp2F</a></p>
<p><b>Keywords</b>: Learning from explanation, Robustness, Interpretability, Shortcuts, Explanations
</p><p><b>Compressor summary</b>: The paper proposes a new approach to machine learning from explanations that improves performance by using robustness techniques instead of model smoothing.</p><hr><h3>NVFi: Neural Velocity Fields for 3D Physics Learning from Dynamic Videos</h3>
<p>Jinxi Li, Ziyang Song, Bo Yang</p>
<p><a href='https://openreview.net/forum?id=gsi9lJ3994'>https://openreview.net/forum?id=gsi9lJ3994</a></p>
<p><b>Keywords</b>: Physics Learning, Velocity Field, Dynamic Radiance Field, Future Frame Extrapolation
</p><p><b>Compressor summary</b>: The paper proposes a method to learn geometry, appearance, and velocity of 3D scenes from videos for various applications such as future frame prediction and 3D scene analysis.</p><hr><h3>Flow-Based Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection</h3>
<p>Haibao Yu, Yingjuan Tang, Enze Xie, Jilei Mao, Ping Luo, Zaiqing Nie</p>
<p><a href='https://openreview.net/forum?id=gsglrhvQxX'>https://openreview.net/forum?id=gsglrhvQxX</a></p>
<p><b>Keywords</b>: vehicle-infrastructure cooperative autonomous driving, 3D object detection, feature flow, self-supervised learning
</p><p><b>Compressor summary</b>: The Feature Flow Net (FFNet) is a novel cooperative detection framework for autonomous driving that predicts future features and uses feature flow transmission to address temporal asynchrony and limited communication conditions in traffic environments, achieving better performance with low transmission cost and latency.</p><hr><h3>Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision</h3>
<p>Ayush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Joshua B. Tenenbaum, Fredo Durand, William T. Freeman, Vincent Sitzmann</p>
<p><a href='https://openreview.net/forum?id=gq4xkwQZ1l'>https://openreview.net/forum?id=gq4xkwQZ1l</a></p>
<p><b>Keywords</b>: 3D generative models, neural rendering, neural scene representations, NeRF, diffusion models, differentiable rendering, inverse graphics, inverse problems
</p><p><b>Compressor summary</b>: The text introduces a new type of generative models that can learn from indirect observations through a known forward model, enabling conditional generation of hidden signals and applications in computer vision tasks like inverse graphics.</p><hr><h3>Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data</h3>
<p>Jang-Hyun Kim, Sangdoo Yun, Hyun Oh Song</p>
<p><a href='https://openreview.net/forum?id=gpyeRyc858'>https://openreview.net/forum?id=gpyeRyc858</a></p>
<p><b>Keywords</b>: Dataset cleaning, Label error detection, Outlier detection, Neural Networks, Robustness
</p><p><b>Compressor summary</b>: The paper proposes a method to identify and clean data problems using the relational structure of data in the feature-embedded space, and demonstrates its effectiveness on various image, speech, and language tasks.</p><hr><h3>Spectral Evolution and Invariance in Linear-width Neural Networks</h3>
<p>Zhichao Wang, Andrew William Engel, Anand Sarwate, Ioana Dumitriu, Tony Chiang</p>
<p><a href='https://openreview.net/forum?id=gpqBGyKeKH'>https://openreview.net/forum?id=gpqBGyKeKH</a></p>
<p><b>Keywords</b>: Random matrix theory, Heavy tails, Feature learning, Linear-width neural networks, Spike phase transition
</p><p><b>Compressor summary</b>: The spectral properties of high-dimensional linear neural networks change under different training strategies, affecting their generalization and feature learning abilities.</p><hr><h3>Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL</h3>
<p>Chen Sun, Wannan Yang, Thomas Jiralerspong, Dane Malenfant, Benjamin Alsbury-Nealy, Yoshua Bengio, Blake Aaron Richards</p>
<p><a href='https://openreview.net/forum?id=gpJw8f4tIU'>https://openreview.net/forum?id=gpJw8f4tIU</a></p>
<p><b>Keywords</b>: Reinforcement learning, long term credit assignment, rapid credit assignment, contrastive learning, few-shot learning in RL
</p><p><b>Compressor summary</b>: The paper introduces Contrastive Retrospection (ConSpec), an RL algorithm that uses offline contrastive learning to identify critical steps in a task and provides intrinsic rewards for matching them, improving learning in various RL tasks.</p><hr><h3>Coop: Memory is not a Commodity</h3>
<p>Jianhao Zhang, Shihan Ma, Peihong Liu, Jinhui Yuan</p>
<p><a href='https://openreview.net/forum?id=gmmXyAq8TI'>https://openreview.net/forum?id=gmmXyAq8TI</a></p>
<p><b>Keywords</b>: Tensor Rematerialization; Gradient Checkpointing; Activation Recomputing; Deep Learning; Deep Learning Frameworks; Memory Allocator
</p><p><b>Compressor summary</b>: Tensor rematerialization technique optimizes memory allocation and saves resources for deep neural networks by using a sliding window and cheap tensor partitioning.</p><hr><h3>Structure of universal formulas</h3>
<p>Dmitry Yarotsky</p>
<p><a href='https://openreview.net/forum?id=gmVoaAxB1R'>https://openreview.net/forum?id=gmVoaAxB1R</a></p>
<p><b>Keywords</b>: VC-dimension, neural networks, activation functions, approximation, polynomials, algebraic functions
</p><p><b>Compressor summary</b>: The paper studies expressive models like neural networks and their limitations in approximating continuous functions on different sets using a hierarchy of classes and a new family of functions with polynomial constraints.</p><hr><h3>Creating Multi-Level Skill Hierarchies in Reinforcement Learning</h3>
<p>Joshua Benjamin Evans, Özgür Şimşek</p>
<p><a href='https://openreview.net/forum?id=gjBk6IQofa'>https://openreview.net/forum?id=gjBk6IQofa</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Hierarchical Reinforcement Learning, RL, HRL, Skill Discovery, Skill Hierarchies, Graph-Based, Graphs, Graph Clustering, Graph Partitioning
</p><p><b>Compressor summary</b>: The text proposes an automatic method to generate a skill hierarchy for autonomous agents based on modularity maximisation and interaction graphs, which improves their learning performance in various environments.</p><hr><h3>SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling</h3>
<p>Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, Mingsheng Long</p>
<p><a href='https://openreview.net/forum?id=ginTcBUnL8'>https://openreview.net/forum?id=ginTcBUnL8</a></p>
<p><b>Keywords</b>: Time-series analysis, pre-training, masked time-series modeling
</p><p><b>Compressor summary</b>: SimMTM is a simple framework that improves masked modeling for time series by aggregating complementary information from neighbors and uncovering local structure.</p><hr><h3>Scaling Laws for Hyperparameter Optimization</h3>
<p>Arlind Kadra, Maciej Janowski, Martin Wistuba, Josif Grabocka</p>
<p><a href='https://openreview.net/forum?id=ghzEUGfRMD'>https://openreview.net/forum?id=ghzEUGfRMD</a></p>
<p><b>Keywords</b>: hyperparameter optimization, multi-fidelity hyperparameter optimization, multi-fidelity hpo, power laws, deep neural networks, deep power laws, deep ensemble, deep learning, large language models, scaling laws, llm
</p><p><b>Compressor summary</b>: The authors propose Deep Power Laws (DPL), a neural network ensemble that optimizes hyperparameters using power law scaling and gray-box evaluations, outperforming seven other methods on various datasets.</p><hr><h3>Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration</h3>
<p>Longlin Yu, Tianyu Xie, Yu Zhu, Tong Yang, Xiangyu Zhang, Cheng Zhang</p>
<p><a href='https://openreview.net/forum?id=ghIBaprxsV'>https://openreview.net/forum?id=ghIBaprxsV</a></p>
<p><b>Keywords</b>: Hierarchical semi-implicit variational inference, Score based training, Diffusion model
</p><p><b>Compressor summary</b>: HSIVI is a hierarchical extension of SIVI that improves expressiveness and allows acceleration of diffusion models using pre-trained score networks.</p><hr><h3>Reducing Shape-Radiance Ambiguity in Radiance Fields with a Closed-Form Color Estimation Method</h3>
<p>Qihang Fang, Yafei Song, Keqiang Li, Liefeng Bo</p>
<p><a href='https://openreview.net/forum?id=gh9JNeqjzo'>https://openreview.net/forum?id=gh9JNeqjzo</a></p>
<p><b>Keywords</b>: Neural radiance field, Novel-view synthesis, Regularization
</p><p><b>Compressor summary</b>: The paper proposes a more adaptive method to reduce shape-radiance ambiguity in neural radiance fields by estimating the color field based on the density field and using it to regularize NeRF's rendering process.</p><hr><h3>Learning to Configure Separators in Branch-and-Cut</h3>
<p>Sirui Li, Wenbin Ouyang, Max B. Paulus, Cathy Wu</p>
<p><a href='https://openreview.net/forum?id=gf5xJVQS5p'>https://openreview.net/forum?id=gf5xJVQS5p</a></p>
<p><b>Keywords</b>: Combinatorial Optimization, Branch-and-Cut, Learning Guided Optimization, Deep Learning
</p><p><b>Compressor summary</b>: This paper proposes a novel data-driven strategy for selecting combinatorial separators in mixed integer linear programs, which can improve the solve time by up to 72% and 37% on synthetic and real-world benchmarks.</p><hr><h3>Learning From Biased Soft Labels</h3>
<p>Hua Yuan, Yu Shi, Ning Xu, Xu Yang, Xin Geng, Yong Rui</p>
<p><a href='https://openreview.net/forum?id=gevmGxsTSI'>https://openreview.net/forum?id=gevmGxsTSI</a></p>
<p><b>Keywords</b>: Soft label; knowledge distillation; weakly-supervised learning; Machine learning.
</p><p><b>Compressor summary</b>: The paper explores how biased soft labels can still be effective for knowledge distillation, proposes indicators to measure their effectiveness, and applies the framework to three weakly-supervised learning scenarios.</p><hr><h3>Combating Representation Learning Disparity with Geometric Harmonization</h3>
<p>Zhihan Zhou, Jiangchao Yao, Feng Hong, Ya Zhang, Bo Han, Yanfeng Wang</p>
<p><a href='https://openreview.net/forum?id=geLARFEK8O'>https://openreview.net/forum?id=geLARFEK8O</a></p>
<p><b>Keywords</b>: Long-tailed learning, self-supervised learning
</p><p><b>Compressor summary</b>: The text introduces Geometric Harmonization (GH), a method that improves self-supervised learning (SSL) under long-tailed distribution by ensuring category-level uniformity in representation learning, avoiding collapse of tail classes and dominance of head classes.</p><hr><h3>How do Minimum-Norm Shallow Denoisers Look in Function Space?</h3>
<p>Chen Zeno, Greg Ongie, Yaniv Blumenfeld, Nir Weinberger, Daniel Soudry</p>
<p><a href='https://openreview.net/forum?id=gdzxWGGxWE'>https://openreview.net/forum?id=gdzxWGGxWE</a></p>
<p><b>Keywords</b>: Denoiser, Denoising, Neural network, Function space
</p><p><b>Compressor summary</b>: The paper studies the theoretical properties of shallow ReLU neural network denoisers for image reconstruction tasks, deriving their functions and showing they generalize better than an alternative method in low noise settings.</p><hr><h3>xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data</h3>
<p>Jing Gong, Minsheng Hao, Xingyi Cheng, Xin Zeng, Chiming Liu, Jianzhu Ma, Xuegong Zhang, Taifeng Wang, Le Song</p>
<p><a href='https://openreview.net/forum?id=gdwcoBCMVi'>https://openreview.net/forum?id=gdwcoBCMVi</a></p>
<p><b>Keywords</b>: pre-train, encoder-decoder, scRNA-seq, scalable
</p><p><b>Compressor summary</b>: The xTrimoGene model is a novel transformer architecture that can efficiently process large single-cell RNA-seq data and achieve state-of-the-art performance on various downstream tasks, while reducing computational and memory costs significantly.</p><hr><h3>Finding Safe Zones of Markov Decision Processes Policies</h3>
<p>Lee Cohen, Yishay Mansour, Michal Moshkovitz</p>
<p><a href='https://openreview.net/forum?id=gdVcFOvxT3'>https://openreview.net/forum?id=gdVcFOvxT3</a></p>
<p><b>Keywords</b>: Theoretical guarantees, algorithms, learning theory, MDP, computational complexity, Interpretability
</p><p><b>Compressor summary</b>: The paper studies safe zones in Markov decision processes, which are subsets of states that most trajectories stay within, and presents an algorithm to find approximate safe zones with low complexity.</p><hr><h3>Towards Optimal Caching and Model Selection for Large Model Inference</h3>
<p>Banghua Zhu, Ying Sheng, Lianmin Zheng, Clark Barrett, Michael Jordan, Jiantao Jiao</p>
<p><a href='https://openreview.net/forum?id=gd20oaZqqF'>https://openreview.net/forum?id=gd20oaZqqF</a></p>
<p><b>Keywords</b>: caching, model selection, large language models, foundation models, inference, bandit, regret
</p><p><b>Compressor summary</b>: The paper proposes two methods to reduce resource consumption and latency of large foundation models during inference: caching previous queries and selecting the best model for each query, and shows their theoretical and empirical benefits.</p><hr><h3>Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions</h3>
<p>Abhineet Agarwal, Anish Agarwal, Suhas Vijaykumar</p>
<p><a href='https://openreview.net/forum?id=gbhixjg2dX'>https://openreview.net/forum?id=gbhixjg2dX</a></p>
<p><b>Keywords</b>: Causal Inference, Matrix Completion, Combinatorial Learning, Ranking
</p><p><b>Compressor summary</b>: The paper proposes Synthetic Combinations, a method to learn causal parameters from observational data by exploiting latent structure across units and interventions.</p><hr><h3>Object-Centric Slot Diffusion</h3>
<p>Jindong Jiang, Fei Deng, Gautam Singh, Sungjin Ahn</p>
<p><a href='https://openreview.net/forum?id=gbOukzirpK'>https://openreview.net/forum?id=gbOukzirpK</a></p>
<p><b>Keywords</b>: Object-Centric Representation Learning, Diffusion Models, Unsupervised Representation Learning
</p><p><b>Compressor summary</b>: The paper introduces Latent Slot Diffusion, a novel model that combines diffusion models with object-centric learning for better image generation, especially in complex scenes, without supervised annotations.</p><hr><h3>Slot-guided Volumetric Object Radiance Fields</h3>
<p>DI QI, Tong Yang, Xiangyu Zhang</p>
<p><a href='https://openreview.net/forum?id=ganlU27uvj'>https://openreview.net/forum?id=ganlU27uvj</a></p>
<p><b>Keywords</b>: 3D object-centric representation learning, NeRF, 3D-aware slot
</p><p><b>Compressor summary</b>: The paper introduces a new method called sVORF that learns 3D object-centric representations from images by decomposing scenes into objects using volumetric radiance fields and object slots.</p><hr><h3>Semi-Implicit Denoising Diffusion Models (SIDDMs)</h3>
<p>yanwu xu, Mingming Gong, Shaoan Xie, Wei Wei, Matthias Grundmann, kayhan Batmanghelich, Tingbo Hou</p>
<p><a href='https://openreview.net/forum?id=gaktiSjatl'>https://openreview.net/forum?id=gaktiSjatl</a></p>
<p><b>Keywords</b>: Diffusion Model, GAN, Semi-implicit Modeling
</p><p><b>Compressor summary</b>: The paragraph introduces a novel approach to improve sample quality and diversity in generative models by combining an implicit model with the forward diffusion process, enabling larger steps during inference without compromising performance.</p><hr><h3>On Private and Robust Bandits</h3>
<p>Yulian Wu, Xingyu Zhou, Youming Tao, Di Wang</p>
<p><a href='https://openreview.net/forum?id=gaXAjtHic2'>https://openreview.net/forum?id=gaXAjtHic2</a></p>
<p><b>Keywords</b>: Bandits, privacy, robustness
</p><p><b>Compressor summary</b>: The paper investigates how to balance privacy, robustness, and accuracy in multi-armed bandits with heavy-tailed rewards under a limited budget.</p><hr><h3>Best Arm Identification with Fixed Budget: A Large Deviation Perspective</h3>
<p>Po-An Wang, Ruo-Chun Tzeng, Alexandre Proutiere</p>
<p><a href='https://openreview.net/forum?id=gYetLsNO8x'>https://openreview.net/forum?id=gYetLsNO8x</a></p>
<p><b>Keywords</b>: Best arm identification, Large deviation
</p><p><b>Compressor summary</b>: The paper studies how to find the best arm in stochastic Multi-Armed Bandits with a fixed sampling budget, and presents a new adaptive algorithm called Continuous Rejects that outperforms existing methods using Large Deviation techniques.</p><hr><h3>Differentiable sorting for censored time-to-event data.</h3>
<p>Andre Vauvelle, Benjamin Wild, Roland Eils, Spiros Denaxas</p>
<p><a href='https://openreview.net/forum?id=gYWjI7wLhc'>https://openreview.net/forum?id=gYWjI7wLhc</a></p>
<p><b>Keywords</b>: Survival Analysis, Censored Data, Semi-supervised Learning, Time-to-event-data, Algorithmic Supervision, Sorting, Risk Prediction, Weakly-supervised Learning, Machine Learning, Cox's Partial Likelihood, Differentiable Sorting Networks, Transitive Inductive Bias, Ranking Losses, Listwise Ranking, Healthcare Applications, Deep Learning, Neural Networks, Top-k Risk Prediction
</p><p><b>Compressor summary</b>: Diffsurv is a new survival analysis method that uses differentiable sorting networks to handle censored data and improve risk prediction in healthcare applications.</p><hr><h3>Global Convergence Analysis of Local SGD for Two-layer Neural Network without Overparameterization</h3>
<p>Yajie Bao, Amarda Shehu, Mingrui Liu</p>
<p><a href='https://openreview.net/forum?id=gVLKXT9JwG'>https://openreview.net/forum?id=gVLKXT9JwG</a></p>
<p><b>Keywords</b>: convolutional neural network, gaussian input, local SGD, global convergence, non-convex optimization
</p><p><b>Compressor summary</b>: This paper presents the first global convergence analysis of local SGD for two-layer neural networks without overparameterization or noise, using a self-correction mechanism and an exact recursive characterization of the direction of global parameters.</p><hr><h3>Learning Provably Robust Estimators for Inverse Problems via Jittering</h3>
<p>Anselm Krainovic, Mahdi Soltanolkotabi, Reinhard Heckel</p>
<p><a href='https://openreview.net/forum?id=gUlcyeHzw1'>https://openreview.net/forum?id=gUlcyeHzw1</a></p>
<p><b>Keywords</b>: Adversarial Training, Jittering, Denoising, Deconvolution, Compressive Sensing, Inverse Problems, Robustness
</p><p><b>Compressor summary</b>: The paper explores if adding noise during training makes neural networks more resistant to worst-case perturbations in inverse problems like denoising and finds that jittering improves robustness but may not work for all inverse problems.</p><hr><h3>Test-time Adaptation of Discriminative Models via Diffusion Generative Feedback</h3>
<p>Mihir Prabhudesai, Tsung-Wei Ke, Alexander Cong Li, Deepak Pathak, Katerina Fragkiadaki</p>
<p><a href='https://openreview.net/forum?id=gUTVpByfVX'>https://openreview.net/forum?id=gUTVpByfVX</a></p>
<p><b>Keywords</b>: test-time adaptation, diffusion models, generative models, classification, segmentation, depth prediction
</p><p><b>Compressor summary</b>: Diffusion-TTA uses generative models to adapt pre-trained discriminative models for different test examples, improving their accuracy and performance in online settings.</p><hr><h3>BiSLS/SPS: Auto-tune Step Sizes for Stable Bi-level Optimization</h3>
<p>Chen Fan, Gaspard Choné-Ducasse, Mark Schmidt, Christos Thrampoulidis</p>
<p><a href='https://openreview.net/forum?id=gUEekxYr6D'>https://openreview.net/forum?id=gUEekxYr6D</a></p>
<p><b>Keywords</b>: Adaptive step sizes, bi-level optimization, convergence rates, line searches
</p><p><b>Compressor summary</b>: The authors propose adaptive step-size methods for bi-level optimization in deep learning to improve convergence speed without requiring fine-tuning.</p><hr><h3>Rethinking Conditional Diffusion Sampling with Progressive Guidance</h3>
<p>Anh-Dung Dinh, Daochang Liu, Chang Xu</p>
<p><a href='https://openreview.net/forum?id=gThGBHhqcU'>https://openreview.net/forum?id=gThGBHhqcU</a></p>
<p><b>Keywords</b>: Diffusion model, conditional generative model, guidance diffusion, generative models, classifier guidance
</p><p><b>Compressor summary</b>: The paper presents Progressive Guidance, a generalized method for diffusion generative models that addresses diversity and adversarial effects issues in classifier guidance by refining image details progressively.</p><hr><h3>Optimistic Rates for Multi-Task Representation Learning</h3>
<p>Austin Watkins, Enayat Ullah, Thanh Nguyen-Tang, Raman Arora</p>
<p><a href='https://openreview.net/forum?id=gQ4h6WvME0'>https://openreview.net/forum?id=gQ4h6WvME0</a></p>
<p><b>Keywords</b>: Learning Theory, Multi-task and Transfer Learning, Classification
</p><p><b>Compressor summary</b>: The paper investigates how multi-task representation learning can improve transfer learning by providing new statistical rates on the excess risk of the target task depending on the difficulty of the learning task.</p><hr><h3>Partial Matrix Completion</h3>
<p>Elad Hazan, Adam Tauman Kalai, Varun Kanade, Clara Mohri, Y. Jennifer Sun</p>
<p><a href='https://openreview.net/forum?id=gPylY8sCbw'>https://openreview.net/forum?id=gPylY8sCbw</a></p>
<p><b>Keywords</b>: matrix completion, online learning
</p><p><b>Compressor summary</b>: The paragraph introduces the Partial Matrix Completion problem, where the goal is to accurately complete a subset of matrix entries, and presents an algorithm that handles complex sampling distributions and performs well in online settings.</p><hr><h3>Content-based Unrestricted Adversarial Attack</h3>
<p>Zhaoyu Chen, Bo Li, Shuang Wu, Kaixun Jiang, Shouhong Ding, Wenqiang Zhang</p>
<p><a href='https://openreview.net/forum?id=gO60SSGOMy'>https://openreview.net/forum?id=gO60SSGOMy</a></p>
<p><b>Keywords</b>: unrestricted attack, adversarial example, diffusion model, black-box attack, adversarial transferability
</p><p><b>Compressor summary</b>: The proposed Content-based Unrestricted Adversarial Attack framework optimizes images along an adversarial direction to create photorealistic adversarial examples that deceive both human perception and deep neural networks with high attack performance.</p><hr><h3>AdaVAE: Bayesian Structural Adaptation for Variational Autoencoders</h3>
<p>Paribesh Regmi, Rui Li</p>
<p><a href='https://openreview.net/forum?id=gMjIUZBKH8'>https://openreview.net/forum?id=gMjIUZBKH8</a></p>
<p><b>Keywords</b>: nonparametric Bayes, variational autoencoders
</p><p><b>Compressor summary</b>: The paragraph introduces a Bayesian inference framework for variational autoencoders (VAEs) that adapts network structures to data and prevents overfitting, leading to improved generative performance.</p><hr><h3>One Fits All: Power General Time Series Analysis by Pretrained LM</h3>
<p>Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, Rong Jin</p>
<p><a href='https://openreview.net/forum?id=gMS6FVZvmF'>https://openreview.net/forum?id=gMS6FVZvmF</a></p>
<p><b>Keywords</b>: general time series analysis, time series forecasting, cross modality knowledge transfer; pretrained language model;
</p><p><b>Compressor summary</b>: The paper presents Frozen Pretrained Transformer (FPT), a model that uses pre-trained language or image models for time series analysis, and shows its comparable or state-of-the-art performance on various tasks.</p><hr><h3>Triangulation Residual Loss for Data-efficient 3D Pose Estimation</h3>
<p>Jiachen Zhao, Tao Yu, Liang An, Yipeng Huang, Fang Deng, Qionghai Dai</p>
<p><a href='https://openreview.net/forum?id=gLwjBDsE3G'>https://openreview.net/forum?id=gLwjBDsE3G</a></p>
<p><b>Keywords</b>: 3D pose estimation, triangulation, animal pose estimation
</p><p><b>Compressor summary</b>: The paper proposes a self-supervised method called Triangulation Residual loss for 3D pose estimation that uses global multiview geometric consistency and achieves state-of-the-art performance with limited labeled data.</p><hr><h3>Labeling Neural Representations with Inverse Recognition</h3>
<p>Kirill Bykov, Laura Kopf, Shinichi Nakajima, Marius Kloft, Marina MC Höhne</p>
<p><a href='https://openreview.net/forum?id=gLfgyIWiWW'>https://openreview.net/forum?id=gLfgyIWiWW</a></p>
<p><b>Keywords</b>: Explainable AI, Mechanistic Interpretability, Machine Learning, Deep Neural Networks
</p><p><b>Compressor summary</b>: INVERT is a scalable method to link DNNs' complex data representations to human-interpretable concepts without relying on segmentation masks or high computational demands, enabling interpretable explanations and assessing their statistical significance.</p><hr><h3>Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer</h3>
<p>Zikai Xiao, Zihan Chen, Songshang Liu, Hualiang Wang, YANG FENG, Jin Hao, Joey Tianyi Zhou, Jian Wu, Howard Hao Yang, Zuozhu Liu</p>
<p><a href='https://openreview.net/forum?id=gJewjFjfN2'>https://openreview.net/forum?id=gJewjFjfN2</a></p>
<p><b>Keywords</b>: Federated learning, Long-tailed learning, Data heterogeneity
</p><p><b>Compressor summary</b>: The paper introduces $\texttt{Fed-GraB}$, a method for federated long-tailed learning that re-weights client gradients based on global distribution feedback to improve performance on minority classes while preserving majority class performance.</p><hr><h3>Pengi: An Audio Language Model for Audio Tasks</h3>
<p>Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang</p>
<p><a href='https://openreview.net/forum?id=gJLAfO4KUq'>https://openreview.net/forum?id=gJLAfO4KUq</a></p>
<p><b>Keywords</b>: audio language model, audio representation learning, audio and speech processing, multi-task and transfer learning
</p><p><b>Compressor summary</b>: Pengi is an audio language model that uses transfer learning to enable open-ended and close-ended tasks without extra fine-tuning, achieving state-of-the-art performance on 21 downstream tasks.</p><hr><h3>NeRF Revisited: Fixing Quadrature Instability in Volume Rendering</h3>
<p>Mikaela Angelina Uy, Kiyohiro Nakayama, Guandao Yang, Rahul Krishna Thomas, Leonidas Guibas, Ke Li</p>
<p><a href='https://openreview.net/forum?id=gJHAT79cZU'>https://openreview.net/forum?id=gJHAT79cZU</a></p>
<p><b>Keywords</b>: neural radiance fields, volumetric rendering, nerfs, numerical quadrature, importance sampling
</p><p><b>Compressor summary</b>: The paragraph discusses quadrature instability in Neural Radiance Fields (NeRF) and proposes a solution that improves texture, geometry, and depth supervision by reformulating the sample-based rendering equation to match piecewise linear volume density.</p><hr><h3>How Does Adaptive Optimization Impact Local Neural Network Geometry?</h3>
<p>Kaiqi Jiang, Dhruv Malik, Yuanzhi Li</p>
<p><a href='https://openreview.net/forum?id=gIG8LvTLuc'>https://openreview.net/forum?id=gIG8LvTLuc</a></p>
<p><b>Keywords</b>: optimization, adaptive algorithms, neural networks
</p><p><b>Compressor summary</b>: Adaptive optimization methods improve neural network training by biasing iterate trajectories towards regions with lower $R^{\text{OPT}}\_{\text{med}}$, a statistic similar to the condition number of the loss Hessian, compared to vanilla gradient methods like SGD.</p><hr><h3>Generalizing Nonlinear ICA Beyond Structural Sparsity</h3>
<p>Yujia Zheng, Kun Zhang</p>
<p><a href='https://openreview.net/forum?id=gI1SOgW3kw'>https://openreview.net/forum?id=gI1SOgW3kw</a></p>
<p><b>Keywords</b>: Latent variable models, nonlinear independent component analysis
</p><p><b>Compressor summary</b>: The paragraph discusses new identifiability results for nonlinear ICA under different conditions, such as undercompleteness, partial sparsity, source dependence, and flexible grouping structures, addressing limitations of previous methods.</p><hr><h3>Theoretical and Practical Perspectives on what Influence Functions Do</h3>
<p>Andrea Schioppa, Katja Filippova, Ivan Titov, Polina Zablotskaia</p>
<p><a href='https://openreview.net/forum?id=gGl0n7Onug'>https://openreview.net/forum?id=gGl0n7Onug</a></p>
<p><b>Keywords</b>: Explainable AI, Influence Functions, Training Data Attribution
</p><p><b>Compressor summary</b>: Influence functions are useful for debugging and correcting deep neural networks but their predictive power is limited by parameter divergence, which causes influence to fade over training time.</p><hr><h3>SOL: Sampling-based Optimal Linear bounding of arbitrary scalar functions</h3>
<p>Yuriy Biktairov, Jyotirmoy Deshmukh</p>
<p><a href='https://openreview.net/forum?id=gAQCx61chN'>https://openreview.net/forum?id=gAQCx61chN</a></p>
<p><b>Keywords</b>: Neural network verification, Robustness, Linear bounding
</p><p><b>Compressor summary</b>: The paper proposes optimal methods for finding tight linear bounds for activation functions in neural networks, which are important for robustness certification tools, using a new sampling-based approach that is efficient and practical.</p><hr><h3>Inverse Preference Learning: Preference-based RL without a Reward Function</h3>
<p>Joey Hejna, Dorsa Sadigh</p>
<p><a href='https://openreview.net/forum?id=gAP52Z2dar'>https://openreview.net/forum?id=gAP52Z2dar</a></p>
<p><b>Keywords</b>: preference learning, preference-based reinforcement learning, human-in-the-loop reinforcement learning
</p><p><b>Compressor summary</b>: The paper proposes Inverse Preference Learning (IPL), a new simple and efficient algorithm for learning from offline preference data in preference-based RL, which eliminates the need for a learned reward function by using the $Q$-function instead.</p><hr><h3>Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective</h3>
<p>Chenyu You, Weicheng Dai, Yifei Min, Fenglin Liu, David A. Clifton, S Kevin Zhou, Lawrence Hamilton Staib, James s Duncan</p>
<p><a href='https://openreview.net/forum?id=g9gjpFOiO4'>https://openreview.net/forum?id=g9gjpFOiO4</a></p>
<p><b>Keywords</b>: Long-tailed Medical Image Segmentation, Contrastive Learning, Variance Reduction, Imbalanced Learning, Semi-Supervised Learning
</p><p><b>Compressor summary</b>: The paper introduces $\texttt{ARCO}$, a semi-supervised contrastive learning framework that improves medical image segmentation by using stratified group theory, variance-reduced estimation, and universal sampling techniques.</p><hr><h3>Where Did I Come From? Origin Attribution of AI-Generated Images</h3>
<p>Zhenting Wang, Chen Chen, Yi Zeng, Lingjuan Lyu, Shiqing Ma</p>
<p><a href='https://openreview.net/forum?id=g8bjq0qxOl'>https://openreview.net/forum?id=g8bjq0qxOl</a></p>
<p><b>Keywords</b>: Origin attribution of generated images
</p><p><b>Compressor summary</b>: The paper proposes an alteration-free, model-agnostic method for determining if an image was generated by a specific image generation model using reverse-engineering and reconstruction loss.</p><hr><h3>Prioritizing Samples in Reinforcement Learning with Reducible Loss</h3>
<p>Shiva Kanth Sujit, Somjit Nath, Pedro Braga, Samira Ebrahimi Kahou</p>
<p><a href='https://openreview.net/forum?id=g78QqvhnDU'>https://openreview.net/forum?id=g78QqvhnDU</a></p>
<p><b>Keywords</b>: reinforcement learning, sample efficiency, experience replay
</p><p><b>Compressor summary</b>: The paper proposes a method to prioritize reinforcement learning samples based on their learn-ability, which is defined as the decrease of training loss over time, to improve sample efficiency and robustness.</p><hr><h3>BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing</h3>
<p>Dongxu Li, Junnan Li, Steven Hoi</p>
<p><a href='https://openreview.net/forum?id=g6We1SwaY9'>https://openreview.net/forum?id=g6We1SwaY9</a></p>
<p><b>Keywords</b>: Diffusion-based Models, Text-to-Image Generation, Image Editing, Vision-and-Language, Multimodal
</p><p><b>Compressor summary</b>: BLIP-Diffusion is a new model that generates images of a given subject based on both image and text inputs, using a pre-trained multimodal encoder and a diffusion model. It enables efficient fine-tuning and can be combined with other techniques for more advanced applications.</p><hr><h3>Transformers over Directed Acyclic Graphs</h3>
<p>Yuankai Luo, Veronika Thost, Lei Shi</p>
<p><a href='https://openreview.net/forum?id=g49s1N5nmO'>https://openreview.net/forum?id=g49s1N5nmO</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Transformers, Graph Classification, Node Classification, Scalability
</p><p><b>Compressor summary</b>: This paper proposes efficient and effective architecture adaptations for transformer models on directed acyclic graphs (DAGs), enabling them to outperform graph neural networks tailored for DAGs.</p><hr><h3>Incentivizing Honesty among Competitors in Collaborative Learning and Optimization</h3>
<p>Florian E. Dorner, Nikola Konstantinov, Georgi Stoyanov Pashaliev, Martin Vechev</p>
<p><a href='https://openreview.net/forum?id=g2ROKOASiv'>https://openreview.net/forum?id=g2ROKOASiv</a></p>
<p><b>Keywords</b>: Game Theory, Federated Learning, Optimization, Strategic Behavior, Economics, Mechanisms
</p><p><b>Compressor summary</b>: This paper studies how to design mechanisms that encourage honest communication between competitors in collaborative machine learning, despite their incentive to manipulate their updates for personal gain, and demonstrates their effectiveness on a non-convex federated learning task.</p><hr><h3>LART: Neural Correspondence Learning with Latent Regularization Transformer for 3D Motion Transfer</h3>
<p>Haoyu Chen, Hao Tang, Radu Timofte, Luc Van Gool, Guoying Zhao</p>
<p><a href='https://openreview.net/forum?id=g27BggUT3L'>https://openreview.net/forum?id=g27BggUT3L</a></p>
<p><b>Keywords</b>: 3D motion transfer, 3D Transformer, geometric preservation, 3D generation, correspondence learning
</p><p><b>Compressor summary</b>: The paper introduces LART, a 3D Transformer framework for transferring realistic and high-fidelity motions between meshes without key point annotations or predefined correspondence, using latent metric regularization to ensure motion quality.</p><hr><h3>MIMEx: Intrinsic Rewards from Masked Input Modeling</h3>
<p>Toru Lin, Allan Jabri</p>
<p><a href='https://openreview.net/forum?id=g1dMYenhe4'>https://openreview.net/forum?id=g1dMYenhe4</a></p>
<p><b>Keywords</b>: reinforcement learning, exploration, intrinsic reward, intrinsic motivation, masked autoencoder
</p><p><b>Compressor summary</b>: The paper proposes MIMEx, a framework for deriving intrinsic rewards in high-dimensional environments using conditional prediction and masked input modeling, which can improve exploration in sparse-reward visuomotor tasks.</p><hr><h3>Supported Value Regularization for Offline Reinforcement Learning</h3>
<p>Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, Xiangyang Ji</p>
<p><a href='https://openreview.net/forum?id=fze7P9oy6l'>https://openreview.net/forum?id=fze7P9oy6l</a></p>
<p><b>Keywords</b>: offline reinforcement learning
</p><p><b>Compressor summary</b>: SVR is a value regularization method for offline RL that penalizes Q-values for out-of-distribution actions and maintains optimal convergence results while achieving state-of-the-art performance on continuous control tasks.</p><hr><h3>Template-free Articulated Neural Point Clouds for Reposable View Synthesis</h3>
<p>Lukas Uzolas, Elmar Eisemann, Petr Kellnhofer</p>
<p><a href='https://openreview.net/forum?id=fyfmHi8ay3'>https://openreview.net/forum?id=fyfmHi8ay3</a></p>
<p><b>Keywords</b>: Radiance Fields, View Synthesis, Kinematics, Reposing, NeRF
</p><p><b>Compressor summary</b>: The paper introduces a new method for synthesizing realistic dynamic scenes using point-based representation, Linear Blend Skinning, and multi-view video, improving visual quality, learning time, and generalization.</p><hr><h3>Neural (Tangent Kernel) Collapse</h3>
<p>Mariia Seleznova, Dana Weitzner, Raja Giryes, Gitta Kutyniok, Hung-Hsu Chou</p>
<p><a href='https://openreview.net/forum?id=fyLvHzEssH'>https://openreview.net/forum?id=fyLvHzEssH</a></p>
<p><b>Keywords</b>: Neural Collapse, Neural Tangent Kernel, NTK alignment, Local Elasticity, Gradient Flow
</p><p><b>Compressor summary</b>: This work connects two ideas in deep learning, shows how class labels affect neural network training dynamics, and proves that well-trained networks develop structure based on these assumptions.</p><hr><h3>Text-to-Image Diffusion Models are Zero Shot Classifiers</h3>
<p>Kevin Clark, Priyank Jaini</p>
<p><a href='https://openreview.net/forum?id=fxNQJVMwK2'>https://openreview.net/forum?id=fxNQJVMwK2</a></p>
<p><b>Keywords</b>: diffusion models, zero-shot, text-to-image, generative models, foundation models, stable diffusion
</p><p><b>Compressor summary</b>: The authors propose a method to evaluate text-to-image diffusion models as zero-shot classifiers, showing they perform competitively with CLIP on image classification tasks and have advantages in attribute binding. They suggest generative pre-training could be useful for vision and vision-language problems.</p><hr><h3>Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping</h3>
<p>Tianhao Wu, Mingdong Wu, Jiyao Zhang, Yunchong Gan, Hao Dong</p>
<p><a href='https://openreview.net/forum?id=fwvfxDbUFw'>https://openreview.net/forum?id=fwvfxDbUFw</a></p>
<p><b>Keywords</b>: Human-asissting Dexterous Grasping, Score-matching, Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper proposes a new task for training robotic hands to assist users in grasping objects, using a hand-object conditioned grasping primitive and a history-conditional policy.</p><hr><h3>Sensitivity in Translation Averaging</h3>
<p>Lalit Manam, Venu Madhav Govindu</p>
<p><a href='https://openreview.net/forum?id=fvm9jVcpBn'>https://openreview.net/forum?id=fvm9jVcpBn</a></p>
<p><b>Keywords</b>: sensitivity, translation averaging, structure from motion, 3D computer vision
</p><p><b>Compressor summary</b>: The paper studies sensitivity in 3D computer vision's translation averaging under uncertainty, provides a criterion for well-conditioned problems, and offers an algorithm to remove ill-conditioned directions, improving 3D reconstructions.</p><hr><h3>Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought</h3>
<p>Huaxiaoyue Wang, Gonzalo Gonzalez-Pumariega, Yash Sharma, Sanjiban Choudhury</p>
<p><a href='https://openreview.net/forum?id=ftPoVcm821'>https://openreview.net/forum?id=ftPoVcm821</a></p>
<p><b>Keywords</b>: Large Language Model, Code Generation, Robotics
</p><p><b>Compressor summary</b>: The paper introduces Demo2Code, a framework that generates code for robotic tasks from demonstrations using a recursive summarization and synthesis process.</p><hr><h3>UE4-NeRF:Neural Radiance Field for Real-Time Rendering of Large-Scale Scene</h3>
<p>Jiaming Gu, Minchao Jiang, Hongsheng Li, Xiaoyuan Lu, Guangming Zhu, Syed Afaq Ali Shah, Liang Zhang, Mohammed Bennamoun</p>
<p><a href='https://openreview.net/forum?id=fsCcGr8YFR'>https://openreview.net/forum?id=fsCcGr8YFR</a></p>
<p><b>Keywords</b>: NeRF, UE4, Large-scale scenes, Real-time rendering, Rasterization
</p><p><b>Compressor summary</b>: The paper introduces UE4-NeRF, a system for real-time rendering of large-scale scenes using NeRF and Unreal Engine 4, achieving high quality results at 4K resolution with fast frame rates.</p><hr><h3>Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task</h3>
<p>Maya Okawa, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka</p>
<p><a href='https://openreview.net/forum?id=frVo9MzRuU'>https://openreview.net/forum?id=frVo9MzRuU</a></p>
<p><b>Keywords</b>: Diffusion model; Science of deep learning; Mechanistic interpretability
</p><p><b>Compressor summary</b>: The paragraph discusses a study of how conditional diffusion models generate and reason over novel samples, focusing on the factors that affect their compositional generalization abilities.</p><hr><h3>Meek Separators and Their Applications in Targeted Causal Discovery</h3>
<p>Kirankumar Shiragur, Jiaqi Zhang, Caroline Uhler</p>
<p><a href='https://openreview.net/forum?id=frSfSaRGXY'>https://openreview.net/forum?id=frSfSaRGXY</a></p>
<p><b>Keywords</b>: Causality, Graphical Models
</p><p><b>Compressor summary</b>: The text discusses a new technique called Meek separator, which helps in learning part of the causal graph with fewer interventions and presents two randomized algorithms that achieve logarithmic approximation for subset search and causal matching.</p><hr><h3>Agnostically Learning Single-Index Models using Omnipredictors</h3>
<p>Aravind Gollakota, Parikshit Gopalan, Adam Klivans, Konstantinos Stavropoulos</p>
<p><a href='https://openreview.net/forum?id=frHPeRedHo'>https://openreview.net/forum?id=frHPeRedHo</a></p>
<p><b>Keywords</b>: generalized linear models, single-index models, agnostic learning, pac learning, logistic regression, omnipredictors, multiaccuracy, calibration
</p><p><b>Compressor summary</b>: The paper presents a new algorithm to learn Single-Index Models with arbitrary activations and weaker distributional assumptions, based on Omniprediction and Bregman divergences.</p><hr><h3>Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection</h3>
<p>Xilie Xu, Jingfeng Zhang, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli</p>
<p><a href='https://openreview.net/forum?id=fpzA8uRA95'>https://openreview.net/forum?id=fpzA8uRA95</a></p>
<p><b>Keywords</b>: robust pre-training, adversarial contrastive learning, coreset selection
</p><p><b>Compressor summary</b>: The paper proposes a fast method called robustness-aware coreset selection (RCS) for adversarial contrastive learning (ACL), which improves the efficiency of generating adversarial variants and achieves a good balance between speed and robustness.</p><hr><h3>Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds</h3>
<p>Jiayi Huang, Han Zhong, Liwei Wang, Lin Yang</p>
<p><a href='https://openreview.net/forum?id=fpHfRD3f4N'>https://openreview.net/forum?id=fpHfRD3f4N</a></p>
<p><b>Keywords</b>: machine learning, reinforcement learning, linear bandits, heavy-tailed rewards, instance-dependent regret
</p><p><b>Compressor summary</b>: The paper proposes efficient algorithms for reinforcement learning with heavy-tailed rewards, achieving near-optimal regret bounds in both linear bandits and linear function approximation settings.</p><hr><h3>Data-Informed Geometric Space Selection</h3>
<p>Shuai Zhang, Wenqi Jiang</p>
<p><a href='https://openreview.net/forum?id=fpElyckKkd'>https://openreview.net/forum?id=fpElyckKkd</a></p>
<p><b>Keywords</b>: Geometric representation learning
</p><p><b>Compressor summary</b>: The paper proposes a method to automatically choose and combine different geometric spaces for each input data point to improve machine learning tasks without human intervention.</p><hr><h3>Permutation Equivariant Neural Functionals</h3>
<p>Allan Zhou, Kaien Yang, Kaylee Burns, Adriano Cardace, Yiding Jiang, Samuel Sokota, J Zico Kolter, Chelsea Finn</p>
<p><a href='https://openreview.net/forum?id=fmYmXNPmhv'>https://openreview.net/forum?id=fmYmXNPmhv</a></p>
<p><b>Keywords</b>: equivariance, permutation, implicit neural representation, generalization
</p><p><b>Compressor summary</b>: The text introduces a framework for building permutation equivariant neural functionals that process the weights of other neural networks using NF-Layers with an inductive bias based on symmetry.</p><hr><h3>Are Diffusion Models Vision-And-Language Reasoners?</h3>
<p>Benno Krojer, Elinor Poole-Dayan, Vikram Voleti, Christopher Pal, Siva Reddy</p>
<p><a href='https://openreview.net/forum?id=fmJv8Hj0yo'>https://openreview.net/forum?id=fmJv8Hj0yo</a></p>
<p><b>Keywords</b>: diffusion model, automatic evaluation, vision-and-language, compositionality
</p><p><b>Compressor summary</b>: The authors present a method to adapt diffusion-based image generation models for vision-and-language tasks and introduce a new benchmark to evaluate their performance, finding that the proposed approach outperforms CLIP on compositional tasks and reduces bias in Stable Diffusion.</p><hr><h3>Prototypical Variational Autoencoder for 3D Few-shot Object Detection</h3>
<p>Weiliang Tang, Biqi YANG, Xianzhi Li, Yun-Hui Liu, Pheng-Ann Heng, Chi-Wing Fu</p>
<p><a href='https://openreview.net/forum?id=fljrZsJ2I8'>https://openreview.net/forum?id=fljrZsJ2I8</a></p>
<p><b>Keywords</b>: 3D Point Cloud Object Detection, Few Shot Learning, Computer Vision, Geometric Prototype
</p><p><b>Compressor summary</b>: The paper proposes a VAE-based method (GP-VAE) for few-shot 3D object detection, which enhances feature diversity and distinctiveness using probabilistic latent space and improves performance over existing methods.</p><hr><h3>Policy Finetuning in Reinforcement Learning via Design of Experiments using Offline Data</h3>
<p>Ruiqi Zhang, Andrea Zanette</p>
<p><a href='https://openreview.net/forum?id=fjXTcUUgaC'>https://openreview.net/forum?id=fjXTcUUgaC</a></p>
<p><b>Keywords</b>: offline RL, online RL, exploration, non-reactive, fine-tuning
</p><p><b>Compressor summary</b>: The paper proposes an algorithm that uses an offline dataset to create a single non-reactive exploration policy with guaranteed performance.</p><hr><h3>Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences</h3>
<p>Minsu Kim, Federico Berto, Sungsoo Ahn, Jinkyoo Park</p>
<p><a href='https://openreview.net/forum?id=fj0ZeRtUTU'>https://openreview.net/forum?id=fj0ZeRtUTU</a></p>
<p><b>Keywords</b>: Biological sequence design, offline model based optimization, conditional generation, bootstrapping, ensemble
</p><p><b>Compressor summary</b>: The BootGen algorithm optimizes biological sequences by training a generator with rank-based weights and then bootstrapping it with self-generated data labeled by a proxy score function, resulting in diverse and high-scoring designs.</p><hr><h3>Rotating Features for Object Discovery</h3>
<p>Sindy Löwe, Phillip Lippe, Francesco Locatello, Max Welling</p>
<p><a href='https://openreview.net/forum?id=fg7iyNK81W'>https://openreview.net/forum?id=fg7iyNK81W</a></p>
<p><b>Keywords</b>: Object Discovery, Object-Centric Representations, Structured Representation Learning
</p><p><b>Compressor summary</b>: The paper proposes Rotating Features, a method to represent objects in distributed representations, and demonstrates its applicability to real-world data.</p><hr><h3>Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes</h3>
<p>Aran Nayebi, Rishi Rajalingham, Mehrdad Jazayeri, Guangyu Robert Yang</p>
<p><a href='https://openreview.net/forum?id=ffOhY40Nrh'>https://openreview.net/forum?id=ffOhY40Nrh</a></p>
<p><b>Keywords</b>: neural coding, mental simulation, foundation models, primate frontal cortex
</p><p><b>Compressor summary</b>: The authors use various machine learning models to investigate the neural mechanisms behind human and animal understanding of the physical world and future prediction, finding that models trained on dynamic video scenes in latent space best match the data.</p><hr><h3>RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers via Randomized Deletion</h3>
<p>Zhuoqun Huang, Neil G Marchant, Keane Lucas, Lujo Bauer, Olga Ohrimenko, Benjamin I. P. Rubinstein</p>
<p><a href='https://openreview.net/forum?id=ffFcRPpnWx'>https://openreview.net/forum?id=ffFcRPpnWx</a></p>
<p><b>Keywords</b>: certified robustness, randomized smoothing, malware detection, sequence classification, edit distance
</p><p><b>Compressor summary</b>: The paper proposes randomized deletion (RS-Del), a randomized smoothing technique for discrete sequence classifiers, which provides certified robustness against edit distance-bounded adversaries and is applied to malware detection.</p><hr><h3>Analysis of Variance of Multiple Causal Networks</h3>
<p>Zhongli Jiang, Dabao Zhang</p>
<p><a href='https://openreview.net/forum?id=fezV91IJIo'>https://openreview.net/forum?id=fezV91IJIo</a></p>
<p><b>Keywords</b>: causal inference, large graphs, multi-task learning, structural model, directed cyclic graph
</p><p><b>Compressor summary</b>: The authors propose a new method for constructing and comparing multiple directed cyclic graphs (DCGs) using a structural model, limited-information, and parallel computation, which can handle algorithmic difficulty, computational burden, and variational causalities.</p><hr><h3>Representation Learning via Consistent Assignment of Views over Random Partitions</h3>
<p>Thalles Santos Silva, Adín Ramírez Rivera</p>
<p><a href='https://openreview.net/forum?id=fem6BIJkdv'>https://openreview.net/forum?id=fem6BIJkdv</a></p>
<p><b>Keywords</b>: representation learning, unsupervised learning, self-supervised learning, computer vision
</p><p><b>Compressor summary</b>: CARP is a self-supervised clustering method that learns visual feature representations using random partitions and improves downstream task performance.</p><hr><h3>LEPARD: Learning Explicit Part Discovery for 3D Articulated Shape Reconstruction</h3>
<p>Di Liu, Anastasis Stathopoulos, Qilong Zhangli, Yunhe Gao, Dimitris N. Metaxas</p>
<p><a href='https://openreview.net/forum?id=fcYObrixSS'>https://openreview.net/forum?id=fcYObrixSS</a></p>
<p><b>Keywords</b>: 3D computer vision, deep learning
</p><p><b>Compressor summary</b>: LEPARD is a framework that uses learning to reconstruct 3D animal shapes from single images by breaking down the object into simpler, robust 3D parts.</p><hr><h3>A fast heuristic to optimize time-space tradeoff for large models</h3>
<p>Akifumi Imanishi, Zijian Xu, Masayuki Takagi, Sixue Wang, Emilio Castillo</p>
<p><a href='https://openreview.net/forum?id=fbpTObq6TW'>https://openreview.net/forum?id=fbpTObq6TW</a></p>
<p><b>Keywords</b>: Recomputation, Gradient checkpointing, Memory reduction, Computational graph optimization
</p><p><b>Compressor summary</b>: The paper introduces FastSA, a new simulated annealing-based recomputation algorithm that significantly reduces GPU memory usage and is faster than current methods for optimizing large neural networks.</p><hr><h3>Multi Time Scale World Models</h3>
<p>Vaisakh Shaj, Saleh GHOLAM ZADEH, Ozan Demir, Luiz Ricardo Douat, Gerhard Neumann</p>
<p><a href='https://openreview.net/forum?id=fY7dShbtmo'>https://openreview.net/forum?id=fY7dShbtmo</a></p>
<p><b>Keywords</b>: Hierarchical Models; Multi Time Scale Learning; World Models
</p><p><b>Compressor summary</b>: The Multi Time Scale State Space (MTS3) model is a probabilistic framework for learning world models that can make accurate long-horizon predictions and uncertainty estimates over multiple time scales.</p><hr><h3>Sample based Explanations via Generalized Representers</h3>
<p>Che-Ping Tsai, Chih-Kuan Yeh, Pradeep Kumar Ravikumar</p>
<p><a href='https://openreview.net/forum?id=fX64q0SNfL'>https://openreview.net/forum?id=fX64q0SNfL</a></p>
<p><b>Keywords</b>: explainable machine learning, sample based explanation, representer point
</p><p><b>Compressor summary</b>: The paper introduces a new class of sample-based explanations for machine learning models that use both global and local importance measures and show how existing methods can be considered as special cases.</p><hr><h3>Rethinking Tokenizer and Decoder in Masked Graph Modeling for Molecules</h3>
<p>Zhiyuan Liu, Yaorui Shi, An Zhang, Enzhi Zhang, Kenji Kawaguchi, Xiang Wang, Tat-Seng Chua</p>
<p><a href='https://openreview.net/forum?id=fWLf8DV0fI'>https://openreview.net/forum?id=fWLf8DV0fI</a></p>
<p><b>Keywords</b>: Molecular Representation Learning, Masked Graph Modeling, Graph Tokenizer
</p><p><b>Compressor summary</b>: The paragraph discusses a new approach to learning representations of molecular graphs, focusing on improving the tokenizer and decoder in the masked graph modeling process, and introduces a novel method called SimSGT that outperforms existing methods.</p><hr><h3>Learning Domain-Aware Detection Head with Prompt Tuning</h3>
<p>Haochen Li, Rui Zhang, Hantao Yao, Xinkai Song, Yifan Hao, Yongwei Zhao, Ling Li, Yunji Chen</p>
<p><a href='https://openreview.net/forum?id=fW5ZUSVTkv'>https://openreview.net/forum?id=fW5ZUSVTkv</a></p>
<p><b>Keywords</b>: domain adaptation, object detection, prompt tuning
</p><p><b>Compressor summary</b>: The paper proposes a method to improve domain adaptive object detection by using a domain-aware detection head with prompt tuning, which generates a dynamic detection head for each domain using learnable tokens and textual descriptions.</p><hr><h3>Two Sides of One Coin: the Limits of Untuned SGD and the Power of Adaptive Methods</h3>
<p>Junchi YANG, Xiang Li, Ilyas Fatkhullin, Niao He</p>
<p><a href='https://openreview.net/forum?id=fUZUoSLXw3'>https://openreview.net/forum?id=fUZUoSLXw3</a></p>
<p><b>Keywords</b>: Nonconvex optimization, Stochastic Gradient Descent, Adaptive methods
</p><p><b>Compressor summary</b>: Untuned Stochastic Gradient Descent (SGD) has a fast convergence rate but an exponential dependency on smoothness, while adaptive methods prevent this issue and are better for practical use.</p><hr><h3>Asynchronous Proportional Response Dynamics: Convergence in Markets with Adversarial Scheduling</h3>
<p>Yoav Kolumbus, Menahem Levy, Noam Nisan</p>
<p><a href='https://openreview.net/forum?id=fU9U7OYxfE'>https://openreview.net/forum?id=fU9U7OYxfE</a></p>
<p><b>Keywords</b>: Asynchronous Dynamics, Fisher Markets, Proportional Response, Best Response, Game Dynamics, Competitive Equilibrium, Convergence
</p><p><b>Compressor summary</b>: In this study, researchers explore how Proportional Response Dynamics (PRD) lead to competitive equilibria in linear Fisher markets with asynchronous bidding and reveal new properties of these markets.</p><hr><h3>Curriculum Learning for Graph Neural Networks: Which Edges Should We Learn First</h3>
<p>Zheng Zhang, Junxiang Wang, Liang Zhao</p>
<p><a href='https://openreview.net/forum?id=fTyGT5fulj'>https://openreview.net/forum?id=fTyGT5fulj</a></p>
<p><b>Keywords</b>: Graph neural networks, Curriculum learning, Graph structure learning
</p><p><b>Compressor summary</b>: The paper proposes a novel curriculum learning strategy for graph neural networks that gradually incorporates edges based on their difficulty, improving representation quality and generalization.</p><hr><h3>Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks</h3>
<p>Eshaan Nichani, Alex Damian, Jason D. Lee</p>
<p><a href='https://openreview.net/forum?id=fShubymWrc'>https://openreview.net/forum?id=fShubymWrc</a></p>
<p><b>Keywords</b>: Deep Learning Theory, Feature Learning, Three-Layer Neural Network, Depth Separation, Gradient Descent, Representation Learning
</p><p><b>Compressor summary</b>: This paper explores how three-layer neural networks have better feature learning capabilities than two-layer networks, and provides theoretical and empirical evidence for this claim.</p><hr><h3>In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer</h3>
<p>Yuzhou Cao, Hussein Mozannar, Lei Feng, Hongxin Wei, Bo An</p>
<p><a href='https://openreview.net/forum?id=fPAAgjISu0'>https://openreview.net/forum?id=fPAAgjISu0</a></p>
<p><b>Keywords</b>: Classification, Learning to Defer, Probability Estimation
</p><p><b>Compressor summary</b>: The paper proposes a new loss function for learning to defer in machine learning classifiers that avoids uncalibrated estimates and improves classification accuracy.</p><hr><h3>Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks</h3>
<p>Blake Bordelon, Cengiz Pehlevan</p>
<p><a href='https://openreview.net/forum?id=fKwG6grp8o'>https://openreview.net/forum?id=fKwG6grp8o</a></p>
<p><b>Keywords</b>: Deep Learning Theory, Feature Learning, Dynamics, Ensembles
</p><p><b>Compressor summary</b>: The paper studies how finite width affects feature learning neural networks, showing how it influences kernel fluctuations, prediction variance, and online learning in different network depths and training regimes.</p><hr><h3>Reduced Policy Optimization for Continuous Control with Hard Constraints</h3>
<p>Shutong Ding, Jingya Wang, Yali Du, Ye Shi</p>
<p><a href='https://openreview.net/forum?id=fKVEMNmWqU'>https://openreview.net/forum?id=fKVEMNmWqU</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Hard Constraint, Generalized Reduced Gradient
</p><p><b>Compressor summary</b>: The paper proposes a reduced policy optimization (RPO) algorithm that combines reinforcement learning with generalized reduced gradient method to handle general hard constraints in continuous control tasks, and introduces three new benchmarks to test its performance.</p><hr><h3>Learning Energy-Based Prior Model with Diffusion-Amortized MCMC</h3>
<p>Peiyu Yu, Yaxuan Zhu, Sirui Xie, Xiaojian Ma, Ruiqi Gao, Song-Chun Zhu, Ying Nian Wu</p>
<p><a href='https://openreview.net/forum?id=fKQEmHoLb6'>https://openreview.net/forum?id=fKQEmHoLb6</a></p>
<p><b>Keywords</b>: Energy-Based Model, Denoising Diffusion Probabilistic Model, MCMC
</p><p><b>Compressor summary</b>: The paper introduces a diffusion-based amortization method for latent space EBMs to improve sampling quality and generation performance, and provides theoretical and experimental evidence for its effectiveness.</p><hr><h3>Entropic Neural Optimal Transport via Diffusion Processes</h3>
<p>Nikita Gushchin, Alexander Kolesov, Alexander Korotin, Dmitry P. Vetrov, Evgeny Burnaev</p>
<p><a href='https://openreview.net/forum?id=fHyLsfMDIs'>https://openreview.net/forum?id=fHyLsfMDIs</a></p>
<p><b>Keywords</b>: Optimal transport, Schrödinger Bridge, Entropy regularized OT, Neural Networks, Unpaired Learning
</p><p><b>Compressor summary</b>: The authors propose a new neural algorithm for computing optimal transport between distributions based on a dynamic Schrödinger Bridge problem, which is faster and more flexible than previous methods.</p><hr><h3>Calibrated Stackelberg Games: Learning Optimal Commitments Against Calibrated Agents</h3>
<p>Nika Haghtalab, Chara Podimata, Kunhe Yang</p>
<p><a href='https://openreview.net/forum?id=fHsBNNDroC'>https://openreview.net/forum?id=fHsBNNDroC</a></p>
<p><b>Keywords</b>: calibration, Stackelberg games, learning in repeated games, strategic agents, best response, strategic classification, Stackelberg Security Games
</p><p><b>Compressor summary</b>: The paper proposes Calibrated Stackelberg Games, a generalization of standard Stackelberg Games where an agent does not directly observe the principal's actions but responds to calibrated forecasts, and shows that this model can capture real-life applications and achieve optimal utility.</p><hr><h3>Switching Autoregressive Low-rank Tensor Models</h3>
<p>Hyun Dong Lee, Andrew Warrington, Joshua I Glaser, Scott Linderman</p>
<p><a href='https://openreview.net/forum?id=fFJThJ94rY'>https://openreview.net/forum?id=fFJThJ94rY</a></p>
<p><b>Keywords</b>: switching, autoregressive, low-rank tensor, time-series, probabilistic, neural, neuroscience, behavioral, arhmm, slds
</p><p><b>Compressor summary</b>: SALT models are a new probabilistic approach to time-series analysis that balance the benefits of ARHMMs and SLDSs by using low-rank factorization to control parameters and capture long-range dependencies.</p><hr><h3>Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning</h3>
<p>Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, Xuelong Li</p>
<p><a href='https://openreview.net/forum?id=fAdMly4ki5'>https://openreview.net/forum?id=fAdMly4ki5</a></p>
<p><b>Keywords</b>: multi-task reinforcement learning, diffusion models, planning, data synthesis
</p><p><b>Compressor summary</b>: The paper proposes a diffusion model that can handle multiple tasks with diverse and multimodal data in offline settings, using Transformer backbones and prompt learning for planning and synthesis.</p><hr><h3>Sequential Memory with Temporal Predictive Coding</h3>
<p>Mufeng Tang, Helen Barron, Rafal Bogacz</p>
<p><a href='https://openreview.net/forum?id=f8zIs2IB6Q'>https://openreview.net/forum?id=f8zIs2IB6Q</a></p>
<p><b>Keywords</b>: Predictive coding, sequential memory, hippocampus
</p><p><b>Compressor summary</b>: The authors propose a novel predictive coding-based model for sequential memory called temporal predictive coding, which can accurately memorize and retrieve sequential inputs with a biologically plausible neural implementation and exhibits properties consistent with neuroscience theories.</p><hr><h3>Learning Re-sampling Methods with Parameter Attribution for Image Super-resolution</h3>
<p>Xiaotong Luo, Yuan Xie, Yanyun Qu</p>
<p><a href='https://openreview.net/forum?id=f7wFwPJwBe'>https://openreview.net/forum?id=f7wFwPJwBe</a></p>
<p><b>Keywords</b>: image super-resolution, long-tail distribution, re-sampling, integrated gradient
</p><p><b>Compressor summary</b>: The paper proposes a bi-sampling method for single image super-resolution, which balances the training data and improves the model's ability to reconstruct hard regions.</p><hr><h3>Provable convergence guarantees for black-box variational inference</h3>
<p>Justin Domke, Robert M. Gower, Guillaume Garrigos</p>
<p><a href='https://openreview.net/forum?id=f71xXsoG1v'>https://openreview.net/forum?id=f71xXsoG1v</a></p>
<p><b>Keywords</b>: optimization, variational inference
</p><p><b>Compressor summary</b>: The text discusses how to provide theoretical guarantees for black-box variational inference by improving gradient estimators with unusual noise bounds and a composite non-smooth objective.</p><hr><h3>Reward Finetuning for Faster and More Accurate Unsupervised Object Discovery</h3>
<p>Katie Z Luo, Zhenzhen Liu, Xiangyu Chen, Yurong You, Sagie Benaim, Cheng Perng Phoo, Mark Campbell, Wen Sun, Bharath Hariharan, Kilian Q Weinberger</p>
<p><a href='https://openreview.net/forum?id=f6rQJ83ycb'>https://openreview.net/forum?id=f6rQJ83ycb</a></p>
<p><b>Keywords</b>: Self Driving, Self-Supervised Object Discovery, Reward Ranked Finetuning
</p><p><b>Compressor summary</b>: The paper proposes a machine learning method for detecting objects from LiDAR points using heuristics as human feedback, leading to faster and more accurate results than previous methods.</p><hr><h3>SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models</h3>
<p>Shuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, Zhi-Ming Ma</p>
<p><a href='https://openreview.net/forum?id=f6a9XVFYIo'>https://openreview.net/forum?id=f6a9XVFYIo</a></p>
<p><b>Keywords</b>: Diffusion Model Sampler, Multi-step SDE Solver
</p><p><b>Compressor summary</b>: The paper proposes SA-Solver, an efficient stochastic method for solving diffusion SDEs and generating high-quality data using a variance-controlled approach.</p><hr><h3>Norm-guided latent space exploration for text-to-image generation</h3>
<p>Dvir Samuel, Rami Ben-Ari, Nir Darshan, Haggai Maron, Gal Chechik</p>
<p><a href='https://openreview.net/forum?id=f56xMRb7Vt'>https://openreview.net/forum?id=f56xMRb7Vt</a></p>
<p><b>Keywords</b>: diffusion models, few-shot learning, long-tail learning
</p><p><b>Compressor summary</b>: The paper proposes a new method for interpolating between text-to-image diffusion seeds that considers their norm values, which improves the generation of rare concepts and performance on few-shot and long-tail learning tasks.</p><hr><h3>The Learnability of In-Context Learning</h3>
<p>Noam Wies, Yoav Levine, Amnon Shashua</p>
<p><a href='https://openreview.net/forum?id=f3JNQd7CHM'>https://openreview.net/forum?id=f3JNQd7CHM</a></p>
<p><b>Keywords</b>: in-context, PAC, language models, foundation models, LLMs
</p><p><b>Compressor summary</b>: This paper proposes a theoretical framework for in-context learning, showing how it can efficiently identify and learn tasks from input examples without modifying large language models' weights.</p><hr><h3>Privacy Auditing with One (1) Training Run</h3>
<p>Thomas Steinke, Milad Nasr, Matthew Jagielski</p>
<p><a href='https://openreview.net/forum?id=f38EY21lBw'>https://openreview.net/forum?id=f38EY21lBw</a></p>
<p><b>Keywords</b>: Differential privacy, membership inference attacks, privacy auditing
</p><p><b>Compressor summary</b>: The proposed auditing scheme for differentially private machine learning systems uses parallelism and statistical generalization to assess privacy with minimal assumptions and fewer training runs than existing methods.</p><hr><h3>Iterative Reachability Estimation for Safe Reinforcement Learning</h3>
<p>Milan Ganai, Zheng Gong, Chenning Yu, Sylvia Lee Herbert, Sicun Gao</p>
<p><a href='https://openreview.net/forum?id=f2U4HCY8bg'>https://openreview.net/forum?id=f2U4HCY8bg</a></p>
<p><b>Keywords</b>: Constraints, Safety, Hamilton Jacobi Reachability, Deep Reinforcement Learning, Robotics
</p><p><b>Compressor summary</b>: The paragraph introduces a new framework called RESPO that optimizes for rewards and safety in reinforcement learning and shows its effectiveness on various environments.</p><hr><h3>HubRouter: Learning Global Routing via Hub Generation and Pin-hub Connection</h3>
<p>Xingbo Du, Chonghua Wang, Ruizhe Zhong, Junchi Yan</p>
<p><a href='https://openreview.net/forum?id=f0Jj3C3Pnp'>https://openreview.net/forum?id=f0Jj3C3Pnp</a></p>
<p><b>Keywords</b>: global routing, generative models
</p><p><b>Compressor summary</b>: The paper proposes a novel hub-based approach for global routing in VLSI systems, using deep generative models for hub generation and an actor-critic model for pin-hub connection, which improves efficiency and quality over existing methods.</p><hr><h3>CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders</h3>
<p>Anthony Fuller, Koreen Millard, James R Green</p>
<p><a href='https://openreview.net/forum?id=ezqI5WgGvY'>https://openreview.net/forum?id=ezqI5WgGvY</a></p>
<p><b>Keywords</b>: Remote Sensing, Earth Observation, Self-supervised learning, Multimodal
</p><p><b>Compressor summary</b>: CROMA is a framework that uses self-supervised learning to learn rich unimodal and multimodal representations from spatially aligned multimodal data in remote sensing, outperforming the current SoTA multispectral model on various benchmarks.</p><hr><h3>$\texttt{TACO}$: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning</h3>
<p>Ruijie Zheng, Xiyao Wang, Yanchao Sun, Shuang Ma, Jieyu Zhao, Huazhe Xu, Hal Daumé III, Furong Huang</p>
<p><a href='https://openreview.net/forum?id=ezCsMOy1w9'>https://openreview.net/forum?id=ezCsMOy1w9</a></p>
<p><b>Keywords</b>: Deep Reinforcement Learning, Visual Reinforcement Learning, Online Visual RL, Offline Visual RL, Action Representation
</p><p><b>Compressor summary</b>: $\texttt{TACO}$ is a temporal contrastive learning method that improves sample efficiency in reinforcement learning by learning concurrent state and action representations.</p><hr><h3>Continual Learning for Instruction Following from Realtime Feedback</h3>
<p>Alane Suhr, Yoav Artzi</p>
<p><a href='https://openreview.net/forum?id=ez6Cb0ZGzG'>https://openreview.net/forum?id=ez6Cb0ZGzG</a></p>
<p><b>Keywords</b>: continual learning, interaction, instruction following, user feedback, natural language processing, language grounding, situated interaction, collaboration
</p><p><b>Compressor summary</b>: The paper presents an approach that trains an agent from user feedback in natural language and binary form, using contextual bandit learning to improve instruction execution accuracy and show robustness and equivalence to supervised data.</p><hr><h3>An Adaptive Algorithm for Learning with Unknown Distribution Drift</h3>
<p>Alessio Mazzetto, Eli Upfal</p>
<p><a href='https://openreview.net/forum?id=exiXmAfuDK'>https://openreview.net/forum?id=exiXmAfuDK</a></p>
<p><b>Keywords</b>: statistical learning, learning theory, machine learning, supervised learning, non-stationary, transfer learning, distribution drift
</p><p><b>Compressor summary</b>: The paper presents a method for learning from data with unknown distribution changes, without needing information about the change magnitude, and shows its effectiveness in binary classification and linear regression tasks.</p><hr><h3>Model Spider: Learning to Rank Pre-Trained Models Efficiently</h3>
<p>Yi-Kai Zhang, Ting-Ji Huang, Yao-Xiang Ding, De-Chuan Zhan, Han-Jia Ye</p>
<p><a href='https://openreview.net/forum?id=exg62lfHrB'>https://openreview.net/forum?id=exg62lfHrB</a></p>
<p><b>Keywords</b>: Pre-trained Model Ranking, Transfer Learning
</p><p><b>Compressor summary</b>: Model Spider is a method that uses vector representations to efficiently select and enrich Pre-Trained Models for various tasks, including visual models and Large Language Models.</p><hr><h3>Learning Dictionary for Visual Attention</h3>
<p>Yingjie Liu, Xuan Liu, Hui Yu, XUAN TANG, Xian Wei</p>
<p><a href='https://openreview.net/forum?id=exPzwOhBgx'>https://openreview.net/forum?id=exPzwOhBgx</a></p>
<p><b>Keywords</b>: dictionary learning, attention, transformer, computer vision, point cloud
</p><p><b>Compressor summary</b>: The authors propose a novel attention module called Dic-Attn, which uses dictionary learning to decompose and reconstruct input data, enabling efficient visual attention construction for computer vision tasks.</p><hr><h3>Geometry-Aware Adaptation for Pretrained Models</h3>
<p>Nicholas Roberts, Xintong Li, Dyah Adila, Sonia Cromp, Tzu-Heng Huang, Jitian Zhao, Frederic Sala</p>
<p><a href='https://openreview.net/forum?id=exGOXqxR0L'>https://openreview.net/forum?id=exGOXqxR0L</a></p>
<p><b>Keywords</b>: structured prediction, learning on graphs, partially observed label spaces, high cardinality label spaces
</p><p><b>Compressor summary</b>: The proposed technique Loki improves the performance of machine learning models by using external or self-derived metrics to adapt to new classes without additional training, achieving significant improvements over existing methods.</p><hr><h3>VPP: Efficient Conditional 3D Generation via Voxel-Point Progressive Representation</h3>
<p>Zekun Qi, Muzhou Yu, Runpei Dong, Kaisheng Ma</p>
<p><a href='https://openreview.net/forum?id=etd0ebzGOG'>https://openreview.net/forum?id=etd0ebzGOG</a></p>
<p><b>Keywords</b>: Text to shape generation, 3D shape generation, Efficient inference, Representation Learning
</p><p><b>Compressor summary</b>: The proposed Voxel-Point Progressive Representation (VPP) method improves 3D generation quality and efficiency by combining structured voxel representation and sparse point representation, enabling diverse and high-fidelity 3D shape creation for various downstream tasks.</p><hr><h3>Causal Discovery from Subsampled Time Series with Proxy Variables</h3>
<p>Mingzhou Liu, Xinwei Sun, Lingjing Hu, Yizhou Wang</p>
<p><a href='https://openreview.net/forum?id=etYk6TeO2q'>https://openreview.net/forum?id=etYk6TeO2q</a></p>
<p><b>Keywords</b>: causal discovery, time series, subsampling, proxy variables
</p><p><b>Compressor summary</b>: The paper proposes a new method to infer causality from time series data with missing measurements, using proxies for hidden variables.</p><hr><h3>Doubly-Robust Self-Training</h3>
<p>Banghua Zhu, Mingyu Ding, Philip Jacobson, Ming Wu, Wei Zhan, Michael Jordan, Jiantao Jiao</p>
<p><a href='https://openreview.net/forum?id=esy7pkZmKn'>https://openreview.net/forum?id=esy7pkZmKn</a></p>
<p><b>Keywords</b>: semi-supervised learning, self-training, auto-labeling, self-labeling, doubly robust
</p><p><b>Compressor summary</b>: The paper proposes a semi-supervised learning algorithm called doubly-robust self-training that adapts to the quality of pseudo-labels and outperforms the standard self-training method in image classification and 3D object detection tasks.</p><hr><h3>Supply-Side Equilibria in Recommender Systems</h3>
<p>Meena Jagadeesan, Nikhil Garg, Jacob Steinhardt</p>
<p><a href='https://openreview.net/forum?id=eqyhjLG5Nr'>https://openreview.net/forum?id=eqyhjLG5Nr</a></p>
<p><b>Keywords</b>: content creator incentives, Nash equilibria, specialization, economic aspects of recommender systems
</p><p><b>Compressor summary</b>: The paragraph discusses how algorithmic recommender systems affect producer behavior and content diversity, and presents a model that analyzes the conditions for specialization and its impact on profitability in such systems.</p><hr><h3>Understanding and Improving Feature Learning for Out-of-Distribution Generalization</h3>
<p>Yongqiang Chen, Wei Huang, Kaiwen Zhou, Yatao Bian, Bo Han, James Cheng</p>
<p><a href='https://openreview.net/forum?id=eozEoAtjG8'>https://openreview.net/forum?id=eozEoAtjG8</a></p>
<p><b>Keywords</b>: Out-of-Distribution Generalization, Feature Learning, Invariant Risk Minimization
</p><p><b>Compressor summary</b>: The paragraph discusses how ERM can learn both spurious and invariant features, but tends to favor spurious ones if they are stronger, making it harder for models to generalize to out-of-distribution data. It proposes Feature Augmented Training (FeAT) to improve feature learning for OOD tasks.</p><hr><h3>Black-Box Differential Privacy for Interactive ML</h3>
<p>Haim Kaplan, Yishay Mansour, Shay Moran, Kobbi Nissim, Uri Stemmer</p>
<p><a href='https://openreview.net/forum?id=eoDNaH3pfB'>https://openreview.net/forum?id=eoDNaH3pfB</a></p>
<p><b>Keywords</b>: Differential privacy, online learning
</p><p><b>Compressor summary</b>: The authors propose an interactive variant of joint differential privacy that can handle online processes more effectively than traditional forms, allowing for private online classification with less error.</p><hr><h3>Train Once and Explain Everywhere: Pre-training Interpretable Graph Neural Networks</h3>
<p>Jun Yin, Chaozhuo Li, Hao Yan, Jianxun Lian, Senzhang Wang</p>
<p><a href='https://openreview.net/forum?id=enfx8HM4Rp'>https://openreview.net/forum?id=enfx8HM4Rp</a></p>
<p><b>Keywords</b>: Intrinsic Interpretability, Graph Neural Networks, Pre-training and Fine-tuning
</p><p><b>Compressor summary</b>: The Pre-training Interpretable Graph Neural Network (π-GNN) is a new model that learns universal patterns of graph structure and local interactions to provide transparent predictions by identifying influential parts of the input graph.</p><hr><h3>Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design</h3>
<p>Ibrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, Lucas Beyer</p>
<p><a href='https://openreview.net/forum?id=en4LGxpd9E'>https://openreview.net/forum?id=en4LGxpd9E</a></p>
<p><b>Keywords</b>: Vision transformer, scaling laws, compute-optimal model design, vision
</p><p><b>Compressor summary</b>: The paper presents SoViT, a shape-optimized vision transformer that achieves competitive results with much larger models while using less compute resources, and evaluates its performance across various tasks.</p><hr><h3>LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections</h3>
<p>Muhammad Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Horst Possegger, Mateusz Kozinski, Rogerio Feris, Horst Bischof</p>
<p><a href='https://openreview.net/forum?id=elPtHcfjpH'>https://openreview.net/forum?id=elPtHcfjpH</a></p>
<p><b>Keywords</b>: VL Models
</p><p><b>Compressor summary</b>: The paper proposes a label-free method to improve zero-shot visual classification using an unlabeled image collection and auto-generated texts describing the categories, achieving significant performance improvements and outperforming some few-shot prompting methods.</p><hr><h3>Simultaneous embedding of multiple attractor manifolds in a recurrent neural network using constrained gradient optimization</h3>
<p>Haggai Agmon, Yoram Burak</p>
<p><a href='https://openreview.net/forum?id=ekMLUoC2sq'>https://openreview.net/forum?id=ekMLUoC2sq</a></p>
<p><b>Keywords</b>: Theoretical Neuroscience, Computational Neuroscience, Recurrent Neural Networks, Attractor models
</p><p><b>Compressor summary</b>: The paragraph discusses how synaptic weight adjustment can reduce interference in recurrent neural networks that store multiple continuous variables in working memory, leading to better memory storage.</p><hr><h3>Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis</h3>
<p>Victor Letzelter, Mathieu Fontaine, Mickael Chen, Patrick Perez, Slim Essid, Gaël Richard</p>
<p><a href='https://openreview.net/forum?id=eibTaY6qGI'>https://openreview.net/forum?id=eibTaY6qGI</a></p>
<p><b>Keywords</b>: Multiple Choice Learning, Audio processing.
</p><p><b>Compressor summary</b>: Resilient Multiple Choice Learning (rMCL) is a new method for estimating conditional distributions in regression settings with multiple targets per input, using a novel scoring scheme based on Voronoi tessellations to preserve diversity in predictions.</p><hr><h3>Learning and processing the ordinal information of temporal sequences in recurrent neural circuits</h3>
<p>Xiaolong Zou, Zhikun Chu, Qinghai Guo, Jie Cheng, Bo Ho, Si Wu, Yuanyuan Mi</p>
<p><a href='https://openreview.net/forum?id=eeeqORvJbf'>https://openreview.net/forum?id=eeeqORvJbf</a></p>
<p><b>Keywords</b>: temporal sequence processing, temporal order structure, tree-structured attractor
</p><p><b>Compressor summary</b>: The paragraph discusses how recurrent neural circuits can learn to represent the abstract order structure of temporal sequences, allowing for flexible and robust processing of temporal sequences, and suggests that understanding this mechanism could help develop brain-inspired algorithms.</p><hr><h3>DiffVL: Scaling Up Soft Body Manipulation using Vision-Language Driven Differentiable Physics</h3>
<p>Zhiao Huang, Feng Chen, Yewen Pu, Chunru Lin, Hao Su, Chuang Gan</p>
<p><a href='https://openreview.net/forum?id=ecRaDicXxw'>https://openreview.net/forum?id=ecRaDicXxw</a></p>
<p><b>Keywords</b>: Differentiable physics; Soft body manipulation
</p><p><b>Compressor summary</b>: DiffVL is a method that lets non-experts create soft-body manipulation tasks using vision and natural language, making it easier for differential physics solvers to find solutions.</p><hr><h3>Semantic HELM: A Human-Readable Memory for Reinforcement Learning</h3>
<p>Fabian Paischer, Thomas Adler, Markus Hofmarcher, Sepp Hochreiter</p>
<p><a href='https://openreview.net/forum?id=ebMPmx5mr7'>https://openreview.net/forum?id=ebMPmx5mr7</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Language Models, History Compression, Partial Observability, Foundation Models, Interpretability, Explainable AI
</p><p><b>Compressor summary</b>: The authors propose a novel memory mechanism for reinforcement learning agents in partially observable environments that uses CLIP to associate visual inputs with language tokens, which are then fed to a pretrained language model to provide a human-readable representation of the past, improving interpretability and performance on tasks requiring memory.</p><hr><h3>Graph-Structured Gaussian Processes for Transferable Graph Learning</h3>
<p>Jun Wu, Lisa Ainsworth, Andrew Leakey, Haixun Wang, Jingrui He</p>
<p><a href='https://openreview.net/forum?id=eZbqD9BoXe'>https://openreview.net/forum?id=eZbqD9BoXe</a></p>
<p><b>Keywords</b>: graph learning, transfer learning, Gaussian process
</p><p><b>Compressor summary</b>: GraphGP is a framework for adaptively transferring knowledge across graphs with different assumptions, addressing challenges in transferable graph learning by using a novel graph structure-aware neural network and showing its effectiveness through experiments.</p><hr><h3>Unleash the Potential of Image Branch for Cross-modal 3D Object Detection</h3>
<p>Yifan Zhang, Qijian Zhang, Junhui Hou, Yixuan Yuan, Guoliang Xing</p>
<p><a href='https://openreview.net/forum?id=eYCGrGdKf3'>https://openreview.net/forum?id=eYCGrGdKf3</a></p>
<p><b>Keywords</b>: 3D object detection, 3D point cloud
</p><p><b>Compressor summary</b>: The paper introduces UPIDet, a new cross-modal 3D object detector for autonomous vehicles that leverages image domain information to improve LiDAR-based detectors' performance and achieves top rank in the KITTI benchmark.</p><hr><h3>Penguin: Parallel-Packed Homomorphic Encryption for Fast Graph Convolutional Network Inference</h3>
<p>Ran Ran, Nuo Xu, Tao Liu, Wei Wang, Gang Quan, Wujie Wen</p>
<p><a href='https://openreview.net/forum?id=eXubleMT0q'>https://openreview.net/forum?id=eXubleMT0q</a></p>
<p><b>Keywords</b>: Cryptographic inference, Graph Convolutional Network, Parallel Packing
</p><p><b>Compressor summary</b>: The authors propose Penguin, a ciphertext packing technique for enhancing the efficiency of homomorphic encryption-based graph convolutional network inference on encrypted data.</p><hr><h3>Flat Seeking Bayesian Neural Networks</h3>
<p>Van-Anh Nguyen, Long Tung Vuong, Hoang Phan, Thanh-Toan Do, Dinh Phung, Trung Le</p>
<p><a href='https://openreview.net/forum?id=eX6xDto3Ed'>https://openreview.net/forum?id=eX6xDto3Ed</a></p>
<p><b>Keywords</b>: Bayesian, sharpness-aware, posterior
</p><p><b>Compressor summary</b>: The paper proposes a method to improve the generalization ability of Bayesian Neural Networks by making the posterior distribution aware of the model's sharpness/flatness.</p><hr><h3>Practical and Asymptotically Exact Conditional Sampling in Diffusion Models</h3>
<p>Luhuan Wu, Brian L. Trippe, Christian A Naesseth, David Blei, John Patrick Cunningham</p>
<p><a href='https://openreview.net/forum?id=eWKqr1zcRv'>https://openreview.net/forum?id=eWKqr1zcRv</a></p>
<p><b>Keywords</b>: diffusion models; conditional sampling; sequential monte carlo methods; generative models; protein design
</p><p><b>Compressor summary</b>: The Twisted Diffusion Sampler (TDS) is a sequential Monte Carlo algorithm that improves conditional generation by incorporating heuristic approximations without compromising exactness, achieving better results in simulation and protein design tasks.</p><hr><h3>Response Length Perception and  Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline</h3>
<p>Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, Yang You</p>
<p><a href='https://openreview.net/forum?id=eW233GDOpm'>https://openreview.net/forum?id=eW233GDOpm</a></p>
<p><b>Keywords</b>: large language models, inference optimization, batch processing
</p><p><b>Compressor summary</b>: The paper proposes an efficient inference pipeline for large language models that groups queries with similar response lengths into micro-batches, improving throughput without sacrificing effectiveness.</p><hr><h3>Inferring the Future by Imagining the Past</h3>
<p>Kartik Chandra, Tony Chen, Tzu-Mao Li, Jonathan Ragan-Kelley, Joshua B. Tenenbaum</p>
<p><a href='https://openreview.net/forum?id=eVrmcOvJV4'>https://openreview.net/forum?id=eVrmcOvJV4</a></p>
<p><b>Keywords</b>: cognitive science, cogsci, inverse planning, Bayesian inference, theory of mind, Monte Carlo, inverse reinforcement learning
</p><p><b>Compressor summary</b>: The paper proposes a Monte Carlo algorithm that models how humans infer complex sequences of events from static comic book panels or other dynamic scenes, by connecting its inference problem to Monte Carlo path tracing in computer graphics.</p><hr><h3>XAGen: 3D Expressive Human Avatars Generation</h3>
<p>Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Jiashi Feng, Mike Zheng Shou</p>
<p><a href='https://openreview.net/forum?id=eUf0CaS5AP'>https://openreview.net/forum?id=eUf0CaS5AP</a></p>
<p><b>Keywords</b>: Human Avatar, 3D-aware GAN
</p><p><b>Compressor summary</b>: XAGen is a 3D generative model for human avatars that can control facial expressions, jaw poses, hand poses, and other expressive attributes, achieving high realism and diversity.</p><hr><h3>Algorithmic Regularization in Tensor Optimization: Towards a Lifted Approach in Matrix Sensing</h3>
<p>Ziye Ma, Javad Lavaei, Somayeh Sojoudi</p>
<p><a href='https://openreview.net/forum?id=eU6P4aUdCA'>https://openreview.net/forum?id=eU6P4aUdCA</a></p>
<p><b>Keywords</b>: non-convex optimization, low-rank matrix optimization, matrix sensing, implicit bias, tensor, over-parametrization
</p><p><b>Compressor summary</b>: The paper investigates how gradient descent helps generalization in machine learning models by inducing implicit regularization for tensor optimization in the lifted matrix sensing framework.</p><hr><h3>ASPEN: Breaking Operator Barriers for Efficient Parallelization of Deep Neural Networks</h3>
<p>Jongseok Park, Kyungmin Bin, Gibum Park, Sangtae Ha, Kyunghan Lee</p>
<p><a href='https://openreview.net/forum?id=eTp4RetK74'>https://openreview.net/forum?id=eTp4RetK74</a></p>
<p><b>Keywords</b>: Deep Neural Network, Deep Learning, Parallel Execution Algorithm, Parallelization, Deep Learning Parallelism, Dynamic, Asynchronous, Scheduling, Dynamic Scheduling, Dynamic Execution, tile, tiling, dataflow, dataflow graph, tile-based dataflow graph, opportunistic parallelism
</p><p><b>Compressor summary</b>: ASPEN is a novel parallel computation solution for DNNs that removes operator barriers, exposes parallelism across operators, and achieves high resource utilization and memory reuse, outperforming TorchScript and TVM by up to 4.3$\times$.</p><hr><h3>Double Randomized Underdamped Langevin with Dimension-Independent Convergence Guarantee</h3>
<p>Yuanshi Liu, Cong Fang, Tong Zhang</p>
<p><a href='https://openreview.net/forum?id=eTMHsUp3Ii'>https://openreview.net/forum?id=eTMHsUp3Ii</a></p>
<p><b>Keywords</b>: Langevin, Dimension dependence, Acceleration
</p><p><b>Compressor summary</b>: The paper presents a double randomization method for fast high-dimensional sampling of log-concave distributions with composite structures, achieving dimension-independent convergence guarantees and outperforming previous results by a $d^{1/3}$ factor.</p><hr><h3>A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods</h3>
<p>Veit David Wild, Sahra Ghalebikesabi, Dino Sejdinovic, Jeremias Knoblauch</p>
<p><a href='https://openreview.net/forum?id=eTHawKFT4h'>https://openreview.net/forum?id=eTHawKFT4h</a></p>
<p><b>Keywords</b>: Wasserstein gradient flow, generalised variational inference, deep ensembles, Bayesian deep learning, variational Bayes
</p><p><b>Compressor summary</b>: The paper presents a unified theory linking Bayesian, variational Bayesian, and ensemble methods for uncertainty quantification in deep learning by reformulating the non-convex optimization problem as a convex one using Wasserstein gradient flows.</p><hr><h3>Direct Training of SNN using Local Zeroth Order Method</h3>
<p>Bhaskar Mukhoty, Velibor Bojkovic, William de Vazelhes, Xiaohan Zhao, Giulia De Masi, Huan Xiong, Bin Gu</p>
<p><a href='https://openreview.net/forum?id=eTF3VDH2b6'>https://openreview.net/forum?id=eTF3VDH2b6</a></p>
<p><b>Keywords</b>: Spiking Neural Network, Zeroth Order, Surrogate Gradient
</p><p><b>Compressor summary</b>: The paragraph describes a new technique for training spiking neural networks that uses zeroth-order methods, surrogate approximations of the Heaviside function, and efficient backpropagation to achieve better accuracy and faster training.</p><hr><h3>Spatially Resolved Gene Expression Prediction from Histology Images via Bi-modal Contrastive Learning</h3>
<p>Ronald Xie, Kuan Pang, Sai W Chung, Catia Perciani, Sonya MacParland, BO WANG, Gary Bader</p>
<p><a href='https://openreview.net/forum?id=eT1tMdAUoc'>https://openreview.net/forum?id=eT1tMdAUoc</a></p>
<p><b>Keywords</b>: BLEEP, Histology, H&E, Gene Expression Prediction, Spatial Transcriptomics, Contrastive Learning
</p><p><b>Compressor summary</b>: BLEEP is a framework that predicts gene expression from histology images, providing insights into tissue architecture and potentially reducing the time and cost of diagnosis and research.</p><hr><h3>Strategyproof Voting under Correlated Beliefs</h3>
<p>Daniel Halpern, Rachel Li, Ariel D. Procaccia</p>
<p><a href='https://openreview.net/forum?id=eT1QOsssRB'>https://openreview.net/forum?id=eT1QOsssRB</a></p>
<p><b>Keywords</b>: social choice, strategyproof, voting
</p><p><b>Compressor summary</b>: The paper explores how relaxing strategyproofness criteria for selecting winners in ranked voting scenarios can still lead to impossibility theorems, and argues that plurality rule is a promising choice due to its strategyproofness under certain belief models.</p><hr><h3>Classification of Heavy-tailed Features in High Dimensions: a Superstatistical Approach</h3>
<p>Urte Adomaityte, Gabriele Sicuro, Pierpaolo Vivo</p>
<p><a href='https://openreview.net/forum?id=eR7PrfJe9o'>https://openreview.net/forum?id=eR7PrfJe9o</a></p>
<p><b>Keywords</b>: Classification, Gaussian Mixture Model, Superstatistics, Empirical Risk Minimization, Replica theory, Power-law distribution
</p><p><b>Compressor summary</b>: The paper investigates how a mixture of two clouds of data points with random variances learns in high dimensions under different assumptions and distributions, testing Gaussian universality claims and analysing generalisation and separability.</p><hr><h3>Combating Bilateral Edge Noise for Robust Link Prediction</h3>
<p>Zhanke Zhou, Jiangchao Yao, Jiaxu Liu, Xiawei Guo, quanming yao, LI He, Liang Wang, Bo Zheng, Bo Han</p>
<p><a href='https://openreview.net/forum?id=ePkLqJh5kw'>https://openreview.net/forum?id=ePkLqJh5kw</a></p>
<p><b>Keywords</b>: Robust link prediction, Edge noise
</p><p><b>Compressor summary</b>: The text proposes a method called Robust Graph Information Bottleneck (RGIB) to improve link prediction on graphs by extracting reliable supervision signals and avoiding representation collapse under edge noise.</p><hr><h3>TRIAGE: Characterizing and auditing training data for improved regression</h3>
<p>Nabeel Seedat, Jonathan Crabbé, Zhaozhi Qian, Mihaela van der Schaar</p>
<p><a href='https://openreview.net/forum?id=eP6cDDwBNC'>https://openreview.net/forum?id=eP6cDDwBNC</a></p>
<p><b>Keywords</b>: data-centric AI, data characterization, data quality
</p><p><b>Compressor summary</b>: TRIAGE is a novel framework for characterizing training data in regression tasks, using conformal predictive distributions to score samples as under-, over-, or well-estimated by the model, and enabling data sculpting/filtering and dataset selection improvements.</p><hr><h3>Contextual Gaussian Process Bandits with Neural Networks</h3>
<p>Haoting Zhang, Jinghai He, Rhonda Righter, Zuo-Jun Shen, Zeyu Zheng</p>
<p><a href='https://openreview.net/forum?id=eNhW9UnlGG'>https://openreview.net/forum?id=eNhW9UnlGG</a></p>
<p><b>Keywords</b>: contextual bandit, Gaussian process, neural network
</p><p><b>Compressor summary</b>: The paper proposes a neural network-accompanied Gaussian process (NN-AGP) model for solving contextual decision-making problems with high accuracy and uncertainty quantification, and proves its theoretical guarantees and shows its empirical performance.</p><hr><h3>Diversify \& Conquer: Outcome-directed Curriculum RL via Out-of-Distribution Disagreement</h3>
<p>Daesol Cho, Seungjae Lee, H. Jin Kim</p>
<p><a href='https://openreview.net/forum?id=eMR57voMz1'>https://openreview.net/forum?id=eMR57voMz1</a></p>
<p><b>Keywords</b>: Curriculum learning, Out-of-distribution disagreement, Underspecification, Outcome-directed RL
</p><p><b>Compressor summary</b>: The paper proposes a new curriculum RL method called D2C that uses diversification of classifiers and bipartite matching to explore and conquer uninformed search problems without access to domain knowledge or environment characteristics.</p><hr><h3>Equivariant flow matching</h3>
<p>Leon Klein, Andreas Krämer, Frank Noe</p>
<p><a href='https://openreview.net/forum?id=eLH2NFOO1B'>https://openreview.net/forum?id=eLH2NFOO1B</a></p>
<p><b>Keywords</b>: Normalizing Flows, Flow Matching, Equivariance, Boltzmann Generators, Molecular Dynamics, Optimal Transport
</p><p><b>Compressor summary</b>: Equivariant flow matching is a new training method for normalizing flows in physics that uses optimal transport and exploits physical symmetries to improve sampling efficiency and scalability.</p><hr><h3>Frequency-Enhanced Data Augmentation for Vision-and-Language Navigation</h3>
<p>Keji He, Chenyang Si, Zhihe Lu, Yan Huang, Liang Wang, Xinchao Wang</p>
<p><a href='https://openreview.net/forum?id=eKFrXWb0sT'>https://openreview.net/forum?id=eKFrXWb0sT</a></p>
<p><b>Keywords</b>: Vision-and-Language Navigation; High-Frequency; Data Augmentation
</p><p><b>Compressor summary</b>: The paper proposes a new technique to improve visual-textual matching in vision-and-language navigation tasks by using high-frequency information and data augmentation, leading to better performance with simple and efficient models.</p><hr><h3>What Planning Problems Can A Relational Neural Network Solve?</h3>
<p>Jiayuan Mao, Tomás Lozano-Pérez, Joshua B. Tenenbaum, Leslie Pack Kaelbling</p>
<p><a href='https://openreview.net/forum?id=eJZ5vJEaaa'>https://openreview.net/forum?id=eJZ5vJEaaa</a></p>
<p><b>Keywords</b>: Planning, Relational Neural Network, Circuit Complexity
</p><p><b>Compressor summary</b>: The paper analyzes how relational neural networks can be used to learn goal-conditioned policies for planning problems, depending on the number of objects and planning horizon.</p><hr><h3>AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset</h3>
<p>Jiakang Yuan, Bo Zhang, Xiangchao Yan, Botian Shi, Tao Chen, Yikang LI, Yu Qiao</p>
<p><a href='https://openreview.net/forum?id=eIFZtkshgH'>https://openreview.net/forum?id=eIFZtkshgH</a></p>
<p><b>Keywords</b>: 3D Object Detection, 3D Pre-training, Autonomous Driving
</p><p><b>Compressor summary</b>: The paper introduces a semi-supervised point cloud pre-training approach for autonomous driving that improves generalizability across various tasks and benchmarks.</p><hr><h3>RGMIL: Guide Your Multiple-Instance Learning Model with Regressor</h3>
<p>Zhaolong Du, Shasha Mao, Yimeng Zhang, Shuiping Gou, Licheng Jiao, Lin Xiong</p>
<p><a href='https://openreview.net/forum?id=eGoE9CVRPc'>https://openreview.net/forum?id=eGoE9CVRPc</a></p>
<p><b>Keywords</b>: Video Analysis, Multiple-Instance Learning, Representation learning
</p><p><b>Compressor summary</b>: The authors propose RGMIL, a method for video analysis with limited annotations, which uses Regressor-Guided Pooling (RGP) to produce discriminative instance-level representations and achieve near ideal performance compared to supervised models.</p><hr><h3>Revisiting Visual Model Robustness: A Frequency Long-Tailed Distribution View</h3>
<p>Zhiyu Lin, Yifei Gao, Yunfan Yang, Jitao Sang</p>
<p><a href='https://openreview.net/forum?id=eE5L1RkxW0'>https://openreview.net/forum?id=eE5L1RkxW0</a></p>
<p><b>Keywords</b>: visual models, robustness, frequency domain, long-tailed distribution
</p><p><b>Compressor summary</b>: The paper proposes a new strategy to improve visual models' robustness and accuracy by addressing the under-fitting behavior on high-frequency components (HFC) in images, which is caused by limited information content in HFC.</p><hr><h3>How to Select Which Active Learning Strategy is Best Suited for Your Specific Problem and Budget</h3>
<p>Guy Hacohen, Daphna Weinshall</p>
<p><a href='https://openreview.net/forum?id=eDDZh8C4W4'>https://openreview.net/forum?id=eDDZh8C4W4</a></p>
<p><b>Keywords</b>: Deep Active learning, Low budget, High budget, Deep learning
</p><p><b>Compressor summary</b>: The paper proposes a practical method for selecting the best active learning query strategy based on the problem characteristics and available budget, and demonstrates its effectiveness in computer vision tasks.</p><hr><h3>Evaluating Post-hoc Explanations for Graph Neural Networks via Robustness Analysis</h3>
<p>Junfeng Fang, Wei Liu, Yuan Gao, Zemin Liu, An Zhang, Xiang Wang, Xiangnan He</p>
<p><a href='https://openreview.net/forum?id=eD534mPhAg'>https://openreview.net/forum?id=eD534mPhAg</a></p>
<p><b>Keywords</b>: Post-hoc Explainability, Explanation Evaluation, Graph Neural Network, Robustness Analysis
</p><p><b>Compressor summary</b>: The paper proposes OAR and SimOAR, novel evaluation metrics for explaining graph neural networks that address the out-of-distribution issue and improve credibility by measuring adversarial robustness.</p><hr><h3>On Sparse Modern Hopfield Model</h3>
<p>Jerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, Han Liu</p>
<p><a href='https://openreview.net/forum?id=eCgWNU2Imw'>https://openreview.net/forum?id=eCgWNU2Imw</a></p>
<p><b>Keywords</b>: Hopfield Models; Modern Hopfield Networks; Sparse Attention; Memory Networks
</p><p><b>Compressor summary</b>: The paper introduces a sparse version of the modern Hopfield model with improved theoretical properties and empirical performance.</p><hr><h3>POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images</h3>
<p>Antonín Vobecký, Oriane Siméoni, David Hurych, Spyros Gidaris, Andrei Bursuc, Patrick Perez, Josef Sivic</p>
<p><a href='https://openreview.net/forum?id=eBXM62SqKY'>https://openreview.net/forum?id=eBXM62SqKY</a></p>
<p><b>Keywords</b>: open-vocabulary segmentation, voxel occupancy prediction, semantic segmentation, autonomous driving, language-image alignment
</p><p><b>Compressor summary</b>: The paper presents a new model for predicting 3D semantic occupancy maps from 2D images using self-supervised learning without 3D annotations.</p><hr><h3>Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger</h3>
<p>Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, George Karypis</p>
<p><a href='https://openreview.net/forum?id=e8i7OaPj0q'>https://openreview.net/forum?id=e8i7OaPj0q</a></p>
<p><b>Keywords</b>: deep learning, differential privacy, optimization, hyper-parameter tuning
</p><p><b>Compressor summary</b>: Automatic clipping is a technique that simplifies differential privacy training for deep learning models by eliminating the need to tune a clipping threshold and achieving similar or better performance than existing methods.</p><hr><h3>Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models</h3>
<p>Naoki Egami, Musashi Hinck, Brandon M. Stewart, Hanying Wei</p>
<p><a href='https://openreview.net/forum?id=e8RZwixcE4'>https://openreview.net/forum?id=e8RZwixcE4</a></p>
<p><b>Keywords</b>: Computational Social Science, Large Language Models, Statistical Inference, Causal Inference
</p><p><b>Compressor summary</b>: The authors propose a new algorithm (DSL) that uses imperfect annotations from large language models for computational social science research, ensuring unbiased and accurate statistical analysis by combining them with a smaller number of high-quality labels.</p><hr><h3>DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets</h3>
<p>Lazar Atanackovic, Alexander Tong, BO WANG, Leo J Lee, Yoshua Bengio, Jason Hartford</p>
<p><a href='https://openreview.net/forum?id=e7MK5Vq44Q'>https://openreview.net/forum?id=e7MK5Vq44Q</a></p>
<p><b>Keywords</b>: Bayesian Structure Learning, Generative Flow Networks, Single-cell, Dynamical Systems
</p><p><b>Compressor summary</b>: The paper proposes a novel approach for inferring gene regulatory networks using RNA velocity techniques and Generative Flow Networks, addressing both the challenges of cyclicity and uncertainty in the data.</p><hr><h3>Accessing Higher Dimensions for Unsupervised Word Translation</h3>
<p>Sida Wang</p>
<p><a href='https://openreview.net/forum?id=e5srDjF9l7'>https://openreview.net/forum?id=e5srDjF9l7</a></p>
<p><b>Keywords</b>: co-occurrences, unsupervised word translation, bilingual lexicon induction, robust statistics, unsupervised machine translation
</p><p><b>Compressor summary</b>: The authors develop a method for unsupervised word translation that uses high-dimensional signals instead of low-dimensional word vectors, achieving better results with less data and computation.</p><hr><h3>Gacs-Korner Common Information Variational Autoencoder</h3>
<p>Michael Kleinman, Alessandro Achille, Stefano Soatto, Jonathan Kao</p>
<p><a href='https://openreview.net/forum?id=e4XidX6AHd'>https://openreview.net/forum?id=e4XidX6AHd</a></p>
<p><b>Keywords</b>: Common Information, Gacs-Korner, Variational Autoencoder
</p><p><b>Compressor summary</b>: The paragraph introduces a concept called common information that separates shared and unique aspects of two random variables, which can be estimated from data using variational autoencoders and measured against ground-truth factors.</p><hr><h3>CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models</h3>
<p>Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng LYU, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, Bernhard Schölkopf</p>
<p><a href='https://openreview.net/forum?id=e2wtjx0Yqu'>https://openreview.net/forum?id=e2wtjx0Yqu</a></p>
<p><b>Keywords</b>: Large Language Models, Causal Reasoning, Causal Inference, Benchmark Dataset, Natural Language Processing
</p><p><b>Compressor summary</b>: The paper introduces a new natural language processing task and dataset to assess whether large language models can perform causal inference based on formal rules.</p><hr><h3>Estimating and Controlling for Equalized Odds via Sensitive Attribute Predictors</h3>
<p>Beepul Bharti, Paul Yi, Jeremias Sulam</p>
<p><a href='https://openreview.net/forum?id=e2aCgjtjMR'>https://openreview.net/forum?id=e2aCgjtjMR</a></p>
<p><b>Keywords</b>: fairness, sensitive attributes, equalized odds, missing data, proxies
</p><p><b>Compressor summary</b>: This paper proposes a new method for controlling fairness in machine learning models without access to sensitive attributes by using post-processing corrections and provides theoretical guarantees and experimental validation.</p><hr><h3>Likelihood-Based Diffusion Language Models</h3>
<p>Ishaan Gulrajani, Tatsunori Hashimoto</p>
<p><a href='https://openreview.net/forum?id=e2MCL6hObn'>https://openreview.net/forum?id=e2MCL6hObn</a></p>
<p><b>Keywords</b>: diffusion, language, model
</p><p><b>Compressor summary</b>: The authors develop and release a diffusion-based language model (Plaid 1B) that surpasses GPT-2 124M in likelihood on benchmarks and produces coherent samples.</p><hr><h3>Multinomial Logistic Regression: Asymptotic Normality on Null Covariates in High-Dimensions</h3>
<p>Kai Tan, Pierre C Bellec</p>
<p><a href='https://openreview.net/forum?id=e1oe8F2tjV'>https://openreview.net/forum?id=e1oe8F2tjV</a></p>
<p><b>Keywords</b>: High-dimensional statistics, statistical inference, multi-class classification, asymptotic normality, multinomial logistic regression
</p><p><b>Compressor summary</b>: The paper explores how to accurately estimate and test the importance of features in high-dimensional multinomial logistic models.</p><hr><h3>MathNAS: If Blocks Have a Role in Mathematical Architecture Design</h3>
<p>Wang Qinsi, Jinghan Ke, Zhi Liang, Sihai Zhang</p>
<p><a href='https://openreview.net/forum?id=e1l4ZYprQH'>https://openreview.net/forum?id=e1l4ZYprQH</a></p>
<p><b>Keywords</b>: Neural Architecture Search
</p><p><b>Compressor summary</b>: MathNAS is a novel NAS framework that predicts network performance training-free by calculating the performances of all possible building blocks first, reducing the complexity of evaluation and achieving state-of-the-art results on large-scale datasets.</p><hr><h3>Large Language Models of Code Fail at Completing Code with Potential Bugs</h3>
<p>Tuan Dinh, Jinman Zhao, Samson Tan, Renato Negrinho, Leonard Lausen, Sheng Zha, George Karypis</p>
<p><a href='https://openreview.net/forum?id=e1WgjvFGWp'>https://openreview.net/forum?id=e1WgjvFGWp</a></p>
<p><b>Keywords</b>: language model of code; code completion; language model; software engineering; machine learning for code
</p><p><b>Compressor summary</b>: The paragraph discusses code completion with buggy code contexts and introduces two datasets to study the problem, showing that current Code-LLMs struggle with this scenario and there is room for improvement in mitigating bugs.</p><hr><h3>Aligning Gradient and Hessian for Neural Signed Distance Function</h3>
<p>Ruian Wang, Zixiong Wang, Yunxiao Zhang, Shuangmin Chen, Shiqing Xin, Changhe Tu, Wenping Wang</p>
<p><a href='https://openreview.net/forum?id=e0tt2G8hqf'>https://openreview.net/forum?id=e0tt2G8hqf</a></p>
<p><b>Keywords</b>: implicit neural representation, signed distance function, shape operator
</p><p><b>Compressor summary</b>: The paper introduces a method to learn the Signed Distance Function (SDF) from point clouds without normals, which improves gradient accuracy and reduces ghost geometry in surface reconstruction.</p><hr><h3>Private (Stochastic) Non-Convex Optimization Revisited: Second-Order Stationary Points and Excess Risks</h3>
<p>Daogao Liu, Arun Ganesh, Sewoong Oh, Abhradeep Guha Thakurta</p>
<p><a href='https://openreview.net/forum?id=e0pRF9tOtm'>https://openreview.net/forum?id=e0pRF9tOtm</a></p>
<p><b>Keywords</b>: Differential Privacy, Non-convex optimization, Stationary points, Exponential Mechanism
</p><p><b>Compressor summary</b>: The paper proposes a novel framework for non-convex optimization under differential privacy constraint that uses two types of gradient oracles to improve accuracy and identifies second-order stationary points.</p><hr><h3>Hypernetwork-based Meta-Learning for Low-Rank Physics-Informed Neural Networks</h3>
<p>Woojin Cho, Kookjin Lee, Donsub Rim, Noseong Park</p>
<p><a href='https://openreview.net/forum?id=dzqKAM2sKa'>https://openreview.net/forum?id=dzqKAM2sKa</a></p>
<p><b>Keywords</b>: Scientific machine learning, Physics-informed neural networks, Meta learning, Hypernetworks
</p><p><b>Compressor summary</b>: The study suggests using lightweight low-rank physics-informed neural networks with meta-learning to solve partial differential equations for various input parameters quickly and effectively.</p><hr><h3>Characterizing Out-of-Distribution Error via Optimal Transport</h3>
<p>Yuzhe Lu, Yilong Qin, Runtian Zhai, Andrew Shen, Ketong Chen, Zhenlin Wang, Soheil Kolouri, Simon Stepputtis, Joseph Campbell, Katia P. Sycara</p>
<p><a href='https://openreview.net/forum?id=dz5X8hnfJc'>https://openreview.net/forum?id=dz5X8hnfJc</a></p>
<p><b>Keywords</b>: Distribution Shift, OOD Error Prediction, Optimal Transport, Deep Learning
</p><p><b>Compressor summary</b>: The paper proposes a new method for estimating model performance on out-of-distribution data using optimal transport theory and an empirically-motivated variant that improves accuracy and robustness.</p><hr><h3>GMSF: Global Matching Scene Flow</h3>
<p>Yushan Zhang, Johan Edstedt, Bastian Wandt, Per-Erik Forssen, Maria Magnusson, Michael Felsberg</p>
<p><a href='https://openreview.net/forum?id=dybrsuNAB9'>https://openreview.net/forum?id=dybrsuNAB9</a></p>
<p><b>Keywords</b>: Scene flow, point clouds, transformers
</p><p><b>Compressor summary</b>: The paper proposes a simple one-shot global matching method for scene flow estimation from point clouds, which achieves state-of-the-art results on various benchmarks and outperforms previous methods.</p><hr><h3>Compositional Foundation Models for Hierarchical Planning</h3>
<p>Anurag Ajay, Seungwook Han, Yilun Du, Shuang Li, Abhi Gupta, Tommi S. Jaakkola, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Akash Srivastava, Pulkit Agrawal</p>
<p><a href='https://openreview.net/forum?id=dyXNh5HLq3'>https://openreview.net/forum?id=dyXNh5HLq3</a></p>
<p><b>Keywords</b>: Foundation Models, Composition, Hierarchical Planning
</p><p><b>Compressor summary</b>: The HiP model combines language, vision, and action models to create symbolic plans for long-horizon tasks involving table-top manipulation.</p><hr><h3>Equivariant Single View Pose Prediction Via Induced and Restriction Representations</h3>
<p>Owen Lewis Howell, David Klee, Ondrej Biza, Linfeng Zhao, Robin Walters</p>
<p><a href='https://openreview.net/forum?id=dxVN2fZjx6'>https://openreview.net/forum?id=dxVN2fZjx6</a></p>
<p><b>Keywords</b>: Equivarient Machine Learning, Pose Prediction, Computer Vision
</p><p><b>Compressor summary</b>: The text discusses how to design neural networks for 3D object recognition from 2D images, considering constraints and consistency properties, and achieving state-of-the-art results on some tasks.</p><hr><h3>Few-shot Generation via Recalling  Brain-Inspired Episodic-Semantic Memory</h3>
<p>Zhibin Duan, Lv Zhiyi, Chaojie Wang, Bo Chen, Bo An, Mingyuan Zhou</p>
<p><a href='https://openreview.net/forum?id=dxPcdEeQk9'>https://openreview.net/forum?id=dxPcdEeQk9</a></p>
<p><b>Keywords</b>: Generative Model, Memory-augmented Generative Model
</p><p><b>Compressor summary</b>: The authors propose a variational structured memory module (VSM) to enhance existing generative models for few-shot generation by mimicking human memory mechanisms.</p><hr><h3>Deep Patch Visual Odometry</h3>
<p>Zachary Teed, Lahav Lipson, Jia Deng</p>
<p><a href='https://openreview.net/forum?id=dwfHbm8g66'>https://openreview.net/forum?id=dwfHbm8g66</a></p>
<p><b>Keywords</b>: SLAM, Simultaneous Localization and Mapping, Visual Odometry, Structure from motion, SfM
</p><p><b>Compressor summary</b>: Deep Patch Visual Odometry (DPVO) is a new system that uses sparse patch-based matching instead of dense flow for monocular visual odometry, achieving better accuracy and efficiency than previous methods.</p><hr><h3>SmoothHess: ReLU Network Feature Interactions via Stein's Lemma</h3>
<p>Max Torop, Aria Masoomi, Davin Hill, Kivanc Kose, Stratis Ioannidis, Jennifer Dy</p>
<p><a href='https://openreview.net/forum?id=dwIeEhbaD0'>https://openreview.net/forum?id=dwIeEhbaD0</a></p>
<p><b>Keywords</b>: Interpretability, Feature Interactions, Stein's Lemma
</p><p><b>Compressor summary</b>: SmoothHess is a method for estimating feature interactions in ReLU neural networks by using Stein's Lemma and efficient sampling, without needing network modifications or changing the architecture.</p><hr><h3>Actively Testing Your Model While It Learns: Realizing Label-Efficient Learning in Practice</h3>
<p>Dayou Yu, Weishi Shi, Qi Yu</p>
<p><a href='https://openreview.net/forum?id=du0hvEpgj8'>https://openreview.net/forum?id=du0hvEpgj8</a></p>
<p><b>Keywords</b>: active learning, active testing
</p><p><b>Compressor summary</b>: The paper proposes a novel active testing while learning (ATL) framework that integrates active learning with active testing, reducing data annotation costs and improving model performance using an "active feedback" mechanism.</p><hr><h3>Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation</h3>
<p>Shengpu Tang, Jenna Wiens</p>
<p><a href='https://openreview.net/forum?id=dsH244r9fA'>https://openreview.net/forum?id=dsH244r9fA</a></p>
<p><b>Keywords</b>: healthcare, reinforcement learning, offline RL, off-policy evaluation, counterfactuals
</p><p><b>Compressor summary</b>: The paper proposes a semi-offline evaluation framework for reinforcement learning that uses counterfactual trajectories annotated by humans to reduce bias and variance in off-policy evaluation.</p><hr><h3>The Memory-Perturbation Equation: Understanding Model's Sensitivity to Data</h3>
<p>Peter Nickl, Lu Xu, Dharmesh Tailor, Thomas Möllenhoff, Mohammad Emtiyaz Khan</p>
<p><a href='https://openreview.net/forum?id=dqS1GuoG2V'>https://openreview.net/forum?id=dqS1GuoG2V</a></p>
<p><b>Keywords</b>: model interpretability, model understanding, bayesian learning, robustness, adaptive learning
</p><p><b>Compressor summary</b>: The Memory-Perturbation Equation (MPE) simplifies the study of model sensitivity to training data perturbations, using Bayesian principles, and shows its usefulness in predicting generalization to unseen test data.</p><hr><h3>Large-Scale Distributed Learning via Private On-Device LSH</h3>
<p>Tahseen Rabbani, Marco Bornstein, Furong Huang</p>
<p><a href='https://openreview.net/forum?id=dpdbbN7AKr'>https://openreview.net/forum?id=dpdbbN7AKr</a></p>
<p><b>Keywords</b>: distributed learning, locality-sensitive hashing, recommender systems, compression
</p><p><b>Compressor summary</b>: The paper proposes a new family of hash functions for locality-sensitive hashing that enables privacy, personalization, and memory efficiency for pruning dense hidden layers in large-scale recommender networks.</p><hr><h3>Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</h3>
<p>Yingying Fan, Yu Wu, Bo Du, Yutian Lin</p>
<p><a href='https://openreview.net/forum?id=doWqIXcRlq'>https://openreview.net/forum?id=doWqIXcRlq</a></p>
<p><b>Keywords</b>: Weakly-Supervised Audio-Visual Video Parsing, Language Guided Segment-Level Label Denoising, Dynamic Re-weighting
</p><p><b>Compressor summary</b>: The paper proposes a language-based method for weakly-supervised audio-visual video parsing that tackles segment-level label noise and outperforms existing approaches.</p><hr><h3>Fully Dynamic $k$-Clustering in $\tilde O(k)$ Update Time</h3>
<p>Sayan Bhattacharya, Martin Costa, Silvio Lattanzi, Nikos Parotsidis</p>
<p><a href='https://openreview.net/forum?id=dnGEPkmnzO'>https://openreview.net/forum?id=dnGEPkmnzO</a></p>
<p><b>Keywords</b>: clustering, k-median, k-means, dynamic algorithms, amortized analysis
</p><p><b>Compressor summary</b>: The paper presents a fast and approximate dynamic algorithm for the k-median and k-means problems on metric spaces, compares it to previous methods, and provides a lower bound analysis.</p><hr><h3>The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions</h3>
<p>Jonathan Schmidt, Philipp Hennig, Jörg Nick, Filip Tronarp</p>
<p><a href='https://openreview.net/forum?id=dnB71DMyDD'>https://openreview.net/forum?id=dnB71DMyDD</a></p>
<p><b>Keywords</b>: Gaussian, filtering, smoothing, bayesian, state-space models, dynamic-low-rank, high-dimensional, spatio-temporal, Gaussian processes, regression, low rank, state estimation
</p><p><b>Compressor summary</b>: The paper proposes a new low-rank Gaussian filtering and smoothing method that is faster and more accurate than existing ensemble-based methods for high-dimensional dynamical systems.</p><hr><h3>Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery</h3>
<p>Mateusz Olko, Michał Zając, Aleksandra Nowak, Nino Scherrer, Yashas Annadani, Stefan Bauer, Łukasz Kuciński, Piotr Miłoś</p>
<p><a href='https://openreview.net/forum?id=dmD63sv0TZ'>https://openreview.net/forum?id=dmD63sv0TZ</a></p>
<p><b>Keywords</b>: causal discovery, experimental design, active learning, neural networks
</p><p><b>Compressor summary</b>: The paper proposes a method called GIT that uses gradient estimators to minimize the number of required experiments for acquiring interventional data and inferring causal structures from data.</p><hr><h3>Sample Complexity of Forecast Aggregation</h3>
<p>Tao Lin, Yiling Chen</p>
<p><a href='https://openreview.net/forum?id=dlDFakG6kJ'>https://openreview.net/forum?id=dlDFakG6kJ</a></p>
<p><b>Keywords</b>: information aggregation, sample complexity, distribution learning, Bayesian forecast aggregation
</p><p><b>Compressor summary</b>: A Bayesian model for forecast aggregation requires exponentially more samples when experts' signals depend on each other, but only linearly more samples if their signals are independent given the event outcome.</p><hr><h3>Scalable Transformer for PDE Surrogate Modeling</h3>
<p>Zijie Li, Dule Shu, Amir Barati Farimani</p>
<p><a href='https://openreview.net/forum?id=djyn8Q0anK'>https://openreview.net/forum?id=djyn8Q0anK</a></p>
<p><b>Keywords</b>: Efficient attention, Neural PDE solver
</p><p><b>Compressor summary</b>: FactFormer is a new model that uses an axial factorized kernel integral to improve the efficiency and stability of applying Transformer models to problems with many grid points, such as simulating 2D and 3D fluid dynamics.</p><hr><h3>Improving Adversarial Transferability via Intermediate-level Perturbation Decay</h3>
<p>Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen</p>
<p><a href='https://openreview.net/forum?id=dikH9tdPi2'>https://openreview.net/forum?id=dikH9tdPi2</a></p>
<p><b>Keywords</b>: adversarial examples, black-box attack, adversarial transferability
</p><p><b>Compressor summary</b>: The ILPD method improves intermediate-level adversarial attacks by crafting effective and strong perturbations in one stage of optimization, leading to better performance against various victim models on ImageNet and CIFAR-10.</p><hr><h3>Fast Exact Leverage Score Sampling from Khatri-Rao Products with Applications to Tensor Decomposition</h3>
<p>Vivek Bharadwaj, Osman Asif Malik, Riley Murray, Laura Grigori, Aydin Buluc, James Demmel</p>
<p><a href='https://openreview.net/forum?id=deaHiTb6Cu'>https://openreview.net/forum?id=deaHiTb6Cu</a></p>
<p><b>Keywords</b>: Tensor Decomposition, Leverage Scores, Randomized Linear Algebra, Sketching, Khatri-Rao Product, Sparse Tensors
</p><p><b>Compressor summary</b>: The paper introduces a fast sampler for the Khatri-Rao product that can handle large matrices and outperforms existing methods in linear least-squares problems.</p><hr><h3>Functional Equivalence and Path Connectivity of Reducible Hyperbolic Tangent Networks</h3>
<p>Matthew Farrugia-Roberts</p>
<p><a href='https://openreview.net/forum?id=ddKCg3OhGw'>https://openreview.net/forum?id=ddKCg3OhGw</a></p>
<p><b>Keywords</b>: theory, neural network theory, structural redundancy, functional equivalence, functional equivalence class, partial identifiability, parameter canonicalisation, parameter space, piecewise-linear, connectivity
</p><p><b>Compressor summary</b>: The paper characterizes unit redundancies and reducible functional equivalence classes in a neural network architecture using an algorithm and shows that these classes are path-connected sets with a small maximum diameter.</p><hr><h3>Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection</h3>
<p>Eli Chien, Wei-Ning Chen, Chao Pan, Pan Li, Ayfer Ozgur, Olgica Milenkovic</p>
<p><a href='https://openreview.net/forum?id=dd3KNayGFz'>https://openreview.net/forum?id=dd3KNayGFz</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Differential Privacy, Multigranular Topology Protection
</p><p><b>Compressor summary</b>: The authors propose a new framework called Graph Differential Privacy (GDP) for graph learning that ensures provably private model parameters and predictions, and compare it to existing Differentially Private Decoupled Graph Convolutions (DPDGCs).</p><hr><h3>Deep Gaussian Markov Random Fields for Graph-Structured Dynamical Systems</h3>
<p>Fiona Lippert, Bart Kranstauber, E. Emiel van Loon, Patrick Forré</p>
<p><a href='https://openreview.net/forum?id=dcw7qRUuD8'>https://openreview.net/forum?id=dcw7qRUuD8</a></p>
<p><b>Keywords</b>: probabilistic inference, graphical models, spatiotemporal dynamical systems, state-space models
</p><p><b>Compressor summary</b>: The paper proposes a computationally efficient method for estimating and learning state variables in graph-structured models with unknown dynamics and limited data by combining deep learning and Gaussian Markov random fields.</p><hr><h3>Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning</h3>
<p>Berken Utku Demirel, Christian Holz</p>
<p><a href='https://openreview.net/forum?id=dbVRDk2wt7'>https://openreview.net/forum?id=dbVRDk2wt7</a></p>
<p><b>Keywords</b>: Contrastive learning, Time-series, Augmentation
</p><p><b>Compressor summary</b>: The paper proposes a new data augmentation method for time-series tasks that connects samples to find order in the latent space and improves performance on three tasks, including heart rate estimation, human activity recognition, and cardiovascular disease detection.</p><hr><h3>Epistemic Neural Networks</h3>
<p>Ian Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Morteza Ibrahimi, Xiuyuan Lu, Benjamin Van Roy</p>
<p><a href='https://openreview.net/forum?id=dZqcC1qCmB'>https://openreview.net/forum?id=dZqcC1qCmB</a></p>
<p><b>Keywords</b>: Uncertainty, Deep Learning, Neural Networks
</p><p><b>Compressor summary</b>: The epinet is an architecture that improves the uncertainty estimation of conventional neural networks by supplementing them with modest computation, enabling them to outperform large ensembles.</p><hr><h3>Causal Discovery in Semi-Stationary Time Series</h3>
<p>Shanyun Gao, Raghavendra Addanki, Tong Yu, Ryan A. Rossi, Murat Kocaoglu</p>
<p><a href='https://openreview.net/forum?id=dYeUvLUxBQ'>https://openreview.net/forum?id=dYeUvLUxBQ</a></p>
<p><b>Keywords</b>: time-series causal discovery, constraint-based causal discovery
</p><p><b>Compressor summary</b>: The paper proposes a method for discovering causal relations from non-stationary time series using a structural causal model that accounts for periodic changes, and tests it on simulated and real-world data.</p><hr><h3>Self-Correcting Bayesian Optimization through Bayesian Active Learning</h3>
<p>Carl Hvarfner, Erik Orm Hellsten, Frank Hutter, Luigi Nardi</p>
<p><a href='https://openreview.net/forum?id=dX9MjUtP1A'>https://openreview.net/forum?id=dX9MjUtP1A</a></p>
<p><b>Keywords</b>: Bayesian Optimization, Bayesian Active Learning, Gaussian Processes
</p><p><b>Compressor summary</b>: The authors propose two methods to improve Gaussian processes for Bayesian optimization and active learning by better learning hyperparameters and incorporating self-correction.</p><hr><h3>Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models</h3>
<p>Yubin Shi, Yixuan Chen, Mingzhi Dong, Xiaochen Yang, Dongsheng Li, Yujiang Wang, Robert P. Dick, Qin Lv, Yingying Zhao, Fan Yang, Tun Lu, Ning Gu, Li Shang</p>
<p><a href='https://openreview.net/forum?id=dWDEBW2raJ'>https://openreview.net/forum?id=dWDEBW2raJ</a></p>
<p><b>Keywords</b>: Modular Adaptive Training, Efficient Training, Over-parameterized Model, Neural Tangent Kernel.
</p><p><b>Compressor summary</b>: This paper introduces modular neural tangent kernel (mNTK) to study learning dynamics of over-parameterized models, proposes Modular Adaptive Training (MAT) to improve efficiency and performance by selectively updating modules based on their mNTK eigenvalues, and shows that MAT reduces computational costs and outperforms baselines.</p><hr><h3>Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language</h3>
<p>Kevin Ellis</p>
<p><a href='https://openreview.net/forum?id=dVnhdm9MIg'>https://openreview.net/forum?id=dVnhdm9MIg</a></p>
<p><b>Keywords</b>: Cognitive science, Bayesian, Language model, Induction, Psychology, Reasoning
</p><p><b>Compressor summary</b>: The model uses natural language hypotheses and Bayesian reasoning to learn a broad range of human-like concepts efficiently.</p><hr><h3>RECKONING: Reasoning through Dynamic Knowledge Encoding</h3>
<p>Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyilmaz, Antoine Bosselut</p>
<p><a href='https://openreview.net/forum?id=dUAcAtCuKk'>https://openreview.net/forum?id=dUAcAtCuKk</a></p>
<p><b>Keywords</b>: natural language processing, multi-hop reasoning, knowledge memorisation
</p><p><b>Compressor summary</b>: RECKONING teaches language models to reason more robustly by encoding contextual knowledge into their parameters before answering questions, improving performance and generalization.</p><hr><h3>Does a sparse ReLU network training problem always admit an optimum ?</h3>
<p>TUNG QUOC LE, Rémi Gribonval, Elisa Riccietti</p>
<p><a href='https://openreview.net/forum?id=dTj5tH94xv'>https://openreview.net/forum?id=dTj5tH94xv</a></p>
<p><b>Keywords</b>: Topology, best approximation property, closedness, function space, sparse neural networks
</p><p><b>Compressor summary</b>: The paper investigates the existence of an optimal solution for sparse ReLU neural networks and shows that it is not always guaranteed, while providing conditions for its existence.</p><hr><h3>Slow and Weak Attractor Computation Embedded in Fast and Strong E-I Balanced Neural Dynamics</h3>
<p>Xiaohan Lin, Liyuan Li, Boxin Shi, Tiejun Huang, Yuanyuan Mi, Si Wu</p>
<p><a href='https://openreview.net/forum?id=dSRyKIYRnP'>https://openreview.net/forum?id=dSRyKIYRnP</a></p>
<p><b>Keywords</b>: Continuous attractor neural network; Excitation inhibition balance; Brain-inspired algorithms; Object tracking;
</p><p><b>Compressor summary</b>: The study explores how a neural circuit can have both attractor states and irregular firings by using two sets of synapses with different strengths and speeds, leading to improved performance in a tracking problem.</p><hr><h3>NeuralGF: Unsupervised Point Normal Estimation by Learning Neural Gradient Function</h3>
<p>Qing Li, Huifang Feng, Kanle Shi, Yue Gao, Yi Fang, Yu-Shen Liu, Zhizhong Han</p>
<p><a href='https://openreview.net/forum?id=dR6p49RYLq'>https://openreview.net/forum?id=dR6p49RYLq</a></p>
<p><b>Keywords</b>: Point Clouds, Normal Estimation, Neural Gradient
</p><p><b>Compressor summary</b>: The authors propose a new neural network approach to estimate oriented normals from point clouds without using ground truth normals, achieving robust and accurate results on benchmarks.</p><hr><h3>Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms</h3>
<p>Akifumi Wachi, Wataru Hashimoto, Xun Shen, Kazumune Hashimoto</p>
<p><a href='https://openreview.net/forum?id=dQLsvKNwZC'>https://openreview.net/forum?id=dQLsvKNwZC</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Safety Exploration
</p><p><b>Compressor summary</b>: MASE is a meta-algorithm for safe exploration in reinforcement learning that combines an unconstrained RL algorithm with an uncertainty quantifier to guarantee safety while penalizing unsafe actions.</p><hr><h3>Locally Invariant Explanations: Towards Stable and Unidirectional Explanations through Local Invariant Learning</h3>
<p>Amit Dhurandhar, Karthikeyan Natesan Ramamurthy, Kartik Ahuja, Vijay Arya</p>
<p><a href='https://openreview.net/forum?id=dOxm4FnMFu'>https://openreview.net/forum?id=dOxm4FnMFu</a></p>
<p><b>Keywords</b>: Explainable AI, Game theory, Invariance
</p><p><b>Compressor summary</b>: The paper proposes a novel method for explaining black-box models using game theory and invariant risk minimization, which can produce high fidelity, stable, and unidirectional explanations with simple and efficient training.</p><hr><h3>From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion</h3>
<p>Robin San Roman, Yossi Adi, Antoine Deleforge, Romain Serizel, Gabriel Synnaeve, Alexandre Défossez</p>
<p><a href='https://openreview.net/forum?id=dOanKg3jKS'>https://openreview.net/forum?id=dOanKg3jKS</a></p>
<p><b>Keywords</b>: diffusion, audio, compression
</p><p><b>Compressor summary</b>: The proposed high-fidelity multi-band diffusion model generates any audio type from low-bitrate discrete representations, outperforming other methods in perceptual quality at equal bit rate.</p><hr><h3>Constrained Policy Optimization with Explicit Behavior Density For Offline Reinforcement Learning</h3>
<p>Jing Zhang, Chi Zhang, Wenjia Wang, Bingyi Jing</p>
<p><a href='https://openreview.net/forum?id=dLmDPVv19z'>https://openreview.net/forum?id=dLmDPVv19z</a></p>
<p><b>Keywords</b>: Offline Reinforcement Learning, GAN, Flow Model, Policy Control
</p><p><b>Compressor summary</b>: CPED is a new method for offline RL that uses a flow-GAN model to estimate behavior policy density, allowing it to safely explore and achieve better performance than existing methods.</p><hr><h3>Double and Single Descent in Causal Inference with an Application to High-Dimensional Synthetic Control</h3>
<p>Jann Spiess, Guido Imbens, Amar Venugopal</p>
<p><a href='https://openreview.net/forum?id=dL0GM9Wwtq'>https://openreview.net/forum?id=dL0GM9Wwtq</a></p>
<p><b>Keywords</b>: Double descent, interpolating regression, synthetic control, causal inference
</p><p><b>Compressor summary</b>: The authors study how over-parameterized models in causal inference, such as high-dimensional linear regression and synthetic control with many control units, can perform better than simpler ones, and provide a unified theoretical perspective on their improvement.</p><hr><h3>SwiFT: Swin 4D fMRI Transformer</h3>
<p>Peter Yongho Kim, Junbeom Kwon, Sunghwan Joo, Sangyoon Bae, Donggyu Lee, Yoonho Jung, Shinjae Yoo, Jiook Cha, Taesup Moon</p>
<p><a href='https://openreview.net/forum?id=dKeWh6EzBB'>https://openreview.net/forum?id=dKeWh6EzBB</a></p>
<p><b>Keywords</b>: fMRI, Swin Transformer, 4D, neuroscience
</p><p><b>Compressor summary</b>: SwiFT is a Swin Transformer architecture that learns brain dynamics directly from fMRI volumes using a 4D window multi-head self-attention mechanism and absolute positional embeddings, achieving better performance than existing models on predicting sex, age, and cognitive intelligence.</p><hr><h3>Keypoint-Augmented Self-Supervised Learning for Medical Image Segmentation with Limited Annotation</h3>
<p>Zhangsihao Yang, Mengwei Ren, Kaize Ding, Guido Gerig, Yalin Wang</p>
<p><a href='https://openreview.net/forum?id=dK0Ew3kkVf'>https://openreview.net/forum?id=dK0Ew3kkVf</a></p>
<p><b>Keywords</b>: Keypoints, Medical image, self-supervised learning, transformer, segmentation
</p><p><b>Compressor summary</b>: Keypoint-augmented fusion layer improves medical image segmentation by enhancing CNN features with long-range spatial self-attention, and incorporates global and local self-supervised pretraining for better representations.</p><hr><h3>Causal-structure Driven Augmentations for Text OOD Generalization</h3>
<p>Amir Feder, Yoav Wald, Claudia Shi, Suchi Saria, David Blei</p>
<p><a href='https://openreview.net/forum?id=dJZ3MvDw86'>https://openreview.net/forum?id=dJZ3MvDw86</a></p>
<p><b>Keywords</b>: Counterfactually Augmented Data, Invariant Learning, Out-of-distribution Generalization, Clinical NLP
</p><p><b>Compressor summary</b>: The authors propose using counterfactual data augmentation to improve the robustness of text classifiers in cases where the label is spuriously correlated with an attribute, and show improved OOD accuracy on medical diagnosis tasks.</p><hr><h3>On the Convergence of Black-Box Variational Inference</h3>
<p>Kyurae Kim, Jisu Oh, Kaiwen Wu, Yian Ma, Jacob R. Gardner</p>
<p><a href='https://openreview.net/forum?id=dHQ2av9NzO'>https://openreview.net/forum?id=dHQ2av9NzO</a></p>
<p><b>Keywords</b>: black-box variational inference, stochastic gradient descent, Bayesian inference, variational inference, probabilistic machine learning, Bayesian machine learning, variational Bayes
</p><p><b>Compressor summary</b>: The paper shows that using a specific algorithm called proximal stochastic gradient descent can improve the performance of black-box variational inference, a method for Bayesian inference, compared to other common approaches.</p><hr><h3>LMC: Large Model Collaboration with Cross-assessment for Training-Free Open-Set Object Recognition</h3>
<p>Haoxuan Qu, Xiaofei Hui, Yujun Cai, Jun Liu</p>
<p><a href='https://openreview.net/forum?id=dHF3Im8Aic'>https://openreview.net/forum?id=dHF3Im8Aic</a></p>
<p><b>Keywords</b>: Deep learning, Open-set object recognition, Large models, Training-free
</p><p><b>Compressor summary</b>: The paper proposes a method called Large Model Collaboration (LMC) that uses multiple pre-trained models to improve open-set object recognition without additional training and by extracting implicit knowledge from the models.</p><hr><h3>Cookie Consent Has Disparate Impact on Estimation Accuracy</h3>
<p>Erik Miehling, Rahul Nair, Elizabeth M. Daly, Karthikeyan Natesan Ramamurthy, Robert Nelson Redmond</p>
<p><a href='https://openreview.net/forum?id=dFtpRphNb3'>https://openreview.net/forum?id=dFtpRphNb3</a></p>
<p><b>Keywords</b>: fairness, cookies, recommender systems
</p><p><b>Compressor summary</b>: The text discusses how user consent on cookie sharing affects the accuracy of personalized ads and recommender systems, and suggests new notions of fairness to address privacy concerns.</p><hr><h3>CP-SLAM: Collaborative Neural Point-based SLAM System</h3>
<p>Jiarui Hu, Mao Mao, Hujun Bao, Guofeng Zhang, Zhaopeng Cui</p>
<p><a href='https://openreview.net/forum?id=dFSeZm6dTC'>https://openreview.net/forum?id=dFSeZm6dTC</a></p>
<p><b>Keywords</b>: Collaborative SLAM; Neural Point Field; Keyframe-based SLAM; Pose Graph Optimization
</p><p><b>Compressor summary</b>: The paper introduces a collaborative implicit neural SLAM system using RGB-D image sequences, with a novel 3D scene representation and learning strategy that improves accuracy and cooperation.</p><hr><h3>Separable Physics-Informed Neural Networks</h3>
<p>Junwoo Cho, Seungtae Nam, Hyunmo Yang, Seok-Bae Yun, Youngjoon Hong, Eunbyung Park</p>
<p><a href='https://openreview.net/forum?id=dEySGIcDnI'>https://openreview.net/forum?id=dEySGIcDnI</a></p>
<p><b>Keywords</b>: partial differential equations, scientific machine learning, physics-informed neural networks, fluid dynamics
</p><p><b>Compressor summary</b>: The proposed method, SPINN, improves the performance of PINNs by reducing computational costs and increasing accuracy in solving multi-dimensional PDEs using a per-axis basis network architecture and forward-mode automatic differentiation.</p><hr><h3>Fed-CO$_{2}$: Cooperation of Online and Offline Models for Severe Data Heterogeneity in Federated Learning</h3>
<p>Zhongyi Cai, Ye Shi, Wei Huang, Jingya Wang</p>
<p><a href='https://openreview.net/forum?id=dEDdRWunxU'>https://openreview.net/forum?id=dEDdRWunxU</a></p>
<p><b>Keywords</b>: Federated Learning, Data Heterogeneity, Model Cooperation, Mutual Learning, Knowledge Transfer
</p><p><b>Compressor summary</b>: Fed-CO$_2$ is a framework for distributed learning that handles data heterogeneity issues like label and feature skew using cooperation between online and offline models, improving performance over existing algorithms.</p><hr><h3>Online Inventory Problems: Beyond the i.i.d. Setting with Online Convex Optimization</h3>
<p>Massil HIHAT, Stéphane Gaïffas, Guillaume Garrigos, Simon Bussy</p>
<p><a href='https://openreview.net/forum?id=dDk6URGRXP'>https://openreview.net/forum?id=dDk6URGRXP</a></p>
<p><b>Keywords</b>: online convex optimization, inventory control, newsvendor, online learning, regret analysis
</p><p><b>Compressor summary</b>: The paper presents an online algorithm, MaxCOSD, for inventory control problems with general demands, losses, and dynamics, and requires non-degeneracy assumptions on the demand process for learning.</p><hr><h3>Supervised Pretraining Can Learn In-Context Reinforcement Learning</h3>
<p>Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, Emma Brunskill</p>
<p><a href='https://openreview.net/forum?id=dCYBAGQXLo'>https://openreview.net/forum?id=dCYBAGQXLo</a></p>
<p><b>Keywords</b>: decision making, reinforcement learning, in-context learning, bandits, transformers, offline reinforcement learning, exploration, reinforcement learning theory
</p><p><b>Compressor summary</b>: The paper introduces Decision-Pretrained Transformer (DPT), a method for supervised pretraining of transformers to solve decision-making problems using in-context learning, and demonstrates its surprising capabilities in solving RL problems, generalizing to new tasks, and adapting strategies.</p><hr><h3>This Looks Like Those: Illuminating Prototypical Concepts Using Multiple Visualizations</h3>
<p>Chiyu Ma, Brandon Zhao, Chaofan Chen, Cynthia Rudin</p>
<p><a href='https://openreview.net/forum?id=dCAk9VlegR'>https://openreview.net/forum?id=dCAk9VlegR</a></p>
<p><b>Keywords</b>: deep learning, interpretability, prototype-based neural network, case-based reasoning
</p><p><b>Compressor summary</b>: ProtoConcepts is a method for interpretable image classification using multiple image patches to represent prototypical concepts, improving visual explanations and understanding.</p><hr><h3>SmooSeg: Smoothness Prior for Unsupervised Semantic Segmentation</h3>
<p>Mengcheng Lan, Xinjiang Wang, Yiping Ke, Jiaxing Xu, Litong Feng, Wayne Zhang</p>
<p><a href='https://openreview.net/forum?id=dB4lvScPIj'>https://openreview.net/forum?id=dB4lvScPIj</a></p>
<p><b>Keywords</b>: unsupervised semantic segmentation; self-supervised learning; smoothness prior
</p><p><b>Compressor summary</b>: The paper proposes a new method called SmooSeg for unsupervised semantic segmentation that uses smoothness prior to simplify the task, improving performance on three datasets.</p><hr><h3>Contrastive Sampling Chains in Diffusion Models</h3>
<p>Junyu Zhang, Daochang Liu, Shichao Zhang, Chang Xu</p>
<p><a href='https://openreview.net/forum?id=dAbGv5Jz5U'>https://openreview.net/forum?id=dAbGv5Jz5U</a></p>
<p><b>Keywords</b>: diffusion models, contrastive loss, discretization error, contrastive sampling chain
</p><p><b>Compressor summary</b>: The text explains a method that combines contrastive loss and score matching to reduce discretization error in diffusion models, improving image quality and sample efficiency.</p><hr><h3>A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs</h3>
<p>Zhaocheng Zhu, Xinyu Yuan, Mikhail Galkin, Sophie Xhonneux, Ming Zhang, Maxime Gazeau, Jian Tang</p>
<p><a href='https://openreview.net/forum?id=dAJrxQz1lk'>https://openreview.net/forum?id=dAJrxQz1lk</a></p>
<p><b>Keywords</b>: Knowledge Graph Reasoning, Path-based Methods, Scalability, A* Algorithm
</p><p><b>Compressor summary</b>: A\*Net is a scalable path-based method for knowledge graph reasoning that learns a priority function to select important nodes and edges, achieving competitive performance with less computation.</p><hr><h3>ZipLM: Inference-Aware Structured Pruning of Language Models</h3>
<p>Eldar Kurtic, Elias Frantar, Dan Alistarh</p>
<p><a href='https://openreview.net/forum?id=d8j3lsBWpV'>https://openreview.net/forum?id=d8j3lsBWpV</a></p>
<p><b>Keywords</b>: LLMs, pruning, compression, inference
</p><p><b>Compressor summary</b>: ZipLM is a novel compression method for large language models that achieves high accuracy and speedup, outperforming prior techniques in various settings.</p><hr><h3>3D Copy-Paste: Physically Plausible Object Insertion for Monocular 3D Detection</h3>
<p>Yunhao Ge, Hong-Xing Yu, Cheng Zhao, Yuliang Guo, Xinyu Huang, Liu Ren, Laurent Itti, Jiajun Wu</p>
<p><a href='https://openreview.net/forum?id=d86B6Mdweq'>https://openreview.net/forum?id=d86B6Mdweq</a></p>
<p><b>Keywords</b>: 3D Copy-Paste, Object Insertion, Monocular 3D Object Detection
</p><p><b>Compressor summary</b>: The paper presents a method to insert virtual objects into real scenes for monocular 3D object detection, ensuring physical plausibility in locations, appearances, and shadows.</p><hr><h3>Meta-AdaM: An Meta-Learned Adaptive Optimizer with Momentum for Few-Shot Learning</h3>
<p>Siyuan Sun, Hongyang Gao</p>
<p><a href='https://openreview.net/forum?id=d85pPNBHLt'>https://openreview.net/forum?id=d85pPNBHLt</a></p>
<p><b>Keywords</b>: Few shot learning, Meta Learning
</p><p><b>Compressor summary</b>: Meta-AdaM is a meta-learned adaptive optimizer that uses weight-update history and momentum for faster few-shot learning, outperforming existing methods on benchmark datasets.</p><hr><h3>How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization</h3>
<p>Hai Zhang, Hang Yu, Junqiao Zhao, Di Zhang, Chang Huang, Hongtu Zhou, Xiao Zhang, Chen Ye</p>
<p><a href='https://openreview.net/forum?id=d7a5TpePV7'>https://openreview.net/forum?id=d7a5TpePV7</a></p>
<p><b>Keywords</b>: model-based reinforcement learning, model shift, model bias, fine-tuning, performance difference bound
</p><p><b>Compressor summary</b>: The paper proposes USB-PO, an algorithm that adapts model updates to improve MBRL performance while accounting for model shift and bias, achieving state-of-the-art results on multiple benchmark tasks.</p><hr><h3>Unsupervised Image Denoising with Score Function</h3>
<p>Yutong Xie, Mingze Yuan, Bin Dong, Quanzheng Li</p>
<p><a href='https://openreview.net/forum?id=d6LShzSTOP'>https://openreview.net/forum?id=d6LShzSTOP</a></p>
<p><b>Keywords</b>: unsupervised learning, image denoising, score function
</p><p><b>Compressor summary</b>: The paper introduces a new unsupervised learning approach for single image denoising that can handle various noise models and outperforms current methods in complex cases.</p><hr><h3>Structural Pruning for Diffusion Models</h3>
<p>Gongfan Fang, Xinyin Ma, Xinchao Wang</p>
<p><a href='https://openreview.net/forum?id=d4f40zJJIS'>https://openreview.net/forum?id=d4f40zJJIS</a></p>
<p><b>Keywords</b>: Diffusion Model, Network Pruning, Model Compression, Efficient Deep Learning
</p><p><b>Compressor summary</b>: Diff-Pruning is an efficient compression method that reduces FLOPs and training time for diffusion models, preserving their generative behavior.</p><hr><h3>Towards Test-Time Refusals via Concept Negation</h3>
<p>Peiran Dong, Song Guo, Junxiao Wang, Bingjie WANG, Jiewei Zhang, Ziming Liu</p>
<p><a href='https://openreview.net/forum?id=d4X0QWS2Ln'>https://openreview.net/forum?id=d4X0QWS2Ln</a></p>
<p><b>Keywords</b>: Diffusion models, test-time refusal, concept negation, safety in generative models
</p><p><b>Compressor summary</b>: The paper introduces ProtoRe, a framework to improve concept negation for generating refusals in image synthesis by using CLIP's knowledge to identify negative concepts and purify outputs.</p><hr><h3>On the Gini-impurity Preservation For Privacy Random Forests</h3>
<p>XinRan Xie, Man-Jie Yuan, Xuetong Bai, Wei Gao, Zhi-Hua Zhou</p>
<p><a href='https://openreview.net/forum?id=d47iuwOt3j'>https://openreview.net/forum?id=d47iuwOt3j</a></p>
<p><b>Keywords</b>: classification, random forests, privacy-preserving machine learng, data encrytion
</p><p><b>Compressor summary</b>: This paper proposes a new encryption method that preserves the Gini impurity of data for random forests, using modified binary search trees and homomorphic encryption.</p><hr><h3>Unbalanced Low-rank Optimal Transport Solvers</h3>
<p>Meyer Scetbon, Michal Klein, Giovanni Palla, marco cuturi</p>
<p><a href='https://openreview.net/forum?id=d2WsCmoITF'>https://openreview.net/forum?id=d2WsCmoITF</a></p>
<p><b>Keywords</b>: Optimal Transport, Unbalanced
</p><p><b>Compressor summary</b>: The paper proposes new methods to improve optimal transport in machine learning by addressing its computational and modelling limitations, and applies them to solve real-world spatial transcriptomics problems.</p><hr><h3>Zero-Shot Anomaly Detection via Batch Normalization</h3>
<p>Aodong Li, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, Stephan Mandt</p>
<p><a href='https://openreview.net/forum?id=d1wjMBYbP1'>https://openreview.net/forum?id=d1wjMBYbP1</a></p>
<p><b>Keywords</b>: deep anomaly detection, zero-shot learning, batch normalization
</p><p><b>Compressor summary</b>: The paper proposes ACR, a method that helps deep anomaly detectors adapt to new data distributions without training data, achieving good zero-shot performance on tabular and image data.</p><hr><h3>Optimality of Message-Passing Architectures for Sparse Graphs</h3>
<p>Aseem Baranwal, Kimon Fountoulakis, Aukosh Jagannath</p>
<p><a href='https://openreview.net/forum?id=d1knqWjmNt'>https://openreview.net/forum?id=d1knqWjmNt</a></p>
<p><b>Keywords</b>: graph neural networks, message passing, bayesian inference, node classification, contextual stochastic block model
</p><p><b>Compressor summary</b>: The paper proposes an optimal node classification method for sparse feature-decorated graphs using a message-passing graph neural network and compares its performance with existing methods on a statistical model.</p><hr><h3>VRA: Variational Rectified Activation for Out-of-distribution Detection</h3>
<p>Mingyu Xu, Zheng Lian, Bin Liu, Jianhua Tao</p>
<p><a href='https://openreview.net/forum?id=d0VItRE2ZH'>https://openreview.net/forum?id=d0VItRE2ZH</a></p>
<p><b>Keywords</b>: Out-of-distribution Detection
</p><p><b>Compressor summary</b>: The paper introduces a new technique called Variational Rectified Activation (VRA) that improves out-of-distribution detection in machine learning models by simulating suppression and amplification operations using piecewise functions, and shows its effectiveness on multiple benchmark datasets.</p><hr><h3>On the Role of Randomization in Adversarially Robust Classification</h3>
<p>Lucas Gnecco Heredia, Muni Sreenivas Pydi, Laurent Meunier, benjamin negrevergne, Yann Chevaleyre</p>
<p><a href='https://openreview.net/forum?id=d0IEd3VgBh'>https://openreview.net/forum?id=d0IEd3VgBh</a></p>
<p><b>Keywords</b>: adversarial attacks, robustness, adversarial, attacks, deep learning, randomization, randomized ensembles
</p><p><b>Compressor summary</b>: The paper investigates how randomization affects adversarial robustness in classifiers and shows conditions for randomized ensembles to outperform deterministic ones, as well as examples of deterministic classifiers that outperform probabilistic ones.</p><hr><h3>Exploring Diverse In-Context Configurations for Image Captioning</h3>
<p>Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, Xin Geng</p>
<p><a href='https://openreview.net/forum?id=czwZnNf60r'>https://openreview.net/forum?id=czwZnNf60r</a></p>
<p><b>Keywords</b>: Image Caption; Few-shot Prompt; Vision Language Model;
</p><p><b>Compressor summary</b>: The authors explore different strategies for configuring image-text pairs in vision-language tasks, finding that they can significantly improve performance in image captioning compared to random sampling.</p><hr><h3>Efficient Neural Music Generation</h3>
<p>Max W. Y. Lam, Qiao Tian, Tang Li, Zongyu Yin, Siyuan Feng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo Ma, Xuchen Song, Jitong Chen, Yuping Wang, Yuxuan Wang</p>
<p><a href='https://openreview.net/forum?id=cxazQGSsQa'>https://openreview.net/forum?id=cxazQGSsQa</a></p>
<p><b>Keywords</b>: Music Generation, Language Model, Diffusion Model, MusicLM
</p><p><b>Compressor summary</b>: MeLoDy is a new music generation model that uses less computing power than MusicLM while maintaining high quality and adaptability.</p><hr><h3>Privacy Assessment on Reconstructed Images: Are Existing Evaluation Metrics Faithful to Human Perception?</h3>
<p>Xiaoxiao Sun, Nidham Gazagnadou, Vivek Sharma, Lingjuan Lyu, Hongdong Li, Liang Zheng</p>
<p><a href='https://openreview.net/forum?id=cx9a4Xvb3l'>https://openreview.net/forum?id=cx9a4Xvb3l</a></p>
<p><b>Keywords</b>: Privacy Assessment, Reconstructed Images, Evaluation Metrics, Human Perception
</p><p><b>Compressor summary</b>: The paper studies how well hand-crafted metrics reflect human judgement of model privacy leakage in reconstructed images and proposes a learning-based measure called SemSim to better evaluate semantic similarity between original and reconstructed images.</p><hr><h3>GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction</h3>
<p>Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan</p>
<p><a href='https://openreview.net/forum?id=cwjh8lqmOL'>https://openreview.net/forum?id=cwjh8lqmOL</a></p>
<p><b>Keywords</b>: multimodality, foundation models, tool usage
</p><p><b>Compressor summary</b>: The paper proposes GPT4Tools, a method to enable open-source LLMs to use multi-modal tools efficiently and effectively using instruction-following datasets and Low-Rank Adaptation optimization.</p><hr><h3>On the Learnability of Multilabel Ranking</h3>
<p>Vinod Raman, UNIQUE SUBEDI, Ambuj Tewari</p>
<p><a href='https://openreview.net/forum?id=cwBeRBe9hq'>https://openreview.net/forum?id=cwBeRBe9hq</a></p>
<p><b>Keywords</b>: Multilabel Ranking, PAC Learning, Online Learning
</p><p><b>Compressor summary</b>: This paper studies how easy it is to learn multilabel ranking problems with relevance-score feedback in different settings and introduces two classes of ranking losses based on learnability.</p><hr><h3>Demystifying Softmax Gating Function in Gaussian Mixture of Experts</h3>
<p>Huy Nguyen, TrungTin Nguyen, Nhat Ho</p>
<p><a href='https://openreview.net/forum?id=cto6jIIbMZ'>https://openreview.net/forum?id=cto6jIIbMZ</a></p>
<p><b>Keywords</b>: Mixture of Experts, Maximum Likelihood Estimation, Voronoi Loss Function, Algebraic Geometry.
</p><p><b>Compressor summary</b>: The text discusses challenges in estimating parameters for softmax gating Gaussian mixture of experts models and proposes new Voronoi loss functions to address them, connecting convergence rates to solvability problems of polynomial equations.</p><hr><h3>Counterfactual Generation with Identifiability Guarantees</h3>
<p>Hanqi Yan, Lingjing Kong, Lin Gui, Yuejie Chi, Eric Xing, Yulan He, Kun Zhang</p>
<p><a href='https://openreview.net/forum?id=cslnCXE9XA'>https://openreview.net/forum?id=cslnCXE9XA</a></p>
<p><b>Keywords</b>: Causal Representation Learning, Identifiability, Counterfactual Generation, Latent variable models, Disentanglement.
</p><p><b>Compressor summary</b>: The paper proposes a method to generate counterfactual images or texts by identifying content and style latent variables that vary across domains, using theoretical insights from relative sparsity of influences, and achieving state-of-the-art performance in unsupervised style transfer tasks.</p><hr><h3>Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations</h3>
<p>Minshuo Chen, Yu Bai, H. Vincent Poor, Mengdi Wang</p>
<p><a href='https://openreview.net/forum?id=csdEeUn0ve'>https://openreview.net/forum?id=csdEeUn0ve</a></p>
<p><b>Keywords</b>: Delayed and missing observations, MDPs, efficient regret bounds
</p><p><b>Compressor summary</b>: The paper studies how reinforcement learning can be efficient in control systems when agents have delayed or missing state observations and provides regret bounds and comparisons with full observability.</p><hr><h3>RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency</h3>
<p>Zhuoman Liu, Bo Yang, Yan Luximon, Ajay Kumar, Jinxi Li</p>
<p><a href='https://openreview.net/forum?id=crZlhMnfeO'>https://openreview.net/forum?id=crZlhMnfeO</a></p>
<p><b>Keywords</b>: implicit shape representations, multi-view consistency, novel view synthesis
</p><p><b>Compressor summary</b>: The paper introduces RayDF, a fast and accurate method for representing 3D shapes using ray-based neural functions that maintain multi-view geometry consistency, outperforming existing coordinate- and ray-based approaches.</p><hr><h3>No-regret Algorithms for Fair Resource Allocation</h3>
<p>Abhishek Sinha, Ativ Joshi, Rajarshi Bhattacharjee, Cameron N Musco, Mohammad Hajiesmaili</p>
<p><a href='https://openreview.net/forum?id=crNAh1EZKo'>https://openreview.net/forum?id=crNAh1EZKo</a></p>
<p><b>Keywords</b>: Online Learning, Bandit Algorithms, Learning Theory
</p><p><b>Compressor summary</b>: The paper proposes an efficient online resource allocation policy that achieves sublinear approximate regret in a fairness problem with a surprising phase transition phenomenon at the critical exponent $\alpha=\frac{1}{2}.$</p><hr><h3>Back-Modality: Leveraging Modal Transformation for Data Augmentation</h3>
<p>Zhi Li, Yifan Liu, Yin Zhang</p>
<p><a href='https://openreview.net/forum?id=cr99foBDPV'>https://openreview.net/forum?id=cr99foBDPV</a></p>
<p><b>Keywords</b>: data augmentation, cross-modal
</p><p><b>Compressor summary</b>: Back-Modality is a new way to improve data augmentation by transforming data between different modalities and then back again, making it easier to use various techniques for different types of data.</p><hr><h3>Group Fairness in Peer Review</h3>
<p>Haris Aziz, Evi Micha, Nisarg Shah</p>
<p><a href='https://openreview.net/forum?id=cpUuSV8kRw'>https://openreview.net/forum?id=cpUuSV8kRw</a></p>
<p><b>Keywords</b>: peer review; group fairness; core; stable
</p><p><b>Compressor summary</b>: The paper proposes a group fairness concept called the core for large AI conferences, which ensures that every community is treated fairly in the peer review process, and develops an efficient algorithm to find optimal assignments using real data from CVPR and ICLR.</p><hr><h3>Implicit Manifold Gaussian Process Regression</h3>
<p>Bernardo Fichera, Viacheslav Borovitskiy, Andreas Krause, Aude Billard</p>
<p><a href='https://openreview.net/forum?id=co4p15OMoc'>https://openreview.net/forum?id=co4p15OMoc</a></p>
<p><b>Keywords</b>: Gaussian process, manifolds, manifold learning, uncertainty, regression, graph Laplacian
</p><p><b>Compressor summary</b>: The paper proposes a new Gaussian process regression method that can infer implicit structure from data without explicit input of a manifold, potentially improving its performance in high-dimensional settings.</p><hr><h3>Context-PIPs: Persistent Independent Particles Demands Context Features</h3>
<p>Weikang BIAN, Zhaoyang Huang, Xiaoyu Shi, Yitong Dong, Yijin Li, Hongsheng Li</p>
<p><a href='https://openreview.net/forum?id=cnpkzQZaLU'>https://openreview.net/forum?id=cnpkzQZaLU</a></p>
<p><b>Keywords</b>: Optical Flow; Video Correspondence; Computer Vision;
</p><p><b>Compressor summary</b>: The paper introduces Context-PIPs, a framework that improves point trajectory accuracy in videos by using spatial context features from the source and target points.</p><hr><h3>Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space</h3>
<p>Saghar Adler, Vijay Subramanian</p>
<p><a href='https://openreview.net/forum?id=cm53OBkctM'>https://openreview.net/forum?id=cm53OBkctM</a></p>
<p><b>Keywords</b>: Thompson Sampling, Reinforcement Learning, Queueing theory
</p><p><b>Compressor summary</b>: The paper proposes a Bayesian algorithm using Thompson sampling for optimal control of countably-infinite state-space Markov Decision Processes, and shows its applicability in two queueing models.</p><hr><h3>Self-Supervised Visual Acoustic Matching</h3>
<p>Arjun Somayazulu, Changan Chen, Kristen Grauman</p>
<p><a href='https://openreview.net/forum?id=clKbFMt29V'>https://openreview.net/forum?id=clKbFMt29V</a></p>
<p><b>Keywords</b>: Audio-Visual learning, Visual Acoustic Matching
</p><p><b>Compressor summary</b>: The paper proposes a self-supervised method for acoustic matching that uses only target scene images and audio to re-synthesize audio in a different environment, outperforming existing methods.</p><hr><h3>Hierarchical Vector Quantized Transformer for Multi-class Unsupervised Anomaly Detection</h3>
<p>Ruiying Lu, YuJie Wu, Long Tian, Dongsheng Wang, Bo Chen, Xiyang Liu, Ruimin Hu</p>
<p><a href='https://openreview.net/forum?id=clJTNssgn6'>https://openreview.net/forum?id=clJTNssgn6</a></p>
<p><b>Keywords</b>: Anomaly Detection, Transformer, Vector Quantization, Unsupervised Anomaly Detection
</p><p><b>Compressor summary</b>: The paper proposes a hierarchical vector quantized prototype-oriented Transformer for unsupervised image anomaly detection that preserves normal patterns as discrete prototypes and uses optimal transport to evaluate abnormal scores.</p><hr><h3>Convergent Bregman Plug-and-Play Image Restoration for Poisson Inverse Problems</h3>
<p>Samuel Hurault, Ulugbek Kamilov, Arthur Leclaire, Nicolas Papadakis</p>
<p><a href='https://openreview.net/forum?id=clCELP8zFb'>https://openreview.net/forum?id=clCELP8zFb</a></p>
<p><b>Keywords</b>: Plug-and-Play, Poisson Inverse Problems, Bregman distance, Proximal Gradient Descent, nonconvex and nonsmooth optimization, Poisson inverse problems
</p><p><b>Compressor summary</b>: The paper proposes a new method called Bregman Score Denoiser for solving Poisson inverse problems using Plug-and-Play algorithms that better capture the problem's smoothness properties.</p><hr><h3>A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference</h3>
<p>Emile van Krieken, Thiviyan Thanapalasingam, Jakub M. Tomczak, Frank Van Harmelen, Annette Ten Teije</p>
<p><a href='https://openreview.net/forum?id=chlTA9Cegc'>https://openreview.net/forum?id=chlTA9Cegc</a></p>
<p><b>Keywords</b>: Neurosymbolic Learning, Generative Modeling, Approximate Inference
</p><p><b>Compressor summary</b>: A-NeSI is a new framework for probabilistic neurosymbolic learning that allows scalable approximate inference, symbolic explanations of predictions, and guarantees logical constraint satisfaction while maintaining performance.</p><hr><h3>Expressive probabilistic sampling in recurrent neural networks</h3>
<p>Shirui Chen, Linxing Preston Jiang, Rajesh P. N. Rao, Eric Todd SheaBrown</p>
<p><a href='https://openreview.net/forum?id=ch1buUOGa3'>https://openreview.net/forum?id=ch1buUOGa3</a></p>
<p><b>Keywords</b>: neural coding, probabilistic sampling, neural dynamics, recurrent neural network
</p><p><b>Compressor summary</b>: The paper explores how recurrent neural circuits can sample from arbitrary probability distributions, which could improve Bayesian brain models.</p><hr><h3>Fine-Grained Cross-View Geo-Localization Using a Correlation-Aware Homography Estimator</h3>
<p>Xiaolong Wang, Runsen Xu, Zhuofan Cui, Zeyu Wan, Yu Zhang</p>
<p><a href='https://openreview.net/forum?id=cgiP4cMBP9'>https://openreview.net/forum?id=cgiP4cMBP9</a></p>
<p><b>Keywords</b>: Fine-Grained Cross-View Geo-Localization, Homography Estimation
</p><p><b>Compressor summary</b>: The paper presents a new method for accurately aligning ground images with satellite images using a differentiable spherical transform and a correlation-aware homography estimator, achieving sub-pixel resolution and meter-level GPS accuracy, and outperforming existing techniques in localization error reduction.</p><hr><h3>On Separate Normalization in Self-supervised Transformers</h3>
<p>Xiaohui Chen, Yinkai Wang, Yuanqi Du, Soha Hassoun, Liping Liu</p>
<p><a href='https://openreview.net/forum?id=cezKbXsT3V'>https://openreview.net/forum?id=cezKbXsT3V</a></p>
<p><b>Keywords</b>: Transformer, Self-supervised Learning, Normalization
</p><p><b>Compressor summary</b>: The paper proposes a simple modification to self-supervised training methods for transformers by using separate normalization layers for tokens and the [CLS] symbol, which improves downstream task performance in various domains.</p><hr><h3>IBA: Towards Irreversible Backdoor Attacks in Federated Learning</h3>
<p>Dung Thuy Nguyen, Tuan Minh Nguyen, Anh Tuan Tran, Khoa D Doan, KOK SENG WONG</p>
<p><a href='https://openreview.net/forum?id=cemEOP8YoC'>https://openreview.net/forum?id=cemEOP8YoC</a></p>
<p><b>Keywords</b>: Backdoor Attacks, Federated Learning, Durability, Imperceptibility, Stealthiness
</p><p><b>Compressor summary</b>: The paper proposes a novel backdoor attack method for federated learning that is stealthy, efficient, and durable, and bypasses existing defenses.</p><hr><h3>Max-Sliced Mutual Information</h3>
<p>Dor Tsur, Ziv Goldfeld, Kristjan Greenewald</p>
<p><a href='https://openreview.net/forum?id=ce9B2x3zQa'>https://openreview.net/forum?id=ce9B2x3zQa</a></p>
<p><b>Keywords</b>: CCA, dimensionality reduction, information theory, mutual information, neural estimation, slicing
</p><p><b>Compressor summary</b>: The paper introduces a new method called max-sliced mutual information (mSMI) that combines the strengths of CCA and Shannon's mutual information for quantifying dependencies in high-dimensional data.</p><hr><h3>Semi-Supervised Domain Generalization with Known and Unknown Classes</h3>
<p>Lei Zhang, Ji-Fu Li, Wei Wang</p>
<p><a href='https://openreview.net/forum?id=ce59j806df'>https://openreview.net/forum?id=ce59j806df</a></p>
<p><b>Keywords</b>: Domain Generalization, Semi-Supervised Learning, Out-of-Distribution Detection, Deep Learning
</p><p><b>Compressor summary</b>: The CWAEE method is a semi-supervised domain generalization approach that uses class-wise adaptive thresholds and Fourier Transformation to handle unknown classes in unlabeled data, achieving better performance on real-world datasets.</p><hr><h3>Learning non-Markovian Decision-Making from State-only Sequences</h3>
<p>Aoyang Qin, Feng Gao, Qing Li, Song-Chun Zhu, Sirui Xie</p>
<p><a href='https://openreview.net/forum?id=cdlmsnQkZ9'>https://openreview.net/forum?id=cdlmsnQkZ9</a></p>
<p><b>Keywords</b>: Sequential Decision Making, Generative Model, Imitation Learning
</p><p><b>Compressor summary</b>: The paper proposes a deep generative model for imitation learning in non-Markovian settings, using energy-based policy prior and maximum likelihood estimation for inference and planning.</p><hr><h3>DeepACO: Neural-enhanced Ant Systems for Combinatorial Optimization</h3>
<p>Haoran Ye, Jiarui Wang, Zhiguang Cao, Helan Liang, Yong Li</p>
<p><a href='https://openreview.net/forum?id=cd5D1DD923'>https://openreview.net/forum?id=cd5D1DD923</a></p>
<p><b>Keywords</b>: Neural Combinatorial Optimization, Ant Colony Optimization, Evolutionary algorithm, Meta-heuristic, Deep reinforcement learning, Learned heuristic measure, Neural local search, Generalization
</p><p><b>Compressor summary</b>: DeepACO is a framework that uses deep reinforcement learning to improve Ant Colony Optimization algorithms without manual design of heuristics, achieving better results on various Combinatorial Optimization Problems.</p><hr><h3>Towards Distribution-Agnostic Generalized Category Discovery</h3>
<p>Jianhong Bai, Zuozhu Liu, Hualiang Wang, Ruizhe Chen, Lianrui Mu, Xiaomeng Li, Joey Tianyi Zhou, YANG FENG, Jian Wu, Haoji Hu</p>
<p><a href='https://openreview.net/forum?id=cczH4Xl7Zo'>https://openreview.net/forum?id=cczH4Xl7Zo</a></p>
<p><b>Keywords</b>: Generalized Category Discovery, Open-world Recognition, Long-tail Learning, Contrastive Learning
</p><p><b>Compressor summary</b>: The paper introduces a novel framework, BaCon, that tackles the realistic task of distribution-agnostic generalized category discovery (DA-GCD) in computer vision, addressing data imbalance and open-endedness with contrastive learning and pseudo-labeling.</p><hr><h3>Empowering Convolutional Neural Nets with MetaSin Activation</h3>
<p>Farnood Salehi, Tunc Ozan Aydin, André Gaillard, Guglielmo Camporese, Yuxuan Wang</p>
<p><a href='https://openreview.net/forum?id=cay8LnKSro'>https://openreview.net/forum?id=cay8LnKSro</a></p>
<p><b>Keywords</b>: sin activation, image prediction, image resampling, monte-carlo denoising, knowledge distillation
</p><p><b>Compressor summary</b>: The authors propose MetaSin activation, a novel ensemble function that improves performance and reliability in training sin networks for visual data tasks like denoising and resampling.</p><hr><h3>Augmentation-free Dense Contrastive Distillation for Efficient Semantic Segmentation</h3>
<p>Jiawei Fan, Chao Li, Xiaolong Liu, Meina Song, Anbang Yao</p>
<p><a href='https://openreview.net/forum?id=caUhYUVsLl'>https://openreview.net/forum?id=caUhYUVsLl</a></p>
<p><b>Keywords</b>: Knowledge distillation, semantic segmentation, contrastive learning
</p><p><b>Compressor summary</b>: Af-DCD is a new contrastive distillation method for semantic segmentation that uses masked feature mimicking and clever feature partitions to train efficient student models without data augmentation or memory buffer.</p><hr><h3>Bayesian Extensive-Rank Matrix Factorization with Rotational Invariant Priors</h3>
<p>Farzad Pourkamali, Nicolas Macris</p>
<p><a href='https://openreview.net/forum?id=ca2QmdOlIh'>https://openreview.net/forum?id=ca2QmdOlIh</a></p>
<p><b>Keywords</b>: Matrix factorization, Bayesian inference, rotation invariant estimators, random matrix theory, spherical integrals, replica method
</p><p><b>Compressor summary</b>: The paper proposes Rotation Invariant Estimators for matrix factorization with additive noise, which are conjectured to be optimal in high dimensions based on a Bayesian approach using random matrix theory, spherical integrals, and the replica method.</p><hr><h3>Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability</h3>
<p>Revan MacQueen, James R. Wright</p>
<p><a href='https://openreview.net/forum?id=cZVBRg59eb'>https://openreview.net/forum?id=cZVBRg59eb</a></p>
<p><b>Keywords</b>: Algorithmic Game Theory, Self-Play, Regret-Minimization, Multi-agent RL, Multiplayer Games, General-Sum Games
</p><p><b>Compressor summary</b>: The paragraph discusses how self-play can generate strategies with performance guarantees in some multiplayer games if they approximate constant-sum subgames with bounded subgame stability.</p><hr><h3>Data Minimization at Inference Time</h3>
<p>Cuong Tran, Ferdinando Fioretto</p>
<p><a href='https://openreview.net/forum?id=cZS5X3PLOR'>https://openreview.net/forum?id=cZS5X3PLOR</a></p>
<p><b>Keywords</b>: Privacy; data minimization
</p><p><b>Compressor summary</b>: The paper explores whether using only a small fraction of features for inference in high-stakes domains is sufficient to achieve accurate predictions, and proposes an efficient algorithm to determine which attributes are needed for each individual.</p><hr><h3>Context-guided Embedding Adaptation for Effective Topic Modeling in Low-Resource Regimes</h3>
<p>Yishi Xu, Jianqiao Sun, Yudi Su, Xinyang Liu, Zhibin Duan, Bo Chen, Mingyuan Zhou</p>
<p><a href='https://openreview.net/forum?id=cYkSt7jqlx'>https://openreview.net/forum?id=cYkSt7jqlx</a></p>
<p><b>Keywords</b>: Few-shot generative model; topic modeling;
</p><p><b>Compressor summary</b>: The paragraph describes a method for adapting word embeddings based on contextual information to improve topic modeling in low-resourced tasks and achieve better performance than existing methods.</p><hr><h3>Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning</h3>
<p>Seungyong Moon, Junyoung Yeom, Bumsoo Park, Hyun Oh Song</p>
<p><a href='https://openreview.net/forum?id=cUuXVaMmmv'>https://openreview.net/forum?id=cUuXVaMmmv</a></p>
<p><b>Keywords</b>: reinforcement learning, hierarchical reinforcement learning, contrastive learning, procedurally generated environments
</p><p><b>Compressor summary</b>: The paper proposes an improved model-free approach using PPO and contrastive learning to discover hierarchical achievements in procedurally generated environments effectively.</p><hr><h3>Riemannian Residual Neural Networks</h3>
<p>Isay Katsman, Eric Ming Chen, Sidhanth Holalkere, Anna Asch, Aaron Lou, Ser-Nam Lim, Christopher De Sa</p>
<p><a href='https://openreview.net/forum?id=cRzt1umRNx'>https://openreview.net/forum?id=cRzt1umRNx</a></p>
<p><b>Keywords</b>: neural network, riemannian, manifold, resnet
</p><p><b>Compressor summary</b>: The authors extend ResNets to work on general Riemannian manifolds, achieving better performance than existing manifold neural networks for graphs with hierarchical structures or manifold-valued data.</p><hr><h3>Precise asymptotic generalization for multiclass classification with overparameterized linear models</h3>
<p>David Xing Wu, Anant Sahai</p>
<p><a href='https://openreview.net/forum?id=cRGINXQWem'>https://openreview.net/forum?id=cRGINXQWem</a></p>
<p><b>Keywords</b>: overparameterized, multiclass, classification, theory, generalization, interpolation, bi-level, Gaussian model
</p><p><b>Compressor summary</b>: This paper analyzes the generalization performance of a linear model for multiclass and multi-label classification with growing data dimensions, resolves a conjecture from a previous paper, and introduces a new variant of the Hanson-Wright inequality for these problems.</p><hr><h3>GraphMP: Graph Neural Network-based Motion Planning with Efficient Graph Search</h3>
<p>Xiao Zang, Miao Yin, Jinqi Xiao, Saman Zonouz, Bo Yuan</p>
<p><a href='https://openreview.net/forum?id=cQdc9Dyk4i'>https://openreview.net/forum?id=cQdc9Dyk4i</a></p>
<p><b>Keywords</b>: graph neural network, deep learning
</p><p><b>Compressor summary</b>: GraphMP is a neural motion planner that improves graph information extraction and search processing, achieving better path quality and planning speed than existing methods in various environments.</p><hr><h3>The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model</h3>
<p>Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, Matthieu Geist, Yuejie Chi</p>
<p><a href='https://openreview.net/forum?id=cOQH8YO255'>https://openreview.net/forum?id=cOQH8YO255</a></p>
<p><b>Keywords</b>: distributionally robust reinforcement learning, robust Markov decision processes, sample complexity
</p><p><b>Compressor summary</b>: The paper studies how robust reinforcement learning is to different uncertainty sets and shows that learning with total variation is more efficient than standard MDPs.</p><hr><h3>A Scalable Neural Network for DSIC Affine Maximizer Auction Design</h3>
<p>Zhijian Duan, Haoran Sun, Yurong Chen, Xiaotie Deng</p>
<p><a href='https://openreview.net/forum?id=cNb5hkTfGC'>https://openreview.net/forum?id=cNb5hkTfGC</a></p>
<p><b>Keywords</b>: Automated Mechanism Design, Auction Design, Affine Maximizer Auctions, Deep Learning, Game Theory
</p><p><b>Compressor summary</b>: AMenuNet is a scalable neural network that designs dominant strategy incentive compatible auctions by constructing allocation menus from bidder and item representations.</p><hr><h3>PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language Navigation</h3>
<p>Jialu Li, Mohit Bansal</p>
<p><a href='https://openreview.net/forum?id=cNObl6QQEH'>https://openreview.net/forum?id=cNObl6QQEH</a></p>
<p><b>Keywords</b>: Vision-and-Language Navigation, diffusion models, image inpainting for panorama generation
</p><p><b>Compressor summary</b>: PanoGen is a method to create diverse and realistic 3D environments based on text descriptions, which can improve Vision-and-Language Navigation performance and help agents generalize better to new situations.</p><hr><h3>Variational Imbalanced Regression: Fair Uncertainty Quantification via Probabilistic Smoothing</h3>
<p>Ziyan Wang, Hao Wang</p>
<p><a href='https://openreview.net/forum?id=cMUBkkTrMo'>https://openreview.net/forum?id=cMUBkkTrMo</a></p>
<p><b>Keywords</b>: probabilistic methods, imbalanced regression, variational inference
</p><p><b>Compressor summary</b>: The paper introduces a probabilistic deep learning model, VIR, that performs well in imbalanced regression and naturally estimates uncertainty using data borrowing and reweighting techniques.</p><hr><h3>HIQL: Offline Goal-Conditioned RL with Latent States as Actions</h3>
<p>Seohong Park, Dibya Ghosh, Benjamin Eysenbach, Sergey Levine</p>
<p><a href='https://openreview.net/forum?id=cLQCCtVDuW'>https://openreview.net/forum?id=cLQCCtVDuW</a></p>
<p><b>Keywords</b>: reinforcement learning
</p><p><b>Compressor summary</b>: The paper proposes a hierarchical algorithm for goal-conditioned RL from offline data that leverages the structure of reaching distant goals through subgoals, and shows its effectiveness on various benchmarks.</p><hr><h3>On the Role of Entanglement and Statistics in Learning</h3>
<p>Srinivasan A, Vojtěch Havlíček, Louis Schatzki</p>
<p><a href='https://openreview.net/forum?id=cGeLeh995N'>https://openreview.net/forum?id=cGeLeh995N</a></p>
<p><b>Keywords</b>: Quantum Computing, Statistical Learning, Quantum learning theory, Entanglement
</p><p><b>Compressor summary</b>: The paper investigates how entangled, separable, and statistical measurements affect learning in the quantum statistical query model. It reveals different trade-offs depending on the task and introduces a new quantum dimensionality concept to lower bound the complexity of learning various quantum states and functions. It also gives unconditional separation results for error mitigation methods.</p><hr><h3>NeuroGF: A Neural Representation for Fast Geodesic Distance and Path Queries</h3>
<p>Qijian Zhang, Junhui Hou, Yohanes Yudhi Adikusuma, Wenping Wang, Ying He</p>
<p><a href='https://openreview.net/forum?id=cGdGh3Mp2W'>https://openreview.net/forum?id=cGdGh3Mp2W</a></p>
<p><b>Keywords</b>: geodesic distance, implicit representation, 3D geometry
</p><p><b>Compressor summary</b>: The paper introduces NeuroGF, a neural implicit representation of geodesics on 3D mesh models that can efficiently answer queries and encode both geometry and geodesics in a unified format.</p><hr><h3>The geometry of hidden representations of large transformer models</h3>
<p>Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio ansuini, Alberto Cazzaniga</p>
<p><a href='https://openreview.net/forum?id=cCYvakU5Ek'>https://openreview.net/forum?id=cCYvakU5Ek</a></p>
<p><b>Keywords</b>: Representations, transformers, geometry, interpretability
</p><p><b>Compressor summary</b>: Large transformers have similar representation evolution across various data types and can be used to identify layers with high semantic content.</p><hr><h3>Conditional Score Guidance for Text-Driven Image-to-Image Translation</h3>
<p>Hyunsoo Lee, Minsoo Kang, Bohyung Han</p>
<p><a href='https://openreview.net/forum?id=cBS5CU96Jq'>https://openreview.net/forum?id=cBS5CU96Jq</a></p>
<p><b>Keywords</b>: Diffusion, Image-to-Image Translation
</p><p><b>Compressor summary</b>: The paper proposes a text-driven image-to-image translation method that edits regions of interest in a source image based on a modifying text and preserves other parts, using a novel score function and mixup technique for better guidance and fusion.</p><hr><h3>Strategic Apple Tasting</h3>
<p>Keegan Harris, Chara Podimata, Steven Wu</p>
<p><a href='https://openreview.net/forum?id=cBIPcZKFdw'>https://openreview.net/forum?id=cBIPcZKFdw</a></p>
<p><b>Keywords</b>: strategic classification, strategic learning, apple tasting, bandit feedback, learning with incentives
</p><p><b>Compressor summary</b>: The text presents algorithms for decision-making under incentives and one-sided feedback, aiming to minimize strategic regret compared to the optimal policy. One algorithm works for stochastic agents and the other for adversarial ones, with different performance guarantees. The algorithms also apply to bandit feedback settings.</p><hr><h3>Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture</h3>
<p>Daniel Y Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W Thomas, Benjamin Frederick Spector, Michael Poli, Atri Rudra, Christopher Re</p>
<p><a href='https://openreview.net/forum?id=cB0BImqSS9'>https://openreview.net/forum?id=cB0BImqSS9</a></p>
<p><b>Keywords</b>: structured matrices, transformers, efficiency
</p><p><b>Compressor summary</b>: Monarch Mixer (M2) is a new architecture that uses Monarch matrices, a sub-quadratic primitive, to scale in sequence length and model dimension, achieving high performance and efficiency in various domains.</p><hr><h3>Enhancing Motion Deblurring in High-Speed Scenes with Spike Streams</h3>
<p>Shiyan Chen, Jiyuan Zhang, Yajing Zheng, Tiejun Huang, Zhaofei Yu</p>
<p><a href='https://openreview.net/forum?id=cAyLnMxiTl'>https://openreview.net/forum?id=cAyLnMxiTl</a></p>
<p><b>Keywords</b>: spike camera, neuromorphic vision sensors, motion deblurring, high speed imaging
</p><p><b>Compressor summary</b>: The paper proposes a new method to remove motion blur from high-speed scenes using spike camera data and traditional RGB images, improving visual quality over existing methods.</p><hr><h3>Explainable Brain Age Prediction using coVariance Neural Networks</h3>
<p>Saurabh Sihag, Gonzalo Mateos, Corey McMillan, Alejandro Ribeiro</p>
<p><a href='https://openreview.net/forum?id=cAhJF87GN0'>https://openreview.net/forum?id=cAhJF87GN0</a></p>
<p><b>Keywords</b>: graph neural networks, brain age, Alzheimer's disease, interpretability, explainability, computational neuroscience
</p><p><b>Compressor summary</b>: The paper proposes a framework using coVariance neural networks for predicting brain age based on cortical thickness features, which allows for anatomical interpretability in identifying regions related to elevated brain age gap in Alzheimer's disease.</p><hr><h3>Estimating the Rate-Distortion Function by Wasserstein Gradient Descent</h3>
<p>Yibo Yang, Stephan Eckstein, Marcel Nutz, Stephan Mandt</p>
<p><a href='https://openreview.net/forum?id=cAaTbLa3ad'>https://openreview.net/forum?id=cAaTbLa3ad</a></p>
<p><b>Keywords</b>: information theory, rate-distortion function, optimal transport
</p><p><b>Compressor summary</b>: The paper proposes a new method to estimate the rate-distortion function using optimal transport, which learns the support of the optimal reproduction distribution and has advantages over neural network methods in terms of computational effort and tuning.</p><hr><h3>Fair, Polylog-Approximate Low-Cost Hierarchical Clustering</h3>
<p>Marina Knittel, Max Springer, John P Dickerson, MohammadTaghi Hajiaghayi</p>
<p><a href='https://openreview.net/forum?id=cAPMmCl2f3'>https://openreview.net/forum?id=cAPMmCl2f3</a></p>
<p><b>Keywords</b>: Fair machine learning, hierarchical clustering, clustering
</p><p><b>Compressor summary</b>: The paper introduces a new algorithm for fair hierarchical clustering that breaks the polynomial-approximate barrier and achieves low cost.</p><hr><h3>Characterization and Learning of Causal Graphs with Small Conditioning Sets</h3>
<p>Murat Kocaoglu</p>
<p><a href='https://openreview.net/forum?id=cANkPsVtsw'>https://openreview.net/forum?id=cANkPsVtsw</a></p>
<p><b>Keywords</b>: causal discovery
</p><p><b>Compressor summary</b>: The paper proposes a new constraint-based causal discovery algorithm, the k-PC algorithm, which uses conditional independence tests with an upper bound on the conditioning set size and graphically characterizes the resulting k-Markov equivalence class of causal graphs for more robust causal discovery in small data settings.</p><hr><h3>Sequential Subset Matching for Dataset Distillation</h3>
<p>Jiawei Du, Qin Shi, Joey Tianyi Zhou</p>
<p><a href='https://openreview.net/forum?id=c9fXCzR5fK'>https://openreview.net/forum?id=c9fXCzR5fK</a></p>
<p><b>Keywords</b>: Dataset distillation, gradients matching
</p><p><b>Compressor summary</b>: The paper introduces Sequential Subset Matching (SeqMatch), a new method for dataset distillation that adaptively optimizes synthetic data to improve performance by addressing the coupling issue caused by static optimization.</p><hr><h3>Maximum Average Randomly Sampled: A Scale Free and Non-parametric Algorithm for Stochastic Bandits</h3>
<p>Masoud Moravej Khorasani, Erik Weyer</p>
<p><a href='https://openreview.net/forum?id=c8nIdZ5HJJ'>https://openreview.net/forum?id=c8nIdZ5HJJ</a></p>
<p><b>Keywords</b>: Stochastic Multi-armed bandit, Online Learning, Upper Confidence Bound
</p><p><b>Compressor summary</b>: MARS is a data-dependent UCB algorithm for multi-armed bandits that doesn't need scaling, uses maximum average of randomly sampled rewards, and performs similarly to $\psi$-UCB without correction factors.</p><hr><h3>EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual Representation Learning</h3>
<p>Ping Guo, Xiangpeng Wei, Yue Hu, Baosong Yang, Dayiheng Liu, Fei Huang, jun xie</p>
<p><a href='https://openreview.net/forum?id=c5dRV9tA3K'>https://openreview.net/forum?id=c5dRV9tA3K</a></p>
<p><b>Keywords</b>: cross-lingual pretraining;language-agnostic representation
</p><p><b>Compressor summary</b>: The paper introduces Emma-X, a pre-training algorithm to learn cross-lingual universals using non-parallel data, and shows its effectiveness on xrete benchmark tasks.</p><hr><h3>PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning</h3>
<p>Hojoon Lee, Hanseul Cho, Hyunseung Kim, Daehoon Gwak, Joonkee Kim, Jaegul Choo, Se-Young Yun, Chulhee Yun</p>
<p><a href='https://openreview.net/forum?id=c5WOU7p4ES'>https://openreview.net/forum?id=c5WOU7p4ES</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Sharpness Minimization, Generalization, Plasticity, Deep Learning
</p><p><b>Compressor summary</b>: The PLASTIC algorithm, which balances input and label plasticity in off-policy reinforcement learning, enhances sample efficiency by preserving the model's ability to adapt to changing data and goals.</p><hr><h3>Tight Bounds for Volumetric Spanners and Applications</h3>
<p>Aditya Bhaskara, Sepideh Mahabadi, Ali Vakilian</p>
<p><a href='https://openreview.net/forum?id=c4Xc0uTLXW'>https://openreview.net/forum?id=c4Xc0uTLXW</a></p>
<p><b>Keywords</b>: volumetric spanner, well-conditioned basis, determinant maximization, minimum volume enclosing ellipsoid
</p><p><b>Compressor summary</b>: A volumetric spanner is a subset of points that can express all other points with small coefficients, and this paper provides almost optimal bounds on its size and a simple construction method for all $\ell_p$ norms, with applications to finding coresets for the MVEE problem.</p><hr><h3>Tight Risk Bounds for Gradient Descent on Separable Data</h3>
<p>Matan Schliserman, Tomer Koren</p>
<p><a href='https://openreview.net/forum?id=c2eedxSlPJ'>https://openreview.net/forum?id=c2eedxSlPJ</a></p>
<p><b>Keywords</b>: Convex optimization, Gradient Descent, separable data, generalization bounds, Stochastic Gradient Descent.
</p><p><b>Compressor summary</b>: This paper investigates the generalization abilities of unregularized gradient methods in separable linear classification, provides tighter risk bounds depending on the tail decay rate of the loss function, and simplifies the proof technique compared to previous work.</p><hr><h3>BIOT: Biosignal Transformer for Cross-data Learning in the Wild</h3>
<p>Chaoqi Yang, M Brandon Westover, Jimeng Sun</p>
<p><a href='https://openreview.net/forum?id=c2LZyTyddi'>https://openreview.net/forum?id=c2LZyTyddi</a></p>
<p><b>Keywords</b>: biological signal, transformer, cross-data learning, in-the-wild learning
</p><p><b>Compressor summary</b>: The paper introduces Biosignal Transformer, a flexible encoder for biosignals that can handle different formats and enable cross-data learning, outperforming baselines and improving performance on seizure detection task.</p><hr><h3>Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</h3>
<p>Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman</p>
<p><a href='https://openreview.net/forum?id=bzs4uPLXvi'>https://openreview.net/forum?id=bzs4uPLXvi</a></p>
<p><b>Keywords</b>: Natural language processing, large language models, XAI, explainability
</p><p><b>Compressor summary</b>: CoT explanations from LLMs can be misleading due to added biasing features, impacting accuracy and trust in the models.</p><hr><h3>Federated Linear Bandits with Finite Adversarial Actions</h3>
<p>Li Fan, Ruida Zhou, Chao Tian, Cong Shen</p>
<p><a href='https://openreview.net/forum?id=bzXpQUnule'>https://openreview.net/forum?id=bzXpQUnule</a></p>
<p><b>Keywords</b>: Federated bandits, contextual bandits, regret analysis
</p><p><b>Compressor summary</b>: FedSupLinUCB is an order-optimal algorithm for federated linear bandits with adversarial finite action sets, achieving low regret and controlled communication cost in asynchronous and synchronous cases.</p><hr><h3>Natural Language Instruction-following with Task-related Language Development and Translation</h3>
<p>Jing-Cheng Pang, Xinyu Yang, Si-Hang Yang, Xiong-Hui Chen, Yang Yu</p>
<p><a href='https://openreview.net/forum?id=bx0SDRVDzF'>https://openreview.net/forum?id=bx0SDRVDzF</a></p>
<p><b>Keywords</b>: Reinforcement learning, instruction-following, autonomous agent
</p><p><b>Compressor summary</b>: TALAR simplifies natural language instructions into task-related language for efficient policy training in reinforcement learning.</p><hr><h3>Global Structure-Aware Diffusion Process for Low-light Image Enhancement</h3>
<p>Jinhui HOU, Zhiyu Zhu, Junhui Hou, Hui LIU, Huanqiang Zeng, Hui Yuan</p>
<p><a href='https://openreview.net/forum?id=bv9mmH0LGF'>https://openreview.net/forum?id=bv9mmH0LGF</a></p>
<p><b>Keywords</b>: Image enhancement, diffusion models
</p><p><b>Compressor summary</b>: The paper proposes a diffusion-based method for low-light image enhancement, using curvature and uncertainty regularization to improve detail preservation, contrast, and noise suppression, outperforming existing methods.</p><hr><h3>Chatting Makes Perfect: Chat-based Image Retrieval</h3>
<p>Matan Levy, Rami Ben-Ari, Nir Darshan, Dani Lischinski</p>
<p><a href='https://openreview.net/forum?id=bt7pQ7o7zG'>https://openreview.net/forum?id=bt7pQ7o7zG</a></p>
<p><b>Keywords</b>: Image Retrieval, Multi-modal learning
</p><p><b>Compressor summary</b>: The paragraph introduces ChatIR, a chat-based image retrieval system that uses dialog with the user to clarify their search intent and leverages Large Language Models to generate follow-up questions, achieving significant gains in image retrieval compared to existing approaches.</p><hr><h3>Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning</h3>
<p>Fan Feng, Sara Magliacane</p>
<p><a href='https://openreview.net/forum?id=bsNslV3Ahe'>https://openreview.net/forum?id=bsNslV3Ahe</a></p>
<p><b>Keywords</b>: multi-object RL, compositional generalization, factored representations
</p><p><b>Compressor summary</b>: DAFT-RL is a framework that improves object-centric representation learning and generalization in reinforcement learning by factorizing dynamics and rewards according to object attributes.</p><hr><h3>Self-Chained Image-Language Model for Video Localization and Question Answering</h3>
<p>Shoubin Yu, Jaemin Cho, Prateek Yadav, Mohit Bansal</p>
<p><a href='https://openreview.net/forum?id=brOMKBEGXP'>https://openreview.net/forum?id=brOMKBEGXP</a></p>
<p><b>Keywords</b>: Video Question Answering, Video Localization, Image-Language Model
</p><p><b>Compressor summary</b>: The text describes a new video question answering method called SeViLA that uses a single image-language model to perform both keyframe localization and answer prediction, improving performance on various benchmarks.</p><hr><h3>Parallel Sampling of Diffusion Models</h3>
<p>Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, Nima Anari</p>
<p><a href='https://openreview.net/forum?id=bpzwUfX1UP'>https://openreview.net/forum?id=bpzwUfX1UP</a></p>
<p><b>Keywords</b>: diffusion models, parallel sampling
</p><p><b>Compressor summary</b>: ParaDiGMS is a new method that speeds up diffusion model sampling by running multiple denoising steps in parallel, achieving 2-4x faster sampling without sacrificing quality.</p><hr><h3>SyncTREE: Fast Timing Analysis for Integrated Circuit Design through a Physics-informed Tree-based Graph Neural Network</h3>
<p>Yuting Hu, Jiajie Li, Florian Klemme, Gi-Joon Nam, Tengfei Ma, Hussam Amrouch, Jinjun Xiong</p>
<p><a href='https://openreview.net/forum?id=bprclnHNvm'>https://openreview.net/forum?id=bprclnHNvm</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Integrated Circuits, Circuit Timing Analysis, Physics-guided Deep Learning
</p><p><b>Compressor summary</b>: The paper proposes SyncTREE, a tree-based graph neural network that uses AI to improve the speed and accuracy of timing analysis for interconnects in integrated circuits.</p><hr><h3>EDGI: Equivariant Diffusion for Planning with Embodied Agents</h3>
<p>Johann Brehmer, Joey Bose, Pim De Haan, Taco Cohen</p>
<p><a href='https://openreview.net/forum?id=bpmM6SkDUy'>https://openreview.net/forum?id=bpmM6SkDUy</a></p>
<p><b>Keywords</b>: Planning, Diffusion models, Equivariance, Equivariant generative models
</p><p><b>Compressor summary</b>: EDGI is an algorithm that uses a new equivariant diffusion model to efficiently plan and learn in structured environments with various symmetries.</p><hr><h3>Learning Universal Policies via Text-Guided Video Generation</h3>
<p>Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, Pieter Abbeel</p>
<p><a href='https://openreview.net/forum?id=bo8q5MRcwy'>https://openreview.net/forum?id=bo8q5MRcwy</a></p>
<p><b>Keywords</b>: sequential decision making, general-purpose agent, video diffusion
</p><p><b>Compressor summary</b>: The paper explores using text-guided image synthesis to create more general-purpose agents that can plan and execute various tasks by generating realistic videos of their actions.</p><hr><h3>Active Reasoning in an Open-World Environment</h3>
<p>Manjie Xu, Guangyuan Jiang, Wei Liang, Chi Zhang, Yixin Zhu</p>
<p><a href='https://openreview.net/forum?id=bo5oIoL95U'>https://openreview.net/forum?id=bo5oIoL95U</a></p>
<p><b>Keywords</b>: Visual Reasoning, Abductive Reasoning, Active Reasoning
</p><p><b>Compressor summary</b>: The authors introduce Conan, an interactive environment for assessing active reasoning abilities in vision-language models, which differ from humans' ability to explore and reason with incomplete information.</p><hr><h3>Hierarchical Open-vocabulary Universal Image Segmentation</h3>
<p>Xudong Wang, Shufan Li, Konstantinos Kallidromitis, Yusuke Kato, Kazuki Kozuka, Trevor Darrell</p>
<p><a href='https://openreview.net/forum?id=bn4qZxltsH'>https://openreview.net/forum?id=bn4qZxltsH</a></p>
<p><b>Keywords</b>: Universal Image Segmentation, Hierarchical, Open-vocabulary
</p><p><b>Compressor summary</b>: HIPIE is a model that uses text descriptions and hierarchical representation to perform open-vocabulary image segmentation, achieving state-of-the-art results in various image comprehension tasks.</p><hr><h3>Bringing regularized optimal transport to lightspeed: a splitting method adapted for GPUs</h3>
<p>Jacob Lindbäck, Zesen Wang, Mikael Johansson</p>
<p><a href='https://openreview.net/forum?id=bmdnWIuypV'>https://openreview.net/forum?id=bmdnWIuypV</a></p>
<p><b>Keywords</b>: optimal transport, domain adaptation, splitting methods, gpu computations
</p><p><b>Compressor summary</b>: The algorithm efficiently solves regularized optimal transport problems using a Douglas-Rachford splitting technique with global convergence guarantees, low cost, and GPU parallelization, making it fast for various applications like domain adaptation and generative modeling.</p><hr><h3>Paxion: Patching Action Knowledge in Video-Language Foundation Models</h3>
<p>Zhenhailong Wang, Ansel Blume, Sha Li, Genglin Liu, Jaemin Cho, Zineng Tang, Mohit Bansal, Heng Ji</p>
<p><a href='https://openreview.net/forum?id=blm1pqiOXe'>https://openreview.net/forum?id=blm1pqiOXe</a></p>
<p><b>Keywords</b>: video-language model, action knowledge benchmarking, action understanding, temporal understanding
</p><p><b>Compressor summary</b>: The paper introduces ActionBench, a benchmark to assess video-language models' action knowledge, and Paxion, a framework that improves their performance by training them with DVDM objective.</p><hr><h3>Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction</h3>
<p>Xiaowen Jiang, Sebastian U Stich</p>
<p><a href='https://openreview.net/forum?id=blC2kbzvNC'>https://openreview.net/forum?id=blC2kbzvNC</a></p>
<p><b>Keywords</b>: Convex Optimization, SGD, Adaptive Methods, Variance Reduction, Polyak Stepsize, Line-Search
</p><p><b>Compressor summary</b>: The authors propose new variants of SPS and SLS that achieve optimal rates in various settings and a novel VR method that works well with them.</p><hr><h3>Kiki or Bouba? Sound Symbolism in Vision-and-Language Models</h3>
<p>Morris Alper, Hadar Averbuch-Elor</p>
<p><a href='https://openreview.net/forum?id=bfmSc1ETT9'>https://openreview.net/forum?id=bfmSc1ETT9</a></p>
<p><b>Keywords</b>: multimodal learning, computer vision, NLP, cognitive science
</p><p><b>Compressor summary</b>: This paper investigates whether sound symbolism, the idea that certain sounds are associated with specific meanings across languages, is reflected in vision-and-language models like CLIP and Stable Diffusion, and finds strong evidence that they do show this pattern.</p><hr><h3>Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion</h3>
<p>Yash Sanjay Bhalgat, Iro Laina, Joao F. Henriques, Andrea Vedaldi, Andrew Zisserman</p>
<p><a href='https://openreview.net/forum?id=bbbbbov4Xu'>https://openreview.net/forum?id=bbbbbov4Xu</a></p>
<p><b>Keywords</b>: Neural Radiance Fields, Instance Segmentation, Metric Learning, Clustering, 3D Computer Vision
</p><p><b>Compressor summary</b>: The paper presents a novel 3D instance segmentation method that uses 2D pre-trained models and a slow-fast clustering objective function to handle large numbers of objects in complex scenes, achieving state-of-the-art results on several datasets.</p><hr><h3>Anonymous and Copy-Robust Delegations for Liquid Democracy</h3>
<p>Markus Utke, Ulrike Schmidt-Kraepelin</p>
<p><a href='https://openreview.net/forum?id=bbL20Oupi4'>https://openreview.net/forum?id=bbL20Oupi4</a></p>
<p><b>Keywords</b>: liquid democracy, directed trees, parameterized markov chain, matrix tree theorem, axiomatic method
</p><p><b>Compressor summary</b>: The paragraph discusses a new voting system that combines representative and direct democracy, and two rules that balance anonymity and copy-robustness, which are shown to be equivalent using a polynomial-time algorithm with applications in other fields.</p><hr><h3>Lie Point Symmetry and Physics-Informed Networks</h3>
<p>Tara Akhound-Sadegh, Laurence Perreault-Levasseur, Johannes Brandstetter, Max Welling, Siamak Ravanbakhsh</p>
<p><a href='https://openreview.net/forum?id=ba4boN3W1n'>https://openreview.net/forum?id=ba4boN3W1n</a></p>
<p><b>Keywords</b>: PDE, Lie point symmetry, Symmetry, Neural PDE solver, PINNs
</p><p><b>Compressor summary</b>: The text discusses how incorporating PDE symmetries into physics-informed neural networks can improve their ability to learn solutions and neighboring solutions efficiently.</p><hr><h3>Hybrid Search for Efficient Planning with Completeness Guarantees</h3>
<p>Kalle Kujanpää, Joni Pajarinen, Alexander Ilin</p>
<p><a href='https://openreview.net/forum?id=bY0c46ZtXa'>https://openreview.net/forum?id=bY0c46ZtXa</a></p>
<p><b>Keywords</b>: Planning, Subgoal search, Reinforcement learning, Hierarchical Imitation Learning, Hierarchical planning, Hierarchical reinforcement learning
</p><p><b>Compressor summary</b>: The paper proposes complete subgoal search, an efficient hybrid method that combines high-level and low-level actions to achieve completeness in solving complex planning problems.</p><hr><h3>Kissing to Find a Match: Efficient Low-Rank Permutation Representation</h3>
<p>Hannah Dröge, Zorah Lähner, Yuval Bahat, Onofre Martorell Nadal, Felix Heide, Michael Moeller</p>
<p><a href='https://openreview.net/forum?id=bXvmnpCMmq'>https://openreview.net/forum?id=bXvmnpCMmq</a></p>
<p><b>Keywords</b>: low rank, permutation, kissing number, matrix factorization, assigment problem
</p><p><b>Compressor summary</b>: The paper proposes an efficient method for approximating large permutation matrices using low-rank matrix factorization and reducing memory costs, enabling the solution of previously infeasible problems.</p><hr><h3>Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms</h3>
<p>Shenao Zhang, Boyi Liu, Zhaoran Wang, Tuo Zhao</p>
<p><a href='https://openreview.net/forum?id=bUgqyyNo8j'>https://openreview.net/forum?id=bUgqyyNo8j</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Model-Based Reinforcement Learning, Policy Gradient
</p><p><b>Compressor summary</b>: The paper analyzes the challenges of applying ReParameterization Policy Gradient Methods to long-term reinforcement learning problems and proposes a spectral normalization method to improve gradient estimation and reduce variance.</p><hr><h3>The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter</h3>
<p>AJAY KUMAR JAISWAL, Shiwei Liu, Tianlong Chen, Zhangyang Wang</p>
<p><a href='https://openreview.net/forum?id=bU9hwbsVcy'>https://openreview.net/forum?id=bU9hwbsVcy</a></p>
<p><b>Keywords</b>: Pre-trained Models, Sparsity, Emergence, Transformers, Pruning
</p><p><b>Compressor summary</b>: The paper studies sparse patterns in large pre-trained transformers, proposes a concept of essential sparsity with a sharp dropping point, and finds that self-supervised learning triggers stronger emergent sparsification than supervised learning.</p><hr><h3>Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents</h3>
<p>Woojun Kim, Yongjae Shin, Jongeui Park, Youngchul Sung</p>
<p><a href='https://openreview.net/forum?id=bTidcHIK2t'>https://openreview.net/forum?id=bTidcHIK2t</a></p>
<p><b>Keywords</b>: deep reinforcement learning, primacy bais, reset, deep ensemble learning
</p><p><b>Compressor summary</b>: The paper proposes a new reset method for deep reinforcement learning that uses deep ensembles to address primacy bias, overfitting, and performance collapse issues while improving sample efficiency.</p><hr><h3>Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation</h3>
<p>Wenhao Ding, Laixi Shi, Yuejie Chi, Ding Zhao</p>
<p><a href='https://openreview.net/forum?id=bTL5SNOpfa'>https://openreview.net/forum?id=bTL5SNOpfa</a></p>
<p><b>Keywords</b>: reinforcement learning, robustness, causality, spurious correlation
</p><p><b>Compressor summary</b>: The paper proposes a new robust reinforcement learning framework that avoids learning spurious correlations caused by unobserved confounders and shows improved performance in real-world tasks.</p><hr><h3>Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation</h3>
<p>Yingyi Chen, Qinghua Tao, Francesco Tonin, Johan Suykens</p>
<p><a href='https://openreview.net/forum?id=bRyduWAAVT'>https://openreview.net/forum?id=bRyduWAAVT</a></p>
<p><b>Keywords</b>: Self-attention, primal-dual representations, SVD, kernel method, asymmetry, transformer
</p><p><b>Compressor summary</b>: The paper introduces Primal-Attention, a new attention mechanism based on asymmetric Kernel Singular Value Decomposition (KSVD), which improves efficiency and promotes low-rank property in self-attention.</p><hr><h3>Distributionally Robust Bayesian Optimization with $\varphi$-divergences</h3>
<p>Hisham Husain, Vu Nguyen, Anton van den Hengel</p>
<p><a href='https://openreview.net/forum?id=bRlEwWd7Vy'>https://openreview.net/forum?id=bRlEwWd7Vy</a></p>
<p><b>Keywords</b>: Bayesian Optimization, Distributionally Robust Optimization, φ-divergences
</p><p><b>Compressor summary</b>: The authors propose a new algorithm for Bayesian Optimization that is robust against data-shift in various divergences and achieve sublinear regret bounds with experimental validation.</p><hr><h3>Efficiently incorporating quintuple interactions into geometric deep learning force fields</h3>
<p>Zun Wang, Guoqing Liu, Yichi Zhou, Tong Wang, Bin Shao</p>
<p><a href='https://openreview.net/forum?id=bPJmu1PbZD'>https://openreview.net/forum?id=bPJmu1PbZD</a></p>
<p><b>Keywords</b>: Machine learning force field, graph neural network, many-body interactions
</p><p><b>Compressor summary</b>: The quintuple network (QuinNet) is a graph neural network that efficiently represents many-body interactions up to five-body interactions with high accuracy for molecular dynamics simulations.</p><hr><h3>Online Control for Meta-optimization</h3>
<p>Xinyi Chen, Elad Hazan</p>
<p><a href='https://openreview.net/forum?id=bOQNd7tWAp'>https://openreview.net/forum?id=bOQNd7tWAp</a></p>
<p><b>Keywords</b>: online learning, control, hyperparameter optimization
</p><p><b>Compressor summary</b>: The authors propose a novel meta-optimization approach based on control theory to learn the best optimization algorithm online, overcoming nonconvexity challenges and achieving regret guarantees.</p><hr><h3>Structured Neural Networks for Density Estimation and Causal Inference</h3>
<p>Asic Q Chen, Ruian Shi, Xiang Gao, Ricardo Baptista, Rahul G Krishnan</p>
<p><a href='https://openreview.net/forum?id=bNXVRJjmOl'>https://openreview.net/forum?id=bNXVRJjmOl</a></p>
<p><b>Keywords</b>: generative models, density estimation, normalizing flows, binary matrix factorization, causal inference
</p><p><b>Compressor summary</b>: The Structured Neural Network (StrNN) injects structure into neural networks by masking pathways based on binary matrix factorization, enabling learning functions with desired independencies and improving data-efficient generative modeling and causal effect estimation.</p><hr><h3>Empowering Collaborative Filtering with Principled Adversarial Contrastive Loss</h3>
<p>An Zhang, Leheng Sheng, Zhibo Cai, Xiang Wang, Tat-Seng Chua</p>
<p><a href='https://openreview.net/forum?id=bNNIf8F9OU'>https://openreview.net/forum?id=bNNIf8F9OU</a></p>
<p><b>Keywords</b>: Collaborative filtering, Contrastive loss, Recommendation, Generalization ability
</p><p><b>Compressor summary</b>: The paragraph discusses the challenges of using contrastive learning in collaborative filtering and proposes a new loss function called Adversarial InfoNCE that improves generalization ability for out-of-distribution tasks.</p><hr><h3>Learning Layer-wise Equivariances Automatically using Gradients</h3>
<p>Tycho F.A. van der Ouderaa, Alexander Immer, Mark van der Wilk</p>
<p><a href='https://openreview.net/forum?id=bNIHdyunFC'>https://openreview.net/forum?id=bNIHdyunFC</a></p>
<p><b>Keywords</b>: learning layer-wise relaxed equivariances bayesian symmetry discovery marginal likelihood
</p><p><b>Compressor summary</b>: The authors propose a method to learn flexible symmetry constraints in neural networks from data using gradients and differentiable Laplace approximations, which improves generalization performance on image classification tasks.</p><hr><h3>Sequential Predictive Two-Sample and Independence Testing</h3>
<p>Aleksandr Podkopaev, Aaditya Ramdas</p>
<p><a href='https://openreview.net/forum?id=bN1ZBSOV2f'>https://openreview.net/forum?id=bN1ZBSOV2f</a></p>
<p><b>Keywords</b>: two-sample testing, independence testing, testing by betting, sequential testing
</p><p><b>Compressor summary</b>: The text discusses sequential nonparametric two-sample and independence testing using prediction-based betting strategies that work well on structured high-dimensional data and adapt to changing data distributions.</p><hr><h3>Function Space Bayesian Pseudocoreset for Bayesian Neural Networks</h3>
<p>Balhae Kim, Hyungi Lee, Juho Lee</p>
<p><a href='https://openreview.net/forum?id=bM6mynsusR'>https://openreview.net/forum?id=bM6mynsusR</a></p>
<p><b>Keywords</b>: Bayesian pseudocoresets, Function space variational inference
</p><p><b>Compressor summary</b>: The paper proposes a new method to create a compact synthetic dataset for scalable Bayesian inference that works on function space instead of weight space, improving uncertainty quantification and robustness.</p><hr><h3>Greatness in Simplicity: Unified Self-Cycle Consistency for Parser-Free Virtual Try-On</h3>
<p>Chenghu Du, junyin Wang, Shuqing Liu, Shengwu Xiong</p>
<p><a href='https://openreview.net/forum?id=bLB4vTwSbC'>https://openreview.net/forum?id=bLB4vTwSbC</a></p>
<p><b>Keywords</b>: parser-free virtual try-on, self-cycle consistency, human analysis and understanding, fashion synthesis, Markov Random Field
</p><p><b>Compressor summary</b>: The paragraph discusses a new parser-free virtual try-on network (USC-PFN) that improves garment deformation modeling and alignment with the human body using self-cycle consistency and Markov Random Field, achieving state-of-the-art performance.</p><hr><h3>Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts</h3>
<p>Pritam Sarkar, Ahmad Beirami, Ali Etemad</p>
<p><a href='https://openreview.net/forum?id=bKqrWLCMrX'>https://openreview.net/forum?id=bKqrWLCMrX</a></p>
<p><b>Keywords</b>: computer vision, self-supervised learning, video self-supervised learning, natural distribution shift, video learning, out-of-distribution generalization
</p><p><b>Compressor summary</b>: The paper investigates how six self-supervised video methods handle different types of distribution shifts and reveals their strengths and weaknesses in various scenarios.</p><hr><h3>Deep Optimal Transport: A Practical Algorithm for Photo-realistic Image Restoration</h3>
<p>Theo Joseph Adrai, Guy Ohayon, Michael Elad, Tomer Michaeli</p>
<p><a href='https://openreview.net/forum?id=bJJY9TFfe0'>https://openreview.net/forum?id=bJJY9TFfe0</a></p>
<p><b>Keywords</b>: Computer Vision, Image Restoration, Deep Learning, Perceptual Quality
</p><p><b>Compressor summary</b>: The algorithm improves image restoration by using optimal transport in the latent space of a variational auto-encoder to control the trade-off between perceptual quality and MSE, and it works well with few training images.</p><hr><h3>Neural Lad: A Neural Latent Dynamics Framework for Times Series Modeling</h3>
<p>Ting Li, Jianguo Li, Zhanxing Zhu</p>
<p><a href='https://openreview.net/forum?id=bISkJSa5Td'>https://openreview.net/forum?id=bISkJSa5Td</a></p>
<p><b>Keywords</b>: Neural CDE, Time-series forecasting, Latent Dynamic
</p><p><b>Compressor summary</b>: Neural Lad is a new neural ODE framework that addresses issues in existing models by incorporating local change and seasonality in latent dynamics and achieving better performance in various time series forecasting tasks.</p><hr><h3>Riemannian Laplace approximations for Bayesian neural networks</h3>
<p>Federico Bergamin, Pablo Moreno-Muñoz, Søren Hauberg, Georgios Arvanitidis</p>
<p><a href='https://openreview.net/forum?id=bHS7qjLOAy'>https://openreview.net/forum?id=bHS7qjLOAy</a></p>
<p><b>Keywords</b>: Riemannian geometry, Laplace approximation, Approximate inference, Bayesian neural networks
</p><p><b>Compressor summary</b>: The paper proposes a Riemannian Laplace approximation for Bayesian neural networks that adapts to the true posterior's shape and improves performance across tasks.</p><hr><h3>Asymmetric Certified Robustness via Feature-Convex Neural Networks</h3>
<p>Samuel Pfrommer, Brendon G. Anderson, Julien Piet, Somayeh Sojoudi</p>
<p><a href='https://openreview.net/forum?id=bH4LVNVXUo'>https://openreview.net/forum?id=bH4LVNVXUo</a></p>
<p><b>Keywords</b>: asymmetric certified robustness, input-convex neural networks
</p><p><b>Compressor summary</b>: The paper introduces a new neural network architecture for machine learning models to resist asymmetric adversarial attacks, which induce false negatives, and proves its effectiveness theoretically and empirically.</p><hr><h3>FourierGNN: Rethinking Multivariate Time Series Forecasting from a Pure Graph Perspective</h3>
<p>Kun Yi, Qi Zhang, Wei Fan, Hui He, Liang Hu, Pengyang Wang, Ning An, Longbing Cao, Zhendong Niu</p>
<p><a href='https://openreview.net/forum?id=bGs1qWQ1Fx'>https://openreview.net/forum?id=bGs1qWQ1Fx</a></p>
<p><b>Keywords</b>: multivariate time series forecasting, fourier space
</p><p><b>Compressor summary</b>: The authors propose a novel graph neural network architecture, Fourier Graph Neural Network (FouderGNN), that uses a hypervariate graph data structure to capture spatiotemporal dynamics in multivariate time series forecasting and outperforms existing methods with lower complexity.</p><hr><h3>ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation</h3>
<p>Zhitong Gao, Shipeng Yan, Xuming He</p>
<p><a href='https://openreview.net/forum?id=bGcdjXrU2w'>https://openreview.net/forum?id=bGcdjXrU2w</a></p>
<p><b>Keywords</b>: ood detection, semantic segmentation, anomaly segmentation, test-time adaptation
</p><p><b>Compressor summary</b>: The paper proposes a dual-level framework to detect out-of-distribution images that can handle both domain shift and semantic shift, improving OOD detection performance on various benchmarks.</p><hr><h3>Differentiable Neuro-Symbolic Reasoning on Large-Scale Knowledge Graphs</h3>
<p>CHEN SHENGYUAN, YUNFENG CAI, Huang Fang, Xiao Huang, Mingming Sun</p>
<p><a href='https://openreview.net/forum?id=bETvUctiTR'>https://openreview.net/forum?id=bETvUctiTR</a></p>
<p><b>Keywords</b>: Neuro-Symbolic Reasoning, Knowledge graph embedding, Probabilistic soft logic
</p><p><b>Compressor summary</b>: DiffLogic is a neuro-symbolic reasoning framework that uses a tailored filter to select essential triples and probabilistic soft logic to assess agreement among rules, weights, and observed triples for effective and efficient knowledge graph reasoning.</p><hr><h3>A Bayesian Take on Gaussian Process Networks</h3>
<p>Enrico Giudice, Jack Kuipers, Giusi Moffa</p>
<p><a href='https://openreview.net/forum?id=bBIHqoZ3OR'>https://openreview.net/forum?id=bBIHqoZ3OR</a></p>
<p><b>Keywords</b>: Bayesian networks, structure learning, graphical models, gaussian processes, Bayesian inference, MCMC sampling, importance sampling
</p><p><b>Compressor summary</b>: The paragraph explains how Gaussian Process Networks (GPNs) model continuous distributions with flexible Graphical Models, but their Bayesian structure learning is hard to compute, so the authors propose a Monte Carlo method that samples from the posterior distribution and performs well in simulations.</p><hr><h3>Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack</h3>
<p>Pratik Karmakar, Debabrota Basu</p>
<p><a href='https://openreview.net/forum?id=bAI21VEMvM'>https://openreview.net/forum?id=bAI21VEMvM</a></p>
<p><b>Keywords</b>: Differential Privacy, Model Extraction Attacks, Active Sampling, Max-Information Attack
</p><p><b>Compressor summary</b>: The authors propose a novel black-box model extraction attack called Marich that uses active sampling to select queries from public data and create an accurate replica of the target ML model.</p><hr><h3>A case for reframing automated medical image classification as segmentation</h3>
<p>Sarah Hooper, Mayee F Chen, Khaled Kamal Saab, Kush Bhatia, Curtis Langlotz, Christopher Re</p>
<p><a href='https://openreview.net/forum?id=b8xowIlZ7v'>https://openreview.net/forum?id=b8xowIlZ7v</a></p>
<p><b>Keywords</b>: Medical imaging, segmentation, classification
</p><p><b>Compressor summary</b>: The paper compares classification and segmentation models for medical image analysis, proposes methods to use segmentation models for classification, and discusses the benefits of switching from segmentation to classification in terms of performance, robustness, and interpretability.</p><hr><h3>One-Step Diffusion Distillation via Deep Equilibrium Models</h3>
<p>Zhengyang Geng, Ashwini Pokle, J Zico Kolter</p>
<p><a href='https://openreview.net/forum?id=b6XvK2de99'>https://openreview.net/forum?id=b6XvK2de99</a></p>
<p><b>Keywords</b>: Deep Equilibrium Models, Diffusion Models, Distillation, Generative Models
</p><p><b>Compressor summary</b>: The paper introduces the GET model, which distills diffusion models directly from noise to image using an offline training approach with better performance than existing one-step methods.</p><hr><h3>Convergence of Alternating Gradient Descent for Matrix Factorization</h3>
<p>Rachel Ward, Tamara G. Kolda</p>
<p><a href='https://openreview.net/forum?id=b6FeLpKKjl'>https://openreview.net/forum?id=b6FeLpKKjl</a></p>
<p><b>Keywords</b>: matrix factorization; gradient descent; global convergence; concentration; optimization
</p><p><b>Compressor summary</b>: The paper studies alternating gradient descent with fixed step size for asymmetric matrix factorization, showing that a specific number of iterations is enough to reach an approximate solution with high probability, starting from a certain random initialization, and experiments confirm the theoretical results.</p><hr><h3>On the Robustness of Removal-Based Feature Attributions</h3>
<p>Chris Lin, Ian Connick Covert, Su-In Lee</p>
<p><a href='https://openreview.net/forum?id=b60wLlkBta'>https://openreview.net/forum?id=b60wLlkBta</a></p>
<p><b>Keywords</b>: explainable artificial intelligence, interpretable machine learning, feature attributions, removal-based feature attributions, robustness
</p><p><b>Compressor summary</b>: The paper studies how removal-based feature attribution methods can be more robust to input and model perturbations and provides a unified analysis with theoretical and empirical results.</p><hr><h3>A Heavy-Tailed Algebra for Probabilistic Programming</h3>
<p>Feynman T. Liang, Liam Hodgkinson, Michael W. Mahoney</p>
<p><a href='https://openreview.net/forum?id=b5R8mbqo9Q'>https://openreview.net/forum?id=b5R8mbqo9Q</a></p>
<p><b>Keywords</b>: probabilistic programming, static analysis, heavy tails, monte carlo, mcmc, variational inference
</p><p><b>Compressor summary</b>: The paper proposes a method to improve tail behavior accuracy in probabilistic models using noise in neural networks, based on an algebra using the generalized Gamma distribution.</p><hr><h3>LightSpeed: Light and Fast Neural Light Fields on Mobile Devices</h3>
<p>Aarush Gupta, Junli Cao, Chaoyang Wang, Ju Hu, Sergey Tulyakov, Jian Ren, Laszlo Attila Jeni</p>
<p><a href='https://openreview.net/forum?id=b2wSODM7iG'>https://openreview.net/forum?id=b2wSODM7iG</a></p>
<p><b>Keywords</b>: light field, neural radiance field, novel view synthesis
</p><p><b>Compressor summary</b>: The authors propose a new method for real-time image synthesis on mobile devices using the light slab representation, which improves both quality and speed compared to existing neural light field methods.</p><hr><h3>Stable Nonconvex-Nonconcave Training via Linear Interpolation</h3>
<p>Thomas Pethick, Wanyun Xie, Volkan Cevher</p>
<p><a href='https://openreview.net/forum?id=b1JPBGJhUi'>https://openreview.net/forum?id=b1JPBGJhUi</a></p>
<p><b>Keywords</b>: Minimax optimization, Lookahead, Generative adversarial networks, Stability, Nonconvex-nonconcave, Cohypomonotone
</p><p><b>Compressor summary</b>: The paper proposes a new optimization method called RAPP that uses linear interpolation to stabilize neural network training, and extends it to constrained and regularized settings, as well as to Lookahead algorithms, with experiments on generative adversarial networks showing its benefits.</p><hr><h3>Sharp Recovery Thresholds of Tensor PCA Spectral Algorithms</h3>
<p>Michael Jacob Feldman, David Donoho</p>
<p><a href='https://openreview.net/forum?id=b1BhHjBxsx'>https://openreview.net/forum?id=b1BhHjBxsx</a></p>
<p><b>Keywords</b>: tensor PCA, spectral algorithms, random matrix theory
</p><p><b>Compressor summary</b>: The paper studies different ways to convert noisy tensors into matrices and uses spectral methods to recover the low-rank signal, proving that some methods work better than others in certain conditions.</p><hr><h3>On the Ability of Graph Neural Networks to Model Interactions Between Vertices</h3>
<p>Noam Razin, Tom Verbin, Nadav Cohen</p>
<p><a href='https://openreview.net/forum?id=ayZpFoAu5c'>https://openreview.net/forum?id=ayZpFoAu5c</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Expressivity, Interactions, Edge Sparsification
</p><p><b>Compressor summary</b>: The paper proposes a measure to quantify interaction strength in graph neural networks (GNNs) and introduces an edge sparsification method that preserves interaction ability while reducing computation.</p><hr><h3>Experiment Planning with Function Approximation</h3>
<p>Aldo Pacchiano, Jonathan Lee, Emma Brunskill</p>
<p><a href='https://openreview.net/forum?id=axmY49ahVI'>https://openreview.net/forum?id=axmY49ahVI</a></p>
<p><b>Keywords</b>: regret, model selection, planning, static, lower bound
</p><p><b>Compressor summary</b>: The paper proposes two experiment planning strategies for function approximation in contextual bandits using large datasets of contexts but no rewards, and analyzes their optimality guarantees and differences from adaptive learning.</p><hr><h3>Greedy Poisson Rejection Sampling</h3>
<p>Gergely Flamich</p>
<p><a href='https://openreview.net/forum?id=axRMkinASf'>https://openreview.net/forum?id=axRMkinASf</a></p>
<p><b>Keywords</b>: channel simulation, relative entropy coding, reverse channel coding, rejection sampling, Poisson process
</p><p><b>Compressor summary</b>: The paper presents a new algorithm (GPRS) for efficient one-shot channel simulation in one-dimensional problems with unimodal density ratio and shows its superior performance over existing methods.</p><hr><h3>Characterization of Overfitting in Robust Multiclass Classification</h3>
<p>Jingyuan Xu, Weiwei Liu</p>
<p><a href='https://openreview.net/forum?id=awbWWO0nb6'>https://openreview.net/forum?id=awbWWO0nb6</a></p>
<p><b>Keywords</b>: Learning Theory
</p><p><b>Compressor summary</b>: The paper investigates how adaptive algorithms may overfit the test dataset in multiclass classification, with bounds on the overfitting bias.</p><hr><h3>LEACE: Perfect linear concept erasure in closed form</h3>
<p>Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, Stella Biderman</p>
<p><a href='https://openreview.net/forum?id=awIpKpwTwF'>https://openreview.net/forum?id=awIpKpwTwF</a></p>
<p><b>Keywords</b>: interpretability, fairness, concept erasure, representation, adversarial, robustness
</p><p><b>Compressor summary</b>: LEACE is a method to remove specific features from representations while minimizing changes, and it can be applied to language models for fairness and interpretability tasks.</p><hr><h3>Risk-Averse Active Sensing for Timely Outcome Prediction under Cost Pressure</h3>
<p>Yuchao Qin, Mihaela van der Schaar, Changhee Lee</p>
<p><a href='https://openreview.net/forum?id=aw1vLo7TE7'>https://openreview.net/forum?id=aw1vLo7TE7</a></p>
<p><b>Keywords</b>: active sensing, value of information, risk-averse learning
</p><p><b>Compressor summary</b>: The paper proposes RAS, a novel risk-averse active sensing approach for cost-efficient acquisition of patient covariates in healthcare to enable timely and accurate outcome predictions.</p><hr><h3>Discovering Intrinsic Spatial-Temporal Logic Rules to Explain Human Actions</h3>
<p>Chengzhi Cao, Chao Yang, Ruimao Zhang, Shuang Li</p>
<p><a href='https://openreview.net/forum?id=avuRopYsCg'>https://openreview.net/forum?id=avuRopYsCg</a></p>
<p><b>Keywords</b>: Logic rule, human actions, sports analyze
</p><p><b>Compressor summary</b>: The paper proposes a model that uses spatial-temporal logic rules to capture human movement patterns based on intentions and environmental factors, with potential applications in sports analytics, robotics, and autonomous cars.</p><hr><h3>Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement</h3>
<p>Avani Gupta, Saurabh Saini, P J Narayanan</p>
<p><a href='https://openreview.net/forum?id=arkmhtYLL6'>https://openreview.net/forum?id=arkmhtYLL6</a></p>
<p><b>Keywords</b>: Human Centered Concepts, ML interpretability, XAI based Model Improvement, Debiasing
</p><p><b>Compressor summary</b>: The authors propose CAVs for ante-hoc training with Concept Loss and Concept Distillation to make models more sensitive and robust to concepts, which enhances interpretability and knowledge transfer.</p><hr><h3>LICO: Explainable Models with Language-Image COnsistency</h3>
<p>Yiming Lei, Zilong Li, Yangyang Li, Junping Zhang, Hongming Shan</p>
<p><a href='https://openreview.net/forum?id=apjOYp3mOa'>https://openreview.net/forum?id=apjOYp3mOa</a></p>
<p><b>Keywords</b>: Language-image consistency, prompt learning, image classification, CNN interpretation
</p><p><b>Compressor summary</b>: The paper proposes LICO, a model that uses linguistic prompts and optimal transport theory to generate more accurate and interpretable attention maps for image classification tasks.</p><hr><h3>FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow</h3>
<p>Cameron Omid Smith, Yilun Du, Ayush Tewari, Vincent Sitzmann</p>
<p><a href='https://openreview.net/forum?id=apFDDJOYf5'>https://openreview.net/forum?id=apFDDJOYf5</a></p>
<p><b>Keywords</b>: Pose Estimation, Scene Flow Estimation, Scene Representation Learning, Computer Vision, Neural Implicit Representations, Neural Radiance Fields, View Synthesis, Self-Supervised Representation Learning
</p><p><b>Compressor summary</b>: The authors propose a self-supervised method for learning 3D neural scenes from videos by jointly estimating camera poses and reconstructing the scene in a single forward pass.</p><hr><h3>Factorized Contrastive Learning: Going Beyond Multi-view Redundancy</h3>
<p>Paul Pu Liang, Zihao Deng, Martin Q. Ma, James Zou, Louis-Philippe Morency, Russ Salakhutdinov</p>
<p><a href='https://openreview.net/forum?id=alLs7EtRJP'>https://openreview.net/forum?id=alLs7EtRJP</a></p>
<p><b>Keywords</b>: multimodal learning, contrastive learning, self-supervised learning, information theory
</p><p><b>Compressor summary</b>: FactorCL is a new method for learning multimodal representations that captures both shared and unique information relevant to downstream tasks using factorization, maximizing mutual information lower bounds, minimizing mutual information upper bounds, and data augmentations.</p><hr><h3>Decompose a Task into Generalizable Subtasks in Multi-Agent Reinforcement Learning</h3>
<p>Zikang Tian, Ruizhi Chen, Xing Hu, Ling Li, Rui Zhang, Fan Wu, Shaohui Peng, Jiaming Guo, Zidong Du, Qi Guo, Yunji Chen</p>
<p><a href='https://openreview.net/forum?id=aky0dKv9ip'>https://openreview.net/forum?id=aky0dKv9ip</a></p>
<p><b>Keywords</b>: Multi-Agent Reinforcement Learning, Transfer Learning, Zero-Shot Generalization
</p><p><b>Compressor summary</b>: The paper proposes a novel framework called DT2GS that decomposes MARL tasks into generalizable subtasks using a scalable encoder and an adaptive semantic module to achieve zero-shot generalization and task transferability.</p><hr><h3>Improving Robustness with Adaptive Weight Decay</h3>
<p>Amin Ghiasi, Ali Shafahi, Reza Ardekani</p>
<p><a href='https://openreview.net/forum?id=ajnThDhuq6'>https://openreview.net/forum?id=ajnThDhuq6</a></p>
<p><b>Keywords</b>: Adaptive weight decay, adversarial robustness, weight decay, robust overfitting, overfitting, adversarial attacks, noisy label
</p><p><b>Compressor summary</b>: The paper proposes adaptive weight decay that adjusts the regularization strength during training to improve adversarial robustness without extra data or tuning.</p><hr><h3>Learning Mixtures of Gaussians Using the DDPM Objective</h3>
<p>Kulin Shah, Sitan Chen, Adam Klivans</p>
<p><a href='https://openreview.net/forum?id=aig7sgdRfI'>https://openreview.net/forum?id=aig7sgdRfI</a></p>
<p><b>Keywords</b>: Mixtures of Gaussians, score-based generative models, provable learning of score, Expectation-Maximization, DDPM generative model
</p><p><b>Compressor summary</b>: The paper proves that gradient descent on a specific diffusion model can efficiently learn Gaussian mixture models with different levels of initialization and separation.</p><hr><h3>Cross-Episodic Curriculum for Transformer Agents</h3>
<p>Lucy Xiaoyang Shi, Yunfan Jiang, Jake Grigsby, Linxi Fan, Yuke Zhu</p>
<p><a href='https://openreview.net/forum?id=afKnrwJBAl'>https://openreview.net/forum?id=afKnrwJBAl</a></p>
<p><b>Keywords</b>: Transformers, In-context Learning, Reinforcement Learning, Robotics
</p><p><b>Compressor summary</b>: CEC is a new algorithm that improves Transformer agents' learning efficiency and generalization by sequentially structuring online learning trials and mixed-quality demonstrations, resulting in better performance and strong generalization across episodes.</p><hr><h3>RanPAC: Random Projections and Pre-trained Models for Continual Learning</h3>
<p>Mark McDonnell, Dong Gong, Amin Parvaneh, Ehsan Abbasnejad, Anton van den Hengel</p>
<p><a href='https://openreview.net/forum?id=aec58UfBzA'>https://openreview.net/forum?id=aec58UfBzA</a></p>
<p><b>Keywords</b>: continual learning, class incremental learning, domain incremental learning, pre-trained models, parameter-efficient transfer learning
</p><p><b>Compressor summary</b>: The paper proposes a new approach for continual learning with pre-trained models that uses random projectors and class-prototype accumulation to avoid forgetting without rehearsal memory.</p><hr><h3>Tree Variational Autoencoders</h3>
<p>Laura Manduchi, Moritz Vandenhirtz, Alain Ryser, Julia E Vogt</p>
<p><a href='https://openreview.net/forum?id=adq0oXb9KM'>https://openreview.net/forum?id=adq0oXb9KM</a></p>
<p><b>Keywords</b>: hierarchical clustering, hierarchical VAE, representation learning, VAE, deep clustering
</p><p><b>Compressor summary</b>: TreeVAE is a new model that learns a tree structure to encode latent variables and discover hidden clusters in data, while improving generative performance and allowing conditional sampling.</p><hr><h3>Adapting to Continuous Covariate Shift via Online Density Ratio Estimation</h3>
<p>Yu-Jie Zhang, Zhen-Yu Zhang, Peng Zhao, Masashi Sugiyama</p>
<p><a href='https://openreview.net/forum?id=ad3JNoR2np'>https://openreview.net/forum?id=ad3JNoR2np</a></p>
<p><b>Keywords</b>: Covariate Shift, Density Ratio Estimation, Online Convex Optimization, Dynamic Regret, Logistic Regression
</p><p><b>Compressor summary</b>: The paper studies continuous covariate shift in machine learning, proposes an online density ratio estimation method to adaptively train predictors, and provides theoretical and empirical guarantees.</p><hr><h3>Mixed Samples as Probes for Unsupervised Model Selection in Domain Adaptation</h3>
<p>Dapeng Hu, Jian Liang, Jun Hao Liew, Chuhui Xue, Song Bai, Xinchao Wang</p>
<p><a href='https://openreview.net/forum?id=ackajXqei2'>https://openreview.net/forum?id=ackajXqei2</a></p>
<p><b>Keywords</b>: Unsupervised Domain Adaptation; Model Selection; Hyperparameter Selection; Unsupervised Validation;
</p><p><b>Compressor summary</b>: MixVal is a novel model selection method for unsupervised domain adaptation that uses mixed target samples with pseudo labels to evaluate the performance of different UDA models on unlabeled target data.</p><hr><h3>MarioGPT: Open-Ended Text2Level Generation through Large Language Models</h3>
<p>Shyam Sudhakaran, Miguel González-Duque, Matthias Freiberger, Claire Glanois, Elias Najarro, Sebastian Risi</p>
<p><a href='https://openreview.net/forum?id=aa8KsqfTPa'>https://openreview.net/forum?id=aa8KsqfTPa</a></p>
<p><b>Keywords</b>: Large Language Models, Procedural Content Generation, Open-endedness, Novelty Search
</p><p><b>Compressor summary</b>: MarioGPT is a text-prompted, fine-tuned GPT2 model that generates diverse Super Mario Bros levels and can be controlled by user input to address challenges in procedural content generation.</p><hr><h3>Anchor Data Augmentation</h3>
<p>Nora Schneider, Shirin Goshtasbpour, Fernando Perez-Cruz</p>
<p><a href='https://openreview.net/forum?id=aZ9hvpnp0k'>https://openreview.net/forum?id=aZ9hvpnp0k</a></p>
<p><b>Keywords</b>: Data Augmentation, Regression, Deep Learning
</p><p><b>Compressor summary</b>: The paper introduces a new data augmentation method for nonlinear regression called Anchor Data Augmentation, which uses causality and distributionally robust Anchor regression to generate more training examples and improve prediction accuracy.</p><hr><h3>Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests</h3>
<p>Edward Raff, James Holt</p>
<p><a href='https://openreview.net/forum?id=aZ44Na3l9p'>https://openreview.net/forum?id=aZ44Na3l9p</a></p>
<p><b>Keywords</b>: reproducibility; multiple instance learning
</p><p><b>Compressor summary</b>: The paragraph discusses how five deep-MIL models do not follow the standard assumption of Multiple Instance Learning (MIL) and may lead to incorrect predictions in various applications like healthcare and cyber security.</p><hr><h3>Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning</h3>
<p>Xiaoming Shi, Siqiao Xue, Kangrui Wang, Fan Zhou, James Y. Zhang, JUN ZHOU, Chenhao Tan, Hongyuan Mei</p>
<p><a href='https://openreview.net/forum?id=aW9BqtRQkh'>https://openreview.net/forum?id=aW9BqtRQkh</a></p>
<p><b>Keywords</b>: event sequences, irregular time series, event prediction, large language model, reasoning, few-shot prompting
</p><p><b>Compressor summary</b>: The paper proposes LAMP, a framework that uses a large language model for abductive reasoning to improve event prediction performance on real-world datasets.</p><hr><h3>Drift doesn't Matter: Dynamic Decomposition with Diffusion Reconstruction for Unstable Multivariate Time Series Anomaly Detection</h3>
<p>Chengsen Wang, Zirui Zhuang, Qi Qi, Jingyu Wang, Xingyu Wang, Haifeng Sun, Jianxin Liao</p>
<p><a href='https://openreview.net/forum?id=aW5bSuduF1'>https://openreview.net/forum?id=aW5bSuduF1</a></p>
<p><b>Keywords</b>: Anomaly Detection, Time Series, Diffusion, Transformer
</p><p><b>Compressor summary</b>: D$^3$R is an anomaly detection network for unstable data that uses dynamic decomposition and reconstruction to handle drift and achieve better performance than existing methods.</p><hr><h3>A Dynamical System View of Langevin-Based Non-Convex Sampling</h3>
<p>Mohammad Reza Karimi Jaghargh, Ya-Ping Hsieh, Andreas Krause</p>
<p><a href='https://openreview.net/forum?id=aRBa0lSxEB'>https://openreview.net/forum?id=aRBa0lSxEB</a></p>
<p><b>Keywords</b>: Non-Convex Sampling, Langevin Dynamics, Dynamical Systems
</p><p><b>Compressor summary</b>: The paragraph discusses a novel framework for addressing non-convex sampling challenges in machine learning using tools from dynamical systems theory to improve convergence guarantees.</p><hr><h3>Scale-Space Hypernetworks for Efficient Biomedical Image Analysis</h3>
<p>Jose Javier Gonzalez Ortiz, John Guttag, Adrian V Dalca</p>
<p><a href='https://openreview.net/forum?id=aN0llPIbdg'>https://openreview.net/forum?id=aN0llPIbdg</a></p>
<p><b>Keywords</b>: hypernetworks, amortized learning, computer vision, rescaling, convolutional neural networks, pareto efficiency
</p><p><b>Compressor summary</b>: The authors propose Scale-Space HyperNetworks (SSHN), a method that learns a spectrum of CNNs with varying internal rescaling factors, allowing for better accuracy-efficiency trade-offs in medical image analysis tasks.</p><hr><h3>The emergence of clusters in self-attention dynamics</h3>
<p>Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, Philippe Rigollet</p>
<p><a href='https://openreview.net/forum?id=aMjaEkkXJx'>https://openreview.net/forum?id=aMjaEkkXJx</a></p>
<p><b>Keywords</b>: Transformers, Self-Attention, Clustering, Interacting Particle Systems, Continuous Time
</p><p><b>Compressor summary</b>: The paragraph discusses how learned representations in Transformers form clusters and limiting objects depending on the value matrix spectrum, and proves convergence to low-rank Boolean matrices in one dimension.</p><hr><h3>FourierHandFlow: Neural 4D Hand Representation Using Fourier Query Flow</h3>
<p>Jihyun Lee, Junbong Jang, Donghwan Kim, Minhyuk Sung, Tae-Kyun Kim</p>
<p><a href='https://openreview.net/forum?id=aMTiwdK3y8'>https://openreview.net/forum?id=aMTiwdK3y8</a></p>
<p><b>Keywords</b>: 4D representation, hand reconstruction, implicit representation
</p><p><b>Compressor summary</b>: FourierHandFlow is a novel method for learning spatio-temporally continuous representations of human hands from RGB videos using Fourier series and implicit shape priors.</p><hr><h3>Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</h3>
<p>Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg</p>
<p><a href='https://openreview.net/forum?id=aLLuYpn83y'>https://openreview.net/forum?id=aLLuYpn83y</a></p>
<p><b>Keywords</b>: Large Language Model, AI Safety
</p><p><b>Compressor summary</b>: ITI is a technique that improves the truthfulness of large language models by shifting model activations during inference based on a learned set of directions.</p><hr><h3>Learning to Modulate pre-trained Models in RL</h3>
<p>Thomas Schmied, Markus Hofmarcher, Fabian Paischer, Razvan Pascanu, Sepp Hochreiter</p>
<p><a href='https://openreview.net/forum?id=aIpGtPwXny'>https://openreview.net/forum?id=aIpGtPwXny</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Transformer, Decision Transformer, Multi-task learning, Continual learning, NLP, Fine-tuning, Prompt Tuning, Parameter efficient Fine-tuning
</p><p><b>Compressor summary</b>: The paper investigates catastrophic forgetting in reinforcement learning, compares various fine-tuning methods, and proposes a novel method called Learning-to-Modulate (L2M) that preserves pre-training skills while adapting to new tasks.</p><hr><h3>Feature Adaptation for Sparse Linear Regression</h3>
<p>Jonathan Kelner, Frederic Koehler, Raghu Meka, Dhruv Rohatgi</p>
<p><a href='https://openreview.net/forum?id=aIUnoHuENG'>https://openreview.net/forum?id=aIUnoHuENG</a></p>
<p><b>Keywords</b>: theory, sparse linear regression, feature adaptation, lasso
</p><p><b>Compressor summary</b>: The paper presents an algorithm that adapts the Lasso to handle sparse linear regression with correlated design and ill-conditioned covariates, achieving near-optimal sample complexity.</p><hr><h3>CS4ML: A general framework for active learning with arbitrary data based on Christoffel functions</h3>
<p>Juan M Cardenas, Ben Adcock, Nick Dexter</p>
<p><a href='https://openreview.net/forum?id=aINqoP32cb'>https://openreview.net/forum?id=aINqoP32cb</a></p>
<p><b>Keywords</b>: active learning, regression, arbitrary data, leverage scores, Christoffel functions, generative models, Magnetic Resonance Imaging (MRI), Physics-Informed Neural Networks (PINNs)
</p><p><b>Compressor summary</b>: The paper presents a general active learning framework for regression problems that can handle different types of data and shows its effectiveness in scientific computing applications.</p><hr><h3>Enhancing Adaptive History Reserving by Spiking Convolutional Block Attention Module in Recurrent Neural Networks</h3>
<p>Qi Xu, Yuyuan Gao, Jiangrong Shen, Yaxin Li, Xuming Ran, Huajin Tang, Gang Pan</p>
<p><a href='https://openreview.net/forum?id=aGZp61S9Lj'>https://openreview.net/forum?id=aGZp61S9Lj</a></p>
<p><b>Keywords</b>: Spiking neural networks (SNNs), Recurrent spiking neural network (RSNN), Dynamic Vision Sensor (DVS), Spiking convolutional block attention module (SCBAM)
</p><p><b>Compressor summary</b>: The paper proposes a recurrent spiking neural network with an attention module to process spatio-temporal patterns in time series data by combining both spatial and temporal features efficiently.</p><hr><h3>Optimal Unbiased Randomizers for Regression with Label Differential Privacy</h3>
<p>Ashwinkumar Badanidiyuru, Badih Ghazi, Pritish Kamath, Ravi Kumar, Ethan Jacob Leeman, Pasin Manurangsi, Avinash V Varadarajan, Chiyuan Zhang</p>
<p><a href='https://openreview.net/forum?id=aG6xOP9QY7'>https://openreview.net/forum?id=aG6xOP9QY7</a></p>
<p><b>Keywords</b>: label differential privacy
</p><p><b>Compressor summary</b>: We propose a new family of label randomizers for  training _regression_ models under the constraint of label differential privacy (DP). In particular, we leverage the trade-offs between bias and variance to construct better label randomizers depending on a privately estimated prior distribution over the labels. We demonstrate that these randomizers achieve state-of-the-art privacy-utility trade-offs on several datasets, highlighting the importance of reducing bias when training neural networks with label DP. We also provide theoretical results shedding light on the structural properties of the optimal unbiased randomizers.</p><hr><h3>Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning</h3>
<p>Stefan Stojanovic, Yassir Jedra, Alexandre Proutiere</p>
<p><a href='https://openreview.net/forum?id=aDLmRMb0K9'>https://openreview.net/forum?id=aDLmRMb0K9</a></p>
<p><b>Keywords</b>: Low-rank matrix estimation; low rank bandits; low rank MDP; spectral methods
</p><p><b>Compressor summary</b>: The paper explores matrix estimation methods with low entry-wise prediction error in low-rank reinforcement learning problems, and presents two new algorithms with near-optimal performance.</p><hr><h3>Polynomial-Time Linear-Swap Regret Minimization in Imperfect-Information Sequential Games</h3>
<p>Gabriele Farina, Charilaos Pipis</p>
<p><a href='https://openreview.net/forum?id=aCOKUvqHtD'>https://openreview.net/forum?id=aCOKUvqHtD</a></p>
<p><b>Keywords</b>: online learning, algorithmic game theory, extensive form games, correlated equilibrium, swap regret, linear swap regret
</p><p><b>Compressor summary</b>: The paragraph discusses how no-linear-swap regret learners can achieve sublinear regret in sequential games by considering all linear transformations of the mixed strategy space, and shows this leads to a stronger notion of hindsight rationality than previous ones.</p><hr><h3>On the Convergence of CART under Sufficient Impurity Decrease Condition</h3>
<p>Rahul Mazumder, Haoyue Wang</p>
<p><a href='https://openreview.net/forum?id=a2svOXTVgO'>https://openreview.net/forum?id=a2svOXTVgO</a></p>
<p><b>Keywords</b>: decision tree, CART
</p><p><b>Compressor summary</b>: The paper studies the convergence rate and error bound of CART, a machine-learning model, under a regression setting and introduces sufficient conditions for the SID condition.</p><hr><h3>Students Parrot Their Teachers: Membership Inference on Model Distillation</h3>
<p>Matthew Jagielski, Milad Nasr, Katherine Lee, Christopher A. Choquette-Choo, Nicholas Carlini, Florian Tramèr</p>
<p><a href='https://openreview.net/forum?id=a2Yg9Za6Rb'>https://openreview.net/forum?id=a2Yg9Za6Rb</a></p>
<p><b>Keywords</b>: model distillation, membership inference, privacy, dark knowledge
</p><p><b>Compressor summary</b>: The paper studies how well knowledge distillation protects privacy, and shows that it is not very effective against membership inference attacks, especially if the student and teacher models are similar or the attacker can tamper with the teacher data.</p><hr><h3>Training Chain-of-Thought via Latent-Variable Inference</h3>
<p>Matthew Douglas Hoffman, Du Phan, david dohan, Sholto Douglas, Tuan Anh Le, Aaron T Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, Rif A. Saurous</p>
<p><a href='https://openreview.net/forum?id=a147pIS2Co'>https://openreview.net/forum?id=a147pIS2Co</a></p>
<p><b>Keywords</b>: Large language models, latent-variable models, control variates, chain-of-thought, MCMC
</p><p><b>Compressor summary</b>: The paragraph discusses a new method to improve large language models' performance by combining chain-of-thought prompting and supervised fine-tuning, using a Markov-chain Monte Carlo expectation-maximization algorithm.</p><hr><h3>3D molecule generation by denoising voxel grids</h3>
<p>Pedro O. Pinheiro, Joshua Rackers, joseph Kleinhenz, Michael Maser, Omar Mahmood, Andrew Martin Watkins, Stephen Ra, Vishnu Sresht, Saeed Saremi</p>
<p><a href='https://openreview.net/forum?id=Zyzluw0hC4'>https://openreview.net/forum?id=Zyzluw0hC4</a></p>
<p><b>Keywords</b>: generative model, molecule generation, drug discovery
</p><p><b>Compressor summary</b>: The paragraph describes a new method called VoxMol that generates 3D molecules as atomic densities on grids by training a neural network to denoise noisy molecule distributions.</p><hr><h3>Rehearsal Learning for Avoiding Undesired Future</h3>
<p>Tian Qin, Tian-Zuo Wang, Zhi-Hua Zhou</p>
<p><a href='https://openreview.net/forum?id=ZwQJRXLjVm'>https://openreview.net/forum?id=ZwQJRXLjVm</a></p>
<p><b>Keywords</b>: decision-making, structural rehearsal model, Bayesian inference, probabilistic graphical model
</p><p><b>Compressor summary</b>: The paper introduces a rehearsal learning framework that helps find decisions to avoid undesired outcomes using structural rehearsal models, probabilistic graphical models, and a Bayesian approach with a risk quantification bound.</p><hr><h3>Thought Cloning: Learning to Think while Acting by Imitating Human Thinking</h3>
<p>Shengran Hu, Jeff Clune</p>
<p><a href='https://openreview.net/forum?id=ZvDmna23r3'>https://openreview.net/forum?id=ZvDmna23r3</a></p>
<p><b>Keywords</b>: Reinforcement learning, Imitation Learning, AI Safety, Interpretability
</p><p><b>Compressor summary</b>: Thought Cloning is a novel Imitation Learning framework that trains AI agents to not only mimic human behaviors but also the thoughts behind them, improving performance, safety, and interpretability.</p><hr><h3>Injecting Multimodal Information into Rigid Protein Docking via Bi-level Optimization</h3>
<p>Ruijia Wang, YiWu Sun, Yujie Luo, Shaochuan Li, Cheng Yang, Xingyi Cheng, Hui Li, Chuan Shi, Le Song</p>
<p><a href='https://openreview.net/forum?id=ZuaVKlWdD2'>https://openreview.net/forum?id=ZuaVKlWdD2</a></p>
<p><b>Keywords</b>: complex structure prediction, rigid docking, protein docking, antibody-antigen docking
</p><p><b>Compressor summary</b>: BiDock is a novel rigid protein docking model that uses both sequence and structure information to predict inter-protein distances for better predictions.</p><hr><h3>Differentially Private Approximate Near Neighbor Counting in High Dimensions</h3>
<p>Alexandr Andoni, Piotr Indyk, Sepideh Mahabadi, Shyam Narayanan</p>
<p><a href='https://openreview.net/forum?id=Zt9RzHjSEy'>https://openreview.net/forum?id=Zt9RzHjSEy</a></p>
<p><b>Keywords</b>: Differential Privacy, Near Neighbor Search, Locality Sensitive Hashing, Data Structures, Range Query
</p><p><b>Compressor summary</b>: The paper proposes an efficient algorithm for range counting under differential privacy that balances additive and multiplicative errors without depending on the dimension.</p><hr><h3>WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding</h3>
<p>Yanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, Carl Yang</p>
<p><a href='https://openreview.net/forum?id=ZrG8kTbt70'>https://openreview.net/forum?id=ZrG8kTbt70</a></p>
<p><b>Keywords</b>: Arributed graph, unsupervised graph learning, language models, representation learning
</p><p><b>Compressor summary</b>: The text describes an approach to create unsupervised graph embeddings using language models and random walks, which can handle complex attributes and structures in real-world graphs and improve performance on various downstream tasks.</p><hr><h3>Bypass Exponential Time Preprocessing: Fast Neural Network Training via Weight-Data Correlation Preprocessing</h3>
<p>Josh Alman, Jiehao Liang, Zhao Song, Ruizhe Zhang, Danyang Zhuo</p>
<p><a href='https://openreview.net/forum?id=ZqSx5vXOgC'>https://openreview.net/forum?id=ZqSx5vXOgC</a></p>
<p><b>Keywords</b>: Training neural network, Dynamic activated neuron detection, Sparsity, Fine-grained complexity, Data structure
</p><p><b>Compressor summary</b>: This paper proposes a preprocessing method to reduce the time needed to process each data point in deep neural networks by storing weight-data correlation in a tree structure.</p><hr><h3>SE(3) Diffusion Model-based Point Cloud Registration for Robust 6D Object Pose Estimation</h3>
<p>Haobo Jiang, Mathieu Salzmann, Zheng Dang, Jin Xie, Jian Yang</p>
<p><a href='https://openreview.net/forum?id=Znpz1sv4IP'>https://openreview.net/forum?id=Znpz1sv4IP</a></p>
<p><b>Keywords</b>: 6D object pose estimation, Point cloud registration, Diffusion probabilistic model
</p><p><b>Compressor summary</b>: The paper presents a new point cloud registration method using an SE(3) diffusion model for accurate 6D object pose estimation in real-world scenarios.</p><hr><h3>Time Series as Images: Vision Transformer for Irregularly Sampled Time Series</h3>
<p>Zekun Li, Shiyang Li, Xifeng Yan</p>
<p><a href='https://openreview.net/forum?id=ZmeAoWQqe0'>https://openreview.net/forum?id=ZmeAoWQqe0</a></p>
<p><b>Keywords</b>: irregularly sampled time series, vision transformer, healthcare, time series classification
</p><p><b>Compressor summary</b>: The paper proposes a new method to classify irregularly sampled time series by converting them into images and using pre-trained vision transformers, which shows strong performance and robustness against missing data.</p><hr><h3>Optimistic Meta-Gradients</h3>
<p>Sebastian Flennerhag, Tom Zahavy, Brendan O'Donoghue, Hado van Hasselt, András György, Satinder Singh</p>
<p><a href='https://openreview.net/forum?id=ZmSg4f16uo'>https://openreview.net/forum?id=ZmSg4f16uo</a></p>
<p><b>Keywords</b>: meta-learning, online optimisation, convex optimisation
</p><p><b>Compressor summary</b>: The paper explores how gradient-based meta-learning relates to convex optimization, proves convergence rates for meta learning with momentum, and shows that the Bootstrapped Meta-Gradient method captures optimism in meta-learning.</p><hr><h3>Collapsed Inference for Bayesian Deep Learning</h3>
<p>Zhe Zeng, Guy Van den Broeck</p>
<p><a href='https://openreview.net/forum?id=Zi1KKzh5Aj'>https://openreview.net/forum?id=Zi1KKzh5Aj</a></p>
<p><b>Keywords</b>: Bayesian Model Averaging, Weighted Model Integration, Bayesian Deep Learning, Collapsed Inference
</p><p><b>Compressor summary</b>: The paper introduces a novel inference method for Bayesian neural networks that improves both uncertainty estimation and predictive performance by using collapsed samples and volume computation solvers.</p><hr><h3>A Unified Conditional Framework for Diffusion-based Image Restoration</h3>
<p>Yi Zhang, Xiaoyu Shi, Dasong Li, Xiaogang Wang, Jian Wang, Hongsheng Li</p>
<p><a href='https://openreview.net/forum?id=ZgVJvaAS2h'>https://openreview.net/forum?id=ZgVJvaAS2h</a></p>
<p><b>Keywords</b>: image restoration, diffusion model, denoising, deblurring, JPEG restoration
</p><p><b>Compressor summary</b>: The paper introduces a conditional framework for image restoration using diffusion probabilistic models that integrates guidance from a lightweight UNet and produces high-quality results on challenging tasks.</p><hr><h3>Leveraging the two-timescale regime to demonstrate convergence of neural networks</h3>
<p>Pierre Marion, Raphaël Berthier</p>
<p><a href='https://openreview.net/forum?id=ZfFR4d5gUM'>https://openreview.net/forum?id=ZfFR4d5gUM</a></p>
<p><b>Keywords</b>: neural networks, non-convex optimization, gradient flow, convergence proof, two-timescale algorithm
</p><p><b>Compressor summary</b>: The study analyzes how shallow neural networks trained with different step sizes converge to a global optimum in a simple setting, showing that stochastic gradient descent works well when the stepsizes follow a two-timescale regime.</p><hr><h3>TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models</h3>
<p>Jiaqi Xue, Mengxin Zheng, Ting Hua, Yilin Shen, Yepeng Liu, Ladislau Bölöni, Qian Lou</p>
<p><a href='https://openreview.net/forum?id=ZejTutd7VY'>https://openreview.net/forum?id=ZejTutd7VY</a></p>
<p><b>Keywords</b>: Large Language Model, Trojan Attack, Adversary Attack, Prompt Injection, GPT-4, Black-box
</p><p><b>Compressor summary</b>: The paper introduces TrojLLM, a framework that can generate stealthy triggers to manipulate the outputs of large language models like GPT-3 and GPT-4 for malicious purposes.</p><hr><h3>Local Convergence of Gradient Methods for Min-Max Games: Partial Curvature Generically Suffices</h3>
<p>Guillaume Wang, Lénaïc Chizat</p>
<p><a href='https://openreview.net/forum?id=ZeRiLBvIps'>https://openreview.net/forum?id=ZeRiLBvIps</a></p>
<p><b>Keywords</b>: Gradient methods, min-max optimization, spectral analysis, last-iterate convergence
</p><p><b>Compressor summary</b>: The paper studies how gradient methods for two-player zero-sum differentiable games converge to local Nash equilibria and shows that they are faster when the game has partial curvature, which is related to the average eigenvalues of the symmetric part of the Jacobian.</p><hr><h3>SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning</h3>
<p>Yifan Yang, Peiyao Xiao, Kaiyi Ji</p>
<p><a href='https://openreview.net/forum?id=ZdxGmJGKOo'>https://openreview.net/forum?id=ZdxGmJGKOo</a></p>
<p><b>Keywords</b>: Federated bilevel optimization, federated hypergradient, communication efficiency, system-level heterogeneity, linear speedup
</p><p><b>Compressor summary</b>: The paper introduces SimFBO, a simple and efficient federated bilevel optimization framework, and its variant ShroFBO, which improve convergence speed, communication efficiency, and robustness to heterogeneity in machine learning and edge computing.</p><hr><h3>Optimal testing using combined test statistics across independent studies</h3>
<p>Lasse Vuursteen, Botond Szabo, Aad van der Vaart, Harry van Zanten</p>
<p><a href='https://openreview.net/forum?id=ZcuFDaMTYw'>https://openreview.net/forum?id=ZcuFDaMTYw</a></p>
<p><b>Keywords</b>: testing, meta-analysis, p-values, e-values, optimal, combining trials
</p><p><b>Compressor summary</b>: The paper investigates the power and limitations of combining test statistics from independent trials in high-dimensional models, and introduces methods to improve meta-analysis.</p><hr><h3>Large Language Models Are Semi-Parametric Reinforcement Learning Agents</h3>
<p>Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, Kai Yu</p>
<p><a href='https://openreview.net/forum?id=ZcJa1R6j3v'>https://openreview.net/forum?id=ZcJa1R6j3v</a></p>
<p><b>Keywords</b>: Learning from Experiences, LLM, Reinforcement Learning, Decision Making, Experience Memory
</p><p><b>Compressor summary</b>: The paragraph describes a novel evolvable LLM-based agent framework called Rememberer, which uses a long-term memory to learn from past experiences and improve its performance in different tasks without fine-tuning the parameters of the LLM, achieving superior results on two RL task sets.</p><hr><h3>Continuous Parametric Optical Flow</h3>
<p>Jianqin Luo, Zhexiong Wan, yuxin mao, Bo Li, Yuchao Dai</p>
<p><a href='https://openreview.net/forum?id=ZZgfS1DbmO'>https://openreview.net/forum?id=ZZgfS1DbmO</a></p>
<p><b>Keywords</b>: optical flow, point trajectories, continuous motion, neural ordinary differential equation
</p><p><b>Compressor summary</b>: The paper introduces a new method to represent continuous and dense motion in videos using B-splines and neural ODEs, and proposes a synthetic dataset and evaluation metrics to measure its performance.</p><hr><h3>Generalizable Lightweight Proxy for Robust NAS against Diverse Perturbations</h3>
<p>Hyeonjeong Ha, Minseon Kim, Sung Ju Hwang</p>
<p><a href='https://openreview.net/forum?id=ZZWg9jJQ1j'>https://openreview.net/forum?id=ZZWg9jJQ1j</a></p>
<p><b>Keywords</b>: neural architecture search, generalization, efficiency, zero-cost proxy
</p><p><b>Compressor summary</b>: The paper proposes a novel method to find optimal neural architectures that can learn generalizable features and robustness against various perturbations using a lightweight and efficient proxy.</p><hr><h3>A Definition of Continual Reinforcement Learning</h3>
<p>David Abel, Andre Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, Satinder Singh</p>
<p><a href='https://openreview.net/forum?id=ZZS9WEWYbD'>https://openreview.net/forum?id=ZZS9WEWYbD</a></p>
<p><b>Keywords</b>: Continual Reinforcement Learning, Reinforcement Learning, Lifelong Reinforcement Learning, Continual Learning
</p><p><b>Compressor summary</b>: The paper defines the continual reinforcement learning problem as a setting where agents never stop learning and always search for better policies through an implicit process, and shows that traditional views of multi-task reinforcement learning and continual supervised learning are special cases of this definition.</p><hr><h3>TART: A plug-and-play Transformer module for task-agnostic reasoning</h3>
<p>Kush Bhatia, Avanika Narayan, Christopher De Sa, Christopher Re</p>
<p><a href='https://openreview.net/forum?id=ZXbgVm3PSt'>https://openreview.net/forum?id=ZXbgVm3PSt</a></p>
<p><b>Keywords</b>: In-context learning, task-agnostic methods, large language models
</p><p><b>Compressor summary</b>: The paper proposes TART, a method to improve large language models' reasoning abilities using synthetic tasks, which enhances their in-context learning performance across different models, sizes, tasks, and modalities.</p><hr><h3>Structured Semidefinite Programming for Recovering Structured Preconditioners</h3>
<p>Arun Jambulapati, Jerry Li, Christopher Musco, Kirankumar Shiragur, Aaron Sidford, Kevin Tian</p>
<p><a href='https://openreview.net/forum?id=ZViPzk1sUI'>https://openreview.net/forum?id=ZViPzk1sUI</a></p>
<p><b>Keywords</b>: preconditioning, semidefinite programming, numerical linear algebra, linear regression, semi-random models
</p><p><b>Compressor summary</b>: The authors present a framework and algorithms for finding approximate optimal preconditioners and solving linear systems faster than previous methods.</p><hr><h3>Beyond Confidence: Reliable Models Should Also Consider Atypicality</h3>
<p>Mert Yuksekgonul, Linjun Zhang, James Zou, Carlos Guestrin</p>
<p><a href='https://openreview.net/forum?id=ZVRG3toCTT'>https://openreview.net/forum?id=ZVRG3toCTT</a></p>
<p><b>Keywords</b>: trustworthy machine learning, reliable machine learning, uncertainty
</p><p><b>Compressor summary</b>: The authors study how atypical inputs or classes affect model reliability, accuracy, and uncertainty, and show incorporating atypicality improves performance in various tasks, including skin lesion classification.</p><hr><h3>Self-Supervised Learning with Lie Symmetries for Partial Differential Equations</h3>
<p>Grégoire Mialon, Quentin Garrido, Hannah Lawrence, Danyal Rehman, Yann LeCun, Bobak Kiani</p>
<p><a href='https://openreview.net/forum?id=ZULq9QV8rH'>https://openreview.net/forum?id=ZULq9QV8rH</a></p>
<p><b>Keywords</b>: Self-supervised learning, partial differential equations, Lie symmetries, data augmentation
</p><p><b>Compressor summary</b>: The text discusses using self-supervised learning to learn representations of partial differential equations (PDEs) from heterogeneous data, improving both coefficient regression and neural solver performance.</p><hr><h3>Revisiting Area Convexity: Faster Box-Simplex Games and Spectrahedral Generalizations</h3>
<p>Arun Jambulapati, Kevin Tian</p>
<p><a href='https://openreview.net/forum?id=ZRBGwpeewz'>https://openreview.net/forum?id=ZRBGwpeewz</a></p>
<p><b>Keywords</b>: Optimization, optimal transport, linear programming, semidefinite programming
</p><p><b>Compressor summary</b>: The paper explores area convexity to create a fast first-order algorithm for solving box-simplex games and improve the complexity of other combinatorial optimization problems.</p><hr><h3>Rethinking the Role of Token Retrieval in Multi-Vector Retrieval</h3>
<p>Jinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu, Tao Lei, Iftekhar Naim, Ming-Wei Chang, Vincent Y Zhao</p>
<p><a href='https://openreview.net/forum?id=ZQzm0Z47jz'>https://openreview.net/forum?id=ZQzm0Z47jz</a></p>
<p><b>Keywords</b>: information retrieval, document retrieval, natural language processing
</p><p><b>Compressor summary</b>: XTR simplifies multi-vector retrieval by rethinking token retrieval and improves ranking efficiency without sacrificing performance on information retrieval benchmarks.</p><hr><h3>Normalizing flow neural networks by JKO scheme</h3>
<p>Chen Xu, Xiuyuan Cheng, Yao Xie</p>
<p><a href='https://openreview.net/forum?id=ZQMlfNijY5'>https://openreview.net/forum?id=ZQMlfNijY5</a></p>
<p><b>Keywords</b>: Normalizing flow, invertible neural networks, JKO scheme
</p><p><b>Compressor summary</b>: JKO-iFlow is a neural ODE flow network for efficient sampling and likelihood estimation in high dimensions, which avoids SDE trajectories and score matching, reducing memory load and improving accuracy.</p><hr><h3>Learning Adversarial Low-rank Markov Decision Processes with Unknown Transition and Full-information Feedback</h3>
<p>Canzhe Zhao, Ruofeng Yang, Baoxiang Wang, Xuezhou Zhang, Shuai Li</p>
<p><a href='https://openreview.net/forum?id=ZPtzwr2SwJ'>https://openreview.net/forum?id=ZPtzwr2SwJ</a></p>
<p><b>Keywords</b>: adversarial low-rank mdps
</p><p><b>Compressor summary</b>: This paper studies low-rank MDPs with changing losses and proposes POLO, an oracle-efficient algorithm with a sublinear regret guarantee, which is the first of its kind for this problem setting.</p><hr><h3>PyNeRF: Pyramidal Neural Radiance Fields</h3>
<p>Haithem Turki, Michael Zollhöfer, Christian Richardt, Deva Ramanan</p>
<p><a href='https://openreview.net/forum?id=ZPj7ey5fXa'>https://openreview.net/forum?id=ZPj7ey5fXa</a></p>
<p><b>Keywords</b>: view synthesis, 3d reconstruction, scene representation, 3d deep learning
</p><p><b>Compressor summary</b>: The paper introduces a simple modification to grid-based Neural Radiance Fields that improves rendering quality and reduces error rates by using different spatial grid resolutions at render time.</p><hr><h3>Encoding Human Behavior in Information Design through Deep Learning</h3>
<p>Guanghui Yu, Wei Tang, Saumik Narayanan, Chien-Ju Ho</p>
<p><a href='https://openreview.net/forum?id=ZOKhtz2Z9X'>https://openreview.net/forum?id=ZOKhtz2Z9X</a></p>
<p><b>Keywords</b>: Information design; Human behavior; Behavioral experiments
</p><p><b>Compressor summary</b>: The authors study how to design optimal information policies using a neural network (HAIDNet) that adapts to different human behavior patterns, and show its effectiveness in simulations and real experiments.</p><hr><h3>Depth-discriminative Metric Learning for Monocular 3D Object Detection</h3>
<p>Wonhyeok Choi, Mingyu Shin, Sunghoon Im</p>
<p><a href='https://openreview.net/forum?id=ZNBblMEP16'>https://openreview.net/forum?id=ZNBblMEP16</a></p>
<p><b>Keywords</b>: Monocular 3D object detection, Autonomous driving, Recognition, Regression, Metric learning
</p><p><b>Compressor summary</b>: The text describes a new method for monocular 3D object detection that uses metric learning and an auxiliary head to improve depth estimation without increasing model size or inference time.</p><hr><h3>PanoGRF: Generalizable Spherical Radiance Fields for Wide-baseline Panoramas</h3>
<p>Zheng Chen, Yan-Pei Cao, Yuan-Chen Guo, Chen Wang, Ying Shan, Song-Hai Zhang</p>
<p><a href='https://openreview.net/forum?id=ZKVxABGJ6r'>https://openreview.net/forum?id=ZKVxABGJ6r</a></p>
<p><b>Keywords</b>: neural rendering, neural radiance field, novel view synthesis, panorama, 360-degree image
</p><p><b>Compressor summary</b>: PanoGRF is a method to generate realistic views of virtual environments using wide-baseline panoramas by incorporating 360-degree scene priors and depth information.</p><hr><h3>ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting</h3>
<p>Zongsheng Yue, Jianyi Wang, Chen Change Loy</p>
<p><a href='https://openreview.net/forum?id=ZIyAHaLlsn'>https://openreview.net/forum?id=ZIyAHaLlsn</a></p>
<p><b>Keywords</b>: Super-resolution; Diffusion model; Efficient
</p><p><b>Compressor summary</b>: A novel diffusion model for image super-resolution reduces the number of steps needed, improves efficiency, and maintains or exceeds performance compared to existing methods.</p><hr><h3>Sparse Parameterization for Epitomic Dataset Distillation</h3>
<p>Xing Wei, Anjia Cao, Funing Yang, Zhiheng Ma</p>
<p><a href='https://openreview.net/forum?id=ZIfhYAE2xg'>https://openreview.net/forum?id=ZIfhYAE2xg</a></p>
<p><b>Keywords</b>: Dataset Distillation, Dataset Condensation, Sparse Coding, Dictionary Learning
</p><p><b>Compressor summary</b>: The SPEED framework uses dictionary learning, sparse coding, and feature recurrence to create synthetic datasets that capture the essential information of large and diverse original datasets.</p><hr><h3>A Graph-Theoretic Framework for Understanding Open-World Semi-Supervised Learning</h3>
<p>Yiyou Sun, Zhenmei Shi, Yixuan Li</p>
<p><a href='https://openreview.net/forum?id=ZITOHWeAy7'>https://openreview.net/forum?id=ZITOHWeAy7</a></p>
<p><b>Keywords</b>: open-world learning, clustering, spectral analysis
</p><p><b>Compressor summary</b>: The paper presents a graph-theoretic framework for open-world semi-supervised learning and proposes a new algorithm, SORL, that provides theoretical guarantees and empirical performance on clustering known and novel classes.</p><hr><h3>On kernel-based statistical learning theory in the mean field limit</h3>
<p>Christian Fiedler, Michael Herty, Sebastian Trimpe</p>
<p><a href='https://openreview.net/forum?id=ZGElmTRk3w'>https://openreview.net/forum?id=ZGElmTRk3w</a></p>
<p><b>Keywords</b>: Reproducing Kernel Hilbert Spaces, Kernel Methods, Mean Field Limit, Interacting Particle Systems, Support Vector Machines, Statistical Learning Theory
</p><p><b>Compressor summary</b>: The paper studies how machine learning kernels and Support Vector Machines behave when the number of input variables goes to infinity, providing new theoretical tools and insights for large-scale problems.</p><hr><h3>Learning to Reason and Memorize with Self-Notes</h3>
<p>Jack Lanchantin, Shubham Toshniwal, Jason E Weston, Arthur Szlam, Sainbayar Sukhbaatar</p>
<p><a href='https://openreview.net/forum?id=ZFwNdsDCRL'>https://openreview.net/forum?id=ZFwNdsDCRL</a></p>
<p><b>Keywords</b>: Memory, Reasoning, Language Models
</p><p><b>Compressor summary</b>: The authors propose Self-Notes, a method for improving large language models' multi-step reasoning and memory retention by allowing them to deviate from the input context and write down their thoughts.</p><hr><h3>Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual Prostheses</h3>
<p>Jacob Granley, Tristan Fauvel, Matthew Chalk, Michael Beyeler</p>
<p><a href='https://openreview.net/forum?id=ZED5wdGous'>https://openreview.net/forum?id=ZED5wdGous</a></p>
<p><b>Keywords</b>: Brain Computer Interfaces, BCI, Stimulus Encoding, Visual Prostheses, Bayesian Optimization, Preferential Bayesian Optimization, Human-in-the-loop Optimization, Sensory Neuroprostheses, Neuroprostheses, Patient-Specific Optimization, Latent Space Bayesian Optimization
</p><p><b>Compressor summary</b>: The paragraph discusses a new approach to optimize stimuli for neuroprosthetic devices that improves the quality of restored vision by combining deep learning and Bayesian optimization.</p><hr><h3>On Learning Latent Models with Multi-Instance Weak Supervision</h3>
<p>Kaifu Wang, Efthymia Tsamoura, Dan Roth</p>
<p><a href='https://openreview.net/forum?id=ZD65F3x1jU'>https://openreview.net/forum?id=ZD65F3x1jU</a></p>
<p><b>Keywords</b>: weak supervision, partial label learning, neuro-symbolic learning, latent structural learning
</p><p><b>Compressor summary</b>: The paper introduces a new problem called multi-instance Partial Label Learning and provides the first theoretical analysis of its learnability, error bounds, and experiments.</p><hr><h3>AdANNS: A Framework for Adaptive Semantic Search</h3>
<p>Aniket Rege, Aditya Kusupati, Sharan Ranjit S, Alan Fan, Qingqing Cao, Sham M. Kakade, Prateek Jain, Ali Farhadi</p>
<p><a href='https://openreview.net/forum?id=ZBzYWP2Gpl'>https://openreview.net/forum?id=ZBzYWP2Gpl</a></p>
<p><b>Keywords</b>: Semantic Search, Approximate Nearest Neighbor Search, Large-scale search, Representation Learning
</p><p><b>Compressor summary</b>: The paper proposes AdANNS, a framework that uses adaptive representations with varying capacities to improve accuracy-compute trade-offs in web-scale search systems based on Matryoshka Representations.</p><hr><h3>Stability Guarantees for Feature Attributions with Multiplicative Smoothing</h3>
<p>Anton Xue, Rajeev Alur, Eric Wong</p>
<p><a href='https://openreview.net/forum?id=ZBxycYCuEL'>https://openreview.net/forum?id=ZBxycYCuEL</a></p>
<p><b>Keywords</b>: Feature Attribution, Smoothing, Explainable, Interpretable, Provable Guarantees
</p><p><b>Compressor summary</b>: The paper proposes a smoothing method called Multiplicative Smoothing (MuS) that improves the stability of feature attribution methods for machine learning models by making the model sufficiently Lipschitz.</p><hr><h3>Aiming towards the minimizers: fast convergence of SGD for overparametrized problems</h3>
<p>Chaoyue Liu, Dmitriy Drusvyatskiy, Misha Belkin, Damek Davis, Yian Ma</p>
<p><a href='https://openreview.net/forum?id=ZBB8EFO7ma'>https://openreview.net/forum?id=ZBB8EFO7ma</a></p>
<p><b>Keywords</b>: Polyak-Lojasiewicz condition, SGD, interpolation, fast convergence
</p><p><b>Compressor summary</b>: This paper introduces a regularity condition for interpolation regime in deep learning, allowing stochastic gradient method to have the same worst-case complexity as deterministic gradient method using only one sampled gradient per iteration.</p><hr><h3>Generalized Bayesian Inference for Scientific Simulators via Amortized Cost Estimation</h3>
<p>Richard Gao, Michael Deistler, Jakob H. Macke</p>
<p><a href='https://openreview.net/forum?id=ZARAiV25CW'>https://openreview.net/forum?id=ZARAiV25CW</a></p>
<p><b>Keywords</b>: simulation-based inference, generalized bayesian inference, neural network, machine learning for science
</p><p><b>Compressor summary</b>: ACE is a new method that uses neural networks to approximate the cost function for generalized Bayesian inference, enabling efficient and robust parameter estimation for complex simulators, especially when the model is misspecified.</p><hr><h3>Characterizing the Impacts of Semi-supervised Learning for Weak Supervision</h3>
<p>Jeffrey Li, Jieyu Zhang, Ludwig Schmidt, Alexander Ratner</p>
<p><a href='https://openreview.net/forum?id=Z8TjsPFBSx'>https://openreview.net/forum?id=Z8TjsPFBSx</a></p>
<p><b>Keywords</b>: Weak Supervision, Semi-supervised Learning, Learning From Limited Labels
</p><p><b>Compressor summary</b>: The paper proposes a simple design space to study how semi-supervised learning techniques can improve programmatic weak supervision, finding that simpler methods often perform as well as complex ones, except in certain cases where smaller end models or fewer labeled examples are beneficial.</p><hr><h3>NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks</h3>
<p>Seokil Ham, Jungwuk Park, Dong-Jun Han, Jaekyun Moon</p>
<p><a href='https://openreview.net/forum?id=Z7Cz9un2Fy'>https://openreview.net/forum?id=Z7Cz9un2Fy</a></p>
<p><b>Keywords</b>: Multi-exit Neural Network, Adversarial Training, Knowledge Distillation, Adversarial Transferability
</p><p><b>Compressor summary</b>: NEO-KD is a new adversarial training strategy for multi-exit neural networks that improves robustness against attacks by using neighbor and exit-wise orthogonal knowledge distillation.</p><hr><h3>Puzzlefusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving</h3>
<p>Sepidehsadat Hosseini, Mohammad Amin Shabani, Saghar Irandoust, Yasutaka Furukawa</p>
<p><a href='https://openreview.net/forum?id=Z764QxwETf'>https://openreview.net/forum?id=Z764QxwETf</a></p>
<p><b>Keywords</b>: Diffusion, Jigsaw, puzzle solving
</p><p><b>Compressor summary</b>: The paper proposes a neural architecture using Diffusion Models for solving jigsaw puzzles of room layouts, and introduces new datasets for training an end-to-end system that outperforms other methods.</p><hr><h3>Topology-Aware Uncertainty for Image Segmentation</h3>
<p>Saumya Gupta, Yikai Zhang, Xiaoling Hu, Prateek Prasanna, Chao Chen</p>
<p><a href='https://openreview.net/forum?id=Z6eexoCy7W'>https://openreview.net/forum?id=Z6eexoCy7W</a></p>
<p><b>Keywords</b>: Topological Representation, Discrete Morse Theory, Structural Uncertainty, Image Segmentation
</p><p><b>Compressor summary</b>: This paper proposes a method to estimate uncertainty in the units of topological structures for segmentation tasks, using tools from topological data analysis and a joint prediction model.</p><hr><h3>Tree-Rings Watermarks: Invisible Fingerprints for Diffusion Images</h3>
<p>Yuxin Wen, John Kirchenbauer, Jonas Geiping, Tom Goldstein</p>
<p><a href='https://openreview.net/forum?id=Z57JrmubNl'>https://openreview.net/forum?id=Z57JrmubNl</a></p>
<p><b>Keywords</b>: Diffusion Model, Watermark, Privacy and Security
</p><p><b>Compressor summary</b>: Tree-Ring Watermarking is a novel technique for fingerprinting diffusion models' outputs by embedding a pattern in the initial noise vector, making it invisible to humans and resistant to common image manipulations.</p><hr><h3>Meta-Learning with Neural Bandit Scheduler</h3>
<p>Yunzhe Qi, Yikun Ban, Tianxin Wei, Jiaru Zou, Huaxiu Yao, Jingrui He</p>
<p><a href='https://openreview.net/forum?id=Z2L7F0nekb'>https://openreview.net/forum?id=Z2L7F0nekb</a></p>
<p><b>Keywords</b>: Meta Learning, Contextual Bandits
</p><p><b>Compressor summary</b>: The paper introduces BASS, a novel task scheduling framework for meta-learning that optimizes the task schedule based on the meta-model's status and balances exploitation and exploration.</p><hr><h3>Optimal Extragradient-Based Algorithms for Stochastic Variational Inequalities with Separable Structure</h3>
<p>Angela Yuan, Chris Junchi Li, Gauthier Gidel, Michael Jordan, Quanquan Gu, Simon Shaolei Du</p>
<p><a href='https://openreview.net/forum?id=Z28nPtAVxx'>https://openreview.net/forum?id=Z28nPtAVxx</a></p>
<p><b>Keywords</b>: Stochastic variational inequalities, convex-concave separable saddle-point optimization, extragradient-based algorithm, Nesterov's acceleration, scheduled restarting, scaling reduction
</p><p><b>Compressor summary</b>: The paper proposes a new algorithm, AG-EG, for solving stochastic monotone variational inequalities with a separable structure and proves its optimal convergence rate.</p><hr><h3>Revisiting Logistic-softmax Likelihood in Bayesian Meta-Learning for Few-Shot Classification</h3>
<p>Tianjun Ke, Haoqun Cao, Zenan Ling, Feng Zhou</p>
<p><a href='https://openreview.net/forum?id=Z1W0u3Cr74'>https://openreview.net/forum?id=Z1W0u3Cr74</a></p>
<p><b>Keywords</b>: Few-shot learning, Gaussian processes, Conditional conjugate
</p><p><b>Compressor summary</b>: The paper proposes a redesigned logistic-softmax likelihood for Bayesian meta-learning that controls the confidence level and improves uncertainty estimates, leading to better results on few-shot classification tasks.</p><hr><h3>Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks</h3>
<p>Tolga Ergen, Mert Pilanci</p>
<p><a href='https://openreview.net/forum?id=Z1Aj59LoZD'>https://openreview.net/forum?id=Z1Aj59LoZD</a></p>
<p><b>Keywords</b>: Convex optimization, deep learning theory, path norm, group sparsity, polynomial-time training, ReLU networks, parallel architectures, global optimality, computational complexity
</p><p><b>Compressor summary</b>: The authors study deep neural networks and introduce an analytic method to reveal hidden convexity in their optimization landscape, leading to a parsimonious convex model that can be trained efficiently.</p><hr><h3>A Unified Framework for Rank-based Loss Minimization</h3>
<p>Rufeng Xiao, Yuze Ge, Rujun Jiang, Yifan Yan</p>
<p><a href='https://openreview.net/forum?id=Z16jo3d6OD'>https://openreview.net/forum?id=Z16jo3d6OD</a></p>
<p><b>Keywords</b>: rank-based loss, ADMM, nonconvex nonsmooth optimization, conditional Value-at-Risk, human-aligned risk, ranked range loss
</p><p><b>Compressor summary</b>: The paper presents a new optimization algorithm for rank-based loss, which is used in various machine learning models, and shows its effectiveness and efficiency through experiments.</p><hr><h3>Compositional Policy Learning in Stochastic Control Systems with Formal Guarantees</h3>
<p>Đorđe Žikelić, Mathias Lechner, Abhinav Verma, Krishnendu Chatterjee, Thomas A Henzinger</p>
<p><a href='https://openreview.net/forum?id=Yx8Sw2H5Q7'>https://openreview.net/forum?id=Yx8Sw2H5Q7</a></p>
<p><b>Keywords</b>: Verification, Compositional learning
</p><p><b>Compressor summary</b>: The authors propose a method to learn neural network policies for stochastic environments with formal guarantees, using SpectRL's logical specifications and reach-avoid supermartingales.</p><hr><h3>Text Promptable Surgical Instrument Segmentation with Vision-Language Models</h3>
<p>Zijian Zhou, Oluwatosin Alabi, Meng Wei, Tom Vercauteren, Miaojing Shi</p>
<p><a href='https://openreview.net/forum?id=YwgA3avHrP'>https://openreview.net/forum?id=YwgA3avHrP</a></p>
<p><b>Keywords</b>: Surgical Instrument Segmentation, Vision Language Models, Text Promptable Segmentation
</p><p><b>Compressor summary</b>: The paper presents a text promptable surgical instrument segmentation approach that uses vision-language models and multiple text prompts to improve accuracy and adaptability in minimally invasive surgeries.</p><hr><h3>Integration-free Training for Spatio-temporal Multimodal Covariate Deep Kernel Point Processes</h3>
<p>YIXUAN ZHANG, Quyu Kong, Feng Zhou</p>
<p><a href='https://openreview.net/forum?id=Yvpenkym8A'>https://openreview.net/forum?id=Yvpenkym8A</a></p>
<p><b>Keywords</b>: Spatio-temporal Point Processes, Deep Kernel, Covariate, Integration-free
</p><p><b>Compressor summary</b>: The study introduces a new model called Deep Kernel Mixture Point Processes (DKMPP) that uses a deep kernel to better capture relationships between events and multimodal covariate data, and improves efficiency by using score matching methods.</p><hr><h3>Online Map Vectorization for Autonomous Driving: A Rasterization Perspective</h3>
<p>Gongjie Zhang, Jiahao Lin, Shuang Wu, Yilin Song, Zhipeng Luo, Yang Xue, Shijian Lu, Zuoguan Wang</p>
<p><a href='https://openreview.net/forum?id=YvO5yTVv5Y'>https://openreview.net/forum?id=YvO5yTVv5Y</a></p>
<p><b>Keywords</b>: Online HD Map Construction, Map Vectorization, Autonomous Driving, Evaluation Metric, Rasterization, Differentiable Rasterization, Bird's-Eye-View Perception
</p><p><b>Compressor summary</b>: The paper introduces MapVR, a novel framework that applies rasterization to vectorized maps for more accurate perception and safer autonomous driving.</p><hr><h3>Diversifying Spatial-Temporal Perception for Video Domain Generalization</h3>
<p>Kun-Yu Lin, Jia-Run Du, Yipeng Gao, Jiaming Zhou, Wei-Shi Zheng</p>
<p><a href='https://openreview.net/forum?id=YsZTDcIQwQ'>https://openreview.net/forum?id=YsZTDcIQwQ</a></p>
<p><b>Keywords</b>: video understanding and analysis, video domain generalization
</p><p><b>Compressor summary</b>: The paper introduces Spatial-Temporal Diversification Network (STDN), a model that improves video domain generalization by discovering diverse spatial-temporal cues in videos, to defend against heavy reliance on domain-specific cues.</p><hr><h3>Deep Fractional Fourier Transform</h3>
<p>Hu Yu, Jie Huang, Lingzhi Li, Man Zhou, Feng Zhao</p>
<p><a href='https://openreview.net/forum?id=YsYKv95jy9'>https://openreview.net/forum?id=YsYKv95jy9</a></p>
<p><b>Keywords</b>: Fractional Fourier Transform, image restoration
</p><p><b>Compressor summary</b>: The paper introduces a new spatial-frequency analysis tool, FRFT, which provides comprehensive unified perspectives for image processing and proposes a simple yet effective operator, MFRFC, that significantly improves performance on various computer vision tasks.</p><hr><h3>Federated Learning with Client Subsampling, Data Heterogeneity, and Unbounded Smoothness: A New Algorithm and Lower Bounds</h3>
<p>Michael Crawshaw, Yajie Bao, Mingrui Liu</p>
<p><a href='https://openreview.net/forum?id=Yq6GKgN3RC'>https://openreview.net/forum?id=Yq6GKgN3RC</a></p>
<p><b>Keywords</b>: federated learning, client subsampling, nonconvex optimization, relaxed smoothness, data heterogeneity, lower bound
</p><p><b>Compressor summary</b>: EPISODE++ is a new algorithm for Federated Learning with client subsampling and data heterogeneity that achieves linear speedup, reduces communication rounds, and works well on RNNs for text classification.</p><hr><h3>Pareto Frontiers in Deep Feature Learning: Data, Compute, Width, and Luck</h3>
<p>Benjamin L. Edelman, Surbhi Goel, Sham M. Kakade, eran malach, Cyril Zhang</p>
<p><a href='https://openreview.net/forum?id=Ypbke6biDm'>https://openreview.net/forum?id=Ypbke6biDm</a></p>
<p><b>Keywords</b>: deep learning, feature learning, parity, grokking, lottery tickets, scaling
</p><p><b>Compressor summary</b>: This work studies how algorithm choices affect resource tradeoffs in deep learning for sparse parity learning, and shows that width and sparsity improve sample efficiency.</p><hr><h3>Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection</h3>
<p>Cheng-Ju Ho, Chen-Hsuan Tai, Yen-Yu Lin, Ming-Hsuan Yang, Yi-Hsuan Tsai</p>
<p><a href='https://openreview.net/forum?id=YoghyvSG0H'>https://openreview.net/forum?id=YoghyvSG0H</a></p>
<p><b>Keywords</b>: Semi-supervised learning, 3D object detection, diffusion model
</p><p><b>Compressor summary</b>: The paper proposes Diffusion-SS3D, a method that improves semi-supervised 3D object detection by using a diffusion model to denoise corrupted labels and enhance pseudo-label generation in a teacher-student framework.</p><hr><h3>GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph</h3>
<p>Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, Xinchao Wang</p>
<p><a href='https://openreview.net/forum?id=YmEDnMynuO'>https://openreview.net/forum?id=YmEDnMynuO</a></p>
<p><b>Keywords</b>: Efficient transfer learning, vision-language model, adapter-style tuning
</p><p><b>Compressor summary</b>: GraphAdapter is an efficient tuning strategy for vision-language models that leverages dual-modality knowledge graphs to capture task-specific structure and inter-class relationships, improving performance on downstream tasks.</p><hr><h3>STEVE-1: A Generative Model for Text-to-Behavior in Minecraft</h3>
<p>Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, Sheila A. McIlraith</p>
<p><a href='https://openreview.net/forum?id=YkBDJWerKg'>https://openreview.net/forum?id=YkBDJWerKg</a></p>
<p><b>Keywords</b>: minecraft, instruction following, foundation models, sequence models, reinforcement learning, sequential decision making, goal conditioned reinforcement learning, text conditioned reinforcement learning, transformers, deep learning
</p><p><b>Compressor summary</b>: The text introduces STEVE-1, a cost-effective AI model that follows instructions in Minecraft by adapting pretrained models and using self-supervised learning techniques.</p><hr><h3>Boosting Learning for LDPC Codes to Improve the Error-Floor Performance</h3>
<p>Hee-Youl Kwak, Dae-Young Yun, Yongjune Kim, Sang-Hyo Kim, Jong-Seon No</p>
<p><a href='https://openreview.net/forum?id=Yj3lFEyfnl'>https://openreview.net/forum?id=Yj3lFEyfnl</a></p>
<p><b>Keywords</b>: Error-floor, Low-density parity-check codes, Boosting learning, Training shcedule, weight sharing, Neural decoders, Min-sum
</p><p><b>Compressor summary</b>: The authors propose new training methods for neural min-sum decoders to eliminate the error-floor effect in low-density parity-check codes, achieving better error-floor performance than other methods without extra hardware costs.</p><hr><h3>Evaluating Neuron Interpretation Methods of NLP Models</h3>
<p>Yimin Fan, Fahim Dalvi, Nadir Durrani, Hassan Sajjad</p>
<p><a href='https://openreview.net/forum?id=YiwMpyMdPX'>https://openreview.net/forum?id=YiwMpyMdPX</a></p>
<p><b>Keywords</b>: Neuron interpretation, NLP, Interpretability, Machine Learning
</p><p><b>Compressor summary</b>: The paper proposes a voting-based framework to compare different neuron interpretation methods and finds that they agree on important neurons and focus on last layer representations.</p><hr><h3>Near-Optimal Bounds for Learning Gaussian Halfspaces with Random Classification Noise</h3>
<p>Ilias Diakonikolas, Jelena Diakonikolas, Daniel Kane, Puqian Wang, Nikos Zarifis</p>
<p><a href='https://openreview.net/forum?id=YifKp5b15e'>https://openreview.net/forum?id=YifKp5b15e</a></p>
<p><b>Keywords</b>: PAC Learning, Random Classification Noise
</p><p><b>Compressor summary</b>: The paper studies learning general halfspaces with random classification noise and shows an information-computation gap between algorithmic and statistical query complexity results.</p><hr><h3>Anytime Model Selection in Linear Bandits</h3>
<p>Parnian Kassraie, Nicolas Emmenegger, Andreas Krause, Aldo Pacchiano</p>
<p><a href='https://openreview.net/forum?id=YiRX7nQ77Q'>https://openreview.net/forum?id=YiRX7nQ77Q</a></p>
<p><b>Keywords</b>: bandits, model selection, online learning
</p><p><b>Compressor summary</b>: ALEXP is a model selection algorithm for bandit optimization that uses online learning with emulated full-information feedback to achieve exponentially improved regret compared to existing methods.</p><hr><h3>Autodecoding Latent 3D Diffusion Models</h3>
<p>Evangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski, Chaoyang Wang, Luc Van Gool, Sergey Tulyakov</p>
<p><a href='https://openreview.net/forum?id=YhAZqWhOnS'>https://openreview.net/forum?id=YhAZqWhOnS</a></p>
<p><b>Keywords</b>: 3D Generation, Diffusion Models
</p><p><b>Compressor summary</b>: The proposed 3D autodecoder approach generates high-quality 3D assets from 2D images or monocular videos using a latent space that embeds properties learned from the target data, achieving superior results compared to existing methods on various benchmarks.</p><hr><h3>Bandit Social Learning under Myopic Behavior</h3>
<p>Kiarash Banihashem, MohammadTaghi Hajiaghayi, Suho Shin, Aleksandrs Slivkins</p>
<p><a href='https://openreview.net/forum?id=YeP8osxOht'>https://openreview.net/forum?id=YeP8osxOht</a></p>
<p><b>Keywords</b>: multi-armed bandits, greedy algorithm, social learning, myopic behavior, learning failures, algorithmic game theory
</p><p><b>Compressor summary</b>: The paper examines how social learning agents collectively follow a multi-armed bandit protocol but fail to explore due to myopic behaviors.</p><hr><h3>Your representations are in the network: composable and parallel adaptation for large scale models</h3>
<p>Yonatan Dukler, Alessandro Achille, Hao Yang, Varsha Vivek, Luca Zancato, Benjamin Bowman, Avinash Ravichandran, Charless Fowlkes, Ashwin Swaminathan, Stefano Soatto</p>
<p><a href='https://openreview.net/forum?id=Ydxnan4P2G'>https://openreview.net/forum?id=Ydxnan4P2G</a></p>
<p><b>Keywords</b>: Efficient learning, Compute-efficient deep learning, Deep Learning Theory, class-incremental-learning, downstream adaptation
</p><p><b>Compressor summary</b>: InCA is a transfer learning framework that uses lightweight cross-attention modules to adapt large models efficiently and identify useful representations for downstream tasks.</p><hr><h3>Learning Trajectories are Generalization Indicators</h3>
<p>Jingwen Fu, Zhizheng Zhang, Dacheng Yin, Yan Lu, Nanning Zheng</p>
<p><a href='https://openreview.net/forum?id=YdfcKb4Wif'>https://openreview.net/forum?id=YdfcKb4Wif</a></p>
<p><b>Keywords</b>: Generalization, Learning Trajectory
</p><p><b>Compressor summary</b>: The paper examines how the learning path of DNNs affects their ability to generalize, proposing a new bound that considers trajectory complexity and training set diversity.</p><hr><h3>Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding</h3>
<p>Zhejun Zhang, Alexander Liniger, Christos Sakaridis, Fisher Yu, Luc Van Gool</p>
<p><a href='https://openreview.net/forum?id=YcmGuwdLoU'>https://openreview.net/forum?id=YcmGuwdLoU</a></p>
<p><b>Keywords</b>: Motion Prediction, Autonomous Driving, Transformer
</p><p><b>Compressor summary</b>: The paper introduces a novel attention mechanism (KNARPE) and a hierarchical framework (HPTR) for motion prediction in autonomous driving systems, improving efficiency and scalability while achieving superior performance on public benchmarks.</p><hr><h3>Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms</h3>
<p>Qining Zhang, Lei Ying</p>
<p><a href='https://openreview.net/forum?id=Yc9bqbnrbs'>https://openreview.net/forum?id=Yc9bqbnrbs</a></p>
<p><b>Keywords</b>: stochastic multi-armed bandits, regret optimal best arm identification, commitment
</p><p><b>Compressor summary</b>: The paper proposes ROBAI, a method for solving a stochastic MAB problem with dual objectives of quick optimal arm identification and reward maximization in a limited number of rounds.</p><hr><h3>BiMatting: Efficient Video Matting via Binarization</h3>
<p>Haotong Qin, Lei Ke, Xudong Ma, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Xianglong Liu, Fisher Yu</p>
<p><a href='https://openreview.net/forum?id=YbYQ0JEQ80'>https://openreview.net/forum?id=YbYQ0JEQ80</a></p>
<p><b>Keywords</b>: Video Matting, Model Binarization, Deep Learning
</p><p><b>Compressor summary</b>: BiMatting is a binarized video matting model that overcomes representation degradation and redundant computations, achieving high accuracy, efficiency, and resource savings for real-time video applications.</p><hr><h3>Toolformer: Language Models Can Teach Themselves to Use Tools</h3>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom</p>
<p><a href='https://openreview.net/forum?id=Yacmpz84TH'>https://openreview.net/forum?id=Yacmpz84TH</a></p>
<p><b>Keywords</b>: Language Models, Zero-Shot Learning, Tool Use, APIs
</p><p><b>Compressor summary</b>: The paper introduces Toolformer, a language model that learns to use external APIs to improve its performance on various tasks.</p><hr><h3>Contrast, Attend and Diffuse to Decode High-Resolution Images from Brain Activities</h3>
<p>Jingyuan Sun, Mingxiao Li, Yunhao Zhang, Marie-Francine Moens, Zijiao Chen, Shaonan Wang</p>
<p><a href='https://openreview.net/forum?id=YZSLDEE0mw'>https://openreview.net/forum?id=YZSLDEE0mw</a></p>
<p><b>Keywords</b>: Neural decoding, brain machine interface, mind reader, visual reconstruction, vision decoding
</p><p><b>Compressor summary</b>: The paper introduces a two-phase fMRI representation learning framework that uses denoising and image reconstruction to improve visual decoding from neural responses, achieving superior results compared to previous methods.</p><hr><h3>GAN You See Me? Enhanced Data Reconstruction Attacks against Split Inference</h3>
<p>Ziang Li, Mengda Yang, Yaxin Liu, Juan Wang, Hongxin Hu, Wenzhe Yi, Xiaoyang Xu</p>
<p><a href='https://openreview.net/forum?id=YZGWhs1H7F'>https://openreview.net/forum?id=YZGWhs1H7F</a></p>
<p><b>Keywords</b>: deep learning, split inference, data reconstruction attack
</p><p><b>Compressor summary</b>: GLASS is a novel GAN-based attack that effectively reconstructs private data from Split Inference models, while GLASS++ enhances its stability and both are resistant to common defense mechanisms.</p><hr><h3>Structured Prediction with Stronger Consistency Guarantees</h3>
<p>Anqi Mao, Mehryar Mohri, Yutao Zhong</p>
<p><a href='https://openreview.net/forum?id=YZ7ip645Ra'>https://openreview.net/forum?id=YZ7ip645Ra</a></p>
<p><b>Keywords</b>: structured prediction, consistency, learning theory, natural language processing
</p><p><b>Compressor summary</b>: The authors study surrogate losses for structured prediction and propose new ones with better learning guarantees and efficient algorithms.</p><hr><h3>Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs</h3>
<p>Rajat Vadiraj Dwaraknath, Tolga Ergen, Mert Pilanci</p>
<p><a href='https://openreview.net/forum?id=YWsPN0EMZr'>https://openreview.net/forum?id=YWsPN0EMZr</a></p>
<p><b>Keywords</b>: neural tangent kernel, NTK, ReLU activations, neural networks, gated ReLU, convex optimization, kernel, multiple kernel learning, MKL, group lasso, iterative reweighting, group norm
</p><p><b>Compressor summary</b>: This paper connects neural network training with gradient flow to multiple kernel learning using a gated ReLU network and shows how to improve the weights for better predictions.</p><hr><h3>Predicting a Protein's Stability under a Million Mutations</h3>
<p>Jeffrey Ouyang-Zhang, Daniel Jesus Diaz, Adam Klivans, Philipp Kraehenbuehl</p>
<p><a href='https://openreview.net/forum?id=YWSOpYjyG4'>https://openreview.net/forum?id=YWSOpYjyG4</a></p>
<p><b>Keywords</b>: stability, proteins, biology, physical
</p><p><b>Compressor summary</b>: The authors propose a deep learning algorithm called Mutate Everything that predicts the effect of mutations on protein stability quickly and accurately, using existing models as its base.</p><hr><h3>Offline Reinforcement Learning with Differential Privacy</h3>
<p>Dan Qiao, Yu-Xiang Wang</p>
<p><a href='https://openreview.net/forum?id=YVMc3KiWBQ'>https://openreview.net/forum?id=YVMc3KiWBQ</a></p>
<p><b>Keywords</b>: Differential privacy, offline reinforcement learning, reinforcement learning theory
</p><p><b>Compressor summary</b>: The paper proposes offline reinforcement learning algorithms with differential privacy guarantees that protect sensitive information while maintaining good learning performance and utility.</p><hr><h3>MoVie: Visual Model-Based Policy Adaptation for View Generalization</h3>
<p>Sizhe Yang, Yanjie Ze, Huazhe Xu</p>
<p><a href='https://openreview.net/forum?id=YV1MYtj2AR'>https://openreview.net/forum?id=YV1MYtj2AR</a></p>
<p><b>Keywords</b>: visual reinforcement learning, visual generalization
</p><p><b>Compressor summary</b>: The paper proposes MoVie, a method to improve visual reinforcement learning agents' ability to generalize to unseen views without explicit rewards, achieving significant improvements in 18 real-world robotics tasks.</p><hr><h3>CELLE-2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer</h3>
<p>Emaad Khwaja, Yun S. Song, Aaron Agarunov, Bo Huang</p>
<p><a href='https://openreview.net/forum?id=YSMLVffl5u'>https://openreview.net/forum?id=YSMLVffl5u</a></p>
<p><b>Keywords</b>: text-to-image, protein localization, protein engineering, transformers
</p><p><b>Compressor summary</b>: CELL-E 2 is a transformer that can generate and design protein subcellular localization images and sequences from amino acid sequences, capturing spatial complexity and producing NLS.</p><hr><h3>Implicit Regularization in Over-Parameterized Support Vector Machine</h3>
<p>Yang Sui, Xin HE, Yang Bai</p>
<p><a href='https://openreview.net/forum?id=YSFQRVkkl0'>https://openreview.net/forum?id=YSFQRVkkl0</a></p>
<p><b>Keywords</b>: Over-parameterization, SVM, Sparsity, Lasso
</p><p><b>Compressor summary</b>: The paper proposes a regularization-free algorithm for high-dimensional SVMs using over-parameterization and Nesterov's method, which improves computational efficiency and achieves near-oracle convergence rate.</p><hr><h3>3D-LLM: Injecting the 3D World into Large Language Models</h3>
<p>Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan</p>
<p><a href='https://openreview.net/forum?id=YQA28p7qNz'>https://openreview.net/forum?id=YQA28p7qNz</a></p>
<p><b>Keywords</b>: 3D Visual Reasoning, 3D Large Language Model
</p><p><b>Compressor summary</b>: The paper introduces 3D-LLMs, a new type of language models that can perform various 3D tasks using point clouds and features. The paper collects large amounts of 3D-language data and uses a novel training method to achieve superior performance on several benchmarks.</p><hr><h3>Harnessing Hard Mixed Samples with Decoupled Regularizer</h3>
<p>Zicheng Liu, Siyuan Li, Ge Wang, Lirong Wu, Cheng Tan, Stan Z. Li</p>
<p><a href='https://openreview.net/forum?id=YPQg2RTFD8'>https://openreview.net/forum?id=YPQg2RTFD8</a></p>
<p><b>Keywords</b>: mixup, data augmentation, classification, data efficiency
</p><p><b>Compressor summary</b>: Decoupled mixup is an efficient data augmentation method that smooths decision boundaries and mines discriminative features without extra computation, outperforming dynamic mixup methods.</p><hr><h3>Spatio-Angular Convolutions for Super-resolution in Diffusion MRI</h3>
<p>Matthew Lyon, Paul Armitage, Mauricio A Álvarez</p>
<p><a href='https://openreview.net/forum?id=YPHIrNKI0d'>https://openreview.net/forum?id=YPHIrNKI0d</a></p>
<p><b>Keywords</b>: Diffusion MRI, super-resolution, image synthesis, conditional image synthesis, continuous convolution, parametric continuous convolution
</p><p><b>Compressor summary</b>: The paragraph introduces a novel method for improving diffusion MRI resolution using a parametric continuous convolution network (PCCNN) that performs well and requires fewer parameters than existing models.</p><hr><h3>On Measuring Fairness in Generative Models</h3>
<p>Christopher T.H Teo, Milad Abdollahzadeh, Ngai-man Cheung</p>
<p><a href='https://openreview.net/forum?id=YOZaej0ZC7'>https://openreview.net/forum?id=YOZaej0ZC7</a></p>
<p><b>Keywords</b>: Fairness, Generative models, GAN, Calibration
</p><p><b>Compressor summary</b>: This paper presents CLEAM, a new framework to measure fairness in generative models that reduces errors and has minimal overhead.</p><hr><h3>Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness</h3>
<p>Suraj Srinivas, Sebastian Bordt, Himabindu Lakkaraju</p>
<p><a href='https://openreview.net/forum?id=YMMlHBSQdC'>https://openreview.net/forum?id=YMMlHBSQdC</a></p>
<p><b>Keywords</b>: robustness, generative models, perceptually aligned gradients, bayes optimality, manifold hypothesis
</p><p><b>Compressor summary</b>: This paper explains how perceptually-aligned gradients (PAGs) in robust computer vision models arise from off-manifold robustness and identifies three regimes of robustness affecting both alignment and accuracy.</p><hr><h3>Connecting Pre-trained Language Model and Downstream Task via Properties of Representation</h3>
<p>Chenwei Wu, Holden Lee, Rong Ge</p>
<p><a href='https://openreview.net/forum?id=YLOJ4aKAka'>https://openreview.net/forum?id=YLOJ4aKAka</a></p>
<p><b>Keywords</b>: language model representation, downstream performance, deep learning theory
</p><p><b>Compressor summary</b>: The paper investigates how pre-trained language models' representations depend on downstream tasks' properties and structures, and proposes an "anchor vector" concept to improve performance transfer understanding.</p><hr><h3>ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling</h3>
<p>Yuqi Chen, Kan Ren, Yansen Wang, Yuchen Fang, Weiwei Sun, Dongsheng Li</p>
<p><a href='https://openreview.net/forum?id=YJDz4F2AZu'>https://openreview.net/forum?id=YJDz4F2AZu</a></p>
<p><b>Keywords</b>: Irregular Time Series Modeling, Transformer, Neural Ordinary Differential Equation
</p><p><b>Compressor summary</b>: ContiFormer combines Neural ODEs and Transformers to model continuous-time dynamics on irregular time series, capturing both input relationships and dynamic changes.</p><hr><h3>Conformal Prediction Sets for Ordinal Classification</h3>
<p>PRASENJIT DEY, Srujana Merugu, Sivaramakrishnan R Kaveri</p>
<p><a href='https://openreview.net/forum?id=YI4bn6aAmz'>https://openreview.net/forum?id=YI4bn6aAmz</a></p>
<p><b>Keywords</b>: Ordinal Classification, Conformal Predictions, Unimodal modelling
</p><p><b>Compressor summary</b>: The paper proposes a framework to adapt conformal prediction methods for ordinal classification, using a non-parametric approach to generate contiguous sets with guaranteed coverage and minimal cardinality.</p><hr><h3>NICE: NoIse-modulated Consistency rEgularization for Data-Efficient GANs</h3>
<p>Yao Ni, Piotr Koniusz</p>
<p><a href='https://openreview.net/forum?id=YFW6MVGVTn'>https://openreview.net/forum?id=YFW6MVGVTn</a></p>
<p><b>Keywords</b>: Image Generation, limited dataset, Generative Adversarial Networks
</p><p><b>Compressor summary</b>: NICE is a novel approach that uses adaptive noise to modulate the discriminator's latent features, preventing overfitting and improving GAN training stability on limited data.</p><hr><h3>Inverse Reinforcement Learning with the Average Reward Criterion</h3>
<p>Feiyang Wu, Jingyang Ke, Anqi Wu</p>
<p><a href='https://openreview.net/forum?id=YFSrf8aciU'>https://openreview.net/forum?id=YFSrf8aciU</a></p>
<p><b>Keywords</b>: Machine Learning, Reinforcement Learning, Inverse Reinforcement Learning, Markov Decision Process, stochastic optimization, complexity analysis
</p><p><b>Compressor summary</b>: This paper presents a novel inverse reinforcement learning method that works under an average-reward setting and has lower complexity than previous methods, along with experiments on various tasks.</p><hr><h3>Model-Free Active Exploration in Reinforcement Learning</h3>
<p>Alessio Russo, Alexandre Proutiere</p>
<p><a href='https://openreview.net/forum?id=YEtstXIpP3'>https://openreview.net/forum?id=YEtstXIpP3</a></p>
<p><b>Keywords</b>: reinforcement learning; best policy identification; model free; exploration; sample complexity
</p><p><b>Compressor summary</b>: The paper proposes a new model-free exploration method for Reinforcement Learning that can find near-optimal policies more quickly than existing methods.</p><hr><h3>$\textbf{A}^2\textbf{CiD}^2$: Accelerating Asynchronous Communication in Decentralized Deep Learning</h3>
<p>Adel Nabli, Eugene Belilovsky, Edouard Oyallon</p>
<p><a href='https://openreview.net/forum?id=YE04aRkeZb'>https://openreview.net/forum?id=YE04aRkeZb</a></p>
<p><b>Keywords</b>: Decentralized Optimization for Deep Learning, Asynchronous Optimization, Distributed Training, Data-Parallel
</p><p><b>Compressor summary</b>: The paper introduces a new gossip-based optimization algorithm called $\textbf{A}^2\textbf{CiD}^2$ that speeds up distributed training of deep learning models by using continuous local momentum and reducing idle time.</p><hr><h3>Finding Counterfactually Optimal Action Sequences in Continuous State Spaces</h3>
<p>Stratis Tsirtsis, Manuel Gomez Rodriguez</p>
<p><a href='https://openreview.net/forum?id=YDCpf85eXc'>https://openreview.net/forum?id=YDCpf85eXc</a></p>
<p><b>Keywords</b>: Counterfactual reasoning, Markov decision process, Structural causal model, A* search
</p><p><b>Compressor summary</b>: The paper presents a new method to analyze continuous state environments for retrospective analysis of treatment decisions using causal inference and reinforcement learning techniques.</p><hr><h3>A Unified Algorithm Framework for Unsupervised Discovery of Skills based on Determinantal Point Process</h3>
<p>Jiayu Chen, Vaneet Aggarwal, Tian Lan</p>
<p><a href='https://openreview.net/forum?id=Y8p3ThNDmK'>https://openreview.net/forum?id=Y8p3ThNDmK</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Unsupervised Skill Discovery, Determinantal Point Process, Options
</p><p><b>Compressor summary</b>: The paper proposes a new algorithm that combines diversity and coverage in unsupervised option discovery using Determinantal Point Process, and shows its effectiveness on Mujoco and Atari tasks.</p><hr><h3>Model Shapley: Equitable Model Valuation with Black-box Access</h3>
<p>Xinyi Xu, Thanh Lam, Chuan-Sheng Foo, Bryan Kian Hsiang Low</p>
<p><a href='https://openreview.net/forum?id=Y6IGTNMdLT'>https://openreview.net/forum?id=Y6IGTNMdLT</a></p>
<p><b>Keywords</b>: Model Valuation, Dirichlet Abstraction, Shapley Value
</p><p><b>Compressor summary</b>: The paper proposes a new method for valuing machine learning models in AI marketplaces, called model Shapley, which is fair and can be used to predict the values of many models efficiently.</p><hr><h3>Quantum Bayesian Optimization</h3>
<p>Zhongxiang Dai, Gregory Kang Ruey Lau, Arun Verma, Yao Shu, Bryan Kian Hsiang Low, Patrick Jaillet</p>
<p><a href='https://openreview.net/forum?id=Y44NurSDjq'>https://openreview.net/forum?id=Y44NurSDjq</a></p>
<p><b>Keywords</b>: quantum bandits, kernelized bandits
</p><p><b>Compressor summary</b>: The Q-GP-UCB algorithm optimizes non-linear reward functions with quantum computing, achieving better regret upper bounds than classical methods and previous quantum approaches.</p><hr><h3>PTQD: Accurate Post-Training Quantization for Diffusion Models</h3>
<p>Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, Bohan Zhuang</p>
<p><a href='https://openreview.net/forum?id=Y3g1PV5R9l'>https://openreview.net/forum?id=Y3g1PV5R9l</a></p>
<p><b>Keywords</b>: Diffusion models, Post-training quantization, Mixed precision
</p><p><b>Compressor summary</b>: The paragraph discusses a new post-training quantization method for diffusion models that reduces computational costs and improves sample quality by correcting quantization noise and adjusting bitwidths for each denoising step.</p><hr><h3>Detection Based Part-level Articulated Object Reconstruction from Single RGBD Image</h3>
<p>Yuki Kawana, Tatsuya Harada</p>
<p><a href='https://openreview.net/forum?id=Y3NjoeO4Q1'>https://openreview.net/forum?id=Y3NjoeO4Q1</a></p>
<p><b>Keywords</b>: articulated objects, shape reconstruction, 3D reconstruction
</p><p><b>Compressor summary</b>: The paper presents a novel method for reconstructing multiple man-made articulated objects from a single RGBD image using part-level representation and proposes three techniques to overcome challenges, achieving better shape reconstruction and kinematics estimation on synthetic and real data.</p><hr><h3>Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time</h3>
<p>Arvind Venkat Mahankali, Jeff Z. HaoChen, Kefan Dong, Margalit Glasgow, Tengyu Ma</p>
<p><a href='https://openreview.net/forum?id=Y2hnMZvVDm'>https://openreview.net/forum?id=Y2hnMZvVDm</a></p>
<p><b>Keywords</b>: Nonconvex Optimization, Mean-Field Analysis, Beyond NTK, Deep Learning Theory
</p><p><b>Compressor summary</b>: The paper analyzes projected gradient flow on two-layer neural networks without modifications and shows they achieve better sample complexity than kernel methods.</p><hr><h3>Expert load matters: operating networks at high accuracy and low manual effort</h3>
<p>Sara Sangalli, Ertunc Erdil, Ender Konukoglu</p>
<p><a href='https://openreview.net/forum?id=Y2VQWfi7Vc'>https://openreview.net/forum?id=Y2VQWfi7Vc</a></p>
<p><b>Keywords</b>: human-ai collaboration system, optimization
</p><p><b>Compressor summary</b>: The paper proposes a new loss function for training deep neural networks that considers both model accuracy and expert load, aiming to improve classification and reduce human intervention in critical applications.</p><hr><h3>A Unified Approach for Maximizing Continuous DR-submodular Functions</h3>
<p>Mohammad Pedramfar, Christopher John Quinn, Vaneet Aggarwal</p>
<p><a href='https://openreview.net/forum?id=Y1sJJW3pID'>https://openreview.net/forum?id=Y1sJJW3pID</a></p>
<p><b>Keywords</b>: Stochastic optimization, submodular maximization, Frank-Wolfe algorithm
</p><p><b>Compressor summary</b>: The paper proposes a unified method to maximize continuous DR-submodular functions across various settings and oracle access types, improving or matching existing results in most cases.</p><hr><h3>Posterior Contraction Rates for Matérn Gaussian Processes on Riemannian Manifolds</h3>
<p>Paul Rosa, Viacheslav Borovitskiy, Alexander Terenin, Judith Rousseau</p>
<p><a href='https://openreview.net/forum?id=Y18r0xWkSh'>https://openreview.net/forum?id=Y18r0xWkSh</a></p>
<p><b>Keywords</b>: Gaussian processes, posterior contraction, manifolds, kernels
</p><p><b>Compressor summary</b>: The paper investigates whether intrinsic models of Gaussian processes on Riemannian manifolds lead to better performance than Euclidean ones, by proving optimal contraction rates for both types and showing empirical results that support the theoretical findings.</p><hr><h3>Towards Higher Ranks via Adversarial Weight Pruning</h3>
<p>Yuchuan Tian, Hanting Chen, Tianyu Guo, Chao Xu, Yunhe Wang</p>
<p><a href='https://openreview.net/forum?id=Y17N9B0vXn'>https://openreview.net/forum?id=Y17N9B0vXn</a></p>
<p><b>Keywords</b>: Weight Pruning, Matrix Rank
</p><p><b>Compressor summary</b>: The paper introduces RPG, a pruning method for CNNs that maintains the ranks of sparse weights in an adversarial manner to improve performance on edge devices with high sparsity.</p><hr><h3>Minimax Risks and Optimal Procedures for Estimation under Functional Local Differential Privacy</h3>
<p>Bonwoo Lee, Jeongyoun Ahn, Cheolwoo Park</p>
<p><a href='https://openreview.net/forum?id=XzTM9gVRT4'>https://openreview.net/forum?id=XzTM9gVRT4</a></p>
<p><b>Keywords</b>: Data privacy, Functional local differential privacy, Gaussian mechanism, Minimax risks, Statistical utility
</p><p><b>Compressor summary</b>: This paragraph discusses how functional differential privacy (DP), a type of local DP, balances data privacy and utility by analyzing its performance in estimating mean and density, and suggests it as a reliable standard for local DP.</p><hr><h3>Look Ma, No Hands! Agent-Environment Factorization of Egocentric Videos</h3>
<p>Matthew Chang, Aditya Prakash, Saurabh Gupta</p>
<p><a href='https://openreview.net/forum?id=Xyj46OxEhK'>https://openreview.net/forum?id=Xyj46OxEhK</a></p>
<p><b>Keywords</b>: Inpainting, Diffusion, Robot Learning, Egocentric Vision
</p><p><b>Compressor summary</b>: This paper presents a method to separate human hand and environment in egocentric videos using a diffusion model and attention, improving performance on various robotics tasks.</p><hr><h3>An Empirical Study Towards Prompt-Tuning for Graph Contrastive Pre-Training in Recommendations</h3>
<p>Haoran Yang, Xiangyu Zhao, Yicong Li, Hongxu Chen, Guandong Xu</p>
<p><a href='https://openreview.net/forum?id=XyAP8ScqLV'>https://openreview.net/forum?id=XyAP8ScqLV</a></p>
<p><b>Keywords</b>: graph contrastive learning, prompt tuning, recommendation system
</p><p><b>Compressor summary</b>: CPTPP is a novel framework for graph learning that uses personalized prompts to bridge the gap between pre-training and downstream tasks, achieving better results and modeling user preferences more effectively.</p><hr><h3>CAP:  Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models</h3>
<p>Denis Kuznedelev, Eldar Kurtic, Elias Frantar, Dan Alistarh</p>
<p><a href='https://openreview.net/forum?id=Xy7DoWSNZX'>https://openreview.net/forum?id=Xy7DoWSNZX</a></p>
<p><b>Keywords</b>: neural network pruning, vision transformer, sparsity, model compression
</p><p><b>Compressor summary</b>: The Correlation Aware Pruner (CAP) is a new framework that significantly improves the compressibility of state-of-the-art computer vision models by handling complex weight correlations and providing efficient post-compression recovery.</p><hr><h3>ReSync: Riemannian Subgradient-based Robust Rotation Synchronization</h3>
<p>Huikang Liu, Xiao Li, Anthony Man-Cho So</p>
<p><a href='https://openreview.net/forum?id=Xxllzjt6T5'>https://openreview.net/forum?id=Xxllzjt6T5</a></p>
<p><b>Keywords</b>: Manifold optimization, Riemannian subgradient method, rotation synchronization
</p><p><b>Compressor summary</b>: ReSync is a Riemannian subgradient algorithm for synchronizing robust rotations in various engineering applications, with strong theoretical guarantees and experimentally proven effectiveness.</p><hr><h3>Non-Rigid Shape Registration via Deep Functional Maps Prior</h3>
<p>Puhua Jiang, Mingze Sun, Ruqi Huang</p>
<p><a href='https://openreview.net/forum?id=XvfEYqEbIb'>https://openreview.net/forum?id=XvfEYqEbIb</a></p>
<p><b>Keywords</b>: shape registration; functional maps; unsupervised learning
</p><p><b>Compressor summary</b>: The paper introduces a learning-based method for registering non-rigid shapes without correspondence supervision using deep functional maps and a trained orientation regressor, achieving state-of-the-art results on several benchmarks.</p><hr><h3>Self-supervised Graph Neural Networks via Low-Rank Decomposition</h3>
<p>Liang Yang, Runjie Shi, Qiuliang Zhang, Bingxin Niu, Zhen Wang, Xiaochun Cao, Chuan Wang</p>
<p><a href='https://openreview.net/forum?id=XvGQ6F3sG8'>https://openreview.net/forum?id=XvGQ6F3sG8</a></p>
<p><b>Keywords</b>: Graph neural network, Self-supervised learning, Low-Rank recovery
</p><p><b>Compressor summary</b>: This paper proposes Low-Rank Decomposition-based methods for graph neural networks to address issues with propagation-based approaches and improve local and long-distance node representation.</p><hr><h3>LayoutGPT: Compositional Visual Planning and Generation with Large Language Models</h3>
<p>Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Xuehai He, S Basu, Xin Eric Wang, William Yang Wang</p>
<p><a href='https://openreview.net/forum?id=Xu8aG5Q8M3'>https://openreview.net/forum?id=Xu8aG5Q8M3</a></p>
<p><b>Keywords</b>: Large Language Models, Compositional Image Generation, 3D scene synthesis
</p><p><b>Compressor summary</b>: The paragraph discusses LayoutGPT, a method that uses LLMs to generate layouts from text inputs and enhances the visual planning skills of LLMs, achieving superior performance in text-to-image generation and 3D indoor scene synthesis.</p><hr><h3>Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics</h3>
<p>Anton Voronov, Mikhail Khoroshikh, Artem Babenko, Max Ryabinin</p>
<p><a href='https://openreview.net/forum?id=Xs6Xwc0Glj'>https://openreview.net/forum?id=Xs6Xwc0Glj</a></p>
<p><b>Keywords</b>: text-to-image generation, diffusion models, early stopping
</p><p><b>Compressor summary</b>: The researchers propose a simple early stopping criterion that speeds up text-to-image adaptation by eight times without sacrificing quality.</p><hr><h3>A Randomized Approach to Tight Privacy Accounting</h3>
<p>Jiachen T. Wang, Saeed Mahloujifar, Tong Wu, Ruoxi Jia, Prateek Mittal</p>
<p><a href='https://openreview.net/forum?id=XrqqPDAsRE'>https://openreview.net/forum?id=XrqqPDAsRE</a></p>
<p><b>Keywords</b>: Differential Privacy; Privacy Accounting
</p><p><b>Compressor summary</b>: The paper introduces a new differential privacy approach (EVR) that verifies and releases query outputs based on an estimated privacy parameter, using Monte Carlo techniques to improve accuracy and efficiency in privacy accounting.</p><hr><h3>Locality-Aware Generalizable Implicit Neural Representation</h3>
<p>Doyup Lee, Chiheon Kim, Minsu Cho, Wook-Shin Han</p>
<p><a href='https://openreview.net/forum?id=XqcXf7ix5q'>https://openreview.net/forum?id=XqcXf7ix5q</a></p>
<p><b>Keywords</b>: implicit neural representations, representation learning, neural fields
</p><p><b>Compressor summary</b>: The proposed framework combines a transformer encoder with a locality-aware INR decoder to learn generalizable implicit neural representations that capture fine-grained details in spatial and spectral aspects, improving performance on downstream tasks like image generation.</p><hr><h3>Multi-Prompt Alignment for Multi-Source Unsupervised Domain Adaptation</h3>
<p>Haoran Chen, Xintong Han, Zuxuan Wu, Yu-Gang Jiang</p>
<p><a href='https://openreview.net/forum?id=Xq2s5yxzd2'>https://openreview.net/forum?id=Xq2s5yxzd2</a></p>
<p><b>Keywords</b>: multi source unsupervised domain adaptation; transfer learning; computer vision
</p><p><b>Compressor summary</b>: The text introduces Multi-Prompt Alignment (MPA), a method for unsupervised domain adaptation that uses prompt learning and auto-encoding to align source and target domains, achieving state-of-the-art results.</p><hr><h3>Regularized Behavior Cloning for Blocking the Leakage of Past Action Information</h3>
<p>Seokin Seo, HyeongJoo Hwang, Hongseok Yang, Kee-Eung Kim</p>
<p><a href='https://openreview.net/forum?id=XpmJNP8BVA'>https://openreview.net/forum?id=XpmJNP8BVA</a></p>
<p><b>Keywords</b>: Imitation learning, Information leakage, Causal Confusion
</p><p><b>Compressor summary</b>: The paper proposes a new regularization method called Past Action Leakage Regularization (PALR) for behavior cloning in offline imitation learning, which uses conditional independence to reduce the leakage of past actions into observation histories and improves performance.</p><hr><h3>SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions</h3>
<p>Ilias Diakonikolas, Daniel Kane, Lisheng Ren, Yuxin Sun</p>
<p><a href='https://openreview.net/forum?id=Xp68yXQiRk'>https://openreview.net/forum?id=Xp68yXQiRk</a></p>
<p><b>Keywords</b>: Non-Gaussian Component Analysis
</p><p><b>Compressor summary</b>: The paper investigates whether the chi-squared condition is necessary for proving SQ lower bounds for Non-Gaussian Component Analysis and shows that it is not.</p><hr><h3>Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone</h3>
<p>Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Ao Ma, Yiliang Lv, Yujun Shen, Deli Zhao, Jingren Zhou</p>
<p><a href='https://openreview.net/forum?id=XmpthbaJql'>https://openreview.net/forum?id=XmpthbaJql</a></p>
<p><b>Keywords</b>: Parameter-efficient Transfer Learning, Memory-efficient Transfer Learning, Residual Network, Vision Transformer, Foundation Model
</p><p><b>Compressor summary</b>: Res-Tuning is a new tuning paradigm that separates tuners from the backbone, allowing for flexible combination of various tuning strategies and improved efficiency in foundation models.</p><hr><h3>Distributed Inference and Fine-tuning of Large Language Models Over The Internet</h3>
<p>Alexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, Colin Raffel</p>
<p><a href='https://openreview.net/forum?id=XmN7ZNbUAe'>https://openreview.net/forum?id=XmN7ZNbUAe</a></p>
<p><b>Keywords</b>: volunteer computing, distributed deep learning, distributed inference, efficient inference, large language models
</p><p><b>Compressor summary</b>: The authors propose methods to efficiently run large language models on geodistributed devices, addressing reliability and load-balancing issues, and demonstrating their approach with Petals, a decentralized system that speeds up inference and fine-tuning of LLMs.</p><hr><h3>Exact Representation of Sparse Networks with Symmetric Nonnegative Embeddings</h3>
<p>Sudhanshu Chanpuriya, Ryan A. Rossi, Anup Rao, Tung Mai, Nedim Lipka, Zhao Song, Cameron N Musco</p>
<p><a href='https://openreview.net/forum?id=XlvsieCnAX'>https://openreview.net/forum?id=XlvsieCnAX</a></p>
<p><b>Keywords</b>: graph, network, embeddings, arboricity, factorization, model, community, nonnegative
</p><p><b>Compressor summary</b>: The paragraph describes a new graph factorization model that captures both similar and dissimilar links between nodes and can represent low arboricity graphs with good performance on community detection and link prediction tasks.</p><hr><h3>Structure-free Graph Condensation: From Large-scale Graphs to Condensed Graph-free Data</h3>
<p>Xin Zheng, Miao Zhang, Chunyang Chen, Quoc Viet Hung Nguyen, Xingquan Zhu, Shirui Pan</p>
<p><a href='https://openreview.net/forum?id=XkcufOcgUc'>https://openreview.net/forum?id=XkcufOcgUc</a></p>
<p><b>Keywords</b>: graph neural networks (GNNs), graph condensation, training trajectory meta-matching, graph neural feature score
</p><p><b>Compressor summary</b>: The paper introduces SFGC, a new method for reducing the size of large-scale graphs by synthesizing small-scale graph-free data that implicitly encodes topology structure information without explicit graph structures, and evaluates its effectiveness and generalization ability through two components: meta-matching and graph neural feature score.</p><hr><h3>ASIF: Coupled Data Turns Unimodal Models to Multimodal without Training</h3>
<p>Antonio Norelli, Marco Fumero, Valentino Maiorca, Luca Moschella, Emanuele Rodolà, Francesco Locatello</p>
<p><a href='https://openreview.net/forum?id=XjOj3ZmWEl'>https://openreview.net/forum?id=XjOj3ZmWEl</a></p>
<p><b>Keywords</b>: Representation learning, Multimodal models, Analogy, Sparsity, Nonparametric, Relative representations, Language, Semiotics
</p><p><b>Compressor summary</b>: The paper proposes a new method to create a common space between visual and language domains without training, using single-domain encoders and fewer image-text pairs, and shows its advantages in interpretability, fast deployment, and zero-shot transfer ability.</p><hr><h3>Batch Bayesian Optimization For Replicable Experimental Design</h3>
<p>Zhongxiang Dai, Quoc Phong Nguyen, Sebastian Shenghong Tay, Daisuke Urano, Richalynn Leong, Bryan Kian Hsiang Low, Patrick Jaillet</p>
<p><a href='https://openreview.net/forum?id=Xj4LJiXvlX'>https://openreview.net/forum?id=Xj4LJiXvlX</a></p>
<p><b>Keywords</b>: Bayesian optimization, Gaussian processes, AI4Science
</p><p><b>Compressor summary</b>: The BTS-RED framework proposes three algorithms to tackle trade-offs in experimental design problems with large, heteroscedastic noise by adaptively choosing replications and balancing average performance and variability.</p><hr><h3>Deep Insights into Noisy Pseudo Labeling on Graph Data</h3>
<p>Botao WANG, Jia Li, Yang Liu, Jiashun Cheng, Yu Rong, Wenjia Wang, Fugee Tsung</p>
<p><a href='https://openreview.net/forum?id=XhNlBvb4XV'>https://openreview.net/forum?id=XhNlBvb4XV</a></p>
<p><b>Keywords</b>: Pseudo labeling, Graph data, Error analysis, Cautious
</p><p><b>Compressor summary</b>: Pseudo labeling is a technique to expand labeled data for graph learning models, but it can introduce errors; this paper analyzes these errors and proposes a cautious methodology to improve performance.</p><hr><h3>On the Minimax Regret for Online Learning with Feedback Graphs</h3>
<p>Khaled Eldowa, Emmanuel Esposito, Tommaso Cesari, Nicolò Cesa-Bianchi</p>
<p><a href='https://openreview.net/forum?id=XfYpIaKDb6'>https://openreview.net/forum?id=XfYpIaKDb6</a></p>
<p><b>Keywords</b>: Online learning, Feedback graphs, Multiarmed bandits
</p><p><b>Compressor summary</b>: The paper presents an improved upper and lower bound for the regret of online learning with different types of feedback graphs, using FTRL with $q$-Tsallis entropy and new techniques for time-varying graphs.</p><hr><h3>Towards Robust and Expressive Whole-body Human Pose and Shape Estimation</h3>
<p>Hui En Pang, Zhongang Cai, Lei Yang, Qingyi Tao, Zhonghua Wu, Tianwei Zhang, Ziwei Liu</p>
<p><a href='https://openreview.net/forum?id=XfKnoW4Zef'>https://openreview.net/forum?id=XfKnoW4Zef</a></p>
<p><b>Keywords</b>: Whole-body, SMPLX Model, Human Pose and Shape Estimation, Human Mesh Recovery
</p><p><b>Compressor summary</b>: The paper proposes a new framework to improve whole-body pose and shape estimation by addressing challenges related to bounding box quality, robustness to augmentations, and pixel alignment.</p><hr><h3>Deep Recurrent Optimal Stopping</h3>
<p>NIRANJAN DAMERA VENKATA, Chiranjib Bhattacharyya</p>
<p><a href='https://openreview.net/forum?id=XetXfkYZ6i'>https://openreview.net/forum?id=XetXfkYZ6i</a></p>
<p><b>Keywords</b>: optimal stopping, recurrent neural networks, probabilistic graphical models, policy gradient methods
</p><p><b>Compressor summary</b>: The paper introduces an OSPG algorithm that uses RNNs to optimize value functions in non-Markovian optimal stopping problems without recursion, reducing the curse of non-Markovianity and the need for costly Monte Carlo simulations.</p><hr><h3>Hierarchical Integration Diffusion Model for Realistic Image Deblurring</h3>
<p>Zheng Chen, Yulun Zhang, Ding Liu, Bin Xia, Jinjin Gu, Linghe Kong, Xin Yuan</p>
<p><a href='https://openreview.net/forum?id=XeMryhpniy'>https://openreview.net/forum?id=XeMryhpniy</a></p>
<p><b>Keywords</b>: image deblurring, diffusion model
</p><p><b>Compressor summary</b>: The Hierarchical Integration Diffusion Model (HI-Diff) is a new image deblurring method that uses a compact latent space and multiple scales to improve efficiency, accuracy, and generalization compared to previous diffusion models.</p><hr><h3>Bayesian Learning via Q-Exponential Process</h3>
<p>Shuyi Li, Michael O'Connor, Shiwei Lan</p>
<p><a href='https://openreview.net/forum?id=XddoUFpjkP'>https://openreview.net/forum?id=XddoUFpjkP</a></p>
<p><b>Keywords</b>: Functional Regularization, Besov Process, $Q$-Exponential Distribution, Elliptic Contour Distribution
</p><p><b>Compressor summary</b>: The paper introduces a new stochastic process, Q-exponential (Q-EP), for high-dimensional optimization and modeling that generalizes the $q$-exponential distribution and improves upon Gaussian process in terms of penalty strength and flexibility.</p><hr><h3>On Calibrating Diffusion Probabilistic Models</h3>
<p>Tianyu Pang, Cheng Lu, Chao Du, Min Lin, Shuicheng YAN, Zhijie Deng</p>
<p><a href='https://openreview.net/forum?id=XcQzXeF7fX'>https://openreview.net/forum?id=XcQzXeF7fX</a></p>
<p><b>Keywords</b>: Diffusion Probabilistic Models, Model Calibration
</p><p><b>Compressor summary</b>: The paper proposes a calibration method for diffusion probabilistic models that reduces the score matching loss and increases the model likelihood, with experiments on various datasets to validate the approach.</p><hr><h3>Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target Data</h3>
<p>Cheng-Hao Tu, Hong-You Chen, Zheda Mai, Jike Zhong, Vardaan Pahuja, Tanya Berger-Wolf, Song Gao, Charles Stewart, Yu Su, Wei-Lun Chao</p>
<p><a href='https://openreview.net/forum?id=XbVnNXaIQY'>https://openreview.net/forum?id=XbVnNXaIQY</a></p>
<p><b>Keywords</b>: Fine-tuning, Transfer learning, Domain adaptation, Continual learning, Robustness, Personalization
</p><p><b>Compressor summary</b>: The paragraph discusses a learning problem involving adapting a model to a new domain using limited data and presents challenges and solutions to improve classification accuracy for all classes.</p><hr><h3>DiViNeT: 3D Reconstruction from Disparate Views using Neural Template Regularization</h3>
<p>Aditya Vora, Akshay Gadi Patil, Hao Zhang</p>
<p><a href='https://openreview.net/forum?id=XbInLmYLDr'>https://openreview.net/forum?id=XbInLmYLDr</a></p>
<p><b>Keywords</b>: Multi-view Neural 3D Reconstruction, Sparse and Disparate Views, Neural Rendering, Volume Rendering
</p><p><b>Compressor summary</b>: DiViNet is a method that uses neural templates to reconstruct 3D surface details from few RGB images by regularizing the ill-posed problem.</p><hr><h3>Making Scalable Meta Learning Practical</h3>
<p>Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, Willie Neiswanger, Pengtao Xie, Emma Strubell, Eric Xing</p>
<p><a href='https://openreview.net/forum?id=Xazhn0JoNx'>https://openreview.net/forum?id=Xazhn0JoNx</a></p>
<p><b>Keywords</b>: meta learning, bilevel optimization, large-scale learning, implicit differentiation
</p><p><b>Compressor summary</b>: SAMA is a meta learning framework that improves scalability, reduces computational burden, and supports adaptive optimizers while achieving state-of-the-art results in text and image classification tasks.</p><hr><h3>Provable Training for Graph Contrastive Learning</h3>
<p>Yue Yu, Xiao Wang, Mengmei Zhang, Nian Liu, Chuan Shi</p>
<p><a href='https://openreview.net/forum?id=Xasl21tSOf'>https://openreview.net/forum?id=Xasl21tSOf</a></p>
<p><b>Keywords</b>: Graph Contrastive Learning, Graph Neural Networks, Bound Propagation
</p><p><b>Compressor summary</b>: The paper proposes a metric called "node compactness" and a regularization method named PrOvable Training (POT) to improve Graph Contrastive Learning by addressing its imbalanced training across nodes.</p><hr><h3>Binary Radiance Fields</h3>
<p>Seungjoo Shin, Jaesik Park</p>
<p><a href='https://openreview.net/forum?id=XY6BnwIh4q'>https://openreview.net/forum?id=XY6BnwIh4q</a></p>
<p><b>Keywords</b>: neural radiance fields, inverse rendering, binarization
</p><p><b>Compressor summary</b>: The paper introduces BiRF, a compact way to store radiance fields using binary encoding and 2D-3D grids, which outperforms existing efficient models in static scene reconstruction with minimal storage space.</p><hr><h3>Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification</h3>
<p>Rui Wang, Pei Pei Li, Huaibo Huang, Chunshui Cao, Ran He, Zhaofeng He</p>
<p><a href='https://openreview.net/forum?id=XXagS1RQH0'>https://openreview.net/forum?id=XXagS1RQH0</a></p>
<p><b>Keywords</b>: Ordinal Classification, Representation Learning, Vision-Language, Prompt Learning
</p><p><b>Compressor summary</b>: The paper introduces L2RCLIP, a method that uses language-driven ordering alignment and vision-language models to improve ordinal classification tasks such as age estimation, HCI classification, and aesthetic assessment.</p><hr><h3>Have it your way: Individualized Privacy Assignment for DP-SGD</h3>
<p>Franziska Boenisch, Christopher Mühl, Adam Dziedzic, Roy Rinberg, Nicolas Papernot</p>
<p><a href='https://openreview.net/forum?id=XXPzBhOs4f'>https://openreview.net/forum?id=XXPzBhOs4f</a></p>
<p><b>Keywords</b>: privacy, machine learning, differential privacy, DP-SGD, individualized privacy
</p><p><b>Compressor summary</b>: The paper proposes Individualized DP-SGD (IDP-SGD), a variant of Differentially Private Stochastic Gradient Descent that allows for tailored privacy budgets based on individual user preferences, improving privacy-utility trade-offs.</p><hr><h3>On the Size and Approximation Error of Distilled Datasets</h3>
<p>Alaa Maalouf, Murad Tukan, Noel Loo, Ramin Hasani, Mathias Lechner, Daniela Rus</p>
<p><a href='https://openreview.net/forum?id=XWYv4BNShP'>https://openreview.net/forum?id=XWYv4BNShP</a></p>
<p><b>Keywords</b>: Dataset Distillation, Size and Approximation Error
</p><p><b>Compressor summary</b>: The paper analyzes the theoretical limitations and guarantees of dataset distillation using kernel ridge regression methods and provides the first proof of existence of small distilled datasets with corresponding excess risk for shift-invariant kernels.</p><hr><h3>Implicit Contrastive Representation Learning with Guided Stop-gradient</h3>
<p>Byeongchan Lee, Sehyun Lee</p>
<p><a href='https://openreview.net/forum?id=XUu2GloTXb'>https://openreview.net/forum?id=XUu2GloTXb</a></p>
<p><b>Keywords</b>: representation learning, self-supervised learning, contrastive learning
</p><p><b>Compressor summary</b>: The paper introduces a novel method for self-supervised representation learning using an asymmetric network architecture, which stabilizes training, boosts performance, and works well with small batch sizes and no predictor.</p><hr><h3>A Bayesian Approach To Analysing Training Data Attribution In Deep Learning</h3>
<p>Elisa Nguyen, Minjoon Seo, Seong Joon Oh</p>
<p><a href='https://openreview.net/forum?id=XSCYxDp3yE'>https://openreview.net/forum?id=XSCYxDp3yE</a></p>
<p><b>Keywords</b>: training data attribution, interpretability, explainability, data-driven xai
</p><p><b>Compressor summary</b>: This paper introduces a Bayesian perspective on training data attribution techniques for deep models, highlighting the challenges of noise and suggesting to only use TDA for predictions consistently influenced by specific training data.</p><hr><h3>Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions</h3>
<p>Hao Wang, Luxi He, Rui Gao, Flavio Calmon</p>
<p><a href='https://openreview.net/forum?id=XRy4YQYLe0'>https://openreview.net/forum?id=XRy4YQYLe0</a></p>
<p><b>Keywords</b>: information theory, fair machine learning
</p><p><b>Compressor summary</b>: The text discusses two types of discrimination in ML models and provides methods to measure and reduce them, especially when dealing with missing data.</p><hr><h3>Block-State Transformers</h3>
<p>Jonathan Pilault, Mahan Fathi, Orhan Firat, Christopher Pal, Pierre-Luc Bacon, Ross Goroshin</p>
<p><a href='https://openreview.net/forum?id=XRTxIBs2eu'>https://openreview.net/forum?id=XRTxIBs2eu</a></p>
<p><b>Keywords</b>: State Space Models, Efficient Transformers, Long Range Language Modeling, Language Modeling
</p><p><b>Compressor summary</b>: The Block-State Transformer combines an SSM sublayer for long-range context and a Block Transformer sublayer for short-term sequence representation, improving language modeling perplexity and speed on parallelizable architectures.</p><hr><h3>Mirror Diffusion Models for Constrained and Watermarked Generation</h3>
<p>Guan-Horng Liu, Tianrong Chen, Evangelos Theodorou, Molei Tao</p>
<p><a href='https://openreview.net/forum?id=XPWEtXzlLy'>https://openreview.net/forum?id=XPWEtXzlLy</a></p>
<p><b>Keywords</b>: diffusion models, constrained generation, constrained manifold, mirror map, watermarked generation, generation privacy
</p><p><b>Compressor summary</b>: The paper proposes Mirror Diffusion Models, a new type of diffusion models that can generate data on constrained sets without losing tractability, by using a mirror map in a standard Euclidean space and embedding invisible information for safety and privacy purposes.</p><hr><h3>FreeMask: Synthetic Images with Dense Annotations Make Stronger Segmentation Models</h3>
<p>Lihe Yang, Xiaogang Xu, Bingyi Kang, Yinghuan Shi, Hengshuang Zhao</p>
<p><a href='https://openreview.net/forum?id=XOotfgPiUF'>https://openreview.net/forum?id=XOotfgPiUF</a></p>
<p><b>Keywords</b>: learning from synthetic, semantic segmentation, generative models
</p><p><b>Compressor summary</b>: FreeMask uses synthetic images generated by generative models to ease the data collection and annotation process for semantic segmentation, achieving comparable or better performance than real images.</p><hr><h3>TD Convergence: An Optimization Perspective</h3>
<p>Kavosh Asadi, Shoham Sabach, Yao Liu, Omer Gottesman, Rasool Fakoor</p>
<p><a href='https://openreview.net/forum?id=XOCbdqxAR2'>https://openreview.net/forum?id=XOCbdqxAR2</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Temporal Difference Learning, Value Function Optimization, Convergence
</p><p><b>Compressor summary</b>: The paper analyzes how the temporal-difference learning algorithm works as an iterative optimization process, identifies factors that affect its convergence or divergence, and proves its convergence in different settings, explaining its success in reinforcement learning.</p><hr><h3>CosNet: A Generalized Spectral Kernel Network</h3>
<p>Yanfang Xue, Pengfei Fang, Jinyue Tian, Shipeng Zhu, hui xue</p>
<p><a href='https://openreview.net/forum?id=XNBeTgYcAq'>https://openreview.net/forum?id=XNBeTgYcAq</a></p>
<p><b>Keywords</b>: Spectral kernel; complex-valued networks
</p><p><b>Compressor summary</b>: The CosNet is a generalized spectral kernel network that preserves the inherent complex-valued representation in time-sequential data and improves the analysis of time-varying statistical characteristics by combining complex-valued spectral kernels with neural networks.</p><hr><h3>DiffUTE: Universal Text Editing Diffusion Model</h3>
<p>Haoxing Chen, Zhuoer Xu, Zhangxuan Gu, jun lan, 行 郑, Yaohui Li, Changhua Meng, Huijia Zhu, Weiqiang Wang</p>
<p><a href='https://openreview.net/forum?id=XKeSauhUdJ'>https://openreview.net/forum?id=XKeSauhUdJ</a></p>
<p><b>Keywords</b>: Diffusion model, text editing, self-supervied learning
</p><p><b>Compressor summary</b>: The paper proposes DiffUTE, a self-supervised text editing diffusion model that can draw multilingual characters and edit images realistically using web data.</p><hr><h3>Incentives in Private Collaborative Machine Learning</h3>
<p>Rachael Hwee Ling Sim, Yehong Zhang, Trong Nghia Hoang, Xinyi Xu, Bryan Kian Hsiang Low, Patrick Jaillet</p>
<p><a href='https://openreview.net/forum?id=XKP3mAsNHd'>https://openreview.net/forum?id=XKP3mAsNHd</a></p>
<p><b>Keywords</b>: Incentives, Privacy, Shapley fairness, Collaborative machine learning, data valuation, reward, sufficient statistics
</p><p><b>Compressor summary</b>: The authors propose a method for collaborative machine learning that incentivizes data sharing while preserving privacy by using differential privacy and adjusting sufficient statistics.</p><hr><h3>Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models</h3>
<p>Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alex Dimakis, Sanjay Shakkottai</p>
<p><a href='https://openreview.net/forum?id=XKBFdYwfRo'>https://openreview.net/forum?id=XKBFdYwfRo</a></p>
<p><b>Keywords</b>: Inverse Problems, Posterior Sampling, Latent Diffusion Model, Stable Diffusion, Sample Recovery
</p><p><b>Compressor summary</b>: The framework uses pre-trained latent diffusion models to solve linear inverse problems and shows superior performance in various tasks like image restoration and enhancement.</p><hr><h3>Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise</h3>
<p>Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, Tom Goldstein</p>
<p><a href='https://openreview.net/forum?id=XH3ArccntI'>https://openreview.net/forum?id=XH3ArccntI</a></p>
<p><b>Keywords</b>: Generative Models, Computer Vision, Diffusion Models
</p><p><b>Compressor summary</b>: Deterministic image degradations can be used to create a family of generative diffusion models without the need for added noise.</p><hr><h3>DreamWaltz: Make a Scene with Complex 3D Animatable Avatars</h3>
<p>Yukun Huang, Jianan Wang, Ailing Zeng, He CAO, Xianbiao Qi, Yukai Shi, Zheng-Jun Zha, Lei Zhang</p>
<p><a href='https://openreview.net/forum?id=XGXL1E8Yyo'>https://openreview.net/forum?id=XGXL1E8Yyo</a></p>
<p><b>Keywords</b>: Avatar Generation, 3D Content Creation, NeRF, Diffusion Model
</p><p><b>Compressor summary</b>: DreamWaltz is a framework that generates and animates complex 3D avatars from text guidance using novel techniques like SDS, 3D-aware skeleton conditioning, and image priors.</p><hr><h3>Convergence analysis of ODE models for accelerated first-order methods via positive semidefinite kernels</h3>
<p>Jungbin Kim, Insoon Yang</p>
<p><a href='https://openreview.net/forum?id=XFE6zpevLc'>https://openreview.net/forum?id=XFE6zpevLc</a></p>
<p><b>Keywords</b>: convex optimization, accelerated gradient methods
</p><p><b>Compressor summary</b>: The paper introduces a new way to analyze optimization methods using functional analysis tools that leads to faster convergence rates and reveals a link between function values and gradient norms.</p><hr><h3>Learning Regularized Monotone Graphon Mean-Field Games</h3>
<p>Fengzhuo Zhang, Vincent Tan, Zhaoran Wang, Zhuoran Yang</p>
<p><a href='https://openreview.net/forum?id=XF923QPCGw'>https://openreview.net/forum?id=XF923QPCGw</a></p>
<p><b>Keywords</b>: mean-field approximation, graphon games, multi-agent reinforcement learning
</p><p><b>Compressor summary</b>: The paper explores the existence and efficient learning of Nash Equilibria in regularized Graphon Mean-Field Games, extending previous results and proposing new algorithms for discrete-time learning under weak monotonicity conditions.</p><hr><h3>An Inductive Bias for Tabular Deep Learning</h3>
<p>Ege Beyazit, Jonathan Kozaczuk, Bo Li, Vanessa Wallace, Bilal H Fadlallah</p>
<p><a href='https://openreview.net/forum?id=XEUc1JegGt'>https://openreview.net/forum?id=XEUc1JegGt</a></p>
<p><b>Keywords</b>: Tabular Deep Learning, Spectral Bias, Neural Networks
</p><p><b>Compressor summary</b>: The paper proposes a new neural network layer that helps improve deep learning performance on irregularly shaped tabular data by learning low-frequency representations of the input features.</p><hr><h3>GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection</h3>
<p>Jinggang Chen, Junjie Li, Xiaoyang Qu, Jianzong Wang, Jiguang Wan, Jing Xiao</p>
<p><a href='https://openreview.net/forum?id=XEBzQP3e7B'>https://openreview.net/forum?id=XEBzQP3e7B</a></p>
<p><b>Keywords</b>: out-of-distribution detection, distribution shifts, attribution gradients
</p><p><b>Compressor summary</b>: The paper proposes a method called GAIA that uses gradient abnormality to detect out-of-distribution examples in deep neural networks, improving their reliability and safety.</p><hr><h3>Binarized Neural Machine Translation</h3>
<p>Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, Orhan Firat</p>
<p><a href='https://openreview.net/forum?id=XAyPlfmWpu'>https://openreview.net/forum?id=XAyPlfmWpu</a></p>
<p><b>Keywords</b>: neural network quantization, binarized transformer, machine translation, scaling law
</p><p><b>Compressor summary</b>: The paper introduces BMT, a novel binarization technique for Transformers that achieves the same quality as float models while being much smaller, by using additional LayerNorms and residual connections to address one-bit weight variance issues.</p><hr><h3>Non-Convex Bilevel Optimization with Time-Varying Objective Functions</h3>
<p>Sen Lin, Daouda Sow, Kaiyi Ji, Yingbin Liang, Ness Shroff</p>
<p><a href='https://openreview.net/forum?id=X9Vjq9Fuhq'>https://openreview.net/forum?id=X9Vjq9Fuhq</a></p>
<p><b>Keywords</b>: Bilevel Optimization, Time-Varying Functions, Single-Loop, Sublinear Bilevel Local Regret
</p><p><b>Compressor summary</b>: SOBOW is an efficient online bilevel optimizer with window averaging that works well for streaming data and time-varying functions, achieving sublinear regret.</p><hr><h3>Self-Weighted Contrastive Learning among Multiple Views for Mitigating Representation Degeneration</h3>
<p>Jie Xu, Shuo Chen, Yazhou Ren, Xiaoshuang Shi, Heng Tao Shen, Gang Niu, Xiaofeng Zhu</p>
<p><a href='https://openreview.net/forum?id=X8dbFcAox2'>https://openreview.net/forum?id=X8dbFcAox2</a></p>
<p><b>Keywords</b>: Multi-view learning, Contrastive learning, Representation degeneration, Self-supervised learning
</p><p><b>Compressor summary</b>: SEM is a new contrastive learning framework for multi-view scenarios that adaptively strengthens useful views and weakens unreliable ones, while improving representation quality with reconstruction regularization.</p><hr><h3>Stochastic Approximation Algorithms for Systems of Interacting Particles</h3>
<p>Mohammad Reza Karimi Jaghargh, Ya-Ping Hsieh, Andreas Krause</p>
<p><a href='https://openreview.net/forum?id=X6mwdEVYvc'>https://openreview.net/forum?id=X6mwdEVYvc</a></p>
<p><b>Keywords</b>: Stochastic Approximation, Mean-Field Dynamics, Dynamical Systems, Neural Networks, Sampling
</p><p><b>Compressor summary</b>: The paper proposes a new framework to connect discrete-time interacting particle systems and their mean-field limits, improving the analysis of these systems in machine learning tasks.</p><hr><h3>On the Planning Abilities of Large Language Models - A Critical Investigation</h3>
<p>Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati</p>
<p><a href='https://openreview.net/forum?id=X6dEqXIsEW'>https://openreview.net/forum?id=X6dEqXIsEW</a></p>
<p><b>Keywords</b>: Large Language Models, Planning, LLMs for autonomous and heuristic planning guidance
</p><p><b>Compressor summary</b>: The paper investigates how well large language models (LLMs) can generate plans autonomously and as heuristic guidance for other planners, finding limited success in the former but more promise in the latter.</p><hr><h3>Explain Any Concept: Segment Anything Meets Concept-Based Explanation</h3>
<p>Ao Sun, Pingchuan Ma, Yuanyuan Yuan, Shuai Wang</p>
<p><a href='https://openreview.net/forum?id=X6TBBsz9qi'>https://openreview.net/forum?id=X6TBBsz9qi</a></p>
<p><b>Keywords</b>: EXplainable AI, Machine Learning, Computer Vision
</p><p><b>Compressor summary</b>: The paper introduces Explain Any Concept (EAC), a method that uses the Segment Anything Model (SAM) to explain deep neural network decisions with any concept, improving human understanding of computer vision tasks.</p><hr><h3>Latent SDEs on Homogeneous Spaces</h3>
<p>Sebastian Zeng, Florian Graf, Roland Kwitt</p>
<p><a href='https://openreview.net/forum?id=X6Eapo5paw'>https://openreview.net/forum?id=X6Eapo5paw</a></p>
<p><b>Keywords</b>: Variational Bayesian inference, stochastic differential equation, homogeneous spaces, geometric Euler-Maruyama, time series
</p><p><b>Compressor summary</b>: The authors study variational Bayesian inference for latent stochastic differential equations (SDEs) on the unit sphere, which simplifies the learning process and achieves good results on time series tasks.</p><hr><h3>Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly</h3>
<p>Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen</p>
<p><a href='https://openreview.net/forum?id=X5MH7iut9K'>https://openreview.net/forum?id=X5MH7iut9K</a></p>
<p><b>Keywords</b>: adversarial examples, adversarial transferability, black-box attack
</p><p><b>Compressor summary</b>: The paragraph introduces TA-Bench, a benchmark for transfer-based methods to attack black-box DNNs, which evaluates 30+ methods on 10 popular victim models using ImageNet data.</p><hr><h3>Performance-optimized deep neural networks are evolving into worse models of inferotemporal visual cortex</h3>
<p>Drew Linsley, Ivan F Rodriguez Rodriguez, Thomas FEL, Michael Arcaro, Saloni Sharma, Margaret Livingstone, Thomas Serre</p>
<p><a href='https://openreview.net/forum?id=X4mmXQ4Nxw'>https://openreview.net/forum?id=X4mmXQ4Nxw</a></p>
<p><b>Keywords</b>: neural system identification, behavioral alignment, neural object recognition
</p><p><b>Compressor summary</b>: The authors find that as deep neural networks (DNNs) improve at object recognition, they become worse models of inferotemporal (IT) neuron responses to images, and propose a method to align DNN representations with human vision using the neural harmonizer.</p><hr><h3>Causal Imitability Under Context-Specific Independence Relations</h3>
<p>Fateme Jamshidi, Sina Akbari, Negar Kiyavash</p>
<p><a href='https://openreview.net/forum?id=X3IeHRD0zf'>https://openreview.net/forum?id=X3IeHRD0zf</a></p>
<p><b>Keywords</b>: causal inference, conditional independence, context-specific independence relations, imitability
</p><p><b>Compressor summary</b>: The paper explores the benefits of using context-specific independence information in causal imitation learning and provides a necessary and sufficient graphical criterion for it, as well as an algorithm that considers both CSI and data.</p><hr><h3>Nearly Optimal Bounds for Cyclic Forgetting</h3>
<p>William Joseph Swartworth, Deanna Needell, Rachel Ward, Mark Kong, Halyun Jeong</p>
<p><a href='https://openreview.net/forum?id=X25L5AjHig'>https://openreview.net/forum?id=X25L5AjHig</a></p>
<p><b>Keywords</b>: catastrophic forgetting, linear systems
</p><p><b>Compressor summary</b>: The paper derives theoretical limits on how much information is forgotten in continual learning for linear tasks and introduces a new characterization of numerical ranges of products of projections.</p><hr><h3>Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning</h3>
<p>James Queeney, Mouhacine Benosman</p>
<p><a href='https://openreview.net/forum?id=X0CIxqYc4Z'>https://openreview.net/forum?id=X0CIxqYc4Z</a></p>
<p><b>Keywords</b>: deep reinforcement learning, model uncertainty, safety, risk-averse, distributionally robust
</p><p><b>Compressor summary</b>: The paper presents a deep reinforcement learning method for safe decision making in uncertain environments using risk-averse distortion risk measures, without minimax optimization, and shows its effectiveness on continuous control tasks with safety constraints.</p><hr><h3>STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning</h3>
<p>Weipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, Gao Huang</p>
<p><a href='https://openreview.net/forum?id=WxnrX42rnS'>https://openreview.net/forum?id=WxnrX42rnS</a></p>
<p><b>Keywords</b>: deep learning, reinforcement learning, model-based reinforcement learning, world model, learning in imagination, transformer, variational autoencoders, sequence modeling
</p><p><b>Compressor summary</b>: STORM is a world model architecture that combines Transformers and variational autoencoders to improve sequence modeling and generation in model-based reinforcement learning, achieving a record performance on the Atari 100k benchmark with less interaction experience and faster training.</p><hr><h3>Probabilistic Invariant Learning with Randomized Linear Classifiers</h3>
<p>Leonardo Cotta, Gal Yehuda, Assaf Schuster, Chris J. Maddison</p>
<p><a href='https://openreview.net/forum?id=WwP2JaXAtB'>https://openreview.net/forum?id=WwP2JaXAtB</a></p>
<p><b>Keywords</b>: Invariant Learning, Geometric Deep Learning, Set Representations, Graph Representations, Expressive Power, Randomized Algorithms
</p><p><b>Compressor summary</b>: The paper proposes Randomized Linear Classifiers, a probabilistic model that can approximate any smooth function and preserve invariances while using fewer resources than deterministic neural networks.</p><hr><h3>Weighted ROC Curve in Cost Space: Extending AUC to Cost-Sensitive Learning</h3>
<p>Huiyang Shao, Qianqian Xu, Zhiyong Yang, Peisong Wen, Gao Peifeng, Qingming Huang</p>
<p><a href='https://openreview.net/forum?id=WsmBcJarWW'>https://openreview.net/forum?id=WsmBcJarWW</a></p>
<p><b>Keywords</b>: AUC, Cost Learning, Bilevel, machine learning
</p><p><b>Compressor summary</b>: The paper proposes a new cost-sensitive learning framework that integrates cost distribution into the AUC metric using a bilevel optimization method and shows its effectiveness in handling long-tail datasets.</p><hr><h3>Distributionally Robust Ensemble of Lottery Tickets Towards Calibrated  Sparse Network Training</h3>
<p>Hitesh Sapkota, Dingrong Wang, ZHIQIANG TAO, Qi Yu</p>
<p><a href='https://openreview.net/forum?id=WrRG0C1Vo5'>https://openreview.net/forum?id=WrRG0C1Vo5</a></p>
<p><b>Keywords</b>: sparse network training, model calibration
</p><p><b>Compressor summary</b>: The paper proposes a new method to improve the reliability of sparse network training by using Distributionally Robust Optimization (DRO) to create an ensemble of diverse and complementary sub-networks that capture different data distributions and reduce overconfidence.</p><hr><h3>MotionGPT: Human Motion as a Foreign Language</h3>
<p>Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang YU, Tao Chen</p>
<p><a href='https://openreview.net/forum?id=WqiZJGNkjn'>https://openreview.net/forum?id=WqiZJGNkjn</a></p>
<p><b>Keywords</b>: 3d motion, motion generation, human motion synthesis, text-driven, text-to-motion
</p><p><b>Compressor summary</b>: The paper introduces MotionGPT, a unified model for language and human motion that can handle various motion-related tasks, by treating human motion as a specific language.</p><hr><h3>Regularizing Neural Networks with Meta-Learning Generative Models</h3>
<p>Shin'ya Yamaguchi, Daiki Chijiwa, Sekitoshi Kanai, Atsutoshi Kumagai, Hisashi Kashima</p>
<p><a href='https://openreview.net/forum?id=WpuBEtrn0t'>https://openreview.net/forum?id=WpuBEtrn0t</a></p>
<p><b>Keywords</b>: Deep Learning, Generative Models, Generative Data Augmentation, Regularization, Meta-Learning
</p><p><b>Compressor summary</b>: The paper proposes a method called meta generative regularization (MGR) to improve generative data augmentation for deep learning by using synthetic samples to regularize feature extractors instead of training classifiers, which can avoid performance degradation and boost accuracy, especially on smaller datasets.</p><hr><h3>Joint Training of Deep Ensembles Fails Due to Learner Collusion</h3>
<p>Alan Jeffares, Tennison Liu, Jonathan Crabbé, Mihaela van der Schaar</p>
<p><a href='https://openreview.net/forum?id=WpGLxnOWhn'>https://openreview.net/forum?id=WpGLxnOWhn</a></p>
<p><b>Keywords</b>: Deep Ensembles, Deep Learning
</p><p><b>Compressor summary</b>: Joint optimization of ensemble loss in deep ensembles leads to artificial diversity that fails to generalize, causing a larger performance gap.</p><hr><h3>Accelerating Value Iteration with Anchoring</h3>
<p>Jongmin Lee, Ernest K. Ryu</p>
<p><a href='https://openreview.net/forum?id=Wn82NbmvJy'>https://openreview.net/forum?id=Wn82NbmvJy</a></p>
<p><b>Keywords</b>: Value Iteration, Reinforcement Learning, Reinforcement Learning Theory, Dynamic Programming, Acceleration, Anchoring mechanism
</p><p><b>Compressor summary</b>: Anc-VI is an accelerated value iteration method that reduces Bellman error faster than standard VI by using an anchoring mechanism.</p><hr><h3>Contextual Bandits and Imitation Learning with Preference-Based Active Queries</h3>
<p>Ayush Sekhari, Karthik Sridharan, Wen Sun, Runzhe Wu</p>
<p><a href='https://openreview.net/forum?id=WmqYhqvz5i'>https://openreview.net/forum?id=WmqYhqvz5i</a></p>
<p><b>Keywords</b>: Contextual Bandit, Imitation Learning, Learning from Expert Feedback, Theory
</p><p><b>Compressor summary</b>: The paper proposes an algorithm for preference-based contextual bandits and imitation learning that minimizes regret and query complexity, and can outperform a sub-optimal expert with fewer queries.</p><hr><h3>Large Language Models as Commonsense Knowledge for Large-Scale Task Planning</h3>
<p>Zirui Zhao, Wee Sun Lee, David Hsu</p>
<p><a href='https://openreview.net/forum?id=Wjp1AYB8lH'>https://openreview.net/forum?id=Wjp1AYB8lH</a></p>
<p><b>Keywords</b>: Embodied Task Planning, Large Language Models, Human-Robot Interaction
</p><p><b>Compressor summary</b>: The paper proposes an LLM-MCTS algorithm that combines a commonsense world model from LLMs with MCTS search, improving task planning efficiency and performance on complex tasks.</p><hr><h3>Action Inference by Maximising Evidence: Zero-Shot Imitation from Observation with World Models</h3>
<p>Xingyuan Zhang, Philip Becker-Ehmck, Patrick van der Smagt, Maximilian Karl</p>
<p><a href='https://openreview.net/forum?id=WjlCQxpuxU'>https://openreview.net/forum?id=WjlCQxpuxU</a></p>
<p><b>Keywords</b>: Imitation Learning, World Models, Latent Variable Model, Transfer Learning, Variational Inference
</p><p><b>Compressor summary</b>: The paper proposes AIME, a method for imitating expert behaviors using world models without needing additional environment interactions or training.</p><hr><h3>Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective</h3>
<p>Huayang Li, Tian Lan, Zihao Fu, Deng Cai, Lemao Liu, Nigel Collier, Taro Watanabe, Yixuan Su</p>
<p><a href='https://openreview.net/forum?id=WjgCRrOgip'>https://openreview.net/forum?id=WjgCRrOgip</a></p>
<p><b>Keywords</b>: language modeling, text generation, natural language processing
</p><p><b>Compressor summary</b>: This paper proposes a simple explanation and solution for the neural text degeneration problem by showing that penalizing repetitions in training data reduces this issue.</p><hr><h3>Differentiable Registration of Images and LiDAR Point Clouds with VoxelPoint-to-Pixel Matching</h3>
<p>Junsheng Zhou, Baorui Ma, Wenyuan Zhang, Yi Fang, Yu-Shen Liu, Zhizhong Han</p>
<p><a href='https://openreview.net/forum?id=WjWifKqmcG'>https://openreview.net/forum?id=WjWifKqmcG</a></p>
<p><b>Keywords</b>: LiDAR Point Clouds, 2D images, Cross-modality registration, Matching
</p><p><b>Compressor summary</b>: The paper proposes a method to improve cross-modality registration between 2D images and 3D point clouds using a structured latent space, a triplet network, and a differentiable probabilistic PnP solver, achieving better results on KITTI and nuScenes datasets.</p><hr><h3>Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems</h3>
<p>Sihan Zeng, Thinh T. Doan, Justin Romberg</p>
<p><a href='https://openreview.net/forum?id=WjDj6W872v'>https://openreview.net/forum?id=WjDj6W872v</a></p>
<p><b>Keywords</b>: Reinforcement learning, superlevel sets, minimax optimization, robust reinforcement learning
</p><p><b>Compressor summary</b>: The paper explores new properties of policy optimization landscapes in reinforcement learning and uses them to derive minimax theorems for robust reinforcement learning problems under adversarial rewards.</p><hr><h3>Anonymous Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization</h3>
<p>Adel Javanmard, Vahab Mirrokni</p>
<p><a href='https://openreview.net/forum?id=WfsWy59bX2'>https://openreview.net/forum?id=WfsWy59bX2</a></p>
<p><b>Keywords</b>: high-dimensional regression, generalization error, asymptotic analysis, Convex Gaussian Minimax Theorem, regularization
</p><p><b>Compressor summary</b>: This paper explores how using anonymous cluster centers in personalized recommendation systems can protect user privacy and sometimes improve model performance, by analyzing the generalization error using the Convex Gaussian Minimax Theorem.</p><hr><h3>Towards Semi-Structured Automatic ICD Coding via Tree-based Contrastive Learning</h3>
<p>Chang Lu, Chandan K. Reddy, Ping Wang, Yue Ning</p>
<p><a href='https://openreview.net/forum?id=Wff6DWFY2W'>https://openreview.net/forum?id=Wff6DWFY2W</a></p>
<p><b>Keywords</b>: ICD Coding, Contrastive Learning, NLP, Healthcare, Text Categorization, Pre-training
</p><p><b>Compressor summary</b>: The authors propose an algorithm to segment clinical notes into sections and use contrastive pre-training and masked section training to improve ICD coding performance with limited data.</p><hr><h3>Low Tensor Rank Learning of Neural Dynamics</h3>
<p>Arthur Pellegrino, N Alex Cayco Gajic, Angus Chadwick</p>
<p><a href='https://openreview.net/forum?id=WcoX8eJJjI'>https://openreview.net/forum?id=WcoX8eJJjI</a></p>
<p><b>Keywords</b>: Recurrent Neural Networks, Computational Neuroscience, Neural Data Analysis, Tensor, Learning
</p><p><b>Compressor summary</b>: The study investigates how low rank structures in recurrent neural networks (RNNs) evolve during learning and their implications for population connectivity and learning dynamics in both biological and artificial neural networks.</p><hr><h3>Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense</h3>
<p>Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Frederick Wieting, Mohit Iyyer</p>
<p><a href='https://openreview.net/forum?id=WbFhFvjjKj'>https://openreview.net/forum?id=WbFhFvjjKj</a></p>
<p><b>Keywords</b>: AI-generated text detection, text detection, paraphrasing, attacks, retrieval, defenses, large language models, LLMs
</p><p><b>Compressor summary</b>: The authors develop a paraphrase generation model (DIPPER) that evades several AI text detection algorithms and propose a defense method based on searching for semantically-similar texts in a database.</p><hr><h3>DeWave: Discrete Encoding of EEG Waves for EEG to Text Translation</h3>
<p>Yiqun Duan, Charles Zhou, Zhen Wang, Yu-Kai Wang, Chin-teng Lin</p>
<p><a href='https://openreview.net/forum?id=WaLI8slhLw'>https://openreview.net/forum?id=WaLI8slhLw</a></p>
<p><b>Keywords</b>: EEG; Neural Encoding; Brain Computer Interface
</p><p><b>Compressor summary</b>: DeWave is a novel framework that translates brain signals into natural language without relying on eye-tracking or event markers, improving EEG-to-text translation accuracy.</p><hr><h3>Online learning of long-range dependencies</h3>
<p>Nicolas Zucchet, Robert Meier, Simon Schug, Asier Mujika, Joao Sacramento</p>
<p><a href='https://openreview.net/forum?id=Wa1GGPqjUn'>https://openreview.net/forum?id=Wa1GGPqjUn</a></p>
<p><b>Keywords</b>: online learning, linear recurrent units, temporal credit assignment, biologically-plausible learning, local learning rules, neuromorphic computing
</p><p><b>Compressor summary</b>: The paper introduces an efficient online learning algorithm for recurrent neural networks that leverages independent modules, enabling them to learn long-range dependencies and opening new possibilities in neuromorphic computing.</p><hr><h3>Necessary and Sufficient Conditions for Optimal Decision Trees using Dynamic Programming</h3>
<p>Jacobus G.M. van der Linden, Mathijs de Weerdt, Emir Demirović</p>
<p><a href='https://openreview.net/forum?id=WYYpxVsKpR'>https://openreview.net/forum?id=WYYpxVsKpR</a></p>
<p><b>Keywords</b>: optimal decision trees, dynamic programming, separability
</p><p><b>Compressor summary</b>: The authors propose a dynamic programming framework for global optimization of decision trees that can handle separable objectives and constraints, improving scalability and performance over general-purpose solvers.</p><hr><h3>Max-Margin Token Selection in Attention Mechanism</h3>
<p>Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, Samet Oymak</p>
<p><a href='https://openreview.net/forum?id=WXc8O8ghLH'>https://openreview.net/forum?id=WXc8O8ghLH</a></p>
<p><b>Keywords</b>: attention mechanism, implicit bias, margin maximization, nonconvex optimization, prompt tuning
</p><p><b>Compressor summary</b>: The paper analyzes the attention mechanism in transformer models and proves that it converges to a max-margin solution for optimal token selection.</p><hr><h3>Mixture Weight Estimation and Model Prediction in Multi-source Multi-target Domain Adaptation</h3>
<p>Yuyang Deng, Ilja Kuzborskij, Mehrdad Mahdavi</p>
<p><a href='https://openreview.net/forum?id=WVmus8NWE8'>https://openreview.net/forum?id=WVmus8NWE8</a></p>
<p><b>Keywords</b>: Multi-source domain adaptation; minimax optimization; learning theory
</p><p><b>Compressor summary</b>: The paper proposes algorithms to efficiently learn from heterogeneous data sources and adapt to different target distributions with theoretical guarantees.</p><hr><h3>Rank-DETR for High Quality Object Detection</h3>
<p>Yifan Pu, Weicong Liang, Yiduo Hao, Yuhui Yuan, Yukang Yang, Chao Zhang, Han Hu, Gao Huang</p>
<p><a href='https://openreview.net/forum?id=WUott1ZvRj'>https://openreview.net/forum?id=WUott1ZvRj</a></p>
<p><b>Keywords</b>: Object Detection
</p><p><b>Compressor summary</b>: The Rank-DETR method improves DETR-based object detectors by enhancing ranking accuracy and reducing false positives, leading to better localization and detection performance.</p><hr><h3>Orthogonal Non-negative Tensor Factorization based Multi-view Clustering</h3>
<p>Jing Li, Quanxue Gao, QIANQIAN WANG, Ming Yang, Wei Xia</p>
<p><a href='https://openreview.net/forum?id=WRtlsxA5h7'>https://openreview.net/forum?id=WRtlsxA5h7</a></p>
<p><b>Keywords</b>: Multi-view clustering, tensor Schatten p-norm, non-negative matrix factorization.
</p><p><b>Compressor summary</b>: The paragraph describes a novel multi-view clustering method based on orthogonal non-negative tensor factorization, which considers both within-view and between-view information and achieves good performance on various datasets.</p><hr><h3>DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting</h3>
<p>Salva Rühling Cachay, Bo Zhao, Hailey James, Rose Yu</p>
<p><a href='https://openreview.net/forum?id=WRGldGm5Hz'>https://openreview.net/forum?id=WRGldGm5Hz</a></p>
<p><b>Keywords</b>: AI for science, diffusion models, scientific machine learning, probabilistic forecasting
</p><p><b>Compressor summary</b>: The authors present a dynamics-informed diffusion model that leverages temporal data dynamics for forecasting complex systems and demonstrate its advantages over Gaussian noise-based models.</p><hr><h3>Full-Atom Protein Pocket Design via Iterative Refinement</h3>
<p>ZAIXI ZHANG, Zepu Lu, Zhongkai Hao, Marinka Zitnik, Qi Liu</p>
<p><a href='https://openreview.net/forum?id=WPdGRRJaPb'>https://openreview.net/forum?id=WPdGRRJaPb</a></p>
<p><b>Keywords</b>: Graph Representation Learning, AI for Science
</p><p><b>Compressor summary</b>: FAIR is a framework that co-designs protein pocket sequence and 3D structure for ligand binding by refining residue types, backbone coordinates, and sidechain atoms in an efficient full-shot manner.</p><hr><h3>A Robust Exact Algorithm for the Euclidean Bipartite Matching Problem</h3>
<p>Akshaykumar G Gattani, Sharath Raghvendra, Pouyan Shirzadian</p>
<p><a href='https://openreview.net/forum?id=WPbIAdB6aQ'>https://openreview.net/forum?id=WPbIAdB6aQ</a></p>
<p><b>Keywords</b>: Euclidean bipartite matching, exact algorithms, primal dual method
</p><p><b>Compressor summary</b>: The paper presents a new algorithm for computing minimum-cost bipartite matching between two sets of points, which has faster execution time than previous methods, especially for stochastic point sets with real-valued coordinates and any dimension.</p><hr><h3>Projection-Free Methods for Solving Nonconvex-Concave Saddle Point Problems</h3>
<p>Morteza Boroun, Erfan Yazdandoost Hamedani, Afrooz Jalilzadeh</p>
<p><a href='https://openreview.net/forum?id=WO1kHC5Lfz'>https://openreview.net/forum?id=WO1kHC5Lfz</a></p>
<p><b>Keywords</b>: Saddle Point Problem, Projection-free method
</p><p><b>Compressor summary</b>: The paper proposes efficient single-loop projection-free methods for constrained saddle point problems using regularization and nested approximation techniques, achieving convergence guarantees in limited iterations.</p><hr><h3>Unsupervised Anomaly Detection with Rejection</h3>
<p>Lorenzo Perini, Jesse Davis</p>
<p><a href='https://openreview.net/forum?id=WK8LQzzHwW'>https://openreview.net/forum?id=WK8LQzzHwW</a></p>
<p><b>Keywords</b>: Anomaly Detection, Learning with Rejection, Unsupervised Learning
</p><p><b>Compressor summary</b>: The paper proposes a method to improve anomaly detection by rejecting predictions with high uncertainty using a constant threshold on a stability metric, while providing theoretical guarantees and empirical results.</p><hr><h3>Contrastive Modules with Temporal Attention for Multi-Task Reinforcement Learning</h3>
<p>Siming Lan, Rui Zhang, Qi Yi, Jiaming Guo, Shaohui Peng, Yunkai Gao, Fan Wu, Ruizhi Chen, Zidong Du, Xing Hu, Xishan Zhang, Ling Li, Yunji Chen</p>
<p><a href='https://openreview.net/forum?id=WIrZh2XxLT'>https://openreview.net/forum?id=WIrZh2XxLT</a></p>
<p><b>Keywords</b>: reinforcement learning, multi-task learning, contrastive learning
</p><p><b>Compressor summary</b>: The paper proposes CMTA, a method for multi-task reinforcement learning that uses contrastive learning to prevent conflicts within tasks and temporal attention to combine shared modules at a finer level than tasks, improving performance on Meta-World benchmark tasks.</p><hr><h3>Rank-N-Contrast: Learning Continuous Representations for Regression</h3>
<p>Kaiwen Zha, Peng Cao, Jeany Son, Yuzhe Yang, Dina Katabi</p>
<p><a href='https://openreview.net/forum?id=WHedsAeatp'>https://openreview.net/forum?id=WHedsAeatp</a></p>
<p><b>Keywords</b>: regression, representation learning, continuity
</p><p><b>Compressor summary</b>: Rank-N-Contrast (RNC) is a framework that learns continuous representations for regression by contrasting samples based on their rankings in the target space, improving performance, robustness, efficiency, and generalization.</p><hr><h3>Top-Ambiguity Samples Matter: Understanding Why Deep Ensemble Works in Selective Classification</h3>
<p>Qiang Ding, Yixuan Cao, Ping Luo</p>
<p><a href='https://openreview.net/forum?id=WBq6Q4ml04'>https://openreview.net/forum?id=WBq6Q4ml04</a></p>
<p><b>Keywords</b>: selective classification, uncertainty estimation, ensemble learning
</p><p><b>Compressor summary</b>: The paper analyzes why ensemble methods improve prediction reliability, focusing on top-ambiguity samples where member models differ, and proves that ensembles have lower selective risk under certain conditions.</p><hr><h3>NCDL:  A Framework for Deep Learning on non-Cartesian Lattices</h3>
<p>Joshua John Horacsek, Usman Alim</p>
<p><a href='https://openreview.net/forum?id=WBXYGBQXiB'>https://openreview.net/forum?id=WBXYGBQXiB</a></p>
<p><b>Keywords</b>: Computer Vision and Pattern Recognition
</p><p><b>Compressor summary</b>: The paper introduces the lattice tensor data structure to enable machine learning on non-Cartesian domains using standard algorithms and a software library.</p><hr><h3>Nonparametric Boundary Geometry in Physics Informed Deep Learning</h3>
<p>Scott Alexander Cameron, Arnu Pretorius, Stephen J. Roberts</p>
<p><a href='https://openreview.net/forum?id=WAd5ZRdFoc'>https://openreview.net/forum?id=WAd5ZRdFoc</a></p>
<p><b>Keywords</b>: PINNs, physics informed neural networks, geometric deep learning, neural operator, PDEs
</p><p><b>Compressor summary</b>: The paper proposes a neural network that takes triangular meshes as input and solves partial differential equations quickly, without retraining or fixed geometry representation.</p><hr><h3>BadTrack: A Poison-Only Backdoor Attack on Visual Object Tracking</h3>
<p>Bin Huang, Jiaqian Yu, Yiwei Chen, Siyang Pan, Qiang Wang, Zhi Wang</p>
<p><a href='https://openreview.net/forum?id=W9pJx9sFCh'>https://openreview.net/forum?id=W9pJx9sFCh</a></p>
<p><b>Keywords</b>: Backdoor Attack, Visual Object Tracking, Deep Learning, Poison-Only
</p><p><b>Compressor summary</b>: This paper proposes a poison-only backdoor attack on visual object tracking that attaches a trigger pattern to the background of video frames, which degrades the performance of VOT trackers.</p><hr><h3>Black-box Backdoor Defense via Zero-shot Image Purification</h3>
<p>Yucheng Shi, Mengnan Du, Xuansheng Wu, Zihan Guan, Jin Sun, Ninghao Liu</p>
<p><a href='https://openreview.net/forum?id=W6U2xSbiE1'>https://openreview.net/forum?id=W6U2xSbiE1</a></p>
<p><b>Keywords</b>: backdoor defense, black-box defense, diffusion model
</p><p><b>Compressor summary</b>: The paper proposes a novel defense framework called Zero-shot Image Purification (ZIP) to defend against backdoor attacks on black-box models by applying a linear transformation and using a pre-trained diffusion model to recover the purified image.</p><hr><h3>Toward Understanding Generative Data Augmentation</h3>
<p>Chenyu Zheng, Guoqiang Wu, Chongxuan Li</p>
<p><a href='https://openreview.net/forum?id=W5Clq1bSrR'>https://openreview.net/forum?id=W5Clq1bSrR</a></p>
<p><b>Keywords</b>: generative data augmentation, algorithmic stability, non-i.i.d. learning
</p><p><b>Compressor summary</b>: Generative data augmentation can improve classification performance in various tasks by obtaining fake labeled examples from a trained conditional generative model, but its effect has not been well studied theoretically; this paper establishes a stability bound and shows how it helps learning in different settings.</p><hr><h3>Fair Canonical Correlation Analysis</h3>
<p>Zhuoping Zhou, Davoud Ataee Tarzanagh, Bojian Hou, Boning Tong, Jia Xu, Yanbo Feng, Qi Long, Li Shen</p>
<p><a href='https://openreview.net/forum?id=W3cDd5xlKZ'>https://openreview.net/forum?id=W3cDd5xlKZ</a></p>
<p><b>Keywords</b>: Fairness, Canonical Correlation Analysis, Riemannian Optimization, Pareto Optimization
</p><p><b>Compressor summary</b>: The paper proposes a framework for minimizing unfairness in Canonical Correlation Analysis by balancing correlation levels across groups while maintaining accuracy and evaluating its effectiveness on various datasets.</p><hr><h3>PolyDiffuse: Polygonal Shape Reconstruction via Guided Set Diffusion Models</h3>
<p>Jiacheng Chen, Ruizhi Deng, Yasutaka Furukawa</p>
<p><a href='https://openreview.net/forum?id=W2ZBLdfa16'>https://openreview.net/forum?id=W2ZBLdfa16</a></p>
<p><b>Keywords</b>: Structured Reconstruction, Floorplan Reconstruction, HD Map Construction, Diffusion Models
</p><p><b>Compressor summary</b>: PolyDiffuse is a new algorithm that uses Diffusion Models to reconstruct structured data like polygonal shapes from sensor data, overcoming challenges related to denoising ambiguity and initial noise choice.</p><hr><h3>Are Vision Transformers More Data Hungry Than Newborn Visual Systems?</h3>
<p>Lalit Pandey, Samantha Marie Waters Wood, Justin Newell Wood</p>
<p><a href='https://openreview.net/forum?id=W23ZTdsabj'>https://openreview.net/forum?id=W23ZTdsabj</a></p>
<p><b>Keywords</b>: vision transformer, newborn, controlled rearing, object recognition, data hungry
</p><p><b>Compressor summary</b>: ViTs and chicks both learn view-invariant object recognition in impoverished visual environments, challenging the assumption that ViTs are more data hungry than brains.</p><hr><h3>Exposing Attention Glitches with Flip-Flop Language Modeling</h3>
<p>Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang</p>
<p><a href='https://openreview.net/forum?id=VzmpXQAn6E'>https://openreview.net/forum?id=VzmpXQAn6E</a></p>
<p><b>Keywords</b>: Transformers, language models, hallucinations, long-range dependencies, generalization, extrapolation, out-of-distribution
</p><p><b>Compressor summary</b>: The text discusses attention glitches, a phenomenon where large language models sometimes make errors in reasoning, and introduces flip-flop language modeling as a way to study and potentially improve these models' extrapolative abilities.</p><hr><h3>Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage</h3>
<p>Jose Blanchet, Miao Lu, Tong Zhang, Han Zhong</p>
<p><a href='https://openreview.net/forum?id=VzLBMkc7tB'>https://openreview.net/forum?id=VzLBMkc7tB</a></p>
<p><b>Keywords</b>: distributionally robust offline reinforcement learning, double pessimism, general function approximation
</p><p><b>Compressor summary</b>: The paper introduces Doubly Pessimistic Model-based Policy Optimization ($\texttt{P}^2\texttt{MPO}$), a novel algorithm framework for distributionally robust offline reinforcement learning that combines flexible model estimation and doubly pessimistic policy optimization to overcome distribution shift and perturbation.</p><hr><h3>VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks</h3>
<p>Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai</p>
<p><a href='https://openreview.net/forum?id=Vx1JadlOIt'>https://openreview.net/forum?id=Vx1JadlOIt</a></p>
<p><b>Keywords</b>: Large Vision-Language Model, Detection, Image Caption
</p><p><b>Compressor summary</b>: The paper introduces VisionLLM, an LLM-based framework that treats images as foreign languages and uses language instructions to perform open-ended vision tasks with high task customization and competitive performance on COCO detection benchmark.</p><hr><h3>RevColV2: Exploring Disentangled Representations in Masked Image Modeling</h3>
<p>Qi Han, Yuxuan Cai, Xiangyu Zhang</p>
<p><a href='https://openreview.net/forum?id=VvnfMeC3gQ'>https://openreview.net/forum?id=VvnfMeC3gQ</a></p>
<p><b>Keywords</b>: architecture design, representation learning, masked image modeling, self-supervised learning
</p><p><b>Compressor summary</b>: RevColV2 is a new architecture for masked image modeling that keeps the entire autoencoder during pre-training and fine-tuning, maintaining disentangled low-level and semantic information and achieving competitive performance on various vision tasks.</p><hr><h3>Multiplication-Free Transformer Training via Piecewise Affine Operations</h3>
<p>Atli Kosson, Martin Jaggi</p>
<p><a href='https://openreview.net/forum?id=Vtqymej1tA'>https://openreview.net/forum?id=Vtqymej1tA</a></p>
<p><b>Keywords</b>: multiplication-free, neural architectures, piecewise linear networks, piecewise affine networks, efficient training, efficient arithmetics
</p><p><b>Compressor summary</b>: The authors propose a method to replace matrix multiplications and non-linearities with piecewise affine approximations, achieving fully multiplication-free training of neural networks without sacrificing performance.</p><hr><h3>Evaluating Cognitive Maps and Planning in Large Language Models with CogEval</h3>
<p>Ida Momennejad, Hosein Hasanbeig, Felipe Vieira Frujeri, Hiteshi Sharma, Nebojsa Jojic, Hamid Palangi, Robert Ness, Jonathan Larson</p>
<p><a href='https://openreview.net/forum?id=VtkGvGcGe3'>https://openreview.net/forum?id=VtkGvGcGe3</a></p>
<p><b>Keywords</b>: Large Language Models, LLM evaluation, model comparison, GPT-4, graph analysis, cognitive science, cognitive map, hippocampus, planning, multi-step planning, reasoning, community graph
</p><p><b>Compressor summary</b>: The authors propose CogEval, a systematic evaluation protocol for cognitive abilities in LLMs, and find that current LLMs have limited planning ability and fail to understand cognitive maps.</p><hr><h3>AdaptSSR: Pre-training User Model with Augmentation-Adaptive Self-Supervised Ranking</h3>
<p>Yang Yu, Qi Liu, Kai Zhang, Yuren Zhang, Chao Song, Min Hou, Yuqing Yuan, ZHIhao Ye, ZAIXI ZHANG, Sanshi Lei Yu</p>
<p><a href='https://openreview.net/forum?id=VsbrdJpwpT'>https://openreview.net/forum?id=VsbrdJpwpT</a></p>
<p><b>Keywords</b>: User Model Pre-training, Data Augmentation, Contrastive Learning
</p><p><b>Compressor summary</b>: AdaptSSR is a new pretext task for user modeling that improves performance by capturing similarity orders between implicitly and explicitly augmented views and other users' views, while adjusting the similarity constraint based on estimated similarities.</p><hr><h3>Autonomous Capability Assessment of Sequential Decision-Making Systems in Stochastic Settings</h3>
<p>Pulkit Verma, Rushang Karia, Siddharth Srivastava</p>
<p><a href='https://openreview.net/forum?id=VqclD6Nfaj'>https://openreview.net/forum?id=VqclD6Nfaj</a></p>
<p><b>Keywords</b>: Sequential Decision Making, Interpretable Models, Relational Model Learning, Black-Box Agents, Symbolic Descriptions
</p><p><b>Compressor summary</b>: The paper proposes an active-learning method to learn a probabilistic model of black-box AI systems with sequential decision-making capabilities, allowing users to understand and assess their safety in stochastic settings.</p><hr><h3>Does Graph Distillation See Like Vision Dataset Counterpart?</h3>
<p>Beining Yang, Kai Wang, Qingyun Sun, Cheng Ji, Xingcheng Fu, Hao Tang, Yang You, Jianxin Li</p>
<p><a href='https://openreview.net/forum?id=VqIWgUVsXc'>https://openreview.net/forum?id=VqIWgUVsXc</a></p>
<p><b>Keywords</b>: data-efficient learning, graph generation, graph neural networks
</p><p><b>Compressor summary</b>: The paper proposes a new method, SGDD, to preserve the original structure information of large-scale graphs when generating synthetic ones, improving performance in cross-architecture settings and specific tasks while reducing storage costs.</p><hr><h3>Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution</h3>
<p>Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Peter Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey A. Gritsenko, Mario Lucic, Neil Houlsby</p>
<p><a href='https://openreview.net/forum?id=VpGFHmI7e5'>https://openreview.net/forum?id=VpGFHmI7e5</a></p>
<p><b>Keywords</b>: Vision Transformer, variable aspect ratio, flexible inference, efficient training
</p><p><b>Compressor summary</b>: NaViT is a flexible and efficient computer vision model that can handle inputs of different resolutions and aspect ratios without resizing them, leading to improved performance and robustness.</p><hr><h3>Chanakya: Learning Runtime Decisions for Adaptive Real-Time Perception</h3>
<p>Anurag Ghosh, Vaibhav Balloli, Akshay Nambi, Aditya Singh, Tanuja Ganu</p>
<p><a href='https://openreview.net/forum?id=VpCjozUOM2'>https://openreview.net/forum?id=VpCjozUOM2</a></p>
<p><b>Keywords</b>: approximate execution framework; real time perception; latency-accuracy tradeoffs
</p><p><b>Compressor summary</b>: The paper introduces Chanakya, a learned framework that optimizes real-time perception by automatically balancing accuracy and latency tradeoffs in decisions induced by intrinsic and extrinsic factors.</p><hr><h3>Fine-Tuning Language Models with Just Forward Passes</h3>
<p>Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, Sanjeev Arora</p>
<p><a href='https://openreview.net/forum?id=Vota6rFhBQ'>https://openreview.net/forum?id=Vota6rFhBQ</a></p>
<p><b>Keywords</b>: language models, fine-tuning, zeroth order optimization, memory efficiency
</p><p><b>Compressor summary</b>: MeZO is a memory-efficient zeroth-order optimizer that can fine-tune large language models with less memory and GPU resources than backpropagation, achieving comparable performance on various tasks.</p><hr><h3>Bounded rationality in structured  density estimation</h3>
<p>Tianyuan Teng, Li Kevin Wenliang, Hang Zhang</p>
<p><a href='https://openreview.net/forum?id=VnfeOjR73Q'>https://openreview.net/forum?id=VnfeOjR73Q</a></p>
<p><b>Keywords</b>: human representation of uncertainty; Bayesian inference; bounded rationality; inductive bias; Chinese Restaurant Process
</p><p><b>Compressor summary</b>: This study examines how humans learn to represent environmental uncertainty, finding that they tend to overestimate the number of clusters in a distribution, possibly due to cognitive limitations.</p><hr><h3>Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence</h3>
<p>Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, Trevor Darrell</p>
<p><a href='https://openreview.net/forum?id=Vm1zeYqwdc'>https://openreview.net/forum?id=Vm1zeYqwdc</a></p>
<p><b>Keywords</b>: semantic correspondence, hypercolumns, diffusion models, generative model representations
</p><p><b>Compressor summary</b>: The paper proposes Diffusion Hyperfeatures, a method to consolidate feature maps from diffusion models into per-pixel descriptors for semantic keypoint correspondence tasks on real and synthetic images.</p><hr><h3>Efficient Exploration in Continuous-time Model-based Reinforcement Learning</h3>
<p>Lenart Treven, Jonas Hübotter, Bhavya Sukhija, Florian Dorfler, Andreas Krause</p>
<p><a href='https://openreview.net/forum?id=VkhvDfY2dB'>https://openreview.net/forum?id=VkhvDfY2dB</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Optimal Control, Continuous Time
</p><p><b>Compressor summary</b>: The paper presents a reinforcement learning algorithm that uses nonlinear ODEs to model continuous-time dynamics and shows how to choose when and how often to observe the system for optimal exploration.</p><hr><h3>Nonparametric Teaching for Multiple Learners</h3>
<p>Chen Zhang, Xiaofeng Cao, Weiyang Liu, Ivor Tsang, James Kwok</p>
<p><a href='https://openreview.net/forum?id=VkUNovXoxx'>https://openreview.net/forum?id=VkUNovXoxx</a></p>
<p><b>Keywords</b>: Nonparametric machine teaching, Multiple learners
</p><p><b>Compressor summary</b>: The paper proposes a new framework called Multi-learner Nonparametric Teaching (MINT) that enables teachers to teach multiple students simultaneously more efficiently than traditional single-learner methods, especially when students can communicate with each other.</p><hr><h3>Efficient Activation Function Optimization through Surrogate Modeling</h3>
<p>Garrett Bingham, Risto Miikkulainen</p>
<p><a href='https://openreview.net/forum?id=ViFTWelHVZ'>https://openreview.net/forum?id=ViFTWelHVZ</a></p>
<p><b>Keywords</b>: automl, activation function, surrogate modeling, fisher information matrix, eigenvalues, optimization, umap, imagenet
</p><p><b>Compressor summary</b>: The paper introduces new datasets, methods, and findings to improve the design and optimization of activation functions in neural networks, including a surprising sigmoidal discovery.</p><hr><h3>DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions</h3>
<p>Haochen Wang, Junsong Fan, Yuxi Wang, Kaiyou Song, Tong Wang, Zhaoxiang Zhang</p>
<p><a href='https://openreview.net/forum?id=VhcsIxVEd9'>https://openreview.net/forum?id=VhcsIxVEd9</a></p>
<p><b>Keywords</b>: Self-Supervised Learning, Vision Transformer, Visual Representation Learning
</p><p><b>Compressor summary</b>: DropPos is a self-supervised pretext task that improves location awareness in Vision Transformers by reconstructing dropped positions with visual appearance and attention.</p><hr><h3>Variance-Reduced Gradient Estimation via Noise-Reuse in Online Evolution Strategies</h3>
<p>Oscar Li, James Harrison, Jascha Sohl-Dickstein, Virginia Smith, Luke Metz</p>
<p><a href='https://openreview.net/forum?id=VhbV56AJNt'>https://openreview.net/forum?id=VhbV56AJNt</a></p>
<p><b>Keywords</b>: Evolution Strategies, unrolled computation graph, online gradient estimation, variance reduction, stochastic gradient estimation
</p><p><b>Compressor summary</b>: The paper introduces Noise-Reuse Evolution Strategies (NRES), an unbiased online evolution strategies method for machine learning that has lower variance, faster convergence, and better parallelism than existing methods.</p><hr><h3>Estimating Riemannian Metric with Noise-Contaminated Intrinsic Distance</h3>
<p>Jiaming Qiu, Xiongtao Dai</p>
<p><a href='https://openreview.net/forum?id=VhLU3pStsl'>https://openreview.net/forum?id=VhLU3pStsl</a></p>
<p><b>Keywords</b>: metric learning, manifold learning, local metric, dissimilarity, geometry
</p><p><b>Compressor summary</b>: The paper proposes a new local regression approach to learn the Riemannian metric tensor from similarity measures between data points, and provides theoretical convergence rates and empirical results on various datasets.</p><hr><h3>Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models</h3>
<p>Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, Kwan-Yee K. Wong</p>
<p><a href='https://openreview.net/forum?id=VgQw8zXrH8'>https://openreview.net/forum?id=VgQw8zXrH8</a></p>
<p><b>Keywords</b>: computer vision, diffusion model, text-to-image generation
</p><p><b>Compressor summary</b>: The paper introduces Uni-ControlNet, a framework that allows for flexible and composable control of text-to-image diffusion models using different local and global controls with minimal fine-tuning and model size costs.</p><hr><h3>Learning Large-Scale MTP$_2$ Gaussian Graphical Models via Bridge-Block Decomposition</h3>
<p>Xiwen Wang, Jiaxi Ying, Daniel P. Palomar</p>
<p><a href='https://openreview.net/forum?id=Vfp8sDST4g'>https://openreview.net/forum?id=Vfp8sDST4g</a></p>
<p><b>Keywords</b>: MTP2 Gaussian Graphical Model, High-dimensional precision matrix estimation, Bridge-block decomposition.
</p><p><b>Compressor summary</b>: The paper proposes a method to learn large Gaussian graphical models by decomposing the problem into smaller sub-problems and using bridge concepts, which reduces computational complexity and improves existing algorithms' performance.</p><hr><h3>RDumb: A simple approach that questions our progress in continual test-time adaptation</h3>
<p>Ori Press, Steffen Schneider, Matthias Kuemmerer, Matthias Bethge</p>
<p><a href='https://openreview.net/forum?id=VfP6VTVsHc'>https://openreview.net/forum?id=VfP6VTVsHc</a></p>
<p><b>Keywords</b>: test time adaptation, continual adaptation, benchmarking, imagenet-c, imagenet classification, robustness, continual learning, imagenet benchmark
</p><p><b>Compressor summary</b>: The Continually Changing Corruptions (CCC) benchmark reveals that most Test-Time Adaptation (TTA) methods perform poorly over long timescales and are beaten by a simple resetting strategy.</p><hr><h3>Conformal Prediction for Uncertainty-Aware Planning with Diffusion Dynamics Model</h3>
<p>Jiankai Sun, Yiqi Jiang, Jianing Qiu, Parth Talpur Nobel, Mykel Kochenderfer, Mac Schwager</p>
<p><a href='https://openreview.net/forum?id=VeO03T59Sh'>https://openreview.net/forum?id=VeO03T59Sh</a></p>
<p><b>Keywords</b>: Uncertainty, Conformal Prediction, Dynamics Model
</p><p><b>Compressor summary</b>: PlanCP is a method that uses conformal prediction to quantify and reduce the uncertainty of diffusion dynamics models for trajectory prediction in robotic applications, improving performance on offline reinforcement learning and continuous planning tasks.</p><hr><h3>Efficient Algorithms for Generalized Linear Bandits with Heavy-tailed Rewards</h3>
<p>Bo Xue, Yimu Wang, Yuanyu Wan, Jinfeng Yi, Lijun Zhang</p>
<p><a href='https://openreview.net/forum?id=Vbm5UCaYeh'>https://openreview.net/forum?id=Vbm5UCaYeh</a></p>
<p><b>Keywords</b>: linear bandits, heavy-tailed, truncated, mean of medians
</p><p><b>Compressor summary</b>: The paper proposes two new algorithms for generalized linear bandits with heavy-tailed rewards and shows they have better performance than existing methods in terms of regret bounds.</p><hr><h3>Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing</h3>
<p>Wei Dong, Dawei Yan, Zhijun Lin, Peng Wang</p>
<p><a href='https://openreview.net/forum?id=VbYdaK8ek0'>https://openreview.net/forum?id=VbYdaK8ek0</a></p>
<p><b>Keywords</b>: computer vision; vision transformer; visual adapter; transfer learning
</p><p><b>Compressor summary</b>: The study proposes a new method called Adapter Re-Composing (ARC) to efficiently adapt pre-trained models for downstream tasks by reusing and sharing parameters in adapter design.</p><hr><h3>Correlation Aware Sparsified Mean Estimation Using Random Projection</h3>
<p>Shuli Jiang, Pranay Sharma, Gauri Joshi</p>
<p><a href='https://openreview.net/forum?id=VacSQpbI0U'>https://openreview.net/forum?id=VacSQpbI0U</a></p>
<p><b>Keywords</b>: distributed vector mean estimation, communication efficiency, cross-client correlation
</p><p><b>Compressor summary</b>: Rand-Proj-Spatial is a new method for communication-efficient distributed vector mean estimation that uses projection to a random subspace and correlation information to outperform existing techniques like Rand-$k$-Spatial.</p><hr><h3>Score-based Data Assimilation</h3>
<p>François Rozet, Gilles Louppe</p>
<p><a href='https://openreview.net/forum?id=VUvLSnMZdX'>https://openreview.net/forum?id=VUvLSnMZdX</a></p>
<p><b>Keywords</b>: data assimilation, score-based, generative modeling, posterior inference, dynamical systems
</p><p><b>Compressor summary</b>: The paper introduces score-based data assimilation, a method for inferring state trajectories in stochastic dynamical systems by learning a generative model that can handle long time horizons and complex dynamics without relying on transition dynamics.</p><hr><h3>Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry</h3>
<p>Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, Youngjung Uh</p>
<p><a href='https://openreview.net/forum?id=VUlYp3jiEI'>https://openreview.net/forum?id=VUlYp3jiEI</a></p>
<p><b>Keywords</b>: diffusion models, semantic image editing, differential geometry
</p><p><b>Compressor summary</b>: This paper analyzes the geometric structure of diffusion models' latent space and shows how it enables image editing through traversal without additional training.</p><hr><h3>Batchnorm Allows Unsupervised Radial Attacks</h3>
<p>Amur Ghose, Apurv Gupta, Yaoliang Yu, Pascal Poupart</p>
<p><a href='https://openreview.net/forum?id=VQ1heZKSLQ'>https://openreview.net/forum?id=VQ1heZKSLQ</a></p>
<p><b>Keywords</b>: Adversarial, Batch normalization, Robustness, Geometric, radial
</p><p><b>Compressor summary</b>: The text explains how intermediate latents from batch normalized deep image recognition models can be used to create adversarial examples without labels, and discusses the security implications of this finding.</p><hr><h3>Continuous-Time Functional Diffusion Processes</h3>
<p>Giulio Franzese, Giulio Corallo, Simone Rossi, Markus Heinonen, Maurizio Filippone, Pietro Michiardi</p>
<p><a href='https://openreview.net/forum?id=VPrir0p5b6'>https://openreview.net/forum?id=VPrir0p5b6</a></p>
<p><b>Keywords</b>: Hilbert spaces, Diffusion models, Stochastic Partial Differential Equations
</p><p><b>Compressor summary</b>: Functional Diffusion Processes (FDPs) are a new type of generative model that works with continuous data in function spaces, requiring less complex network architectures and achieving high-quality image generation.</p><hr><h3>LogSpecT: Feasible Graph Learning Model from Stationary Signals with Recovery Guarantees</h3>
<p>Shangyuan LIU, Linglingzhi Zhu, Anthony Man-Cho So</p>
<p><a href='https://openreview.net/forum?id=VPTZVVP4tm'>https://openreview.net/forum?id=VPTZVVP4tm</a></p>
<p><b>Keywords</b>: Graph Signal Processing, Spectral Template, Network Inference, Optimization, Linearized ADMM
</p><p><b>Compressor summary</b>: The paper introduces LogSpecT, a novel graph learning model that is always feasible and has recovery guarantees, and proposes an efficient algorithm with convergence guarantees to learn graphs from stationary signals.</p><hr><h3>Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery</h3>
<p>Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, Tom Goldstein</p>
<p><a href='https://openreview.net/forum?id=VOstHxDdsN'>https://openreview.net/forum?id=VOstHxDdsN</a></p>
<p><b>Keywords</b>: Diffusion Model, Generative AI, Prompt Discovery
</p><p><b>Compressor summary</b>: The paragraph discusses an optimization method for hard text prompts that can be used in various applications and allows users to easily generate and mix image concepts without prior knowledge.</p><hr><h3>Federated Learning via Meta-Variational Dropout</h3>
<p>Insu Jeon, Minui Hong, Junhyeog Yun, Gunhee Kim</p>
<p><a href='https://openreview.net/forum?id=VNyKBipt91'>https://openreview.net/forum?id=VNyKBipt91</a></p>
<p><b>Keywords</b>: Personalized Federated Learning, Variational Dropout, Meta-Learning, Bayesian Neural Network
</p><p><b>Compressor summary</b>: MetaVD is a novel Bayesian meta-learning approach that predicts dropout rates per client and improves Federated Learning in non-IID data settings by personalizing models and reducing communication costs.</p><hr><h3>Graph of Circuits with GNN for Exploring the Optimal Design Space</h3>
<p>Aditya Hemant Shahane, Saripilli Venkata Swapna Manjiri, Ankesh Jain, Sandeep Kumar</p>
<p><a href='https://openreview.net/forum?id=VNjJAWjuEU'>https://openreview.net/forum?id=VNjJAWjuEU</a></p>
<p><b>Keywords</b>: Analog design optimization, Analog synthesis, Graph Neural Networks, EDA, Graph learning, Optimization
</p><p><b>Compressor summary</b>: The paper introduces GCX, a framework that uses graph structure learning and graph neural networks to create a surrogate model for efficient exploration of the analog circuit design space in a semi-supervised learning framework.</p><hr><h3>Hyperbolic Space with Hierarchical Margin Boosts Fine-Grained Learning from Coarse Labels</h3>
<p>ShuLin Xu, Yifan Sun, Faen Zhang, Anqi Xu, Xiu-Shen Wei, Yi Yang</p>
<p><a href='https://openreview.net/forum?id=VMz5GhfxgV'>https://openreview.net/forum?id=VMz5GhfxgV</a></p>
<p><b>Keywords</b>: Fine-grained learning, Coarse-to-fine learning, Hyperbolic space, Hierarchical margin
</p><p><b>Compressor summary</b>: The paper proposes a novel hyperbolic space method for fine-grained recognition tasks that leverages hierarchical cosine margins to improve discriminative ability and achieves state-of-the-art results on five benchmark datasets.</p><hr><h3>UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models</h3>
<p>Xin Li, Sima Behpour, Thang Doan, Wenbin He, Liang Gou, Liu Ren</p>
<p><a href='https://openreview.net/forum?id=VMAgvbBBts'>https://openreview.net/forum?id=VMAgvbBBts</a></p>
<p><b>Keywords</b>: Unsupervised prompt learning, UP-DP, Data preselection
</p><p><b>Compressor summary</b>: This study proposes UP-DP, an unsupervised prompt learning method that improves data pre-selection by combining vision and text features from foundation models like BLIP-2, achieving up to 20% performance gain and showing generalizability across datasets.</p><hr><h3>Regret Minimization via Saddle Point Optimization</h3>
<p>Johannes Kirschner, Alireza Bakhtiari, Kushagra Chandak, Volodymyr Tkachuk, Csaba Szepesvari</p>
<p><a href='https://openreview.net/forum?id=VLnEFGu9V7'>https://openreview.net/forum?id=VLnEFGu9V7</a></p>
<p><b>Keywords</b>: sequential decision-making, decision-estimation coefficient, regret minimization, bandits, reinforcement learning, partial monitoring
</p><p><b>Compressor summary</b>: The paper presents a new algorithm (Anytime-E2D) that balances exploration and exploitation in sequential decision-making, based on re-parametrizing the existing DEC method, and demonstrates its effectiveness in high-dimensional linear bandits.</p><hr><h3>Test-Time Distribution Normalization for Contrastively Learned Visual-language Models</h3>
<p>Yifei Zhou, Juntao Ren, Fengyu Li, Ramin Zabih, Ser-Nam Lim</p>
<p><a href='https://openreview.net/forum?id=VKbEO2eh5w'>https://openreview.net/forum?id=VKbEO2eh5w</a></p>
<p><b>Keywords</b>: contrastive learning, pre-trained visual-language models, zero-shot learning, test-time augmentation
</p><p><b>Compressor summary</b>: The paper introduces Distribution Normalization (DN), which improves test-time performance in visual-language contrastive learning by approximating negative samples during inference without retraining or fine-tuning.</p><hr><h3>Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision</h3>
<p>Jiaxin Zhang, Zhuohang Li, Kamalika Das, Sricharan Kumar</p>
<p><a href='https://openreview.net/forum?id=VIaw1XHb4G'>https://openreview.net/forum?id=VIaw1XHb4G</a></p>
<p><b>Keywords</b>: multi-fidelity optimization, cost-effective learning, exploration-exploitation query, limited annotation budgets
</p><p><b>Compressor summary</b>: IMFL is a framework for developing small domain-specific language models using a mix of low-cost automatic and high-quality human annotations, achieving superior performance with limited budgets.</p><hr><h3>Delegated Classification</h3>
<p>Eden Saig, Inbal Talgam-Cohen, Nir Rosenfeld</p>
<p><a href='https://openreview.net/forum?id=VGLXjbTSYa'>https://openreview.net/forum?id=VGLXjbTSYa</a></p>
<p><b>Keywords</b>: Delegation, Algorithmic Contract Design, Moral Hazard, Learning Curves
</p><p><b>Compressor summary</b>: The paper proposes a framework for incentive-aware delegation of machine learning tasks using performance-based contracts that balance budget and accuracy, connecting classic statistical theory with modern learning curves and scaling laws.</p><hr><h3>Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features</h3>
<p>Mingli Zhu, Shaokui Wei, Hongyuan Zha, Baoyuan Wu</p>
<p><a href='https://openreview.net/forum?id=VFhN15Vlkj'>https://openreview.net/forum?id=VFhN15Vlkj</a></p>
<p><b>Keywords</b>: Backdoor Defense, Backdoor Learning, Trustworthy AI
</p><p><b>Compressor summary</b>: The proposed method filters trigger information from poisoned samples using a lightweight neural polarizer inserted as an intermediate layer in the backdoored model, requiring less clean data and being more efficient than other defense methods.</p><hr><h3>Mixed-Initiative Multiagent Apprenticeship Learning for Human Training of Robot Teams</h3>
<p>Esmaeil Seraj, Jerry Yuyang Xiong, Mariah L Schrum, Matthew Gombolay</p>
<p><a href='https://openreview.net/forum?id=VCOZaczCHg'>https://openreview.net/forum?id=VCOZaczCHg</a></p>
<p><b>Keywords</b>: Learning from Demonstration, Multi-Robot Systems, Teaching Robot Teams
</p><p><b>Compressor summary</b>: MixTURE is a novel Multi-Agent Learning from Demonstration framework that learns both collaborative tasks and inter-agent communication from human expert data, reducing human workload and improving usability.</p><hr><h3>Training Neural Networks is NP-Hard in Fixed Dimension</h3>
<p>Vincent Froese, Christoph Hertrich</p>
<p><a href='https://openreview.net/forum?id=VAQp2EnZeW'>https://openreview.net/forum?id=VAQp2EnZeW</a></p>
<p><b>Keywords</b>: Computational Complexity, Neural Network, Rectified Linear Unit, Empirical Risk Minimization, Parameterized Complexity
</p><p><b>Compressor summary</b>: The paper investigates the complexity of training two-layer neural networks with different activation functions in various input dimensions and neuron counts, answering several open questions and proving some fixed-parameter tractability results.</p><hr><h3>Video-Mined Task Graphs for Keystep Recognition in Instructional Videos</h3>
<p>Kumar Ashutosh, Santhosh Kumar Ramakrishnan, Triantafyllos Afouras, Kristen Grauman</p>
<p><a href='https://openreview.net/forum?id=VAC7aB6qSG'>https://openreview.net/forum?id=VAC7aB6qSG</a></p>
<p><b>Keywords</b>: Instructional Videos, Task Graph, Keystep Recognition
</p><p><b>Compressor summary</b>: The paragraph describes a method for automatically discovering task graphs from how-to videos to improve keystep recognition in novel videos.</p><hr><h3>Temporally Disentangled Representation Learning under Unknown Nonstationarity</h3>
<p>Xiangchen Song, Weiran Yao, Yewen Fan, Xinshuai Dong, Guangyi Chen, Juan Carlos Niebles, Eric Xing, Kun Zhang</p>
<p><a href='https://openreview.net/forum?id=V8GHCGYLkf'>https://openreview.net/forum?id=V8GHCGYLkf</a></p>
<p><b>Keywords</b>: Unsupervised learning, Temporal disentanglement, Nonlinear ICA, Identifiability theory
</p><p><b>Compressor summary</b>: The paper proposes NCTRL, a method for identifying time-delayed causal influences in nonstationary sequential data without using auxiliary information or simplifying latent dynamics.</p><hr><h3>Nonparametric Identifiability of Causal Representations from Unknown Interventions</h3>
<p>Julius von Kügelgen, Michel Besserve, Wendong Liang, Luigi Gresele, Armin Kekić, Elias Bareinboim, David Blei, Bernhard Schölkopf</p>
<p><a href='https://openreview.net/forum?id=V87gZeSOL4'>https://openreview.net/forum?id=V87gZeSOL4</a></p>
<p><b>Keywords</b>: Causal representation learning, identifiability, theory, nonparametric, interventions, multi-environment
</p><p><b>Compressor summary</b>: The paper proposes a method to learn latent causal variables and their relations from high-dimensional functions, using multiple datasets from unknown interventions and showing identifiability conditions without restrictive assumptions or partial knowledge.</p><hr><h3>SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models</h3>
<p>Martin Gonzalez, Nelson Fernandez, Thuy Vinh Dinh Tran, Elies Gherbi, Hatem Hajri, Nader Masmoudi</p>
<p><a href='https://openreview.net/forum?id=V6IgkYKD8P'>https://openreview.net/forum?id=V6IgkYKD8P</a></p>
<p><b>Keywords</b>: Diffusion Probabilistic Models, Exponential SDE methods, Image Generation, Generative Models
</p><p><b>Compressor summary</b>: SEEDS are stochastic exponential derivative-free solvers that improve and generalize Exponential Integrator approaches for fast sampling from Diffusion Probabilistic Models without requiring derivatives or training, achieving optimal quality and speed.</p><hr><h3>Localized Symbolic Knowledge Distillation for Visual Commonsense Models</h3>
<p>Jae Sung Park, Jack Hessel, Khyathi Chandu, Paul Pu Liang, Ximing Lu, Peter West, Youngjae Yu, Qiuyuan Huang, Jianfeng Gao, Ali Farhadi, Yejin Choi</p>
<p><a href='https://openreview.net/forum?id=V5eG47pyVl'>https://openreview.net/forum?id=V5eG47pyVl</a></p>
<p><b>Keywords</b>: multimodal, commonsense reasoning, instruction tuning, large language model
</p><p><b>Compressor summary</b>: The Localized Visual Commonsense model allows users to specify regions within images as input, enabling precise within-image reasoning and improving multimodal tasks with a reference-as-input interface.</p><hr><h3>Lockdown: Backdoor Defense for Federated Learning  with Isolated Subspace Training</h3>
<p>Tiansheng Huang, Sihao Hu, Ka-Ho Chow, Fatih Ilhan, Selim Furkan Tekin, Ling Liu</p>
<p><a href='https://openreview.net/forum?id=V5cQH7JbGo'>https://openreview.net/forum?id=V5cQH7JbGo</a></p>
<p><b>Keywords</b>: Federated learning, backdoor defense, isolated subspace training.
</p><p><b>Compressor summary</b>: Lockdown is a novel isolated subspace training method that effectively defends against backdoor attacks in federated learning, while reducing communication and model complexity.</p><hr><h3>Causal Effect Regularization: Automated Detection and Removal of Spurious Correlations</h3>
<p>Abhinav Kumar, Amit Deshpande, Amit Sharma</p>
<p><a href='https://openreview.net/forum?id=V5Oh7Aqfft'>https://openreview.net/forum?id=V5Oh7Aqfft</a></p>
<p><b>Keywords</b>: Spurious Correlation, Out of Distribution Generalization
</p><p><b>Compressor summary</b>: The paper proposes a method to automatically identify and remove spurious attributes in classification datasets, improving generalization and reducing dependence on noisy factors.</p><hr><h3>Strategic Behavior in Two-sided Matching Markets with Prediction-enhanced Preference-formation</h3>
<p>Stefania Ionescu, Yuhao Du, Kenneth Joseph, Aniko Hannak</p>
<p><a href='https://openreview.net/forum?id=V5FNSilWiC'>https://openreview.net/forum?id=V5FNSilWiC</a></p>
<p><b>Keywords</b>: matching markets, strategic behaviour, ML-based forecasting, recommender systems, adversarial attacks, agent-based modelling
</p><p><b>Compressor summary</b>: The paper introduces adversarial interaction attacks, where agents return to a matching market and act non-optimally with their matches to disrupt future predictions, and shows how these attacks can benefit returning agents and increase inequality.</p><hr><h3>Perceptual Kalman Filters: Online State Estimation under a Perfect Perceptual-Quality Constraint</h3>
<p>Dror Freirich, Tomer Michaeli, Ron Meir</p>
<p><a href='https://openreview.net/forum?id=V4hqq2NGTW'>https://openreview.net/forum?id=V4hqq2NGTW</a></p>
<p><b>Keywords</b>: Kalman filter, estimation theory, causal filtering, signal processing, distortion-perception tradeoff
</p><p><b>Compressor summary</b>: The text discusses the challenge of achieving perfect perceptual-quality in reconstructing temporal signals from corrupted or missing data under a causal filtering constraint, which may require ignoring new information and increasing MSE.</p><hr><h3>Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes</h3>
<p>Gehua Ma, Runhao Jiang, Rui Yan, Huajin Tang</p>
<p><a href='https://openreview.net/forum?id=V4YeOvsQfu'>https://openreview.net/forum?id=V4YeOvsQfu</a></p>
<p><b>Keywords</b>: neuroscience, neural coding, sensory neuroscience, visual coding, SNN, spiking neural networks, generative model, latent variable model, cognitive computational neuroscience, computational neuroscience
</p><p><b>Compressor summary</b>: TeCoS-LVM is a new spiking neural network model that can capture temporal dependencies in natural stimuli and produce realistic spike activities, outperforming current methods.</p><hr><h3>Effective Human-AI Teams via Learned Natural Language Rules and Onboarding</h3>
<p>Hussein Mozannar, Jimin J Lee, Dennis Wei, Prasanna Sattigeri, Subhro Das, David Sontag</p>
<p><a href='https://openreview.net/forum?id=V2yFumwo5B'>https://openreview.net/forum?id=V2yFumwo5B</a></p>
<p><b>Keywords</b>: human-ai, collaboration, onboarding, region-discovery, LLM, data description
</p><p><b>Compressor summary</b>: The paper proposes learning natural language rules for human-AI collaboration based on data regions discovered by an algorithm.</p><hr><h3>A Path to Simpler Models Starts With Noise</h3>
<p>Lesia Semenova, Harry Chen, Ronald Parr, Cynthia Rudin</p>
<p><a href='https://openreview.net/forum?id=Uzi22WryyX'>https://openreview.net/forum?id=Uzi22WryyX</a></p>
<p><b>Keywords</b>: Rashomon Set, Simplicity, Interpretable Machine Learning, Model Selection, Model Multiplicity
</p><p><b>Compressor summary</b>: This paragraph discusses how data noise affects the performance of different models and introduces a measure called pattern diversity to quantify the difference in predictions between them.</p><hr><h3>$k$-Means Clustering with Distance-Based Privacy</h3>
<p>Alessandro Epasto, Vahab Mirrokni, Shyam Narayanan, Peilin Zhong</p>
<p><a href='https://openreview.net/forum?id=UzUhiKACmS'>https://openreview.net/forum?id=UzUhiKACmS</a></p>
<p><b>Keywords</b>: differential Privacy, k-means, k-median, clustering, distance-based privacy
</p><p><b>Compressor summary</b>: The paper proposes constant-approximate algorithms for Euclidean clustering with Distance-based privacy, which protect exact locations and outperform existing methods.</p><hr><h3>Guiding Large Language Models via Directional Stimulus Prompting</h3>
<p>Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan</p>
<p><a href='https://openreview.net/forum?id=UvIN8oQ4uI'>https://openreview.net/forum?id=UvIN8oQ4uI</a></p>
<p><b>Keywords</b>: Black-box Large Language Models, Directional Stimulus Prompting, Hint, Reinforcement learning, Prompt optimization
</p><p><b>Compressor summary</b>: The paragraph introduces Directional Stimulus Prompting, a framework that uses a small policy model to generate hints for large language models, guiding them towards specific outputs and improving their performance on various tasks.</p><hr><h3>UP-NeRF: Unconstrained Pose Prior-Free Neural Radiance Field</h3>
<p>Injae Kim, Minhyuk Choi, Hyunwoo J. Kim</p>
<p><a href='https://openreview.net/forum?id=UvBwXdL95b'>https://openreview.net/forum?id=UvBwXdL95b</a></p>
<p><b>Keywords</b>: neural radiance field, pose estimation
</p><p><b>Compressor summary</b>: UP-NeRF optimizes NeRF with unconstrained images without pose priors by using surrogate tasks, a separate module for transient occluders, and improved pose estimation and depth supervision.</p><hr><h3>Bayesian Optimisation of Functions on Graphs</h3>
<p>Xingchen Wan, Pierre Osselin, Henry Kenlay, Binxin Ru, Michael A Osborne, Xiaowen Dong</p>
<p><a href='https://openreview.net/forum?id=UuNd9A6noD'>https://openreview.net/forum?id=UuNd9A6noD</a></p>
<p><b>Keywords</b>: graphs, Bayesian optimisation, scalability
</p><p><b>Compressor summary</b>: The paper proposes a new Bayesian optimization framework that adapts to graph-structured functions and improves sample efficiency for large-scale graphs using local modeling and suitable kernels.</p><hr><h3>Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Winning Lottery Tickets</h3>
<p>Arthur da Cunha, Francesco D'Amore, Emanuele Natale</p>
<p><a href='https://openreview.net/forum?id=UqYrYB3dp5'>https://openreview.net/forum?id=UqYrYB3dp5</a></p>
<p><b>Keywords</b>: lottery ticket hypothesis, convolutional neural network, network pruning, structured pruning, random subset sum
</p><p><b>Compressor summary</b>: The paper introduces a new mathematical tool to prove that random neural networks have efficient subnetworks, which can help understand the role of over-parameterization in deep learning.</p><hr><h3>Language Is Not All You Need: Aligning Perception with Language Models</h3>
<p>Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei</p>
<p><a href='https://openreview.net/forum?id=UpN2wfrLec'>https://openreview.net/forum?id=UpN2wfrLec</a></p>
<p><b>Keywords</b>: multimodal large language model
</p><p><b>Compressor summary</b>: KOSMOS-1 is a Multimodal Large Language Model that can perceive various modalities, learn in context, and follow instructions, achieving impressive performance on language and perception tasks without gradient updates or finetuning.</p><hr><h3>Extending the Design Space of Graph Neural Networks by Rethinking Folklore Weisfeiler-Lehman</h3>
<p>Jiarui Feng, Lecheng Kong, Hao Liu, Dacheng Tao, Fuhai Li, Muhan Zhang, Yixin Chen</p>
<p><a href='https://openreview.net/forum?id=UlJcZoawgU'>https://openreview.net/forum?id=UlJcZoawgU</a></p>
<p><b>Keywords</b>: Graph neural network, expressive power, Folklore Weisfeiler-Lehman test.
</p><p><b>Compressor summary</b>: The paper proposes $(k,t)$-FWL+ and Neighborhood$^2$-FWL (N$^2$-FWL), which are more powerful and flexible graph neural network frameworks than MPNNS, with better performance on various tasks.</p><hr><h3>Textually Pretrained Speech Language Models</h3>
<p>Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Défossez, Gabriel Synnaeve, Emmanuel Dupoux, Roy Schwartz, Yossi Adi</p>
<p><a href='https://openreview.net/forum?id=UlHueVjAKr'>https://openreview.net/forum?id=UlHueVjAKr</a></p>
<p><b>Keywords</b>: LLM, speech, generative, GSLM
</p><p><b>Compressor summary</b>: TWIST trains SpeechLMs using a warm-start from pretrained textual models, leading to better performance than cold-start methods, and introduces new benchmarks for evaluation.</p><hr><h3>Approximate Heavy Tails in Offline (Multi-Pass) Stochastic Gradient Descent</h3>
<p>Krunoslav Lehman Pavasovic, Alain Durmus, Umut Simsekli</p>
<p><a href='https://openreview.net/forum?id=UkPeUXML7s'>https://openreview.net/forum?id=UkPeUXML7s</a></p>
<p><b>Keywords</b>: SGD, heavy-tails, wasserstein convergence
</p><p><b>Compressor summary</b>: The paper explores why stochastic gradient descent (SGD) shows heavy-tailed behavior in practical settings and investigates the role of offline SGD's stationary distribution in achieving power-law tails as more data points are added.</p><hr><h3>Inner-Outer Aware Reconstruction Model for Monocular 3D Scene Reconstruction</h3>
<p>Yu-Kun Qiu, Guohao Xu, Wei-Shi Zheng</p>
<p><a href='https://openreview.net/forum?id=UkAGqeWTuL'>https://openreview.net/forum?id=UkAGqeWTuL</a></p>
<p><b>Keywords</b>: 3D reconstruction
</p><p><b>Compressor summary</b>: IOAR is a 3D scene reconstruction method that uses a coarse-to-fine strategy to distinguish outer-surface, inner-surface, and surface voxels, resulting in more precise mesh predictions.</p><hr><h3>LoCoOp: Few-Shot Out-of-Distribution Detection via Prompt Learning</h3>
<p>Atsuyuki Miyai, Qing Yu, Go Irie, Kiyoharu Aizawa</p>
<p><a href='https://openreview.net/forum?id=UjtiLdXGMC'>https://openreview.net/forum?id=UjtiLdXGMC</a></p>
<p><b>Keywords</b>: out-of-distribution detection, vision-language foundation model, prompt learning
</p><p><b>Compressor summary</b>: The authors propose LoCoOp, a novel prompt learning method for few-shot OOD detection that uses CLIP's local features to remove ID-irrelevant information and achieve superior performance over existing methods.</p><hr><h3>Multi-Agent First Order Constrained Optimization in Policy Space</h3>
<p>Youpeng Zhao, Yaodong Yang, Zhenbo Lu, Wengang Zhou, Houqiang Li</p>
<p><a href='https://openreview.net/forum?id=UgomCjCWjC'>https://openreview.net/forum?id=UgomCjCWjC</a></p>
<p><b>Keywords</b>: Safe Multi-agent Reinforcement Learning, constrained policy optimisation, first-order optimisation
</p><p><b>Compressor summary</b>: MAFOCOPS is a novel safety-aware method for multi-agent systems in MARL, which effectively addresses high performance and enforcing safety constraints by solving a constrained optimization problem in the policy space.</p><hr><h3>DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of mixture-of-datasets</h3>
<p>Yash Jain, Harkirat Behl, Zsolt Kira, Vibhav Vineet</p>
<p><a href='https://openreview.net/forum?id=UgSSOpqvPI'>https://openreview.net/forum?id=UgSSOpqvPI</a></p>
<p><b>Keywords</b>: mixture-of-experts, moe, object detection, mixture of datasets, multiple datasets
</p><p><b>Compressor summary</b>: The paper proposes a method called DAMEX that improves object detection by learning to route different datasets to their corresponding experts, achieving state-of-the-art results on various datasets and scenarios.</p><hr><h3>When can Regression-Adjusted Control Variate Help? Rare Events, Sobolev Embedding and Minimax Optimality</h3>
<p>Jose Blanchet, Haoxuan Chen, Yiping Lu, Lexing Ying</p>
<p><a href='https://openreview.net/forum?id=UdrybSp67L'>https://openreview.net/forum?id=UdrybSp67L</a></p>
<p><b>Keywords</b>: Information-theoretic Lower Bounds, Sobolev Embedding Theorem, Quadrature Rule
</p><p><b>Compressor summary</b>: The paper explores how machine learning-based control variates can reduce the variance of Monte Carlo sampling for estimating moments of a Sobolev function, and shows that they can improve the estimation rate under certain conditions and not in the presence of rare events.</p><hr><h3>Double Gumbel Q-Learning</h3>
<p>David Yu-Tung Hui, Aaron Courville, Pierre-Luc Bacon</p>
<p><a href='https://openreview.net/forum?id=UdaTyy0BNB'>https://openreview.net/forum?id=UdaTyy0BNB</a></p>
<p><b>Keywords</b>: deep reinforcement learning, Q-Learning, TD-Learning with function approximation, extreme value theory, maximum-likelihood estimation, moment-matching
</p><p><b>Compressor summary</b>: Double Gumbel Q-Learning is a new algorithm that handles heteroscedastic noise in Deep Q-Learning and performs well on various control tasks.</p><hr><h3>MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks</h3>
<p>Allen Nie, Yuhui Zhang, Atharva Amdekar, Christopher J Piech, Tatsunori Hashimoto, Tobias Gerstenberg</p>
<p><a href='https://openreview.net/forum?id=UdByCgCNdr'>https://openreview.net/forum?id=UdByCgCNdr</a></p>
<p><b>Keywords</b>: cognitive science, causal reasoning, moral reasoning, dataset, language models
</p><p><b>Compressor summary</b>: The paragraph discusses a study that examines how well large language models make causal and moral judgments about text-based scenarios compared to human participants, using a dataset of stories from cognitive science papers annotated with factors influencing judgments.</p><hr><h3>Generating Images with Multimodal Language Models</h3>
<p>Jing Yu Koh, Daniel Fried, Ruslan Salakhutdinov</p>
<p><a href='https://openreview.net/forum?id=Uczck6TlSZ'>https://openreview.net/forum?id=Uczck6TlSZ</a></p>
<p><b>Keywords</b>: multimodal, vision-and-language, language models
</p><p><b>Compressor summary</b>: The proposed method fuses large language models with image encoder and decoder models for multimodal capabilities including image retrieval, novel image generation, and multimodal dialogue.</p><hr><h3>Identification of Nonlinear Latent Hierarchical Models</h3>
<p>Lingjing Kong, Biwei Huang, Feng Xie, Eric Xing, Yuejie Chi, Kun Zhang</p>
<p><a href='https://openreview.net/forum?id=Uc5yyiytR1'>https://openreview.net/forum?id=Uc5yyiytR1</a></p>
<p><b>Keywords</b>: Causal discovery, causal representation learning, latent variable models, causal structure learning, causal identifiability.
</p><p><b>Compressor summary</b>: This paper presents a method for identifying causal structures and latent variables in nonlinear latent hierarchical models using novel identifiability guarantees and an estimation procedure.</p><hr><h3>Active Negative Loss Functions for Learning with Noisy Labels</h3>
<p>Xichen Ye, Xiaoqiang Li, Songmin Dai, Tong Liu, Yan Sun, Weiqin Tong</p>
<p><a href='https://openreview.net/forum?id=Uafbv4rfJc'>https://openreview.net/forum?id=Uafbv4rfJc</a></p>
<p><b>Keywords</b>: noisy label learning, robust loss function, multiclass classification, computer vision
</p><p><b>Compressor summary</b>: The authors propose a new robust passive loss function called Normalized Negative Loss Functions (NNLFs) that improves the Active Passive Loss (APL) framework by focusing more on memorized clean samples and outperform state-of-the-art methods.</p><hr><h3>Model-enhanced Vector Index</h3>
<p>Hailin Zhang, Yujing Wang, Qi Chen, Ruiheng Chang, Ting Zhang, Ziming Miao, Yingyan Hou, Yang Ding, Xupeng Miao, Haonan Wang, Bochen Pang, Yuefeng Zhan, Hao Sun, Weiwei Deng, Qi Zhang, Fan Yang, Xing Xie, Mao Yang, Bin CUI</p>
<p><a href='https://openreview.net/forum?id=UZlAjSnmvB'>https://openreview.net/forum?id=UZlAjSnmvB</a></p>
<p><b>Keywords</b>: document retrieval, model-based index, dense retrieval, residual quantization
</p><p><b>Compressor summary</b>: The paper proposes MEVI, a differentiable index enhanced by deep generative models, which improves vector index search efficiency while maintaining performance on academic benchmarks.</p><hr><h3>Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery</h3>
<p>Tingyu Weng, Jun Xiao, Haiyong Jiang</p>
<p><a href='https://openreview.net/forum?id=UYl9IIsjq7'>https://openreview.net/forum?id=UYl9IIsjq7</a></p>
<p><b>Keywords</b>: 3D point clouds, 3D recognition, part-based representation, unsupervised class discovery
</p><p><b>Compressor summary</b>: The paper proposes DNIK, a method to discover novel 3D classes by decomposing them into known parts and using a part relation encoding module (PRE) for better recognition.</p><hr><h3>Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks</h3>
<p>Alexander Modell, Ian Gallagher, Emma Ceccherini, Nick Whiteley, Patrick Rubin-Delanchy</p>
<p><a href='https://openreview.net/forum?id=UXtLrsG4Rf'>https://openreview.net/forum?id=UXtLrsG4Rf</a></p>
<p><b>Keywords</b>: dynamic networks, representation learning, spectral methods
</p><p><b>Compressor summary</b>: The paragraph introduces a new method to represent network data over time using continuous trajectories that satisfy structural and temporal coherence, with estimation theory providing error control and smoothing trade-offs.</p><hr><h3>Expressive Sign Equivariant Networks for Spectral Geometric Learning</h3>
<p>Derek Lim, Joshua Robinson, Stefanie Jegelka, Haggai Maron</p>
<p><a href='https://openreview.net/forum?id=UWd4ysACo4'>https://openreview.net/forum?id=UWd4ysACo4</a></p>
<p><b>Keywords</b>: Eigenvectors, spectral, geometry, universal approximation, graph, equivariance, invariance
</p><p><b>Compressor summary</b>: The authors propose novel sign equivariant neural network architectures for tasks such as building orthogonally equivariant models and learning node positional encodings, which they show have better performance than sign invariant models.</p><hr><h3>PAC-Bayes Generalization Certificates for Learned Inductive Conformal Prediction</h3>
<p>Apoorva Sharma, Sushant Veer, Asher Hancock, Heng Yang, Marco Pavone, Anirudha Majumdar</p>
<p><a href='https://openreview.net/forum?id=URrUpcp6Qh'>https://openreview.net/forum?id=URrUpcp6Qh</a></p>
<p><b>Keywords</b>: Conformal Prediction, PAC Bayes, Generalization Theory
</p><p><b>Compressor summary</b>: The authors propose a framework for optimizing the efficiency of set-valued predictions based on PAC-Bayes theory, which allows using the entire calibration dataset and provides test-time coverage and efficiency guarantees.</p><hr><h3>Horospherical Decision Boundaries for Large Margin Classification in Hyperbolic Space</h3>
<p>Xiran Fan, Chun-Hao Yang, Baba C. Vemuri</p>
<p><a href='https://openreview.net/forum?id=URAZeoIC1q'>https://openreview.net/forum?id=URAZeoIC1q</a></p>
<p><b>Keywords</b>: Large-margin clssifier, Hyperbolic space, Horosphere, SVM, Geodesically convex, Global optimility, Busemann function
</p><p><b>Compressor summary</b>: The paper introduces a new large margin classifier with horospherical decision boundaries for hyperbolic spaces, which optimizes geodesically convex problems and outperforms existing methods.</p><hr><h3>Fast Asymptotically Optimal Algorithms for Non-Parametric Stochastic Bandits</h3>
<p>Dorian Baudry, Fabien Pesquerel, Rémy Degenne, Odalric-Ambrym Maillard</p>
<p><a href='https://openreview.net/forum?id=UPo8vlZ0wQ'>https://openreview.net/forum?id=UPo8vlZ0wQ</a></p>
<p><b>Keywords</b>: Multi-Armed Bandits
</p><p><b>Compressor summary</b>: The paper proposes methods to approximate the infimum KL in non-parametric stochastic bandits, reducing computational and memory costs while maintaining regret guaranties.</p><hr><h3>Brain encoding models based on multimodal transformers can transfer across language and vision</h3>
<p>Jerry Tang, Meng Du, Vy A. Vo, Vasudev Lal, Alexander Huth</p>
<p><a href='https://openreview.net/forum?id=UPefaFqjNQ'>https://openreview.net/forum?id=UPefaFqjNQ</a></p>
<p><b>Keywords</b>: fMRI, neuroscience, encoding models, multimodal transformers, language, vision
</p><p><b>Compressor summary</b>: The study used multimodal transformers to train encoding models that can transfer across fMRI responses to stories and movies, revealing shared semantic dimensions in language and vision processing.</p><hr><h3>Class-Distribution-Aware Pseudo-Labeling for Semi-Supervised Multi-Label Learning</h3>
<p>Ming-Kun Xie, Jia-Hao Xiao, Hao-Zhe Liu, Gang Niu, Masashi Sugiyama, Sheng-Jun Huang</p>
<p><a href='https://openreview.net/forum?id=UOB1UgPjuG'>https://openreview.net/forum?id=UOB1UgPjuG</a></p>
<p><b>Keywords</b>: Semi-supervised multi-label learning, pseudo labeling.
</p><p><b>Compressor summary</b>: The paper proposes Class-Aware Pseudo-Labeling (CAP), which uses class-aware thresholds to control pseudo-label assignment and improve semi-supervised multi-label learning by aligning pseudo-label distribution with the true one.</p><hr><h3>The s-value: evaluating stability with respect to distributional shifts</h3>
<p>Suyash Gupta, Dominik Rothenhaeusler</p>
<p><a href='https://openreview.net/forum?id=UKtjq3dIs0'>https://openreview.net/forum?id=UKtjq3dIs0</a></p>
<p><b>Keywords</b>: Distributional Stability, Distributional Robustness, Distributional Shifts, Generalizability
</p><p><b>Compressor summary</b>: The paper proposes a new measure of instability for statistical parameters that accounts for both distributional changes and directional shifts, and demonstrates its usefulness in transfer learning and improving estimation accuracy.</p><hr><h3>Learning to Tokenize for Generative Retrieval</h3>
<p>Weiwei Sun, Lingyong Yan, Zheng Chen, Shuaiqiang Wang, Haichao Zhu, Pengjie Ren, Zhumin Chen, Dawei Yin, Maarten de Rijke, Zhaochun Ren</p>
<p><a href='https://openreview.net/forum?id=UKd6dpVGdu'>https://openreview.net/forum?id=UKd6dpVGdu</a></p>
<p><b>Keywords</b>: Information Retrieval, Document Retrieval, Generative Retrieval
</p><p><b>Compressor summary</b>: The paper proposes a novel document tokenization learning method, GenRet, which learns to encode document semantics into docids for generative retrieval models.</p><hr><h3>Simple and Asymmetric Graph Contrastive Learning without Augmentations</h3>
<p>Teng Xiao, Huaisheng Zhu, Zhengyu Chen, Suhang Wang</p>
<p><a href='https://openreview.net/forum?id=UK8mA3DRnb'>https://openreview.net/forum?id=UK8mA3DRnb</a></p>
<p><b>Keywords</b>: Contrastive Learning, Graph Representation Learning
</p><p><b>Compressor summary</b>: GraphACL is a simple algorithm for contrastive learning on graph-structured data that works well on both homophilic and heterophilic graphs without relying on prefabricated augmentations or homophily assumptions.</p><hr><h3>Provably Safe Reinforcement Learning with Step-wise Violation Constraints</h3>
<p>Nuoya Xiong, Yihan Du, Longbo Huang</p>
<p><a href='https://openreview.net/forum?id=UJ9o8wbB5U'>https://openreview.net/forum?id=UJ9o8wbB5U</a></p>
<p><b>Keywords</b>: safe reinforcement learning, step-wise violation, reinforcement learning theory
</p><p><b>Compressor summary</b>: The paper proposes two algorithms for safe reinforcement learning problems with step-wise violation constraints, one for when safe actions are known and another for when they are not, achieving near-optimal performance in both violation and regret.</p><hr><h3>Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention</h3>
<p>Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, François Fleuret</p>
<p><a href='https://openreview.net/forum?id=UINHuKeWUa'>https://openreview.net/forum?id=UINHuKeWUa</a></p>
<p><b>Keywords</b>: self-attention, large language models, transformers
</p><p><b>Compressor summary</b>: The paper proposes FlashAttention improvements that allow for more dynamic sparse attention patterns with no computational overhead and significant runtime speedup, while maintaining perplexity and increasing training speed for longer sequences in transformer language models.</p><hr><h3>Attacks on Online Learners: a Teacher-Student Analysis</h3>
<p>Riccardo Giuseppe Margiotta, Sebastian Goldt, Guido Sanguinetti</p>
<p><a href='https://openreview.net/forum?id=UHwmoJYwSV'>https://openreview.net/forum?id=UHwmoJYwSV</a></p>
<p><b>Keywords</b>: Adversarial attacks, data poisoning, online learning, optimal control, teacher-student setup, solvable model
</p><p><b>Compressor summary</b>: The paper investigates how attackers can manipulate online learning dynamics by perturbing data labels and studies the effects on learners with different architectures using both theory and experiments.</p><hr><h3>Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow</h3>
<p>Sizhe Wei, Yuxi Wei, Yue Hu, Yifan Lu, Yiqi Zhong, Siheng Chen, Ya Zhang</p>
<p><a href='https://openreview.net/forum?id=UHIDdtxmVS'>https://openreview.net/forum?id=UHIDdtxmVS</a></p>
<p><b>Keywords</b>: Collaborative Perception; BEV Flow; Time Asynchronization
</p><p><b>Compressor summary</b>: CoBEVFlow is a system to align asynchronous messages in collaborative perception using bird's eye view flow, handling irregular and continuous time stamps without discretization, and improving performance on real-world datasets.</p><hr><h3>Segment Everything Everywhere All at Once</h3>
<p>Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, Yong Jae Lee</p>
<p><a href='https://openreview.net/forum?id=UHBrWeFWlL'>https://openreview.net/forum?id=UHBrWeFWlL</a></p>
<p><b>Keywords</b>: Generaic segmentation, interactive segmentation, referring segmentation, multi-modality prompting.
</p><p><b>Compressor summary</b>: SEEM is an interactive model for image segmentation that unifies different prompts, learns from segmentation history, and encodes text queries and mask labels semantically.</p><hr><h3>MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly Detection</h3>
<p>Junho Song, Keonwoo Kim, Jeonglyul Oh, Sungzoon Cho</p>
<p><a href='https://openreview.net/forum?id=UFW67uduJd'>https://openreview.net/forum?id=UFW67uduJd</a></p>
<p><b>Keywords</b>: Multivariate time series, Anomaly detection
</p><p><b>Compressor summary</b>: MEMTO is a memory-guided Transformer that uses a novel memory module and bi-dimensional deviation-based detection criterion to achieve high anomaly detection performance on real-world multivariate time series data.</p><hr><h3>Can Language Models Solve Graph Problems in Natural Language?</h3>
<p>Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov</p>
<p><a href='https://openreview.net/forum?id=UDqHhbqYJV'>https://openreview.net/forum?id=UDqHhbqYJV</a></p>
<p><b>Keywords</b>: large language models, graph reasoning, structured reasoning
</p><p><b>Compressor summary</b>: The authors introduce NLGraph, a natural language benchmark for graph-based problem solving, and evaluate GPT-3/4 on it, finding that LLMs have preliminary graph reasoning abilities but are brittle in complex scenarios and improve with instruction-based prompting approaches.</p><hr><h3>Time Series Kernels based on Nonlinear Vector AutoRegressive Delay Embeddings</h3>
<p>Giovanni De Felice, John Y Goulermas, Vladimir Gusev</p>
<p><a href='https://openreview.net/forum?id=UBUWFEwn7p'>https://openreview.net/forum?id=UBUWFEwn7p</a></p>
<p><b>Keywords</b>: Time Series, Kernel methods, NVAR processes, Dynamical systems, Reservoir Computing
</p><p><b>Compressor summary</b>: The authors propose a new kernel for time series analysis that is based on Nonlinear Vector AutoRegressive processes, simplifies hyperparameter setting, and performs well in classification tasks.</p><hr><h3>Cross-modal Active Complementary Learning with Self-refining Correspondence</h3>
<p>Yang Qin, Yuan Sun, Dezhong Peng, Joey Tianyi Zhou, Xi Peng, Peng Hu</p>
<p><a href='https://openreview.net/forum?id=UBBeUjTja8'>https://openreview.net/forum?id=UBBeUjTja8</a></p>
<p><b>Keywords</b>: Cross-modal learning, Image-text matching, Noisy correspondence.
</p><p><b>Compressor summary</b>: The text describes a new framework called CRCL that improves image-text matching by addressing challenges caused by noisy correspondence between visual and textual modalities using active complementary learning and self-refining correction.</p><hr><h3>A Unified Generalization Analysis of Re-Weighting and Logit-Adjustment for Imbalanced Learning</h3>
<p>Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, Qingming Huang</p>
<p><a href='https://openreview.net/forum?id=UAow2kPsYP'>https://openreview.net/forum?id=UAow2kPsYP</a></p>
<p><b>Keywords</b>: Imbalanced Learning, Re-weighting, Logit Adjustment, Genralization Analysis
</p><p><b>Compressor summary</b>: The paragraph discusses imbalanced real-world datasets and proposes a novel technique called data-dependent contraction to analyze modified losses for such datasets, along with a fine-grained generalization bound, a learning algorithm, and empirical validation.</p><hr><h3>Unsupervised Graph Neural Architecture Search with Disentangled Self-Supervision</h3>
<p>Zeyang Zhang, Xin Wang, Ziwei Zhang, Guangyao Shen, Shiqi Shen, Wenwu Zhu</p>
<p><a href='https://openreview.net/forum?id=UAFa5ZhR85'>https://openreview.net/forum?id=UAFa5ZhR85</a></p>
<p><b>Keywords</b>: Graph Neural Architecture Search, Unsupervised Learning, Self-supervised Learning
</p><p><b>Compressor summary</b>: The paper proposes a novel model for unsupervised graph neural architecture search, which can discover optimal architectures by disentangling latent graph factors and using self-supervised training and contrastive search.</p><hr><h3>A Hierarchical Spatial Transformer for Massive Point Samples  in Continuous Space</h3>
<p>Wenchong He, Zhe Jiang, Tingsong Xiao, Zelin Xu, Shigang Chen, Ronald Fick, MILES D MEDINA, Christine Angelini</p>
<p><a href='https://openreview.net/forum?id=U9zRgpgdFI'>https://openreview.net/forum?id=U9zRgpgdFI</a></p>
<p><b>Keywords</b>: Spatial representation learning, transformer, quadtree, efficiency
</p><p><b>Compressor summary</b>: The paper introduces a new transformer model for massive spatial point data, addressing challenges such as long-range dependencies, non-uniform distributions, and high computational costs, by using a hierarchical structure, coarse approximation, and uncertainty estimation.</p><hr><h3>Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers</h3>
<p>Yiwei Lu, Yaoliang Yu, Xinlin Li, Vahid Partovi Nia</p>
<p><a href='https://openreview.net/forum?id=U6fp6IUBdr'>https://openreview.net/forum?id=U6fp6IUBdr</a></p>
<p><b>Keywords</b>: Neural network quantization, Model compression, Conditional gradient algorithm
</p><p><b>Compressor summary</b>: The paper introduces ProxConnect++, a generalization of BinaryConnect with forward-backward quantizers that enables principled binarization methods for neural networks without relying on heuristics or training tricks, and shows its effectiveness in image classification tasks.</p><hr><h3>Last-Iterate Convergent Policy Gradient Primal-Dual Methods for Constrained MDPs</h3>
<p>Dongsheng Ding, Chen-Yu Wei, Kaiqing Zhang, Alejandro Ribeiro</p>
<p><a href='https://openreview.net/forum?id=U6bhCLSPun'>https://openreview.net/forum?id=U6bhCLSPun</a></p>
<p><b>Keywords</b>: Constrained Markov decision processes, policy gradient primal-dual methods, non-convex saddle-point problem, last-iterate convergence, entropy regularization, optimistic gradient
</p><p><b>Compressor summary</b>: The paper proposes two new methods for solving constrained Markov decision processes (MDPs) that have better convergence properties than existing Lagrangian-based policy search methods.</p><hr><h3>Masked Two-channel Decoupling Framework for Incomplete Multi-view Weak Multi-label Learning</h3>
<p>Chengliang Liu, Jie Wen, Yabo Liu, Chao Huang, Zhihao Wu, Xiaoling Luo, Yong Xu</p>
<p><a href='https://openreview.net/forum?id=U4pFV192JQ'>https://openreview.net/forum?id=U4pFV192JQ</a></p>
<p><b>Keywords</b>: Incomplete Multi-view Weak Multi-label Learning, Multi-view learning, Multi-label Classification
</p><p><b>Compressor summary</b>: The paper proposes a new deep neural network framework for incomplete multi-view weak multi-label learning, which decouples single-channel view-level representation into shared and view-specific representations, and uses cross-channel contrastive loss and label-guided graph regularization to enhance the features and preserve geometric structure.</p><hr><h3>Towards Efficient and Accurate Winograd Convolution via Full Quantization</h3>
<p>Chen Tianqi, Weixiang Xu, Weihan Chen, Peisong Wang, Jian Cheng</p>
<p><a href='https://openreview.net/forum?id=U4WTG06Yu3'>https://openreview.net/forum?id=U4WTG06Yu3</a></p>
<p><b>Keywords</b>: Winograd Convolution, Quantization
</p><p><b>Compressor summary</b>: The paper introduces PAW, which optimizes transformation procedures for Winograd convolution with post-training quantization and FSQ, a hardware-friendly method that balances range differences in the domain, improving accuracy and efficiency.</p><hr><h3>Topological RANSAC for instance verification and retrieval without fine-tuning</h3>
<p>Guoyuan An, Ju-hyeong Seon, Inkyu An, Yuchi Huo, Sung-eui Yoon</p>
<p><a href='https://openreview.net/forum?id=U1Kr8FTyhQ'>https://openreview.net/forum?id=U1Kr8FTyhQ</a></p>
<p><b>Keywords</b>: Landmarks retrieval, non-fine-tuning, spatial verification, explainable AI, hypothesis and test
</p><p><b>Compressor summary</b>: The paper proposes a new image retrieval method that uses a topological model instead of a spatial one to improve performance and overcome limitations in recognizing features, while retaining high explainability and being lightweight.</p><hr><h3>Long Sequence Hopfield Memory</h3>
<p>Hamza Tahir Chaudhry, Jacob A Zavatone-Veth, Dmitry Krotov, Cengiz Pehlevan</p>
<p><a href='https://openreview.net/forum?id=Tz2uONpgpy'>https://openreview.net/forum?id=Tz2uONpgpy</a></p>
<p><b>Keywords</b>: Sequence Recall, Dense Associative Memory, Memory Capacity, Hopfield Networks, Biological Motor Control
</p><p><b>Compressor summary</b>: The authors propose a new sequence memory model that can store longer sequences by using nonlinear interactions and a novel recall rule, and discuss its biological implications for motor neuroscience.</p><hr><h3>An Efficient Doubly-Robust Test for the Kernel Treatment Effect</h3>
<p>Diego Martinez-Taboada, Aaditya Ramdas, Edward Kennedy</p>
<p><a href='https://openreview.net/forum?id=TyLjNSbSOe'>https://openreview.net/forum?id=TyLjNSbSOe</a></p>
<p><b>Keywords</b>: kernel treatment effect, causal inference, maximum mean discrepancy
</p><p><b>Compressor summary</b>: The authors propose a new kernel-based test for distributional effects of binary treatments that is computationally efficient and has valid type-I error.</p><hr><h3>Tools for Verifying Neural Models' Training Data</h3>
<p>Dami Choi, Yonadav G Shavit, David Duvenaud</p>
<p><a href='https://openreview.net/forum?id=TwLHB8sKme'>https://openreview.net/forum?id=TwLHB8sKme</a></p>
<p><b>Keywords</b>: Large Scale Learning, ML Security, AI Governance
</p><p><b>Compressor summary</b>: The authors propose and explore efficient strategies for verifying the provenance of large neural models using "Proof-of-Training-Data" protocols that can detect various attacks, such as those based on harmful or beneficial data sources.</p><hr><h3>Optimal Parameter and Neuron Pruning for Out-of-Distribution Detection</h3>
<p>Chao Chen, Zhihang Fu, Kai Liu, Ze Chen, Mingyuan Tao, Jieping Ye</p>
<p><a href='https://openreview.net/forum?id=TtCPFN5fhO'>https://openreview.net/forum?id=TtCPFN5fhO</a></p>
<p><b>Keywords</b>: Out-of-Distribution Detection, Parameter Sensitivity, Parameter Pruning, Neuron Pruning
</p><p><b>Compressor summary</b>: The paper proposes an efficient method called OPNP to detect out-of-distribution samples in machine learning models without requiring extra training data or resources, and demonstrates its effectiveness on various tasks and architectures.</p><hr><h3>GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning</h3>
<p>Haiteng Zhao, Shengchao Liu, Chang Ma, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, Qi Liu</p>
<p><a href='https://openreview.net/forum?id=Tt6DrRCgJV'>https://openreview.net/forum?id=Tt6DrRCgJV</a></p>
<p><b>Keywords</b>: Instruction, Molecule, Zero Shot, Graph, Language Model
</p><p><b>Compressor summary</b>: GIMLET is a model that uses language instructions and graph structures to accomplish molecule-related tasks without expensive lab experiments, outperforming existing methods in instruction-based zero-shot learning.</p><hr><h3>Meta-Adapter: An Online Few-shot Learner for Vision-Language Model</h3>
<p>Cheng Cheng, Lin Song, Ruoyi Xue, Hang Wang, Hongbin Sun, Yixiao Ge, Ying Shan</p>
<p><a href='https://openreview.net/forum?id=Ts0d8PvTeB'>https://openreview.net/forum?id=Ts0d8PvTeB</a></p>
<p><b>Keywords</b>: Few-shot Learning; Vision-Language Model Adaption
</p><p><b>Compressor summary</b>: CLIP is a powerful vision-language pre-training method for image recognition, but it needs offline fine-tuning and can overfit. The proposed Meta-Adapter improves CLIP's few-shot learning capabilities online, without extra fine-tuning, achieving high efficiency and performance.</p><hr><h3>SatLM: Satisfiability-Aided Language Models Using Declarative Prompting</h3>
<p>Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett</p>
<p><a href='https://openreview.net/forum?id=TqW5PL1Poi'>https://openreview.net/forum?id=TqW5PL1Poi</a></p>
<p><b>Keywords</b>: Reasoning, Chain-of-thought, Logical Reasoning, Arithmetic Reasoning, Prompting, In-Context Learning, Large Language Model
</p><p><b>Compressor summary</b>: The paper proposes SatLM, which uses an LLM to generate a declarative task specification and an automated theorem prover to derive the answer, improving reasoning capabilities for constraint solving problems.</p><hr><h3>Predict-then-Calibrate: A New Perspective of Robust Contextual LP</h3>
<p>Chunlin Sun, Linyu Liu, Xiaocheng Li</p>
<p><a href='https://openreview.net/forum?id=TnTDiCppx5'>https://openreview.net/forum?id=TnTDiCppx5</a></p>
<p><b>Keywords</b>: Uncertainty Quantification, Contextual LP, Robust Optimization, Distributionally Robust Optimization
</p><p><b>Compressor summary</b>: The paper proposes a method called predict-then-calibrate for solving optimization problems with contextual information, which separates the prediction and uncertainty quantification steps and allows for more flexible machine learning models and improved performance.</p><hr><h3>Kernel Stein Discrepancy thinning: a theoretical perspective of pathologies and a practical fix with regularization</h3>
<p>Clement Benard, Brian Staber, Sébastien Da Veiga</p>
<p><a href='https://openreview.net/forum?id=TjgG4UT62W'>https://openreview.net/forum?id=TjgG4UT62W</a></p>
<p><b>Keywords</b>: Bayesian inference, Markov chain Monte Carlo, kernelized Stein discrepancy, Stein thinning, kernel methods
</p><p><b>Compressor summary</b>: Stein thinning is a post-processing method for MCMC outputs that minimizes kernelized Stein discrepancy, but has some drawbacks; the paper analyzes these issues and proposes an improved algorithm with theoretical guarantees and experimental results.</p><hr><h3>Exact recovery and Bregman hard clustering of node-attributed Stochastic Block Model</h3>
<p>Maximilien Dreveton, Felipe Schreiber Fernandes, Daniel R. Figueiredo</p>
<p><a href='https://openreview.net/forum?id=TjJJmcHw9p'>https://openreview.net/forum?id=TjJJmcHw9p</a></p>
<p><b>Keywords</b>: community detection, stochastic block model, bregman divergence
</p><p><b>Compressor summary</b>: The text describes a new approach to cluster nodes in networks based on their connections and attributes, using an information-theoretic criterion and an iterative algorithm that performs better than existing methods.</p><hr><h3>Loss Dynamics of Temporal Difference Reinforcement Learning</h3>
<p>Blake Bordelon, Paul Masset, Henry Kuo, Cengiz Pehlevan</p>
<p><a href='https://openreview.net/forum?id=Tj0eXVPnRX'>https://openreview.net/forum?id=Tj0eXVPnRX</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Statistical Mechanics, Stochastic Gradient Descent
</p><p><b>Compressor summary</b>: The paper uses statistical physics concepts to study the effects of parameter choice and feature representation on the learning dynamics of reinforcement learning models with linear function approximators and sparse feedback.</p><hr><h3>Bayesian target optimisation for high-precision holographic optogenetics</h3>
<p>Marcus Triplett, Marta Agnieszka Gajowa, Hillel Adesnik, Liam Paninski</p>
<p><a href='https://openreview.net/forum?id=TiFMYdQiqp'>https://openreview.net/forum?id=TiFMYdQiqp</a></p>
<p><b>Keywords</b>: Neuroscience, neural stimulation, optogenetics, calcium imaging
</p><p><b>Compressor summary</b>: Bayesian target optimization is a novel computational approach that reduces off-target stimulation in two-photon optogenetics by modeling neural responses and optimizing laser powers and locations for precise stimulation of desired activity patterns.</p><hr><h3>Sparse Modular Activation for Efficient Sequence Modeling</h3>
<p>Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, ChengXiang Zhai</p>
<p><a href='https://openreview.net/forum?id=TfbzX6I14i'>https://openreview.net/forum?id=TfbzX6I14i</a></p>
<p><b>Keywords</b>: Sequence Modeling, Modularity, Sparsity, Attention Mechanism, State Space Model, Mixture of Experts, Neural Network, Transformer
</p><p><b>Compressor summary</b>: Sparse Modular Activation (SMA) improves sequence modeling efficiency by allowing neural networks to dynamically activate sub-modules, enabling linear inference complexity and better quality-efficiency trade-offs.</p><hr><h3>4M: Massively Multimodal Masked Modeling</h3>
<p>David Mizrahi, Roman Bachmann, Oguzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, Amir Zamir</p>
<p><a href='https://openreview.net/forum?id=TegmlsD8oQ'>https://openreview.net/forum?id=TegmlsD8oQ</a></p>
<p><b>Keywords</b>: multimodal learning, multitask learning, representation learning, transfer learning, foundation models, generative models, computer vision
</p><p><b>Compressor summary</b>: The paper introduces 4M, a multimodal training scheme that trains a unified Transformer encoder-decoder on various input/output modalities, achieving versatile models for computer vision tasks.</p><hr><h3>The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance</h3>
<p>Jon Donnelly, Srikar Katta, Cynthia Rudin, Edward P Browne</p>
<p><a href='https://openreview.net/forum?id=TczT2jiPT5'>https://openreview.net/forum?id=TczT2jiPT5</a></p>
<p><b>Keywords</b>: Rashomon Effect, Variable Importance, XAI, Stability, Interpretable Machine Learning
</p><p><b>Compressor summary</b>: The authors propose a new variable importance framework that quantifies the importance of a variable across all good models and is stable across data distribution, which can handle complex simulation setups and estimate true importance for real-world case studies like predicting HIV load.</p><hr><h3>L-CAD: Language-based Colorization with Any-level Descriptions using Diffusion Priors</h3>
<p>Zheng Chang, Shuchen Weng, Peixuan Zhang, Yu Li, Si Li, Boxin Shi</p>
<p><a href='https://openreview.net/forum?id=TcmjewOAd1'>https://openreview.net/forum?id=TcmjewOAd1</a></p>
<p><b>Keywords</b>: Colorization, Language-based generation, Diffusion model
</p><p><b>Compressor summary</b>: The paper proposes a model that can colorize images using natural language descriptions of any level of detail, handling ambiguity and preserving local structures, while achieving better results than previous methods.</p><hr><h3>BERT Lost Patience Won't Be Robust to Adversarial Slowdown</h3>
<p>Zachary Coalson, Gabriel Ritter, Rakesh B Bobba, Sanghyun Hong</p>
<p><a href='https://openreview.net/forum?id=TcG8jhOPdv'>https://openreview.net/forum?id=TcG8jhOPdv</a></p>
<p><b>Keywords</b>: Efficient Methods for NLP; Multi-exit Language Models; Adversarial Slowdown
</p><p><b>Compressor summary</b>: The paper evaluates the robustness of multi-exit language models to an adversarial slowdown attack called WAFFLE, which generates natural text perturbations that reduce computational savings and require further research on efficient yet robust models.</p><hr><h3>Fast Trainable Projection for Robust Fine-tuning</h3>
<p>Junjiao Tian, Yen-Cheng Liu, James Smith, Zsolt Kira</p>
<p><a href='https://openreview.net/forum?id=Tb7np0MInj'>https://openreview.net/forum?id=Tb7np0MInj</a></p>
<p><b>Keywords</b>: fine-tuning, transfer learning, regularization
</p><p><b>Compressor summary</b>: The paper introduces Fast Trainable Projection, a new projection-based fine-tuning algorithm that improves efficiency and scalability for robust fine-tuning of pre-trained models in various vision tasks.</p><hr><h3>MIM4DD: Mutual Information Maximization for Dataset Distillation</h3>
<p>Yuzhang Shang, Zhihang Yuan, Yan Yan</p>
<p><a href='https://openreview.net/forum?id=TZtw5YgxTE'>https://openreview.net/forum?id=TZtw5YgxTE</a></p>
<p><b>Keywords</b>: Dataset Distillation
</p><p><b>Compressor summary</b>: MIM4DD uses mutual information to measure and optimize the shared information between synthetic and real datasets in dataset distillation, improving their performance.</p><hr><h3>A Variational Perspective on High-Resolution ODEs</h3>
<p>Hoomaan Maskan, Konstantinos C. Zygalakis, Alp Yurtsever</p>
<p><a href='https://openreview.net/forum?id=TXq8PCRSoY'>https://openreview.net/forum?id=TXq8PCRSoY</a></p>
<p><b>Keywords</b>: Nesterov's accelerated gradient, gradient descent, Lyapunov function, gradient norm minimization, rate-matching, stochastic variance reduction, stochastic gradient descent, noisy gradient
</p><p><b>Compressor summary</b>: The text discusses a novel variational approach to unconstrained minimization that leads to faster convergence, interprets Nesterov's method as a high-resolution ODE discretization, and proposes a stochastic method for noisy gradients.</p><hr><h3>Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures</h3>
<p>Hamish Flynn, David Reeb, Melih Kandemir, Jan Peters</p>
<p><a href='https://openreview.net/forum?id=TXoZiUZywf'>https://openreview.net/forum?id=TXoZiUZywf</a></p>
<p><b>Keywords</b>: Linear bandits, confidence sequences, martingales, convex optimization, cumulative regret, regret analysis
</p><p><b>Compressor summary</b>: The paper proposes new algorithms with better regret guarantees for the stochastic linear bandit problem using tailored confidence sequences for reward function estimation and action selection.</p><hr><h3>Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets</h3>
<p>Zhang-Wei Hong, Aviral Kumar, Sathwik Karnik, Abhishek Bhandwaldar, Akash Srivastava, Joni Pajarinen, Romain Laroche, Abhishek Gupta, Pulkit Agrawal</p>
<p><a href='https://openreview.net/forum?id=TW99HrZCJU'>https://openreview.net/forum?id=TW99HrZCJU</a></p>
<p><b>Keywords</b>: offline reinforcement learning, reinforcement learning, sampling, experience replay
</p><p><b>Compressor summary</b>: Our method improves offline RL by using importance sampling weights to emulate data from an optimal policy, addressing the challenge of distributional mismatch due to imbalanced datasets.</p><hr><h3>Fair Streaming Principal Component Analysis: Statistical and Algorithmic Viewpoint</h3>
<p>Junghyun Lee, Hanseul Cho, Se-Young Yun, Chulhee Yun</p>
<p><a href='https://openreview.net/forum?id=TW3ipYdDQG'>https://openreview.net/forum?id=TW3ipYdDQG</a></p>
<p><b>Keywords</b>: streaming, PCA, memory-limited, fair representation, online learning
</p><p><b>Compressor summary</b>: The paper proposes a new notion of fair principal component analysis (PCA) called PAFO-learnability and a memory-efficient algorithm for fair streaming PCA called fair noisy power method (FNPM).</p><hr><h3>Bounce: Reliable High-Dimensional Bayesian Optimization for Combinatorial and Mixed Spaces</h3>
<p>Leonard Papenmeier, Luigi Nardi, Matthias Poloczek</p>
<p><a href='https://openreview.net/forum?id=TVD3wNVH9A'>https://openreview.net/forum?id=TVD3wNVH9A</a></p>
<p><b>Keywords</b>: Bayesian optimization, global optimization, Gaussian process, combinatorial optimization, high-dimensional
</p><p><b>Compressor summary</b>: Bounce is a new algorithm for optimizing black-box functions with mixed and combinatorial input spaces, which outperforms existing methods in high-dimensional problems.</p><hr><h3>Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry</h3>
<p>Bariscan Bozkurt, Cengiz Pehlevan, Alper Tunga Erdogan</p>
<p><a href='https://openreview.net/forum?id=TUGoUNkccV'>https://openreview.net/forum?id=TUGoUNkccV</a></p>
<p><b>Keywords</b>: Correlative information maximization, Biologically-plausible learning, Multi-compartment neural model
</p><p><b>Compressor summary</b>: The authors propose a new approach for signal propagation in biological neural networks that addresses concerns about the plausibility of conventional artificial neural networks and backpropagation algorithm, and provides a natural resolution to the weight symmetry problem.</p><hr><h3>NAP: Neural 3D Articulated Object Prior</h3>
<p>Jiahui Lei, Congyue Deng, Bokui Shen, Leonidas Guibas, Kostas Daniilidis</p>
<p><a href='https://openreview.net/forum?id=TTkklyFv7e'>https://openreview.net/forum?id=TTkklyFv7e</a></p>
<p><b>Keywords</b>: 3D articulated objects, diffusion models, generative models
</p><p><b>Compressor summary</b>: NAP is a 3D deep generative model that synthesizes articulated objects using an innovative articulation tree/graph parameterization and diffusion-denoising probabilistic model.</p><hr><h3>Scalarization for Multi-Task and Multi-Domain Learning at Scale</h3>
<p>Amelie Royer, Tijmen Blankevoort, Babak Ehteshami Bejnordi</p>
<p><a href='https://openreview.net/forum?id=TSuq3debnD'>https://openreview.net/forum?id=TSuq3debnD</a></p>
<p><b>Keywords</b>: multitask, multidomain, optimization, population based training
</p><p><b>Compressor summary</b>: The paragraph discusses how training a single model on multiple inputs and outputs improves efficiency and transfer learning, but optimizing such networks is challenging, and proposes a population-based training method to find optimal scalarization weights.</p><hr><h3>Context Shift Reduction for Offline Meta-Reinforcement Learning</h3>
<p>Yunkai Gao, Rui Zhang, Jiaming Guo, Fan Wu, Qi Yi, Shaohui Peng, Siming Lan, Ruizhi Chen, Zidong Du, Xing Hu, Qi Guo, Ling Li, Yunji Chen</p>
<p><a href='https://openreview.net/forum?id=TStMZH3Xqx'>https://openreview.net/forum?id=TStMZH3Xqx</a></p>
<p><b>Keywords</b>: offline meta-reinforcement learning, offline reinforcement learning, meta-reinforcement learning
</p><p><b>Compressor summary</b>: The paper proposes a new approach called CSRO to address the context shift problem in offline meta-reinforcement learning by reducing the influence of policy in context during both training and testing phases using mutual information representation learning and non-prior context collection strategy.</p><hr><h3>BayesTune: Bayesian Sparse Deep Model Fine-tuning</h3>
<p>Minyoung Kim, Timothy Hospedales</p>
<p><a href='https://openreview.net/forum?id=TRuqrVsmZK'>https://openreview.net/forum?id=TRuqrVsmZK</a></p>
<p><b>Keywords</b>: Parameter-efficient Foundation model fine-tuning, Bayesian methods, Stochastic-Gradient MCMC
</p><p><b>Compressor summary</b>: The paper proposes a new Bayesian sparse fine-tuning algorithm that uses a Laplace prior to select which parameters of foundation models need to be updated for specific tasks, achieving significant improvement over existing methods on NLP and vision benchmarks.</p><hr><h3>GLOBER: Coherent Non-autoregressive Video Generation via GLOBal Guided Video DecodER</h3>
<p>Mingzhen Sun, Weining Wang, Zihan Qin, Jiahui Sun, Sihan Chen, Jing Liu</p>
<p><a href='https://openreview.net/forum?id=TRbklCR2ZW'>https://openreview.net/forum?id=TRbklCR2ZW</a></p>
<p><b>Keywords</b>: Video Generation, Video Autoencoder, Diffusion Probabilistic Model
</p><p><b>Compressor summary</b>: The GLOBER method generates coherent videos using a novel non-autoregressive approach that first creates global features and then synthesizes video frames based on them.</p><hr><h3>Neural Injective Functions for Multisets, Measures and Graphs via a Finite Witness Theorem</h3>
<p>Tal Amir, Steven J. Gortler, Ilai Avni, Ravina Ravina, Nadav Dym</p>
<p><a href='https://openreview.net/forum?id=TQlpqmCeMe'>https://openreview.net/forum?id=TQlpqmCeMe</a></p>
<p><b>Keywords</b>: Equivariant Neural Networks, Universal approximation, Geometric deep learning, multiset learning, injective multiset functions, learning on measures. WL test
</p><p><b>Compressor summary</b>: This paper shows that using non-polynomial activation in neural networks creates injective multiset functions, filling a gap between theory and practice, and also provides new results for graph neural networks and multiset functions approximation.</p><hr><h3>Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models</h3>
<p>Qiong Wu, Wei Yu, Yiyi Zhou, Shubin Huang, Xiaoshuai Sun, Rongrong Ji</p>
<p><a href='https://openreview.net/forum?id=TPeAmxwPK2'>https://openreview.net/forum?id=TPeAmxwPK2</a></p>
<p><b>Keywords</b>: vision and language, parameter and computation efficient transfer learning
</p><p><b>Compressor summary</b>: The paper proposes a dynamic architecture skipping approach to achieve efficient transfer learning for vision-language models by observing module significance with reinforcement learning and reducing redundant parameters and computation.</p><hr><h3>Multi-task Graph Neural Architecture Search with Task-aware Collaboration and Curriculum</h3>
<p>Yijian Qin, Xin Wang, Ziwei Zhang, Hong Chen, Wenwu Zhu</p>
<p><a href='https://openreview.net/forum?id=TOxpAwp0VE'>https://openreview.net/forum?id=TOxpAwp0VE</a></p>
<p><b>Keywords</b>: graph neural network, neural architecture search, multi-task learning
</p><p><b>Compressor summary</b>: The paper proposes a novel method called MTGC3 that can automatically design optimal graph neural network architectures for multiple tasks simultaneously by learning their collaborative relationships and using a task-wise curriculum training strategy.</p><hr><h3>Information Geometry of the Retinal Representation Manifold</h3>
<p>Xuehao Ding, Dongsoo Lee, Joshua Brendan Melander, George Sivulka, Surya Ganguli, Stephen Baccus</p>
<p><a href='https://openreview.net/forum?id=TNLO8KNFFZ'>https://openreview.net/forum?id=TNLO8KNFFZ</a></p>
<p><b>Keywords</b>: neural coding, theoretical neuroscience, stochastic methods, neural networks
</p><p><b>Compressor summary</b>: The study uses a neural network model of salamander retinal cells to show that retinal noise correlations limit information transmission in natural scenes and that population coding benefits from complementary coding.</p><hr><h3>Learning Rate Free Bayesian Inference in Constrained Domains</h3>
<p>Louis Sharrock, Lester Mackey, Christopher Nemeth</p>
<p><a href='https://openreview.net/forum?id=TNAGFUcSP7'>https://openreview.net/forum?id=TNAGFUcSP7</a></p>
<p><b>Keywords</b>: Bayesian Inference, Particle Based Variational Inference, Sampling, Wasserstein Gradient Descent, Coin Betting, Constrained Domains
</p><p><b>Compressor summary</b>: The paper presents new particle-based algorithms for sampling on constrained domains that don't require learning rate adjustments and are based on convex optimization ideas and probability measure space perspective.</p><hr><h3>CoDet: Co-occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection</h3>
<p>Chuofan Ma, Yi Jiang, Xin Wen, Zehuan Yuan, XIAOJUAN QI</p>
<p><a href='https://openreview.net/forum?id=TKjX41IP7n'>https://openreview.net/forum?id=TKjX41IP7n</a></p>
<p><b>Keywords</b>: Open-vocabulary Object Detection; Object-level Vision-Language Pretraining
</p><p><b>Compressor summary</b>: The paper introduces CoDet, a new method for open-vocabulary object detection that improves region-word alignment by grouping images with shared caption concepts and leveraging visual similarities.</p><hr><h3>White-Box Transformers via Sparse Rate Reduction</h3>
<p>Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Benjamin David Haeffele, Yi Ma</p>
<p><a href='https://openreview.net/forum?id=THfl8hdVxH'>https://openreview.net/forum?id=THfl8hdVxH</a></p>
<p><b>Keywords</b>: white-box deep neural networks, representation learning, transformer, sparse coding
</p><p><b>Compressor summary</b>: The paper argues that representation learning aims to compress and transform data distributions using low-dimensional Gaussian mixtures, and shows how transformer blocks achieve this using sparse rate reduction as a unified objective.</p><hr><h3>Uniform Convergence with Square-Root Lipschitz Loss</h3>
<p>Lijia Zhou, Zhen Dai, Frederic Koehler, Nathan Srebro</p>
<p><a href='https://openreview.net/forum?id=TEpRn67828'>https://openreview.net/forum?id=TEpRn67828</a></p>
<p><b>Keywords</b>: Uniform Convergence, Square-Root Lipschitz, Benign Overfitting, Minimal Norm Interpolation, Phase Retrieval, ReLU Regression, Matrix Sensing
</p><p><b>Compressor summary</b>: The paper provides uniform convergence guarantees for Gaussian data using Radamacher complexity and square-root-Lipschitz losses, generalizing previous smoothness-based results and covering non-smooth loss functions like phase retrieval and ReLU regression.</p><hr><h3>REx: Data-Free Residual Quantization Error Expansion</h3>
<p>Edouard YVINEC, Arnaud Dapogny, Matthieu Cord, Kevin Bailly</p>
<p><a href='https://openreview.net/forum?id=TDS3kqRteY'>https://openreview.net/forum?id=TDS3kqRteY</a></p>
<p><b>Keywords</b>: deep learning, quantization, compression, acceleration, data-free
</p><p><b>Compressor summary</b>: REx is a data-free quantization method for DNNs that adapts to different devices and bit widths, improving accuracy-speed trade-offs and solving the outlier problem in large language models.</p><hr><h3>GEQ: Gaussian Kernel Inspired Equilibrium Models</h3>
<p>Mingjie Li, Yisen Wang, Zhouchen Lin</p>
<p><a href='https://openreview.net/forum?id=TBOfDCX4Gz'>https://openreview.net/forum?id=TBOfDCX4Gz</a></p>
<p><b>Keywords</b>: equilibirum models, neural networks
</p><p><b>Compressor summary</b>: The paper proposes GEQ, a new equilibrium model that uses Gaussian kernels to capture nonlinear feature dependencies in input data, improving performance over traditional OptEqs models.</p><hr><h3>Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models</h3>
<p>Julien Niklas Siems, Konstantin Ditschuneit, Winfried Ripken, Alma Lindborg, Maximilian Schambach, Johannes Otterbach, Martin Genzel</p>
<p><a href='https://openreview.net/forum?id=TAIYBdRb3C'>https://openreview.net/forum?id=TAIYBdRb3C</a></p>
<p><b>Keywords</b>: Interpretable Machine Learning, Generalized Additive Models, Concurvity, Multicollinearity, Regularization, Time-Series Forecasting, Interpretability
</p><p><b>Compressor summary</b>: The paper introduces a regularizer to improve interpretability of generalized additive models by reducing feature dependencies, and demonstrates its effectiveness on synthetic and real datasets.</p><hr><h3>Mass-Producing Failures of Multimodal Systems with Language Models</h3>
<p>Shengbang Tong, Erik Jones, Jacob Steinhardt</p>
<p><a href='https://openreview.net/forum?id=T6iiOqsGOh'>https://openreview.net/forum?id=T6iiOqsGOh</a></p>
<p><b>Keywords</b>: safety, red-teaming, robustness, explainability, failures, multimodal models, vision-language, natural-language explanations
</p><p><b>Compressor summary</b>: MultiMon is a system that automatically finds and describes systematic failures in multimodal models using natural language.</p><hr><h3>UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures</h3>
<p>Zhong-Qiu Wang, Shinji Watanabe</p>
<p><a href='https://openreview.net/forum?id=T5h69frFF7'>https://openreview.net/forum?id=T5h69frFF7</a></p>
<p><b>Keywords</b>: Speech separation, microphone array processing, deep learning
</p><p><b>Compressor summary</b>: UNSSOR is an algorithm that uses over-determined mixtures to separate speakers in noisy recordings with deep neural networks and linear filters.</p><hr><h3>PUCA: Patch-Unshuffle and Channel Attention for Enhanced Self-Supervised Image Denoising</h3>
<p>Hyemi Jang, Junsung Park, Dahuin Jung, Jaihyun Lew, Ho Bae, Sungroh Yoon</p>
<p><a href='https://openreview.net/forum?id=T3SstRu5fq'>https://openreview.net/forum?id=T3SstRu5fq</a></p>
<p><b>Keywords</b>: self-supervised image denoising, low-level vision
</p><p><b>Compressor summary</b>: PUCA is a novel J-invariant U-Net architecture for self-supervised image denoising that leverages patch-unshuffle/shuffle and dilated attention blocks to achieve state-of-the-art performance.</p><hr><h3>Connecting Certified and Adversarial Training</h3>
<p>Yuhao Mao, Mark Niklas Mueller, Marc Fischer, Martin Vechev</p>
<p><a href='https://openreview.net/forum?id=T2lM4ohRwb'>https://openreview.net/forum?id=T2lM4ohRwb</a></p>
<p><b>Keywords</b>: Certified Training, Certified Robustness, Adversarial Robustness, Robustness Verification
</p><p><b>Compressor summary</b>: TAPS is a new certified training method that combines IBP and PGD to optimize worst-case loss approximations, improving both certification and standard accuracies.</p><hr><h3>Precision-Recall Divergence Optimization for Generative Modeling with GANs and Normalizing Flows</h3>
<p>Alexandre Verine, benjamin negrevergne, Muni Sreenivas Pydi, Yann Chevaleyre</p>
<p><a href='https://openreview.net/forum?id=SzYHu7EIwZ'>https://openreview.net/forum?id=SzYHu7EIwZ</a></p>
<p><b>Keywords</b>: Generative Models, Precision, Recall, Optimization, f-Divergeces
</p><p><b>Compressor summary</b>: The paper proposes a new method for training generative models that optimizes a user-defined trade-off between image quality (precision) and diversity (recall), based on a novel family of $f$-divergences called PR-divergences.</p><hr><h3>Implicit Variational Inference for High-Dimensional Posteriors</h3>
<p>Anshuk Uppal, Kristoffer Stensbo-Smidt, Wouter Boomsma, Jes Frellsen</p>
<p><a href='https://openreview.net/forum?id=Sxu7xlUJGx'>https://openreview.net/forum?id=Sxu7xlUJGx</a></p>
<p><b>Keywords</b>: Implicit models, Variational Inference, Bayesian Deep Learning, Bayesian Inference, Generative Modelling
</p><p><b>Compressor summary</b>: The paper proposes a new method for Bayesian models using neural samplers with implicit distributions to capture complex posteriors in high-dimensional spaces, enabling better performance in large Bayesian neural networks.</p><hr><h3>Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management</h3>
<p>Dhawal Gupta, Yinlam Chow, Azamat Tulepbergenov, Mohammad Ghavamzadeh, Craig Boutilier</p>
<p><a href='https://openreview.net/forum?id=SxXN3kNTsV'>https://openreview.net/forum?id=SxXN3kNTsV</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Mixture of Experts, Dialogue Management
</p><p><b>Compressor summary</b>: The text discusses challenges and solutions for using reinforcement learning in developing conversational chatbots, focusing on combining mixture-of-expert language models to reduce action space and improve dialogue management.</p><hr><h3>DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via Physics Simulation</h3>
<p>Rong Wang, Wei Mao, Hongdong Li</p>
<p><a href='https://openreview.net/forum?id=SxVHyYavHy'>https://openreview.net/forum?id=SxVHyYavHy</a></p>
<p><b>Keywords</b>: hand-object pose estimation, physics simulation
</p><p><b>Compressor summary</b>: The paper proposes DeepSimHO, a method that combines deep learning with physics simulation to improve 3D pose estimation of hands interacting with objects in a single image.</p><hr><h3>The Distortion of Binomial Voting Defies Expectation</h3>
<p>Yannai Gonczarowski, Gregory Kehne, Ariel D. Procaccia, Ben Schiffer, Shirley Zhang</p>
<p><a href='https://openreview.net/forum?id=Sv5bo2StIx'>https://openreview.net/forum?id=Sv5bo2StIx</a></p>
<p><b>Keywords</b>: computational social choice, statistics, distortion
</p><p><b>Compressor summary</b>: The paper explores how well voting rules perform on average with different preference distributions, proposing a new rule called binomial voting that works well across various situations.</p><hr><h3>Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations</h3>
<p>Edward Raff, Amol Ashish Khanna, Fred Lu</p>
<p><a href='https://openreview.net/forum?id=SuvDnzrKCo'>https://openreview.net/forum?id=SuvDnzrKCo</a></p>
<p><b>Keywords</b>: Sparsity, Differential Privacy, Regression
</p><p><b>Compressor summary</b>: The paper proposes an algorithm for training private regression models on sparse data, which can greatly reduce the runtime compared to existing methods.</p><hr><h3>Topological Parallax: A Geometric Specification for Deep Perception Models</h3>
<p>Abraham David Smith, Michael J. Catanzaro, Gabrielle Angeloro, Nirav Patel, Paul Bendich</p>
<p><a href='https://openreview.net/forum?id=SthlUe5xDP'>https://openreview.net/forum?id=SthlUe5xDP</a></p>
<p><b>Keywords</b>: topological data analysis, persistent homology, convexity, AI safety, interpolation
</p><p><b>Compressor summary</b>: Topological parallax is a tool that compares AI models and datasets to ensure their geometric similarity, which is important for safety and robustness.</p><hr><h3>Dataset Diffusion: Diffusion-based Synthetic Data Generation for Pixel-Level Semantic Segmentation</h3>
<p>Quang Ho Nguyen, Truong Tuan Vu, Anh Tuan Tran, Khoi Nguyen</p>
<p><a href='https://openreview.net/forum?id=StD4J5ZlI5'>https://openreview.net/forum?id=StD4J5ZlI5</a></p>
<p><b>Keywords</b>: Deep learning; Diffusion Models; Semantic Segmentation; Text-to-Image
</p><p><b>Compressor summary</b>: The paragraph describes a novel method for generating pixel-level semantic segmentation labels using the text-to-image generative model Stable Diffusion, which reduces the need for labor-intensive pixel-wise annotation and outperforms current approaches.</p><hr><h3>Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer</h3>
<p>Bowen Tan, Yun Zhu, Lijuan Liu, Eric Xing, Zhiting Hu, Jindong Chen</p>
<p><a href='https://openreview.net/forum?id=Srt1hhQgqa'>https://openreview.net/forum?id=Srt1hhQgqa</a></p>
<p><b>Keywords</b>: multi-task, large language models, pretrain model
</p><p><b>Compressor summary</b>: Cappy is a small pretrained scorer that improves the performance and efficiency of multi-task large language models without requiring their finetuning or access to their parameters.</p><hr><h3>Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model</h3>
<p>Zirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, Daochen Zha, Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, Vipin Chaudhary, Shuai Xu, Xia Hu</p>
<p><a href='https://openreview.net/forum?id=SquMNyrk1O'>https://openreview.net/forum?id=SquMNyrk1O</a></p>
<p><b>Keywords</b>: memory-efficient tuning, language model, transformers
</p><p><b>Compressor summary</b>: The authors propose a new unbiased estimator called \sas\ for reducing memory usage in large pre-trained language models, which can achieve significant memory reduction and improve downstream task performance.</p><hr><h3>Persuading Farsighted Receivers in MDPs: the Power of Honesty</h3>
<p>Martino Bernasconi, Matteo Castiglioni, Alberto Marchesi, Mirco Mutti</p>
<p><a href='https://openreview.net/forum?id=SqTUGq0R7j'>https://openreview.net/forum?id=SqTUGq0R7j</a></p>
<p><b>Keywords</b>: Bayesian Persuasion, MPD, information design, signaling
</p><p><b>Compressor summary</b>: The paper proposes a new class of optimal signaling schemes for Bayesian persuasion that consider the receiver's future rewards and can be computed efficiently using a promise-form representation.</p><hr><h3>Solving a Class of Non-Convex  Minimax Optimization in Federated Learning</h3>
<p>Xidong Wu, Jianhui Sun, Zhengmian Hu, Aidong Zhang, Heng Huang</p>
<p><a href='https://openreview.net/forum?id=SpStmVboGy'>https://openreview.net/forum?id=SpStmVboGy</a></p>
<p><b>Keywords</b>: Federated Learning, Non-Convex  Optimization, Minimax Optimization
</p><p><b>Compressor summary</b>: The paper proposes FL algorithms for federated nonconvex minimax optimization problems, reducing communication complexity and achieving similar performance to centralized methods in some settings.</p><hr><h3>On Generalization Bounds for Projective Clustering</h3>
<p>Maria Sofia Bucarelli, Matilde Fjeldsø Larsen, Chris Schwiegelshohn, Mads Toftrup</p>
<p><a href='https://openreview.net/forum?id=Sp0yOBfelp'>https://openreview.net/forum?id=Sp0yOBfelp</a></p>
<p><b>Keywords</b>: Subspace Clustering, Learning Theory, Clustering, Error bounds
</p><p><b>Compressor summary</b>: The paper studies learning bounds for clustering problems with center-based and subspace-based objectives and provides near-optimal convergence rates for both cases.</p><hr><h3>Robust and Actively Secure Serverless Collaborative Learning</h3>
<p>Nicholas Franzese, Adam Dziedzic, Christopher A. Choquette-Choo, Mark R. Thomas, Muhammad Ahmad Kaleem, Stephan Rabanser, Congyu Fang, Somesh Jha, Nicolas Papernot, Xiao Wang</p>
<p><a href='https://openreview.net/forum?id=SouroWC5Un'>https://openreview.net/forum?id=SouroWC5Un</a></p>
<p><b>Keywords</b>: collaborative learning, robust aggregation, secure machine learning
</p><p><b>Compressor summary</b>: The paper proposes a peer-to-peer learning scheme that is secure from malicious servers and robust to malicious clients, and transforms any compatible algorithm for aggregation of model updates in such settings.</p><hr><h3>ARTree: A Deep Autoregressive Model for Phylogenetic Inference</h3>
<p>Tianyu Xie, Cheng Zhang</p>
<p><a href='https://openreview.net/forum?id=SoLebIqHgZ'>https://openreview.net/forum?id=SoLebIqHgZ</a></p>
<p><b>Keywords</b>: phylogenetic inference, autoregressive model, graph neural network, density estimation, variational inference
</p><p><b>Compressor summary</b>: The paper introduces ARTree, a deep autoregressive model for phylogenetic inference using graph neural networks, which can efficiently learn probabilistic models over tree topologies without relying on hand-engineered features.</p><hr><h3>Bayesian Risk-Averse Q-Learning with Streaming Observations</h3>
<p>Yuhao Wang, Enlu Zhou</p>
<p><a href='https://openreview.net/forum?id=SjiLtmZETc'>https://openreview.net/forum?id=SjiLtmZETc</a></p>
<p><b>Keywords</b>: Q-learning, risk-averse reinforcement learning, off-policy learning, Bayesian risk Markov decision process, distributionally robust Markov decision process
</p><p><b>Compressor summary</b>: The paper proposes a robust reinforcement learning framework using BRMDP and a multi-stage Q-learning algorithm that learns a risk-averse optimal policy with real environment observations and strong convergence.</p><hr><h3>Errors-in-variables Fr\'echet Regression with Low-rank Covariate Approximation</h3>
<p>Dogyoon Song, Kyunghee Han</p>
<p><a href='https://openreview.net/forum?id=Sg3aCpWUQP'>https://openreview.net/forum?id=Sg3aCpWUQP</a></p>
<p><b>Keywords</b>: Frechet regression, principal component regression, non-Euclidean, low-rank matrix, errors-in-variables analysis
</p><p><b>Compressor summary</b>: The paper proposes a new method that combines global Fr\'echet regression and principal component regression to improve efficiency and accuracy in non-Euclidean regression analysis by leveraging low-rank structure in the covariate matrix.</p><hr><h3>An Optimal Structured Zeroth-order Algorithm for Non-smooth Optimization</h3>
<p>Marco Rando, Cesare Molinari, Lorenzo Rosasco, Silvia Villa</p>
<p><a href='https://openreview.net/forum?id=SfdkS6tt81'>https://openreview.net/forum?id=SfdkS6tt81</a></p>
<p><b>Keywords</b>: nonsmooth optimization;zeroth order optimization;nonsmooth zeroth-order
</p><p><b>Compressor summary</b>: O-ZD is a new structured finite-difference algorithm for non-smooth black-box optimization that uses smooth approximations of the target function and orthogonal directions to approximate its gradient efficiently.</p><hr><h3>Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent</h3>
<p>Jihao Andreas Lin, Javier Antoran, Shreyas Padhy, David Janz, José Miguel Hernández-Lobato, Alexander Terenin</p>
<p><a href='https://openreview.net/forum?id=Sf9goJtTCE'>https://openreview.net/forum?id=Sf9goJtTCE</a></p>
<p><b>Keywords</b>: Gaussian processes, scalable learning, posterior sampling, Bayesian optimization
</p><p><b>Compressor summary</b>: Stochastic gradient descent is a computationally efficient method for solving linear systems in Gaussian processes, achieving state-of-the-art performance and accurate predictions even when not converging quickly to the optimum.</p><hr><h3>Enhancing Sharpness-Aware Optimization Through Variance Suppression</h3>
<p>Bingcong Li, Georgios B. Giannakis</p>
<p><a href='https://openreview.net/forum?id=Sf3t6Bth4P'>https://openreview.net/forum?id=Sf3t6Bth4P</a></p>
<p><b>Keywords</b>: generalization, optimization, neural networks
</p><p><b>Compressor summary</b>: VaSSO improves SAM by making adversaries more aggressive, leading to better generalization, stability, and robustness in neural network training.</p><hr><h3>Optimistic Exploration in Reinforcement Learning Using Symbolic Model Estimates</h3>
<p>Sarath Sreedharan, Michael Katz</p>
<p><a href='https://openreview.net/forum?id=Sf17j2pkCU'>https://openreview.net/forum?id=Sf17j2pkCU</a></p>
<p><b>Keywords</b>: Planning, Reinforcement Learning, Exploration
</p><p><b>Compressor summary</b>: The paragraph discusses a new method for learning optimistic symbolic models that guide reinforcement learning agents in sparse reward settings, improving exploration speed through action generalization.</p><hr><h3>DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation</h3>
<p>Shentong Mo, Enze Xie, Ruihang Chu, Lanqing HONG, Matthias Nießner, Zhenguo Li</p>
<p><a href='https://openreview.net/forum?id=Se71ks7Mfz'>https://openreview.net/forum?id=Se71ks7Mfz</a></p>
<p><b>Keywords</b>: Diffusion Models, Transformers, 3D Shape Generation
</p><p><b>Compressor summary</b>: The paper proposes a novel Diffusion Transformer (DiT-3D) for generating high-quality 3D shapes from voxelized point clouds, which improves scalability and quality over existing U-Net methods and leverages pre-trained DiT-2D on ImageNet.</p><hr><h3>SpecTr: Fast Speculative Decoding via Optimal Transport</h3>
<p>Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, Felix Yu</p>
<p><a href='https://openreview.net/forum?id=SdYHLTCC5J'>https://openreview.net/forum?id=SdYHLTCC5J</a></p>
<p><b>Keywords</b>: autoregressive sampling; computation efficiency; optimal transport
</p><p><b>Compressor summary</b>: The authors propose SpecTr, a new autoregressive sampling algorithm that uses optimal transport with membership cost to improve the speed and quality of decoding from large language models.</p><hr><h3>ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation</h3>
<p>Shuyang Sun, Weijun Wang, Andrew G. Howard, Qihang Yu, Philip Torr, Liang-Chieh Chen</p>
<p><a href='https://openreview.net/forum?id=SaMrN9tnxE'>https://openreview.net/forum?id=SaMrN9tnxE</a></p>
<p><b>Keywords</b>: Panoptic segmentation, efficient models
</p><p><b>Compressor summary</b>: The paper introduces ReMaX, a technique that improves the training of mask transformers for efficient panoptic segmentation by adding relaxations to predictions.</p><hr><h3>Learning to Parameterize Visual Attributes for Open-set Fine-grained Retrieval</h3>
<p>Shijie Wang, Jianlong Chang, Haojie Li, Zhihui Wang, Wanli Ouyang, Qi Tian</p>
<p><a href='https://openreview.net/forum?id=SaII5qMgKH'>https://openreview.net/forum?id=SaII5qMgKH</a></p>
<p><b>Keywords</b>: Open-set Fine-grained Retrieval, Visual Attribute, Unknown Categories
</p><p><b>Compressor summary</b>: VAPNet is a novel network that learns visual attributes from known categories and integrates them into a retrieval model for unknown categories, without manual annotations.</p><hr><h3>Neural Sampling in Hierarchical Exponential-family Energy-based Models</h3>
<p>Xingsi Dong, Si Wu</p>
<p><a href='https://openreview.net/forum?id=SWU8YLlFVH'>https://openreview.net/forum?id=SWU8YLlFVH</a></p>
<p><b>Keywords</b>: Baysian brain, sampling-based inference, energy-based models, local learning, exponential-family
</p><p><b>Compressor summary</b>: The Hierarchical Exponential-family Energy-based (HEE) model is a new method for inferring and learning in the brain, which uses neural adaptation and allows fast computation and representation similar to biological vision systems.</p><hr><h3>Improving CLIP Training with Language Rewrites</h3>
<p>Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, Yonglong Tian</p>
<p><a href='https://openreview.net/forum?id=SVjDiiVySh'>https://openreview.net/forum?id=SVjDiiVySh</a></p>
<p><b>Keywords</b>: contrastive learning; CLIP; large language model
</p><p><b>Compressor summary</b>: LaCLIP is a method that enhances CLIP training by rewriting image text descriptions with diverse sentences while preserving original meanings, improving transfer performance without extra computation or memory.</p><hr><h3>DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall's Rank Correlation</h3>
<p>Kaipeng Zheng, Huishuai Zhang, Weiran Huang</p>
<p><a href='https://openreview.net/forum?id=SVUQX1W7RL'>https://openreview.net/forum?id=SVUQX1W7RL</a></p>
<p><b>Keywords</b>: Few-shot learning
</p><p><b>Compressor summary</b>: This paper shows that using Kendall's rank correlation instead of geometric similarity metrics improves few-shot learning performance across different methods and datasets.</p><hr><h3>Language Models Meet World Models: Embodied Experiences Enhance Language Models</h3>
<p>Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, Zhiting Hu</p>
<p><a href='https://openreview.net/forum?id=SVBR6xBaMl'>https://openreview.net/forum?id=SVBR6xBaMl</a></p>
<p><b>Keywords</b>: Language Model, World Model, Embodied Experience
</p><p><b>Compressor summary</b>: The paper proposes a method to improve large language models' abilities in reasoning and acting in the physical world by finetuning them with embodied experiences from a simulated environment and using techniques like EWC and LoRA to preserve generality.</p><hr><h3>Learning Robust Statistics for Simulation-based Inference under Model Misspecification</h3>
<p>Daolang Huang, Ayush Bharti, Amauri H Souza, Luigi Acerbi, Samuel Kaski</p>
<p><a href='https://openreview.net/forum?id=STrXsSIEiq'>https://openreview.net/forum?id=STrXsSIEiq</a></p>
<p><b>Keywords</b>: Simulation-based inference, model misspecification, likelihood-free inference, approximate Bayesian computation, neural posterior estimation
</p><p><b>Compressor summary</b>: The authors propose a general approach to handle model misspecification in simulation-based inference methods and demonstrate its superior performance on artificially and realistically misspecified models.</p><hr><h3>Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication</h3>
<p>Chaofei Fan, Nick Hahn, Foram Kamdar, Donald Avansino, Guy H Wilson, Leigh Hochberg, Krishna V. Shenoy, Jaimie M. Henderson, Francis R Willett</p>
<p><a href='https://openreview.net/forum?id=STqaMqhtDi'>https://openreview.net/forum?id=STqaMqhtDi</a></p>
<p><b>Keywords</b>: brain-computer interface, self-training, continual online learning
</p><p><b>Compressor summary</b>: The paper proposes a method to improve intracortical brain-computer interfaces (iBCIs) for communication by enabling self-recalibration using large language models, achieving high performance and long-term stability in a human participant.</p><hr><h3>MomentDiff: Generative Video Moment Retrieval from Random to Real</h3>
<p>Pandeng Li, Chen-Wei Xie, Hongtao Xie, Liming Zhao, Lei Zhang, Yun Zheng, Deli Zhao, Yongdong Zhang</p>
<p><a href='https://openreview.net/forum?id=SQouRKRIXY'>https://openreview.net/forum?id=SQouRKRIXY</a></p>
<p><b>Keywords</b>: Video Moment Retrieval, Diffusion Model
</p><p><b>Compressor summary</b>: MomentDiff is a generative diffusion-based framework that learns to retrieve specific temporal segments of videos based on language descriptions, overcoming temporal location biases in existing methods.</p><hr><h3>Leveraging Locality and Robustness to Achieve Massively Scalable Gaussian Process Regression</h3>
<p>Robert F Allison, Anthony Stephenson, Samuel F, Edward Pyzer-Knapp</p>
<p><a href='https://openreview.net/forum?id=SQP1H9Jy8W'>https://openreview.net/forum?id=SQP1H9Jy8W</a></p>
<p><b>Keywords</b>: Gaussian Processes, Bayesian Inference, Regression, Bayesian Nonparametrics, Kernel Methods
</p><p><b>Compressor summary</b>: GP nearest-neighbour prediction is accurate and computationally efficient even when data size is large, as parameter estimation and model assumptions become less important.</p><hr><h3>Cognitive Model Discovery via Disentangled RNNs</h3>
<p>Kevin J Miller, Maria K Eckstein, Matthew Botvinick, Zeb Kurth-Nelson</p>
<p><a href='https://openreview.net/forum?id=SOEF0i0G1z'>https://openreview.net/forum?id=SOEF0i0G1z</a></p>
<p><b>Keywords</b>: Cognitive modeling, neural networks, interpretability, disentangling, neuroscience, rodent behavior
</p><p><b>Compressor summary</b>: The authors propose an alternative approach to construct cognitive models from behavioral data using sparse and interpretable recurrent neural networks.</p><hr><h3>Variational Annealing on Graphs for Combinatorial Optimization</h3>
<p>Sebastian Sanokowski, Wilhelm Franz Berghammer, Sepp Hochreiter, Sebastian Lehner</p>
<p><a href='https://openreview.net/forum?id=SLx7paoaTU'>https://openreview.net/forum?id=SLx7paoaTU</a></p>
<p><b>Keywords</b>: Combinatorial Optimization, Entropy Regularization, Graph Neural Networks, Statistical Mechanics
</p><p><b>Compressor summary</b>: Unsupervised learning methods using probabilistic approaches for combinatorial optimization may not perform well on difficult problems due to the assumption of independent solution variables; author propose Subgraph Tokenization, which represents a set of solution variables by one token, and annealed entropy regularization, which improves efficiency and stability.</p><hr><h3>PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model</h3>
<p>Yizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Joshua M. Susskind, Navdeep Jaitly</p>
<p><a href='https://openreview.net/forum?id=SLwy8UVS8Y'>https://openreview.net/forum?id=SLwy8UVS8Y</a></p>
<p><b>Keywords</b>: Text generation, diffusion model, NLP
</p><p><b>Compressor summary</b>: PLANNER combines latent semantic diffusion with autoregressive generation to produce fluent and controllable long-form text for various tasks.</p><hr><h3>CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra</h3>
<p>Andres Potapczynski, Marc Anton Finzi, Geoff Pleiss, Andrew Gordon Wilson</p>
<p><a href='https://openreview.net/forum?id=SLtNFERsHo'>https://openreview.net/forum?id=SLtNFERsHo</a></p>
<p><b>Keywords</b>: Machine Learning, Numerical Linear Algebra, partial differential equations, Gaussian processes, equivariance, graph learning, spectral analysis
</p><p><b>Compressor summary</b>: CoLA is a framework for large-scale linear algebra problems in machine learning that combines a linear operator abstraction with compositional dispatch rules to automatically construct memory and runtime efficient numerical algorithms.</p><hr><h3>Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning</h3>
<p>Tyler Kastner, Murat A Erdogdu, Amir-massoud Farahmand</p>
<p><a href='https://openreview.net/forum?id=SLTQluG80x'>https://openreview.net/forum?id=SLTQluG80x</a></p>
<p><b>Keywords</b>: Reinforcement learning, Risk-Sensitive Reinforcement Learning, Model-Based Reinforcement Learning, Distributional Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper explores learning optimal models for risk-sensitive reinforcement learning using distributional reinforcement learning and introduces two new notions of model equivalence.</p><hr><h3>Adversarial Robustness through Random Weight Sampling</h3>
<p>Yanxiang Ma, Minjing Dong, Chang Xu</p>
<p><a href='https://openreview.net/forum?id=SIE9N5nnHg'>https://openreview.net/forum?id=SIE9N5nnHg</a></p>
<p><b>Keywords</b>: Adversarial robustness; Randomized defense; Random parameters optimization
</p><p><b>Compressor summary</b>: The authors propose a new randomized defense mechanism for deep neural networks, called Constrained Trainable Random Weight (CTRW), which optimizes randomness parameters to balance natural performance and adversarial robustness, achieving better results on several datasets and benchmarks.</p><hr><h3>Unlocking Deterministic Robustness Certification on ImageNet</h3>
<p>Kai Hu, Andy Zou, Zifan Wang, Klas Leino, Matt Fredrikson</p>
<p><a href='https://openreview.net/forum?id=SHyVaWGTO4'>https://openreview.net/forum?id=SHyVaWGTO4</a></p>
<p><b>Keywords</b>: adversarial robustness, ImageNet, Lipschitz-based certification, ResNet, adversarial examples, ML security
</p><p><b>Compressor summary</b>: The paper proposes LiResNet, a new architecture and EMMA, a loss function, that improve the efficiency of calculating Lipschitz bounds and stabilize robust training for deep networks on large-scale datasets like ImageNet.</p><hr><h3>Hyperbolic Graph Neural Networks at Scale: A Meta Learning Approach</h3>
<p>Nurendra Choudhary, Nikhil Rao, Chandan K. Reddy</p>
<p><a href='https://openreview.net/forum?id=SHVwG9yOEk'>https://openreview.net/forum?id=SHVwG9yOEk</a></p>
<p><b>Keywords</b>: meta learning, hyperbolic networks, scalability, graph neural networks
</p><p><b>Compressor summary</b>: H-GRAM is a novel meta-learning method for hyperbolic neural networks that learns inductive biases from local subgraphs and transfers them to new tasks, improving generalization and scalability in few-shot settings.</p><hr><h3>Contextual Stochastic Bilevel Optimization</h3>
<p>Yifan Hu, Jie Wang, Yao Xie, Andreas Krause, Daniel Kuhn</p>
<p><a href='https://openreview.net/forum?id=SHBksHKutP'>https://openreview.net/forum?id=SHBksHKutP</a></p>
<p><b>Keywords</b>: stochastic optimization, bilevel optimization, contextual stochastic optimization, Multilevel Monte Carlo
</p><p><b>Compressor summary</b>: CSBO is a new framework for bilevel optimization with contextual information that requires a double-loop gradient method based on MLMC to handle its challenges and has applications in various areas like meta-learning and federated learning.</p><hr><h3>Concept Algebra for (Score-Based) Text-Controlled Generative Models</h3>
<p>Zihao Wang, Lin Gui, Jeffrey Negrea, Victor Veitch</p>
<p><a href='https://openreview.net/forum?id=SGlrCuwdsB'>https://openreview.net/forum?id=SGlrCuwdsB</a></p>
<p><b>Keywords</b>: disentanglement; representation learning; text-controlled generative models; diffusion models
</p><p><b>Compressor summary</b>: The paper explores how text-guided generative models like Stable Diffusion represent and manipulate disentangled concepts in a subspace of their representation space.</p><hr><h3>Tracking Most Significant Shifts in Nonparametric Contextual Bandits</h3>
<p>Joe Suk, Samory Kpotufe</p>
<p><a href='https://openreview.net/forum?id=SGerL9HMrp'>https://openreview.net/forum?id=SGerL9HMrp</a></p>
<p><b>Keywords</b>: multi-armed bandits, non-stationary, contextual bandits, nonparametric, Lipschitz
</p><p><b>Compressor summary</b>: The paper studies nonparametric contextual bandits with changing reward functions and proposes a new concept of change that accounts for locality and significance, achieving adaptivity in the minimax regret rate.</p><hr><h3>Collaborative Learning via Prediction Consensus</h3>
<p>Dongyang Fan, Celestine Mendler-Dünner, Martin Jaggi</p>
<p><a href='https://openreview.net/forum?id=SGKbHXoLCI'>https://openreview.net/forum?id=SGKbHXoLCI</a></p>
<p><b>Keywords</b>: Collaborative training, decentralized learning, consensus reaching
</p><p><b>Compressor summary</b>: Our method helps agents improve their models by sharing unlabeled data and using trust weights to reach consensus on pseudo-labels, which enhances performance and reduces bad influence.</p><hr><h3>Graph Convolutional Kernel Machine versus Graph Convolutional Networks</h3>
<p>Zhihao Wu, Zhao Zhang, Jicong Fan</p>
<p><a href='https://openreview.net/forum?id=SFfOt1oDsX'>https://openreview.net/forum?id=SFfOt1oDsX</a></p>
<p><b>Keywords</b>: graph neural network, kernel method
</p><p><b>Compressor summary</b>: Graph convolutional kernel machine (GCKM) is a framework for graph-based machine learning that uses kernel functions with graph convolution and has advantages over deep graph convolutional networks (GCN).</p><hr><h3>Nearly Optimal VC-Dimension and Pseudo-Dimension Bounds for Deep Neural Network Derivatives</h3>
<p>Yahong Yang, Haizhao Yang, Yang Xiang</p>
<p><a href='https://openreview.net/forum?id=SE73LzWNjr'>https://openreview.net/forum?id=SE73LzWNjr</a></p>
<p><b>Keywords</b>: VC-dimension, pseudo-dimension, Sobolev space, generalization error, nearly optimal approximation
</p><p><b>Compressor summary</b>: The paper studies how to accurately estimate the complexity of derivative functions in deep neural networks and applies these estimations to various physics-informed machine learning tasks.</p><hr><h3>Structure Learning with Adaptive Random Neighborhood Informed MCMC</h3>
<p>Xitong Liang, Alberto Caron, Samuel Livingstone, Jim Griffin</p>
<p><a href='https://openreview.net/forum?id=SCsJFNcSHQ'>https://openreview.net/forum?id=SCsJFNcSHQ</a></p>
<p><b>Keywords</b>: Bayesian Networks, structure MCMC on graphs, Structure Learning, Random neighborhood samplers, Locally informed Metropolis-Hastings schemes
</p><p><b>Compressor summary</b>: PARNI-DAG is a new MCMC sampler that efficiently learns Directed Acyclic Graphs (DAGs) from observational data under causal sufficiency by using adaptive random neighborhood proposals and pre-tuning parameters based on a skeleton graph.</p><hr><h3>Towards Self-Interpretable Graph-Level Anomaly Detection</h3>
<p>Yixin Liu, Kaize Ding, Qinghua Lu, Fuyi Li, Leo Yu Zhang, Shirui Pan</p>
<p><a href='https://openreview.net/forum?id=SAzaC8f3cM'>https://openreview.net/forum?id=SAzaC8f3cM</a></p>
<p><b>Keywords</b>: Anomaly Detection, Graph Neural Networks, Explanation, Self-Interpretation
</p><p><b>Compressor summary</b>: SIGNET is a model that detects abnormal graphs and explains why they are abnormal using subgraphs from the input graph and its dual hypergraph.</p><hr><h3>Category-Extensible Out-of-Distribution Detection via Hierarchical Context Descriptions</h3>
<p>Kai Liu, Zhihang Fu, Chao Chen, Sheng Jin, Ze Chen, Mingyuan Tao, Rongxin Jiang, Jieping Ye</p>
<p><a href='https://openreview.net/forum?id=SA2KrosYjY'>https://openreview.net/forum?id=SA2KrosYjY</a></p>
<p><b>Keywords</b>: out-of-distribution detection, vision-language models, category-extendable classification
</p><p><b>Compressor summary</b>: This paper introduces two contexts, perceptual and spurious, for detecting out-of-distribution samples in vision-language models using automatic prompt tuning and applies them to a novel detection method called CATEX.</p><hr><h3>$p$-Poisson surface reconstruction in curl-free flow from point clouds</h3>
<p>Yesom Park, Taekyung Lee, Jooyoung Hahn, Myungjoo Kang</p>
<p><a href='https://openreview.net/forum?id=S8hg5LpFvz'>https://openreview.net/forum?id=S8hg5LpFvz</a></p>
<p><b>Keywords</b>: Surface reconstruction, Signed distance function, Implicit neural representations, Point cloud
</p><p><b>Compressor summary</b>: The paper proposes a new surface reconstruction method using implicit neural representations, which learns a signed distance function from a point cloud and imposes physical constraints for better results.</p><hr><h3>Learning Nonparametric Latent Causal Graphs with Unknown Interventions</h3>
<p>Yibo Jiang, Bryon Aragam</p>
<p><a href='https://openreview.net/forum?id=S8DFqgmEbe'>https://openreview.net/forum?id=S8DFqgmEbe</a></p>
<p><b>Keywords</b>: graphical models, directed acyclic graphs, causality, identifiability, causal representation learning, unknown interventions
</p><p><b>Compressor summary</b>: The paper presents methods for identifying latent causal graphs from interventions without making parametric assumptions or knowing the number of hidden variables, introducing new graphical concepts and characterizing the limits of edge orientations in a general setting.</p><hr><h3>Posterior Sampling for Competitive RL: Function Approximation and Partial Observation</h3>
<p>Shuang Qiu, Ziyu Dai, Han Zhong, Zhaoran Wang, Zhuoran Yang, Tong Zhang</p>
<p><a href='https://openreview.net/forum?id=S75ccNdOYG'>https://openreview.net/forum?id=S75ccNdOYG</a></p>
<p><b>Keywords</b>: Markov game, Partial observation, Function approximation, Posterior sampling, Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper presents new algorithms for competitive reinforcement learning in different game settings, using complexity measures and posterior sampling methods that handle partial observability and trade-off exploration and exploitation.</p><hr><h3>A3FL: Adversarially Adaptive Backdoor Attacks to Federated Learning</h3>
<p>Hangfan Zhang, Jinyuan Jia, Jinghui Chen, Lu Lin, Dinghao Wu</p>
<p><a href='https://openreview.net/forum?id=S6ajVZy6FA'>https://openreview.net/forum?id=S6ajVZy6FA</a></p>
<p><b>Keywords</b>: Backdoor Attack, Federated Learning
</p><p><b>Compressor summary</b>: The paper introduces a new backdoor attack, A3FL, that adapts the trigger to survive global training dynamics in federated learning.</p><hr><h3>The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks</h3>
<p>Ziqian Zhong, Ziming Liu, Max Tegmark, Jacob Andreas</p>
<p><a href='https://openreview.net/forum?id=S5wmbQc1We'>https://openreview.net/forum?id=S5wmbQc1We</a></p>
<p><b>Keywords</b>: mechanistic interpretability, algorithmic phase transitions, arithmetic learning, neural network, transformer, ensemble
</p><p><b>Compressor summary</b>: The study shows that neural networks can learn different algorithms, including some novel ones, when trained on modular addition tasks, indicating the complexity of algorithm discovery in neural networks.</p><hr><h3>Efficient Equivariant Transfer Learning from Pretrained Models</h3>
<p>Sourya Basu, Pulkit Katdare, Prasanna Sattigeri, Vijil Chenthamarakshan, Katherine Rose Driggs-Campbell, Payel Das, Lav R. Varshney</p>
<p><a href='https://openreview.net/forum?id=S4NN3OOiwP'>https://openreview.net/forum?id=S4NN3OOiwP</a></p>
<p><b>Keywords</b>: zero-shot learning, equivariant machine learning, equivariant fine-tuning, pretrained models
</p><p><b>Compressor summary</b>: λ-equitune is a method to improve equivariant outputs from non-equivariant neural networks by averaging features with importance weights learned from data, leading to better zero-shot and finetuned results than equitune on diverse applications and models.</p><hr><h3>The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning</h3>
<p>Kaiwen Wang, Kevin Zhou, Runzhe Wu, Nathan Kallus, Wen Sun</p>
<p><a href='https://openreview.net/forum?id=S3Y0VvegGm'>https://openreview.net/forum?id=S3Y0VvegGm</a></p>
<p><b>Keywords</b>: Reinforcement Learning Theory, Distributional Reinforcement Learning, Small-Loss Bounds, First-order regret
</p><p><b>Compressor summary</b>: The paper explains why distributional reinforcement learning (DistRL) is better than non-distributional RL by using small-loss bounds, proposes a DistCB algorithm with empirical results, and shows that pessimistic DistRL has novel small-loss PAC bounds in offline RL.</p><hr><h3>Self-Refine: Iterative Refinement with Self-Feedback</h3>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark</p>
<p><a href='https://openreview.net/forum?id=S37hOerQLB'>https://openreview.net/forum?id=S37hOerQLB</a></p>
<p><b>Keywords</b>: LLMs, Iterative Refinement, Feedback-driven Generation
</p><p><b>Compressor summary</b>: The Self-Refine approach improves the output of large language models by iteratively refining their own outputs based on feedback, without needing additional data or training.</p><hr><h3>Transportability for Bandits with Data from Different Environments</h3>
<p>Alexis Bellot, Alan Malek, Silvia Chiappa</p>
<p><a href='https://openreview.net/forum?id=S2k5dBb91q'>https://openreview.net/forum?id=S2k5dBb91q</a></p>
<p><b>Keywords</b>: Transportability, transfer learning, bandits
</p><p><b>Compressor summary</b>: The paper proposes a new bandit algorithm that uses causal models to learn from batch data and exploit invariances across related environments, leading to lower regret and faster learning.</p><hr><h3>ClusterFomer: Clustering As A Universal Visual Learner</h3>
<p>James Chenhao Liang, Yiming Cui, Qifan Wang, Tong Geng, Wenguan Wang, Dongfang Liu</p>
<p><a href='https://openreview.net/forum?id=S1KGaTSOTS'>https://openreview.net/forum?id=S1KGaTSOTS</a></p>
<p><b>Keywords</b>: Universal Model, Clustering
</p><p><b>Compressor summary</b>: ClusterFormer is a novel vision model that uses clustering with TransFormers, enabling explainable and transferable image processing for various tasks and outperforming specialized architectures.</p><hr><h3>Dense-Exponential Random Features: Sharp Positive Estimators of the Gaussian Kernel</h3>
<p>Valerii Likhosherstov, Krzysztof Marcin Choromanski, Kumar Avinava Dubey, Frederick Liu, Tamas Sarlos, Adrian Weller</p>
<p><a href='https://openreview.net/forum?id=S0xrBMFihS'>https://openreview.net/forum?id=S0xrBMFihS</a></p>
<p><b>Keywords</b>: Gaussian kernel, softmax kernel
</p><p><b>Compressor summary</b>: The authors propose new parameterized random features to approximate Gaussian and softmax kernels, which reduce variance and improve performance in kernel methods and Transformers.</p><hr><h3>SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks</h3>
<p>Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, Xiang Ren</p>
<p><a href='https://openreview.net/forum?id=Rzk3GP1HN7'>https://openreview.net/forum?id=Rzk3GP1HN7</a></p>
<p><b>Keywords</b>: interactive reasoning, text game, agents, action planning, large language models
</p><p><b>Compressor summary</b>: SwiftSage is a new agent framework that combines behavior cloning and large language models to improve action planning for complex reasoning tasks by using two modules: one for fast thinking and another for deliberate thought.</p><hr><h3>Similarity, Compression and Local Steps: Three Pillars of Efficient Communications for Distributed Variational Inequalities</h3>
<p>Aleksandr Beznosikov, Martin Takáč, Alexander Gasnikov</p>
<p><a href='https://openreview.net/forum?id=Rvk1wdwz1L'>https://openreview.net/forum?id=Rvk1wdwz1L</a></p>
<p><b>Keywords</b>: convex optimization, variational inequalities, similarity, local methods, compression, partial participation
</p><p><b>Compressor summary</b>: The paper proposes a novel triple synergy technique to reduce communication costs for solving large-scale variational inequality problems in machine learning, achieving the best theoretical guarantees and outperforming existing methods.</p><hr><h3>Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation</h3>
<p>Jianing Zhu, Geng Yu, Jiangchao Yao, Tongliang Liu, Gang Niu, Masashi Sugiyama, Bo Han</p>
<p><a href='https://openreview.net/forum?id=RuxBLfiEqI'>https://openreview.net/forum?id=RuxBLfiEqI</a></p>
<p><b>Keywords</b>: out-of-distribution detection, outlier exposure
</p><p><b>Compressor summary</b>: DivOE is a novel framework for OOD detection that diversifies auxiliary outliers and extrapolates them during training using a multi-step optimization method.</p><hr><h3>Dynamic Personalized Federated Learning with Adaptive Differential Privacy</h3>
<p>Xiyuan Yang, Wenke Huang, Mang Ye</p>
<p><a href='https://openreview.net/forum?id=RteNLuc8D9'>https://openreview.net/forum?id=RteNLuc8D9</a></p>
<p><b>Keywords</b>: federated learning, differential privacy, personalization
</p><p><b>Compressor summary</b>: The paper proposes FedDPA, a novel federated learning method that uses layer-wise Fisher information to flexibly personalize models while mitigating convergence difficulties caused by clipping operations in differential privacy.</p><hr><h3>A Partially-Supervised Reinforcement Learning Framework for Visual Active Search</h3>
<p>Anindya Sarkar, Nathan Jacobs, Yevgeniy Vorobeychik</p>
<p><a href='https://openreview.net/forum?id=Rs6pzz21U4'>https://openreview.net/forum?id=Rs6pzz21U4</a></p>
<p><b>Keywords</b>: Visual Active Search, Reinforcement Learning
</p><p><b>Compressor summary</b>: Visual active search (VAS) has been proposed as a  modeling framework in which visual cues are used to guide exploration, with the goal of identifying regions of interest in a large geospatial area. Its potential applications include identifying hot spots of rare wildlife poaching activity, search-and-rescue scenarios, identifying illegal trafficking of weapons, drugs, or people, and many others. State of the art approaches to VAS include applications of deep reinforcement learning (DRL), which yield end-to-end search policies, and traditional active search, which combines predictions with custom algorithmic approaches. While the DRL framework has been shown to greatly outperform traditional active search in such domains, its end-to-end nature does not make full use of supervised information attained either during training, or during actual search, a significant limitation if search tasks differ significantly from those in the training distribution. We propose an approach that combines the strength of both DRL and conventional active search approaches by decomposing the search policy into a prediction module, which produces a geospatial distribution of regions of interest based on task embedding and search history, and a search module, which takes the predictions and search history as input and outputs the search distribution. In addition, we develop a novel meta-learning approach for jointly learning the resulting combined policy that can make effective use of supervised information obtained both at training and decision time. Our extensive experiments demonstrate that the proposed representation and meta-learning frameworks significantly outperform state of the art in visual active search on several problem domains.</p><hr><h3>Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering</h3>
<p>Yijun Dong, Kevin Miller, Qi Lei, Rachel Ward</p>
<p><a href='https://openreview.net/forum?id=RrdBNXBUIF'>https://openreview.net/forum?id=RrdBNXBUIF</a></p>
<p><b>Keywords</b>: Relational knowledge distillation, Semi-supervised learning, Spectral clustering, Sample complexity
</p><p><b>Compressor summary</b>: The paper provides a theoretical understanding and analysis of relational knowledge distillation in semi-supervised classification problems using spectral clustering and cluster-aware frameworks.</p><hr><h3>Spectral Co-Distillation for Personalized Federated Learning</h3>
<p>Zihan Chen, Howard Hao Yang, Tony Quek, Kai Fong Ernest Chong</p>
<p><a href='https://openreview.net/forum?id=RqjQL08UFc'>https://openreview.net/forum?id=RqjQL08UFc</a></p>
<p><b>Keywords</b>: Personalized federated learning, spectral bias, co-distillation, communication efficiency
</p><p><b>Compressor summary</b>: The text introduces spectral distillation, a new personalized federated learning method that leverages model spectrum information to improve performance and efficiency on diverse data settings.</p><hr><h3>Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective</h3>
<p>Pengfei Wei, Lingdong Kong, Xinghua Qu, Yi Ren, zhiqiang xu, Jing Jiang, Xiang Yin</p>
<p><a href='https://openreview.net/forum?id=Rp4PA0ez0m'>https://openreview.net/forum?id=Rp4PA0ez0m</a></p>
<p><b>Keywords</b>: action recognition, unsupervised domain adaptation, video analysis
</p><p><b>Compressor summary</b>: This paper introduces a new method called TranSVAE to generate videos across different domains by disentangling static and dynamic information and removing spatial and temporal domain differences using constraints and adversarial learning.</p><hr><h3>Agnostic Multi-Group Active Learning</h3>
<p>Nicholas Rittler, Kamalika Chaudhuri</p>
<p><a href='https://openreview.net/forum?id=RmxP5ZcQhC'>https://openreview.net/forum?id=RmxP5ZcQhC</a></p>
<p><b>Keywords</b>: learning theory, active learning, multi-group learning
</p><p><b>Compressor summary</b>: The paper presents a novel active learning algorithm for learning from multiple groups with minimal label queries, and shows its advantages in some scenarios, as well as providing results for special cases of realizable groups and approximations for the general case.</p><hr><h3>H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</h3>
<p>Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, Beidi Chen</p>
<p><a href='https://openreview.net/forum?id=RkRrPp7GKO'>https://openreview.net/forum?id=RkRrPp7GKO</a></p>
<p><b>Keywords</b>: Large Language Models; Efficient Generative Inference
</p><p><b>Compressor summary</b>: The paper proposes a new memory management technique for large language models that reduces their cost by focusing on the most important tokens and evicting less relevant ones.</p><hr><h3>Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation</h3>
<p>Nikki Lijing Kuang, Ming Yin, Mengdi Wang, Yu-Xiang Wang, Yian Ma</p>
<p><a href='https://openreview.net/forum?id=RiyH3z7oIF'>https://openreview.net/forum?id=RiyH3z7oIF</a></p>
<p><b>Keywords</b>: Posterior Sampling, Reinforcement Learning Theory, Linear Markov Decision Processes, Delayed Feedback, Langevin Monte Carlo
</p><p><b>Compressor summary</b>: This paper proposes two optimistic value-based algorithms for reinforcement learning with delayed feedback, and analyzes their performance in terms of regret.</p><hr><h3>Active representation learning for general task space with applications in robotics</h3>
<p>Yifang Chen, Yingbing Huang, Simon Shaolei Du, Kevin Jamieson, Guanya Shi</p>
<p><a href='https://openreview.net/forum?id=RiwPYAMLur'>https://openreview.net/forum?id=RiwPYAMLur</a></p>
<p><b>Keywords</b>: active learning, representation learning, robotics, theory
</p><p><b>Compressor summary</b>: The paper proposes a general framework for active representation learning that optimally chooses source tasks to sample from and shows its effectiveness in various domains with theoretical and empirical results.</p><hr><h3>SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization</h3>
<p>Hao Dong, Ismail Nejjar, Han Sun, Eleni Chatzi, Olga Fink</p>
<p><a href='https://openreview.net/forum?id=RiSMijlsLT'>https://openreview.net/forum?id=RiSMijlsLT</a></p>
<p><b>Keywords</b>: Domain Generalization, Multi-modal Learning, Distribution Shift, Out-of-distribution Generalization
</p><p><b>Compressor summary</b>: SimMMDG is a multi-modal domain generalization framework that splits features into modality-specific and modality-shared components, uses supervised contrastive learning to encourage joint properties, imposes distance constraints for diversity, and employs cross-modal translation for regularization and missing-modality generalization.</p><hr><h3>Feature Selection in the Contrastive Analysis Setting</h3>
<p>Ethan Weinberger, Ian Connick Covert, Su-In Lee</p>
<p><a href='https://openreview.net/forum?id=RhE01dqo8u'>https://openreview.net/forum?id=RhE01dqo8u</a></p>
<p><b>Keywords</b>: Feature selection, contrastive analysis, computational biology, representation learning, information theory
</p><p><b>Compressor summary</b>: The paper introduces contrastive feature selection (CFS), a method for selecting features in the contrastive analysis setting, and shows that it outperforms existing methods on biomedical datasets.</p><hr><h3>Reusing Pretrained Models by Multi-linear Operators for Efficient Training</h3>
<p>Yu Pan, Ye Yuan, Yichun Yin, Zenglin Xu, Lifeng Shang, Xin Jiang, Qun Liu</p>
<p><a href='https://openreview.net/forum?id=RgNXKIrWyU'>https://openreview.net/forum?id=RgNXKIrWyU</a></p>
<p><b>Keywords</b>: Model Growth, Efficient Training, Pretrained Model, Multi-linearity
</p><p><b>Compressor summary</b>: The paper proposes a method to accelerate training large models by linearly correlating each weight of the target model with all weights of the pretrained model, reducing computational and spatial complexity.</p><hr><h3>DeepPCR: Parallelizing Sequential Operations in Neural Networks</h3>
<p>Federico Danieli, Miguel Sarabia, Xavier Suau, Pau Rodriguez, Luca Zappella</p>
<p><a href='https://openreview.net/forum?id=RgD92idA32'>https://openreview.net/forum?id=RgD92idA32</a></p>
<p><b>Keywords</b>: Acceleration, layer-parallelization, diffusion, Parallel Cyclic Reduction
</p><p><b>Compressor summary</b>: DeepPCR is a novel algorithm that parallelizes typically sequential operations in deep neural networks, such as forward and backward passes, to achieve significant speedups.</p><hr><h3>Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions</h3>
<p>Ruihai Wu, Kai Cheng, Yan Zhao, Chuanruo Ning, Guanqi Zhan, Hao Dong</p>
<p><a href='https://openreview.net/forum?id=Re2NHYoZ5l'>https://openreview.net/forum?id=Re2NHYoZ5l</a></p>
<p><b>Keywords</b>: Visual Affordance for Robotics, Articulated Object Manipulation, Occlusion Handling
</p><p><b>Compressor summary</b>: The paper proposes an environment-aware affordance framework for home-assistant robots that learns from object-level actionable priors and various occlusions using a contrastive learning approach.</p><hr><h3>GenS: Generalizable Neural Surface Reconstruction from Multi-View Images</h3>
<p>Rui Peng, Xiaodong Gu, Luyang Tang, Shihe Shen, Fanqi Yu, Ronggang Wang</p>
<p><a href='https://openreview.net/forum?id=Rcit6V3vus'>https://openreview.net/forum?id=Rcit6V3vus</a></p>
<p><b>Keywords</b>: Generalizable Neural Surface, Volume Rendering, Signed Distance Function
</p><p><b>Compressor summary</b>: The paper introduces GenS, a neural surface reconstruction model that generalizes well to new scenes and outperforms existing methods without 3D supervision or long-time optimizations.</p><hr><h3>Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder</h3>
<p>Huiwon Jang, Jihoon Tack, Daewon Choi, Jongheon Jeong, Jinwoo Shin</p>
<p><a href='https://openreview.net/forum?id=RZGtK2nDDJ'>https://openreview.net/forum?id=RZGtK2nDDJ</a></p>
<p><b>Keywords</b>: Self-Supervised Learning, Modality-Agnostic Self-Supervised Learning, Meta-Learning, Masked Auto-Encoder
</p><p><b>Compressor summary</b>: The paper proposes MetaMAE, a meta-learning enhanced version of Masked Auto-Encoder, to improve self-supervised learning across diverse modalities.</p><hr><h3>BanditPAM++: Faster $k$-medoids Clustering</h3>
<p>Mo Tiwari, Ryan Kang, Donghyun Lee, Sebastian Thrun, Ilan Shomorony, Martin Jinye Zhang</p>
<p><a href='https://openreview.net/forum?id=RWcfpmjlYm'>https://openreview.net/forum?id=RWcfpmjlYm</a></p>
<p><b>Keywords</b>: multi-armed bandits, clustering, k-medoids, best-arm identification
</p><p><b>Compressor summary</b>: BanditPAM++ is a faster and improved version of BanditPAM, which is a randomized $k$-medoids algorithm with state-of-the-art complexity and accuracy.</p><hr><h3>Federated Spectral Clustering via Secure Similarity Reconstruction</h3>
<p>Dong Qiao, Chris Ding, Jicong Fan</p>
<p><a href='https://openreview.net/forum?id=RW7rZ8Y3Bp'>https://openreview.net/forum?id=RW7rZ8Y3Bp</a></p>
<p><b>Keywords</b>: clustering, federated learning, privacy
</p><p><b>Compressor summary</b>: Federated learning has a significant advantage in protecting information privacy. Many scholars proposed various secure learning methods within the framework of federated learning but the study on secure federated unsupervised learning especially clustering is limited. We in this work propose a secure kernelized factorization method for federated spectral clustering on distributed dataset. The method is non-trivial because the kernel or similarity matrix for spectral clustering is computed by data pairs, which violates the principle of privacy protection. Our method implicitly constructs an approximation for the kernel matrix on distributed data such that we can perform spectral clustering under the constraint of privacy protection. We provide a convergence guarantee of the optimization algorithm, reconstruction error bounds of the Gaussian kernel matrix, and the sufficient condition of correct clustering of our method. We also present some results of differential privacy. Numerical results on synthetic and real datasets demonstrate that the proposed method is efficient and accurate in comparison to the baselines.</p><hr><h3>Promises and Pitfalls of Threshold-based Auto-labeling</h3>
<p>Harit Vishwakarma, Heguang Lin, Frederic Sala, Ramya Korlakai Vinayak</p>
<p><a href='https://openreview.net/forum?id=RUCFAKNDb2'>https://openreview.net/forum?id=RUCFAKNDb2</a></p>
<p><b>Keywords</b>: Auto Labeling, Active Learning, Selective Classification
</p><p><b>Compressor summary</b>: The paragraph discusses threshold-based auto-labeling (TBAL), a technique to reduce manual annotation in machine learning, and analyzes its sample complexity, quality, and potential pitfalls using theoretical and empirical methods.</p><hr><h3>ANPL: Towards Natural Programming with Interactive Decomposition</h3>
<p>Di Huang, Ziyuan Nan, Xing Hu, Pengwei Jin, Shaohui Peng, Yuanbo Wen, Rui Zhang, Zidong Du, Qi Guo, Yewen Pu, Yunji Chen</p>
<p><a href='https://openreview.net/forum?id=RTRS3ZTsSj'>https://openreview.net/forum?id=RTRS3ZTsSj</a></p>
<p><b>Keywords</b>: programming language, large language models, program synthesis, code generation, human-ai interaction
</p><p><b>Compressor summary</b>: ANPL is an interactive programming system that allows users to refine code generated by LLMs using structured decompositions and natural language specifications.</p><hr><h3>How to Turn Your Knowledge Graph Embeddings into Generative Models</h3>
<p>Lorenzo Loconte, Nicola Di Mauro, Robert Peharz, Antonio Vergari</p>
<p><a href='https://openreview.net/forum?id=RSGNGiB1q4'>https://openreview.net/forum?id=RSGNGiB1q4</a></p>
<p><b>Keywords</b>: knowledge graph, knowledge graph embeddings, probabilistic circuits, probabilistic reasoning, tractable inference
</p><p><b>Compressor summary</b>: The authors propose generative circuit models for knowledge graph embedding that enable exact maximum-likelihood estimation, efficient sampling, and logical constraint satisfaction while maintaining performance for link prediction.</p><hr><h3>Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts</h3>
<p>Zeyang Zhang, Xin Wang, Ziwei Zhang, Zhou Qin, Weigao Wen, Hui Xue', Haoyang Li, Wenwu Zhu</p>
<p><a href='https://openreview.net/forum?id=RRUVZygUtr'>https://openreview.net/forum?id=RRUVZygUtr</a></p>
<p><b>Keywords</b>: Dynamic Graph Neural Networks, Out-of-Distribution Generalization
</p><p><b>Compressor summary</b>: The paper proposes a new method, SILD, to handle distribution shifts in dynamic graphs by capturing and utilizing invariant and variant spectral patterns.</p><hr><h3>IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based  Generative AI</h3>
<p>Bochuan Cao, Changjiang Li, Ting Wang, Jinyuan Jia, Bo Li, Jinghui Chen</p>
<p><a href='https://openreview.net/forum?id=RRSltzPc7w'>https://openreview.net/forum?id=RRSltzPc7w</a></p>
<p><b>Keywords</b>: Image Generation Godels, Latent Diffusion Models, Image Purifying
</p><p><b>Compressor summary</b>: IMPRESS is a platform that evaluates the effectiveness of imperceptible perturbations in protecting original images from unauthorized data usage by diffusion-based image generation models.</p><hr><h3>On Imitation in Mean-field Games</h3>
<p>Giorgia Ramponi, Pavel Kolev, Olivier Pietquin, Niao He, Mathieu Lauriere, Matthieu Geist</p>
<p><a href='https://openreview.net/forum?id=RPFd3D3P3L'>https://openreview.net/forum?id=RPFd3D3P3L</a></p>
<p><b>Keywords</b>: Mean-field games, Imitation Learning
</p><p><b>Compressor summary</b>: The paper studies imitation learning in mean-field games and introduces a new solution concept called the Nash imitation gap, showing that it is harder than single-agent imitation learning when the dynamics depend on the population distribution.</p><hr><h3>Towards Consistent Video Editing with Text-to-Image Diffusion Models</h3>
<p>Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han, Tiande Guo, Luoqi Liu</p>
<p><a href='https://openreview.net/forum?id=RNVwm4BzXO'>https://openreview.net/forum?id=RNVwm4BzXO</a></p>
<p><b>Keywords</b>: diffusion model, video editing, text-to-video diffusion model
</p><p><b>Compressor summary</b>: The paper proposes a new EI$^2$ model to improve Text-to-Image diffusion models for video editing by addressing inconsistency issues caused by modules learning temporal information and using two attention mechanisms to enhance spatial and temporal features.</p><hr><h3>Elastic Decision Transformer</h3>
<p>Yueh-Hua Wu, Xiaolong Wang, Masashi Hamaya</p>
<p><a href='https://openreview.net/forum?id=RMeQjexaRj'>https://openreview.net/forum?id=RMeQjexaRj</a></p>
<p><b>Keywords</b>: Offline Reinforcement Learning, Trajectory Stitching, Decision Transformer
</p><p><b>Compressor summary</b>: The Elastic Decision Transformer (EDT) improves upon Decision Transformer (DT) by better handling trajectory stitching and adapting history length for optimal performance, leading to competitive results with Q Learning-based methods.</p><hr><h3>Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning</h3>
<p>Xiaoqian Wu, Yong-Lu Li, Jianhua Sun, Cewu Lu</p>
<p><a href='https://openreview.net/forum?id=RJq9bVEf6N'>https://openreview.net/forum?id=RJq9bVEf6N</a></p>
<p><b>Keywords</b>: neuro-symbolic, visual reasoning, human activity understanding
</p><p><b>Compressor summary</b>: The paragraph discusses a new symbolic system for visual activity understanding that uses large language models to improve explainability, generalization, and data efficiency.</p><hr><h3>Cheaply Estimating Inference Efficiency Metrics for Autoregressive Transformer Models</h3>
<p>Deepak Narayanan, Keshav Santhanam, Peter Henderson, Rishi Bommasani, Tony Lee, Percy Liang</p>
<p><a href='https://openreview.net/forum?id=RJpAz15D0S'>https://openreview.net/forum?id=RJpAz15D0S</a></p>
<p><b>Keywords</b>: Systems for Machine Learning, Inference efficiency, Transformer models, Text generation APIs, Capability-efficiency tradeoffs
</p><p><b>Compressor summary</b>: The authors propose idealized runtime and cost models to measure inference efficiency for large language models, and compare ten LLMs from 2022 using these metrics.</p><hr><h3>Relax, it doesn’t matter how you get there: A new self-supervised approach for multi-timescale behavior analysis</h3>
<p>Mehdi Azabou, Michael Jacob Mendelson, Nauman Ahad, Maks Sorokin, Shantanu Thakoor, Carolina Urzay, Eva L Dyer</p>
<p><a href='https://openreview.net/forum?id=RInTOCEL3l'>https://openreview.net/forum?id=RInTOCEL3l</a></p>
<p><b>Keywords</b>: animal behavior, behavioral neuroscience, self-supervised learning, multi-timescale
</p><p><b>Compressor summary</b>: The paper presents a multi-task representation learning model for animal behavior that predicts actions over future timesteps and incorporates short- and long-term dynamics, achieving success in various environments and ranking first in the MABe 2022 Multi-Agent Behavior challenge.</p><hr><h3>Learning Rule-Induced Subgraph Representations for Inductive Relation Prediction</h3>
<p>Tianyu Liu, Qitan Lv, Jie Wang, Shuling Yang, Hanzhu Chen</p>
<p><a href='https://openreview.net/forum?id=RHDXkRPNQa'>https://openreview.net/forum?id=RHDXkRPNQa</a></p>
<p><b>Keywords</b>: inductive relation prediction, knowledge graph completion, knowledge graph reasoning
</p><p><b>Compressor summary</b>: REST is a novel GNN model for inductive relation prediction that initializes edge features only for the target link and uses RNN-based functions for edge-wise message passing to learn rule-induced subgraph representations.</p><hr><h3>On-the-Fly Adapting Code Summarization on Trainable Cost-Effective Language Models</h3>
<p>Yufan Cai, Yun Lin, Chenyan Liu, Jinglian Wu, Yifan Zhang, Yiming Liu, Yeyun Gong, Jin Song Dong</p>
<p><a href='https://openreview.net/forum?id=RFgv7cfMUy'>https://openreview.net/forum?id=RFgv7cfMUy</a></p>
<p><b>Keywords</b>: Code Summarization, Adaptation, Language Model
</p><p><b>Compressor summary</b>: Adacom is a novel approach to improve code comment generation by adapting deep learning models on the fly using contradictory training samples.</p><hr><h3>Public Opinion Field Effect Fusion in Representation Learning for Trending Topics Diffusion</h3>
<p>Junliang Li, Yajun Yang, Qinghua Hu, Xin Wang, Hong Gao</p>
<p><a href='https://openreview.net/forum?id=RFE1eI0zNZ'>https://openreview.net/forum?id=RFE1eI0zNZ</a></p>
<p><b>Keywords</b>: public opinion field effect, heterogeneous networks, representation learning, trending topic diffusion
</p><p><b>Compressor summary</b>: The paper proposes a new representation learning framework that considers public opinion field and social circle influence effects for trending topic diffusion analysis.</p><hr><h3>Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization</h3>
<p>Fu Luo, Xi Lin, Fei Liu, Qingfu Zhang, Zhenkun Wang</p>
<p><a href='https://openreview.net/forum?id=RBI4oAbdpm'>https://openreview.net/forum?id=RBI4oAbdpm</a></p>
<p><b>Keywords</b>: Neural Combinatorial Optimization, Generalization, Large scale problem, Heavy decoder
</p><p><b>Compressor summary</b>: The LEHD model is a novel neural combinatorial optimization method that can learn to generalize across problem sizes and scales, enabling it to solve large-scale real-world problems more effectively than previous methods.</p><hr><h3>Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions</h3>
<p>Tongxin Li, Yiheng Lin, Shaolei Ren, Adam Wierman</p>
<p><a href='https://openreview.net/forum?id=RACcp8Zbr9'>https://openreview.net/forum?id=RACcp8Zbr9</a></p>
<p><b>Keywords</b>: Time-varying MDP, Learning-augmented online algorithm, consistency and robustness tradeoff
</p><p><b>Compressor summary</b>: The paper investigates how using information about the advice generation process affects the tradeoff between consistency and robustness in single-trajectory time-varying MDPs with machine-learned Q-value advice, showing that it can lead to near-optimal performance.</p><hr><h3>Segment Anything in High Quality</h3>
<p>Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan liu, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu</p>
<p><a href='https://openreview.net/forum?id=RA7ND878XP'>https://openreview.net/forum?id=RA7ND878XP</a></p>
<p><b>Keywords</b>: segment anything, zero-shot segmentation, high-quality segmentation
</p><p><b>Compressor summary</b>: The authors propose HQ-SAM, an improved version of the Segment Anything Model (SAM) that can accurately segment any object with minimal additional parameters and computation, while maintaining SAM's efficiency and zero-shot generalizability.</p><hr><h3>PROTES: Probabilistic Optimization with Tensor Sampling</h3>
<p>Anastasia Batsheva, Andrei Chertkov, Gleb Ryzhakov, Ivan Oseledets</p>
<p><a href='https://openreview.net/forum?id=R9R7YDOar1'>https://openreview.net/forum?id=R9R7YDOar1</a></p>
<p><b>Keywords</b>: Tensor Train, Black Box Optimization, Sampling, Optimal Control
</p><p><b>Compressor summary</b>: PROTES is a novel black-box optimization method that uses probabilistic sampling from a low-parametric tensor train format and performs better than popular discrete methods on complex problems.</p><hr><h3>Projection-Free Methods for Stochastic Simple Bilevel Optimization with Convex Lower-level Problem</h3>
<p>Jincheng Cao, Ruichen Jiang, Nazanin Abolfazli, Erfan Yazdandoost Hamedani, Aryan Mokhtari</p>
<p><a href='https://openreview.net/forum?id=R8GF0EsNsI'>https://openreview.net/forum?id=R8GF0EsNsI</a></p>
<p><b>Keywords</b>: Bilevel optimization, stochastic optimization
</p><p><b>Compressor summary</b>: The paper proposes new methods for solving stochastic bilevel optimization problems that improve the complexity of existing methods by using stochastic cutting planes and conditional gradient updates with variance reduction.</p><hr><h3>The Utility of “Even if” Semifactual Explanation to Optimise Positive Outcomes</h3>
<p>Eoin M. Kenny, Weipeng Fuzzy Huang</p>
<p><a href='https://openreview.net/forum?id=R6wXP7txer'>https://openreview.net/forum?id=R6wXP7txer</a></p>
<p><b>Keywords</b>: Semifactual Explanation, Counterfactual Explanation, Explainable AI, Recourse, User Study
</p><p><b>Compressor summary</b>: The paper introduces semifactuals, a new type of XAI that optimizes positive outcomes without crossing decision boundaries, and shows they are more useful than counterfactuals for users who receive positive outcomes like loan approvals.</p><hr><h3>Dynamo-Depth: Fixing Unsupervised Depth Estimation for Dynamical Scenes</h3>
<p>Yihong Sun, Bharath Hariharan</p>
<p><a href='https://openreview.net/forum?id=R6qMmdl4qP'>https://openreview.net/forum?id=R6qMmdl4qP</a></p>
<p><b>Keywords</b>: monocular, depth estimation, dynamical scenes, motion segmentation, self-supervised
</p><p><b>Compressor summary</b>: Dynamo-Depth is a method that estimates depth, motion, and segmentation from unlabeled videos by first separating moving objects from static scenes.</p><hr><h3>Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation</h3>
<p>Sebastien Lachapelle, Divyat Mahajan, Ioannis Mitliagkas, Simon Lacoste-Julien</p>
<p><a href='https://openreview.net/forum?id=R6KJN1AUAR'>https://openreview.net/forum?id=R6KJN1AUAR</a></p>
<p><b>Keywords</b>: identifiability, nonlinear ICA, causal representation learning, disentanglement, object-centric representation learning, extrapolation
</p><p><b>Compressor summary</b>: The paragraph discusses how additive decoders can identify latent variables and generate "out-of-support" images for representation learning, with theoretical guarantees and applications in nonlinear ICA and OCRL methods.</p><hr><h3>Simplifying and Empowering Transformers for Large-Graph Representations</h3>
<p>Qitian Wu, Wentao Zhao, Chenxiao Yang, Hengrui Zhang, Fan Nie, Haitian Jiang, Yatao Bian, Junchi Yan</p>
<p><a href='https://openreview.net/forum?id=R4xpvDTWkV'>https://openreview.net/forum?id=R4xpvDTWkV</a></p>
<p><b>Keywords</b>: graph transformers, large graphs
</p><p><b>Compressor summary</b>: The authors propose Simplified Graph Transformers (SGFormer), which uses one-layer attention to efficiently predict node properties on large graphs without complex models or pre-processing, achieving significant inference acceleration and scalability.</p><hr><h3>Scaling MLPs: A Tale of Inductive Bias</h3>
<p>Gregor Bachmann, Sotiris Anagnostidis, Thomas Hofmann</p>
<p><a href='https://openreview.net/forum?id=R45A8eKcax'>https://openreview.net/forum?id=R45A8eKcax</a></p>
<p><b>Keywords</b>: MLP, scaling-laws, inductive bias, DL theory
</p><p><b>Compressor summary</b>: This paper investigates the limits and performance of multi-layer perceptrons (MLPs) in vision tasks, showing that they improve with scale and can mimic modern models.</p><hr><h3>Two Sides of The Same Coin: Bridging Deep Equilibrium Models and Neural ODEs via Homotopy Continuation</h3>
<p>Shutong Ding, Tianyu Cui, Jingya Wang, Ye Shi</p>
<p><a href='https://openreview.net/forum?id=R2rJq5OHdr'>https://openreview.net/forum?id=R2rJq5OHdr</a></p>
<p><b>Keywords</b>: Deep Equilibrium Models, Neural Ordinary Differential Equations, Homotopy Continuation
</p><p><b>Compressor summary</b>: The authors propose a new implicit model called HomoODE, which combines the advantages of DEQs and Neural ODEs, and show its effectiveness on image classification tasks using homotopy continuation.</p><hr><h3>Emergence of Shape Bias in Convolutional Neural Networks through Activation Sparsity</h3>
<p>Tianqin Li, Ziqi Wen, Yangfan Li, Tai Sing Lee</p>
<p><a href='https://openreview.net/forum?id=QzcZb3fWmW'>https://openreview.net/forum?id=QzcZb3fWmW</a></p>
<p><b>Keywords</b>: neuroscience, computer vision, shape & texture bias
</p><p><b>Compressor summary</b>: The paper proposes that sparse coding can introduce shape bias into deep learning models, improving their robustness and structure generation, and provides code at a specified GitHub repository.</p><hr><h3>Equal Opportunity of Coverage in Fair Regression</h3>
<p>Fangxin Wang, Lu Cheng, Ruocheng Guo, Kay Liu, Philip S. Yu</p>
<p><a href='https://openreview.net/forum?id=QxYzmYmQQe'>https://openreview.net/forum?id=QxYzmYmQQe</a></p>
<p><b>Keywords</b>: Equal Opportunity; Fair Machine Learning; Conformal Prediction; Uncertainty Quantification
</p><p><b>Compressor summary</b>: The authors propose Equal Opportunity of Coverage (EOC), a fair machine learning approach that ensures equal coverage rates for different groups and maintains population-level coverage, while using Binned Fair Quantile Regression to improve prediction interval width.</p><hr><h3>Beyond Myopia: Learning from Positive and Unlabeled Data through Holistic Predictive Trends</h3>
<p>Wang Xinrui, wan wenhai, Chuanxing Geng, Shao-Yuan Li, Songcan Chen</p>
<p><a href='https://openreview.net/forum?id=QwvaqV48fB'>https://openreview.net/forum?id=QwvaqV48fB</a></p>
<p><b>Keywords</b>: positive and unlabeled learning, machine learning, deep learning, temporal point process, data imbalance
</p><p><b>Compressor summary</b>: The paper proposes a novel balanced resampling technique for PUL methods and uses temporal point processes to detect trends in scores, improving performance in real-world settings.</p><hr><h3>Is Distance Matrix Enough for Geometric Deep Learning?</h3>
<p>Zian Li, Xiyuan Wang, Yinan Huang, Muhan Zhang</p>
<p><a href='https://openreview.net/forum?id=QwQ5HhhSNo'>https://openreview.net/forum?id=QwQ5HhhSNo</a></p>
<p><b>Keywords</b>: geometric deep learning, expressiveness, equivariant neural networks, universality
</p><p><b>Compressor summary</b>: $k$-DisGNNs improve geometric deep learning by capturing high-order information, unifying existing models, and being universal function approximators.</p><hr><h3>SOAR: Improved Indexing for Approximate Nearest Neighbor Search</h3>
<p>Philip Sun, David Simcha, Dave Dopson, Ruiqi Guo, Sanjiv Kumar</p>
<p><a href='https://openreview.net/forum?id=QvIvWMaQdX'>https://openreview.net/forum?id=QvIvWMaQdX</a></p>
<p><b>Keywords</b>: ann, quantization, mips, nearest neighbor search, retrieval
</p><p><b>Compressor summary</b>: SOAR is a new data indexing technique that uses multiple redundant representations and an orthogonality-amplified residual loss to improve approximate nearest neighbor search performance, speed, and memory efficiency.</p><hr><h3>Sorting with Predictions</h3>
<p>Xingjian Bai, Christian Coester</p>
<p><a href='https://openreview.net/forum?id=Qv7rWR9JWa'>https://openreview.net/forum?id=Qv7rWR9JWa</a></p>
<p><b>Keywords</b>: sorting, learning-augmented algorithms, algorithms with predictions, adaptive sorting
</p><p><b>Compressor summary</b>: The paper explores how learning-augmented algorithms can use predictions to improve sorting efficiency, and designs new algorithms that adapt to prediction quality and achieve optimal comparison complexity.</p><hr><h3>PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers</h3>
<p>Phillip Lippe, Bastiaan S. Veeling, Paris Perdikaris, Richard E Turner, Johannes Brandstetter</p>
<p><a href='https://openreview.net/forum?id=Qv6468llWS'>https://openreview.net/forum?id=Qv6468llWS</a></p>
<p><b>Keywords</b>: Neural PDE Solvers, Neural Operators, Temporal Stability, Long-Horizon Modeling, Autoregressive Forecasting
</p><p><b>Compressor summary</b>: This paper introduces a new neural network model called PDE-Refiner that improves the accuracy and stability of solving time-dependent partial differential equations by using a multistep refinement process and spectral data augmentation, outperforming existing methods.</p><hr><h3>Streaming Factor Trajectory Learning for Temporal Tensor Decomposition</h3>
<p>Shikai Fang, Xin Yu, Shibo Li, Zheng Wang, Robert Kirby, Shandian Zhe</p>
<p><a href='https://openreview.net/forum?id=Qu6Ln7d9df'>https://openreview.net/forum?id=Qu6Ln7d9df</a></p>
<p><b>Keywords</b>: Tensor Decomposition, streaming method, Bayesian model
</p><p><b>Compressor summary</b>: SFTL is a method that uses Gaussian processes to model the temporal evolution of factors in tensor decomposition for streaming data.</p><hr><h3>Towards a Unified Framework of Contrastive Learning for Disentangled Representations</h3>
<p>Stefan Matthes, Zhiwei Han, Hao Shen</p>
<p><a href='https://openreview.net/forum?id=QrB38MAAEP'>https://openreview.net/forum?id=QrB38MAAEP</a></p>
<p><b>Keywords</b>: Disentanglement, Contrastive Learning, Identifiability, Representation Learning, Nonlinear ICA
</p><p><b>Compressor summary</b>: This paper extends disentanglement guarantees and identifiability of latents to a broader family of contrastive learning methods and evaluates them on benchmark datasets.</p><hr><h3>Advice Querying under Budget Constraint for Online Algorithms</h3>
<p>Ziyad Benomar, Vianney Perchet</p>
<p><a href='https://openreview.net/forum?id=QpZubU4yD9'>https://openreview.net/forum?id=QpZubU4yD9</a></p>
<p><b>Keywords</b>: online algorithms, competitive ratio, learning augmented algorithms, scheduling, ski-rental, secretary
</p><p><b>Compressor summary</b>: The paper explores algorithms that can request a limited number of predictions at any time while solving classical problems in competitive analysis.</p><hr><h3>Selective Sampling and Imitation Learning via Online Regression</h3>
<p>Ayush Sekhari, Karthik Sridharan, Wen Sun, Runzhe Wu</p>
<p><a href='https://openreview.net/forum?id=QoeOVgayLp'>https://openreview.net/forum?id=QoeOVgayLp</a></p>
<p><b>Keywords</b>: Selective Sampling, Imitation Learning, Learning from Expert Feedback, Theory, General purpose algorithms
</p><p><b>Compressor summary</b>: This paper proposes an interactive algorithm for Imitation Learning that uses selective sampling to query a noisy expert for feedback and achieves the best-known bounds for regret and queries in general function classes and multiple actions, with tight theoretical results.</p><hr><h3>Bifurcations and loss jumps in RNN training</h3>
<p>Lukas Eisenmann, Zahra Monfared, Niclas Alexander Göring, Daniel Durstewitz</p>
<p><a href='https://openreview.net/forum?id=QmPf29EHyI'>https://openreview.net/forum?id=QmPf29EHyI</a></p>
<p><b>Keywords</b>: dynamical systems, bifurcations, Recurrent Neural Networks, attractors, training algorithm, BPTT, exploding and vanishing gradient problem, nonlinear dynamics, time series
</p><p><b>Compressor summary</b>: The authors propose a novel heuristic algorithm to detect bifurcations and fixed points in ReLU-based RNNs, which can help understand their dynamical behavior and the effects of parameter variations during training.</p><hr><h3>Convergence of Actor-Critic with Multi-Layer Neural Networks</h3>
<p>Haoxing Tian, Alex Olshevsky, Ioannis Paschalidis</p>
<p><a href='https://openreview.net/forum?id=QlfGOVD5PO'>https://openreview.net/forum?id=QlfGOVD5PO</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Actor-Critic, gradient splitting, neural network
</p><p><b>Compressor summary</b>: This paper demonstrates that actor-critic methods can converge using deep neural networks with any number of hidden layers, improving the connection between theory and practice in this area.</p><hr><h3>Goal-conditioned Offline Planning from Curious Exploration</h3>
<p>Marco Bagatella, Georg Martius</p>
<p><a href='https://openreview.net/forum?id=QlbZabgMdK'>https://openreview.net/forum?id=QlbZabgMdK</a></p>
<p><b>Keywords</b>: deep reinforcement learning, unsupervised reinforcement learning, goal-conditioned reinforcement learning, model-based planning
</p><p><b>Compressor summary</b>: Curiosity-driven exploration helps learn robust dynamics models, but extracting goal-conditioned behavior from unsupervised data is challenging; combining model-based planning and graph-based aggregation improves zero-shot goal-reaching performance.</p><hr><h3>Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping</h3>
<p>Chunming He, Kai Li, Yachao Zhang, Guoxia Xu, Longxiang Tang, Yulun Zhang, Zhenhua Guo, Xiu Li</p>
<p><a href='https://openreview.net/forum?id=QlHosp050r'>https://openreview.net/forum?id=QlHosp050r</a></p>
<p><b>Keywords</b>: Concealed Object Segmentation, Weakly-Supervised Learning, Segment Anything Model
</p><p><b>Compressor summary</b>: The paper proposes a new WSCOS method that tackles intrinsic similarity and weak supervision challenges by using multi-scale feature grouping and SAM-based segmentation with strategies to improve supervision quality, achieving state-of-the-art performance.</p><hr><h3>ProPILE: Probing Privacy Leakage in Large Language Models</h3>
<p>Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, Seong Joon Oh</p>
<p><a href='https://openreview.net/forum?id=QkLpGxUboF'>https://openreview.net/forum?id=QkLpGxUboF</a></p>
<p><b>Keywords</b>: Personal identifiable information, Private data leakage, Large language model
</p><p><b>Compressor summary</b>: ProPILE is a tool that helps people check how much of their personal information might be leaked by large language models like OPT-1.3B trained on web data.</p><hr><h3>Neural-Logic Human-Object Interaction Detection</h3>
<p>Liulei Li, Jianan Wei, Wenguan Wang, Yi Yang</p>
<p><a href='https://openreview.net/forum?id=QjI36zxjbW'>https://openreview.net/forum?id=QjI36zxjbW</a></p>
<p><b>Keywords</b>: Human-Object Interaction, Neuro-Symbolic Computing, Compositional Generalization
</p><p><b>Compressor summary</b>: LogicHOI is a new HOI detector that uses neural-logic reasoning and Transformer to infer feasible interactions between entities, taking into account affordances and proxemics, and achieves significant improvements over existing methods.</p><hr><h3>Adaptive Privacy Composition for Accuracy-first Mechanisms</h3>
<p>Ryan Rogers, Gennady Samorodnitsky, Steven Wu, Aaditya Ramdas</p>
<p><a href='https://openreview.net/forum?id=QezJbfW01r'>https://openreview.net/forum?id=QezJbfW01r</a></p>
<p><b>Keywords</b>: differential privacy, brownian motion, composition, martingale
</p><p><b>Compressor summary</b>: The authors develop privacy filters that combine differentially private mechanisms with ex-post private mechanisms, such as noise reduction, for better performance in tasks requiring accuracy and privacy.</p><hr><h3>Adaptive Data Analysis in a Balanced Adversarial Model</h3>
<p>Kobbi Nissim, Uri Stemmer, Eliad Tsfadia</p>
<p><a href='https://openreview.net/forum?id=QatZNssk7T'>https://openreview.net/forum?id=QatZNssk7T</a></p>
<p><b>Keywords</b>: Adaptive Data Analysis, Differential Privacy, Statistical Queries
</p><p><b>Compressor summary</b>: The paper investigates adaptive data analysis with a balanced adversary model, where the analyst does not know the underlying distribution and shows that it is hard to answer more than n^2 queries under standard cryptography assumptions.</p><hr><h3>Counterfactually Fair Representation</h3>
<p>Zhiqun Zuo, Mohammad Mahdi Khalili, Xueru Zhang</p>
<p><a href='https://openreview.net/forum?id=QZo1cge4Tc'>https://openreview.net/forum?id=QZo1cge4Tc</a></p>
<p><b>Keywords</b>: Counterfactual fairness, Representation learning
</p><p><b>Compressor summary</b>: This paper introduces a new algorithm for training machine learning models that are fair under Counterfactual Fairness, a notion that requires individuals to be treated equally regardless of their social group, by using all available features instead of excluding those related to sensitive attributes.</p><hr><h3>CoDA: Collaborative Novel Box Discovery and Cross-modal Alignment for Open-vocabulary 3D Object Detection</h3>
<p>Yang Cao, Yihan Zeng, Hang Xu, Dan Xu</p>
<p><a href='https://openreview.net/forum?id=QW5ouyyIgG'>https://openreview.net/forum?id=QW5ouyyIgG</a></p>
<p><b>Keywords</b>: 3D vision, open-vocabulary perception, multi-modal learning, point cloud, 3D object detection
</p><p><b>Compressor summary</b>: The paper proposes CoDA, a unified framework for simultaneous novel object localization and classification in 3D scenes using both geometric and semantic priors, as well as cross-modal alignment between point cloud and image/text modalities.</p><hr><h3>Bottleneck Structure in Learned Features: Low-Dimension vs Regularity Tradeoff</h3>
<p>Arthur Jacot</p>
<p><a href='https://openreview.net/forum?id=QVpfk2C3Dm'>https://openreview.net/forum?id=QVpfk2C3Dm</a></p>
<p><b>Keywords</b>: Feature Learning, Symmetry Learning, Theory of Deep Learning, Weight Decay
</p><p><b>Compressor summary</b>: The paper investigates how deep neural networks learn low-dimensional representations and balance complexity/irregularity, proving a conjectured bottleneck structure in the limit of infinite depth.</p><hr><h3>FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models</h3>
<p>Hao ZHANG, Tianyuan DAI, Yanbo Xu, Yu-Wing Tai, Chi-Keung Tang</p>
<p><a href='https://openreview.net/forum?id=QUkYZNhfc6'>https://openreview.net/forum?id=QUkYZNhfc6</a></p>
<p><b>Keywords</b>: NeRF Editing, NeRF Relighting, Face, Diffusion model, 3d synthesis, GAN inversion
</p><p><b>Compressor summary</b>: The paper introduces FaceDNeRF, a method to create high-quality 3D faces from single images with semantic editing and relighting capabilities, outperforming existing 2D editing approaches.</p><hr><h3>Networks are Slacking Off: Understanding Generalization Problem in Image Deraining</h3>
<p>Jinjin Gu, Xianzheng Ma, Xiangtao Kong, Yu Qiao, Chao Dong</p>
<p><a href='https://openreview.net/forum?id=QRWA5nTWuM'>https://openreview.net/forum?id=QRWA5nTWuM</a></p>
<p><b>Keywords</b>: Image Deraining, Generalization, Interpretation
</p><p><b>Compressor summary</b>: Despite being successful in labs, deep deraining networks struggle with real-world applications due to overfitting; simplifying training background images can improve their generalization performance.</p><hr><h3>CADet: Fully Self-Supervised Out-Of-Distribution Detection With Contrastive Learning</h3>
<p>Charles Guille-Escuret, Pau Rodriguez, David Vazquez, Ioannis Mitliagkas, Joao Monteiro</p>
<p><a href='https://openreview.net/forum?id=QRAS5wSgEy'>https://openreview.net/forum?id=QRAS5wSgEy</a></p>
<p><b>Keywords</b>: Contrastive learning, OOD detection, adversarial detection, MMD, ImageNet-O, Anomaly detection, CIFAR-10.1
</p><p><b>Compressor summary</b>: The authors propose a novel self-supervised contrastive learning method (CADet) that can detect both unseen classes and adversarial perturbations in machine learning systems without needing labels or additional OOD samples.</p><hr><h3>Fractal Landscapes in Policy Optimization</h3>
<p>Tao Wang, Sylvia Lee Herbert, Sicun Gao</p>
<p><a href='https://openreview.net/forum?id=QQidjdmyPp'>https://openreview.net/forum?id=QQidjdmyPp</a></p>
<p><b>Keywords</b>: Reinforcement learning, policy gradient, non-smooth landscape
</p><p><b>Compressor summary</b>: The paper proposes a framework to understand and handle non-smooth or fractal optimization landscapes in deep reinforcement learning, which can cause failures in policy gradient methods.</p><hr><h3>Adversarial Self-Training Improves Robustness and Generalization for Gradual Domain Adaptation</h3>
<p>Lianghe Shi, Weiwei Liu</p>
<p><a href='https://openreview.net/forum?id=QNUs3Ramad'>https://openreview.net/forum?id=QNUs3Ramad</a></p>
<p><b>Keywords</b>: learning theory
</p><p><b>Compressor summary</b>: Gradual self-training with adversarial training improves both clean and adversarial accuracies in domains with intermediate adaptations, as it outperforms standard training when dealing with incorrect pseudo-labels.</p><hr><h3>Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects</h3>
<p>Chuanruo Ning, Ruihai Wu, Haoran Lu, Kaichun Mo, Hao Dong</p>
<p><a href='https://openreview.net/forum?id=QLllDwizVd'>https://openreview.net/forum?id=QLllDwizVd</a></p>
<p><b>Keywords</b>: articulated object manipulation, few-shot learning, visual affordance for robotics
</p><p><b>Compressor summary</b>: Where2Explore is a framework that helps robots learn how to manipulate novel object categories by efficiently exploring local geometries shared across different objects.</p><hr><h3>An Optimal and Scalable Matrix Mechanism for Noisy Marginals under Convex Loss Functions</h3>
<p>Yingtai Xiao, Guanlin He, Danfeng Zhang, Daniel Kifer</p>
<p><a href='https://openreview.net/forum?id=QKSejqE8Vp'>https://openreview.net/forum?id=QKSejqE8Vp</a></p>
<p><b>Keywords</b>: differential privacy, marginals, matrix mechanism, scalability
</p><p><b>Compressor summary</b>: ResidualPlanner is an optimal and scalable matrix mechanism for confidentiality-protecting data release that can optimize various loss functions and handle large scale settings efficiently.</p><hr><h3>Causal normalizing flows: from theory to practice</h3>
<p>Adrián Javaloy, Pablo Sanchez Martin, Isabel Valera</p>
<p><a href='https://openreview.net/forum?id=QIFoCI7ca1'>https://openreview.net/forum?id=QIFoCI7ca1</a></p>
<p><b>Keywords</b>: causality, causal inference, normalizing flows, identifiability, interventions, counterfactuals
</p><p><b>Compressor summary</b>: The authors present a method to identify and learn causal models from observational data using autoregressive normalizing flows, and demonstrate its applicability on real-world problems with mixed discrete-continuous data.</p><hr><h3>Lossy Image Compression with Conditional Diffusion Models</h3>
<p>Ruihan Yang, Stephan Mandt</p>
<p><a href='https://openreview.net/forum?id=QIBpzaDCAv'>https://openreview.net/forum?id=QIBpzaDCAv</a></p>
<p><b>Keywords</b>: generative model, diffusion model, image compression, computer vision
</p><p><b>Compressor summary</b>: The paper presents a lossy image compression framework using diffusion generative models that maps images to and from latent space, achieving good performance on various metrics.</p><hr><h3>RADAR: Robust AI-Text Detection via Adversarial Learning</h3>
<p>Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho</p>
<p><a href='https://openreview.net/forum?id=QGrkbaan79'>https://openreview.net/forum?id=QGrkbaan79</a></p>
<p><b>Keywords</b>: Large Language Models, Text Detection, Adversarial Learning, Paraphrase
</p><p><b>Compressor summary</b>: RADAR is a framework that uses adversarial learning to train a robust AI-text detector capable of detecting paraphrased texts generated by large language models, outperforming existing methods.</p><hr><h3>Learning Large-scale Neural Fields via Context Pruned Meta-Learning</h3>
<p>Jihoon Tack, Subin Kim, Sihyun Yu, Jaeho Lee, Jinwoo Shin, Jonathan Richard Schwarz</p>
<p><a href='https://openreview.net/forum?id=QGmNMtK3pQ'>https://openreview.net/forum?id=QGmNMtK3pQ</a></p>
<p><b>Keywords</b>: Meta-Learning, Efficient Meta-Learning, Neural Fields, Implicit Neural Representations, Data Pruning
</p><p><b>Compressor summary</b>: The paragraph describes an efficient optimization-based meta-learning technique for large-scale neural field training that reduces memory usage, improves model quality, and allows fast learning of high-quality neural fields across multiple modalities.</p><hr><h3>Neural Oscillators are Universal</h3>
<p>Samuel Lanthaler, T. Konstantin Rusch, Siddhartha Mishra</p>
<p><a href='https://openreview.net/forum?id=QGQsOZcQ2H'>https://openreview.net/forum?id=QGQsOZcQ2H</a></p>
<p><b>Keywords</b>: neural ODE, universal approximation, oscillator
</p><p><b>Compressor summary</b>: The paper introduces neural oscillators as a universal class for machine learning architectures and proves their ability to approximate continuous and causal operator mappings using forced harmonic oscillators with a nonlinear read-out.</p><hr><h3>CAT-Walk: Inductive Hypergraph Learning via Set Walks</h3>
<p>Ali Behrouz, Farnoosh Hashemi, Sadaf Sadeghian, Margo Seltzer</p>
<p><a href='https://openreview.net/forum?id=QG4nJBNEar'>https://openreview.net/forum?id=QG4nJBNEar</a></p>
<p><b>Keywords</b>: Hypergraph Learning, Temporal Networks, Higher-order Temporal Motifs, Inductive Representation Learning
</p><p><b>Compressor summary</b>: CAT-Walk is a method to learn dynamic laws of temporal hypergraphs using a higher-order walk, SetWalk, and an adaptive pooling strategy, SetMixer, that achieves high performance in various tasks.</p><hr><h3>Nearly Tight Bounds For Differentially Private Multiway Cut</h3>
<p>Mina Dalirrooyfard, Slobodan Mitrovic, Yuriy Nevmyvaka</p>
<p><a href='https://openreview.net/forum?id=QDByreuQyk'>https://openreview.net/forum?id=QDByreuQyk</a></p>
<p><b>Keywords</b>: Differential Privacy, clustering, multiway cut, min cut, graph partitioning
</p><p><b>Compressor summary</b>: The authors study the complexity of maintaining differential privacy while finding minimal edge removals to disconnect two or more nodes in a graph, and develop efficient algorithms for this problem with near optimal trade-offs between privacy and efficiency.</p><hr><h3>DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial Purification</h3>
<p>Mintong Kang, Dawn Song, Bo Li</p>
<p><a href='https://openreview.net/forum?id=QB7ot7p6j7'>https://openreview.net/forum?id=QB7ot7p6j7</a></p>
<p><b>Keywords</b>: adversarial attack, adversarial purification, adversarial robustness, diffusion model
</p><p><b>Compressor summary</b>: The paper proposes DiffAttack, an efficient method to break diffusion-based purification defenses in adversarial examples, improving on existing attacks by over 10% on ImageNet and over 20% on CIFAR-10.</p><hr><h3>SegRefiner: Towards Model-Agnostic Segmentation Refinement with Discrete Diffusion Process</h3>
<p>Mengyu Wang, Henghui Ding, Jun Hao Liew, Jiajun Liu, Yao Zhao, Yunchao Wei</p>
<p><a href='https://openreview.net/forum?id=Q9CNA7B7v2'>https://openreview.net/forum?id=Q9CNA7B7v2</a></p>
<p><b>Keywords</b>: Refinement, Segmentation, Discrete Diffusion
</p><p><b>Compressor summary</b>: The paper introduces SegRefiner, a model-agnostic method that improves object masks using denoising diffusion steps and shows its superior performance on various segmentation tasks.</p><hr><h3>Unified 3D Segmenter As Prototypical Classifiers</h3>
<p>Zheyun Qin, Cheng Han, Qifan Wang, Xiushan Nie, Yilong Yin, Xiankai Lu</p>
<p><a href='https://openreview.net/forum?id=Q6zd1hr7sD'>https://openreview.net/forum?id=Q6zd1hr7sD</a></p>
<p><b>Keywords</b>: Point Cloud Segmentation, Prototypical Classifier, Unified Framework
</p><p><b>Compressor summary</b>: ProtoSEG is a prototype-based model that unifies semantic, instance, and panoptic segmentation tasks for point clouds using a Transformer architecture and achieves competitive results on several benchmarks.</p><hr><h3>Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic Detection of Infeasible Plans</h3>
<p>Kyowoon Lee, Seongun Kim, Jaesik Choi</p>
<p><a href='https://openreview.net/forum?id=Q5tuGgqJwt'>https://openreview.net/forum?id=Q5tuGgqJwt</a></p>
<p><b>Keywords</b>: Offline Reinforcement Learning, Trajectory Optimization, Diffusion Models, Sequential Decision Making
</p><p><b>Compressor summary</b>: The paper proposes a method to improve the reliability of long-horizon plans generated by diffusion models using a restoration gap metric and an attribution map regularizer, which enables refinement of infeasible plans and provides explainability.</p><hr><h3>VanillaNet: the Power of Minimalism in Deep Learning</h3>
<p>Hanting Chen, Yunhe Wang, Jianyuan Guo, Dacheng Tao</p>
<p><a href='https://openreview.net/forum?id=Q5Eb6qIKux'>https://openreview.net/forum?id=Q5Eb6qIKux</a></p>
<p><b>Keywords</b>: computer vision, foundation models.
</p><p><b>Compressor summary</b>: VanillaNet is a simple neural network architecture that achieves high performance in computer vision tasks while being efficient and easy to deploy.</p><hr><h3>Fast and Simple Spectral Clustering in Theory and Practice</h3>
<p>Peter Macgregor</p>
<p><a href='https://openreview.net/forum?id=Q3FXnCPZ1X'>https://openreview.net/forum?id=Q3FXnCPZ1X</a></p>
<p><b>Keywords</b>: spectral clustering, power method, spectral graph theory, graph algorithms
</p><p><b>Compressor summary</b>: The paper proposes a fast and accurate spectral clustering algorithm using fewer eigenvectors and evaluates its performance on various datasets.</p><hr><h3>Approximate Allocation Matching for Structural Causal Bandits with Unobserved Confounders</h3>
<p>Lai Wei, Muhammad Qasim Elahi, Mahsa Ghasemi, Murat Kocaoglu</p>
<p><a href='https://openreview.net/forum?id=Q3CRHnttxW'>https://openreview.net/forum?id=Q3CRHnttxW</a></p>
<p><b>Keywords</b>: multi-armed bandits, causal Inference, sequential decision-making
</p><p><b>Compressor summary</b>: Structural causal bandit is an online decision-making framework using causal models to learn from unknown environments, balancing exploration and exploitation for maximizing rewards and minimizing regret.</p><hr><h3>OneNet: Enhancing Time Series Forecasting Models under Concept Drift by Online Ensembling</h3>
<p>YiFan Zhang, Qingsong Wen, Xue Wang, Weiqi Chen, Liang Sun, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan</p>
<p><a href='https://openreview.net/forum?id=Q25wMXsaeZ'>https://openreview.net/forum?id=Q25wMXsaeZ</a></p>
<p><b>Keywords</b>: Time series forecasting, concept drift, online learning, online convex programming
</p><p><b>Compressor summary</b>: OneNet is an online time series forecasting algorithm that combines two models, one for temporal dependency and one for cross-variable dependency, using reinforcement learning to adjust their weights dynamically and improve performance against concept drift.</p><hr><h3>Near-optimal learning with average Hölder smoothness</h3>
<p>Guy Kornowski, Steve Hanneke, Aryeh Kontorovich</p>
<p><a href='https://openreview.net/forum?id=Q0ntwxVtcy'>https://openreview.net/forum?id=Q0ntwxVtcy</a></p>
<p><b>Keywords</b>: Hölder smoothness, average smoothness, bracketing numbers, generalization, risk bounds, metric space
</p><p><b>Compressor summary</b>: The paper generalizes a smoothness measure for functions and shows that using an average smoothness instead of a worst-case one leads to better learning rates in different settings.</p><hr><h3>Does Visual Pretraining Help End-to-End Reasoning?</h3>
<p>Chen Sun, Calvin Luo, Xingyi Zhou, Anurag Arnab, Cordelia Schmid</p>
<p><a href='https://openreview.net/forum?id=PzYAMXmIT3'>https://openreview.net/forum?id=PzYAMXmIT3</a></p>
<p><b>Keywords</b>: visual reasoning, self-supervised learning
</p><p><b>Compressor summary</b>: The authors propose a self-supervised method to compress video frames into tokens using a transformer network, and show that visual pretraining is crucial for end-to-end visual reasoning.</p><hr><h3>Vulnerabilities in Video Quality Assessment Models: The Challenge of Adversarial Attacks</h3>
<p>Aoxiang Zhang, Yu Ran, Weixuan Tang, Yuan-Gen Wang</p>
<p><a href='https://openreview.net/forum?id=Pz8xvVCLNJ'>https://openreview.net/forum?id=Pz8xvVCLNJ</a></p>
<p><b>Keywords</b>: video quality assessment, adversarial attack, black-box, just noticeable difference
</p><p><b>Compressor summary</b>: The paper introduces a method to evaluate the robustness of NR-VQA models against adversarial attacks and proposes a patch-based random search technique for black-box attacks.</p><hr><h3>Private estimation algorithms for stochastic block models and mixture models</h3>
<p>Hongjie Chen, Vincent Cohen-Addad, Tommaso d'Orsi, Alessandro Epasto, Jacob Imola, David Steurer, Stefan Tiegel</p>
<p><a href='https://openreview.net/forum?id=Pya0kCEpDk'>https://openreview.net/forum?id=Pya0kCEpDk</a></p>
<p><b>Keywords</b>: differential privacy, stochastic block model, Gaussian mixture model, sum of squares
</p><p><b>Compressor summary</b>: The authors propose efficient private estimation algorithms that match non-private counterparts in high-dimensional settings and apply them to stochastic block models and mixtures of spherical Gaussians problems.</p><hr><h3>Learning Exponential Families from Truncated Samples</h3>
<p>Jane Lee, Andre Wibisono, Manolis Zampetakis</p>
<p><a href='https://openreview.net/forum?id=PxcWJqO3qj'>https://openreview.net/forum?id=PxcWJqO3qj</a></p>
<p><b>Keywords</b>: truncated statistics, robustness, exponential families, extrapolation
</p><p><b>Compressor summary</b>: The paper presents an estimation algorithm that can extrapolate from truncated data for log-concave exponential families, using Projected Stochastic Gradient Descent, which is simpler and more efficient than previous methods.</p><hr><h3>Understanding Few-Shot Learning: Measuring Task Relatedness and Adaptation Difficulty via Attributes</h3>
<p>Minyang Hu, Hong Chang, Zong Guo, Bingpeng Ma, Shiguang Shan, Xilin CHEN</p>
<p><a href='https://openreview.net/forum?id=Pvgxecj5aS'>https://openreview.net/forum?id=Pvgxecj5aS</a></p>
<p><b>Keywords</b>: Few-shot Learning, Meta-Learning, Task Relatedness, Task Adaptation Difficulty
</p><p><b>Compressor summary</b>: The paper proposes a metric called Task Attribute Distance (TAD) to measure the similarity between training and novel tasks in few-shot learning, and shows how it relates to adaptation difficulty.</p><hr><h3>Interpretable Graph Networks Formulate Universal Algebra Conjectures</h3>
<p>Francesco Giannini, Stefano Fioravanti, Oguzhan Keskin, Alisia Maria Lupidi, Lucie Charlotte Magister, Pietro Lio, Pietro Barbiero</p>
<p><a href='https://openreview.net/forum?id=Psnph85KYc'>https://openreview.net/forum?id=Psnph85KYc</a></p>
<p><b>Keywords</b>: universal algebra, interpretability, graph neural networks, concept-based models
</p><p><b>Compressor summary</b>: This paper uses AI to explore Universal Algebra's conjectures, creating datasets and a new neural layer for interpretable graph networks that can validate and generate conjectures.</p><hr><h3>Contextually Affinitive Neighborhood Refinery for Deep Clustering</h3>
<p>Chunlin Yu, Ye Shi, Jingya Wang</p>
<p><a href='https://openreview.net/forum?id=Psj0jHocm1'>https://openreview.net/forum?id=Psj0jHocm1</a></p>
<p><b>Keywords</b>: Deep Clustering, Self-supervised learning, re-ranking
</p><p><b>Compressor summary</b>: The paragraph discusses a new method for deep clustering that uses online re-ranking to find more informative neighbors, cross-view consistency, and boundary filtering to handle noisy data and improve performance on benchmark datasets.</p><hr><h3>The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit</h3>
<p>Lorenzo Noci, Chuning Li, Mufan Bill Li, Bobby He, Thomas Hofmann, Chris J. Maddison, Daniel M. Roy</p>
<p><a href='https://openreview.net/forum?id=PqfPjS9JRX'>https://openreview.net/forum?id=PqfPjS9JRX</a></p>
<p><b>Keywords</b>: Deep Learning Theory, Covariance SDE, Attention Mechanism, Infinite-Depth-and-Width, Scaling Limit
</p><p><b>Compressor summary</b>: The paper studies how modifying a Softmax-based attention model with skip connections and width-dependent temperature can improve trainability and stability in deep learning models, leading to a new architecture called shaped Transformer.</p><hr><h3>Metis: Understanding and Enhancing In-Network Regular Expressions</h3>
<p>Zhengxin Zhang, Yucheng Huang, Guanglin Duan, Qing Li, Dan Zhao, Yong Jiang, Lianbo Ma, Xi Xiao, Hengyang Xu</p>
<p><a href='https://openreview.net/forum?id=Pplq1TRnma'>https://openreview.net/forum?id=Pplq1TRnma</a></p>
<p><b>Keywords</b>: Network Security; Regular Expression; Knowledge Distillation; Machine Learning; Programmable Switch
</p><p><b>Compressor summary</b>: Metis converts regular expressions to neural networks for better accuracy and device deployment in networking tasks.</p><hr><h3>A Spectral Algorithm for List-Decodable Covariance Estimation in Relative Frobenius Norm</h3>
<p>Ilias Diakonikolas, Daniel Kane, Jasper C.H. Lee, Ankit Pensia, Thanasis Pittas</p>
<p><a href='https://openreview.net/forum?id=PpI7XvOXkF'>https://openreview.net/forum?id=PpI7XvOXkF</a></p>
<p><b>Keywords</b>: robust statistics, covariance estimation, list-decodable learning
</p><p><b>Compressor summary</b>: The paper presents a spectral algorithm for list-decodable Gaussian covariance estimation that works with unknown fractions of Gaussian samples and can also learn robust partial clusters of GMMs efficiently.</p><hr><h3>What Knowledge Gets Distilled in Knowledge Distillation?</h3>
<p>Utkarsh Ojha, Yuheng Li, Anirudh Sundara Rajan, Yingyu Liang, Yong Jae Lee</p>
<p><a href='https://openreview.net/forum?id=Poj71ASubN'>https://openreview.net/forum?id=Poj71ASubN</a></p>
<p><b>Keywords</b>: knowledge distillation
</p><p><b>Compressor summary</b>: The paragraph discusses the need to understand what kind of knowledge is transferred in knowledge distillation techniques and how it affects a student network's properties beyond task performance.</p><hr><h3>Goal Driven Discovery of Distributional Differences via Language Descriptions</h3>
<p>Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt</p>
<p><a href='https://openreview.net/forum?id=PnbCA4ylIc'>https://openreview.net/forum?id=PnbCA4ylIc</a></p>
<p><b>Keywords</b>: large language model, prompting, exploratory text analysis
</p><p><b>Compressor summary</b>: The authors introduce a new task (D5) that uses language models to automatically discover differences between two large corpora based on user-specified goals, and evaluate its performance on synthetic and real datasets.</p><hr><h3>FABind: Fast and Accurate Protein-Ligand Binding</h3>
<p>Qizhi Pei, Kaiyuan Gao, Lijun Wu, Jinhua Zhu, Yingce Xia, Shufang Xie, Tao Qin, Kun He, Tie-Yan Liu, Rui Yan</p>
<p><a href='https://openreview.net/forum?id=PnWakgg1RL'>https://openreview.net/forum?id=PnWakgg1RL</a></p>
<p><b>Keywords</b>: protein-ligand docking
</p><p><b>Compressor summary</b>: The authors propose FABind, a fast and accurate model for predicting protein-ligand binding structures using a pocket prediction and docking approach that leverages ligand information.</p><hr><h3>Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory</h3>
<p>Minhak Song, Chulhee Yun</p>
<p><a href='https://openreview.net/forum?id=PnJaA0A8Lr'>https://openreview.net/forum?id=PnJaA0A8Lr</a></p>
<p><b>Keywords</b>: non-convex optimization, trajectory alignment of GD, edge of stability, progressive sharpening, bifurcation theory
</p><p><b>Compressor summary</b>: The paper investigates how the sharpness of neural networks changes during training, and shows that different gradient descent trajectories align on a bifurcation diagram when the Edge of Stability occurs.</p><hr><h3>Adaptive Principal Component Regression with Applications to Panel Data</h3>
<p>Anish Agarwal, Keegan Harris, Justin Whitehouse, Steven Wu</p>
<p><a href='https://openreview.net/forum?id=PmqBJ02V1p'>https://openreview.net/forum?id=PmqBJ02V1p</a></p>
<p><b>Keywords</b>: adaptive data collection, principal component regression, error-in-variables regression, panel data, synthetic controls, synthetic interventions, causal inference
</p><p><b>Compressor summary</b>: The paper introduces time-uniform finite sample guarantees for online principal component regression in error-in-variables settings and uses it to estimate counterfactual treatment effects in panel data with adaptive interventions.</p><hr><h3>Neural Processes with Stability</h3>
<p>Huafeng Liu, Liping Jing, Jian Yu</p>
<p><a href='https://openreview.net/forum?id=PmlNxZoXr4'>https://openreview.net/forum?id=PmlNxZoXr4</a></p>
<p><b>Keywords</b>: Neural processes, stability
</p><p><b>Compressor summary</b>: The paper introduces a method to improve neural processes by adding algorithmic stability, which leads to better accuracy and robustness.</p><hr><h3>Taking the neural sampling code very seriously: A data-driven approach for evaluating generative models of the visual system</h3>
<p>Suhas Shrinivasan, Konstantin-Klemens Lurz, Kelli Restivo, George Denfield, Andreas S. Tolias, Edgar Y. Walker, Fabian H. Sinz</p>
<p><a href='https://openreview.net/forum?id=Pl416tPkNv'>https://openreview.net/forum?id=Pl416tPkNv</a></p>
<p><b>Keywords</b>: Neural Sampling Code, Probabilistic Inference, Bayesian Brain, Macaque V1, Natural Images, Population Recordings, Normalizing Flows, Probabilistic Models, Computational Neuroscience, Theoretical Neuroscience
</p><p><b>Compressor summary</b>: The authors propose a novel formalization of the Neural Sampling Code theory that allows fitting more flexible generative models to recorded neuronal activity, enabling quantitative evaluation and comparison of different models on natural images.</p><hr><h3>Truncating Trajectories in Monte Carlo Policy Evaluation: an Adaptive Approach</h3>
<p>Riccardo Poiani, Nicole Nobili, Alberto Maria Metelli, Marcello Restelli</p>
<p><a href='https://openreview.net/forum?id=PkKpTK7hJ6'>https://openreview.net/forum?id=PkKpTK7hJ6</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Policy Evaluation, Budget Optimization, Monte Carlo
</p><p><b>Compressor summary</b>: The paper proposes RIDO, an adaptive algorithm that splits the interaction budget into mini-batches and adjusts the length of trajectories to minimize the variance of policy return estimators in Monte Carlo Reinforcement Learning.</p><hr><h3>Optimal Preconditioning and Fisher Adaptive Langevin Sampling</h3>
<p>Michalis Titsias</p>
<p><a href='https://openreview.net/forum?id=Pk9CdOZYRA'>https://openreview.net/forum?id=Pk9CdOZYRA</a></p>
<p><b>Keywords</b>: MCMC, Langevin diffusion, preconditioning, Fisher information, adaptive MCMC, score function
</p><p><b>Compressor summary</b>: The paper proposes an optimal preconditioning for the Langevin diffusion that improves the performance of the Metropolis adjusted Langevin algorithm (MALA) in high dimensions, and compares it with other methods.</p><hr><h3>ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields</h3>
<p>Jiahua Dong, Yu-Xiong Wang</p>
<p><a href='https://openreview.net/forum?id=Pk49a9snPe'>https://openreview.net/forum?id=Pk49a9snPe</a></p>
<p><b>Keywords</b>: neural radiance field, diffusion model, editing
</p><p><b>Compressor summary</b>: ViCA-NeRF is a method for 3D editing with text instructions that uses view-consistency and two types of regularization to ensure multi-view consistency and produce detailed results.</p><hr><h3>One Less Reason for Filter Pruning: Gaining Free Adversarial Robustness with Structured Grouped Kernel Pruning</h3>
<p>Shaochen Zhong, Zaichuan You, Jiamu Zhang, Sebastian Zhao, Zachary LeClaire, Zirui Liu, Daochen Zha, Vipin Chaudhary, Shuai Xu, Xia Hu</p>
<p><a href='https://openreview.net/forum?id=Pjky9XG8zP'>https://openreview.net/forum?id=Pjky9XG8zP</a></p>
<p><b>Keywords</b>: pruning, structured pruning, adversarial robustness, grouped kernel pruning, CNN, one-shot
</p><p><b>Compressor summary</b>: The authors investigate the vulnerability of modern structured pruning methods to adversarial attacks and propose a new method that improves robustness while maintaining compression and acceleration benefits.</p><hr><h3>Implicit Bias of (Stochastic) Gradient Descent for Rank-1 Linear Neural Network</h3>
<p>Bochen Lyu, Zhanxing Zhu</p>
<p><a href='https://openreview.net/forum?id=PjBEUTVzoe'>https://openreview.net/forum?id=PjBEUTVzoe</a></p>
<p><b>Keywords</b>: implicit bias, gradient descent, stochastic gradient descent, linear networks
</p><p><b>Compressor summary</b>: The paper investigates the implicit bias of rank-1 linear networks and finds new insights on how gradient descent and stochastic gradient descent behave in over-parameterized regression problems, which could help understand deep learning better.</p><hr><h3>Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos via Temporal Alignment</h3>
<p>Zihui Xue, Kristen Grauman</p>
<p><a href='https://openreview.net/forum?id=Pj6X6GqNy8'>https://openreview.net/forum?id=Pj6X6GqNy8</a></p>
<p><b>Keywords</b>: fine-grained video understanding, egocentric video, self-supervised learning, temporal alignment
</p><p><b>Compressor summary</b>: The paragraph describes a new method called AE2 that learns action features invariant to viewpoints by aligning egocentric and exocentric videos, even when not captured at the same time or place, and shows its effectiveness on four datasets.</p><hr><h3>Provable Guarantees for Generative Behavior Cloning: Bridging Low-Level Stability and High-Level Behavior</h3>
<p>Adam Block, Ali Jadbabaie, Daniel Pfrommer, Max Simchowitz, Russ Tedrake</p>
<p><a href='https://openreview.net/forum?id=PhFVF0gwid'>https://openreview.net/forum?id=PhFVF0gwid</a></p>
<p><b>Keywords</b>: Imitation Learning, Control, Diffusion Models, Optimal Transport
</p><p><b>Compressor summary</b>: The paper presents a theory for imitating expert actions using generative models and low-level controllers to ensure stability and match the distribution of the expert trajectories.</p><hr><h3>Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation</h3>
<p>Yilin Lyu, Liyuan Wang, Xingxing Zhang, Zicheng Sun, Hang Su, Jun Zhu, Liping Jing</p>
<p><a href='https://openreview.net/forum?id=Ph65E1bE6A'>https://openreview.net/forum?id=Ph65E1bE6A</a></p>
<p><b>Keywords</b>: Continual Learning, Batch Normalization, Recency Bias, Catastrophic Forgetting
</p><p><b>Compressor summary</b>: The paper analyzes the limitations of Batch Normalization in continual learning and proposes Adaptive Balance of BN, which adapts to task-wise contributions and balances BN statistics, achieving significant performance gains on benchmarks.</p><hr><h3>Behavior Alignment via Reward Function Optimization</h3>
<p>Dhawal Gupta, Yash Chandak, Scott M. Jordan, Philip S. Thomas, Bruno Castro da Silva</p>
<p><a href='https://openreview.net/forum?id=PfpAQuyZCB'>https://openreview.net/forum?id=PfpAQuyZCB</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Behavior Alignment, Implicit Gradient, Bi-level Optimization
</p><p><b>Compressor summary</b>: The paper introduces a new framework that uses a bi-level objective to learn behavior alignment reward functions that integrate designer's heuristics with environment's primary rewards for guiding reinforcement learning agents efficiently and robustly.</p><hr><h3>Learning Transformer Programs</h3>
<p>Dan Friedman, Alexander Wettig, Danqi Chen</p>
<p><a href='https://openreview.net/forum?id=Pe9WxkN8Ff'>https://openreview.net/forum?id=Pe9WxkN8Ff</a></p>
<p><b>Keywords</b>: mechanistic interpretability, transformers
</p><p><b>Compressor summary</b>: The authors propose a method for training mechanistically interpretable Transformers by converting them into human-readable programs and show their effectiveness on various tasks, including NLP problems.</p><hr><h3>An Information-Theoretic Evaluation of Generative Models in Learning Multi-modal Distributions</h3>
<p>Mohammad Jalali, Cheuk Ting Li, Farzan Farnia</p>
<p><a href='https://openreview.net/forum?id=PdZhf6PiAb'>https://openreview.net/forum?id=PdZhf6PiAb</a></p>
<p><b>Keywords</b>: Generative Models; Evaluation in Learning; Information Measures
</p><p><b>Compressor summary</b>: The paper proposes an information-theoretic method to measure the number of modes in multi-modal image datasets using R\'enyi Kernel Entropy and evaluates state-of-the-art generative models with it.</p><hr><h3>Responsible AI (RAI) Games and Ensembles</h3>
<p>Yash Gupta, Runtian Zhai, Arun Suggala, Pradeep Kumar Ravikumar</p>
<p><a href='https://openreview.net/forum?id=PcNpL9Q39p'>https://openreview.net/forum?id=PcNpL9Q39p</a></p>
<p><b>Keywords</b>: Responsible AI, fairness, DRO, robustness
</p><p><b>Compressor summary</b>: The authors propose a general framework called Responsible AI (RAI) games to study societal effects of AI, and present two classes of algorithms for solving them.</p><hr><h3>On the Statistical Consistency of Risk-Sensitive Bayesian Decision-Making</h3>
<p>Prateek Jaiswal, Harsha Honnappa, Vinayak Rao</p>
<p><a href='https://openreview.net/forum?id=PcKHQFsvel'>https://openreview.net/forum?id=PcKHQFsvel</a></p>
<p><b>Keywords</b>: Variational Bayes, Loss Calibration, Bayesian Statistics, Variational Inference, Statistical Theory
</p><p><b>Compressor summary</b>: The authors propose a new variational Bayesian method for risk-sensitive decision-making when the posterior distribution is difficult to compute, and study its theoretical and practical properties.</p><hr><h3>Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models</h3>
<p>Naman Deep Singh, Francesco Croce, Matthias Hein</p>
<p><a href='https://openreview.net/forum?id=Pbpk9jUzAi'>https://openreview.net/forum?id=Pbpk9jUzAi</a></p>
<p><b>Keywords</b>: adversarial robustness, deep learning, vision transformers, convnext
</p><p><b>Compressor summary</b>: The paper compares adversarial training on ImageNet for ViTs and ConvNeXts using different architectural modifications and shows that these changes affect robustness in different ways depending on the threat model.</p><hr><h3>Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction</h3>
<p>Quentin Delfosse, Hikaru Shindo, Devendra Singh Dhami, Kristian Kersting</p>
<p><a href='https://openreview.net/forum?id=PbMBfRpVgU'>https://openreview.net/forum?id=PbMBfRpVgU</a></p>
<p><b>Keywords</b>: Reinforcement Learning, First-Order-Logic, Symbolic Abstraction, Interpretable Reinforcement Learning, Logic Reinforcement Learning
</p><p><b>Compressor summary</b>: NUDGE is a neuro-symbolic RL method that combines neural networks and differentiable logic to create interpretable and explainable policies, performing better than pure neural methods in various environments.</p><hr><h3>Performance Bounds for Policy-Based Average Reward Reinforcement Learning Algorithms</h3>
<p>Yashaswini Murthy, Mehrdad Moharrami, R. Srikant</p>
<p><a href='https://openreview.net/forum?id=PaSpImjKm2'>https://openreview.net/forum?id=PaSpImjKm2</a></p>
<p><b>Keywords</b>: Average Reward MDPs, Reinforcement Learning Theory, Approximate Policy Iteration, Policy Based Methods, Performance Bounds
</p><p><b>Compressor summary</b>: The authors propose a solution to the open problem of obtaining performance bounds for approximate policy iteration and reinforcement learning algorithms in the average-reward setting, by deriving non-trivial finite time error bounds that converge to zero as policy evaluation and improvement errors decrease.</p><hr><h3>ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning</h3>
<p>Mingyu Xu, Zheng Lian, Lei Feng, Bin Liu, Jianhua Tao</p>
<p><a href='https://openreview.net/forum?id=PYSfn5xXEe'>https://openreview.net/forum?id=PYSfn5xXEe</a></p>
<p><b>Keywords</b>: Partial label learning; Noisy label learning
</p><p><b>Compressor summary</b>: The paragraph introduces a new method called "Adjusting Label Importance Mechanism" (ALIM) for handling noisy partial label learning, which reduces the impact of detection errors by adjusting the candidate set and model outputs.</p><hr><h3>Feature Learning for Interpretable, Performant Decision Trees</h3>
<p>Jack Henry Good, Torin Kovach, Kyle Miller, Artur Dubrawski</p>
<p><a href='https://openreview.net/forum?id=PYEgC56flW'>https://openreview.net/forum?id=PYEgC56flW</a></p>
<p><b>Keywords</b>: explainability, interpretability, decision tree, feature learning
</p><p><b>Compressor summary</b>: The text discusses a new system that combines sparse feature learning and differentiable decision tree construction to create small, interpretable trees with good performance.</p><hr><h3>Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback</h3>
<p>Han Shao, Lee Cohen, Avrim Blum, Yishay Mansour, Aadirupa Saha, Matthew Walter</p>
<p><a href='https://openreview.net/forum?id=PYASzxr2OP'>https://openreview.net/forum?id=PYASzxr2OP</a></p>
<p><b>Keywords</b>: preference learning, algorithms, linear model, Markov decision processes, learning theory, multi-objective decision making, preference elicitation
</p><p><b>Compressor summary</b>: The paper presents a method to learn user preferences over multiple objectives and find near-optimal policies based on feedback from comparisons between policies or trajectories.</p><hr><h3>SHOT: Suppressing the Hessian along the Optimization Trajectory for Gradient-Based Meta-Learning</h3>
<p>JunHoo Lee, Jayeon Yoo, Nojun Kwak</p>
<p><a href='https://openreview.net/forum?id=PXsqbAjpQd'>https://openreview.net/forum?id=PXsqbAjpQd</a></p>
<p><b>Keywords</b>: meta learning, Hessian, Gradient-Based meta learning, Feature Reuse, Implicit Prior
</p><p><b>Compressor summary</b>: SHOT is a new algorithm that suppresses the Hessian in gradient-based meta-learning, improving performance on few-shot learning tasks.</p><hr><h3>Universality laws for Gaussian mixtures in generalized linear models</h3>
<p>Yatin Dandi, Ludovic Stephan, Florent Krzakala, Bruno Loureiro, Lenka Zdeborova</p>
<p><a href='https://openreview.net/forum?id=PU3deePP2S'>https://openreview.net/forum?id=PU3deePP2S</a></p>
<p><b>Keywords</b>: theoretical analysis, high-dimensional statistics, Universality, weak convergence, mixture models, sampling, statistical physics
</p><p><b>Compressor summary</b>: The paragraph discusses results in high-dimensional statistics under the Gaussian mixture hypothesis and provides rigorous proofs for applying these results to a general class of datasets using generalized linear models and investigating their asymptotic joint statistics.</p><hr><h3>Simplicity Bias in 1-Hidden Layer Neural Networks</h3>
<p>Depen Morwani, jatin batra, Prateek Jain, Praneeth Netrapalli</p>
<p><a href='https://openreview.net/forum?id=PTvxck0QDE'>https://openreview.net/forum?id=PTvxck0QDE</a></p>
<p><b>Keywords</b>: Simplicity Bias, Gradient Descent, Implicit Bias, Neural Networks
</p><p><b>Compressor summary</b>: The paper investigates simplicity bias in neural networks with one hidden layer, showing that they learn only the simplest features even when more complex ones exist, and propose an ensemble method to improve robustness.</p><hr><h3>Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence</h3>
<p>Yuki Takezawa, Ryoma Sato, Han Bao, Kenta Niwa, Makoto Yamada</p>
<p><a href='https://openreview.net/forum?id=PSngfm5B9q'>https://openreview.net/forum?id=PSngfm5B9q</a></p>
<p><b>Keywords</b>: decentralized learning, distributed optimization, network topology, consensus rate
</p><p><b>Compressor summary</b>: The study proposes a new graph topology for decentralized learning that combines fast consensus rate and small maximum degree, improving accuracy and communication efficiency over existing topologies.</p><hr><h3>Policy Space Diversity for Non-Transitive Games</h3>
<p>Jian Yao, Weiming Liu, Haobo Fu, Yaodong Yang, Stephen Marcus McAleer, QIANG FU, Yang Wei</p>
<p><a href='https://openreview.net/forum?id=PRgvdEbhdH'>https://openreview.net/forum?id=PRgvdEbhdH</a></p>
<p><b>Keywords</b>: Policy Diversity, Policy-Space Response Oracles, Nash Equilibrium, Multi-agent Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper proposes a new diversity metric for Policy Space Response Oracles (PSRO) that improves the approximation of Nash Equilibrium and presents PSD-PSRO, a variant that converges and performs better in experiments.</p><hr><h3>Efficient Beam Tree Recursion</h3>
<p>Jishnu Ray Chowdhury, Cornelia Caragea</p>
<p><a href='https://openreview.net/forum?id=PR5znB6BZ2'>https://openreview.net/forum?id=PR5znB6BZ2</a></p>
<p><b>Keywords</b>: Recursive Models, Recursive Neural Networks, RvNNs, Length Generalization, Structured Encoding, Representation Learning
</p><p><b>Compressor summary</b>: The paper proposes strategies to reduce memory usage of BT-RvNN, a neural network for encoding sentences, and to use it as a token contextualizer.</p><hr><h3>Fair Adaptive Experiments</h3>
<p>Waverly Wei, Xinwei Ma, Jingshen Wang</p>
<p><a href='https://openreview.net/forum?id=PMvudWa53L'>https://openreview.net/forum?id=PMvudWa53L</a></p>
<p><b>Keywords</b>: Adaptive Randomized Experiment; Adaptive Design; Causal Inference
</p><p><b>Compressor summary</b>: The paragraph describes a fair adaptive experiment strategy that balances data efficiency, equity, and participant welfare in randomized experiments without assuming parametric models on outcomes.</p><hr><h3>How Re-sampling Helps for Long-Tail Learning?</h3>
<p>Jiang-Xin Shi, Tong Wei, Yuke Xiang, Yu-Feng Li</p>
<p><a href='https://openreview.net/forum?id=PLzCXefcpE'>https://openreview.net/forum?id=PLzCXefcpE</a></p>
<p><b>Keywords</b>: long-tail learning, class-imbalanced learning, re-sampling
</p><p><b>Compressor summary</b>: This paper investigates how re-sampling affects long-tail learning, finding that it can improve generalization if the training data has relevant contexts but may introduce spurious correlations otherwise; they propose a new context shift augmentation module to address this issue.</p><hr><h3>Dynamics Generalisation in Reinforcement Learning via Adaptive Context-Aware Policies</h3>
<p>Michael Beukman, Devon Jarvis, Richard Klein, Steven James, Benjamin Rosman</p>
<p><a href='https://openreview.net/forum?id=PJhjkSFlbG'>https://openreview.net/forum?id=PJhjkSFlbG</a></p>
<p><b>Keywords</b>: Deep Reinforment Learning, Contextual Markov Decision Process, Neural Network Architecture
</p><p><b>Compressor summary</b>: The paper proposes the Decision Adapter, a neural network architecture that incorporates context information into behaviour learning and improves generalisation performance and robustness.</p><hr><h3>User-Level Differential Privacy With Few Examples Per User</h3>
<p>Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Raghu Meka, Chiyuan Zhang</p>
<p><a href='https://openreview.net/forum?id=PITeSdYQkv'>https://openreview.net/forum?id=PITeSdYQkv</a></p>
<p><b>Keywords</b>: differential privacy, user-level privacy, PAC learning
</p><p><b>Compressor summary</b>: This paper presents algorithms for user-level differential privacy in the example-scarce regime, improving existing results and providing new bounds for various learning tasks.</p><hr><h3>Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand</h3>
<p>Junfeng Guo, Yiming Li, Lixu Wang, Shu-Tao Xia, Heng Huang, Cong Liu, Bo Li</p>
<p><a href='https://openreview.net/forum?id=PIDNxRRJ8w'>https://openreview.net/forum?id=PIDNxRRJ8w</a></p>
<p><b>Keywords</b>: Ownership Verification, Dataset Protection, Copyright Protection, Backdoor Attack, AI Security
</p><p><b>Compressor summary</b>: The paper proposes a new approach for dataset ownership verification using domain watermarks that make watermarked DNNs classify hard samples correctly while maintaining stealthiness.</p><hr><h3>Contrastive Moments: Unsupervised Halfspace Learning in Polynomial Time</h3>
<p>Xinyuan Cao, Santosh Vempala</p>
<p><a href='https://openreview.net/forum?id=PHbqznMa1i'>https://openreview.net/forum?id=PHbqznMa1i</a></p>
<p><b>Keywords</b>: Unsupervised Learning, Learning Halfspaces, Non-Gaussian Component analysis
</p><p><b>Compressor summary</b>: The paper presents a polynomial-time algorithm for learning high-dimensional halfspaces with margins from an unknown affine transformation and a symmetric one-dimensional logconcave distribution, using only the first two moments of suitable re-weightings and achieving TV distance guarantees.</p><hr><h3>An Iterative Self-Learning Framework for Medical Domain Generalization</h3>
<p>Zhenbang Wu, Huaxiu Yao, David Liebovitz, Jimeng Sun</p>
<p><a href='https://openreview.net/forum?id=PHKkBbuJWM'>https://openreview.net/forum?id=PHKkBbuJWM</a></p>
<p><b>Keywords</b>: healthcare, clinical predictive model, domain generalization
</p><p><b>Compressor summary</b>: SLGD is a self-learning framework that trains personalized classifiers for decoupled domains, improving deep learning models' domain generalization performance on EHR datasets.</p><hr><h3>ATMAN: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation</h3>
<p>Björn Deiseroth, Mayukh Deb, Samuel Weinbach, Manuel Brack, Patrick Schramowski, Kristian Kersting</p>
<p><a href='https://openreview.net/forum?id=PBpEb86bj7'>https://openreview.net/forum?id=PBpEb86bj7</a></p>
<p><b>Keywords</b>: explainability, attention manipulation, perturbation, large language model, multi-modality, generative decoder, efficiency, transformer
</p><p><b>Compressor summary</b>: AtMan explains generative transformer models efficiently by manipulating their attention mechanisms using cosine similarity neighborhood in the embedding space instead of backpropagation.</p><hr><h3>Effective Robustness against Natural Distribution Shifts for Models with Different Training Data</h3>
<p>Zhouxing Shi, Nicholas Carlini, Ananth Balashankar, Ludwig Schmidt, Cho-Jui Hsieh, Alex Beutel, Yao Qin</p>
<p><a href='https://openreview.net/forum?id=PAYXfIUKWY'>https://openreview.net/forum?id=PAYXfIUKWY</a></p>
<p><b>Keywords</b>: Effective robustness, natural distribution shifts, out-of-distribution robustness
</p><p><b>Compressor summary</b>: The paper introduces a new metric to evaluate and compare the effective robustness of models trained on different data distributions by controlling for accuracy on multiple in-distribution test sets.</p><hr><h3>Type-to-Track: Retrieve Any Object via Prompt-based Tracking</h3>
<p>Pha Nguyen, Kha Gia Quach, Kris M. Kitani, Khoa Luu</p>
<p><a href='https://openreview.net/forum?id=PARMyW6xX0'>https://openreview.net/forum?id=PARMyW6xX0</a></p>
<p><b>Keywords</b>: Grounded Object Tracking, Multiple Object Tracking, Vision Language
</p><p><b>Compressor summary</b>: The paper proposes a new method for tracking objects in videos using natural language captions and introduces a new dataset, GroOT, as well as evaluation protocols and metrics for this task. It also presents a fast and accurate model called MENDER based on tensor decomposition.</p><hr><h3>Cause-Effect Inference in Location-Scale Noise Models: Maximum Likelihood vs. Independence Testing</h3>
<p>Xiangyu Sun, Oliver Schulte</p>
<p><a href='https://openreview.net/forum?id=P9I2VQv1uC'>https://openreview.net/forum?id=P9I2VQv1uC</a></p>
<p><b>Keywords</b>: Causal Discovery, Cause-Effect Inference, Location-Scale Noise Models
</p><p><b>Compressor summary</b>: The paper discusses a problem of inferring cause-effect relationships between random variables and proposes a more robust method for selecting causal models when the noise distribution assumptions are incorrect.</p><hr><h3>Temporal Robustness against Data poisoning</h3>
<p>Wenxiao Wang, Soheil Feizi</p>
<p><a href='https://openreview.net/forum?id=P5vzRpoOj2'>https://openreview.net/forum?id=P5vzRpoOj2</a></p>
<p><b>Keywords</b>: Robustness, Data Poisoning, Security, Machine Learning, Backdoor, Adversarial
</p><p><b>Compressor summary</b>: The paper proposes a temporal threat model of data poisoning that considers how long an attack started and lasted, and evaluates a baseline defense called temporal aggregation.</p><hr><h3>Window-Based Distribution Shift Detection for Deep Neural Networks</h3>
<p>Guy Bar-Shalom, Yonatan Geifman, Ran El-Yaniv</p>
<p><a href='https://openreview.net/forum?id=P3n4wFJGs5'>https://openreview.net/forum?id=P3n4wFJGs5</a></p>
<p><b>Keywords</b>: Distribution shift detection, Window-based detection
</p><p><b>Compressor summary</b>: The authors propose a method to detect distribution deviations in deep neural networks, which is faster and more efficient than existing methods, and can handle large datasets.</p><hr><h3>EvoFed: Leveraging Evolutionary Strategies for Communication-Efficient Federated Learning</h3>
<p>Mohammad Mahdi Rahimi, Hasnain Irshad Bhatti, Younghyun Park, Humaira Kousar, Do-Yeon Kim, Jaekyun Moon</p>
<p><a href='https://openreview.net/forum?id=P3Z59Okb5I'>https://openreview.net/forum?id=P3Z59Okb5I</a></p>
<p><b>Keywords</b>: evolutionary strategies, federated learning, gradient compression, distributed learning
</p><p><b>Compressor summary</b>: EvoFed is a novel approach for federated learning that reduces communication costs by using evolutionary strategies and fitness-based information sharing instead of exchanging model parameters.</p><hr><h3>Hierarchically Gated Recurrent Neural Network for Sequence Modeling</h3>
<p>Zhen Qin, Songlin Yang, Yiran Zhong</p>
<p><a href='https://openreview.net/forum?id=P1TCHxJwLB'>https://openreview.net/forum?id=P1TCHxJwLB</a></p>
<p><b>Keywords</b>: RNN, Sequence Modeling, NLP
</p><p><b>Compressor summary</b>: The paper introduces HGRN, a gated linear RNN model that uses forget gates with a learnable lower bound to enable efficient long-term dependency modeling in different tasks.</p><hr><h3>Differentially Private Image Classification by Learning Priors from Random Processes</h3>
<p>Xinyu Tang, Ashwinee Panda, Vikash Sehwag, Prateek Mittal</p>
<p><a href='https://openreview.net/forum?id=P0Avuii9iI'>https://openreview.net/forum?id=P0Avuii9iI</a></p>
<p><b>Keywords</b>: Differential privacy, image classification, deep learning
</p><p><b>Compressor summary</b>: DP-RandP improves privacy-utility tradeoff of DP-SGD by learning priors from random image generation and transferring them to private data, achieving state-of-the-art accuracy on various datasets.</p><hr><h3>Rethinking the Backward Propagation for Adversarial Transferability</h3>
<p>Xiaosen Wang, Kangheng Tong, Kun He</p>
<p><a href='https://openreview.net/forum?id=OzpTd2EsH1'>https://openreview.net/forum?id=OzpTd2EsH1</a></p>
<p><b>Keywords</b>: Adversarial examples, Convolutional neural networks, Adversarial transferability, Backward propagation
</p><p><b>Compressor summary</b>: The paper proposes Backward Propagation Attack (BPA), a novel method that mitigates information loss in non-linear layers to improve the transferability of adversarial examples.</p><hr><h3>Achieving $\mathcal{O}(\epsilon^{-1.5})$ Complexity in Hessian/Jacobian-free Stochastic Bilevel Optimization</h3>
<p>Yifan Yang, Peiyao Xiao, Kaiyi Ji</p>
<p><a href='https://openreview.net/forum?id=OzjBohmLvE'>https://openreview.net/forum?id=OzjBohmLvE</a></p>
<p><b>Keywords</b>: Stochastic bilevel optimization, Hessian-free algorithms, near-optimal complexity
</p><p><b>Compressor summary</b>: The paper introduces FdeHBO, a novel method for solving nonconvex-strongly-convex bilevel optimization without second-order derivative computation, achieving an $\mathcal{O}(\epsilon^{-1.5})$ sample complexity and iteration complexity.</p><hr><h3>VCC: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens</h3>
<p>Zhanpeng Zeng, Cole Hawkins, Mingyi Hong, Aston Zhang, Nikolaos Pappas, Vikas Singh, Shuai Zheng</p>
<p><a href='https://openreview.net/forum?id=Ozc8XVzwd4'>https://openreview.net/forum?id=Ozc8XVzwd4</a></p>
<p><b>Keywords</b>: efficient, transformer, roberta, T5, language modeling, question answering, summarization
</p><p><b>Compressor summary</b>: The paper proposes a method to improve the efficiency and performance of Transformers for ultra long sequences by compressing them based on VIP-tokens that are most relevant to the final prediction.</p><hr><h3>Jigsaw: Learning to Assemble Multiple Fractured Objects</h3>
<p>Jiaxin Lu, Yifan Sun, Qixing Huang</p>
<p><a href='https://openreview.net/forum?id=OwpaO4w6K7'>https://openreview.net/forum?id=OwpaO4w6K7</a></p>
<p><b>Keywords</b>: Shape Matching; Reassembly; Shape Segmentation;
</p><p><b>Compressor summary</b>: Jigsaw is a novel framework that uses hierarchical features to assemble physically broken 3D objects from multiple pieces and outperforms existing methods.</p><hr><h3>Towards Unbounded Machine Unlearning</h3>
<p>Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, Eleni Triantafillou</p>
<p><a href='https://openreview.net/forum?id=OveBaTtUAT'>https://openreview.net/forum?id=OveBaTtUAT</a></p>
<p><b>Keywords</b>: machine unlearning, deep learning
</p><p><b>Compressor summary</b>: The paper introduces SCRUB, a novel unlearning algorithm for neural networks that performs well across various applications and metrics, including privacy protection.</p><hr><h3>Importance-aware Co-teaching for Offline Model-based Optimization</h3>
<p>Ye Yuan, Can Chen, Zixuan Liu, Willie Neiswanger, Xue Liu</p>
<p><a href='https://openreview.net/forum?id=OvPnc5kVsb'>https://openreview.net/forum?id=OvPnc5kVsb</a></p>
<p><b>Keywords</b>: offline model-based optimization, co-teaching, meta-learning, sample reweighting
</p><p><b>Compressor summary</b>: ICT is a method that uses three symmetric proxies and pseudo-labeling to improve offline model-based optimization, addressing the out-of-distribution issue in gradient ascent.</p><hr><h3>Learning Multi-agent Behaviors from Distributed and Streaming Demonstrations</h3>
<p>Shicheng Liu, Minghui Zhu</p>
<p><a href='https://openreview.net/forum?id=Ou1VRZ4j4y'>https://openreview.net/forum?id=Ou1VRZ4j4y</a></p>
<p><b>Keywords</b>: inverse reinforcement learning; distributed online bi-level optimization
</p><p><b>Compressor summary</b>: The paper presents MA-BIRDS, an algorithm that infers expert behaviors from sequential demonstrations using a distributed bi-level optimization framework and achieves consensus, low regret, and sub-linear constraint violation.</p><hr><h3>Learning to Augment Distributions for Out-of-distribution Detection</h3>
<p>Qizhou Wang, Zhen Fang, Yonggang Zhang, Feng Liu, Yixuan Li, Bo Han</p>
<p><a href='https://openreview.net/forum?id=OtU6VvXJue'>https://openreview.net/forum?id=OtU6VvXJue</a></p>
<p><b>Keywords</b>: OOD Detection
</p><p><b>Compressor summary</b>: The text discusses a problem in open-world classification systems and proposes a solution called Distributional-Augmented OOD Learning (DAOL) that improves performance by reducing distribution discrepancy between auxiliary and unseen out-of-distribution data.</p><hr><h3>Weakly Supervised 3D Open-vocabulary Segmentation</h3>
<p>Kunhao Liu, Fangneng Zhan, Jiahui Zhang, MUYU XU, Yingchen Yu, Abdulmotaleb El Saddik, Christian Theobalt, Eric Xing, Shijian Lu</p>
<p><a href='https://openreview.net/forum?id=Orp1K2dZvY'>https://openreview.net/forum?id=Orp1K2dZvY</a></p>
<p><b>Keywords</b>: 3D, open-vocabulary segmentation, neural radiance field
</p><p><b>Compressor summary</b>: The paper proposes a method to learn 3D open-vocabulary segmentation from 2D images and text using pre-trained models CLIP and DINO without any manual annotations.</p><hr><h3>Modulated Neural ODEs</h3>
<p>Ilze Amanda Auzina, Cagatay Yildiz, Sara Magliacane, Matthias Bethge, Efstratios Gavves</p>
<p><a href='https://openreview.net/forum?id=Op9z2QfXbC'>https://openreview.net/forum?id=Op9z2QfXbC</a></p>
<p><b>Keywords</b>: Neural ODEs, Modulator Variables, Dynamical Systems, Disentanglment
</p><p><b>Compressor summary</b>: The paper introduces MoNODEs, a novel framework for neural ordinary differential equations that improves generalization and far-horizon forecasting by learning time-invariant modulator variables from data.</p><hr><h3>Semantic Image Synthesis with Unconditional Generator</h3>
<p>JungWoo Chae, Hyunin Cho, Sooyeon Go, Kyungmook Choi, Youngjung Uh</p>
<p><a href='https://openreview.net/forum?id=OoPLRGBKjM'>https://openreview.net/forum?id=OoPLRGBKjM</a></p>
<p><b>Keywords</b>: Generative model
</p><p><b>Compressor summary</b>: The paper presents a novel method for semantic image synthesis that uses a proxy mask derived from intermediate feature maps to guide a pretrained unconditional generator without heavy annotation.</p><hr><h3>Reading Relevant Feature from Global Representation Memory for Visual Object Tracking</h3>
<p>Xinyu Zhou, Pinxue Guo, Lingyi Hong, Jinglun Li, Wei Zhang, Weifeng Ge, Wenqiang Zhang</p>
<p><a href='https://openreview.net/forum?id=On0IDMYKw2'>https://openreview.net/forum?id=On0IDMYKw2</a></p>
<p><b>Keywords</b>: object tracking;global representation memory;transformer
</p><p><b>Compressor summary</b>: The text introduces a new tracking paradigm that uses a relevance attention mechanism and a global representation memory to select and read the most relevant historical information for visual object tracking, improving performance and reducing redundancy.</p><hr><h3>MAViL: Masked Audio-Video Learners</h3>
<p>Po-Yao Huang, Vasu Sharma, Hu Xu, Chaitanya Ryali, Haoqi Fan, Yanghao Li, Shang-Wen Li, Gargi Ghosh, Jitendra Malik, Christoph Feichtenhofer</p>
<p><a href='https://openreview.net/forum?id=OmTMaTbjac'>https://openreview.net/forum?id=OmTMaTbjac</a></p>
<p><b>Keywords</b>: self-supervised learning, audio representation learning, audio classification
</p><p><b>Compressor summary</b>: MAViL learns audio-visual representations using three forms of self-supervision, achieves state-of-the-art performance in audio-video classification and improves the quality of both modalities individually.</p><hr><h3>Federated Multi-Objective Learning</h3>
<p>Haibo Yang, Zhuqing Liu, Jia Liu, Chaosheng Dong, Michinari Momma</p>
<p><a href='https://openreview.net/forum?id=OlSTwlz96r'>https://openreview.net/forum?id=OlSTwlz96r</a></p>
<p><b>Keywords</b>: Multi-Objective Learning, Federated Learning
</p><p><b>Compressor summary</b>: The text introduces a new federated multi-objective learning framework that allows multiple clients to collaboratively solve a multi-objective optimization problem while keeping their data private and proposes two new algorithms for this framework.</p><hr><h3>KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs</h3>
<p>Lujun Li, Peijie Dong, Anggeng Li, Zimian Wei, Yang Ya</p>
<p><a href='https://openreview.net/forum?id=OlMKa5YZ8e'>https://openreview.net/forum?id=OlMKa5YZ8e</a></p>
<p><b>Keywords</b>: Knowledge distillation
</p><p><b>Compressor summary</b>: The paper introduces KD-Zero, a novel framework that uses evolutionary search to automatically discover optimal knowledge distillation designs for any teacher-student model pairs, achieving superior performance across various tasks and architectures.</p><hr><h3>Percentile Criterion Optimization in Offline Reinforcement Learning</h3>
<p>Cyrus Cousins, Elita Lobo, Marek Petrik, Yair Zick</p>
<p><a href='https://openreview.net/forum?id=OjlZqQzw51'>https://openreview.net/forum?id=OjlZqQzw51</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Bayesian Uncertainty, Robustness
</p><p><b>Compressor summary</b>: The paper proposes a new dynamic programming algorithm for reinforcement learning that optimizes the percentile criterion without explicitly creating uncertainty sets, leading to smaller sets and more efficient policies.</p><hr><h3>Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples Regularization</h3>
<p>Runqi Lin, Chaojian Yu, Tongliang Liu</p>
<p><a href='https://openreview.net/forum?id=Oj7Mrb4009'>https://openreview.net/forum?id=Oj7Mrb4009</a></p>
<p><b>Keywords</b>: adversarial training, catastrophic overfitting
</p><p><b>Compressor summary</b>: The paper proposes AAER, a method to prevent catastrophic overfitting in SSAT by regularizing the variation of abnormal adversarial examples, which are associated with classifier distortion.</p><hr><h3>Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars</h3>
<p>Kaiyue Wen, Yuchen Li, Bingbin Liu, Andrej Risteski</p>
<p><a href='https://openreview.net/forum?id=OitmaxSAUu'>https://openreview.net/forum?id=OitmaxSAUu</a></p>
<p><b>Keywords</b>: Transformer, Self Attention, Dyck Language, Context Free Grammar, Formal Language, Theory, Interpretability
</p><p><b>Compressor summary</b>: This paper critically examines transformer interpretability methods that focus only on parts of the model instead of considering it holistically, using theoretical results and synthetic data experiments.</p><hr><h3>Augmentation-Aware Self-Supervision for Data-Efficient GAN Training</h3>
<p>Liang Hou, Qi Cao, Yige Yuan, Songtao Zhao, Chongyang Ma, Siyuan Pan, Pengfei Wan, Zhongyuan Wang, Huawei Shen, Xueqi Cheng</p>
<p><a href='https://openreview.net/forum?id=OiivS2mqQf'>https://openreview.net/forum?id=OiivS2mqQf</a></p>
<p><b>Keywords</b>: generative adversarial networks, limited data, self-supervised learning
</p><p><b>Compressor summary</b>: The paper proposes a novel self-supervised discriminator that predicts the augmentation parameter and encourages the generator to produce realistic and augmentation-predictable data, improving data efficiency for training GANs with limited data.</p><hr><h3>Quantum speedups for stochastic optimization</h3>
<p>Aaron Sidford, Chenyi Zhang</p>
<p><a href='https://openreview.net/forum?id=OiatK9W6tR'>https://openreview.net/forum?id=OiatK9W6tR</a></p>
<p><b>Keywords</b>: continuous optimization, quantum algorithms, stochastic optimization, gradient oracle
</p><p><b>Compressor summary</b>: The text describes new quantum methods for minimizing a Lipschitz convex function, achieving unachievable trade-offs and optimal rates in low dimensions, as well as quantum algorithms for solving smooth non-convex functions using quantum multivariate mean estimation and variance reduction techniques.</p><hr><h3>EgoDistill: Egocentric Head Motion Distillation for Efficient Video Understanding</h3>
<p>Shuhan Tan, Tushar Nagarajan, Kristen Grauman</p>
<p><a href='https://openreview.net/forum?id=OfjVAKx44G'>https://openreview.net/forum?id=OfjVAKx44G</a></p>
<p><b>Keywords</b>: Egocentric Video; IMU; Efficient Video Understanding
</p><p><b>Compressor summary</b>: EgoDistill is a distillation-based approach that improves efficiency of ego-centric video understanding by combining semantics from sparse video frames and head motion from IMU readings, achieving state-of-the-art results on Ego4D and EPIC-Kitchens datasets.</p><hr><h3>The Transient Nature of Emergent In-Context Learning in Transformers</h3>
<p>Aaditya K Singh, Stephanie C.Y. Chan, Ted Moskovitz, Erin Grant, Andrew M Saxe, Felix Hill</p>
<p><a href='https://openreview.net/forum?id=Of0GBzow8P'>https://openreview.net/forum?id=Of0GBzow8P</a></p>
<p><b>Keywords</b>: in-context learning, transformers, emergence, transience
</p><p><b>Compressor summary</b>: The paragraph discusses how in-context learning in transformer neural networks is often transient and not persistent, raising questions about overtraining and suggesting L2 regularization as a way to achieve more lasting ICL.</p><hr><h3>Practical Differentially Private Hyperparameter Tuning with Subsampling</h3>
<p>Antti Koskela, Tejas Kulkarni</p>
<p><a href='https://openreview.net/forum?id=OeLInnFKUK'>https://openreview.net/forum?id=OeLInnFKUK</a></p>
<p><b>Keywords</b>: differential privacy, hyperparameter tuning, Rényi differential privacy, computational efficiency, DP-SGD
</p><p><b>Compressor summary</b>: The paper proposes a method to lower the privacy and compute cost of differentially private hyperparameter tuning algorithms by using a random subset of sensitive data and extrapolating optimal values.</p><hr><h3>Red Teaming Deep Neural Networks with Feature Synthesis Tools</h3>
<p>Stephen Casper, Tong Bu, Yuxiao Li, Jiawei Li, Kevin Zhang, Kaivalya Hariharan, Dylan Hadfield-Menell</p>
<p><a href='https://openreview.net/forum?id=Od6CHhPM7I'>https://openreview.net/forum?id=Od6CHhPM7I</a></p>
<p><b>Keywords</b>: interpretability, benchmarking, auditing, diagnostics, debugging, adversarial attacks, feature synthesis
</p><p><b>Compressor summary</b>: The paper introduces a benchmark with human-interpretable trojans to test interpretability tools for model debugging, showing that current methods often fail to discover them.</p><hr><h3>Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors</h3>
<p>Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, Marzyeh Ghassemi</p>
<p><a href='https://openreview.net/forum?id=Oc1SIKxwdV'>https://openreview.net/forum?id=Oc1SIKxwdV</a></p>
<p><b>Keywords</b>: Model Editing, Continual Learning, Model Repair
</p><p><b>Compressor summary</b>: GRACE is a method that allows targeted edits on deployed language models without retraining or degrading performance, using streaming errors to update a latent codebook.</p><hr><h3>The Gain from Ordering in Online Learning</h3>
<p>Vasilis Kontonis, Mingchen Ma, Christos Tzamos</p>
<p><a href='https://openreview.net/forum?id=OaUT4hX40s'>https://openreview.net/forum?id=OaUT4hX40s</a></p>
<p><b>Keywords</b>: Online Learning, Self-directed Learning, Hardness of Approximation
</p><p><b>Compressor summary</b>: The paper explores how to order datapoints in online linear regression to minimize regret and shows that efficient algorithms are hard for arbitrary datasets but can achieve a log(d)-approximation for structured ones.</p><hr><h3>SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions</h3>
<p>Yuseung Lee, Kunho Kim, Hyunjin Kim, Minhyuk Sung</p>
<p><a href='https://openreview.net/forum?id=OZEfMD7axv'>https://openreview.net/forum?id=OZEfMD7axv</a></p>
<p><b>Keywords</b>: Diffusion model, Text-to-image generation, Panorama generation
</p><p><b>Compressor summary</b>: SyncDiffusion is a module that helps generate coherent image montages by synchronizing multiple diffusions using perceptual similarity loss, and it can be applied to various image generation tasks.</p><hr><h3>Scale Alone Does not Improve Mechanistic Interpretability in Vision Models</h3>
<p>Roland S. Zimmermann, Thomas Klein, Wieland Brendel</p>
<p><a href='https://openreview.net/forum?id=OZ7aImD4uQ'>https://openreview.net/forum?id=OZ7aImD4uQ</a></p>
<p><b>Keywords</b>: feature visualization, interpretability, explainability, deep learning, neural networks, analysis, activation maximization, psychophysics
</p><p><b>Compressor summary</b>: The study finds that larger neural networks do not improve interpretability and suggests the need for more interpretable models and methods.</p><hr><h3>Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions</h3>
<p>Stefano Massaroli, Michael Poli, Daniel Y Fu, Hermann Kumbong, Rom Nishijima Parnichkun, David W. Romero, Aman Timalsina, Quinn McIntyre, Beidi Chen, Atri Rudra, Ce Zhang, Christopher Re, Stefano Ermon, Yoshua Bengio</p>
<p><a href='https://openreview.net/forum?id=OWELckerm6'>https://openreview.net/forum?id=OWELckerm6</a></p>
<p><b>Keywords</b>: Long convolutions, recurrence, attention, language models, signal processing, throughput, auto-regressive generation
</p><p><b>Compressor summary</b>: The paper proposes methods to improve the efficiency of attention-free sequence models using linear state-space models and architectural improvements like Hyena, achieving better performance than Transformers and Hyena.</p><hr><h3>QLoRA: Efficient Finetuning of Quantized LLMs</h3>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer</p>
<p><a href='https://openreview.net/forum?id=OUIFPHEgJU'>https://openreview.net/forum?id=OUIFPHEgJU</a></p>
<p><b>Keywords</b>: finetuning, llama, instructions, quantization
</p><p><b>Compressor summary</b>: QLora is a memory-efficient finetuning method that achieves near-ChatGPT performance with the Guanaco model family, while introducing new innovations to save memory without sacrificing performance.</p><hr><h3>Rigorous Runtime Analysis of MOEA/D for Solving Multi-Objective Minimum Weight Base Problems</h3>
<p>Anh Viet Do, Aneta Neumann, Frank Neumann, Andrew M. Sutton</p>
<p><a href='https://openreview.net/forum?id=ORmVvN94B9'>https://openreview.net/forum?id=ORmVvN94B9</a></p>
<p><b>Keywords</b>: minimum weight base problem, multi-objective optimization, approximation, evolutionary algorithm
</p><p><b>Compressor summary</b>: The paper studies the multi-objective minimum weight base problem, proves properties of its convex hull, and analyzes the performance of the MOEA/D algorithm on this problem compared to another algorithm called GSEMO.</p><hr><h3>Are aligned neural networks adversarially aligned?</h3>
<p>Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei Koh, Daphne Ippolito, Florian Tramèr, Ludwig Schmidt</p>
<p><a href='https://openreview.net/forum?id=OQQoD8Vc3B'>https://openreview.net/forum?id=OQQoD8Vc3B</a></p>
<p><b>Keywords</b>: Adversarial examples, large language models, alignment
</p><p><b>Compressor summary</b>: The paper investigates how well large language models can resist harmful inputs from adversarial users, and finds that multimodal models are more vulnerable than text-only models.</p><hr><h3>Robust Contrastive Language-Image Pretraining against Data Poisoning and Backdoor Attacks</h3>
<p>Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman</p>
<p><a href='https://openreview.net/forum?id=ONwL9ucoYG'>https://openreview.net/forum?id=ONwL9ucoYG</a></p>
<p><b>Keywords</b>: Contrastive Learning, Adversarial Learning, Model Robustness
</p><p><b>Compressor summary</b>: RoCLIP is a method for pre-training multimodal vision-language models that defends against targeted data poisoning and backdoor attacks by using random captions and image/text augmentations.</p><hr><h3>EICIL: Joint Excitatory Inhibitory Cycle Iteration Learning for Deep Spiking Neural Networks</h3>
<p>Zihang Shao, Xuanye Fang, Yaxin Li, Chaoran Feng, Jiangrong Shen, Qi Xu</p>
<p><a href='https://openreview.net/forum?id=OMDgOjdqoZ'>https://openreview.net/forum?id=OMDgOjdqoZ</a></p>
<p><b>Keywords</b>: spiking neural networks cycle learning  spike encoding
</p><p><b>Compressor summary</b>: EICIL is a novel learning method for spiking neural networks that mimics biological neurons by integrating excitatory and inhibitory behaviors, improving bio-mimicry and adaptability, and expanding the representation space.</p><hr><h3>An $\varepsilon$-Best-Arm Identification Algorithm for Fixed-Confidence and Beyond</h3>
<p>Marc Jourdan, Rémy Degenne, Emilie Kaufmann</p>
<p><a href='https://openreview.net/forum?id=OLk3F64eSg'>https://openreview.net/forum?id=OLk3F64eSg</a></p>
<p><b>Keywords</b>: multi-armed bandits, pure-exploration, epsilon best arm identification, Top Two algorithm, anytime
</p><p><b>Compressor summary</b>: EB-TCε is a new sampling method for finding the best arm in stochastic bandits with theoretical guarantees on its performance and simulation results showing its effectiveness.</p><hr><h3>LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation</h3>
<p>Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, William Yang Wang</p>
<p><a href='https://openreview.net/forum?id=OJ0c6um1An'>https://openreview.net/forum?id=OJ0c6um1An</a></p>
<p><b>Keywords</b>: Text-to-Image Evaluation, Visio-linguistic Compositionality, Large Language Models
</p><p><b>Compressor summary</b>: LLMScore is a new framework that uses large language models to evaluate text-to-image synthesis by considering object-level compositionality, achieving high correlation with human judgments and outperforming existing metrics.</p><hr><h3>RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability</h3>
<p>Chuning Zhu, Max Simchowitz, Siri Gadipudi, Abhishek Gupta</p>
<p><a href='https://openreview.net/forum?id=OIJ3VXDy6s'>https://openreview.net/forum?id=OIJ3VXDy6s</a></p>
<p><b>Keywords</b>: Model-Based Reinforcement Learning, Deep Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper proposes a visual model-based RL method that learns a latent representation resilient to spurious variations and shows how it can adapt to different environments without relearning the dynamics and policy.</p><hr><h3>Inserting Anybody in Diffusion Models via Celeb Basis</h3>
<p>Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang Qi, Xintao Wang, Ying Shan, Huicheng Zheng</p>
<p><a href='https://openreview.net/forum?id=OGQWZ3p0Zn'>https://openreview.net/forum?id=OGQWZ3p0Zn</a></p>
<p><b>Keywords</b>: Text-to-Image Synthesis, Personalized Synthesis, Face Embedding
</p><p><b>Compressor summary</b>: The paper proposes a new personalization method for text-to-image models that uses one facial photo and 1024 learnable parameters to integrate a unique individual into the model, enabling generation of stunning images featuring the person in various scenarios.</p><hr><h3>Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels</h3>
<p>Zifu Wang, Xuefei Ning, Matthew B. Blaschko</p>
<p><a href='https://openreview.net/forum?id=OFMPrCAMKi'>https://openreview.net/forum?id=OFMPrCAMKi</a></p>
<p><b>Keywords</b>: Semantic Segmentation
</p><p><b>Compressor summary</b>: JMLs are compatible with soft labels and improve semantic segmentation performance in various scenarios, such as label smoothing, knowledge distillation, and semi-supervised learning.</p><hr><h3>Experimental Designs for Heteroskedastic Variance</h3>
<p>Justin David Naggar Weltz, Tanner Fiez, Alexander Volfovsky, Eric Laber, Blake Mason, houssam nassif, Lalit K Jain</p>
<p><a href='https://openreview.net/forum?id=OFDApY678F'>https://openreview.net/forum?id=OFDApY678F</a></p>
<p><b>Keywords</b>: Heteroskedastic Variance, Linear Bandits, Experimental design
</p><p><b>Compressor summary</b>: The paragraph discusses a novel design for adaptive experimental design problems under heteroskedastic noise, which improves sample complexity and estimation accuracy by bounding the error of variance parameters.</p><hr><h3>A Guide Through the Zoo of Biased SGD</h3>
<p>Yury Demidovich, Grigory Malinovsky, Igor Sokolov, Peter Richtárik</p>
<p><a href='https://openreview.net/forum?id=OCtv4NyahI'>https://openreview.net/forum?id=OCtv4NyahI</a></p>
<p><b>Keywords</b>: Stochastic optimization, biased SGD, Non-convex analysis
</p><p><b>Compressor summary</b>: The paper studies SGD with biased gradient estimators, clarifies existing assumptions and relationships among them, introduces a new set of weaker assumptions, and shows advantages of biased estimators over unbiased ones in various settings using theory and experiments.</p><hr><h3>Scalable Primal-Dual Actor-Critic Method for Safe Multi-Agent RL with General Utilities</h3>
<p>Donghao Ying, YUNKAI ZHANG, Yuhao Ding, Alec Koppel, Javad Lavaei</p>
<p><a href='https://openreview.net/forum?id=O63qgtebjH'>https://openreview.net/forum?id=O63qgtebjH</a></p>
<p><b>Keywords</b>: Reinforcement Learning Theory, Safe reinforcement learning, Multi-agent reinforcement learning
</p><p><b>Compressor summary</b>: The paper presents a safe multi-agent reinforcement learning method with general utilities and safety constraints, and proves its convergence and sample efficiency. It also demonstrates its effectiveness in simulations.</p><hr><h3>Training biologically plausible recurrent neural networks on cognitive tasks with long-term dependencies</h3>
<p>Wayne WM Soo, Vishwa Goudar, Xiao-Jing Wang</p>
<p><a href='https://openreview.net/forum?id=O453PHSthc'>https://openreview.net/forum?id=O453PHSthc</a></p>
<p><b>Keywords</b>: neuroscience, recurrent neural network, neural circuits, cortical circuits, cognitive tasks, working memory
</p><p><b>Compressor summary</b>: The authors propose a method to train recurrent neural networks (RNNs) for cognitive tasks with long temporal dependencies by adding specialized skip-connections through time and revert to the original architecture, enabling RNNs to learn tasks that are difficult or impossible using conventional methods.</p><hr><h3>S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions</h3>
<p>Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jinwoo Shin</p>
<p><a href='https://openreview.net/forum?id=O1lYncfVOO'>https://openreview.net/forum?id=O1lYncfVOO</a></p>
<p><b>Keywords</b>: vision-language model, semi-supervised learning, specialist domain
</p><p><b>Compressor summary</b>: S-CLIP is a semi-supervised learning method that uses unpaired images and pseudo-labeling strategies to improve vision-language models like CLIP in specialized domains with limited training data.</p><hr><h3>A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning</h3>
<p>Alicia Curth, Alan Jeffares, Mihaela van der Schaar</p>
<p><a href='https://openreview.net/forum?id=O0Lz8XZT2b'>https://openreview.net/forum?id=O0Lz8XZT2b</a></p>
<p><b>Keywords</b>: Double Descent, Statistical Machine Learning, Interpolation Regime, Effective Parameters
</p><p><b>Compressor summary</b>: The paragraph discusses how recent work on the relationship between model complexity and prediction error has suggested a new phenomenon called double descent, but argues that this is not incompatible with conventional statistical wisdom if one considers multiple axes of complexity.</p><hr><h3>Evaluating the Moral Beliefs Encoded in LLMs</h3>
<p>Nino Scherrer, Claudia Shi, Amir Feder, David Blei</p>
<p><a href='https://openreview.net/forum?id=O06z2G18me'>https://openreview.net/forum?id=O06z2G18me</a></p>
<p><b>Keywords</b>: Language Models, Moral Decision Making, Social Aspects of Machine Learning, Ethics
</p><p><b>Compressor summary</b>: The paper investigates how LLMs make moral choices and express uncertainty in ambiguous situations using a large survey with moral dilemmas.</p><hr><h3>Information Design in Multi-Agent Reinforcement Learning</h3>
<p>Yue Lin, Wenhao Li, Hongyuan Zha, Baoxiang Wang</p>
<p><a href='https://openreview.net/forum?id=NyQwBttTnG'>https://openreview.net/forum?id=NyQwBttTnG</a></p>
<p><b>Keywords</b>: multi-agent reinforcement learning, multi-agent communication, information design, signaling gradient, obedience constraints
</p><p><b>Compressor summary</b>: The paragraph discusses how reinforcement learning agents can influence each other through information design, addressing challenges like non-stationarity and information obedience in a Markov signaling game framework.</p><hr><h3>Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents</h3>
<p>Wonje Choi, Woo Kyung Kim, SeungHyun Kim, Honguk Woo</p>
<p><a href='https://openreview.net/forum?id=Ny3GcHLyzj'>https://openreview.net/forum?id=Ny3GcHLyzj</a></p>
<p><b>Keywords</b>: Prompt Learining, Domain Adaptation, Embodied AI
</p><p><b>Compressor summary</b>: ConPE is a novel framework that uses visual prompts and a pretrained vision-language model to enable efficient policy learning and adaptation for embodied agents in various environments.</p><hr><h3>Sheaf Hypergraph Networks</h3>
<p>Iulia Duta, Giulia Cassarà, Fabrizio Silvestri, Pietro Lio</p>
<p><a href='https://openreview.net/forum?id=NvcVXzJvhX'>https://openreview.net/forum?id=NvcVXzJvhX</a></p>
<p><b>Keywords</b>: hypergraph neural networks, hypergraph, sheaf, higher-order
</p><p><b>Compressor summary</b>: The authors propose cellular sheaves for hypergraphs to enhance representation of complex interactions and develop two types of models that improve hypergraph node classification performance.</p><hr><h3>Label-Only Model Inversion Attacks via Knowledge Transfer</h3>
<p>Ngoc-Bao Nguyen, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, Ngai-man Cheung</p>
<p><a href='https://openreview.net/forum?id=NuoIThPPag'>https://openreview.net/forum?id=NuoIThPPag</a></p>
<p><b>Keywords</b>: Model Inversion attacks, Generative models, Surrogate models, Knowledge transfer
</p><p><b>Compressor summary</b>: The authors propose LOKT, a method to perform label-only MI attacks using surrogate models and knowledge transfer from the target model, achieving significant improvements over existing methods.</p><hr><h3>Blocked Collaborative Bandits: Online Collaborative Filtering with Per-Item Budget Constraints</h3>
<p>Soumyabrata Pal, Arun Suggala, Karthikeyan Shanmugam, Prateek Jain</p>
<p><a href='https://openreview.net/forum?id=Ntd6X7uWYF'>https://openreview.net/forum?id=Ntd6X7uWYF</a></p>
<p><b>Keywords</b>: Blocked Bandits, Collaborative Filtering, Clustering
</p><p><b>Compressor summary</b>: B-LATTICE is a novel algorithm that efficiently solves the blocked collaborative bandit problem by clustering users with similar rewards, collaborating across clusters, and respecting the arm sampling budget.</p><hr><h3>Reverse Engineering Self-Supervised Learning</h3>
<p>Ido Ben-Shaul, Ravid Shwartz-Ziv, Tomer Galanti, Shai Dekel, Yann LeCun</p>
<p><a href='https://openreview.net/forum?id=NsVEjx6YPd'>https://openreview.net/forum?id=NsVEjx6YPd</a></p>
<p><b>Keywords</b>: Self-Supervised Learning, Deep Learning, Representation Learning
</p><p><b>Compressor summary</b>: The paper analyzes self-supervised learning models to reveal that they naturally form semantic label-based clusters, which improve downstream tasks and information compression.</p><hr><h3>On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence</h3>
<p>Achraf Azize, Marc Jourdan, Aymen Al Marjani, Debabrota Basu</p>
<p><a href='https://openreview.net/forum?id=NsPbMwyxRl'>https://openreview.net/forum?id=NsPbMwyxRl</a></p>
<p><b>Keywords</b>: Differential Privacy, Multi-armed Bandits, Best Arm Identification, Fixed Confidence
</p><p><b>Compressor summary</b>: The authors study best arm identification problems under differential privacy and propose a new algorithm (AdaP-TT) with a good trade-off between privacy and utility.</p><hr><h3>On the Convergence to a Global Solution of Shuffling-Type Gradient Algorithms</h3>
<p>Lam M. Nguyen, Trang H. Tran</p>
<p><a href='https://openreview.net/forum?id=Nr1XSeDzpn'>https://openreview.net/forum?id=Nr1XSeDzpn</a></p>
<p><b>Keywords</b>: stochastic gradient, shuffling type gradient method, global convergence
</p><p><b>Compressor summary</b>: Shuffling SGD, a scalable and efficient machine learning method, can converge to global solutions for certain non-convex functions even when over-parameterized.</p><hr><h3>Distributionally Robust Skeleton Learning of Discrete Bayesian Networks</h3>
<p>Yeshu Li, Brian D Ziebart</p>
<p><a href='https://openreview.net/forum?id=NpyZkaEEun'>https://openreview.net/forum?id=NpyZkaEEun</a></p>
<p><b>Keywords</b>: structure learning, Bayesian network, robustness
</p><p><b>Compressor summary</b>: The paper proposes a method to learn the structure of discrete Bayesian networks from potentially corrupted data using distributionally robust optimization and regression, with guarantees for successful learning and efficient algorithms.</p><hr><h3>IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers</h3>
<p>Zhenglin Huang, Xiaoan Bao, Na Zhang, Qingqi Zhang, Xiao mei Tu, Biao Wu, Xi Yang</p>
<p><a href='https://openreview.net/forum?id=No52399wXA'>https://openreview.net/forum?id=No52399wXA</a></p>
<p><b>Keywords</b>: data augmentation, robustness, safety
</p><p><b>Compressor summary</b>: IPMix is a data augmentation technique that improves robustness of deep neural networks without sacrificing clean accuracy by applying image-level, patch-level, and pixel-level augmentations with structural complexity and random mixing.</p><hr><h3>Human spatiotemporal pattern learning as probabilistic program synthesis</h3>
<p>Tracey Mills, Joshua B. Tenenbaum, Samuel J Cheyette</p>
<p><a href='https://openreview.net/forum?id=NnXznLurw5'>https://openreview.net/forum?id=NnXznLurw5</a></p>
<p><b>Keywords</b>: pattern learning; probabilistic programs; program synthesis; gaussian process; human learning
</p><p><b>Compressor summary</b>: The paragraph discusses a study on how humans learn structured patterns from small amounts of data and suggests that people might "learn by programming" using probabilistic models, with the best fit being a structured "Language of Thought" model.</p><hr><h3>Understanding Diffusion Objectives as the ELBO with Simple Data Augmentation</h3>
<p>Diederik P Kingma, Ruiqi Gao</p>
<p><a href='https://openreview.net/forum?id=NnMEadcdyD'>https://openreview.net/forum?id=NnMEadcdyD</a></p>
<p><b>Keywords</b>: Diffusion Model, Evidence Lower Bound, Maximum Likelihood
</p><p><b>Compressor summary</b>: This paper shows that diffusion model objectives are closely related to the ELBO and explores new monotonic weightings for better performance.</p><hr><h3>Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models</h3>
<p>Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, WUYOU XIAO, Rui Zhao, Shuning Chang, Weijia Wu, Yixiao Ge, Ying Shan, Mike Zheng Shou</p>
<p><a href='https://openreview.net/forum?id=NnIaEaBfXD'>https://openreview.net/forum?id=NnIaEaBfXD</a></p>
<p><b>Keywords</b>: Text-to-Image Diffusion Models, Concept Customization
</p><p><b>Compressor summary</b>: The paper proposes Mix-of-Show, a framework for decentralized multi-concept customization in large-scale text-to-image diffusion models, which uses embedding-decomposed LoRAs and regionally controllable sampling to preserve concept identity and handle attribute binding.</p><hr><h3>Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control</h3>
<p>Nathan Rahn, Pierluca D'Oro, Harley Wiltzer, Pierre-Luc Bacon, Marc G Bellemare</p>
<p><a href='https://openreview.net/forum?id=Nn0daSf6CW'>https://openreview.net/forum?id=Nn0daSf6CW</a></p>
<p><b>Keywords</b>: deep reinforcement learning, continuous control, return landscape, stability
</p><p><b>Compressor summary</b>: The authors study how deep reinforcement learning agents for continuous control behave over time and propose a method to improve their stability by navigating the return landscape.</p><hr><h3>Towards Generic Semi-Supervised Framework for Volumetric Medical Image Segmentation</h3>
<p>Haonan Wang, Xiaomeng Li</p>
<p><a href='https://openreview.net/forum?id=NibgkUin5n'>https://openreview.net/forum?id=NibgkUin5n</a></p>
<p><b>Keywords</b>: Volumetric Medical Image Segmentation, Semi-supervised Learning, Unsupervised Domain Adaptation, Semi-supervised Domain Generalization
</p><p><b>Compressor summary</b>: The paper proposes a new semi-supervised learning framework that can handle different settings, such as unsupervised domain adaptation and semi-supervised domain generalization, by addressing the issues of capturing distribution-invariant features and avoiding over-fitting to labeled data.</p><hr><h3>Lexinvariant Language Models</h3>
<p>Qian Huang, Eric Zelikman, Sarah Li Chen, Yuhuai Wu, Gregory Valiant, Percy Liang</p>
<p><a href='https://openreview.net/forum?id=NiQTy0NW1L'>https://openreview.net/forum?id=NiQTy0NW1L</a></p>
<p><b>Keywords</b>: Large Language Model, in-context learning, pretraining
</p><p><b>Compressor summary</b>: The paper explores lexinvariant language models that do not use fixed token embeddings but rely on context to determine token meanings, and shows their performance, properties, and potential applications.</p><hr><h3>Improving neural network representations using human similarity judgments</h3>
<p>Lukas Muttenthaler, Lorenz Linhardt, Jonas Dippel, Robert A. Vandermeulen, Katherine Hermann, Andrew Kyle Lampinen, Simon Kornblith</p>
<p><a href='https://openreview.net/forum?id=Nh5dp6Uuvx'>https://openreview.net/forum?id=Nh5dp6Uuvx</a></p>
<p><b>Keywords</b>: representational alignment; human similarity judgments; neural networks; representation learning; few-shot learning; anomaly detection
</p><p><b>Compressor summary</b>: The authors explore how supervising the global structure of deep neural networks' representations with human similarity judgments can improve their performance on computer vision tasks such as few-shot learning and anomaly detection.</p><hr><h3>Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders</h3>
<p>Jan Dubiński, Stanisław Pawlak, Franziska Boenisch, Tomasz Trzcinski, Adam Dziedzic</p>
<p><a href='https://openreview.net/forum?id=NfpYgGZC3B'>https://openreview.net/forum?id=NfpYgGZC3B</a></p>
<p><b>Keywords</b>: model stealing, model defenses, self-supervised learning
</p><p><b>Compressor summary</b>: B4B is a novel defense for MLaaS APIs that protects against model stealing attacks by dynamically changing the quality and uniqueness of vector representations for each user.</p><hr><h3>GNeSF: Generalizable Neural Semantic Fields</h3>
<p>Hanlin Chen, Chen Li, Mengqi Guo, Zhiwen Yan, Gim Hee Lee</p>
<p><a href='https://openreview.net/forum?id=NemifGnD2E'>https://openreview.net/forum?id=NemifGnD2E</a></p>
<p><b>Keywords</b>: NeRF; Semantic Segmentation; 3D vision; Scene understanding; Generalizable
</p><p><b>Compressor summary</b>: The authors propose a generalizable 3D segmentation framework based on neural implicit representation that uses multi-view image features and semantic maps as inputs, and achieves comparable or better performance than existing methods with only 2D supervision.</p><hr><h3>Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning</h3>
<p>Gen Li, Wenhao Zhan, Jason D. Lee, Yuejie Chi, Yuxin Chen</p>
<p><a href='https://openreview.net/forum?id=Nd3FennRJZ'>https://openreview.net/forum?id=Nd3FennRJZ</a></p>
<p><b>Keywords</b>: reward-agnostic reinforcement learning, policy finetuning, offline reinforcement learning, online reinforcement learning
</p><p><b>Compressor summary</b>: The paper proposes a hybrid RL algorithm that combines tabular methods with reward-agnostic exploration and offline RL, achieving better sample complexity than pure offline or online RL methods without needing reward information during data collection.</p><hr><h3>Minimum Description Length and Generalization Guarantees for Representation Learning</h3>
<p>Milad Sefidgaran, Abdellatif Zaidi, Piotr Krasnowski</p>
<p><a href='https://openreview.net/forum?id=Ncb0MvVqRV'>https://openreview.net/forum?id=Ncb0MvVqRV</a></p>
<p><b>Keywords</b>: Information Bottleneck, Representation Learning, Generalization Error, Minimum Description Length
</p><p><b>Compressor summary</b>: The paper proposes an information-theoretic framework to derive upper bounds on generalization error for representation learning algorithms, using the Minimum Description Length of labels or latent variables.</p><hr><h3>Provably Robust Temporal Difference Learning for Heavy-Tailed Rewards</h3>
<p>Semih Cayci, Atilla Eryilmaz</p>
<p><a href='https://openreview.net/forum?id=NapL36HSBT'>https://openreview.net/forum?id=NapL36HSBT</a></p>
<p><b>Keywords</b>: temporal difference learning, natural actor-critic, reinforcement learning, policy evaluation, policy gradient, markov decision processes
</p><p><b>Compressor summary</b>: This paper proposes a dynamic gradient clipping mechanism for robustifying temporal difference and natural actor-critic methods against heavy-tailed rewards in reinforcement learning, and proves their sample complexities under certain assumptions.</p><hr><h3>Geometric Neural Diffusion Processes</h3>
<p>Emile Mathieu, Vincent Dutordoir, Michael John Hutchinson, Valentin De Bortoli, Yee Whye Teh, Richard E Turner</p>
<p><a href='https://openreview.net/forum?id=NaYAsbv2jF'>https://openreview.net/forum?id=NaYAsbv2jF</a></p>
<p><b>Keywords</b>: diffusion model, functional space, stochastic process, time-series, neural processes, Gaussian processes, random fields, invariance, equivariance, symmetries, stationarity
</p><p><b>Compressor summary</b>: The authors propose a geometric diffusion model that incorporates symmetries in infinite-dimensional modelling and show its applicability to weather data.</p><hr><h3>Disambiguated Attention Embedding for Multi-Instance Partial-Label Learning</h3>
<p>Wei Tang, Weijia Zhang, Min-Ling Zhang</p>
<p><a href='https://openreview.net/forum?id=NYwbmCrrni'>https://openreview.net/forum?id=NYwbmCrrni</a></p>
<p><b>Keywords</b>: Machine Learning, Multi-Instance Partial-Label Learning, Multi-Instance Learning, Partial-Label Learning
</p><p><b>Compressor summary</b>: DEMIPL is a novel algorithm that uses disambiguation attention to embed multi-instance bags into single vectors, improving performance in Multi-Instance Partial-Label Learning tasks for colorectal cancer classification.</p><hr><h3>Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP</h3>
<p>Qi Qian, Yuanhong Xu, Juhua Hu</p>
<p><a href='https://openreview.net/forum?id=NXLjaYdgaL'>https://openreview.net/forum?id=NXLjaYdgaL</a></p>
<p><b>Keywords</b>: zero-shot; clip; proxy learning
</p><p><b>Compressor summary</b>: The paper proposes InMaP, a method to learn vision proxies from unlabeled data using text proxies and refining pseudo labels for zero-shot transfer tasks.</p><hr><h3>Moment Matching Denoising Gibbs Sampling</h3>
<p>Mingtian Zhang, Alex Hawkins-Hooker, Brooks Paige, David Barber</p>
<p><a href='https://openreview.net/forum?id=NWrN6cMG2x'>https://openreview.net/forum?id=NWrN6cMG2x</a></p>
<p><b>Keywords</b>: denoising score-matching, gibbs sampling, diffusion model
</p><p><b>Compressor summary</b>: The authors propose a framework for efficient sampling from EBMs trained with DSM, which addresses the inconsistency issues of DSM and allows for scalability to high-dimensional data.</p><hr><h3>Prefix-Tree Decoding for Predicting Mass Spectra from Molecules</h3>
<p>Samuel Goldman, John Bradshaw, Jiayi Xin, Connor W. Coley</p>
<p><a href='https://openreview.net/forum?id=NWEbeI2HNQ'>https://openreview.net/forum?id=NWEbeI2HNQ</a></p>
<p><b>Keywords</b>: molecules, prefix tree, mass spectra, mass spectrum prediction, metabolomics, GNNs, chemistry, biology
</p><p><b>Compressor summary</b>: The paper presents a new method for predicting mass spectra from molecules by treating them as sets of subformulae and using a prefix tree structure to decode them, improving on existing limitations in computational tools.</p><hr><h3>SceneScape: Text-Driven Consistent Scene Generation</h3>
<p>Rafail Fridman, Amit Abecasis, Yoni Kasten, Tali Dekel</p>
<p><a href='https://openreview.net/forum?id=NU2kGsA4TT'>https://openreview.net/forum?id=NU2kGsA4TT</a></p>
<p><b>Keywords</b>: Computer Vision, Image & Video Editing, Video Generation, Perpetual View Generation, Texture Synthesis & Inpainting
</p><p><b>Compressor summary</b>: The paragraph describes a method for generating realistic long-term videos of various scenes based on text prompts and camera poses, using a combination of text-to-image and depth prediction models with online test-time training to ensure 3D consistency.</p><hr><h3>First- and Second-Order Bounds for Adversarial Linear Contextual Bandits</h3>
<p>Julia Olkhovskaya, Jack Mayo, Tim van Erven, Gergely Neu, Chen-Yu Wei</p>
<p><a href='https://openreview.net/forum?id=NTSbj2otOA'>https://openreview.net/forum?id=NTSbj2otOA</a></p>
<p><b>Keywords</b>: contextual bandits, bandits, sequential learning, regret bounds
</p><p><b>Compressor summary</b>: The paper studies an online learning problem where the loss functions for each arm change over time and gives improved bounds on regret using a modified continuous exponential weights algorithm.</p><hr><h3>Geometric Analysis of Matrix Sensing over Graphs</h3>
<p>Haixiang Zhang, Ying Chen, Javad Lavaei</p>
<p><a href='https://openreview.net/forum?id=NRnm5xO8Hz'>https://openreview.net/forum?id=NRnm5xO8Hz</a></p>
<p><b>Keywords</b>: Low-rank matrix optimization, non-convex optimization
</p><p><b>Compressor summary</b>: This paper introduces a new condition called the $\Omega$-RIP condition for matrix sensing over graphs and proves that it leads to polynomial-time global convergence of saddle-avoiding methods.</p><hr><h3>Fairly Recommending with Social Attributes: A Flexible and Controllable Optimization Approach</h3>
<p>Jinqiu Jin, Haoxuan Li, Fuli Feng, Sihao Ding, Peng Wu, Xiangnan He</p>
<p><a href='https://openreview.net/forum?id=NP5xb00Y6a'>https://openreview.net/forum?id=NP5xb00Y6a</a></p>
<p><b>Keywords</b>: Recommender System, Fairness
</p><p><b>Compressor summary</b>: This paper proposes social attribute-aware item group fairness metrics for recommender systems, and develops a gradient-based optimization algorithm to balance direct and social utility in training such models.</p><hr><h3>Efficient Low-rank Backpropagation for Vision Transformer Adaptation</h3>
<p>Yuedong Yang, Hung-Yueh Chiang, Guihong Li, Diana Marculescu, Radu Marculescu</p>
<p><a href='https://openreview.net/forum?id=NNtsO5L27J'>https://openreview.net/forum?id=NNtsO5L27J</a></p>
<p><b>Keywords</b>: Low-rank backpropagation, model adaptation, transfer learning, vision transformer, Edge AI
</p><p><b>Compressor summary</b>: LBP-WHT is a new method that projects gradients into a low-rank space, reducing the computation needed for adapting large vision transformers (ViT) models and improving their accuracy on various datasets.</p><hr><h3>To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning</h3>
<p>Ildus Sadrtdinov, Dmitrii Pozdeev, Dmitry P. Vetrov, Ekaterina Lobacheva</p>
<p><a href='https://openreview.net/forum?id=NNooZoQpP4'>https://openreview.net/forum?id=NNooZoQpP4</a></p>
<p><b>Keywords</b>: ensembles, transfer learning, loss landscape basins, model soups
</p><p><b>Compressor summary</b>: StarSSE is a modified Snapshot Ensembles method that improves the performance and diversity of neural network ensembles trained from a single pre-trained checkpoint by better exploring the pre-train basin without losing the benefits of transfer learning.</p><hr><h3>Extracting Reward Functions from Diffusion Models</h3>
<p>Felipe Pinto Coelho Nuti, Tim Franzmeyer, Joao F. Henriques</p>
<p><a href='https://openreview.net/forum?id=NN60HKTur2'>https://openreview.net/forum?id=NN60HKTur2</a></p>
<p><b>Keywords</b>: Diffusion models, sequential decision making, inverse reinforcement learning
</p><p><b>Compressor summary</b>: The authors propose a method to extract a reward function from two diffusion models with different behavior, and show its effectiveness in navigation and image generation tasks.</p><hr><h3>Policy Gradient for Rectangular Robust Markov Decision Processes</h3>
<p>Navdeep Kumar, Esther Derman, Matthieu Geist, Kfir Yehuda Levy, Shie Mannor</p>
<p><a href='https://openreview.net/forum?id=NLpXRrjpa6'>https://openreview.net/forum?id=NLpXRrjpa6</a></p>
<p><b>Keywords</b>: robust Markov decision process, policy gradient
</p><p><b>Compressor summary</b>: The paper introduces robust policy gradient (RPG), a method to train reinforcement learning agents that account for transition uncertainty and is efficient in computation.</p><hr><h3>Understanding the Limitations of Deep Models for Molecular property prediction: Insights and Solutions</h3>
<p>Jun Xia, Lecheng Zhang, Xiao Zhu, Yue Liu, Zhangyang Gao, Bozhen Hu, Cheng Tan, Jiangbin Zheng, Siyuan Li, Stan Z. Li</p>
<p><a href='https://openreview.net/forum?id=NLFqlDeuzt'>https://openreview.net/forum?id=NLFqlDeuzt</a></p>
<p><b>Keywords</b>: Graph Neural Networks
</p><p><b>Compressor summary</b>: The study compares 12 models on MPP tasks, finding that deep models generally underperform non-deep ones, and proposes a feature mapping method to improve their performance.</p><hr><h3>Latent Diffusion for Language Generation</h3>
<p>Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Seo Shekhtman, Kilian Q Weinberger</p>
<p><a href='https://openreview.net/forum?id=NKdtztladR'>https://openreview.net/forum?id=NKdtztladR</a></p>
<p><b>Keywords</b>: diffusion, language generation
</p><p><b>Compressor summary</b>: The paragraph discusses a new method for language generation using encoder-decoder language models and continuous diffusion models in the latent space, which improves over previous diffusion language models on various data sets.</p><hr><h3>Robust low-rank training via approximate orthonormal constraints</h3>
<p>Dayana Savostianova, Emanuele Zangrando, Gianluca Ceruti, Francesco Tudisco</p>
<p><a href='https://openreview.net/forum?id=NJPSvv0u3R'>https://openreview.net/forum?id=NJPSvv0u3R</a></p>
<p><b>Keywords</b>: low-rank neural networks, Stiefel manifold, orthogonal neural networks, pruning, adversarial robustness, neural network condition number, neural network singular values
</p><p><b>Compressor summary</b>: The authors propose a new method to prune neural networks while maintaining accuracy, robustness, and efficiency in the face of adversarial attacks.</p><hr><h3>Variational Gaussian processes for linear inverse problems</h3>
<p>Thibault Christophe RANDRIANARISOA, Botond Szabo</p>
<p><a href='https://openreview.net/forum?id=NJK3aSB0z4'>https://openreview.net/forum?id=NJK3aSB0z4</a></p>
<p><b>Keywords</b>: Linear inverse problems, Gaussian processes, Variational inference, Inducing variables, Asymptotics, Contraction rates
</p><p><b>Compressor summary</b>: The paper analyzes variational Bayesian methods for Gaussian process priors to solve linear inverse problems with different levels of ill-posedness and shows that inducing variable methods can attain the minimax estimation rate.</p><hr><h3>Boundary Guided Learning-Free Semantic Control with Diffusion Models</h3>
<p>Ye Zhu, Yu Wu, Zhiwei Deng, Olga Russakovsky, Yan Yan</p>
<p><a href='https://openreview.net/forum?id=NIrTSCiIZ7'>https://openreview.net/forum?id=NIrTSCiIZ7</a></p>
<p><b>Keywords</b>: Diffusion probabilistic models, learning-free applications, high-dimensional semantic boundary, markov mixing
</p><p><b>Compressor summary</b>: BoundaryDiffusion is a learning-free method for efficient and effective image semantic editing using pre-trained generative denoising diffusion models without fine-tuning or extra networks.</p><hr><h3>Zero-sum Polymatrix Markov Games: Equilibrium Collapse and Efficient Computation of Nash Equilibria</h3>
<p>Fivos Kalogiannis, Ioannis Panageas</p>
<p><a href='https://openreview.net/forum?id=NGiq8qCQNk'>https://openreview.net/forum?id=NGiq8qCQNk</a></p>
<p><b>Keywords</b>: network games, Nash equilibrium, equilibrium, game theory, learning
</p><p><b>Compressor summary</b>: The paper proposes a class of zero-sum multi-agent Markov games with pairwise interactions and shows how to efficiently find an approximate Nash equilibrium using generalized techniques from previous work on coarse-correlated equilibria.</p><hr><h3>Monte Carlo Tree Search with Boltzmann Exploration</h3>
<p>Michael Painter, Mohamed Baioumy, Nick Hawes, Bruno Lacerda</p>
<p><a href='https://openreview.net/forum?id=NG4DaApavi'>https://openreview.net/forum?id=NG4DaApavi</a></p>
<p><b>Keywords</b>: Monte Carlo Tree Search, Planning, Entropy, Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper introduces two improved MCTS methods (BTS and DENTS) that balance exploration and exploitation better than previous methods (MENTS and UCT) in automated planning problems like the game of Go.</p><hr><h3>Frequency Domain-Based Dataset Distillation</h3>
<p>DongHyeok Shin, Seungjae Shin, Il-chul Moon</p>
<p><a href='https://openreview.net/forum?id=NEawU0TgKG'>https://openreview.net/forum?id=NEawU0TgKG</a></p>
<p><b>Keywords</b>: Dataset distillation, Frequency domain, Dataset condensation
</p><p><b>Compressor summary</b>: FreD is a new method to create smaller synthetic datasets from large ones by optimizing frequency representations and selecting relevant dimensions, improving distillation performance and preserving original data information.</p><hr><h3>Alternation makes the adversary weaker in two-player games</h3>
<p>Volkan Cevher, Ashok Cutkosky, Ali Kavis, Georgios Piliouras, Stratis Skoulakis, Luca Viano</p>
<p><a href='https://openreview.net/forum?id=NBMIsOS6B7'>https://openreview.net/forum?id=NBMIsOS6B7</a></p>
<p><b>Keywords</b>: Online Learning, Regret Minimization, Game Theory
</p><p><b>Compressor summary</b>: The paper proposes and analyzes two algorithms for alternating online linear optimization that achieve sub-linear regrets, depending on the problem domain and the number of actions.</p><hr><h3>SLaM: Student-Label Mixing for  Distillation with Unlabeled Examples</h3>
<p>Vasilis Kontonis, Fotis Iliopoulos, Khoa Trinh, Cenk Baykal, Gaurav Menghani, Erik Vee</p>
<p><a href='https://openreview.net/forum?id=N7tw0QXx3z'>https://openreview.net/forum?id=N7tw0QXx3z</a></p>
<p><b>Keywords</b>: Distillation, teacher, student
</p><p><b>Compressor summary</b>: The paper proposes SLaM, a method to improve knowledge distillation with unlabeled data by mixing student and teacher labels, and provides theoretical guarantees and empirical results.</p><hr><h3>Online Learning under Adversarial Nonlinear Constraints</h3>
<p>Pavel Kolev, Georg Martius, Michael Muehlebach</p>
<p><a href='https://openreview.net/forum?id=N6YNe4KxDc'>https://openreview.net/forum?id=N6YNe4KxDc</a></p>
<p><b>Keywords</b>: online learning, online convex optimization, constrained optimization, adversarial nonlinear constraints, constraint violation oracle
</p><p><b>Compressor summary</b>: The CVV-Pro algorithm can learn from continuous non-stationary data streams with adversarial and time-varying constraints by using local sparse linear approximations of the feasible set.</p><hr><h3>Label Robust and Differentially Private Linear Regression: Computational and Statistical Efficiency</h3>
<p>Xiyang Liu, Prateek Jain, Weihao Kong, Sewoong Oh, Arun Suggala</p>
<p><a href='https://openreview.net/forum?id=N6FhEMnxCU'>https://openreview.net/forum?id=N6FhEMnxCU</a></p>
<p><b>Keywords</b>: Differential Privacy; Private Estimation
</p><p><b>Compressor summary</b>: The paper proposes an efficient linear regression method under differential privacy that works well even when some data points are corrupted or in the no-corruption setting.</p><hr><h3>Understanding and Improving Ensemble Adversarial Defense</h3>
<p>Yian Deng, Tingting Mu</p>
<p><a href='https://openreview.net/forum?id=N5uUTWLz0E'>https://openreview.net/forum?id=N5uUTWLz0E</a></p>
<p><b>Keywords</b>: adversarial defense, ensemble diversity, robustness, curvature
</p><p><b>Compressor summary</b>: The paragraph describes a new error theory for ensemble adversarial defense, which improves robustness by selectively allocating challenging examples to base classifiers and addressing their weaknesses.</p><hr><h3>PackQViT: Faster Sub-8-bit Vision Transformers via Full and Packed Quantization on the Mobile</h3>
<p>Peiyan Dong, LEI LU, Chao Wu, Cheng Lyu, Geng Yuan, Hao Tang, Yanzhi Wang</p>
<p><a href='https://openreview.net/forum?id=N56hAiQvot'>https://openreview.net/forum?id=N56hAiQvot</a></p>
<p><b>Keywords</b>: Vision Transformers, Quantization, Real-time on mobile, Sub-8-bit
</p><p><b>Compressor summary</b>: The paper proposes PackQViT, a framework for efficiently and accurately accelerating ViTs on mobile devices using sub-8-bit quantization and activation-aware training techniques.</p><hr><h3>Neural Multi-Objective Combinatorial Optimization with Diversity Enhancement</h3>
<p>Jinbiao Chen, Zizhen Zhang, Zhiguang Cao, Yaoxin Wu, Yining Ma, Te Ye, Jiahai Wang</p>
<p><a href='https://openreview.net/forum?id=N4JkStI1fe'>https://openreview.net/forum?id=N4JkStI1fe</a></p>
<p><b>Keywords</b>: neural heuristic, diversity enhancement, deep reinforcement learning, multi-objective combinatorial optimization
</p><p><b>Compressor summary</b>: NHDE is a novel neural heuristic method that uses deep reinforcement learning and graph attention to generate diverse Pareto solutions for multi-objective combinatorial optimization problems, outperforming existing decomposition-based approaches.</p><hr><h3>Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization</h3>
<p>Sanath Kumar Krishnamurthy, Ruohan Zhan, Susan Athey, Emma Brunskill</p>
<p><a href='https://openreview.net/forum?id=N1feehMSG9'>https://openreview.net/forum?id=N1feehMSG9</a></p>
<p><b>Keywords</b>: Contextual Bandits; Adaptive Experimentation; Simple Regret; Reinforcement Learning
</p><p><b>Compressor summary</b>: The authors propose efficient bandit algorithms for learning optimal treatment policies in stochastic settings, using conformal arm sets to balance simple and cumulative regret guarantees.</p><hr><h3>Active Learning-Based Species Range Estimation</h3>
<p>Christian Lange, Elijah Cole, Grant Van Horn, Oisin Mac Aodha</p>
<p><a href='https://openreview.net/forum?id=N0m9c0FqUV'>https://openreview.net/forum?id=N0m9c0FqUV</a></p>
<p><b>Keywords</b>: species range estimation, active learning, implicit networks
</p><p><b>Compressor summary</b>: The paper proposes an active learning method that uses transfer learned spatial representations to estimate the geographic range of unmapped species from limited observations and crowdsourced data.</p><hr><h3>Theoretical Analysis of the Inductive Biases in Deep Convolutional Networks</h3>
<p>Zihao Wang, Lei Wu</p>
<p><a href='https://openreview.net/forum?id=N0KwVdaaaJ'>https://openreview.net/forum?id=N0KwVdaaaJ</a></p>
<p><b>Keywords</b>: Convolutional neural network, Inductive bias, Universality, Sparse function, Equivariance group
</p><p><b>Compressor summary</b>: This paper analyzes CNNs' inductive biases, shows they can approximate any continuous function with depth $\mathcal{O}(\log d)$, and reveals the importance of weight sharing and locality for efficient learning.</p><hr><h3>A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation</h3>
<p>Thomas FEL, Victor Boutin, Louis Béthune, Remi Cadene, Mazda Moayeri, Léo Andéol, Mathieu Chalvidal, Thomas Serre</p>
<p><a href='https://openreview.net/forum?id=MziFFGjpkb'>https://openreview.net/forum?id=MziFFGjpkb</a></p>
<p><b>Keywords</b>: Explainable AI, Concept-based explainability, Interpretability, Concept extraction, Concept importance, Attribution methods
</p><p><b>Compressor summary</b>: The paragraph introduces a unifying framework for concept-based explainability methods in ANNs, which connects concept extraction and importance estimation as special cases of dictionary learning and attribution methods, respectively, and provides tools to evaluate and visualize them.</p><hr><h3>Mitigating Test-Time Bias for Fair Image Retrieval</h3>
<p>Fanjie Kong, Shuai Yuan, Weituo Hao, Ricardo Henao</p>
<p><a href='https://openreview.net/forum?id=Mxhb2lCOKL'>https://openreview.net/forum?id=Mxhb2lCOKL</a></p>
<p><b>Keywords</b>: Vision-language, Fairness, Text-based Image Retrieval, Deep Learning, Application
</p><p><b>Compressor summary</b>: The paper proposes a post-processing technique, Post-hoc Bias Mitigation (PBM), to generate fair and unbiased image retrieval results from neutral textual queries while maintaining the performance of the vision-language model.</p><hr><h3>State Sequences Prediction via Fourier Transform for Representation Learning</h3>
<p>Mingxuan Ye, Yufei Kuang, Jie Wang, Rui Yang, Wengang Zhou, Houqiang Li, Feng Wu</p>
<p><a href='https://openreview.net/forum?id=MvoMDD6emT'>https://openreview.net/forum?id=MvoMDD6emT</a></p>
<p><b>Keywords</b>: Reinforcement learning, Representation learning, State sequences prediction, Fourier transform
</p><p><b>Compressor summary</b>: SPF uses Fourier transform of state sequences to learn efficient representations for data-efficient RL, improving sample efficiency and long-term decision-making.</p><hr><h3>Mitigating the Popularity Bias of  Graph Collaborative Filtering: A Dimensional Collapse Perspective</h3>
<p>Yifei Zhang, Hao Zhu, yankai Chen, Zixing Song, Piotr Koniusz, Irwin King</p>
<p><a href='https://openreview.net/forum?id=MvCq52yt9Y'>https://openreview.net/forum?id=MvCq52yt9Y</a></p>
<p><b>Keywords</b>: Graph, Collaborative Filtering, Recommendation
</p><p><b>Compressor summary</b>: The paper proposes a decorrelation-enhanced graph-based collaborative filtering (GCF) method using non-Euclidean geometry to improve feature diversity and reduce popularity bias in personalized recommendation systems.</p><hr><h3>Near-Linear Time Algorithm for the Chamfer Distance</h3>
<p>Ainesh Bakshi, Piotr Indyk, Rajesh Jayaram, Sandeep Silwal, Erik Waingarten</p>
<p><a href='https://openreview.net/forum?id=Mv96iC6TMX'>https://openreview.net/forum?id=Mv96iC6TMX</a></p>
<p><b>Keywords</b>: chamfer distance, earth mover distance, high dimensional data analysis, nearest neighbor search, high dimensional data, high-dimensional geometry, sublinear algorithms, point clouds, theory
</p><p><b>Compressor summary</b>: The paper proposes an efficient algorithm for approximating the Chamfer distance between large point sets in high-dimensional spaces, which can enable new applications and analyses of such data.</p><hr><h3>An Efficient Dataset Condensation Plugin and Its Application to Continual Learning</h3>
<p>Enneng Yang, Li Shen, Zhenyi Wang, Tongliang Liu, Guibing Guo</p>
<p><a href='https://openreview.net/forum?id=Murj6wcjRw'>https://openreview.net/forum?id=Murj6wcjRw</a></p>
<p><b>Keywords</b>: Data Condensation, Continual Learning, Few-shot Learning
</p><p><b>Compressor summary</b>: The authors propose a new dataset condensation method that uses low-rank matrices to represent raw images, improving performance and reducing privacy risks in training networks from synthetic data.</p><hr><h3>Convolutional Neural Operators for robust and accurate learning of PDEs</h3>
<p>Bogdan Raonic, Roberto Molinaro, Tim De Ryck, Tobias Rohner, Francesca Bartolucci, Rima Alaifari, Siddhartha Mishra, Emmanuel de Bezenac</p>
<p><a href='https://openreview.net/forum?id=MtekhXRP4h'>https://openreview.net/forum?id=MtekhXRP4h</a></p>
<p><b>Keywords</b>: PDEs, Neural Operators, Scientific Machine Learning, Convolutional Neural Networks
</p><p><b>Compressor summary</b>: The authors propose convolutional neural operators (CNOs), which are adaptations of convolutional neural networks that can learn operators for partial differential equations (PDEs) while preserving their continuous nature, achieving better performance than existing methods.</p><hr><h3>Task-Robust Pre-Training for Worst-Case Downstream Adaptation</h3>
<p>Jianghui Wang, Yang Chen, Xingyu Xie, Cong Fang, Zhouchen Lin</p>
<p><a href='https://openreview.net/forum?id=Mr4OpbZEiB'>https://openreview.net/forum?id=Mr4OpbZEiB</a></p>
<p><b>Keywords</b>: Pre-training, Robustness, Multi-task learning
</p><p><b>Compressor summary</b>: The paper proposes a pre-training method that ensures good performance on downstream tasks by using a minimax loss and shows improved results on natural language processing and computer vision datasets.</p><hr><h3>Trade-off Between Efficiency and Consistency for Removal-based Explanations</h3>
<p>Yifan Zhang, Haowei He, Zhiquan Tan, Yang Yuan</p>
<p><a href='https://openreview.net/forum?id=MmCtXvW6GO'>https://openreview.net/forum?id=MmCtXvW6GO</a></p>
<p><b>Keywords</b>: AI interpretability, explainable AI, deep learning theory
</p><p><b>Compressor summary</b>: The paper shows that popular feature importance methods have a trade-off between interpretability, efficiency, and consistency, and proposes two new algorithms to minimize this trade-off using interpretation error as a metric.</p><hr><h3>Minimum norm interpolation by perceptra: Explicit regularization and implicit bias</h3>
<p>Jiyoung Park, Ian Pelakh, Stephan Wojtowytsch</p>
<p><a href='https://openreview.net/forum?id=MlrFYNo1yc'>https://openreview.net/forum?id=MlrFYNo1yc</a></p>
<p><b>Keywords</b>: Artificial neural network, interpolation, explicit regularization, implicit bias, weight decay, Barron class
</p><p><b>Compressor summary</b>: The paper explores how deep ReLU networks approach the minimal error solution when given enough data and relaxed regularization.</p><hr><h3>Birder: Communication-Efficient 1-bit Adaptive Optimizer for Practical Distributed DNN Training</h3>
<p>Hanyang Peng, Shuang Qin, Yue Yu, Jin Wang, Hui Wang, Ge Li</p>
<p><a href='https://openreview.net/forum?id=Mlo2kM11ZB'>https://openreview.net/forum?id=Mlo2kM11ZB</a></p>
<p><b>Keywords</b>: optimizer, 1-bit optimizer, distributed learning, optimization for deep networks, communication efficiency
</p><p><b>Compressor summary</b>: The paper proposes Birder, a 1-bit adaptive optimizer that offers fast distributed DNN training with similar convergence rate and inference performance as uncompressed SGDM/Adam, while reducing communication volume.</p><hr><h3>Diverse Conventions for Human-AI Collaboration</h3>
<p>Bidipta Sarkar, Andy Shih, Dorsa Sadigh</p>
<p><a href='https://openreview.net/forum?id=MljeRycu9s'>https://openreview.net/forum?id=MljeRycu9s</a></p>
<p><b>Keywords</b>: Multi-Agent RL, Multi-Agent Coordination, Human-AI Coordination
</p><p><b>Compressor summary</b>: The paper proposes a method for creating diverse and effective cooperative strategies in multi-agent games by combining self-play and cross-play optimization techniques.</p><hr><h3>Delayed Algorithms for Distributed Stochastic Weakly Convex Optimization</h3>
<p>Wenzhi Gao, Qi Deng</p>
<p><a href='https://openreview.net/forum?id=MirclT6zpv'>https://openreview.net/forum?id=MirclT6zpv</a></p>
<p><b>Keywords</b>: Stochastic optimization, Distributed optimization, Prox-linear method, Stochastic gradient method
</p><p><b>Compressor summary</b>: The paper presents new delayed stochastic optimization algorithms with improved convergence rates and robustness against network delays, and shows their effectiveness in distributed weakly convex problems.</p><hr><h3>Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal</h3>
<p>Leah Chrestien, Stefan Edelkamp, Antonin Komenda, Tomáš Pevný</p>
<p><a href='https://openreview.net/forum?id=Mgy6sgslPY'>https://openreview.net/forum?id=Mgy6sgslPY</a></p>
<p><b>Keywords</b>: Learning heuristic functions, deep learning, Immitation learning, planning, A*, best first search
</p><p><b>Compressor summary</b>: The paper studies optimal heuristics for forward search algorithms and proposes ranking-based loss functions for imitation learning, with experiments showing good results.</p><hr><h3>Protein Design with Guided Discrete Diffusion</h3>
<p>Nate Gruver, Samuel Don Stanton, Nathan C. Frey, Tim G. J. Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, Andrew Gordon Wilson</p>
<p><a href='https://openreview.net/forum?id=MfiK69Ga6p'>https://openreview.net/forum?id=MfiK69Ga6p</a></p>
<p><b>Keywords</b>: protein design, diffusion model, classifier guidance
</p><p><b>Compressor summary</b>: The authors propose NOS, a guidance method for discrete diffusion models that uses gradients from the denoising network to optimize protein design directly in sequence space, improving performance and addressing limitations of structure-based methods.</p><hr><h3>Optimization of Inter-group criteria for clustering with minimum size constraints</h3>
<p>Eduardo Sany Laber, Lucas Murtinho</p>
<p><a href='https://openreview.net/forum?id=MdJX5wwKwx'>https://openreview.net/forum?id=MdJX5wwKwx</a></p>
<p><b>Keywords</b>: Single Link, clustering, approximation algorithms, complexity, inter-group criterion
</p><p><b>Compressor summary</b>: The authors propose algorithms for optimizing inter-group criteria in clustering and show provable guarantees for two natural measures, with results for both unrestricted and constrained cases, and an empirical study on 10 real datasets.</p><hr><h3>Energy Transformer</h3>
<p>Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik Strobelt, Duen Horng Chau, Mohammed J Zaki, Dmitry Krotov</p>
<p><a href='https://openreview.net/forum?id=MbwVNEx9KS'>https://openreview.net/forum?id=MbwVNEx9KS</a></p>
<p><b>Keywords</b>: Hopfield Network, Dense Associative Memory, Energy-based models, Attention Mechanism
</p><p><b>Compressor summary</b>: The Energy Transformer combines attention mechanism, energy-based models, and associative memory to improve machine learning tasks such as image completion, graph anomaly detection, and graph classification.</p><hr><h3>Approximate inference of marginals using the IBIA framework</h3>
<p>Shivani Bathla, Vinita Vasudevan</p>
<p><a href='https://openreview.net/forum?id=MamHShmHiX'>https://openreview.net/forum?id=MamHShmHiX</a></p>
<p><b>Keywords</b>: Bayesian inference, posterior marginals, probabilistic graphical models
</p><p><b>Compressor summary</b>: The paper introduces a new algorithm for inferring marginals in probabilistic graphical models using a sequence of linked clique tree forests and a heuristic belief update method, which is faster and more accurate than existing methods.</p><hr><h3>Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning</h3>
<p>Xiaojun Guo, Yifei Wang, Zeming Wei, Yisen Wang</p>
<p><a href='https://openreview.net/forum?id=MYfqIVcQrp'>https://openreview.net/forum?id=MYfqIVcQrp</a></p>
<p><b>Keywords</b>: graph contrastive learning
</p><p><b>Compressor summary</b>: The paragraph discusses common phenomena in graph contrastive learning (GCL) methods that differ from visual contrastive learning (VCL), and suggests paying attention to the unique architecture of graph learning when designing GCL methods.</p><hr><h3>Efficient Training of Energy-Based Models Using Jarzynski Equality</h3>
<p>Davide Carbone, Mengjian Hua, Simon Coste, Eric Vanden-Eijnden</p>
<p><a href='https://openreview.net/forum?id=MXxZ0Z5MNz'>https://openreview.net/forum?id=MXxZ0Z5MNz</a></p>
<p><b>Keywords</b>: Energy-Based Model, contrastive learning, generative models, Jarzynski identity, ULA
</p><p><b>Compressor summary</b>: The text describes a new method to train energy-based models more efficiently by using tools from nonequilibrium thermodynamics and sequential Monte-Carlo sampling, which improves performance over existing methods.</p><hr><h3>Explainable and Efficient Randomized Voting Rules</h3>
<p>Soroush Ebadian, Aris Filos-Ratsikas, Mohamad Latifian, Nisarg Shah</p>
<p><a href='https://openreview.net/forum?id=MWxsYPVmLS'>https://openreview.net/forum?id=MWxsYPVmLS</a></p>
<p><b>Keywords</b>: explainability, efficiency, voting, distortion, randomized decision-making
</p><p><b>Compressor summary</b>: The text discusses how adding simple randomization to deterministic voting rules can improve efficiency without losing explainability, and focuses on two families of such rules.</p><hr><h3>MCUFormer: Deploying Vision Tranformers on Microcontrollers with Limited Memory</h3>
<p>Yinan Liang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, Jiwen Lu</p>
<p><a href='https://openreview.net/forum?id=MWp3SwoHmH'>https://openreview.net/forum?id=MWp3SwoHmH</a></p>
<p><b>Keywords</b>: Vision transformer, microcontroller, network architecture search
</p><p><b>Compressor summary</b>: The paper introduces MCUFormer, a method to deploy vision transformers on IoT devices with limited memory by jointly designing architecture and constructing an inference operator library for efficient memory usage.</p><hr><h3>Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption</h3>
<p>Yige Hong, Qiaomin Xie, Yudong Chen, Weina Wang</p>
<p><a href='https://openreview.net/forum?id=MWQjqtV1z4'>https://openreview.net/forum?id=MWQjqtV1z4</a></p>
<p><b>Keywords</b>: restless bandits, average reward MDP, simulation-based method, asymptotic optimality
</p><p><b>Compressor summary</b>: The paper proposes a simulation-based method to design near-optimal policies for multi-armed bandit problems without relying on complex assumptions.</p><hr><h3>Tanimoto Random Features for Scalable Molecular Machine Learning</h3>
<p>Austin Tripp, Sergio Bacallado, Sukriti Singh, José Miguel Hernández-Lobato</p>
<p><a href='https://openreview.net/forum?id=MV0INFAKGq'>https://openreview.net/forum?id=MV0INFAKGq</a></p>
<p><b>Keywords</b>: Tanimoto, Kernel, MinMax, Gaussian process, molecule, chemistry, interpretability
</p><p><b>Compressor summary</b>: The paper introduces new random features that enable faster computation of the Tanimoto kernel for large molecular datasets, and extend it to real-valued vectors with theoretical and empirical analysis.</p><hr><h3>Improvements on Uncertainty Quantification for Node Classification via Distance Based Regularization</h3>
<p>Russell Alan Hart, Linlin Yu, Yifei Lou, Feng Chen</p>
<p><a href='https://openreview.net/forum?id=MUzdCW2hC6'>https://openreview.net/forum?id=MUzdCW2hC6</a></p>
<p><b>Keywords</b>: Uncertainty Quantification, Graph Posterior Network, Bayesian
</p><p><b>Compressor summary</b>: The paragraph discusses uncertainty quantification for interdependent node-level classification using graph posterior networks, highlighting the limitations of the uncertainty cross-entropy loss function and proposing a distance-based regularization to improve out-of-distribution and misclassification detection.</p><hr><h3>HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds</h3>
<p>Gang Zhang, Chen Junnan, Guohuan Gao, Jianmin Li, Xiaolin Hu</p>
<p><a href='https://openreview.net/forum?id=MUwr2YVJfN'>https://openreview.net/forum?id=MUwr2YVJfN</a></p>
<p><b>Keywords</b>: 3D object detection; encoder-decoder structure
</p><p><b>Compressor summary</b>: HEDNet is a hierarchical encoder-decoder network for 3D object detection in point clouds that captures long-range dependencies among features and outperforms previous methods on Waymo Open and nuScenes datasets.</p><hr><h3>On the Exploration of Local Significant Differences For Two-Sample Test</h3>
<p>Zhijian Zhou, Jie Ni, Jia-He Yao, Wei Gao</p>
<p><a href='https://openreview.net/forum?id=MRiitgpcUy'>https://openreview.net/forum?id=MRiitgpcUy</a></p>
<p><b>Keywords</b>: two-sample test, local significant difference, directional information
</p><p><b>Compressor summary</b>: The paper proposes ME$_\text{MaBiD}$, a new method for two-sample testing that uses multiple Mahalanobis kernels, partitions the embedding space into regions, and tests for local significant differences with a bi-directional approach.</p><hr><h3>Enhancing Adversarial Robustness via Score-Based Optimization</h3>
<p>Boya Zhang, Weijian Luo, Zhihua Zhang</p>
<p><a href='https://openreview.net/forum?id=MOAHXRzHhm'>https://openreview.net/forum?id=MOAHXRzHhm</a></p>
<p><b>Keywords</b>: Adversarial Defense, Adversarial Attack, Score-based Models, Diffusion Models
</p><p><b>Compressor summary</b>: The paper proposes ScoreOpt, a novel defense against adversarial attacks for deep neural networks that optimizes adversarial samples towards clean data, achieving better robustness and faster inference than existing methods.</p><hr><h3>Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models</h3>
<p>Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, Zhihua Zhang</p>
<p><a href='https://openreview.net/forum?id=MLIs5iRq4w'>https://openreview.net/forum?id=MLIs5iRq4w</a></p>
<p><b>Keywords</b>: diffusion model, data-free distillation, implicit generator, knowledge transfer
</p><p><b>Compressor summary</b>: Diff-Instruct is a framework for instructing the training of different generative models using pre-trained diffusion models, based on a novel divergence called Integral Kullback-Leibler (IKL) divergence.</p><hr><h3>Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models</h3>
<p>Zhimin Chen, Longlong Jing, Yingwei Li, Bing Li</p>
<p><a href='https://openreview.net/forum?id=MJbDy2155j'>https://openreview.net/forum?id=MJbDy2155j</a></p>
<p><b>Keywords</b>: 3D self-supervised learning, Multi-modal Representation Learning, Masked autoencoders, Knowledge distillation
</p><p><b>Compressor summary</b>: The authors propose Bridge3D, a method that uses features, semantic masks, and captions from foundation models to pre-train 3D models for better 3D scene representation learning and improve 3D object detection and semantic segmentation tasks.</p><hr><h3>Sequential Preference Ranking for Efficient Reinforcement Learning from Human Feedback</h3>
<p>Minyoung Hwang, Gunmin Lee, Hogun Kee, Chan Woo Kim, Kyungjae Lee, Songhwai Oh</p>
<p><a href='https://openreview.net/forum?id=MIYBTjCVjR'>https://openreview.net/forum?id=MIYBTjCVjR</a></p>
<p><b>Keywords</b>: Reinforcement Learning; Reinforcement Learning from Human Feedback; Preference-based Reinforcement Learning; Human-Robot Interaction
</p><p><b>Compressor summary</b>: The proposed SeqRank framework uses sequential preference ranking to enhance feedback efficiency in reinforcement learning from human feedback, leading to higher average feedback efficiency and improved task performance.</p><hr><h3>Debiasing Conditional Stochastic Optimization</h3>
<p>Lie He, Shiva Kasiviswanathan</p>
<p><a href='https://openreview.net/forum?id=MH7E7AME1r'>https://openreview.net/forum?id=MH7E7AME1r</a></p>
<p><b>Keywords</b>: Optimization, Bilevel Optimization, Stochastic Optimization
</p><p><b>Compressor summary</b>: The paper proposes a general stochastic extrapolation technique to reduce bias and improve sample complexity for conditional stochastic optimization (CSO) problems, which have applications in various fields such as portfolio selection and reinforcement learning.</p><hr><h3>Learning Efficient Surrogate Dynamic Models with Graph Spline Networks</h3>
<p>Chuanbo Hua, Federico Berto, Michael Poli, Stefano Massaroli, Jinkyoo Park</p>
<p><a href='https://openreview.net/forum?id=MGPST5I9DO'>https://openreview.net/forum?id=MGPST5I9DO</a></p>
<p><b>Keywords</b>: Graph, Spline Collocation Method, Graph Neural Networks, Simulation, Partial Differential Equations, PDEs, Physics, Scientific Computing, Surrogate Models, Weather Forecasting
</p><p><b>Compressor summary</b>: GraphSplineNets is a deep learning method that uses spline collocation to forecast physical systems faster by reducing grid size and iteration steps, with an adaptive strategy for sampling important regions.</p><hr><h3>Should Under-parameterized Student Networks Copy or Average Teacher Weights?</h3>
<p>Berfin Simsek, Amire Bendjeddou, Wulfram Gerstner, Johanni Brea</p>
<p><a href='https://openreview.net/forum?id=MG0mYskXN2'>https://openreview.net/forum?id=MG0mYskXN2</a></p>
<p><b>Keywords</b>: shallow neural networks, non-convex optimization, approximation error, loss landscape
</p><p><b>Compressor summary</b>: The paragraph discusses how an under-parameterized neural network (student) can be fitted to an over-parameterized one (teacher) and shows that "copy-average" configurations are critical points for shallow networks with specific activation functions.</p><hr><h3>Random Cuts are Optimal for Explainable k-Medians</h3>
<p>Konstantin Makarychev, Liren Shan</p>
<p><a href='https://openreview.net/forum?id=MFWgLCWgUB'>https://openreview.net/forum?id=MFWgLCWgUB</a></p>
<p><b>Keywords</b>: Clustering, k-medians, Decision Tree, Explainability
</p><p><b>Compressor summary</b>: The RandomCoordinateCut algorithm achieves the best possible performance for solving the explainable $k$-medians problem in the $\ell_1$ norm, with a competitive ratio of at most $2\ln k+2$.</p><hr><h3>Imbalanced Mixed Linear Regression</h3>
<p>Pini Zilber, Boaz Nadler</p>
<p><a href='https://openreview.net/forum?id=MDxZYFR5Me'>https://openreview.net/forum?id=MDxZYFR5Me</a></p>
<p><b>Keywords</b>: Mixture regression model, Mixture of linear models, Iteratively reweighted least squares
</p><p><b>Compressor summary</b>: Mix-IRLS is a novel algorithm that efficiently solves mixed linear regression problems with imbalanced and challenging data settings, outperforming existing methods.</p><hr><h3>Nash Regret Guarantees for Linear Bandits</h3>
<p>Ayush Sawarni, Soumyabrata Pal, Siddharth Barman</p>
<p><a href='https://openreview.net/forum?id=MCkUS1P3Sh'>https://openreview.net/forum?id=MCkUS1P3Sh</a></p>
<p><b>Keywords</b>: Sub-Poisson Distribution, Nash Social Welfare, Fairness Quantification, John Ellipsoid, Kiefer-Wolfowitz Optimal Design, Algorithmic Game Theory, Online Learning
</p><p><b>Compressor summary</b>: The paper presents a new notion of regret called Nash regret, which measures the performance of a bandit algorithm as collective welfare, and develops an algorithm that achieves tight upper bounds on it for stochastic linear bandits with sub-Poisson rewards.</p><hr><h3>Adversarial Attacks on Online Learning to Rank with Click Feedback</h3>
<p>Jinhang Zuo, Zhiyao Zhang, Zhiyong Wang, Shuai Li, Mohammad Hajiesmaili, Adam Wierman</p>
<p><a href='https://openreview.net/forum?id=MCj7DLkYqS'>https://openreview.net/forum?id=MCj7DLkYqS</a></p>
<p><b>Keywords</b>: online learning to rank, adversarial attack, click model
</p><p><b>Compressor summary</b>: This paper investigates various attack strategies against online learning to rank algorithms and proposes a general attack strategy that works under different feedback models.</p><hr><h3>Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples</h3>
<p>Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Mehran Kazemi, Najoung Kim, He He</p>
<p><a href='https://openreview.net/forum?id=MCVfX7HgPO'>https://openreview.net/forum?id=MCVfX7HgPO</a></p>
<p><b>Keywords</b>: large language models, reasoning, out-of-distribution generalization, chain-of-thought, in-context learning
</p><p><b>Compressor summary</b>: The paragraph discusses testing the general deductive reasoning ability of large language models on various deduction rules and complex proofs, using a new synthetic dataset.</p><hr><h3>Fast Rank-1 Lattice Targeted Sampling for Black-box Optimization</h3>
<p>Yueming Lyu</p>
<p><a href='https://openreview.net/forum?id=M8CYKLHoEN'>https://openreview.net/forum?id=M8CYKLHoEN</a></p>
<p><b>Keywords</b>: Black-box Optimization, Derivate-free Optimization, Kernel methods
</p><p><b>Compressor summary</b>: RLTS is a novel technique that uses rank-1 lattices to improve query efficiency in high-dimensional optimization problems, enabling faster training and inference of Gaussian processes.</p><hr><h3>Geometric Algebra Transformer</h3>
<p>Johann Brehmer, Pim De Haan, Sönke Behrends, Taco Cohen</p>
<p><a href='https://openreview.net/forum?id=M7r2CO4tJC'>https://openreview.net/forum?id=M7r2CO4tJC</a></p>
<p><b>Keywords</b>: Geometry, geometric algebra, equivariance, transformer
</p><p><b>Compressor summary</b>: The Geometric Algebra Transformer (GATr) is a new architecture for geometric data that leverages projective geometric algebra and symmetry to achieve better performance in various applications.</p><hr><h3>Feature Dropout: Revisiting the Role of Augmentations in Contrastive Learning</h3>
<p>Alex Tamkin, Margalit Glasgow, Xiluo He, Noah Goodman</p>
<p><a href='https://openreview.net/forum?id=M7hijAPA4B'>https://openreview.net/forum?id=M7hijAPA4B</a></p>
<p><b>Keywords</b>: self-supervised learning, contrastive learning
</p><p><b>Compressor summary</b>: Augmentations that destroy labels can be beneficial in foundation model settings, where diverse, general-purpose representations are needed for multiple downstream tasks.</p><hr><h3>Enhancing Minority Classes by Mixing: An Adaptative Optimal Transport Approach for Long-tailed Classification</h3>
<p>Jintong Gao, He Zhao, Zhuo Li, Dan dan Guo</p>
<p><a href='https://openreview.net/forum?id=M7FQpIdo0X'>https://openreview.net/forum?id=M7FQpIdo0X</a></p>
<p><b>Keywords</b>: Long-tailed Classification, Optimal Transport, Image-mixing, Semantic Similarity
</p><p><b>Compressor summary</b>: The authors propose an adaptive image-mixing method based on optimal transport to generate semantically reasonable mixed images for minority classes in long-tailed classification tasks, improving performance and serving as a general data augmentation method.</p><hr><h3>Multiply Robust Federated Estimation of Targeted Average Treatment Effects</h3>
<p>Larry Han, Zhu Shen, Jose R Zubizarreta</p>
<p><a href='https://openreview.net/forum?id=M6UccKMFGl'>https://openreview.net/forum?id=M6UccKMFGl</a></p>
<p><b>Keywords</b>: Causal inference, Covariate mismatch, Federated learning, Multiple robustness, Transportation
</p><p><b>Compressor summary</b>: The paper proposes a new method to make valid causal inferences from multi-site studies while preserving privacy and accounting for differences between sites.</p><hr><h3>Language Models can Solve Computer Tasks</h3>
<p>Geunwoo Kim, Pierre Baldi, Stephen Marcus McAleer</p>
<p><a href='https://openreview.net/forum?id=M6OmjAZ4CX'>https://openreview.net/forum?id=M6OmjAZ4CX</a></p>
<p><b>Keywords</b>: Large Language models, Web Navigation, Foundation Models, Decision Making
</p><p><b>Compressor summary</b>: The paper presents a new method called RCI, which uses a pre-trained language model to automate computer tasks via natural language commands with fewer demonstrations and without task-specific rewards, outperforming existing methods on benchmarks.</p><hr><h3>H-nobs: Achieving Certified Fairness and Robustness in Distributed Learning on Heterogeneous Datasets</h3>
<p>Guanqiang Zhou, Ping Xu, Yue Wang, Zhi Tian</p>
<p><a href='https://openreview.net/forum?id=M4h1UAxI3b'>https://openreview.net/forum?id=M4h1UAxI3b</a></p>
<p><b>Keywords</b>: distributed learning, federated learning, fairness, robustness, Byzantine attack, norm-based screening, q-FFL, optimization, convergence analysis
</p><p><b>Compressor summary</b>: The paper proposes a framework called H-nobs that combines fairness and robustness in distributed learning systems, addressing challenges like data heterogeneity and trade-offs between the two properties.</p><hr><h3>Transformers learn to implement preconditioned gradient descent for in-context learning</h3>
<p>Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, Suvrit Sra</p>
<p><a href='https://openreview.net/forum?id=LziniAXEI9'>https://openreview.net/forum?id=LziniAXEI9</a></p>
<p><b>Keywords</b>: Transformers, In-context learning, adaptive gradient methods
</p><p><b>Compressor summary</b>: The authors explore if transformers can learn to implement algorithms like gradient descent by training over random problem instances and provide theoretical analysis of the loss landscape for linear transformers in this setting.</p><hr><h3>Diffused Redundancy in Pre-trained Representations</h3>
<p>Vedant Nanda, Till Speicher, John P Dickerson, Krishna P. Gummadi, Soheil Feizi, Adrian Weller</p>
<p><a href='https://openreview.net/forum?id=LyAuNoZkGP'>https://openreview.net/forum?id=LyAuNoZkGP</a></p>
<p><b>Keywords</b>: representation learning, redundancy, transfer learning, fairness
</p><p><b>Compressor summary</b>: This study analyzes how pre-trained neural network features encode diffuse redundancy, where a subset of neurons can perform similar tasks as the whole layer, depending on the downstream task and pre-training parameters.</p><hr><h3>Sharp Spectral Rates for Koopman Operator Learning</h3>
<p>Vladimir R Kostic, Karim Lounici, Pietro Novelli, massimiliano pontil</p>
<p><a href='https://openreview.net/forum?id=Lt3jqxsbVO'>https://openreview.net/forum?id=Lt3jqxsbVO</a></p>
<p><b>Keywords</b>: Statistical Learning Theory, Dynamical Systems
</p><p><b>Compressor summary</b>: The paper presents non-asymptotic learning bounds for Koopman eigenvalues and eigenfunctions in time-reversal-invariant stochastic dynamical systems, comparing two estimators (EDMD and RRR) and discussing their bias and variance trade-off.</p><hr><h3>Prompt-augmented Temporal Point Process for Streaming Event Sequence</h3>
<p>Siqiao Xue, Yan Wang, Zhixuan Chu, Xiaoming Shi, Caigao JIANG, Hongyan Hao, Gangwei Jiang, Xiaoyun Feng, James Y. Zhang, JUN ZHOU</p>
<p><a href='https://openreview.net/forum?id=LswqtKU9op'>https://openreview.net/forum?id=LswqtKU9op</a></p>
<p><b>Keywords</b>: prompt, point process, event sequence, continual learning.
</p><p><b>Compressor summary</b>: The text introduces PromptTPP, a framework that uses Continual Learning to monitor Neural Temporal Point Processes and learn event sequences without forgetting or buffering past data.</p><hr><h3>Bridging RL Theory and Practice with the Effective Horizon</h3>
<p>Cassidy Laidlaw, Stuart Russell, Anca Dragan</p>
<p><a href='https://openreview.net/forum?id=Lr2swAfwff'>https://openreview.net/forum?id=Lr2swAfwff</a></p>
<p><b>Keywords</b>: reinforcement learning, RL theory, theory of reinforcement learning, instance-dependent bounds, empirical validation of theory
</p><p><b>Compressor summary</b>: The authors introduce BRIDGE, a new dataset for comparing deep RL algorithms and bounds, and propose the effective horizon as a better measure of MDP complexity that correlates with empirical performance and exploration techniques.</p><hr><h3>Stabilizing the Optimization of Neural Signed Distance Functions and Finer Shape Representation</h3>
<p>Huizong Yang, Yuxin Sun, Ganesh Sundaramoorthi, Anthony Yezzi</p>
<p><a href='https://openreview.net/forum?id=Lqv7VS1iBF'>https://openreview.net/forum?id=Lqv7VS1iBF</a></p>
<p><b>Keywords</b>: Implicit Neural Representation, Surface Reconstruction
</p><p><b>Compressor summary</b>: The authors propose a novel approach to learning implicit neural representations of shapes, which addresses the instability issues in existing methods and enables capturing finer shape details.</p><hr><h3>Compositional Generalization from First Principles</h3>
<p>Thaddäus Wiedemer, Prasanna Mayilvahanan, Matthias Bethge, Wieland Brendel</p>
<p><a href='https://openreview.net/forum?id=LqOQ1uJmSx'>https://openreview.net/forum?id=LqOQ1uJmSx</a></p>
<p><b>Keywords</b>: compositional generalization, compositionality, generalization, combinatorial generalization, out-of-distribution, out-of-domain, identifiability, disentanglement, object-centric learning, DSprites
</p><p><b>Compressor summary</b>: The text discusses an approach to improve machine learning's compositional generalization by considering it as a property of the data-generating process, rather than the data itself, and provides conditions and empirical validation for this approach.</p><hr><h3>Flow Factorized Representation Learning</h3>
<p>Yue Song, T. Anderson Keller, Nicu Sebe, Max Welling</p>
<p><a href='https://openreview.net/forum?id=LnySNEJAQt'>https://openreview.net/forum?id=LnySNEJAQt</a></p>
<p><b>Keywords</b>: Generative modelling, latent disentanglement, variational autoencoders
</p><p><b>Compressor summary</b>: The authors propose Flow Factorized Representation Learning, a novel approach to structured representation learning that uses latent flow paths generated by learned potentials to achieve more efficient and better structured representations than existing methods.</p><hr><h3>From Tempered to Benign Overfitting in ReLU Neural Networks</h3>
<p>Guy Kornowski, Gilad Yehudai, Ohad Shamir</p>
<p><a href='https://openreview.net/forum?id=LnZuxp3Tx7'>https://openreview.net/forum?id=LnZuxp3Tx7</a></p>
<p><b>Keywords</b>: benign overfitting, implicit bias, interpolating predictors, neural networks, theory
</p><p><b>Compressor summary</b>: The text discusses how the type of overfitting in neural networks depends on input dimension, sample size, architecture, and training algorithm, and provides theoretical and empirical evidence for this claim.</p><hr><h3>Individualized Dosing Dynamics via Neural Eigen Decomposition</h3>
<p>Stav Belogolovsky, Ido Greenberg, Danny Eytan, Shie Mannor</p>
<p><a href='https://openreview.net/forum?id=Lmxo0RVNx2'>https://openreview.net/forum?id=Lmxo0RVNx2</a></p>
<p><b>Keywords</b>: personalized medicine, dosing dynamics, sequential prediction, stochastic differential equations, Kalman filter, recurrent neural networks, medical drug control
</p><p><b>Compressor summary</b>: NESDE is a novel algorithm that uses neural differential equations to model biological dynamics in individual patients, generalize to new treatment policies, and handle different noise levels while providing fast and continuous predictions.</p><hr><h3>What functions can Graph Neural Networks compute on random graphs? The role of Positional Encoding</h3>
<p>Nicolas Keriven, Samuel Vaiter</p>
<p><a href='https://openreview.net/forum?id=LmmjiTwYm0'>https://openreview.net/forum?id=LmmjiTwYm0</a></p>
<p><b>Keywords</b>: graph neural network; random graph; positional encoding
</p><p><b>Compressor summary</b>: This paper investigates the expressive power of equivariant Graph Neural Networks (GNNs) for node-tasks on large graphs, considering input features and positional encodings, and provides theoretical and empirical insights.</p><hr><h3>Non-autoregressive Machine Translation with Probabilistic Context-free Grammar</h3>
<p>Shangtong Gui, Chenze Shao, Zhengrui Ma, Xishan Zhang, Yunji Chen, Yang Feng</p>
<p><a href='https://openreview.net/forum?id=LloZFVwWvj'>https://openreview.net/forum?id=LloZFVwWvj</a></p>
<p><b>Keywords</b>: Machine translation, Non-autoregressive generation, Probabilistic Context-free Grammar
</p><p><b>Compressor summary</b>: PCFG-NAT improves non-autoregressive neural machine translation by using a probabilistic context-free grammar to capture complex dependencies among output tokens and enhance explainability.</p><hr><h3>Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?</h3>
<p>Erin George, Michael Murray, William Joseph Swartworth, Deanna Needell</p>
<p><a href='https://openreview.net/forum?id=LlERoXEKjh'>https://openreview.net/forum?id=LlERoXEKjh</a></p>
<p><b>Keywords</b>: benign overfitting, neural networks, relu, hinge loss
</p><p><b>Compressor summary</b>: The paper investigates how two-layer ReLU networks with gradient descent and hinge loss handle linearly separable data with some noisy labels, and identifies three possible outcomes depending on the margin conditions of the clean data.</p><hr><h3>CS-Isolate: Extracting Hard Confident Examples by Content and Style Isolation</h3>
<p>Yexiong Lin, Yu Yao, Xiaolong Shi, Mingming Gong, Xu Shen, Dong Xu, Tongliang Liu</p>
<p><a href='https://openreview.net/forum?id=Lkc0KjsDFv'>https://openreview.net/forum?id=Lkc0KjsDFv</a></p>
<p><b>Keywords</b>: learning with label errors
</p><p><b>Compressor summary</b>: The paper proposes a method to infer content factors from noisy labels by using data augmentation and regularization, improving the learning of hard examples for image classification.</p><hr><h3>When Can We Track Significant Preference Shifts in Dueling Bandits?</h3>
<p>Joe Suk, Arpit Agarwal</p>
<p><a href='https://openreview.net/forum?id=LjWJLkSpjh'>https://openreview.net/forum?id=LjWJLkSpjh</a></p>
<p><b>Keywords</b>: non-stationary, multi-armed bandits, dueling bandits, preference-based learning
</p><p><b>Compressor summary</b>: The paper investigates how preferences can change over time in dueling bandits problems and explores the feasibility of adaptive algorithms with low dynamic regret under different preference distributions.</p><hr><h3>AlberDICE: Addressing Out-Of-Distribution Joint Actions in Offline Multi-Agent RL via Alternating Stationary Distribution Correction Estimation</h3>
<p>Daiki E. Matsunaga, Jongmin Lee, Jaeseok Yoon, Stefanos Leonardos, Pieter Abbeel, Kee-Eung Kim</p>
<p><a href='https://openreview.net/forum?id=LhVJdq4cZm'>https://openreview.net/forum?id=LhVJdq4cZm</a></p>
<p><b>Keywords</b>: Offline Reinforcement Learning, Multi-Agent Reinforcement Learning
</p><p><b>Compressor summary</b>: AlberDICE is an offline multi-agent reinforcement learning algorithm that trains agents individually based on stationary distribution optimization and avoids out-of-distribution joint actions, achieving better performance than baselines.</p><hr><h3>Deep Stochastic Processes via Functional Markov Transition Operators</h3>
<p>Jin Xu, Emilien Dupont, Kaspar Märtens, Tom Rainforth, Yee Whye Teh</p>
<p><a href='https://openreview.net/forum?id=Lg1ODJGGiI'>https://openreview.net/forum?id=Lg1ODJGGiI</a></p>
<p><b>Keywords</b>: Neural Processes, Bayesian Nonparammetric Models
</p><p><b>Compressor summary</b>: Markov Neural Processes combine neural networks with stochastic processes, preserving their properties and improving performance on various tasks.</p><hr><h3>Hybrid Policy Optimization from Imperfect Demonstrations</h3>
<p>Hanlin Yang, Chao Yu, peng sun, Siji Chen</p>
<p><a href='https://openreview.net/forum?id=LftAvFt54C'>https://openreview.net/forum?id=LftAvFt54C</a></p>
<p><b>Keywords</b>: reinforcement learning, sparse reward, exploration, learning from demonstrations
</p><p><b>Compressor summary</b>: The paper proposes a novel RL algorithm called HYPO that uses imperfect demonstrations to train an offline guider policy to instruct an online agent policy for efficient exploration in challenging tasks with sparse rewards.</p><hr><h3>On Dynamic Programming Decompositions of Static Risk Measures in Markov Decision Processes</h3>
<p>Jia Lin Hau, Erick Delage, Mohammad Ghavamzadeh, Marek Petrik</p>
<p><a href='https://openreview.net/forum?id=LelK6Mfoey'>https://openreview.net/forum?id=LelK6Mfoey</a></p>
<p><b>Keywords</b>: reinforcement learning, markov decision processes, monetary risk measures
</p><p><b>Compressor summary</b>: The paragraph discusses the limitations of common risk-averse optimization methods in Markov decision processes and proposes a new decomposition for Value-at-Risk that avoids suboptimality.</p><hr><h3>ODE-based Recurrent Model-free Reinforcement Learning for POMDPs</h3>
<p>Xuanle Zhao, Duzhen Zhang, Liyuan Han, Tielin Zhang, Bo XU</p>
<p><a href='https://openreview.net/forum?id=LdvVd0bNyO'>https://openreview.net/forum?id=LdvVd0bNyO</a></p>
<p><b>Keywords</b>: neural ode, POMDPs, reinforcement learning
</p><p><b>Compressor summary</b>: The paragraph discusses a novel ODE-based recurrent model combined with model-free RL for solving POMDPs and demonstrates its effectiveness in various tasks, including robustness against irregular observations.</p><hr><h3>Residual Q-Learning: Offline and Online Policy Customization without Value</h3>
<p>Chenran Li, Chen Tang, Haruki Nishimura, Jean Mercat, Masayoshi Tomizuka, Wei Zhan</p>
<p><a href='https://openreview.net/forum?id=LaNeRwDrTk'>https://openreview.net/forum?id=LaNeRwDrTk</a></p>
<p><b>Keywords</b>: reinforcement learning, imitation learning
</p><p><b>Compressor summary</b>: The paragraph discusses imitation learning and introduces a new problem setting called policy customization, which involves training a policy that inherits the characteristics of a prior policy while satisfying additional requirements from a downstream task, using a novel framework called Residual Q-learning.</p><hr><h3>Echoes Beyond Points: Unleashing the Power of Raw Radar Data in Multi-modality Fusion</h3>
<p>Yang Liu, Feng Wang, Naiyan Wang, Zhaoxiang Zhang</p>
<p><a href='https://openreview.net/forum?id=LZzsn51DPr'>https://openreview.net/forum?id=LZzsn51DPr</a></p>
<p><b>Keywords</b>: 4D Radar; Transformer; Multi-modality
</p><p><b>Compressor summary</b>: The paper introduces EchoFusion, a novel radar signal processing method that fuses raw radar data with other sensors to improve autonomous driving performance and approach LiDAR accuracy.</p><hr><h3>High-dimensional Contextual Bandit Problem without Sparsity</h3>
<p>Junpei Komiyama, Masaaki Imaizumi</p>
<p><a href='https://openreview.net/forum?id=LZ4WgwmrUJ'>https://openreview.net/forum?id=LZ4WgwmrUJ</a></p>
<p><b>Keywords</b>: multi-armed bandits, linear bandits, contextual bandits, overparameterized models, high-dimensional models, online learning
</p><p><b>Compressor summary</b>: The paper studies how to solve high-dimensional linear contextual bandit problems using explore-then-commit and adaptive explore-then-commit algorithms, without imposing sparsity on regression coefficients.</p><hr><h3>Large Language Models can Implement Policy Iteration</h3>
<p>Ethan Brooks, Logan A Walls, Richard Lewis, Satinder Singh</p>
<p><a href='https://openreview.net/forum?id=LWxjWoBTsr'>https://openreview.net/forum?id=LWxjWoBTsr</a></p>
<p><b>Keywords</b>: Reinforcement Learning, In-Context Learning, Foundation Models
</p><p><b>Compressor summary</b>: The authors propose a method to use a large language model for policy iteration without expert demonstrations or gradients, by iteratively updating the prompt and interacting with an RL environment.</p><hr><h3>Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels</h3>
<p>Zebin You, Yong Zhong, Fan Bao, Jiacheng Sun, Chongxuan Li, Jun Zhu</p>
<p><a href='https://openreview.net/forum?id=LVHEcVgEGm'>https://openreview.net/forum?id=LVHEcVgEGm</a></p>
<p><b>Keywords</b>: diffusion models, semi-supervised generation, semi-supervised diffusion models, semi-supervised classification, image generation.
</p><p><b>Compressor summary</b>: The authors propose dual pseudo training, a method that uses partial labels to train a classifier, then a generative model, and finally retrains the classifier with real and generated images, achieving state-of-the-art results in semi-supervised image generation and classification.</p><hr><h3>Exploring Question Decomposition for Zero-Shot VQA</h3>
<p>Zaid Khan, Vijay Kumar b g, Samuel Schulter, Manmohan Chandraker, Yun Fu</p>
<p><a href='https://openreview.net/forum?id=LV83JEihHu'>https://openreview.net/forum?id=LV83JEihHu</a></p>
<p><b>Keywords</b>: visual question answering, in-context learning, vision-language
</p><p><b>Compressor summary</b>: The authors propose a question decomposition strategy for visual question answering, which improves performance by selectively decomposing questions and using human-written decompositions to guide large vision-language models.</p><hr><h3>Stochastic Optimal Control for Collective Variable Free Sampling of Molecular Transition Paths</h3>
<p>Lars Holdijk, Yuanqi Du, Ferry Hooft, Priyank Jaini, Bernd Ensing, Max Welling</p>
<p><a href='https://openreview.net/forum?id=LUVqEs90mq'>https://openreview.net/forum?id=LUVqEs90mq</a></p>
<p><b>Keywords</b>: Transition Path Sampling, Stochastic Optimal Control
</p><p><b>Compressor summary</b>: The paragraph describes a problem of finding transition paths between metastable states in molecular systems, proposes a machine learning method called PIPS that does not rely on collective variables, and demonstrates its effectiveness on small proteins.</p><hr><h3>Learning Visual Prior via Generative Pre-Training</h3>
<p>Jinheng Xie, Kai Ye, Yudong Li, Yuexiang Li, Kevin Qinghong Lin, Yefeng Zheng, Linlin Shen, Mike Zheng Shou</p>
<p><a href='https://openreview.net/forum?id=LUT4b9gOtS'>https://openreview.net/forum?id=LUT4b9gOtS</a></p>
<p><b>Keywords</b>: Visual Prior, Generative Pre-Training, Conditional Image Synthesis
</p><p><b>Compressor summary</b>: The paper proposes VisorGPT, a method to learn and customize visual prior using generative pre-training and prompt engineering for conditional image synthesis tasks.</p><hr><h3>Feature-Learning Networks Are Consistent Across Widths At Realistic Scales</h3>
<p>Nikhil Vyas, Alexander Atanasov, Blake Bordelon, Depen Morwani, Sabarish Sainathan, Cengiz Pehlevan</p>
<p><a href='https://openreview.net/forum?id=LTdfYIvbHc'>https://openreview.net/forum?id=LTdfYIvbHc</a></p>
<p><b>Keywords</b>: mean-field, muP, feature learning, infinite width, deep ensembles
</p><p><b>Compressor summary</b>: The dynamics of feature-learning neural networks differ across various architectures and datasets, but their behavior can be captured by infinite-width limits for simple tasks, while deviations grow systematically for harder tasks due to initialization-dependent output variance scaling and the bias of narrower width.</p><hr><h3>Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks</h3>
<p>Honghao Wei, Xin Liu, Weina Wang, Lei Ying</p>
<p><a href='https://openreview.net/forum?id=LTbIUkN95h'>https://openreview.net/forum?id=LTbIUkN95h</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Mixed Systems, Queueing Network, Sample Efficient
</p><p><b>Compressor summary</b>: The paper proposes a sample-efficient reinforcement learning method for mixed systems with stochastic and pseudo-stochastic states, which improves learning by generating augmented data samples and reducing the sample complexity.</p><hr><h3>Three Towers: Flexible Contrastive Learning with Pretrained Image Models</h3>
<p>Jannik Kossen, Mark Collier, Basil Mustafa, Xiao Wang, Xiaohua Zhai, Lucas Beyer, Andreas Peter Steiner, Jesse Berent, Rodolphe Jenatton, Effrosyni Kokiopoulou</p>
<p><a href='https://openreview.net/forum?id=LSYQB4CwD3'>https://openreview.net/forum?id=LSYQB4CwD3</a></p>
<p><b>Keywords</b>: three towers, contrastive learning, transformers, vision transformers, pretrained models, representation learning, finetuning, CLIP, ALIGN, classification, zero-shot, few-shot, retrieval
</p><p><b>Compressor summary</b>: Three Towers (3T) is a method that enhances contrastive learning of vision-language models by combining pretrained image classifiers with contrastive training, improving performance on retrieval tasks and classification for some pretraining datasets.</p><hr><h3>Asymptotically Optimal Quantile Pure Exploration for Infinite-Armed Bandits</h3>
<p>Xiao-Yue Gong, Mark Sellke</p>
<p><a href='https://openreview.net/forum?id=LROEcjVkv5'>https://openreview.net/forum?id=LROEcjVkv5</a></p>
<p><b>Keywords</b>: pure exploration, multi-armed bandits, Fisher information
</p><p><b>Compressor summary</b>: The paper studies how to efficiently select one high quality arm in a setting with infinitely many arms, considering both fixed confidence and fixed budget scenarios.</p><hr><h3>An Inverse Scaling Law for CLIP Training</h3>
<p>Xianhang Li, Zeyu Wang, Cihang Xie</p>
<p><a href='https://openreview.net/forum?id=LMU2RNwdh2'>https://openreview.net/forum?id=LMU2RNwdh2</a></p>
<p><b>Keywords</b>: CLIP, inverse scaling, efficient training
</p><p><b>Compressor summary</b>: The paper finds an inverse scaling law for CLIP training, reducing its computational cost and enabling faster training with limited resources.</p><hr><h3>Disentanglement via Latent Quantization</h3>
<p>Kyle Hsu, Will Dorrell, James C. R. Whittington, Jiajun Wu, Chelsea Finn</p>
<p><a href='https://openreview.net/forum?id=LLETO26Ga2'>https://openreview.net/forum?id=LLETO26Ga2</a></p>
<p><b>Keywords</b>: disentanglement, unsupervised learning, quantization
</p><p><b>Compressor summary</b>: The authors propose an inductive bias for latent space encoding and decoding using quantization and regularization, which improves disentangled representation learning in various generative models, and introduce new metrics called InfoMEC to evaluate disentanglement reliably.</p><hr><h3>Variational Weighting for Kernel Density Ratios</h3>
<p>Sangwoong Yoon, Frank C. Park, Gunsu S YUN, Iljung Kim, Yung-Kyun Noh</p>
<p><a href='https://openreview.net/forum?id=LIsJHQHi4z'>https://openreview.net/forum?id=LIsJHQHi4z</a></p>
<p><b>Keywords</b>: Kernel Density Estimation, KL-divergence, Density Ratio
</p><p><b>Compressor summary</b>: Kernel density estimation (KDE) improves predictions and information-theoretic measures by using an optimal weight function derived from multidimensional calculus of variations for density ratios.</p><hr><h3>Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes</h3>
<p>Zheng Wang, Shikai Fang, Shibo Li, Shandian Zhe</p>
<p><a href='https://openreview.net/forum?id=LGqIAn2OaZ'>https://openreview.net/forum?id=LGqIAn2OaZ</a></p>
<p><b>Keywords</b>: Tensor Decomposition, Representation Learning
</p><p><b>Compressor summary</b>: DEMOTE is a method that uses neural networks to capture temporal structure in sparse multiway data, by estimating dynamic embeddings for entities in each mode of a tensor.</p><hr><h3>Handling Data Heterogeneity via Architectural Design for Federated Visual Recognition</h3>
<p>Sara Pieri, Jose Renato Restom, Samuel Horváth, Hisham Cholakkal</p>
<p><a href='https://openreview.net/forum?id=LGKxz9clGG'>https://openreview.net/forum?id=LGKxz9clGG</a></p>
<p><b>Keywords</b>: Computer Vision, Federated Learning, Image Classification, Neural Network Architectures, Transformer, CNN, Data Hetereogenity, non-IID
</p><p><b>Compressor summary</b>: The paper reviews federated learning for visual recognition, highlighting the crucial role of architecture choices to achieve optimal performance and narrow the gap with centralized learning.</p><hr><h3>Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization</h3>
<p>Mahyar Fazlyab, Taha Entesari, Aniket Roy, Rama Chellappa</p>
<p><a href='https://openreview.net/forum?id=LDhhi8HBO3'>https://openreview.net/forum?id=LDhhi8HBO3</a></p>
<p><b>Keywords</b>: Deep Learning, Adversarial Robustness, Certified Radius, Lipschitz Constants
</p><p><b>Compressor summary</b>: The paper proposes a differentiable regularizer that uses Lipschitz bounds on neural networks to improve robustness against adversarial perturbations, and shows better results than existing methods on image classification tasks.</p><hr><h3>Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products</h3>
<p>Tamas Sarlos, Xingyou Song, David Woodruff, Qiuyi Zhang</p>
<p><a href='https://openreview.net/forum?id=LCwToX315b'>https://openreview.net/forum?id=LCwToX315b</a></p>
<p><b>Keywords</b>: Low rank approximation, kernel methods, fine-grained complexity
</p><p><b>Compressor summary</b>: The authors study low rank approximation of a function involving two matrices, and show that certain conditions are necessary for fast algorithms, while also giving new hardness results and efficient algorithms for some cases.</p><hr><h3>Normalization-Equivariant Neural Networks with Application to Image Denoising</h3>
<p>Sébastien Herbreteau, Emmanuel Moebel, Charles Kervrann</p>
<p><a href='https://openreview.net/forum?id=LCnjG1IEfm'>https://openreview.net/forum?id=LCnjG1IEfm</a></p>
<p><b>Keywords</b>: equivariance, normalization, image denoising, activation functions, ReLU, interpretability, robustness, deep learning, analysis of neural networks
</p><p><b>Compressor summary</b>: The text proposes a method to adapt neural networks for normalization-equivariance, which is beneficial for many applications, by replacing existing layers and functions with new ones.</p><hr><h3>Cascading Bandits: Optimizing Recommendation Frequency in Delayed Feedback Environments</h3>
<p>Dairui Wang, Junyu Cao, Yan Zhang, Wei Qi</p>
<p><a href='https://openreview.net/forum?id=LClyG4vZmS'>https://openreview.net/forum?id=LClyG4vZmS</a></p>
<p><b>Keywords</b>: delayed feedback, recommender system, frequency control
</p><p><b>Compressor summary</b>: This paper proposes a new bandit problem in dynamic recommender systems and presents several algorithms to optimize recommendation frequency and user engagement based on delayed feedback.</p><hr><h3>SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding</h3>
<p>Paul-Edouard Sarlin, Eduard Trulls, Marc Pollefeys, Jan Hosang, Simon Lynen</p>
<p><a href='https://openreview.net/forum?id=LCHmP68Gtj'>https://openreview.net/forum?id=LCHmP68Gtj</a></p>
<p><b>Keywords</b>: neural maps, visual positioning, semantic mapping
</p><p><b>Compressor summary</b>: SNAP is a deep network that learns 2D neural maps from images and can create better, more detailed, and semantically richer maps than traditional methods.</p><hr><h3>Trial matching: capturing variability with data-constrained spiking neural networks</h3>
<p>Christos Sourmpis, Carl C. H. Petersen, Wulfram Gerstner, Guillaume Bellec</p>
<p><a href='https://openreview.net/forum?id=LAbxkhkjbD'>https://openreview.net/forum?id=LAbxkhkjbD</a></p>
<p><b>Keywords</b>: neuroscience, spiking networks, data-constrained modeling, electrophysiological recordings, optimal transport, trial variability, RNN, interpretable machine learning
</p><p><b>Compressor summary</b>: The authors use a neural network model fitted to mouse behavioral and neural data to simulate sensory-motor processes and identify different modes of variability in the data.</p><hr><h3>Explaining the Uncertain: Stochastic Shapley Values for Gaussian Process Models</h3>
<p>Siu Lun Chau, Krikamol Muandet, Dino Sejdinovic</p>
<p><a href='https://openreview.net/forum?id=LAGxc2ybuH'>https://openreview.net/forum?id=LAGxc2ybuH</a></p>
<p><b>Keywords</b>: Gaussian Processes, Shapley values, Uncertainty Modelling
</p><p><b>Compressor summary</b>: The authors introduce a method to explain Gaussian processes using Shapley values from stochastic cooperative games, which allows for uncertainty quantification and predictive explanations.</p><hr><h3>GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients</h3>
<p>Sima Behpour, Thang Doan, Xin Li, Wenbin He, Liang Gou, Liu Ren</p>
<p><a href='https://openreview.net/forum?id=L9nTuSbAws'>https://openreview.net/forum?id=L9nTuSbAws</a></p>
<p><b>Keywords</b>: out-of-distribution detection, OOD, uncertainty estimation, gradient projection
</p><p><b>Compressor summary</b>: GradOrth is a novel approach for detecting out-of-distribution data that uses important features from lower-rank subspaces of in-distribution data, achieving better performance than existing methods.</p><hr><h3>Latent Field Discovery in Interacting Dynamical Systems with Neural Fields</h3>
<p>Miltiadis Kofinas, Erik J Bekkers, Naveen Shankar Nagaraja, Efstratios Gavves</p>
<p><a href='https://openreview.net/forum?id=L9ZTvJ5jVx'>https://openreview.net/forum?id=L9ZTvJ5jVx</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Neural Fields, Field Discovery, Equivariance, Interacting Dynamical Systems, Geometric Graphs
</p><p><b>Compressor summary</b>: The authors propose a method to infer hidden force fields from observed dynamics using equivariant graph networks and neural fields, which can improve prediction of system evolution.</p><hr><h3>Efficient Batched Algorithm for Contextual Linear Bandits with Large Action Space via Soft Elimination</h3>
<p>Osama Hanna, Lin Yang, Christina Fragouli</p>
<p><a href='https://openreview.net/forum?id=L7Whl9pXd0'>https://openreview.net/forum?id=L7Whl9pXd0</a></p>
<p><b>Keywords</b>: efficient bandit algorithms, contextual linear bandits
</p><p><b>Compressor summary</b>: The paper presents an efficient batched algorithm for contextual linear bandits with large action spaces using a novel soft elimination approach and achieving low regret bounds.</p><hr><h3>PrObeD: Proactive Object Detection Wrapper</h3>
<p>Vishal Asnani, Abhinav Kumar, Suya You, Xiaoming Liu</p>
<p><a href='https://openreview.net/forum?id=L74NTrzH1O'>https://openreview.net/forum?id=L74NTrzH1O</a></p>
<p><b>Keywords</b>: Object detection, proactive, Camouflage, 2D
</p><p><b>Compressor summary</b>: PrObeD is a proactive scheme that enhances 2D object detection performance by learning an image-dependent template that acts as a mask for the input images, improving detection for generic and camouflaged objects.</p><hr><h3>Truly Scale-Equivariant Deep Nets with Fourier Layers</h3>
<p>Md Ashiqur Rahman, Raymond A. Yeh</p>
<p><a href='https://openreview.net/forum?id=L0QwnevT0F'>https://openreview.net/forum?id=L0QwnevT0F</a></p>
<p><b>Keywords</b>: Scale Equivariance, Fourier Neural Network
</p><p><b>Compressor summary</b>: The paper presents a new scale-equivariant convolutional neural network architecture that considers anti-aliasing in down-scaling and performs well on image classification tasks.</p><hr><h3>Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation</h3>
<p>Zihao Yue, Anwen Hu, Liang Zhang, Qin Jin</p>
<p><a href='https://openreview.net/forum?id=Kvaa3DhvlZ'>https://openreview.net/forum?id=Kvaa3DhvlZ</a></p>
<p><b>Keywords</b>: Image Captioning, Learning Objective, Natural Language Processing
</p><p><b>Compressor summary</b>: SMILE is a method for image captioning that encourages richer and more detailed descriptions by preventing conciseness optimization.</p><hr><h3>Spatial-frequency channels, shape bias, and adversarial robustness</h3>
<p>Ajay Subramanian, Elena Sizikova, Najib J. Majaj, Denis G. Pelli</p>
<p><a href='https://openreview.net/forum?id=KvPwXVcslY'>https://openreview.net/forum?id=KvPwXVcslY</a></p>
<p><b>Keywords</b>: object recognition, critical band masking, spatial-frequency channels, shape bias, adversarial robustness
</p><p><b>Compressor summary</b>: Critical band masking shows humans and neural networks use different frequency filters for object recognition, with networks having broader filters that make them less robust to noise.</p><hr><h3>Tester-Learners for Halfspaces: Universal Algorithms</h3>
<p>Aravind Gollakota, Adam Klivans, Konstantinos Stavropoulos, Arsen Vasilyan</p>
<p><a href='https://openreview.net/forum?id=Kv8GJkV19S'>https://openreview.net/forum?id=Kv8GJkV19S</a></p>
<p><b>Keywords</b>: testable learning, pac learning, agnostic learning, Massart label noise, adversarial label noise, distribution testing
</p><p><b>Compressor summary</b>: The paragraph describes a universal tester-learner for halfspaces that works on many structured distributions, has low error, and uses sum-of-squares programs to check hypercontractivity of unknown distributions.</p><hr><h3>Describe, Explain, Plan and Select: Interactive Planning with LLMs Enables Open-World Multi-Task Agents</h3>
<p>Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang</p>
<p><a href='https://openreview.net/forum?id=KtvPdGb31Z'>https://openreview.net/forum?id=KtvPdGb31Z</a></p>
<p><b>Keywords</b>: open-ended learning, multi task, large language models, zero-shot planning
</p><p><b>Compressor summary</b>: The paper presents DESPs, a novel interactive planning method for Minecraft agents using LLMs, which enables a zero-shot multi-task agent to achieve better results in various domains.</p><hr><h3>IDRNet: Intervention-Driven Relation Network for Semantic Segmentation</h3>
<p>Zhenchao Jin, Xiaowei Hu, Lingting Zhu, Luchuan Song, Li Yuan, Lequan Yu</p>
<p><a href='https://openreview.net/forum?id=KtHquQuyA5'>https://openreview.net/forum?id=KtHquQuyA5</a></p>
<p><b>Keywords</b>: semantic segmentation, relation modeling, object detection
</p><p><b>Compressor summary</b>: IDRNet is a novel context modeling paradigm that uses deletion diagnostics to improve dense prediction tasks by enhancing pixel-level representations with semantic-level interactions.</p><hr><h3>Circuit as Set of Points</h3>
<p>Jialv Zou, Xinggang Wang, JiaHao Guo, Wenyu Liu, Qian Zhang, Chang Huang</p>
<p><a href='https://openreview.net/forum?id=KsICioDlYs'>https://openreview.net/forum?id=KsICioDlYs</a></p>
<p><b>Keywords</b>: EDA, Circuit Design, Congestion prediction, DRC violation prediction
</p><p><b>Compressor summary</b>: The authors propose a new method for circuit design using Transformer-based point cloud perception to extract features directly from raw data, achieving high performance in various tasks.</p><hr><h3>Time-Independent Information-Theoretic Generalization Bounds for SGLD</h3>
<p>Futoshi Futami, Masahiro Fujisawa</p>
<p><a href='https://openreview.net/forum?id=Ks0RSFNxPO'>https://openreview.net/forum?id=Ks0RSFNxPO</a></p>
<p><b>Keywords</b>: SGLD, Langevin dynamics, Generalization, Information theoretic analysis
</p><p><b>Compressor summary</b>: The paper proposes new generalization bounds for SGLD that are time-independent, decay with sample size, and account for stability of datasets, improving upon previous work.</p><hr><h3>StyleDrop: Text-to-Image Synthesis of Any Style</h3>
<p>Kihyuk Sohn, Lu Jiang, Jarred Barber, Kimin Lee, Nataniel Ruiz, Dilip Krishnan, Huiwen Chang, Yuanzhen Li, Irfan Essa, Michael Rubinstein, Yuan Hao, Glenn Entis, Irina Blok, Daniel Castro Chin</p>
<p><a href='https://openreview.net/forum?id=KoaFh16uOc'>https://openreview.net/forum?id=KoaFh16uOc</a></p>
<p><b>Keywords</b>: text-to-image synthesis, fine-tuning, stylization
</p><p><b>Compressor summary</b>: StyleDrop is a versatile method that allows synthesizing images with specific styles using text-to-image models, even with just one image as input, and outperforms other methods in style tuning.</p><hr><h3>Distributed Personalized Empirical Risk Minimization</h3>
<p>Yuyang Deng, Mohammad Mahdi Kamani, Pouria Mahdavinia, Mehrdad Mahdavi</p>
<p><a href='https://openreview.net/forum?id=KoQgA0coZ9'>https://openreview.net/forum?id=KoQgA0coZ9</a></p>
<p><b>Keywords</b>: distributed learning, heterogeneous data, heterogeneous system, convergence analysis
</p><p><b>Compressor summary</b>: The paper introduces PERM, a new learning method for heterogeneous data that adapts models to each client's needs and constraints, using a distributed algorithm that optimizes PERM objectives and allows different model architectures.</p><hr><h3>Disentangling Voice and Content with Self-Supervision for Speaker Recognition</h3>
<p>TIANCHI LIU, Kong Aik Lee, Qiongqiong Wang, Haizhou Li</p>
<p><a href='https://openreview.net/forum?id=KoFYzuwjCA'>https://openreview.net/forum?id=KoFYzuwjCA</a></p>
<p><b>Keywords</b>: speaker recognition, disentanglement learning, self-supervision
</p><p><b>Compressor summary</b>: The paper introduces a framework that uses three Gaussian inference layers to model speaker traits and content variability in speech, disentangling them without labels other than speaker identities, and shows its effectiveness on VoxCeleb and SITW datasets.</p><hr><h3>Generalizing Importance Weighting to A Universal Solver for Distribution Shift Problems</h3>
<p>Tongtong Fang, Nan Lu, Gang Niu, Masashi Sugiyama</p>
<p><a href='https://openreview.net/forum?id=KmdlUP23qh'>https://openreview.net/forum?id=KmdlUP23qh</a></p>
<p><b>Keywords</b>: importance weighting, distribution shift, deep learning
</p><p><b>Compressor summary</b>: The paper proposes a generalized importance weighting method to handle distribution shift problems in machine learning, addressing the limitations of existing methods.</p><hr><h3>An Information Theory Perspective on Variance-Invariance-Covariance Regularization</h3>
<p>Ravid Shwartz-Ziv, Randall Balestriero, Kenji Kawaguchi, Tim G. J. Rudner, Yann LeCun</p>
<p><a href='https://openreview.net/forum?id=KipjqOPaZ0'>https://openreview.net/forum?id=KipjqOPaZ0</a></p>
<p><b>Keywords</b>: Self-Supervised Learning, Generalization Bounds, Information-Theory, Deep Neural Networks
</p><p><b>Compressor summary</b>: The paper explores the mechanisms behind VICReg, an SSL method, and introduces new SSL methods based on information-theoretic principles that perform better than existing ones.</p><hr><h3>Transferable Adversarial Robustness for Categorical Data via Universal Robust Embeddings</h3>
<p>Klim Kireev, Maksym Andriushchenko, Carmela Troncoso, Nicolas Flammarion</p>
<p><a href='https://openreview.net/forum?id=Kig2YJVYfq'>https://openreview.net/forum?id=Kig2YJVYfq</a></p>
<p><b>Keywords</b>: Tabular data, Categorical data, Robust ML, Adversarial Robustness
</p><p><b>Compressor summary</b>: The paper proposes a method to train robust deep networks and transfer robustness to other classifiers using categorical feature embeddings for tabular data, which improves performance under adversarial attacks compared to existing methods.</p><hr><h3>Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms</h3>
<p>Dheeraj Baby, Saurabh Garg, Tzu-Ching Yen, Sivaraman Balakrishnan, Zachary Chase Lipton, Yu-Xiang Wang</p>
<p><a href='https://openreview.net/forum?id=Ki6DqBXss4'>https://openreview.net/forum?id=Ki6DqBXss4</a></p>
<p><b>Keywords</b>: online learning, label shift, distribution shift, unsupervised domain adaptation
</p><p><b>Compressor summary</b>: The paper presents algorithms for adapting to online label shift in both supervised and unsupervised settings, using bootstrapped online regression oracles to track drifting class marginals and achieve optimal dynamic regret with high accuracy.</p><hr><h3>VoxDet: Voxel Learning for Novel Instance Detection</h3>
<p>Bowen Li, Jiashun Wang, Yaoyu Hu, Chen Wang, Sebastian Scherer</p>
<p><a href='https://openreview.net/forum?id=KgqucdSwIe'>https://openreview.net/forum?id=KgqucdSwIe</a></p>
<p><b>Keywords</b>: Unseen object detection, instance perception, voxel representation
</p><p><b>Compressor summary</b>: The text introduces VoxDet, a 3D geometry-aware framework that uses voxel representation and matching techniques to improve object detection in challenging scenarios involving occlusion and pose variations.</p><hr><h3>Demystifying Oversmoothing in Attention-Based Graph Neural Networks</h3>
<p>Xinyi Wu, Amir Ajorlou, Zihui Wu, Ali Jadbabaie</p>
<p><a href='https://openreview.net/forum?id=Kg65qieiuB'>https://openreview.net/forum?id=Kg65qieiuB</a></p>
<p><b>Keywords</b>: graph neural networks, attention mechanisms, oversmoothing, dynamical systems, theory
</p><p><b>Compressor summary</b>: The paper analyzes the effect of graph attention on oversmoothing in different types of Graph Neural Networks (GNNs) using mathematical tools and shows that it does not prevent oversmoothing and loses expressive power exponentially.</p><hr><h3>Towards Optimal Effective Resistance Estimation</h3>
<p>Rajat Vadiraj Dwaraknath, Ishani Karmarkar, Aaron Sidford</p>
<p><a href='https://openreview.net/forum?id=KffE8iXAw7'>https://openreview.net/forum?id=KffE8iXAw7</a></p>
<p><b>Keywords</b>: effective resistances, spectral sketch, fine-grained complexity, triangle detection, numerical linear algebra
</p><p><b>Compressor summary</b>: The paper presents new algorithms for estimating effective resistances in undirected expander graphs with faster runtimes than previous methods, as well as a conditional lower bound showing that these algorithms are optimal up to a factor of $\epsilon$.</p><hr><h3>Wasserstein distributional robustness of neural networks</h3>
<p>Xingjian Bai, Guangyi He, Yifan Jiang, Jan Obloj</p>
<p><a href='https://openreview.net/forum?id=KfOUAlraMP'>https://openreview.net/forum?id=KfOUAlraMP</a></p>
<p><b>Keywords</b>: adversarial attack, adversarial robustness of DNN, adversarial training, Wasserstein distance, distributionally robust optimization, sensitivity analysis, asymptotic bounds
</p><p><b>Compressor summary</b>: The authors propose a method for evaluating the robustness of deep neural networks against adversarial attacks using Wasserstein distributionally robust optimization and show its effectiveness on various image recognition tasks.</p><hr><h3>Layer-Neighbor Sampling --- Defusing Neighborhood Explosion in GNNs</h3>
<p>Muhammed Fatih Balin, Umit Catalyurek</p>
<p><a href='https://openreview.net/forum?id=Kd5W4JRsfV'>https://openreview.net/forum?id=Kd5W4JRsfV</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Graph Sampling, GNN, Layer Sampling, Minibatch Training
</p><p><b>Compressor summary</b>: LABOR is a new sampling algorithm for Graph Neural Networks that samples fewer vertices with better performance and convergence than existing methods.</p><hr><h3>Blockwise Parallel Transformers for Large Context Models</h3>
<p>Hao Liu, Pieter Abbeel</p>
<p><a href='https://openreview.net/forum?id=KbqQMoqfLQ'>https://openreview.net/forum?id=KbqQMoqfLQ</a></p>
<p><b>Keywords</b>: Language Model, Long Context Modeling, Reinforcement Learning
</p><p><b>Compressor summary</b>: The Blockwise Parallel Transformer (BPT) is a new method that reduces memory costs and enables longer input sequences for natural language processing models by processing them in blocks, outperforming previous methods on language modeling and reinforcement learning tasks.</p><hr><h3>Prediction and Control in Continual Reinforcement Learning</h3>
<p>Nishanth Anand, Doina Precup</p>
<p><a href='https://openreview.net/forum?id=KakzVASqul'>https://openreview.net/forum?id=KakzVASqul</a></p>
<p><b>Keywords</b>: reinforcement learning, continual reinforcement learning, lifelong learning, never-ending learning, prediction, control, multi-task learning, complementary learning systems
</p><p><b>Compressor summary</b>: The paper proposes a value function decomposition into permanent and transient components for continual reinforcement learning, showing theoretical and empirical benefits.</p><hr><h3>ReContrast: Domain-Specific Anomaly Detection via Contrastive Reconstruction</h3>
<p>Jia Guo, shuai lu, LIze JIa, Weihang Zhang, Huiqi Li</p>
<p><a href='https://openreview.net/forum?id=KYxD9YCQBH'>https://openreview.net/forum?id=KYxD9YCQBH</a></p>
<p><b>Keywords</b>: Unsupervised Anomaly Detection, Contrastive Learning, Medical Anomaly Detection, Transfer Learning
</p><p><b>Compressor summary</b>: The paper introduces ReContrast, a new unsupervised anomaly detection method that adapts to different domains by optimizing the entire network for feature reconstruction and contrastive learning.</p><hr><h3>Faster Relative Entropy Coding with Greedy Rejection Coding</h3>
<p>Gergely Flamich, Stratis Markou, José Miguel Hernández-Lobato</p>
<p><a href='https://openreview.net/forum?id=KXbAgvLi2l'>https://openreview.net/forum?id=KXbAgvLi2l</a></p>
<p><b>Keywords</b>: Compression, Learnt Compression, Relative Entropy Coding, Information Theory
</p><p><b>Compressor summary</b>: The paper introduces Greedy Rejection Coding (GRC), a new algorithm for relative entropy coding that improves runtime and codelength for continuous distributions, and evaluates it on a compression pipeline with variational autoencoders.</p><hr><h3>Trading-off price for data quality to achieve fair online allocation</h3>
<p>Mathieu Molina, Nicolas Gast, Patrick Loiseau, Vianney Perchet</p>
<p><a href='https://openreview.net/forum?id=KUBFYAPdqN'>https://openreview.net/forum?id=KUBFYAPdqN</a></p>
<p><b>Keywords</b>: Fairness, Online allocation, Bandits algorithms
</p><p><b>Compressor summary</b>: The paper proposes an algorithm that balances fairness and efficiency in online allocation problems by choosing data sources and allocating resources based on multi-armed bandit theory.</p><hr><h3>Reinforcement Learning with Fast and Forgetful Memory</h3>
<p>Steven Morad, Ryan Kortvelesy, Stephan Liwicki, Amanda Prorok</p>
<p><a href='https://openreview.net/forum?id=KTfAtro6vP'>https://openreview.net/forum?id=KTfAtro6vP</a></p>
<p><b>Keywords</b>: reinforcement learning, partially observable, POMDP, memory, rnn, transformer
</p><p><b>Compressor summary</b>: Fast and Forgetful Memory is a memory model for Reinforcement Learning that improves reward and training speed compared to recurrent neural networks, without changing hyperparameters or adding more resources.</p><hr><h3>On the Constrained Time-Series Generation Problem</h3>
<p>Andrea Coletta, Sriram Gopalakrishnan, Daniel Borrajo, Svitlana Vyetrenko</p>
<p><a href='https://openreview.net/forum?id=KTZttLZekH'>https://openreview.net/forum?id=KTZttLZekH</a></p>
<p><b>Keywords</b>: time-series, generative models, constrained optimization, machine learning
</p><p><b>Compressor summary</b>: The paper introduces novel methods for generating realistic constrained time series using a constrained optimization framework and a guided diffusion model, achieving better performance and efficiency than existing approaches.</p><hr><h3>Conformal Prediction for Time Series with Modern Hopfield Networks</h3>
<p>Andreas Auer, Martin Gauch, Daniel Klotz, Sepp Hochreiter</p>
<p><a href='https://openreview.net/forum?id=KTRwpWCMsC'>https://openreview.net/forum?id=KTRwpWCMsC</a></p>
<p><b>Keywords</b>: time series, uncertainty, prediction interval, conformal prediction, modern hopfield networks
</p><p><b>Compressor summary</b>: HopCPT is a novel conformal prediction method for time series that handles temporal dependencies and performs better than existing methods on various datasets.</p><hr><h3>Aligning Optimization Trajectories with Diffusion Models for Constrained Design Generation</h3>
<p>Giorgio Giannone, Akash Srivastava, Ole Winther, Faez Ahmed</p>
<p><a href='https://openreview.net/forum?id=KTR33hMnMX'>https://openreview.net/forum?id=KTR33hMnMX</a></p>
<p><b>Keywords</b>: diffusion models, engineering design, generative optimization, trajectory matching
</p><p><b>Compressor summary</b>: Diffusion Optimization Models (DOM) align diffusion model sampling with physics-based optimization for efficient, feasible, and high-performance designs in constrained settings with limited data.</p><hr><h3>DAW: Exploring the Better Weighting Function for Semi-supervised Semantic Segmentation</h3>
<p>Rui Sun, Huayu Mai, Tianzhu Zhang, Feng Wu</p>
<p><a href='https://openreview.net/forum?id=KRlG7NJUCD'>https://openreview.net/forum?id=KRlG7NJUCD</a></p>
<p><b>Keywords</b>: semi-supervised semantic segmentation
</p><p><b>Compressor summary</b>: The paper analyzes a trade-off in semi-supervised semantic segmentation methods when handling pseudo-labels and proposes Distribution-Aware Weighting (DAW) to improve the model's generalization performance.</p><hr><h3>SOC: Semantic-Assisted  Object Cluster for Referring Video Object Segmentation</h3>
<p>Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yitong Wang, Yansong Tang, Xiu Li, Yujiu Yang</p>
<p><a href='https://openreview.net/forum?id=KQyXyIAfK8'>https://openreview.net/forum?id=KQyXyIAfK8</a></p>
<p><b>Keywords</b>: Referring Video Object Segmentation, Video-Level Multi-Modal Understanding, Object Cluster, Visual-Linguistic Contrastive Learning
</p><p><b>Compressor summary</b>: The paper proposes a new approach, Semantic-assisted Object Cluster (SOC), that improves referring video object segmentation by aggregating video content and textual guidance for unified temporal modeling and cross-modal alignment.</p><hr><h3>Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments</h3>
<p>Thanh-Dat Truong, Hoang-Quan Nguyen, Bhiksha Raj, Khoa Luu</p>
<p><a href='https://openreview.net/forum?id=KQ25VgEvOJ'>https://openreview.net/forum?id=KQ25VgEvOJ</a></p>
<p><b>Keywords</b>: Fairness Continual Learning; Semantic Segmentation; Contrastive Clustering;
</p><p><b>Compressor summary</b>: The paper proposes a new fairness continual learning framework for semantic segmentation that addresses challenges like catastrophic forgetting and background shift using novel losses and achieves state-of-the-art results on three benchmarks.</p><hr><h3>Fast Attention Requires Bounded Entries</h3>
<p>Josh Alman, Zhao Song</p>
<p><a href='https://openreview.net/forum?id=KOVWXcrFIK'>https://openreview.net/forum?id=KOVWXcrFIK</a></p>
<p><b>Keywords</b>: fast attention computation, algorithm, hardness
</p><p><b>Compressor summary</b>: The paper investigates faster algorithms for inner product attention computation using implicit use of the attention matrix A, and shows a sharp transition at matrix dimensions B = Θ(√log n).</p><hr><h3>One-Pass Distribution Sketch for Measuring Data Heterogeneity in Federated Learning</h3>
<p>Zichang Liu, Zhaozhuo Xu, Benjamin Coleman, Anshumali Shrivastava</p>
<p><a href='https://openreview.net/forum?id=KMxRQO7P98'>https://openreview.net/forum?id=KMxRQO7P98</a></p>
<p><b>Keywords</b>: Distribution sketch, federated learning
</p><p><b>Compressor summary</b>: The paper proposes a one-pass distribution sketch that measures data heterogeneity in federated learning, improving client selection and handling cold start problems.</p><hr><h3>First Order Methods with Markovian Noise: from Acceleration to Variational Inequalities</h3>
<p>Aleksandr Beznosikov, Sergey Samsonov, Marina Sheshukova, Alexander Gasnikov, Alexey Naumov, Eric Moulines</p>
<p><a href='https://openreview.net/forum?id=KMeFZopsqP'>https://openreview.net/forum?id=KMeFZopsqP</a></p>
<p><b>Keywords</b>: convex optimization, stochastic optimization, Markovian noise, acceleration, variational inequalities, lower bounds
</p><p><b>Compressor summary</b>: The paper proposes a unified approach for analyzing first-order gradient methods for stochastic optimization and variational inequalions with Markovian noise, eliminating some limiting assumptions and achieving optimal dependence on mixing time.</p><hr><h3>SE(3) Equivariant Augmented Coupling Flows</h3>
<p>Laurence Illing Midgley, Vincent Stimper, Javier Antoran, Emile Mathieu, Bernhard Schölkopf, José Miguel Hernández-Lobato</p>
<p><a href='https://openreview.net/forum?id=KKxO6wwx8p'>https://openreview.net/forum?id=KKxO6wwx8p</a></p>
<p><b>Keywords</b>: Boltzmann generator, normalizing flow
</p><p><b>Compressor summary</b>: Coupling normalizing flows with SE(3) and permutation equivariance are proposed for fast probabilistic modeling of physical systems, enabling efficient sampling and estimations.</p><hr><h3>SEGA: Instructing Text-to-Image Models using Semantic Guidance</h3>
<p>Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, Kristian Kersting</p>
<p><a href='https://openreview.net/forum?id=KIPAIy329j'>https://openreview.net/forum?id=KIPAIy329j</a></p>
<p><b>Keywords</b>: diffusion, text-to-image, generation, semantics
</p><p><b>Compressor summary</b>: The paper introduces a method to interact with text-to-image diffusion models for semantic guidance, enabling users to control the image generation process more effectively.</p><hr><h3>MeCo: Zero-Shot NAS with One Data and Single Forward Pass via Minimum Eigenvalue of Correlation</h3>
<p>Tangyu Jiang, Haodi Wang, Rongfang Bie</p>
<p><a href='https://openreview.net/forum?id=KFm2lZiI7n'>https://openreview.net/forum?id=KFm2lZiI7n</a></p>
<p><b>Keywords</b>: Neural Architecture Search, Zero-Cost Proxy, Evaluation Strategy, Feature Map
</p><p><b>Compressor summary</b>: The paper proposes a novel zero-cost proxy called $\mathsf{MeCo}$ for neural architecture search, which uses the Pearson correlation matrix of feature maps and requires only one random data for a single forward pass.</p><hr><h3>Multi-Step Generalized Policy Improvement by Leveraging Approximate Models</h3>
<p>Lucas Nunes Alegre, Ana L. C. Bazzan, Ann Nowe, Bruno Castro da Silva</p>
<p><a href='https://openreview.net/forum?id=KFj0Q1EXvU'>https://openreview.net/forum?id=KFj0Q1EXvU</a></p>
<p><b>Keywords</b>: generalized policy improvement, successor features, transfer learning, model-based reinforcement learning
</p><p><b>Compressor summary</b>: The paper presents a new method, h-GPI, for zero-shot transfer in reinforcement learning that combines model-free and model-based approaches and improves performance over existing methods.</p><hr><h3>On the Generalization Error of Stochastic Mirror Descent for Quadratically-Bounded Losses: an Improved Analysis</h3>
<p>Ta Duy Nguyen, Alina Ene, Huy Nguyen</p>
<p><a href='https://openreview.net/forum?id=KF4LCXz8Np'>https://openreview.net/forum?id=KF4LCXz8Np</a></p>
<p><b>Keywords</b>: high probability, generalization, convex optimization, nonconvex optimization
</p><p><b>Compressor summary</b>: The authors propose a new method to analyze the generalization error of stochastic mirror descent for quadratically bounded losses and obtain improved bounds in various settings.</p><hr><h3>The probability flow ODE is provably fast</h3>
<p>Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, Adil Salim</p>
<p><a href='https://openreview.net/forum?id=KD6MFeWSAd'>https://openreview.net/forum?id=KD6MFeWSAd</a></p>
<p><b>Keywords</b>: DDIM, deterministic samplers, diffusion models, predictor-corrector, probability flow ODE, score-based generative modeling
</p><p><b>Compressor summary</b>: The text describes a probabilistic flow ODE method for generative modeling that has polynomial-time convergence guarantees and outperforms DDPM in terms of dimension dependence.</p><hr><h3>Towards Free Data Selection with General-Purpose Models</h3>
<p>Yichen Xie, Mingyu Ding, Masayoshi Tomizuka, Wei Zhan</p>
<p><a href='https://openreview.net/forum?id=KBXcDAaZE7'>https://openreview.net/forum?id=KBXcDAaZE7</a></p>
<p><b>Keywords</b>: data selection, unsupervised learning
</p><p><b>Compressor summary</b>: The paper proposes FreeSel, an efficient data selection method that uses existing models to select data without training or supervision, achieving significant speedups compared to current active learning methods.</p><hr><h3>LIMA: Less Is More for Alignment</h3>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy</p>
<p><a href='https://openreview.net/forum?id=KBMOKmX2he'>https://openreview.net/forum?id=KBMOKmX2he</a></p>
<p><b>Keywords</b>: large language models, supervised instruction fine-tuning, chat assistant
</p><p><b>Compressor summary</b>: The paragraph discusses LIMA, a language model fine-tuned with supervised loss on a small number of prompts and responses, which shows strong performance and generalization, suggesting that pretraining is the most important stage for large language models.</p><hr><h3>PromptIR: Prompting for All-in-One Image Restoration</h3>
<p>Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, Fahad Khan</p>
<p><a href='https://openreview.net/forum?id=KAlSIL4tXU'>https://openreview.net/forum?id=KAlSIL4tXU</a></p>
<p><b>Keywords</b>: Image Restoration
</p><p><b>Compressor summary</b>: The paper introduces PromptIR, a method that uses prompts to guide a restoration network and achieve high-quality image restoration from various degradation types and levels without prior knowledge of the corruptions.</p><hr><h3>DropCompute: simple and more robust  distributed synchronous training via compute variance reduction</h3>
<p>Niv Giladi, Shahar Gottlieb, Moran Shkolnik, Asaf Karnieli, Ron Banner, Elad Hoffer, Kfir Yehuda Levy, Daniel Soudry</p>
<p><a href='https://openreview.net/forum?id=KAWaeKOEkx'>https://openreview.net/forum?id=KAWaeKOEkx</a></p>
<p><b>Keywords</b>: distributed optimization, large-scale parallel SGD, synchronous training
</p><p><b>Compressor summary</b>: The paper analyzes how worker variability affects the scalability and robustness of synchronous distributed DNN training, and proposes a decentralized method to reduce this variation.</p><hr><h3>Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling</h3>
<p>Haotao Wang, Ziyu Jiang, Yuning You, Yan Han, Gaowen Liu, Jayanth Srinivasa, Ramana Rao Kompella, Zhangyang Wang</p>
<p><a href='https://openreview.net/forum?id=K9xHDD6mic'>https://openreview.net/forum?id=K9xHDD6mic</a></p>
<p><b>Keywords</b>: graph neural networks
</p><p><b>Compressor summary</b>: The GMoE model improves GNNs by allowing nodes to select expert aggregators that capture different graph structures and information distances, leading to better performance on various graph tasks.</p><hr><h3>A Recurrent Neural Circuit Mechanism of Temporal-scaling Equivariant Representation</h3>
<p>Junfeng Zuo, Xiao Liu, Ying Nian Wu, Si Wu, Wenhao Zhang</p>
<p><a href='https://openreview.net/forum?id=K9dmkfZcMu'>https://openreview.net/forum?id=K9dmkfZcMu</a></p>
<p><b>Keywords</b>: temporal-scaling group, equivariant representation, disentangled representation, motor timing, continuous attractor networks
</p><p><b>Compressor summary</b>: The study proposes a recurrent circuit model that explains how the brain generates temporal sequences at different speeds, using control inputs and Lie group operators.</p><hr><h3>Corruption-Robust Offline Reinforcement Learning with General Function Approximation</h3>
<p>Chenlu Ye, Rui Yang, Quanquan Gu, Tong Zhang</p>
<p><a href='https://openreview.net/forum?id=K9M7XNS9BX'>https://openreview.net/forum?id=K9M7XNS9BX</a></p>
<p><b>Keywords</b>: offline RL; adversarial corruption; general function approximation
</p><p><b>Compressor summary</b>: The paper proposes a new algorithm to solve offline RL problems under corruption and provides a suboptimality bound that depends on the corruption level and other factors, showing tightness for linear MDPs.</p><hr><h3>Regularization properties of adversarially-trained linear regression</h3>
<p>Antonio H. Ribeiro, Dave Zachariah, Francis Bach, Thomas B. Schön</p>
<p><a href='https://openreview.net/forum?id=K8gLHZIgVW'>https://openreview.net/forum?id=K8gLHZIgVW</a></p>
<p><b>Keywords</b>: adversarial training; regularization; linear models
</p><p><b>Compressor summary</b>: Adversarial training for linear models can lead to minimum-norm interpolators, equivalent parameter shrinking methods, or independent of noise variance in some cases.</p><hr><h3>Neural Modulation for Flash Memory: An Unsupervised Learning Framework for Improved Reliability</h3>
<p>Jonathan Zedaka, Elisha Halperin, Evgeny Blaichman, Amit Berman</p>
<p><a href='https://openreview.net/forum?id=K7u3RkoBP9'>https://openreview.net/forum?id=K7u3RkoBP9</a></p>
<p><b>Keywords</b>: WGAN, GAN, Autoencoder, Unsupervised Learning, Generative models, Flash Memory, NAND, Modulation, Reliability, Flash, Communication system
</p><p><b>Compressor summary</b>: The authors propose a machine learning approach that reduces errors and data degradation in NAND flash memory by using a neural modulator that adapts programming operations for each memory cell.</p><hr><h3>Invariant Learning via Probability of Sufficient and Necessary Causes</h3>
<p>Mengyue Yang, Yonggang Zhang, Zhen Fang, Yali Du, Furui Liu, Jean-Francois Ton, Jianhong Wang, Jun Wang</p>
<p><a href='https://openreview.net/forum?id=K5e5tFZuur'>https://openreview.net/forum?id=K5e5tFZuur</a></p>
<p><b>Keywords</b>: OOD Generalization, Invariant Representation Learning
</p><p><b>Compressor summary</b>: The paper proposes a method to improve out-of-distribution generalization by using probability of sufficiency and necessary causes (PNS) to learn representations with high PNS value, which is theoretically analyzed and experimentally validated.</p><hr><h3>MAG-GNN: Reinforcement Learning Boosted Graph Neural Network</h3>
<p>Lecheng Kong, Jiarui Feng, Hao Liu, Dacheng Tao, Yixin Chen, Muhan Zhang</p>
<p><a href='https://openreview.net/forum?id=K4FK7I8Jnl'>https://openreview.net/forum?id=K4FK7I8Jnl</a></p>
<p><b>Keywords</b>: Graph Neural Network; Reinforcement Learning
</p><p><b>Compressor summary</b>: MAG-GNN is a reinforcement learning boosted graph neural network that uses combinatorial optimization to find the optimal subset of subgraphs for efficient and expressive graph learning tasks.</p><hr><h3>Fast Optimal Locally Private Mean Estimation via Random Projections</h3>
<p>Hilal Asi, Vitaly Feldman, Jelani Nelson, Huy Nguyen, Kunal Talwar</p>
<p><a href='https://openreview.net/forum?id=K3JgUvDSYX'>https://openreview.net/forum?id=K3JgUvDSYX</a></p>
<p><b>Keywords</b>: Differential Privacy, mean estimation, private federated learning, communication complexity
</p><p><b>Compressor summary</b>: The paper proposes ProjUnit, a simple and efficient algorithmic framework for private mean estimation of high-dimensional vectors with low communication and run-time complexity.</p><hr><h3>Controlling Text-to-Image Diffusion by Orthogonal Finetuning</h3>
<p>Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, Bernhard Schölkopf</p>
<p><a href='https://openreview.net/forum?id=K30wTdIIYc'>https://openreview.net/forum?id=K30wTdIIYc</a></p>
<p><b>Keywords</b>: Text-to-image, diffusion models, finetuning, generative models, orthogonality
</p><p><b>Compressor summary</b>: The Orthogonal Finetuning (OFT) method helps adapt text-to-image diffusion models for different tasks by preserving their semantic generation ability and improving finetuning stability.</p><hr><h3>Learning Mask-aware CLIP Representations for Zero-Shot Segmentation</h3>
<p>Siyu Jiao, Yunchao Wei, Yaowei Wang, Yao Zhao, Humphrey Shi</p>
<p><a href='https://openreview.net/forum?id=K1Uzj8tuwd'>https://openreview.net/forum?id=K1Uzj8tuwd</a></p>
<p><b>Keywords</b>: Zero-Shot Segmentation, Open-Vocabulary Segmentation, Fine-tuning
</p><p><b>Compressor summary</b>: The paper proposes Mask-aware Fine-tuning (MAFT), a method to improve zero-shot segmentation by making CLIP more responsive to different mask proposals using a new encoder and two types of loss functions.</p><hr><h3>Block Low-Rank Preconditioner with Shared Basis for Stochastic Optimization</h3>
<p>Jui-Nan Yen, Sai Surya Duvvuri, Inderjit S Dhillon, Cho-Jui Hsieh</p>
<p><a href='https://openreview.net/forum?id=JzQlGqBm8d'>https://openreview.net/forum?id=JzQlGqBm8d</a></p>
<p><b>Keywords</b>: Second Order Optimization, Optimization for deep networks
</p><p><b>Compressor summary</b>: The paper proposes a low-rank approximation method for preconditioning in second-order optimization that reduces time and memory complexity while maintaining performance on deep neural networks.</p><hr><h3>Opening the Vocabulary of Egocentric Actions</h3>
<p>Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao</p>
<p><a href='https://openreview.net/forum?id=JzQ7QClAdF'>https://openreview.net/forum?id=JzQ7QClAdF</a></p>
<p><b>Keywords</b>: video understanding, egocentric videos, open vocabulary
</p><p><b>Compressor summary</b>: The paper introduces a new action recognition task that can handle verbs and objects not seen during training using an object-agnostic verb encoder and a prompt-based object encoder based on CLIP representations.</p><hr><h3>Provably (More) Sample-Efficient Offline RL with Options</h3>
<p>Xiaoyan Hu, Ho-fung Leung</p>
<p><a href='https://openreview.net/forum?id=JwNXeBdkeo'>https://openreview.net/forum?id=JwNXeBdkeo</a></p>
<p><b>Keywords</b>: Learning with Options, Offline RL, Provably Efficient RL
</p><p><b>Compressor summary</b>: The paper studies how using options in offline reinforcement learning can improve sample efficiency and provide a novel information-theoretic lower bound for this scenario. It also proposes an algorithm (PEVIO) and analyzes two data-collection methods.</p><hr><h3>CODA: Generalizing to Open and Unseen Domains with Compaction and Disambiguation</h3>
<p>Chaoqi Chen, Luyao Tang, Yue Huang, Xiaoguang Han, Yizhou Yu</p>
<p><a href='https://openreview.net/forum?id=Jw0KRTjsGA'>https://openreview.net/forum?id=Jw0KRTjsGA</a></p>
<p><b>Keywords</b>: Domain generalization, domain shift, open class, source compaction, target disambiguation
</p><p><b>Compressor summary</b>: CODA is a novel framework for learning compact representations and adapting to open classes in machine learning, addressing domain shift and unseen categories by introducing virtual unknown classes and optimizing a new training objective.</p><hr><h3>DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation</h3>
<p>Qingkai Fang, Yan Zhou, Yang Feng</p>
<p><a href='https://openreview.net/forum?id=JvYSSPtQyk'>https://openreview.net/forum?id=JvYSSPtQyk</a></p>
<p><b>Keywords</b>: speech-to-speech translation, non-autoregressive translation, speech translation, directed acyclic transformer
</p><p><b>Compressor summary</b>: DASpeech is a non-autoregressive direct speech-to-speech translation model that uses a two-pass architecture, achieving fast and high-quality translations while preserving the source speaker's voice.</p><hr><h3>Train Hard, Fight Easy: Robust Meta Reinforcement Learning</h3>
<p>Ido Greenberg, Shie Mannor, Gal Chechik, Eli Meirom</p>
<p><a href='https://openreview.net/forum?id=JvOZ4IIjwP'>https://openreview.net/forum?id=JvOZ4IIjwP</a></p>
<p><b>Keywords</b>: meta reinforcement learning, robust reinforcement learning, safe reinforcement learning, risk sensitive reinforcement learning
</p><p><b>Compressor summary</b>: The paper proposes Robust Meta RL (RoML), a meta-algorithm that improves the performance of standard Meta RL methods in high-risk or difficult tasks by over-sampling them, thus increasing system reliability.</p><hr><h3>Penalising the biases in norm regularisation enforces sparsity</h3>
<p>Etienne Boursier, Nicolas Flammarion</p>
<p><a href='https://openreview.net/forum?id=JtIqG47DAQ'>https://openreview.net/forum?id=JtIqG47DAQ</a></p>
<p><b>Keywords</b>: Neural networks, Min norm interpolators, Sparsity, Representational cost
</p><p><b>Compressor summary</b>: This paper investigates how controlling the norm of neural network parameters affects generalization and shows that using a specific weighting factor ensures unique and sparse solutions for one hidden ReLU layer networks with unidimensional data.</p><hr><h3>From Distribution Learning in Training to Gradient Search in Testing for Combinatorial Optimization</h3>
<p>Yang Li, Jinpei Guo, Runzhong Wang, Junchi Yan</p>
<p><a href='https://openreview.net/forum?id=JtF0ugNMv2'>https://openreview.net/forum?id=JtF0ugNMv2</a></p>
<p><b>Keywords</b>: Machine Learning, Combinatorial Optimization, Generative Modeling, Diffusion Model
</p><p><b>Compressor summary</b>: The paragraph describes a new framework called T2TCO that improves Combinatorial Optimization problem solving by using generative modeling and gradient-based search, achieving significant performance gains on TSP and MIS problems.</p><hr><h3>Attentive Transfer Entropy to Exploit Transient Emergence of Coupling Effect</h3>
<p>Xiaolei Ru, Xin-Ya Zhang, Zijia Liu, Jack Murdoch Moore, Gang Yan</p>
<p><a href='https://openreview.net/forum?id=JpU5YmMKx7'>https://openreview.net/forum?id=JpU5YmMKx7</a></p>
<p><b>Keywords</b>: Directed coupled network reconstruction; Neuronal dynamics; Mutual information estimator; Attention mechanism; Transfer entropy.
</p><p><b>Compressor summary</b>: The text describes a novel method to reconstruct coupled biological neural networks from time series data using attention mechanism and Attentive Transfer Entropy, which can identify critical regions of strong coupling-drive.</p><hr><h3>ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers</h3>
<p>Kexun Zhang, Danqing Wang, Jingtao Xia, William Yang Wang, Lei Li</p>
<p><a href='https://openreview.net/forum?id=JolrEmMim6'>https://openreview.net/forum?id=JolrEmMim6</a></p>
<p><b>Keywords</b>: Large Language Models, Code Generation, Code Intelligence, Automatic Verification
</p><p><b>Compressor summary</b>: The proposed ALGO framework synthesizes algorithms with LLM-generated oracles to guide code generation and verify correctness, achieving significantly improved one-submission pass rates on algorithmic problems.</p><hr><h3>On Single-Index Models beyond Gaussian Data</h3>
<p>Aaron Zweig, Loucas Pillaud-Vivien, Joan Bruna</p>
<p><a href='https://openreview.net/forum?id=JkmvrheMe7'>https://openreview.net/forum?id=JkmvrheMe7</a></p>
<p><b>Keywords</b>: gradient descent, shallow neural networks
</p><p><b>Compressor summary</b>: This paper studies how gradient descent methods can learn non-linear features from data using single-index models, and shows that it works well for non-Gaussian data as well.</p><hr><h3>Towards Symmetry-Aware Generation of Periodic Materials</h3>
<p>Youzhi Luo, Chengkai Liu, Shuiwang Ji</p>
<p><a href='https://openreview.net/forum?id=Jkc74vn1aZ'>https://openreview.net/forum?id=Jkc74vn1aZ</a></p>
<p><b>Keywords</b>: material generation, symmetries, variational auto-encoder, score-based diffusion model
</p><p><b>Compressor summary</b>: SyMat is a new method for generating periodic materials with different symmetries using deep models, which can create atom types, lattices, and coordinates that are invariant to various symmetry transformations.</p><hr><h3>Module-wise Adaptive Distillation for Multimodality Foundation Models</h3>
<p>Chen Liang, Jiahui Yu, Ming-Hsuan Yang, Matthew Brown, Yin Cui, Tuo Zhao, Boqing Gong, Tianyi Zhou</p>
<p><a href='https://openreview.net/forum?id=JhQP33aMx2'>https://openreview.net/forum?id=JhQP33aMx2</a></p>
<p><b>Keywords</b>: Multimodality foundation models, knowledge distillation
</p><p><b>Compressor summary</b>: The proposed method uses a modified-Thompson sampling algorithm to selectively distill different modules of a multimodal foundation model based on their contributions to reduce its size and maintain performance.</p><hr><h3>Minimax-Optimal Location Estimation</h3>
<p>Shivam Gupta, Jasper C.H. Lee, Eric Price, Paul Valiant</p>
<p><a href='https://openreview.net/forum?id=JeKXmYb4kd'>https://openreview.net/forum?id=JeKXmYb4kd</a></p>
<p><b>Keywords</b>: location estimation, minimax estimation
</p><p><b>Compressor summary</b>: The paper proposes two optimal location estimators for parametric statistics with different criteria, one minimizing estimation error and success probability, and another minimizing expected squared interval width.</p><hr><h3>Neural Functional Transformers</h3>
<p>Allan Zhou, Kaien Yang, Yiding Jiang, Kaylee Burns, Winnie Xu, Samuel Sokota, J Zico Kolter, Chelsea Finn</p>
<p><a href='https://openreview.net/forum?id=JdhyIa0azI'>https://openreview.net/forum?id=JdhyIa0azI</a></p>
<p><b>Keywords</b>: equivariance, permutation, implicit neural representation, generalization, transformers, attention
</p><p><b>Compressor summary</b>: The paper proposes neural functional Transformers (NFTs) that use attention mechanisms to process neural networks' weights and achieve better performance in weight-space methods, enabling improved classification of implicit neural representations (INRs).</p><hr><h3>Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time</h3>
<p>Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, Anshumali Shrivastava</p>
<p><a href='https://openreview.net/forum?id=JZfg6wGi6g'>https://openreview.net/forum?id=JZfg6wGi6g</a></p>
<p><b>Keywords</b>: Large language model; KV Cache Compression
</p><p><b>Compressor summary</b>: Scissorhands is a system that reduces memory usage and inference time for large language models by storing pivotal tokens more frequently.</p><hr><h3>Joint Attribute and Model Generalization Learning for Privacy-Preserving Action Recognition</h3>
<p>Duo Peng, Li Xu, Qiuhong Ke, Ping Hu, Jun Liu</p>
<p><a href='https://openreview.net/forum?id=JYUN0vYjh9'>https://openreview.net/forum?id=JYUN0vYjh9</a></p>
<p><b>Keywords</b>: Privacy Preservation, Action Recognition, Meta-Learning
</p><p><b>Compressor summary</b>: The paper proposes MPPAR, a meta-learning framework for privacy-preserving action recognition that can adapt to novel privacy attributes and attack models.</p><hr><h3>Switching Temporary Teachers for Semi-Supervised Semantic Segmentation</h3>
<p>Jaemin Na, Jung-Woo Ha, Hyung Jin Chang, Dongyoon Han, Wonjun Hwang</p>
<p><a href='https://openreview.net/forum?id=JXvszuOqY3'>https://openreview.net/forum?id=JXvszuOqY3</a></p>
<p><b>Keywords</b>: Semi-supervised Learning, Semantic Segmentation
</p><p><b>Compressor summary</b>: The paper proposes Dual Teacher, a method to improve semi-supervised semantic segmentation by using two temporary teachers that shift roles and prevent the student model from losing its unique features.</p><hr><h3>Parameterizing Non-Parametric Meta-Reinforcement Learning Tasks via Subtask Decomposition</h3>
<p>Suyoung Lee, Myungsik Cho, Youngchul Sung</p>
<p><a href='https://openreview.net/forum?id=JX6UloWrmE'>https://openreview.net/forum?id=JX6UloWrmE</a></p>
<p><b>Keywords</b>: Deep reinforcement learning, Meta-reinforcement learning, Subtask decomposition
</p><p><b>Compressor summary</b>: SDVT is a novel meta-RL approach that decomposes non-parametric tasks into subtasks, uses a Gaussian mixture VAE to meta-learn the decomposition, and employs virtual training for better generalization.</p><hr><h3>ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation</h3>
<p>Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, Yuxiao Dong</p>
<p><a href='https://openreview.net/forum?id=JVzeOYEx6d'>https://openreview.net/forum?id=JVzeOYEx6d</a></p>
<p><b>Keywords</b>: Generative Models, Text-to-Image, Learning from Human Feedback, Multimodality, Evaluation
</p><p><b>Compressor summary</b>: The authors introduce ImageReward, a new model that encodes human preferences for text-to-image synthesis, and ReFL, an algorithm to optimize diffusion models using ImageReward.</p><hr><h3>DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization</h3>
<p>Zhiqing Sun, Yiming Yang</p>
<p><a href='https://openreview.net/forum?id=JV8Ff0lgVV'>https://openreview.net/forum?id=JV8Ff0lgVV</a></p>
<p><b>Keywords</b>: neural-symbolic reasoning, combinatorial optimization, diffusion models
</p><p><b>Compressor summary</b>: The paper introduces DIFUSCO, a graph-based diffusion framework that generates high-quality solutions for NP-complete problems using neural networks and outperforms previous state-of-the-art methods.</p><hr><h3>MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers</h3>
<p>LILI YU, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, Mike Lewis</p>
<p><a href='https://openreview.net/forum?id=JTmO2V9Xpz'>https://openreview.net/forum?id=JTmO2V9Xpz</a></p>
<p><b>Keywords</b>: byte level language model, model architecture, efficient pretraining
</p><p><b>Compressor summary</b>: Megabyte is a multi-scale decoder architecture that enables efficient and large-scale tokenization-free autoregressive sequence modeling for various domains such as language, image, and audio.</p><hr><h3>Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks</h3>
<p>Jules Berman, Benjamin Peherstorfer</p>
<p><a href='https://openreview.net/forum?id=JTKd7zYROf'>https://openreview.net/forum?id=JTKd7zYROf</a></p>
<p><b>Keywords</b>: numerical methods, deep networks, evolution equations, scientific computing, partial differential equations, model reduction
</p><p><b>Compressor summary</b>: The paper proposes Neural Galerkin schemes that use randomized sparse updates to train neural networks for time-dependent partial differential equations, improving accuracy and speed over dense updates.</p><hr><h3>Latent exploration for Reinforcement Learning</h3>
<p>Alberto Silvio Chiappa, Alessandro Marin Vargas, Ann Huang, Alexander Mathis</p>
<p><a href='https://openreview.net/forum?id=JSVXZKqfLU'>https://openreview.net/forum?id=JSVXZKqfLU</a></p>
<p><b>Keywords</b>: Reinforcement learning, efficient exploration, curse of dimensionality, motor control, musculoskeletal control
</p><p><b>Compressor summary</b>: Lattice introduces temporally-correlated noise into policy networks to improve exploration and control in overactuated systems, achieving state of the art results on locomotion and musculoskeletal tasks.</p><hr><h3>MMD-Fuse: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting</h3>
<p>Felix Biggs, Antonin Schrab, Arthur Gretton</p>
<p><a href='https://openreview.net/forum?id=JOkgEY9os2'>https://openreview.net/forum?id=JOkgEY9os2</a></p>
<p><b>Keywords</b>: Testing, MMD, Kernel Methods, Two-sample testing
</p><p><b>Compressor summary</b>: The paper introduces new statistics that improve the ability to test if two samples come from the same distribution using a modified version of the Maximum Mean Discrepancy (MMD) test, and shows their effectiveness on both synthetic and real data.</p><hr><h3>Towards Combinatorial Generalization for Catalysts: A Kohn-Sham Charge-Density Approach</h3>
<p>Phil Pope, David Jacobs</p>
<p><a href='https://openreview.net/forum?id=JOHp5SmckS'>https://openreview.net/forum?id=JOHp5SmckS</a></p>
<p><b>Keywords</b>: graph neural networks, equivariance, materials science, chemistry, density functional theory, combinatorial generalization, catalysts
</p><p><b>Compressor summary</b>: The authors propose using pointwise learning of the Kohn-Sham charge-density to model catalysts, which shows improved convergence and combinatorial generalization compared to energy prediction.</p><hr><h3>On Slicing Optimality for Mutual Information</h3>
<p>Ammar Fayad, Majd Ibrahim</p>
<p><a href='https://openreview.net/forum?id=JMuKfZx2xU'>https://openreview.net/forum?id=JMuKfZx2xU</a></p>
<p><b>Keywords</b>: Mutual information, Information Theory
</p><p><b>Compressor summary</b>: The paper proposes a framework to find the optimal distribution of slices for measuring mutual information between high-dimensional variables, improving on existing methods that use uniform distributions and discard informative features.</p><hr><h3>Generalised f-Mean Aggregation for Graph Neural Networks</h3>
<p>Ryan Kortvelesy, Steven Morad, Amanda Prorok</p>
<p><a href='https://openreview.net/forum?id=JMrIeKjTAe'>https://openreview.net/forum?id=JMrIeKjTAe</a></p>
<p><b>Keywords</b>: Aggregation, Graph Neural Networks
</p><p><b>Compressor summary</b>: The paper introduces GenAgg, a generalised aggregation operator for graph neural networks that improves performance by parametrising a function space including all standard aggregators.</p><hr><h3>Vocabulary-free Image Classification</h3>
<p>Alessandro Conti, Enrico Fini, Massimiliano Mancini, Paolo Rota, Yiming Wang, Elisa Ricci</p>
<p><a href='https://openreview.net/forum?id=JKhyQHpx7B'>https://openreview.net/forum?id=JKhyQHpx7B</a></p>
<p><b>Keywords</b>: language and vision, zero-shot classification, image classification
</p><p><b>Compressor summary</b>: The paragraph introduces a new task called Vocabulary-free Image Classification (VIC), which aims to classify images without knowing the categories beforehand, and proposes CaSED, a method that uses a pre-trained vision-language model and an external database to achieve this goal.</p><hr><h3>Global Identifiability of  $\ell_1$-based Dictionary Learning via Matrix Volume Optimization</h3>
<p>Jingzhou Hu, Kejun Huang</p>
<p><a href='https://openreview.net/forum?id=JK2oPrP8B3'>https://openreview.net/forum?id=JK2oPrP8B3</a></p>
<p><b>Keywords</b>: dictionary learning, matrix volume, nonconvex optimization
</p><p><b>Compressor summary</b>: The authors propose a novel dictionary learning method that ensures global identifiability of the groundtruth matrices by minimizing the volume of the dictionary matrix while maintaining unit $\ell_1$ norm for each row of the sparse coefficient matrix, without requiring mutual incoherence of dictionary atoms.</p><hr><h3>Simple, Scalable and Effective Clustering via One-Dimensional Projections</h3>
<p>Moses Charikar, Monika Henzinger, Lunjia Hu, Maximilian Vötsch, Erik Waingarten</p>
<p><a href='https://openreview.net/forum?id=JIYdbHDonF'>https://openreview.net/forum?id=JIYdbHDonF</a></p>
<p><b>Keywords</b>: clustering, k-means, random projection, massive datasets
</p><p><b>Compressor summary</b>: A simple randomized clustering algorithm with improved running time and cluster quality compared to popular methods is introduced for data analysis tasks such as k-means clustering and coreset construction.</p><hr><h3>DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models</h3>
<p>Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, Chunhua Shen</p>
<p><a href='https://openreview.net/forum?id=JIKM2vS8XU'>https://openreview.net/forum?id=JIKM2vS8XU</a></p>
<p><b>Keywords</b>: Diffusion Model; Text-guided dataset generation
</p><p><b>Compressor summary</b>: DatasetDM is a model that generates synthetic images and perception annotations using a diffusion model, enabling efficient training of various perception models on downstream tasks with minimal effort and cost.</p><hr><h3>Partial Multi-Label Learning with Probabilistic Graphical Disambiguation</h3>
<p>Jun-Yi Hang, Min-Ling Zhang</p>
<p><a href='https://openreview.net/forum?id=JDw50IX4TY'>https://openreview.net/forum?id=JDw50IX4TY</a></p>
<p><b>Keywords</b>: Machine learning, multi-label learning, partial multi-label learning, label disambiguation
</p><p><b>Compressor summary</b>: The paper proposes a probabilistic graphical model for partial multi-label learning, which uses stochastic gradient variational Bayes to infer ground-truth labeling information from inaccurate annotations and performs better than existing methods.</p><hr><h3>Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness</h3>
<p>Ambar Pal, Jeremias Sulam, Rene Vidal</p>
<p><a href='https://openreview.net/forum?id=JDoA6admhv'>https://openreview.net/forum?id=JDoA6admhv</a></p>
<p><b>Keywords</b>: Adversarial Robustness, Geometry in Data, Low Dimensional Modeling
</p><p><b>Compressor summary</b>: The authors explore whether adversarial examples are unavoidable by analyzing how data distribution concentration affects the existence of robust machine learning classifiers.</p><hr><h3>Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data</h3>
<p>Alon Albalak, Colin Raffel, William Yang Wang</p>
<p><a href='https://openreview.net/forum?id=JDnLXc4NOn'>https://openreview.net/forum?id=JDnLXc4NOn</a></p>
<p><b>Keywords</b>: Few-shot learning, natural language processing, few shot learning, NLP, multi-armed bandit, multi armed bandit
</p><p><b>Compressor summary</b>: FLAD is a method to improve few-shot learning with auxiliary data, and this paper introduces two algorithms that explore and exploit different datasets, achieving better results than previous methods and enabling larger language models.</p><hr><h3>Deep Non-line-of-sight Imaging from Under-scanning Measurements</h3>
<p>Yue Li, Yueyi Zhang, Juntian Ye, Feihu Xu, Zhiwei Xiong</p>
<p><a href='https://openreview.net/forum?id=JCN9YsZiwB'>https://openreview.net/forum?id=JCN9YsZiwB</a></p>
<p><b>Keywords</b>: Non-line-of-sight imaging, Transient Recovery, Volume Reconstruction
</p><p><b>Compressor summary</b>: The paragraph describes a new deep learning approach to non-line-of-sight imaging that uses under-scanning measurements to reconstruct hidden volumes with high resolution and robustness, outperforming traditional methods.</p><hr><h3>Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents</h3>
<p>Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, brian ichter</p>
<p><a href='https://openreview.net/forum?id=JCCi58IUsh'>https://openreview.net/forum?id=JCCi58IUsh</a></p>
<p><b>Keywords</b>: robotics, language models, embodied agents
</p><p><b>Compressor summary</b>: The paragraph discusses the challenge of combining language models' semantic knowledge with grounded robots' real-world understanding and proposes a solution that decodes action sequences using both types of models.</p><hr><h3>Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning</h3>
<p>Baohao Liao, Shaomu Tan, Christof Monz</p>
<p><a href='https://openreview.net/forum?id=J8McuwS3zY'>https://openreview.net/forum?id=J8McuwS3zY</a></p>
<p><b>Keywords</b>: large language model, parameter-efficient learning, memory-efficient learning, reversible neural network
</p><p><b>Compressor summary</b>: MEFT is a memory-efficient fine-tuning method that uses adapters to insert reversibility into PLMs, preserving their starting point and achieving comparable performance to full fine-tuning with much less activation memory.</p><hr><h3>LLM-Pruner: On the Structural Pruning of Large Language Models</h3>
<p>Xinyin Ma, Gongfan Fang, Xinchao Wang</p>
<p><a href='https://openreview.net/forum?id=J8Ajf9WfXP'>https://openreview.net/forum?id=J8Ajf9WfXP</a></p>
<p><b>Keywords</b>: model compression, structural pruning, large language model
</p><p><b>Compressor summary</b>: The paper introduces LLM-pruner, a method to compress large language models without relying on their original training data, by selectively removing non-critical structures based on gradient information.</p><hr><h3>Unlocking Feature Visualization for Deep Network with MAgnitude Constrained Optimization</h3>
<p>Thomas FEL, Thibaut Boissin, Victor Boutin, Agustin Martin Picard, Paul Novello, Julien Colin, Drew Linsley, Tom ROUSSEAU, Remi Cadene, Lore Goetschalckx, Laurent Gardes, Thomas Serre</p>
<p><a href='https://openreview.net/forum?id=J7VoDuzuKs'>https://openreview.net/forum?id=J7VoDuzuKs</a></p>
<p><b>Keywords</b>: explainable AI, feature visualization, interpretability, optimization
</p><p><b>Compressor summary</b>: MACO is a method for improving feature visualization in neural networks by optimizing image phase spectrum and introducing three metrics for evaluation.</p><hr><h3>Glance and Focus: Memory Prompting for Multi-Event Video Question Answering</h3>
<p>Ziyi Bai, Ruiping Wang, Xilin CHEN</p>
<p><a href='https://openreview.net/forum?id=J6Niv3yrMq'>https://openreview.net/forum?id=J6Niv3yrMq</a></p>
<p><b>Keywords</b>: Video Question Answering; Multi-Event Reasoning; Spatial-Temporal Reasoning
</p><p><b>Compressor summary</b>: The Glance- Focus model uses dynamic event memories to improve Video Question Answering by mimicking human reasoning strategies, achieving state-of-the-art results on four benchmarks.</p><hr><h3>Kernel Quadrature with Randomly Pivoted Cholesky</h3>
<p>Ethan Nicholas Epperly, Elvira Moreno Ferreira</p>
<p><a href='https://openreview.net/forum?id=J66ptjMkAG'>https://openreview.net/forum?id=J66ptjMkAG</a></p>
<p><b>Keywords</b>: kernel quadrature, Nyström approximation, reproducing kernel Hilbert space, randomly pivoted Cholesky
</p><p><b>Compressor summary</b>: The paper introduces a fast and accurate kernel quadrature method using randomly pivoted Choleskey nodes that works well with complex geometries and kernels.</p><hr><h3>Stability-penalty-adaptive follow-the-regularized-leader: Sparsity, game-dependency, and best-of-both-worlds</h3>
<p>Taira Tsuchiya, Shinji Ito, Junya Honda</p>
<p><a href='https://openreview.net/forum?id=J3taqrzyyA'>https://openreview.net/forum?id=J3taqrzyyA</a></p>
<p><b>Keywords</b>: follow-the-regularized-leader, adaptive learning rate, multi-armed bandits, partial monitoring, data-dependent bound, sparsity, game-dependency, best-of-both-worlds
</p><p><b>Compressor summary</b>: The paper proposes a new adaptive learning rate for FTRL called SPA, which enables algorithms with three types of adaptivity in sequential decision-making problems, including sparsity, game-dependency, and best-of-both-worlds.</p><hr><h3>DesCo: Learning Object Recognition with Rich Language Descriptions</h3>
<p>Liunian Harold Li, Zi-Yi Dou, Nanyun Peng, Kai-Wei Chang</p>
<p><a href='https://openreview.net/forum?id=J2Cso0wWZX'>https://openreview.net/forum?id=J2Cso0wWZX</a></p>
<p><b>Keywords</b>: Vision language, fine-grained recognition, object detection
</p><p><b>Compressor summary</b>: The text describes a new vision-language model for object detection that uses rich language descriptions and context-sensitive queries to improve accuracy and adaptability.</p><hr><h3>Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework</h3>
<p>Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard J. Chen, Zihao Deng, Nicholas Allen, Randy Auerbach, Faisal Mahmood, Ruslan Salakhutdinov, Louis-Philippe Morency</p>
<p><a href='https://openreview.net/forum?id=J1gBijopla'>https://openreview.net/forum?id=J1gBijopla</a></p>
<p><b>Keywords</b>: multimodal learning, feature interactions, partial information decomposition, information theory, quantification, model selection
</p><p><b>Compressor summary</b>: The paragraph discusses an information-theoretic approach called PID to measure interactions between input modalities and output tasks in multimodal applications, and its usefulness in various aspects of multimodal modeling.</p><hr><h3>A graphon-signal analysis of graph neural networks</h3>
<p>Ron Levie</p>
<p><a href='https://openreview.net/forum?id=J0RD92Tmfc'>https://openreview.net/forum?id=J0RD92Tmfc</a></p>
<p><b>Keywords</b>: graph neural network, graphon, generalization, stability, sampling, Szemerédi regularity lemma
</p><p><b>Compressor summary</b>: The paper proposes a new similarity measure for graph-signals in message passing graph neural networks (MPNNs) and shows how it leads to Lipschitz continuity and generalization bounds for MPNNs.</p><hr><h3>SUBP: Soft Uniform Block Pruning for 1$\times$N Sparse CNNs Multithreading Acceleration</h3>
<p>Jingyang Xiang, Siqi Li, Jun Chen, Guang Dai, Shipeng Bai, Yukai Ma, Yong Liu</p>
<p><a href='https://openreview.net/forum?id=J0Pvvxspmz'>https://openreview.net/forum?id=J0Pvvxspmz</a></p>
<p><b>Keywords</b>: Soft Uniform Block Pruning, Block Angular Redundancy, Hardware Acceleration
</p><p><b>Compressor summary</b>: The paper proposes a new method to train sparse neural networks that reduces storage, improves performance, and balances workload across threads.</p><hr><h3>Smoothed Online Learning for Prediction in Piecewise Affine Systems</h3>
<p>Adam Block, Max Simchowitz, Russ Tedrake</p>
<p><a href='https://openreview.net/forum?id=Izt7rDD7jN'>https://openreview.net/forum?id=Izt7rDD7jN</a></p>
<p><b>Keywords</b>: Smoothed Online Learning, Piecewise Affine Prediction, Learning Dynamics
</p><p><b>Compressor summary</b>: The paper presents polynomial-regret algorithms for prediction and simulation in piecewise affine systems using smoothed online learning and introduces new technical tools.</p><hr><h3>Swap Agnostic Learning, or Characterizing Omniprediction via Multicalibration</h3>
<p>Parikshit Gopalan, Michael P. Kim, Omer Reingold</p>
<p><a href='https://openreview.net/forum?id=IzlRh5qwmG'>https://openreview.net/forum?id=IzlRh5qwmG</a></p>
<p><b>Keywords</b>: Agnostic Learning, Omniprediction, Multicalibration
</p><p><b>Compressor summary</b>: Swap Agnostic Learning is a game where a predictor selects a hypothesis and an adversary tries to minimize the loss, and the paper shows how it relates to other concepts like Omniprediction and Multicalibration.</p><hr><h3>Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference</h3>
<p>Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Y Zhao, Yuexin Wu, Bo Li, Yu Zhang, Ming-Wei Chang</p>
<p><a href='https://openreview.net/forum?id=IyYyKov0Aj'>https://openreview.net/forum?id=IyYyKov0Aj</a></p>
<p><b>Keywords</b>: conditional computation, inference efficiency, parameter efficiency, large models
</p><p><b>Compressor summary</b>: CoDA is a transfer learning method that boosts inference efficiency while maintaining accuracy and parameter efficiency on various tasks.</p><hr><h3>Block Coordinate Plug-and-Play Methods for Blind Inverse Problems</h3>
<p>Weijie Gan, Shirin Shoushtari, Yuyang Hu, Jiaming Liu, Hongyu An, Ulugbek Kamilov</p>
<p><a href='https://openreview.net/forum?id=IyWpP2e0bF'>https://openreview.net/forum?id=IyWpP2e0bF</a></p>
<p><b>Keywords</b>: inverse problems, plug-and-play priors, computational imaging, nonconvex optimization
</p><p><b>Compressor summary</b>: BC-PnP is a new method that uses learned denoisers to solve blind imaging inverse problems by combining physical measurements and image estimation.</p><hr><h3>Assumption violations in causal discovery and the robustness of score matching</h3>
<p>Francesco Montagna, Atalanti A. Mastakouri, Elias Eulig, Nicoletta Noceti, Lorenzo Rosasco, Dominik Janzing, Bryon Aragam, Francesco Locatello</p>
<p><a href='https://openreview.net/forum?id=IyTArtpuCK'>https://openreview.net/forum?id=IyTArtpuCK</a></p>
<p><b>Keywords</b>: Causal discovery; empirical study; robust inference; benchmark
</p><p><b>Compressor summary</b>: The paper benchmarks various causal discovery algorithms on observational data with different conditions, finding that score matching-based methods perform well and providing theoretical explanations.</p><hr><h3>A Unified Approach to Count-Based Weakly Supervised Learning</h3>
<p>Vinay Shukla, Zhe Zeng, Kareem Ahmed, Guy Van den Broeck</p>
<p><a href='https://openreview.net/forum?id=IyAHCbMq3a'>https://openreview.net/forum?id=IyAHCbMq3a</a></p>
<p><b>Keywords</b>: weakly supervised learning, constraint, label proportion, learning from positive and unlabeled data, multiple instance learning
</p><p><b>Compressor summary</b>: The paper proposes a method called count-based weakly-supervised learning that learns from data with inferred weak labels by computing and penalizing deviations from expected label counts.</p><hr><h3>Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning</h3>
<p>Austin Xu, Andrew McRae, Jingyan Wang, Mark A. Davenport, Ashwin Pananjady</p>
<p><a href='https://openreview.net/forum?id=IwyymRXfzL'>https://openreview.net/forum?id=IwyymRXfzL</a></p>
<p><b>Keywords</b>: human querying, high dimensional low rank matrix estimation, metric learning
</p><p><b>Compressor summary</b>: The paper introduces PAQ, a query mechanism that combines cardinal and ordinal feedback to learn an unknown Mahalanobis distance using a two-stage estimator with sample complexity guarantees.</p><hr><h3>Conformal Meta-learners for Predictive Inference of Individual Treatment Effects</h3>
<p>Ahmed Alaa, Zaid Ahmad, Mark van der Laan</p>
<p><a href='https://openreview.net/forum?id=IwnINorSZ5'>https://openreview.net/forum?id=IwnINorSZ5</a></p>
<p><b>Keywords</b>: Heterogeneous treatment effects, conformal prediction
</p><p><b>Compressor summary</b>: The paper proposes a method to estimate uncertainty in individual treatment effects using machine learning and conformal prediction techniques, showing that it is valid under certain conditions and performs well in simulations.</p><hr><h3>Accelerating Exploration with Unlabeled Prior Data</h3>
<p>Qiyang Li, Jason Zhang, Dibya Ghosh, Amy Zhang, Sergey Levine</p>
<p><a href='https://openreview.net/forum?id=Itorzn4Kwf'>https://openreview.net/forum?id=Itorzn4Kwf</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Exploration
</p><p><b>Compressor summary</b>: The paper proposes a method to improve exploration in sparse reward tasks by using prior data without reward labels to guide an agent, leading to better performance in challenging domains.</p><hr><h3>Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks</h3>
<p>Jimmy Z. Di, Jack Douglas, Jayadev Acharya, Gautam Kamath, Ayush Sekhari</p>
<p><a href='https://openreview.net/forum?id=Isy7gl1Hqc'>https://openreview.net/forum?id=Isy7gl1Hqc</a></p>
<p><b>Keywords</b>: Machine unlearning, new attack vector, Camouflaging poisoning attacks
</p><p><b>Compressor summary</b>: The paper introduces a new data poisoning attack that uses camouflaged points to affect model predictions when some points are removed during model retraining or unlearning.</p><hr><h3>Model-free Posterior Sampling via Learning Rate Randomization</h3>
<p>Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Eric Moulines, Remi Munos, Alexey Naumov, pierre perrault, Michal Valko, Pierre MENARD</p>
<p><a href='https://openreview.net/forum?id=IrjXmIKFyx'>https://openreview.net/forum?id=IrjXmIKFyx</a></p>
<p><b>Keywords</b>: reinforcement learning, exploration, q-learning
</p><p><b>Compressor summary</b>: Randomized Q-learning (RandQL) is a new algorithm for minimizing regret in episodic MDPs that uses random learning rates and achieves better performance than existing methods.</p><hr><h3>Why Did This Model Forecast This Future? Information-Theoretic Saliency for Counterfactual Explanations of Probabilistic Regression Models</h3>
<p>Chirag Raman, Alec Nonnemaker, Amelia Villegas-Morcillo, Hayley Hung, Marco Loog</p>
<p><a href='https://openreview.net/forum?id=IrEYkhuxup'>https://openreview.net/forum?id=IrEYkhuxup</a></p>
<p><b>Keywords</b>: Probabilistic Forecasting, Saliency, Explainability, XAI, Probabilistic Regression
</p><p><b>Compressor summary</b>: The authors propose a method to explain probabilistic time-series forecasts by identifying salient timesteps based on human visual cognition and test it on synthetic and real data.</p><hr><h3>Debiasing Pretrained Generative Models by Uniformly Sampling Semantic Attributes</h3>
<p>Walter Gerych, Kevin Hickey, Luke Buquicchio, Kavin Chandrasekaran, Abdulaziz Alajaji, Elke Rundensteiner, Emmanuel Agu</p>
<p><a href='https://openreview.net/forum?id=Iq7v0sZw2H'>https://openreview.net/forum?id=Iq7v0sZw2H</a></p>
<p><b>Keywords</b>: generative models, generative modeling, bias, GANs, debiasing
</p><p><b>Compressor summary</b>: The distribution mapping module proposes a way to debias generative models without retraining them by using fair noise distribution, and it works better than current methods.</p><hr><h3>Emergent and Predictable Memorization in Large Language Models</h3>
<p>Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Gregory Anthony, Shivanshu Purohit, Edward Raff</p>
<p><a href='https://openreview.net/forum?id=Iq0DvhB4Kf'>https://openreview.net/forum?id=Iq0DvhB4Kf</a></p>
<p><b>Keywords</b>: large language model, emergent properties, memorization
</p><p><b>Compressor summary</b>: The paper proposes a method to predict memorization in language models using lower-compute trials, and provides insights on memorization behavior and recommendations for reliability.</p><hr><h3>Improving the Privacy and Practicality of Objective Perturbation for Differentially Private Linear Learners</h3>
<p>Rachel Emily Redberg, Antti Koskela, Yu-Xiang Wang</p>
<p><a href='https://openreview.net/forum?id=IpUJd3KG3c'>https://openreview.net/forum?id=IpUJd3KG3c</a></p>
<p><b>Keywords</b>: differential privacy, empirical risk minimization, objective perturbation
</p><p><b>Compressor summary</b>: The paper improves the objective perturbation mechanism for privacy-preserving machine learning, making it more efficient and competitive with differentially private stochastic gradient descent.</p><hr><h3>Skill-it! A data-driven skills framework for understanding and training language models</h3>
<p>Mayee F Chen, Nicholas Roberts, Kush Bhatia, Jue WANG, Ce Zhang, Frederic Sala, Christopher Re</p>
<p><a href='https://openreview.net/forum?id=IoizwO1NLf'>https://openreview.net/forum?id=IoizwO1NLf</a></p>
<p><b>Keywords</b>: language models, data selection
</p><p><b>Compressor summary</b>: The authors propose a framework for selecting and ordering training data based on language model skills, which improves performance on downstream tasks with fewer tokens.</p><hr><h3>SLM: A Smoothed First-Order Lagrangian Method for Structured Constrained Nonconvex Optimization</h3>
<p>Songtao Lu</p>
<p><a href='https://openreview.net/forum?id=IobxuwPnWt'>https://openreview.net/forum?id=IobxuwPnWt</a></p>
<p><b>Keywords</b>: Functional constrained optimization, bilevel optimization, primal dual method, Lagrangian method
</p><p><b>Compressor summary</b>: The paper proposes a smoothed first-order Lagrangian method for solving nonconvex functional constrained optimization problems with nonconvex constraints, and shows its effectiveness through theoretical convergence guarantees and numerical results.</p><hr><h3>Utilitarian Algorithm Configuration</h3>
<p>Devon R. Graham, Kevin Leyton-Brown, Tim Roughgarden</p>
<p><a href='https://openreview.net/forum?id=InB9Loet1u'>https://openreview.net/forum?id=InB9Loet1u</a></p>
<p><b>Keywords</b>: algorithm configuration, algorithm selection, data-driven algorithm design, utility of runtime
</p><p><b>Compressor summary</b>: The authors propose a new method for configuring heuristic algorithms that maximizes utility and offers theoretical guarantees about performance, unlike existing methods that only minimize expected runtime.</p><hr><h3>Improved Bayes Risk Can Yield Reduced Social Welfare Under Competition</h3>
<p>Meena Jagadeesan, Michael Jordan, Jacob Steinhardt, Nika Haghtalab</p>
<p><a href='https://openreview.net/forum?id=IltQ87ZdT6'>https://openreview.net/forum?id=IltQ87ZdT6</a></p>
<p><b>Keywords</b>: competition, equilibria, inverse scaling, digital marketplaces
</p><p><b>Compressor summary</b>: Increasing the scale of machine learning models can hurt overall predictive accuracy when multiple providers compete for users, as improving data representation quality may reduce social welfare.</p><hr><h3>Perturbation Towards Easy Samples Improves Targeted Adversarial Transferability</h3>
<p>Junqi Gao, Biqing Qi, Yao Li, Zhichang Guo, Dong Li, Yuming Xing, Dazhi Zhang</p>
<p><a href='https://openreview.net/forum?id=IkD1EWFF8c'>https://openreview.net/forum?id=IkD1EWFF8c</a></p>
<p><b>Keywords</b>: Adversarial Attacks; Generative Attack; Transferable Targeted Attack
</p><p><b>Compressor summary</b>: The paper proposes ESMA, a generative targeted attack strategy that improves transferability by adding perturbations to easy samples in the target class, reducing storage space and computation time compared to current methods.</p><hr><h3>StableFDG: Style and Attention Based Learning for Federated Domain Generalization</h3>
<p>Jungwuk Park, Dong-Jun Han, Jinho Kim, Shiqiang Wang, Christopher Brinton, Jaekyun Moon</p>
<p><a href='https://openreview.net/forum?id=IjZa2fQ8tL'>https://openreview.net/forum?id=IjZa2fQ8tL</a></p>
<p><b>Keywords</b>: Federated Learning, Domain Generalization
</p><p><b>Compressor summary</b>: The paper proposes StableFDG, a learning strategy for federated domain generalization, which uses style and attention-based methods to improve domain diversity and learn domain-invariant characteristics in data-poor scenarios.</p><hr><h3>On the Adversarial Robustness of Out-of-distribution Generalization Models</h3>
<p>Xin Zou, Weiwei Liu</p>
<p><a href='https://openreview.net/forum?id=IiwTFcGGTq'>https://openreview.net/forum?id=IiwTFcGGTq</a></p>
<p><b>Keywords</b>: Adversarial Robustness, Out-of-distribution Generalization
</p><p><b>Compressor summary</b>: The text discusses out-of-distribution generalization, its vulnerability to attacks, and proposes methods to improve it.</p><hr><h3>Calibration by Distribution Matching: Trainable Kernel Calibration Metrics</h3>
<p>Charles Thomas Marx, Sofian Zalouk, Stefano Ermon</p>
<p><a href='https://openreview.net/forum?id=IhxD94i5ra'>https://openreview.net/forum?id=IhxD94i5ra</a></p>
<p><b>Keywords</b>: Uncertainty Quantification, Calibration, Decision Making, Probabilistic Forecasting
</p><p><b>Compressor summary</b>: The authors propose kernel-based calibration metrics for both classification and regression that improve predictive uncertainty, sharpness, and decision-making by incorporating them into empirical risk minimization.</p><hr><h3>Bayes beats Cross Validation: Efficient and Accurate Ridge Regression via Expectation Maximization</h3>
<p>Shu Tew, Mario Boley, Daniel F. Schmidt</p>
<p><a href='https://openreview.net/forum?id=Ih2yL7o2Gq'>https://openreview.net/forum?id=Ih2yL7o2Gq</a></p>
<p><b>Keywords</b>: Ridge Regression, Cross validation, Expectation Maximisation, Bayesian methods
</p><p><b>Compressor summary</b>: A faster and better method for tuning ridge regression hyper-parameter $\lambda$ using Bayesian formulation and iterative EM procedure is proposed.</p><hr><h3>Efficient Model-Free Exploration in Low-Rank MDPs</h3>
<p>Zakaria Mhammedi, Adam Block, Dylan J Foster, Alexander Rakhlin</p>
<p><a href='https://openreview.net/forum?id=IgDa5Ynm9l'>https://openreview.net/forum?id=IgDa5Ynm9l</a></p>
<p><b>Keywords</b>: Reinforcement learning, Representation Learning, Low-rank MDPs, Model-Free Learning
</p><p><b>Compressor summary</b>: The paper proposes SpanRL, a sample-efficient exploration algorithm for high-dimensional reinforcement learning that works without structural assumptions and uses barycentric spanners for efficient exploration.</p><hr><h3>Conditional independence testing under misspecified inductive biases</h3>
<p>Felipe Maia Polo, Yuekai Sun, Moulinath Banerjee</p>
<p><a href='https://openreview.net/forum?id=Ifq8GMdqJK'>https://openreview.net/forum?id=Ifq8GMdqJK</a></p>
<p><b>Keywords</b>: conditional independence, hypothesis testing, misspecification
</p><p><b>Compressor summary</b>: The paper studies how regression-based tests for conditional independence can fail due to misspecified models or algorithms, and proposes a new test (RBPT) that is robust against such failures.</p><hr><h3>Online Performative Gradient Descent for Learning Nash Equilibria in Decision-Dependent Games</h3>
<p>Zihan Zhu, Ethan X Fang, Zhuoran Yang</p>
<p><a href='https://openreview.net/forum?id=IdF7VT6eEs'>https://openreview.net/forum?id=IdF7VT6eEs</a></p>
<p><b>Keywords</b>: Performative Prediction, Nash Equilibrium, Reproducing Kernel Hilbert Space, Online Learning, Stochastic Gradient Methods
</p><p><b>Compressor summary</b>: The paper proposes a novel online algorithm (OPGD) to find the Nash equilibrium of decision-dependent games in the bandit feedback setting, where agents' actions affect population data and traditional gradient-based methods are infefficient without a gradient oracle.</p><hr><h3>Stochastic Approximation Approaches to Group Distributionally Robust Optimization</h3>
<p>Lijun Zhang, Peng Zhao, Zhenhua Zhuang, Tianbao Yang, Zhi-Hua Zhou</p>
<p><a href='https://openreview.net/forum?id=IcIQbCWoFj'>https://openreview.net/forum?id=IcIQbCWoFj</a></p>
<p><b>Keywords</b>: Group distributionally robust optimization, Stochastic mirror descent, Non-oblivious online learning, Sample complexity, Stochastic mirror-prox algorithm, Mini-batch
</p><p><b>Compressor summary</b>: The paper studies group distributionally robust optimization (GDRO) and proposes methods to reduce sample complexity using techniques from online learning and non-uniform sampling.</p><hr><h3>Can Language Models Teach? Teacher Explanations Improve Student Performance via Personalization</h3>
<p>Swarnadeep Saha, Peter Hase, Mohit Bansal</p>
<p><a href='https://openreview.net/forum?id=IacxcFpvWQ'>https://openreview.net/forum?id=IacxcFpvWQ</a></p>
<p><b>Keywords</b>: Language Models, Reasoning, Explanations
</p><p><b>Compressor summary</b>: The study explores how to make LLMs good teachers for weaker agents by using natural language explanations and a communication budget. It shows that teacher LLMs can improve student performance by intervening at the right time, personalizing explanations, and avoiding misinformation.</p><hr><h3>ResoNet: Noise-Trained Physics-Informed MRI Off-Resonance Correction</h3>
<p>Alfredo De Goyeneche, Shreya Ramachandran, Ke Wang, Ekin Karasan, Joseph Yitan Cheng, Stella X. Yu, Michael Lustig</p>
<p><a href='https://openreview.net/forum?id=Ia4dmqst0Z'>https://openreview.net/forum?id=Ia4dmqst0Z</a></p>
<p><b>Keywords</b>: Inverse problem, MRI, Medical Imaging, Computational Imaging, Deep Learning, Off-Resonance
</p><p><b>Compressor summary</b>: The paper proposes a physics-informed deep learning framework for correcting off-resonance artifacts in MRI using synthetic data, which could enable faster and more accurate scans with non-Cartesian sampling trajectories.</p><hr><h3>Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction</h3>
<p>Zeshuai Deng, Zhuokun Chen, Shuaicheng Niu, Thomas H. Li, Bohan Zhuang, Mingkui Tan</p>
<p><a href='https://openreview.net/forum?id=IZRlMABK4l'>https://openreview.net/forum?id=IZRlMABK4l</a></p>
<p><b>Keywords</b>: Image Super-resolution, Test-time Adaptation, Self-supervised Learning, Second-Order Degradation
</p><p><b>Compressor summary</b>: The authors propose a test-time adaptation framework for image super-resolution, called SRTTA, that can handle different degradation types by predicting the degradation type and adapting the model using feature-level reconstruction learning.</p><hr><h3>Intriguing Properties of Quantization at Scale</h3>
<p>Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Zhen Stephen Gou, Phil Blunsom, Ahmet Üstün, Sara Hooker</p>
<p><a href='https://openreview.net/forum?id=IYe8j7Gy8f'>https://openreview.net/forum?id=IYe8j7Gy8f</a></p>
<p><b>Keywords</b>: Quantization, optimization, language modelling, efficiency
</p><p><b>Compressor summary</b>: This paper investigates if quantization cliffs, which cause performance drops in large models, are solely a factor of scale and proposes a method to optimize for efficient quantization by suppressing outlier dimensions.</p><hr><h3>Optimization or Architecture: How to Hack Kalman Filtering</h3>
<p>Ido Greenberg, Netanel Yannay, Shie Mannor</p>
<p><a href='https://openreview.net/forum?id=IXWaWPkGke'>https://openreview.net/forum?id=IXWaWPkGke</a></p>
<p><b>Keywords</b>: non-linear filtering, Kalman filter, noise estimation, optimization, Cholesky parameterization
</p><p><b>Compressor summary</b>: The Optimized Kalman Filter (OKF) improves the performance of the standard Kalman Filter by optimizing both its architecture and parameters, making it competitive with non-linear models like neural networks.</p><hr><h3>Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering</h3>
<p>Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, Bill Byrne</p>
<p><a href='https://openreview.net/forum?id=IWWWulAX7g'>https://openreview.net/forum?id=IWWWulAX7g</a></p>
<p><b>Keywords</b>: knowledge-based visual question answering, knowledge retrieval, multi-modality, vision-and-language
</p><p><b>Compressor summary</b>: FLMR improves knowledge retrieval in RA-VQA by using finer-grained image and question embeddings and complementary image representations from a vision model.</p><hr><h3>Are Emergent Abilities of Large Language Models a Mirage?</h3>
<p>Rylan Schaeffer, Brando Miranda, Sanmi Koyejo</p>
<p><a href='https://openreview.net/forum?id=ITw9edRDlD'>https://openreview.net/forum?id=ITw9edRDlD</a></p>
<p><b>Keywords</b>: large language models, foundation models, natural language processing, language modeling, emergent abilities
</p><p><b>Compressor summary</b>: The paper suggests that emergent abilities in large language models are due to the choice of metric rather than scale, and provides evidence for this claim through various analyses and experiments.</p><hr><h3>Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability</h3>
<p>Jingfeng Wu, Vladimir Braverman, Jason D. Lee</p>
<p><a href='https://openreview.net/forum?id=IT9mWLYNpQ'>https://openreview.net/forum?id=IT9mWLYNpQ</a></p>
<p><b>Keywords</b>: gd; implicit bias; edge of stability
</p><p><b>Compressor summary</b>: This paper studies how gradient descent with constant stepsize behaves on linearly separable data with non-monotonic losses, proving its ability to minimize logistic loss and revealing its divergence under exponential loss in the edge of stability regime.</p><hr><h3>CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training</h3>
<p>A. Feder Cooper, Wentao Guo, Khiem Pham, Tiancheng Yuan, Charlie F. Ruan, Yucheng Lu, Christopher De Sa</p>
<p><a href='https://openreview.net/forum?id=ISRyILhAyS'>https://openreview.net/forum?id=ISRyILhAyS</a></p>
<p><b>Keywords</b>: permuted example ordering, distributed training, scalable training, herding
</p><p><b>Compressor summary</b>: CD-GraB is a new method that improves the speed of distributed machine learning by using example ordering based on stale gradients, leading to faster convergence than existing methods.</p><hr><h3>Mutual-Information Regularized Multi-Agent Policy Iteration</h3>
<p>Jiangxing Wang, Deheng Ye, Zongqing Lu</p>
<p><a href='https://openreview.net/forum?id=IQRc3FrYOG'>https://openreview.net/forum?id=IQRc3FrYOG</a></p>
<p><b>Keywords</b>: Multi-Agent Reinforcement Learning
</p><p><b>Compressor summary</b>: The text proposes using mutual information as an augmented reward to help multi-agent reinforcement learning algorithms adapt to changing team compositions and perform well in complex cooperative tasks.</p><hr><h3>Towards Characterizing the First-order Query Complexity of Learning (Approximate) Nash Equilibria in Zero-sum Matrix Games</h3>
<p>Hedi Hadiji, Sarah Sachs, Tim van Erven, Wouter M Koolen</p>
<p><a href='https://openreview.net/forum?id=IPNg84RF1k'>https://openreview.net/forum?id=IPNg84RF1k</a></p>
<p><b>Keywords</b>: game theory, minimax optimization, lower bounds
</p><p><b>Compressor summary</b>: The paragraph discusses the query complexity of learning equilibria in zero-sum matrix games, presenting new lower bounds and upper bounds for different values of K and epsilon, and proposing future research directions.</p><hr><h3>HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack on Text</h3>
<p>Han Liu, Zhi Xu, Xiaotong Zhang, Feng Zhang, Fenglong Ma, Hongyang Chen, Hong Yu, Xianchao Zhang</p>
<p><a href='https://openreview.net/forum?id=IOuuLBrGJR'>https://openreview.net/forum?id=IOuuLBrGJR</a></p>
<p><b>Keywords</b>: High-quality adversarial example, Black-box hard-label textual adversarial attack
</p><p><b>Compressor summary</b>: HQA-Attack is a simple framework for generating high quality adversarial examples on text data under black-box hard-label attack scenarios, by substituting original words and using synonym sets to optimize the example while minimizing perturbation rate and query number.</p><hr><h3>DSR: Dynamical Surface Representation as Implicit Neural Networks for Protein</h3>
<p>Daiwen Sun, He Huang, Yao Li, Xinqi Gong, Qiwei Ye</p>
<p><a href='https://openreview.net/forum?id=IOSaJ7ukgf'>https://openreview.net/forum?id=IOSaJ7ukgf</a></p>
<p><b>Keywords</b>: Protein molecular dynamics, Protein surface representation, Implicit neural representation, Signed distance function, Continuous time modeling
</p><p><b>Compressor summary</b>: The authors propose a new neural network method for modeling protein dynamics using an implicit representation of protein surfaces based on signed distance functions, which can accurately capture and interpolate large-scale protein motions, and offer advantages over existing methods.</p><hr><h3>TopoSRL: Topology preserving self-supervised Simplicial Representation Learning</h3>
<p>Hiren Madhu, Sundeep Prabhakar Chepuri</p>
<p><a href='https://openreview.net/forum?id=INS3ltgjg7'>https://openreview.net/forum?id=INS3ltgjg7</a></p>
<p><b>Keywords</b>: Simplicial representation learning, Self-supervised learning, Message passing simplicial networks
</p><p><b>Compressor summary</b>: $\texttt{TopoSRL}$ is a new self-supervised learning method for simplicial complexes that captures higher-order interactions and preserves topology, outperforming existing graph-based methods and supervised models.</p><hr><h3>Task-aware world model learning with meta weighting via bi-level optimization</h3>
<p>Huining Yuan, Hongkun Dou, Xingyu Jiang, Yue Deng</p>
<p><a href='https://openreview.net/forum?id=IN3hQx1BrC'>https://openreview.net/forum?id=IN3hQx1BrC</a></p>
<p><b>Keywords</b>: Model-based reinforcement learning, world model, generative model, meta-learning, bi-level optimization
</p><p><b>Compressor summary</b>: TEMPO is a bi-level model learning framework for reinforcement learning that combines maximum-likelihood and value-equivalent models using a meta weighter network to improve task awareness and semantic information.</p><hr><h3>“Why Not Looking backward?” A Robust Two-Step Method to Automatically Terminate Bayesian Optimization</h3>
<p>Shuang Li, Ke Li, Wei Li</p>
<p><a href='https://openreview.net/forum?id=IMiGRqltQQ'>https://openreview.net/forum?id=IMiGRqltQQ</a></p>
<p><b>Keywords</b>: Bayesian Optimization, Termination Criterion, Looking Backward
</p><p><b>Compressor summary</b>: The paper presents a novel, theoretically grounded method to terminate Bayesian Optimization early when it is in a convex region, saving computation while maintaining good performance.</p><hr><h3>EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought</h3>
<p>Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo</p>
<p><a href='https://openreview.net/forum?id=IL5zJqfxAa'>https://openreview.net/forum?id=IL5zJqfxAa</a></p>
<p><b>Keywords</b>: Embodied AI, Multi-modal Foundation Model, Embodied Control
</p><p><b>Compressor summary</b>: EmbodiedGPT is an end-to-end multi-modal model for embodied AI that uses a large language model to generate plans and extract features for effective embodied tasks.</p><hr><h3>Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks</h3>
<p>Jiayuan Ye, Zhenyu Zhu, Fanghui Liu, Reza Shokri, Volkan Cevher</p>
<p><a href='https://openreview.net/forum?id=IKvxmnHjkL'>https://openreview.net/forum?id=IKvxmnHjkL</a></p>
<p><b>Keywords</b>: overparameterized neural network, privacy
</p><p><b>Compressor summary</b>: This paper analyzes how over-parameterized models in machine learning algorithms can leak information about their training data and explores how initialization distribution affects this privacy loss depending on the model's depth.</p><hr><h3>Towards Label Position Bias in Graph Neural Networks</h3>
<p>Haoyu Han, Xiaorui Liu, Feng Shi, MohamadAli Torkamani, Charu C. Aggarwal, Jiliang Tang</p>
<p><a href='https://openreview.net/forum?id=IKjOMA8olL'>https://openreview.net/forum?id=IKjOMA8olL</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Label Position Bias, Graph Structure Learning
</p><p><b>Compressor summary</b>: The paper introduces label position bias as a new bias in graph neural networks and proposes an optimization framework to reduce it.</p><hr><h3>QuACK: Accelerating Gradient-Based Quantum Optimization with Koopman Operator Learning</h3>
<p>Di Luo, Jiayu Shen, Rumen Dangovski, Marin Soljacic</p>
<p><a href='https://openreview.net/forum?id=IKQOS8rqwr'>https://openreview.net/forum?id=IKQOS8rqwr</a></p>
<p><b>Keywords</b>: Koopman operator, quantum optimization, machine learning
</p><p><b>Compressor summary</b>: QuACK is a novel framework that uses Koopman operator theory to significantly accelerate quantum optimization and machine learning by efficiently predicting gradient dynamics on quantum computers.</p><hr><h3>Goal-Conditioned Predictive Coding for Offline Reinforcement Learning</h3>
<p>Zilai Zeng, Ce Zhang, Shijie Wang, Chen Sun</p>
<p><a href='https://openreview.net/forum?id=IJblKO45YU'>https://openreview.net/forum?id=IJblKO45YU</a></p>
<p><b>Keywords</b>: reinforcement learning, offline RL, self-supervised learning
</p><p><b>Compressor summary</b>: This paper explores how sequence modeling can improve policy learning by condensing trajectories into useful representations using Goal-Conditioned Predictive Coding (GCPC).</p><hr><h3>Leveraging sparse and shared feature activations for disentangled representation learning</h3>
<p>Marco Fumero, Florian Wenzel, Luca Zancato, Alessandro Achille, Emanuele Rodolà, Stefano Soatto, Bernhard Schölkopf, Francesco Locatello</p>
<p><a href='https://openreview.net/forum?id=IHR83ufYPy'>https://openreview.net/forum?id=IHR83ufYPy</a></p>
<p><b>Keywords</b>: disentanglement, OOD generalization, multitask learning
</p><p><b>Compressor summary</b>: The authors propose a method to learn disentangled features from multiple supervised tasks without directly observing the factors of variation, and show its effectiveness on various real-world data.</p><hr><h3>Connecting Multi-modal Contrastive Representations</h3>
<p>Zehan Wang, Yang Zhao, Xize Cheng, Haifeng Huang, Jiageng Liu, Aoxiong Yin, Li Tang, Linjun Li, Yongqi Wang, Ziang Zhang, Zhou Zhao</p>
<p><a href='https://openreview.net/forum?id=IGTbT9P1ti'>https://openreview.net/forum?id=IGTbT9P1ti</a></p>
<p><b>Keywords</b>: multi-modal, representation learning, contrastive learning
</p><p><b>Compressor summary</b>: The paper proposes a method called C-MCR to learn multi-modal representations without paired data by connecting existing MCRs trained on different modality pairs using overlapping modalities and enhancing semantic consistency.</p><hr><h3>SHAP-IQ: Unified Approximation of any-order Shapley Interactions</h3>
<p>Fabian Fumagalli, Maximilian Muschalik, Patrick Kolpaczki, Eyke Hüllermeier, Barbara Eva Hammer</p>
<p><a href='https://openreview.net/forum?id=IEMLNF4gK4'>https://openreview.net/forum?id=IEMLNF4gK4</a></p>
<p><b>Keywords</b>: Explainable Artificial Intelligence, Feature Interaction, Shapley Interaction, Shapley Value
</p><p><b>Compressor summary</b>: The paper introduces SHAP-IQ, a method for computing Shapley interactions in XAI using a novel representation and sampling, with theoretical guarantees and improved calculations.</p><hr><h3>Estimating Causal Effects Identifiable from a Combination of Observations and Experiments</h3>
<p>Yonghan Jung, Ivan Diaz, Jin Tian, Elias Bareinboim</p>
<p><a href='https://openreview.net/forum?id=IEJzoOBM0z'>https://openreview.net/forum?id=IEJzoOBM0z</a></p>
<p><b>Keywords</b>: Causal Effect Estimation, Causal Effect Identification, Data Fusion, Double Machine Learning, Doubly Robust Estimator
</p><p><b>Compressor summary</b>: The paper proposes a new estimator for learning causal relations from observational and interventional data that is robust to bias and can handle multiple outcomes.</p><hr><h3>Evaluating and Inducing Personality in Pre-trained Language Models</h3>
<p>Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, Yixin Zhu</p>
<p><a href='https://openreview.net/forum?id=I9xE1Jsjfx'>https://openreview.net/forum?id=I9xE1Jsjfx</a></p>
<p><b>Keywords</b>: machine personality, machine behavior, personality trait theory, psychometric, large language models, prompt
</p><p><b>Compressor summary</b>: The authors propose using a Machine Personality Inventory (MPI) tool to study and induce specific personalities in large language models (LLMs), inspired by human psychometric tests and the Big Five Personality Factors theory.</p><hr><h3>Formulating Discrete Probability Flow Through Optimal Transport</h3>
<p>Pengze Zhang, Hubery Yin, Chen Li, Xiaohua Xie</p>
<p><a href='https://openreview.net/forum?id=I9GNrInbdf'>https://openreview.net/forum?id=I9GNrInbdf</a></p>
<p><b>Keywords</b>: Discrete Probability Flow, Optimal Transport
</p><p><b>Compressor summary</b>: The paper introduces a new theory for discrete diffusion models, based on optimal transport principles, and shows how it leads to a better sampling method with more certain outcomes.</p><hr><h3>State Regularized Policy Optimization on Data with Dynamics Shift</h3>
<p>Zhenghai Xue, Qingpeng Cai, Shuchang Liu, Dong Zheng, Peng Jiang, Kun Gai, Bo An</p>
<p><a href='https://openreview.net/forum?id=I8t9RKDnz2'>https://openreview.net/forum?id=I8t9RKDnz2</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Dynamics Shift, Stationary State Distribution, Offline RL, Off-Policy RL
</p><p><b>Compressor summary</b>: The paper proposes a new Reinforcement Learning algorithm (SRPO) that uses stationary state distributions to regularize policies, enabling efficient data reuse and improving the performance of context-based algorithms in different environments with similar structures but different dynamics.</p><hr><h3>Robust Concept Erasure via Kernelized Rate-Distortion Maximization</h3>
<p>Somnath Basu Roy Chowdhury, Nicholas Monath, Kumar Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi</p>
<p><a href='https://openreview.net/forum?id=I6aOjhpcNQ'>https://openreview.net/forum?id=I6aOjhpcNQ</a></p>
<p><b>Keywords</b>: Concept Erasure, Representation Learning, Rate distortion, Fairness, Debiasing
</p><p><b>Compressor summary</b>: The paper introduces KRaM, a new objective for concept erasure in distributed representations, which uses a rate-distortion function to transform representations while preserving other information. KRaM can handle various concepts and domains and shows good results in experiments.</p><hr><h3>Deductive Verification of Chain-of-Thought Reasoning</h3>
<p>Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su</p>
<p><a href='https://openreview.net/forum?id=I5rsM4CY2z'>https://openreview.net/forum?id=I5rsM4CY2z</a></p>
<p><b>Keywords</b>: Chain-of-thought, Large language model, Reasoning
</p><p><b>Compressor summary</b>: The text discusses a proposed method to improve large language models' deductive reasoning through natural language-based reasoning verification in step-by-step subprocesses.</p><hr><h3>On Robust Streaming for Learning with Experts: Algorithms and Lower Bounds</h3>
<p>David Woodruff, Fred Zhang, Samson Zhou</p>
<p><a href='https://openreview.net/forum?id=I5SM5y57k2'>https://openreview.net/forum?id=I5SM5y57k2</a></p>
<p><b>Keywords</b>: online learning, memory efficiency, sub-linear algorithm, communication lower bound
</p><p><b>Compressor summary</b>: The paper studies robust algorithms for online learning with experts under memory constraints and shows a trade-off between space and regret, while also providing a lower bound on space usage for deterministic algorithms.</p><hr><h3>Provably Bounding Neural Network Preimages</h3>
<p>Suhas Kotha, Christopher Brix, J Zico Kolter, Krishnamurthy Dj Dvijotham, Huan Zhang</p>
<p><a href='https://openreview.net/forum?id=I50HbChk3U'>https://openreview.net/forum?id=I50HbChk3U</a></p>
<p><b>Keywords</b>: Trustworthy ML, Formal Verification, Safe Control, OOD Detection
</p><p><b>Compressor summary</b>: The INVPROP algorithm efficiently over-approximates inputs that lead to specific outputs for neural networks and other systems, outperforming prior work in speed and precision.</p><hr><h3>Randomized and Deterministic Maximin-share Approximations for Fractionally Subadditive Valuations</h3>
<p>Hannaneh Akrami, Kurt Mehlhorn, Masoud Seddighin, Golnoosh Shahkarami</p>
<p><a href='https://openreview.net/forum?id=I3k2NHt1zu'>https://openreview.net/forum?id=I3k2NHt1zu</a></p>
<p><b>Keywords</b>: Algorithmic game theory, Fairness, Randomized, Allocation, Maximin-share, Fractionally Subadditive
</p><p><b>Compressor summary</b>: The paper studies how to fairly allocate indivisible items to agents with subadditive valuations, improving both deterministic and randomized allocation methods.</p><hr><h3>GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization</h3>
<p>Vicente Vivanco Cepeda, Gaurav Kumar Nayak, Mubarak Shah</p>
<p><a href='https://openreview.net/forum?id=I18BXotQ7j'>https://openreview.net/forum?id=I18BXotQ7j</a></p>
<p><b>Keywords</b>: Geo-localization, Image-to-GPS retrieval, CLIP, Random Fourier Features
</p><p><b>Compressor summary</b>: GeoCLIP is a new method for pinpointing images' locations on Earth using GPS encoding and a hierarchical representation, overcoming limitations of previous approaches.</p><hr><h3>Future-Dependent Value-Based Off-Policy Evaluation in POMDPs</h3>
<p>Masatoshi Uehara, Haruka Kiyohara, Andrew Bennett, Victor Chernozhukov, Nan Jiang, Nathan Kallus, Chengchun Shi, Wen Sun</p>
<p><a href='https://openreview.net/forum?id=HwhRehMr4a'>https://openreview.net/forum?id=HwhRehMr4a</a></p>
<p><b>Keywords</b>: Reinforcement learning theory, POMDP, PAC RL, Off-policy evaluation, Offilne reinforcement learning
</p><p><b>Compressor summary</b>: The paper proposes a new model-free off-policy evaluation method for partially observable MDPs using future-dependent value functions and conditional moment equations, with PAC result and Bellman completeness guarantees.</p><hr><h3>Synthetic-to-Real Pose Estimation with Geometric Reconstruction</h3>
<p>Qiuxia Lin, Kerui Gu, Linlin Yang, Angela Yao</p>
<p><a href='https://openreview.net/forum?id=HvhagNdf5z'>https://openreview.net/forum?id=HvhagNdf5z</a></p>
<p><b>Keywords</b>: pose estimation, domain adaptation
</p><p><b>Compressor summary</b>: The paper proposes a reconstruction-based strategy to adapt models trained on synthetic data to real-world domains using unlabelled data, improving keypoint localization accuracy for hand and human pose estimation tasks.</p><hr><h3>Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models</h3>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao</p>
<p><a href='https://openreview.net/forum?id=HtqnVSCj3q'>https://openreview.net/forum?id=HtqnVSCj3q</a></p>
<p><b>Keywords</b>: large language models, compositional reasoning, tool use, multi-modal reasoning, mathematical reasoning
</p><p><b>Compressor summary</b>: Chameleon is an AI system that augments large language models with plug-and-play modules for compositional reasoning, enabling them to access up-to-date information and perform complex tasks.</p><hr><h3>Understanding and Mitigating Copying in Diffusion Models</h3>
<p>Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, Tom Goldstein</p>
<p><a href='https://openreview.net/forum?id=HtMXRGbUMt'>https://openreview.net/forum?id=HtMXRGbUMt</a></p>
<p><b>Keywords</b>: diffusion models, memorization, data replication, model safety
</p><p><b>Compressor summary</b>: The paper analyzes how text-to-image diffusion models can unintentionally copy images from their training data and proposes techniques to reduce this problem by modifying image captions.</p><hr><h3>Addressing the speed-accuracy simulation trade-off for adaptive spiking neurons</h3>
<p>Luke Taylor, Andrew J King, Nicol Spencer Harper</p>
<p><a href='https://openreview.net/forum?id=Ht79ZTVMsn'>https://openreview.net/forum?id=Ht79ZTVMsn</a></p>
<p><b>Keywords</b>: spiking neural network, surrogate gradient descent, adaptive leaky integrate and fire neuron, speed-accuracy trade-off, electrophysiological recordings
</p><p><b>Compressor summary</b>: The ALIF model is a brain simulation tool that can be made faster and more accurate by using parallel processing on GPUs and smaller time-steps.</p><hr><h3>Causal Component Analysis</h3>
<p>Wendong Liang, Armin Kekić, Julius von Kügelgen, Simon Buchholz, Michel Besserve, Luigi Gresele, Bernhard Schölkopf</p>
<p><a href='https://openreview.net/forum?id=HszLRiHyfO'>https://openreview.net/forum?id=HszLRiHyfO</a></p>
<p><b>Keywords</b>: Causality, independent component analysis, causal inference, interventions, latent variable models, identifiability
</p><p><b>Compressor summary</b>: Causal Component Analysis (CauCA) is an extension of Independent Component Analysis (ICA) that incorporates causal relationships among latent variables, and can be seen as a special case of Causal Representation Learning (CRL).</p><hr><h3>Assessor360: Multi-sequence Network for Blind Omnidirectional Image Quality Assessment</h3>
<p>Tianhe Wu, Shuwei Shi, Haoming Cai, Mingdeng Cao, Jing Xiao, Yinqiang Zheng, Yujiu Yang</p>
<p><a href='https://openreview.net/forum?id=HrL1oblm1a'>https://openreview.net/forum?id=HrL1oblm1a</a></p>
<p><b>Keywords</b>: Blind omnidirectional image quality assessment, Multi-sequence network, Viewport sequence
</p><p><b>Compressor summary</b>: The paper introduces Assessor360, a novel network for objectively assessing the quality of omnidirectional images (ODIs) used in virtual reality, by modeling the observer's browsing process and using a multi-sequence approach with recursive probability sampling, multi-scale feature aggregation, and temporal modeling.</p><hr><h3>Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars</h3>
<p>Simon Schrodi, Danny Stoll, Binxin Ru, Rhea Sanjay Sukthanker, Thomas Brox, Frank Hutter</p>
<p><a href='https://openreview.net/forum?id=Hpt1i5j6wh'>https://openreview.net/forum?id=Hpt1i5j6wh</a></p>
<p><b>Keywords</b>: Neural Architecture Search, Hierarchical Search Space, Context-free Grammars, Bayesian Optimization
</p><p><b>Compressor summary</b>: The paper introduces a new framework for generating large hierarchical search spaces for Neural Architecture Search using context-free grammars and an efficient kernel design for Bayesian Optimization.</p><hr><h3>Ensemble-based Deep Reinforcement Learning for Vehicle Routing Problems under Distribution Shift</h3>
<p>Yuan Jiang, Zhiguang Cao, Yaoxin Wu, Wen Song, Jie Zhang</p>
<p><a href='https://openreview.net/forum?id=HoBbZ1vPAh'>https://openreview.net/forum?id=HoBbZ1vPAh</a></p>
<p><b>Keywords</b>: Vehicle Routing Problem, Distribution shift, Deep Reinforcement Learning, Ensemble Learning
</p><p><b>Compressor summary</b>: The authors propose a diverse ensemble of sub-policies using deep reinforcement learning to improve vehicle routing performance across different instance distributions.</p><hr><h3>Learning in the Presence of Low-dimensional Structure: A Spiked Random Matrix Perspective</h3>
<p>Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu</p>
<p><a href='https://openreview.net/forum?id=HlIAoCHDWW'>https://openreview.net/forum?id=HlIAoCHDWW</a></p>
<p><b>Keywords</b>: random matrix theory, high-dimensional statistics, neural network, kernel method, feature learning
</p><p><b>Compressor summary</b>: The paper investigates how the strength of a low-dimensional component in high-dimensional data affects the learning performance of kernel methods and neural networks with gradient descent.</p><hr><h3>Diffusion-Based Probabilistic Uncertainty Estimation for Active Domain Adaptation</h3>
<p>Zhekai Du, Jingjing Li</p>
<p><a href='https://openreview.net/forum?id=HffQOS3mk8'>https://openreview.net/forum?id=HffQOS3mk8</a></p>
<p><b>Keywords</b>: diffusion-based models, active learning, domain adaptation, source-free domain adaptation, uncertainty estimation
</p><p><b>Compressor summary</b>: The paragraph describes a probabilistic framework for active domain adaptation that uses variational inference to capture data-level and prediction-level uncertainties, enabling efficient sampling of all possible predictions and selective annotation of informative target samples based on p-values.</p><hr><h3>ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets</h3>
<p>Damien Teney, LIN Yong, Seong Joon Oh, Ehsan Abbasnejad</p>
<p><a href='https://openreview.net/forum?id=HZQZli6amV'>https://openreview.net/forum?id=HZQZli6amV</a></p>
<p><b>Keywords</b>: Generalisation; machine learning
</p><p><b>Compressor summary</b>: The paper reveals that there are cases where in-distribution and out-of-distribution performance trade-offs exist, contrary to previous findings, and suggests that relying solely on in-distribution performance for model selection can lead to suboptimal out-of-distribution generalization.</p><hr><h3>SLIBO-Net: Floorplan Reconstruction via Slicing Box Representation with Local Geometry Regularization</h3>
<p>Jheng-Wei Su, Kuei-Yu Tung, Chi-Han Peng, Peter Wonka, Hung-Kuo Chu</p>
<p><a href='https://openreview.net/forum?id=HYo2Ao3hP8'>https://openreview.net/forum?id=HYo2Ao3hP8</a></p>
<p><b>Keywords</b>: deep learning, layout reconstruction
</p><p><b>Compressor summary</b>: The paper introduces SLIBO-Net, a novel transformer-based approach to reconstruct 2D floorplans from unstructured 3D point clouds with improved semantic quality, efficient representation, and local geometric details, achieving state-of-the-art results on the Structure3D dataset.</p><hr><h3>Alignment with human representations supports robust few-shot learning</h3>
<p>Ilia Sucholutsky, Thomas L. Griffiths</p>
<p><a href='https://openreview.net/forum?id=HYGnmSLBCf'>https://openreview.net/forum?id=HYGnmSLBCf</a></p>
<p><b>Keywords</b>: representation learning, supervised learning, human alignment, few-shot learning
</p><p><b>Compressor summary</b>: AI systems with representations similar to humans perform better on few-shot learning tasks, are more robust to attacks and domain shifts, but having human alignment is not always necessary.</p><hr><h3>Video Prediction Models as Rewards for Reinforcement Learning</h3>
<p>Alejandro Escontrela, Ademi Adeniji, Wilson Yan, Ajay Jain, Xue Bin Peng, Ken Goldberg, Youngwoon Lee, Danijar Hafner, Pieter Abbeel</p>
<p><a href='https://openreview.net/forum?id=HWNl9PAYIP'>https://openreview.net/forum?id=HWNl9PAYIP</a></p>
<p><b>Keywords</b>: Reinforcement learning, generative modeling, learning from demonstrations, video prediction, unsupervised reinforcement learning
</p><p><b>Compressor summary</b>: VIPER is an algorithm that learns expert-level behaviors for reinforcement learning agents by extracting preferences from unlabeled videos using pretrained video prediction models.</p><hr><h3>On the Convergence and Sample Complexity Analysis of Deep Q-Networks with $\epsilon$-Greedy Exploration</h3>
<p>Shuai Zhang, Hongkang Li, Meng Wang, Miao Liu, Pin-Yu Chen, Songtao Lu, Sijia Liu, Keerthiram Murugesan, Subhajit Chaudhury</p>
<p><a href='https://openreview.net/forum?id=HWGWeaN76q'>https://openreview.net/forum?id=HWGWeaN76q</a></p>
<p><b>Keywords</b>: Reinforcement learning, Deep Q Network, Convergence analysis, Sample complexity, Generalization analysis
</p><p><b>Compressor summary</b>: The paper analyzes the theoretical aspects of deep Q-Network with epsilon-greedy exploration in reinforcement learning and shows how its convergence and sample complexity depend on the epsilon value.</p><hr><h3>Online RL in Linearly $q^\pi$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore</h3>
<p>Gellért Weisz, András György, Csaba Szepesvari</p>
<p><a href='https://openreview.net/forum?id=HV85SiyrsV'>https://openreview.net/forum?id=HV85SiyrsV</a></p>
<p><b>Keywords</b>: Reinforcement learning, linear function approximation, online learning
</p><p><b>Compressor summary</b>: The paper proposes a novel online reinforcement learning algorithm for linearly $q^\pi$-realizable MDPs that can efficiently learn policies even in the presence of states with nearly equal action-values and misspecified features.</p><hr><h3>Detecting hidden confounding in observational data using multiple environments</h3>
<p>Rickard Karlsson, JH Krijthe</p>
<p><a href='https://openreview.net/forum?id=HUuEMMM8Ik'>https://openreview.net/forum?id=HUuEMMM8Ik</a></p>
<p><b>Keywords</b>: causal inference, hidden confounding, multiple environments, independent causal mechansisms, independence testing
</p><p><b>Compressor summary</b>: The paper proposes a method to detect hidden confounding factors in multiple observational datasets by testing conditional independencies and evaluates its performance using simulations and real-world data.</p><hr><h3>Guiding The Last Layer in Federated Learning with Pre-Trained Models</h3>
<p>Gwen Legate, Nicolas Bernier, Lucas Caccia, Edouard Oyallon, Eugene Belilovsky</p>
<p><a href='https://openreview.net/forum?id=HRGd5dcVfw'>https://openreview.net/forum?id=HRGd5dcVfw</a></p>
<p><b>Keywords</b>: federated learning; transfer learning; nearest mean classifier; continual learning;
</p><p><b>Compressor summary</b>: The text describes how using pre-trained models and efficient transfer learning methods can improve federated learning by reducing costs and improving performance.</p><hr><h3>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</h3>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn</p>
<p><a href='https://openreview.net/forum?id=HPuSIXJaa9'>https://openreview.net/forum?id=HPuSIXJaa9</a></p>
<p><b>Keywords</b>: reinforcement learning from human feedback, language models, RLHF, preferences
</p><p><b>Compressor summary</b>: DPO is a simple and effective method for aligning large unsupervised language models with human preferences, outperforming existing methods like RLHF.</p><hr><h3>Learning Dense Flow Field for Highly-accurate Cross-view Camera Localization</h3>
<p>Zhenbo Song, XiangHui Ze, Jianfeng Lu, Yujiao Shi</p>
<p><a href='https://openreview.net/forum?id=HPrd17Qvbp'>https://openreview.net/forum?id=HPrd17Qvbp</a></p>
<p><b>Keywords</b>: Optical flow, correspondence learning, cross-view, camera localization
</p><p><b>Compressor summary</b>: The paper proposes a novel method to estimate camera pose using pixel-wise flow fields learned from ground and satellite images, improving localization accuracy significantly.</p><hr><h3>A Heat Diffusion Perspective on Geodesic Preserving Dimensionality Reduction</h3>
<p>Guillaume Huguet, Alexander Tong, Edward De Brouwer, Yanlei Zhang, Guy Wolf, Ian Adelstein, Smita Krishnaswamy</p>
<p><a href='https://openreview.net/forum?id=HNd4qTJxkW'>https://openreview.net/forum?id=HNd4qTJxkW</a></p>
<p><b>Keywords</b>: manifold learning, heat diffusion, geodesic, metric preserving, dimensionality reduction, embedding
</p><p><b>Compressor summary</b>: The authors propose a new heat geodesic embedding method for manifold learning and denoising, which connects heat diffusion to manifold distances using Riemannian geometry and outperforms existing methods in preserving ground truth distances and cluster structure.</p><hr><h3>A Simple Yet Effective Strategy to Robustify the Meta Learning Paradigm</h3>
<p>Cheems Wang, Yiqin Lv, Yanghe Feng, Zheng Xie, Jincai Huang</p>
<p><a href='https://openreview.net/forum?id=HMqGYxnlpv'>https://openreview.net/forum?id=HMqGYxnlpv</a></p>
<p><b>Keywords</b>: meta learning, robust fast adaptation, model agnostic meta learning
</p><p><b>Compressor summary</b>: This paper proposes a distributionally robust optimization approach for meta learning, which improves its robustness to task distributions and reduces the worst fast adaptation risk.</p><hr><h3>Unifying GANs and Score-Based Diffusion as Generative Particle Models</h3>
<p>Jean-Yves Franceschi, Mike Gartrell, Ludovic Dos Santos, Thibaut Issenhuth, Emmanuel de Bezenac, Mickael Chen, Alain Rakotomamonjy</p>
<p><a href='https://openreview.net/forum?id=HMhEFKDQ6J'>https://openreview.net/forum?id=HMhEFKDQ6J</a></p>
<p><b>Keywords</b>: deep learning, generative models, GANs, generative adversarial networks, diffusion, score-based, gradient flows
</p><p><b>Compressor summary</b>: The paper proposes a unified framework for particle and adversarial generative models, suggesting that a generator is an optional addition to any generative model, and empirically tests its applications.</p><hr><h3>Embedding Space Interpolation Beyond Mini-Batch, Beyond Pairs and Beyond Examples</h3>
<p>Shashanka Venkataramanan, Ewa Kijak, laurent amsaleg, Yannis Avrithis</p>
<p><a href='https://openreview.net/forum?id=HKueO74ZTB'>https://openreview.net/forum?id=HKueO74ZTB</a></p>
<p><b>Keywords</b>: Interpolation based data augmentation, mixup, dense interpolation, robustness, representation learning
</p><p><b>Compressor summary</b>: The authors propose MultiMix, a data augmentation method that generates more interpolated examples in the embedding space than previous methods, leading to better performance on sequence data tasks.</p><hr><h3>Aligning Language Models with Human Preferences via a Bayesian Approach</h3>
<p>Jiashuo WANG, Haozhao Wang, Shichao Sun, Wenjie Li</p>
<p><a href='https://openreview.net/forum?id=HGFcM3UU50'>https://openreview.net/forum?id=HGFcM3UU50</a></p>
<p><b>Keywords</b>: Aligned Models; Human-centric NLG
</p><p><b>Compressor summary</b>: The paper proposes a novel Bayesian framework called d-PM to account for the distribution of disagreements among human preferences in NLG systems and uses contrastive learning to train the model efficiently, achieving better results than previous methods.</p><hr><h3>ResMem: Learn what you can and memorize the rest</h3>
<p>Zitong Yang, Michal Lukasik, Vaishnavh Nagarajan, Zonglin Li, Ankit Singh Rawat, Manzil Zaheer, Aditya Krishna Menon, Sanjiv Kumar</p>
<p><a href='https://openreview.net/forum?id=HFQFAyNucq'>https://openreview.net/forum?id=HFQFAyNucq</a></p>
<p><b>Keywords</b>: deep learning, generalization, memorization, deep learning theory, boosting, nearest neighbor
</p><p><b>Compressor summary</b>: ResMem is a method to improve model generalization by explicitly memorizing training labels through fitting residuals with a nearest-neighbor based regressor.</p><hr><h3>On quantum backpropagation, information reuse, and cheating measurement collapse</h3>
<p>Amira Abbas, Robbie King, Hsin-Yuan Huang, William J. Huggins, Ramis Movassagh, Dar Gilboa, Jarrod Ryan McClean</p>
<p><a href='https://openreview.net/forum?id=HF6bnhfSqH'>https://openreview.net/forum?id=HF6bnhfSqH</a></p>
<p><b>Keywords</b>: Backpropagation, quantum computing, shadow tomography, gentle measurement
</p><p><b>Compressor summary</b>: The paper explores how to train quantum neural networks efficiently by using multiple copies of a state and shadow tomography, similar to backpropagation in classical deep learning.</p><hr><h3>Training Transformers with 4-bit Integers</h3>
<p>Haocheng Xi, ChangHao Li, Jianfei Chen, Jun Zhu</p>
<p><a href='https://openreview.net/forum?id=H9hWlfMT6O'>https://openreview.net/forum?id=H9hWlfMT6O</a></p>
<p><b>Keywords</b>: neural network quantization, transformer, matrix multiplication, randomized numerical linear algebra
</p><p><b>Compressor summary</b>: The paper proposes a new method to train transformer models with 4-bit precision, which is faster and can work on current GPUs, by using dedicated quantizers for activations, weights, and gradients.</p><hr><h3>Failure-Aware Gaussian Process Optimization with Regret Bounds</h3>
<p>Shogo Iwazaki, Shion Takeno, Tomohiko Tanabe, Mitsuru Irie</p>
<p><a href='https://openreview.net/forum?id=H5pwAeYAun'>https://openreview.net/forum?id=H5pwAeYAun</a></p>
<p><b>Keywords</b>: Gaussian process optimization, regret analysis, black-box optimization, Bayesian optimization
</p><p><b>Compressor summary</b>: F-GP-UCB is a black-box optimization method that handles observation failure with complex latent constraints and provides regret upper bounds and convergence guarantees.</p><hr><h3>Facilitating Graph Neural Networks with Random Walk on Simplicial Complexes</h3>
<p>Cai Zhou, Xiyuan Wang, Muhan Zhang</p>
<p><a href='https://openreview.net/forum?id=H57w5EOj6O'>https://openreview.net/forum?id=H57w5EOj6O</a></p>
<p><b>Keywords</b>: random walk on simplicials, Hodge Laplacian, graph neural networks, edge-level positional encoding
</p><p><b>Compressor summary</b>: The paper studies how random walk on different orders of simplicial complexes improves Graph Neural Networks' theoretical expressivity and proposes new positional encoding methods based on Hodge Laplacians.</p><hr><h3>On the Overlooked Structure of Stochastic Gradients</h3>
<p>Zeke Xie, Qian-Yuan Tang, Mingming Sun, Ping Li</p>
<p><a href='https://openreview.net/forum?id=H4GsteoL0M'>https://openreview.net/forum?id=H4GsteoL0M</a></p>
<p><b>Keywords</b>: Gradient Noise, SGD, Deep Learning
</p><p><b>Compressor summary</b>: This paper investigates the heavy-tail properties of stochastic gradients in deep neural networks and reveals new insights into their covariance structures.</p><hr><h3>Grassmann Manifold Flows for Stable Shape Generation</h3>
<p>Ryoma Yataka, Kazuki Hirashima, Masashi Shiraishi</p>
<p><a href='https://openreview.net/forum?id=H2udtfMbl4'>https://openreview.net/forum?id=H2udtfMbl4</a></p>
<p><b>Keywords</b>: Generative Models, Geometric Deep Learning, Normalizing Flows, Shape Analysis, Grassmann Manifold
</p><p><b>Compressor summary</b>: This paper proposes a novel method to generate stable shapes using Grassmann manifolds and continuous normalization flows in machine learning, outperforming existing methods.</p><hr><h3>Tree-Based Diffusion Schrödinger Bridge with Applications to Wasserstein Barycenters</h3>
<p>Maxence Noble, Valentin De Bortoli, Arnaud Doucet, Alain Durmus</p>
<p><a href='https://openreview.net/forum?id=H2SuXHbFIn'>https://openreview.net/forum?id=H2SuXHbFIn</a></p>
<p><b>Keywords</b>: Schrödinger bridge, optimal transport, diffusion model, Wasserstein barycenter
</p><p><b>Compressor summary</b>: The paper introduces TreeDSB, an extension of DSB for solving entropic multi-marginal optimal transport problems with tree-structured quadratic costs, which can be used for computing Wasserstein barycenters and applying to high-dimensional tasks like image interpolation and Bayesian fusion.</p><hr><h3>Accelerated Training via Incrementally Growing Neural Networks using Variance Transfer and Learning Rate Adaptation</h3>
<p>Xin Yuan, Pedro Henrique Pamplona Savarese, Michael Maire</p>
<p><a href='https://openreview.net/forum?id=H1a7bVVnPK'>https://openreview.net/forum?id=H1a7bVVnPK</a></p>
<p><b>Keywords</b>: network growing, efficient network training
</p><p><b>Compressor summary</b>: The approach grows neural networks efficiently by designing parameterization and optimization strategies for the training dynamics, achieving comparable or better accuracy than fixed-size models with less computation budget and faster training speed.</p><hr><h3>CBD: A Certified Backdoor Detector Based on Local Dominant Probability</h3>
<p>Zhen Xiang, Zidi Xiong, Bo Li</p>
<p><a href='https://openreview.net/forum?id=H1CQZqpgdQ'>https://openreview.net/forum?id=H1CQZqpgdQ</a></p>
<p><b>Keywords</b>: backdoor, Trojan, certification, adversarial learning, deep neural network, conformal prediction
</p><p><b>Compressor summary</b>: The paper introduces a novel certified backdoor detector (CBD) that uses conformal prediction and local dominant probability statistics to detect and guarantee the detection of various backdoor attacks on deep neural networks.</p><hr><h3>Training Fully Connected Neural Networks is $\exists\mathbb{R}$-Complete</h3>
<p>Daniel Bertschinger, Christoph Hertrich, Paul Jungeblut, Tillmann Miltzow, Simon Weber</p>
<p><a href='https://openreview.net/forum?id=H15KtcyHvn'>https://openreview.net/forum?id=H15KtcyHvn</a></p>
<p><b>Keywords</b>: Neural Network Training, Computational Complexity, Existential Theory of the Reals, Algebraic Universality, Empirical Risk Minimization
</p><p><b>Compressor summary</b>: The paper shows that finding the optimal weights and biases for a two-layer neural network is as hard as solving real roots of multivariate polynomials with integer coefficients.</p><hr><h3>Functional Renyi Differential Privacy for Generative Modeling</h3>
<p>Dihong Jiang, Sun Sun, Yaoliang Yu</p>
<p><a href='https://openreview.net/forum?id=GzlDKZlwie'>https://openreview.net/forum?id=GzlDKZlwie</a></p>
<p><b>Keywords</b>: Renyi differential privacy, RKHS, MMD, Gaussian process, generative model
</p><p><b>Compressor summary</b>: The paper extends differential privacy to functional outputs and applies it to a generative model, improving the privacy-utility trade-off.</p><hr><h3>Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation</h3>
<p>Seunghwan An, Jong-June Jeon</p>
<p><a href='https://openreview.net/forum?id=GxL6PrmEUw'>https://openreview.net/forum?id=GxL6PrmEUw</a></p>
<p><b>Keywords</b>: Variational AutoEncoder, distributional learning, synthetic data generation, CRPS, asymmetric Laplace distribution
</p><p><b>Compressor summary</b>: The paper introduces a new VAE model with an infinite mixture of asymmetric Laplace distribution that can fit any continuous distribution and has better quantile estimation and privacy control properties.</p><hr><h3>Unified Segment-to-Segment Framework for Simultaneous Sequence Generation</h3>
<p>Shaolei Zhang, Yang Feng</p>
<p><a href='https://openreview.net/forum?id=GuErIOGLie'>https://openreview.net/forum?id=GuErIOGLie</a></p>
<p><b>Keywords</b>: Machine Translation, Speech Translation, Speech Recognition, Simultaneous Generation, Simultaneous Translation
</p><p><b>Compressor summary</b>: The paper introduces a framework called Seg2Seg that learns to generate target sequences adaptively and simultaneously from source sequences in different tasks, using latent segments as the bridge between them.</p><hr><h3>Robustifying Generalizable Implicit Shape Networks with a Tunable Non-Parametric Model</h3>
<p>Amine Ouasfi, Adnane Boukhayma</p>
<p><a href='https://openreview.net/forum?id=Gtse4R6iS4'>https://openreview.net/forum?id=Gtse4R6iS4</a></p>
<p><b>Keywords</b>: implicit neural representations, 3D reconstruction from unoriented point could, kernel ridge regression
</p><p><b>Compressor summary</b>: The authors propose a method to improve implicit shape reconstruction from point clouds by combining inter-shape and intra-shape priors, achieving better stability and efficiency.</p><hr><h3>Joint Data-Task Generation for Auxiliary Learning</h3>
<p>Hong Chen, Xin Wang, Yuwei Zhou, Yijian Qin, Chaoyu Guan, Wenwu Zhu</p>
<p><a href='https://openreview.net/forum?id=GtgFo5lmOB'>https://openreview.net/forum?id=GtgFo5lmOB</a></p>
<p><b>Keywords</b>: auxiliary learning, data-task joint generation
</p><p><b>Compressor summary</b>: The paper proposes a new method for auxiliary learning that generates data and tasks jointly to improve the primary task performance and avoid relying on domain knowledge for data collection.</p><hr><h3>Robust covariance estimation with missing values and cell-wise contamination</h3>
<p>gregoire pacreau, Karim Lounici</p>
<p><a href='https://openreview.net/forum?id=GtYlxtwO74'>https://openreview.net/forum?id=GtYlxtwO74</a></p>
<p><b>Keywords</b>: robust statistics, missing values, cell-wise contamination
</p><p><b>Compressor summary</b>: The paper proposes a new estimator for covariance that works well with missing data and cell-wise outliers, without needing to remove or impute any data points.</p><hr><h3>REFINE: A Fine-Grained Medication Recommendation System Using Deep Learning and Personalized Drug Interaction Modeling</h3>
<p>Suman Bhoi, Mong-Li Lee, Wynne Hsu, Ngiap Chuan Tan</p>
<p><a href='https://openreview.net/forum?id=GsCTjmYe5v'>https://openreview.net/forum?id=GsCTjmYe5v</a></p>
<p><b>Keywords</b>: Fine-grained Medication recommendation, Drug Interaction Severity
</p><p><b>Compressor summary</b>: REFINE is a deep learning system that recommends fine-grained medications to patients with co-morbidities, considering their health conditions and drug interactions, and improves treatment outcomes and safety.</p><hr><h3>STXD: Structural and Temporal Cross-Modal Distillation for Multi-View 3D Object Detection</h3>
<p>Sujin Jang, Dae Ung Jo, Sung Ju Hwang, Dongwook Lee, Daehyun Ji</p>
<p><a href='https://openreview.net/forum?id=Grz2ijKrWI'>https://openreview.net/forum?id=Grz2ijKrWI</a></p>
<p><b>Keywords</b>: knowledge distillation, cross-modal learning, 3d object detection
</p><p><b>Compressor summary</b>: The paper proposes a novel framework called STXD to improve multi-view 3D object detection by transferring structural, temporal, and output knowledge from LiDAR-based teachers using cross-modal distillation.</p><hr><h3>Demographic Parity Constrained Minimax Optimal Regression under Linear Model</h3>
<p>Kazuto Fukuchi, Jun Sakuma</p>
<p><a href='https://openreview.net/forum?id=GrFsx4mBWF'>https://openreview.net/forum?id=GrFsx4mBWF</a></p>
<p><b>Keywords</b>: demographic parity, regression, minimax optimal
</p><p><b>Compressor summary</b>: We explore the minimax optimal error associated with a demographic parity-constrained regression problem within the context of a linear model. Our proposed model encompasses a broader range of discriminatory bias sources compared to the model presented by Chzhen and Schreuder. Our analysis reveals that the minimax optimal error for the demographic parity-constrained regression problem under our model is characterized by $\Theta(\frac{dM}{n})$, where $n$ denotes the sample size, $d$ represents the dimensionality, and $M$ signifies the number of demographic groups arising from sensitive attributes. Moreover, we demonstrate that the minimax error increases in conjunction with a larger bias present in the model.</p><hr><h3>Score-based Generative Modeling through Stochastic Evolution Equations in Hilbert Spaces</h3>
<p>Sungbin Lim, Eunbi Yoon, Taehyun Byun, Taewon Kang, Seungwoo Kim, Kyungjae Lee, Sungjoon Choi</p>
<p><a href='https://openreview.net/forum?id=GrElRvXnEj'>https://openreview.net/forum?id=GrElRvXnEj</a></p>
<p><b>Keywords</b>: Generative Model, Score-based Method, Diffusion Model
</p><p><b>Compressor summary</b>: The paper introduces Hilbert Diffusion Model (HDM), which extends score-based generative models to function spaces by using stochastic evolution equations in Hilbert spaces and achieves better performance in sampling functions and synthesizing motion.</p><hr><h3>$\varepsilon$-fractional core stability in Hedonic Games.</h3>
<p>Simone Fioravanti, Michele Flammini, Bojana Kodric, Giovanna Varricchio</p>
<p><a href='https://openreview.net/forum?id=GqtpYUCwnu'>https://openreview.net/forum?id=GqtpYUCwnu</a></p>
<p><b>Keywords</b>: Game Theory, Hedonic Games, Core stability, Coalition Formation, Social Choice, PAC learning
</p><p><b>Compressor summary</b>: The text introduces a new concept called $\varepsilon$-fractional core-stability to address the challenges of finding stable coalition structures in hedonic games, and presents efficient algorithms for two classes of games.</p><hr><h3>Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks</h3>
<p>Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang</p>
<p><a href='https://openreview.net/forum?id=GqXbfVmEPW'>https://openreview.net/forum?id=GqXbfVmEPW</a></p>
<p><b>Keywords</b>: few-shot learning, prompt learning, language model, backdoor defense
</p><p><b>Compressor summary</b>: The authors propose a defense method (MDP) for language models that are vulnerable to backdoor attacks in few-shot learning scenarios, which identifies poisoned samples by comparing their masking-sensitivity with clean samples.</p><hr><h3>Exploiting Contextual Objects and Relations for 3D Visual Grounding</h3>
<p>Li Yang, Chunfeng Yuan, Ziqi Zhang, Zhongang Qi, Yan Xu, Wei Liu, Ying Shan, Bing Li, Weiping Yang, Peng Li, Yan Wang, Weiming Hu</p>
<p><a href='https://openreview.net/forum?id=GlWzQhf2lV'>https://openreview.net/forum?id=GlWzQhf2lV</a></p>
<p><b>Keywords</b>: 3D Visual Grounding, Contextual Object, Contextual Relation
</p><p><b>Compressor summary</b>: The paper presents a new model, CORE-3DVG, that learns to identify objects in 3D scenes from natural language inputs by explicitly learning contextual information and achieves state-of-the-art performance on three datasets.</p><hr><h3>Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning</h3>
<p>Pier Giuseppe Sessa, Pierre Laforgue, Nicolò Cesa-Bianchi, Andreas Krause</p>
<p><a href='https://openreview.net/forum?id=GjJRbEZ1dc'>https://openreview.net/forum?id=GjJRbEZ1dc</a></p>
<p><b>Keywords</b>: multitask learning, confidence intervals, online learning theory, active learning, regret
</p><p><b>Compressor summary</b>: The text discusses novel confidence intervals for multitask regression in the agnostic setting, regret guarantees, an online learning algorithm that adapts to task similarity, and a multitask active learning setup with no-regret algorithms.</p><hr><h3>Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization</h3>
<p>Haitz Sáez de Ocáriz Borde, Alvaro Arroyo, Ismael Morales López, Ingmar Posner, Xiaowen Dong</p>
<p><a href='https://openreview.net/forum?id=Gij638d76O'>https://openreview.net/forum?id=Gij638d76O</a></p>
<p><b>Keywords</b>: Representation Learning, Product Manifolds, Bayesian Optimization, Gromov-Hausdorff Distance
</p><p><b>Compressor summary</b>: The paper proposes NLGS, a method to automatically find the best latent geometry for machine learning models using Bayesian optimization and metric geometry techniques.</p><hr><h3>Multi-Player Zero-Sum Markov Games with Networked Separable Interactions</h3>
<p>Chanwoo Park, Kaiqing Zhang, Asuman E. Ozdaglar</p>
<p><a href='https://openreview.net/forum?id=GiiOpKinGm'>https://openreview.net/forum?id=GiiOpKinGm</a></p>
<p><b>Keywords</b>: Markov Games, Local Interaction, PPAD-Hardness, Fictitious Play
</p><p><b>Compressor summary</b>: The paragraph introduces a new class of Markov games called zero-sum NMGs, which model non-cooperative multi-agent sequential decision-making with networked separable interactions, and studies their equilibrium properties and learning dynamics.</p><hr><h3>Bi-Level Offline Policy Optimization with Limited Exploration</h3>
<p>Wenzhuo Zhou</p>
<p><a href='https://openreview.net/forum?id=GiUe0ZFiVe'>https://openreview.net/forum?id=GiUe0ZFiVe</a></p>
<p><b>Keywords</b>: Offline Reinforcement Learning, Sample Efficiency, Regret Bound, Data Coverage
</p><p><b>Compressor summary</b>: The paper proposes a bi-level structured policy optimization algorithm for offline reinforcement learning that addresses the distributional shift problem by constructing a confidence set of value estimates and maximizing a conservative value estimate at the upper level.</p><hr><h3>Combining Behaviors with the Successor Features Keyboard</h3>
<p>Wilka Carvalho, Andre Saraiva, Angelos Filos, Andrew Kyle Lampinen, Loic Matthey, Richard Lewis, Honglak Lee, Satinder Singh, Danilo Jimenez Rezende, Daniel Zoran</p>
<p><a href='https://openreview.net/forum?id=GhNCFtLSsy'>https://openreview.net/forum?id=GhNCFtLSsy</a></p>
<p><b>Keywords</b>: deep reinforcement learning, successor features, transfer, generalization, feature-discovery
</p><p><b>Compressor summary</b>: The paper proposes a new method, Successor Features Keyboard (SFK), for transferring behavioral knowledge across tasks using discovered state-features and task encodings, and demonstrates its effectiveness in a 3D environment.</p><hr><h3>PreDiff: Precipitation Nowcasting with Latent Diffusion Models</h3>
<p>Zhihan Gao, Xingjian Shi, Boran Han, Hao Wang, Xiaoyong Jin, Danielle C. Maddix, Yi Zhu, Mu Li, Bernie Wang</p>
<p><a href='https://openreview.net/forum?id=Gh67ZZ6zkS'>https://openreview.net/forum?id=Gh67ZZ6zkS</a></p>
<p><b>Keywords</b>: Machine Learning for Earth Science, Spatiotemporal Forecasting, Generative Models, Diffusion Models
</p><p><b>Compressor summary</b>: The authors propose a two-stage pipeline for probabilistic spatiotemporal forecasting using a conditional latent diffusion model (PreDiff) and a knowledge alignment mechanism to handle uncertainty and incorporate physical constraints.</p><hr><h3>CoDrug: Conformal Drug Property Prediction with Density Estimation under Covariate Shift</h3>
<p>Siddhartha Laghuvarapu, Zhen Lin, Jimeng Sun</p>
<p><a href='https://openreview.net/forum?id=GgdFLb94Ld'>https://openreview.net/forum?id=GgdFLb94Ld</a></p>
<p><b>Keywords</b>: drug discovery, molecule property prediction, conformal prediction
</p><p><b>Compressor summary</b>: CoDrug is a method that uses an energy-based model and Kernel Density Estimation to create reliable uncertainty estimates for drug molecules, accounting for distribution shifts in drug discovery tasks.</p><hr><h3>Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent</h3>
<p>Giannis Daras, Yuval Dagan, Alex Dimakis, Constantinos Costis Daskalakis</p>
<p><a href='https://openreview.net/forum?id=GfZGdJHj27'>https://openreview.net/forum?id=GfZGdJHj27</a></p>
<p><b>Keywords</b>: diffusion models, sampling drift, Fokker-Planck, invariances, Stochastic Differential Equations, Martingales
</p><p><b>Compressor summary</b>: The paper proposes a method to improve the quality of generated images by enforcing a Consistency property that ensures predictions on generated data are consistent across time, and shows empirical results on various image datasets.</p><hr><h3>Human-Aligned Calibration for AI-Assisted Decision Making</h3>
<p>Nina L. Corvelo Benz, Manuel Gomez Rodriguez</p>
<p><a href='https://openreview.net/forum?id=GfITbjrIOd'>https://openreview.net/forum?id=GfITbjrIOd</a></p>
<p><b>Keywords</b>: Calibration, Trustworthy Machine Learning, Human-Centric ML, Probabilistic Models and Methods
</p><p><b>Compressor summary</b>: The paper explores why decision makers struggle with confidence values provided by binary classifiers and proposes constructing more useful confidence values based on alignment with the decision maker's own confidence.</p><hr><h3>May the Force be with You: Unified Force-Centric Pre-Training for 3D Molecular Conformations</h3>
<p>Rui Feng, Qi Zhu, Huan Tran, Binghong Chen, Aubrey Toland, Rampi Ramprasad, Chao Zhang</p>
<p><a href='https://openreview.net/forum?id=Ge8Mhggq0z'>https://openreview.net/forum?id=Ge8Mhggq0z</a></p>
<p><b>Keywords</b>: molecular pretraining, molecular representation learning
</p><p><b>Compressor summary</b>: The paper proposes a new force-centric pretraining model for 3D molecular conformations that covers both equilibrium and off-equilibrium data, improving forces accuracy, simulation performance, and inference speed.</p><hr><h3>Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning</h3>
<p>Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, Sergey Levine</p>
<p><a href='https://openreview.net/forum?id=GcEIvidYSw'>https://openreview.net/forum?id=GcEIvidYSw</a></p>
<p><b>Keywords</b>: offline reinforcement learning, online fine-tuning
</p><p><b>Compressor summary</b>: The paper proposes calibrated Q-learning, a method that initializes offline RL policies with conservative value functions, enabling fast online fine-tuning and improving performance on benchmark tasks.</p><hr><h3>Conditional Matrix Flows for Gaussian Graphical Models</h3>
<p>Marcello Massimo Negri, Fabricio Arend Torres, Volker Roth</p>
<p><a href='https://openreview.net/forum?id=GYnbubCXhE'>https://openreview.net/forum?id=GYnbubCXhE</a></p>
<p><b>Keywords</b>: normalizing flow, variational inference, graphical lasso, gaussian graphical model, bayesian inference
</p><p><b>Compressor summary</b>: The paper presents a new method to study conditional independence among many variables using Gaussian Graphical Models that unifies frequentist and Bayesian approaches and handles different sparsity-inducing norms.</p><hr><h3>FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing</h3>
<p>Mingyuan Zhang, Huirong Li, Zhongang Cai, Jiawei Ren, Lei Yang, Ziwei Liu</p>
<p><a href='https://openreview.net/forum?id=GYjV1M5s0D'>https://openreview.net/forum?id=GYjV1M5s0D</a></p>
<p><b>Keywords</b>: Motion Generation, Diffusion Model
</p><p><b>Compressor summary</b>: FineMoGen is a diffusion-based framework that generates and edits fine-grained motions using a novel spatio-temporal attention mechanism and a large dataset, enabling zero-shot motion editing with language models.</p><hr><h3>Structured Neural-PI Control with End-to-End Stability and Output Tracking Guarantees</h3>
<p>Wenqi Cui, Yan Jiang, Baosen Zhang, Yuanyuan Shi</p>
<p><a href='https://openreview.net/forum?id=GWIRpKF6yU'>https://openreview.net/forum?id=GWIRpKF6yU</a></p>
<p><b>Keywords</b>: Control, Stability, Tracking, Passivity, Neural network-based controllers, Power systems
</p><p><b>Compressor summary</b>: The paper proposes neural PI controllers with provable stability and output tracking guarantees for multiple-input and multiple-output systems using equilibrium-independent passivity and strictly convex neural networks.</p><hr><h3>ChatGPT-Powered Hierarchical Comparisons for Image Classification</h3>
<p>Zhiyuan Ren, Yiyang Su, Xiaoming Liu</p>
<p><a href='https://openreview.net/forum?id=GTYaYNsFyv'>https://openreview.net/forum?id=GTYaYNsFyv</a></p>
<p><b>Keywords</b>: ChatGPT, Hierarchical Comparisons, Image Classification, Zero shot
</p><p><b>Compressor summary</b>: The authors propose a novel image classification framework using hierarchical comparisons of text embeddings to overcome CLIP's limitations and achieve better accuracy.</p><hr><h3>4D Panoptic Scene Graph Generation</h3>
<p>Jingkang Yang, Jun CEN, Wenxuan Peng, Shuai Liu, Fangzhou Hong, Xiangtai Li, Kaiyang Zhou, Qifeng Chen, Ziwei Liu</p>
<p><a href='https://openreview.net/forum?id=GRHZiTbDDI'>https://openreview.net/forum?id=GRHZiTbDDI</a></p>
<p><b>Keywords</b>: Scene Graph Generation, 4D Understanding, 4D Perception.
</p><p><b>Compressor summary</b>: The paragraph introduces PSG-4D, a new 4D visual representation for AI, and presents a dataset, model, and application example for it.</p><hr><h3>Adversarial Training for Graph Neural Networks: Pitfalls, Solutions, and New Directions</h3>
<p>Lukas Gosch, Simon Geisler, Daniel Sturm, Bertrand Charpentier, Daniel Zügner, Stephan Günnemann</p>
<p><a href='https://openreview.net/forum?id=GPtroppvUM'>https://openreview.net/forum?id=GPtroppvUM</a></p>
<p><b>Keywords</b>: adversarial training, adversarial examples, robust graph learning, graph machine learning, graph neural networks, graphs
</p><p><b>Compressor summary</b>: The authors propose and evaluate a flexible graph neural network model with learnable graph diffusion that can defend against adversarial perturbations in the graph structure, while also introducing a new attack method for such perturbations.</p><hr><h3>D4Explainer: In-distribution Explanations of Graph Neural Network via Discrete Denoising Diffusion</h3>
<p>Jialin Chen, Shirley Wu, Abhijit Gupta, Zhitao Ying</p>
<p><a href='https://openreview.net/forum?id=GJtP1ZEzua'>https://openreview.net/forum?id=GJtP1ZEzua</a></p>
<p><b>Keywords</b>: Explainability, Graph Neural Network, Diffusion Model
</p><p><b>Compressor summary</b>: D4Explainer is a novel approach for generating reliable and diverse explanations for Graph Neural Networks by learning graph distributions that conform to the in-distribution property.</p><hr><h3>Estimating Koopman operators with sketching to provably learn large scale dynamical systems</h3>
<p>Giacomo Meanti, Antoine Chatalic, Vladimir R Kostic, Pietro Novelli, massimiliano pontil, Lorenzo Rosasco</p>
<p><a href='https://openreview.net/forum?id=GItLpB1vhK'>https://openreview.net/forum?id=GItLpB1vhK</a></p>
<p><b>Keywords</b>: dynamical systems, kernel methods, koopman operator, sketching, molecular dynamics, efficient machine learning
</p><p><b>Compressor summary</b>: The paper presents new methods to learn complex dynamics from data using Koopman operators and kernel methods, making them more efficient with random projections.</p><hr><h3>Two-Stage Learning to Defer with Multiple Experts</h3>
<p>Anqi Mao, Christopher Mohri, Mehryar Mohri, Yutao Zhong</p>
<p><a href='https://openreview.net/forum?id=GIlsH0T4b2'>https://openreview.net/forum?id=GIlsH0T4b2</a></p>
<p><b>Keywords</b>: learning to defer, learning theory
</p><p><b>Compressor summary</b>: The authors propose a new method to learn from multiple experts in two stages, using surrogate loss functions that ensure consistency and perform well in experiments.</p><hr><h3>Machine learning detects terminal singularities</h3>
<p>Tom Coates, Alexander M. Kasprzyk, Sara Veneziale</p>
<p><a href='https://openreview.net/forum?id=GI4Pp01prW'>https://openreview.net/forum?id=GI4Pp01prW</a></p>
<p><b>Keywords</b>: mathematics, geometry, Fano varieties, terminal singularities, theorem discovery, neural network classifier, supervised learning
</p><p><b>Compressor summary</b>: The paper uses machine learning to classify eight-dimensional positively curved algebraic varieties with toric symmetry and Picard rank two, revealing the landscape of Q-Fano varieties and providing new evidence for machine learning's role in mathematical discovery.</p><hr><h3>High dimensional, tabular deep learning with an auxiliary knowledge graph</h3>
<p>Camilo Ruiz, Hongyu Ren, Kexin Huang, Jure Leskovec</p>
<p><a href='https://openreview.net/forum?id=GGylthmehy'>https://openreview.net/forum?id=GGylthmehy</a></p>
<p><b>Keywords</b>: Tabular Data, Deep Learning, Knowledge Graph, Regularization
</p><p><b>Compressor summary</b>: PLATO is a method that improves machine learning performance on tabular data with high dimensions and low samples by using a heterogeneous knowledge graph as an auxiliary source of information to regularize a multilayer perceptron.</p><hr><h3>Domain Adaptive Imitation Learning with Visual Observation</h3>
<p>Sungho Choi, Seungyul Han, Woojun Kim, Jongseong Chae, Whiyoung Jung, Youngchul Sung</p>
<p><a href='https://openreview.net/forum?id=GGbBXSkX3r'>https://openreview.net/forum?id=GGbBXSkX3r</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Deep Reinforcement Learning, Imitation Learning
</p><p><b>Compressor summary</b>: The paper presents a new method for teaching a robot to perform a task by observing other robots, even if they look different or are viewed from different angles.</p><hr><h3>CORNN: Convex optimization of recurrent neural networks for rapid inference of neural dynamics</h3>
<p>Fatih Dinc, Adam Shai, Mark Schnitzer, Hidenori Tanaka</p>
<p><a href='https://openreview.net/forum?id=GGIA1p9fDT'>https://openreview.net/forum?id=GGIA1p9fDT</a></p>
<p><b>Keywords</b>: brain-machine interfaces, recurrent neural networks, convex optimization, computational neuroscience
</p><p><b>Compressor summary</b>: CORNN is a new training method for recurrent neural networks that enables faster and more accurate modeling of large neural datasets, potentially enabling real-time analysis and control of animal behavior.</p><hr><h3>iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models</h3>
<p>Tianyu Chen, Kevin Bello, Bryon Aragam, Pradeep Kumar Ravikumar</p>
<p><a href='https://openreview.net/forum?id=GEtXhqKW6X'>https://openreview.net/forum?id=GEtXhqKW6X</a></p>
<p><b>Keywords</b>: distribution shifts, heterogeneous data, feature-shift, structural causal models, additive noise models, causality
</p><p><b>Compressor summary</b>: This paper proposes a method for identifying causal mechanism shifts in related datasets without estimating their full causal structure, assuming nonlinear additive noise models.</p><hr><h3>Transition-constant Normalization for Image Enhancement</h3>
<p>Jie Huang, Man Zhou, JingHao Zhang, Gang Yang, Mingde Yao, Chongyi Li, Zhiwei Xiong, Feng Zhao</p>
<p><a href='https://openreview.net/forum?id=GEWzHeHpLr'>https://openreview.net/forum?id=GEWzHeHpLr</a></p>
<p><b>Keywords</b>: Image Enhancement, Normalization, Image Restoration
</p><p><b>Compressor summary</b>: The authors propose a novel normalization technique, Transition-Constant Normalization (TCN), for various image enhancement tasks that has several advantages and shows performance improvements in multiple experiments.</p><hr><h3>$L_2$-Uniform Stability of Randomized Learning Algorithms: Sharper Generalization Bounds and Confidence Boosting</h3>
<p>Xiaotong Yuan, Ping Li</p>
<p><a href='https://openreview.net/forum?id=GEQZ52oqxa'>https://openreview.net/forum?id=GEQZ52oqxa</a></p>
<p><b>Keywords</b>: Uniform stability, Randomized learning algorithms, Confidence boosting, Generalization bounds, Stochastic gradient methods
</p><p><b>Compressor summary</b>: The paper introduces a new concept of uniform stability and proves strong exponential bounds on generalization error, leading to near-optimal results for randomized learning algorithms like SGD.</p><hr><h3>Smooth Flipping Probability for Differential Private Sign Random Projection Methods</h3>
<p>Ping Li, Xiaoyun Li</p>
<p><a href='https://openreview.net/forum?id=GEMHw2sd9S'>https://openreview.net/forum?id=GEMHw2sd9S</a></p>
<p><b>Keywords</b>: Differential Privacy, Random Projection
</p><p><b>Compressor summary</b>: The text introduces new differential privacy algorithms using random projection and sign random projection methods, improving performance and utility for data protection in machine learning and search applications.</p><hr><h3>Facing Off World Model Backbones: RNNs, Transformers, and S4</h3>
<p>Fei Deng, Junyeong Park, Sungjin Ahn</p>
<p><a href='https://openreview.net/forum?id=GDYuzX0rwj'>https://openreview.net/forum?id=GDYuzX0rwj</a></p>
<p><b>Keywords</b>: world models, structured state space sequence models, S4, long-term memory, model-based reinforcement learning
</p><p><b>Compressor summary</b>: This paper explores alternative world model backbones for improving long-term memory in model-based reinforcement learning, comparing Transformers, Structured State Space Sequence models, and recurrent neural networks across various tasks.</p><hr><h3>TaskMet: Task-driven Metric Learning for Model Learning</h3>
<p>Dishank Bansal, Ricky T. Q. Chen, Mustafa Mukadam, Brandon Amos</p>
<p><a href='https://openreview.net/forum?id=GCY9C43A4L'>https://openreview.net/forum?id=GCY9C43A4L</a></p>
<p><b>Keywords</b>: task-based learning, decision-focused learning
</p><p><b>Compressor summary</b>: The proposed method improves deep learning models' performance on downstream tasks by using a task loss to learn a metric that emphasizes important information for the desired task without altering the original prediction model.</p><hr><h3>KAKURENBO: Adaptively Hiding Samples in Deep Neural Network Training</h3>
<p>Thao Nguyen Truong, Balazs Gerofi, Edgar Josafat Martinez-Noriega, François Trahay, Mohamed Wahib</p>
<p><a href='https://openreview.net/forum?id=GAsRl2ElHk'>https://openreview.net/forum?id=GAsRl2ElHk</a></p>
<p><b>Keywords</b>: Deep learning, Sampling
</p><p><b>Compressor summary</b>: The paper presents a method to hide less important samples in deep neural network training to save time without much accuracy loss.</p><hr><h3>Guide Your Agent with Adaptive Multimodal Rewards</h3>
<p>Changyeon Kim, Younggyo Seo, Hao Liu, Lisa Lee, Jinwoo Shin, Honglak Lee, Kimin Lee</p>
<p><a href='https://openreview.net/forum?id=G8nal7MpIQ'>https://openreview.net/forum?id=G8nal7MpIQ</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Multimodal Representation, Imitation Learning
</p><p><b>Compressor summary</b>: The paper proposes ARP, an imitation learning framework that uses multimodal rewards based on natural language descriptions and visual observations to enhance generalization and adapt to unseen environments.</p><hr><h3>On the Pareto Front of Multilingual Neural Machine Translation</h3>
<p>Liang Chen, Shuming Ma, Dongdong Zhang, Furu Wei, Baobao Chang</p>
<p><a href='https://openreview.net/forum?id=G7sQlfTzmY'>https://openreview.net/forum?id=G7sQlfTzmY</a></p>
<p><b>Keywords</b>: Multilinugal Neural Machine Translation, Multitask Learning, Pareto Optimization
</p><p><b>Compressor summary</b>: The paper investigates how different translation directions perform in multilingual neural machine translation, proposes a method to predict and optimize their performance trade-offs, and shows that it improves over existing methods.</p><hr><h3>CSLP-AE: A Contrastive Split-Latent Permutation Autoencoder Framework for Zero-Shot Electroencephalography Signal Conversion</h3>
<p>Anders Vestergaard Nørskov, Alexander Neergaard Zahid, Morten Mørup</p>
<p><a href='https://openreview.net/forum?id=G7Y145tm2F'>https://openreview.net/forum?id=G7Y145tm2F</a></p>
<p><b>Keywords</b>: zero-shot conversion, representations learning, contrastive learning, electroencephalography, autoencoder, subject variability, permutation invariant training
</p><p><b>Compressor summary</b>: The authors propose a new method (CSLP-AE) to convert EEG signals between tasks and subjects by extracting latent representations that account for both content and style, inspired by voice conversion technologies.</p><hr><h3>Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation</h3>
<p>Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, Omer Levy</p>
<p><a href='https://openreview.net/forum?id=G5RwHpBUv0'>https://openreview.net/forum?id=G5RwHpBUv0</a></p>
<p><b>Keywords</b>: text-to-image, human-preferences, dataset
</p><p><b>Compressor summary</b>: The authors create a web app to collect user preferences for text-to-image generation and use it to build a large dataset (Pick-a-Pic) and a scoring function (PickScore) that predicts human preferences better than existing metrics and can improve existing models.</p><hr><h3>Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data</h3>
<p>Yiwen Kou, Zixiang Chen, Quanquan Gu</p>
<p><a href='https://openreview.net/forum?id=G560qr59Gi'>https://openreview.net/forum?id=G560qr59Gi</a></p>
<p><b>Keywords</b>: ReLU Neural Networks, Implicit Bias, Deep Learning Theory
</p><p><b>Compressor summary</b>: The paper investigates how gradient descent impacts the implicit bias of non-smooth neural networks, especially for leaky ReLU activation functions, and shows that it leads to stable rank and uniform normalized margin in some cases.</p><hr><h3>On the Sublinear Regret of GP-UCB</h3>
<p>Justin Whitehouse, Aaditya Ramdas, Steven Wu</p>
<p><a href='https://openreview.net/forum?id=G3aubF5Wnw'>https://openreview.net/forum?id=G3aubF5Wnw</a></p>
<p><b>Keywords</b>: Kernel Bandits, Online Learning, Self-Normalized Concentration, Online Regression
</p><p><b>Compressor summary</b>: The paper presents a new analysis of the popular Gaussian Process Upper Confidence Bound (GP-UCB) algorithm that shows it achieves nearly optimal regret and improves over previous results for the Matern kernel.</p><hr><h3>Evolutionary Neural Architecture Search for Transformer in Knowledge Tracing</h3>
<p>Shangshang Yang, Xiaoshan Yu, Ye Tian, Xueming Yan, Haiping Ma, Xingyi Zhang</p>
<p><a href='https://openreview.net/forum?id=G14N38AjpU'>https://openreview.net/forum?id=G14N38AjpU</a></p>
<p><b>Keywords</b>: Knowledge tracing, intelligent education, neural architecture search, Transformer
</p><p><b>Compressor summary</b>: The paper proposes a new approach for knowledge tracing that uses convolution operations in Transformers, automates input feature selection, and applies evolutionary neural architecture search to balance local and global context modelling.</p><hr><h3>A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains</h3>
<p>Minkyu Choi, Kuan Han, Xiaokai Wang, Yizhen Zhang, Zhongming Liu</p>
<p><a href='https://openreview.net/forum?id=Fy1S3v4UAk'>https://openreview.net/forum?id=Fy1S3v4UAk</a></p>
<p><b>Keywords</b>: brain-inspired AI, retina transformation, eye movements, deep neural networks
</p><p><b>Compressor summary</b>: The authors developed a dual-stream vision model inspired by human vision, which uses two CNN branches for spatial processing and object recognition, and found that it matches the dorsal and ventral pathways of the visual cortex in terms of functional alignment.</p><hr><h3>Neural Image Compression: Generalization, Robustness, and Spectral Biases</h3>
<p>Kelsey Lieberman, James Diffenderfer, Charles Godfrey, Bhavya Kailkhura</p>
<p><a href='https://openreview.net/forum?id=FxRfAIj4s2'>https://openreview.net/forum?id=FxRfAIj4s2</a></p>
<p><b>Keywords</b>: image compression, robustness, generalization
</p><p><b>Compressor summary</b>: The paper introduces a benchmark suite, inspection tools, and theoretical analysis to evaluate the out-of-distribution performance of neural image compression methods for real-world applications.</p><hr><h3>Data Quality in Imitation Learning</h3>
<p>Suneel Belkhale, Yuchen Cui, Dorsa Sadigh</p>
<p><a href='https://openreview.net/forum?id=FwmvbuDiMk'>https://openreview.net/forum?id=FwmvbuDiMk</a></p>
<p><b>Keywords</b>: Imitation Learning, Robotics, Data Quality
</p><p><b>Compressor summary</b>: The paper proposes two properties for defining high quality datasets in imitation learning: action divergence and transition diversity, which help the learned policy to stay in distribution at test time.</p><hr><h3>On Differentially Private Sampling from Gaussian and Product Distributions</h3>
<p>Badih Ghazi, Xiao Hu, Ravi Kumar, Pasin Manurangsi</p>
<p><a href='https://openreview.net/forum?id=FviF8vuz5B'>https://openreview.net/forum?id=FviF8vuz5B</a></p>
<p><b>Keywords</b>: privacy, sampling, Gaussian distribution, product distributions
</p><p><b>Compressor summary</b>: The paper proposes new differentially private algorithms for generating samples from multi-dimensional Gaussian distributions with different covariance assumptions and improves the performance in some settings.</p><hr><h3>Balancing memorization and generalization in RNNs for high performance brain-machine Interfaces</h3>
<p>Joseph T Costello, Hisham Temmar, Luis H Cubillos, Matthew J Mender, Dylan M Wallace, Matthew S Willsey, Parag G Patil, Cynthia Chestek</p>
<p><a href='https://openreview.net/forum?id=FujJO3dsNj'>https://openreview.net/forum?id=FujJO3dsNj</a></p>
<p><b>Keywords</b>: brain computer interface, brain machine interface, neural decoding, prosthetic control, recurrent neural network, RNN, transformer, real time, closed-loop, user interface
</p><p><b>Compressor summary</b>: The study evaluated recurrent neural networks (RNNs) for real-time brain-machine interface control, showing improved performance over other architectures in decoding finger movements from primate neural signals.</p><hr><h3>Dynamic Pricing and Learning with Bayesian Persuasion</h3>
<p>Shipra Agrawal, Yiding Feng, Wei Tang</p>
<p><a href='https://openreview.net/forum?id=FtZ7lUwH99'>https://openreview.net/forum?id=FtZ7lUwH99</a></p>
<p><b>Keywords</b>: dynamic pricing, information design, regret minimization
</p><p><b>Compressor summary</b>: The paper proposes an algorithm that learns optimal pricing and advertising strategies for a seller in a dynamic setting with linear buyer valuations.</p><hr><h3>Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach</h3>
<p>Fabian Zaiser, Andrzej S Murawski, Luke Ong</p>
<p><a href='https://openreview.net/forum?id=FtNruwFEs3'>https://openreview.net/forum?id=FtNruwFEs3</a></p>
<p><b>Keywords</b>: Bayesian statistics, probabliistic programming, exact inference, discrete models, probability generating functions
</p><p><b>Compressor summary</b>: The authors present an exact Bayesian inference method for discrete models using a probabilistic programming language and probability generating functions, which they implement in a tool called Genfer that outperforms existing tools on many problems.</p><hr><h3>RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization</h3>
<p>Siqi Shen, Chennan Ma, Chao Li, Weiquan Liu, Yongquan Fu, Songzhu Mei, Xinwang Liu, Cheng Wang</p>
<p><a href='https://openreview.net/forum?id=FskZtRvMJI'>https://openreview.net/forum?id=FskZtRvMJI</a></p>
<p><b>Keywords</b>: multi-agent reinforcement learning, value factorization, individual global max, risk-sensitive
</p><p><b>Compressor summary</b>: RIGM principle introduces a new approach to coordinate risk-sensitive MARL policies, and RiskQ is a value factorization method that satisfies the RIGM principle for various risk metrics.</p><hr><h3>Differentiable Blocks World: Qualitative 3D Decomposition by Rendering Primitives</h3>
<p>Tom Monnier, Jake Austin, Angjoo Kanazawa, Alexei A Efros, Mathieu Aubry</p>
<p><a href='https://openreview.net/forum?id=FsQWxU5TOL'>https://openreview.net/forum?id=FsQWxU5TOL</a></p>
<p><b>Keywords</b>: 3D decomposition, 3D reconstruction, MVS, primitives, qualitative 3D
</p><p><b>Compressor summary</b>: The authors present a method to generate simple 3D world representations from images using textured superquadric meshes, which model transparency and handle varying numbers of primitives. The method operates on images through differentiable rendering and surpasses existing approaches on various scenes.</p><hr><h3>Faster approximate subgraph counts with privacy</h3>
<p>Dung Nguyen, Mahantesh M Halappanavar, Venkatesh Srinivasan, Anil Vullikanti</p>
<p><a href='https://openreview.net/forum?id=Fqg9vGWy4k'>https://openreview.net/forum?id=Fqg9vGWy4k</a></p>
<p><b>Keywords</b>: differential privacy, subgraph counting, smooth sensitivity, local sensitivity
</p><p><b>Compressor summary</b>: The paper proposes new algorithms for privately counting subgraphs in graph data, using approximated sensitivity metrics and reducing noise.</p><hr><h3>3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive Physics under Challenging Scenes</h3>
<p>Haotian Xue, Antonio Torralba, Joshua B. Tenenbaum, Daniel LK Yamins, Yunzhu Li, Hsiao-Yu Tung</p>
<p><a href='https://openreview.net/forum?id=Fp5uC6YHwe'>https://openreview.net/forum?id=Fp5uC6YHwe</a></p>
<p><b>Keywords</b>: Intuitive Physics, Computer Vision
</p><p><b>Compressor summary</b>: The paper presents a framework to learn 3D visual intuitive physics models from videos of complex scenes with fluids using a conditional NeRF-style visual frontend and a point-based dynamics prediction backend, which can handle challenging scenarios without relying on dense point trajectory supervision.</p><hr><h3>NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and Repulsive UDF</h3>
<p>Stefan Lionar, Xiangyu Xu, Min Lin, Gim Hee Lee</p>
<p><a href='https://openreview.net/forum?id=FmpH0CYWiX'>https://openreview.net/forum?id=FmpH0CYWiX</a></p>
<p><b>Keywords</b>: single-view 3d reconstruction, neural fields, 3d reconstruction
</p><p><b>Compressor summary</b>: NU-MCC is a new approach for single-view 3D reconstruction that improves efficiency and detail recovery by using a neighborhood decoder and a repulsive unsigned distance function, achieving state-of-the-art results on the CO3D-v2 dataset.</p><hr><h3>Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms</h3>
<p>Alexander Bukharin, Yan Li, Yue Yu, Qingru Zhang, Zhehui Chen, Simiao Zuo, Chao Zhang, Songan Zhang, Tuo Zhao</p>
<p><a href='https://openreview.net/forum?id=FmZVRe0gn8'>https://openreview.net/forum?id=FmZVRe0gn8</a></p>
<p><b>Keywords</b>: Multi-Agent Reinforcement Learning, Theory of Robust Reinforcement Learning, Adversarial Regularization
</p><p><b>Compressor summary</b>: The paper introduces ERNIE, a robust multi-agent reinforcement learning framework that uses adversarial regularization to control policy's Lipschitz constant and improve stability against environmental changes.</p><hr><h3>Entropy-dissipation Informed Neural Network for McKean-Vlasov Type PDEs</h3>
<p>Zebang Shen, Zhenfu Wang</p>
<p><a href='https://openreview.net/forum?id=FkpMm9avyP'>https://openreview.net/forum?id=FkpMm9avyP</a></p>
<p><b>Keywords</b>: Entropy-dissipation, McKean-Vlasov, Navier-Stokes, PDE, Coulomb, singular interaction
</p><p><b>Compressor summary</b>: The paper proposes a new neural network-based method to solve McKean-Vlasov equations involving singular interactions, using entropy dissipation as a control criterion.</p><hr><h3>Faith and Fate: Limits of Transformers on Compositionality</h3>
<p>Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, Yejin Choi</p>
<p><a href='https://openreview.net/forum?id=Fkckkr3ya8'>https://openreview.net/forum?id=Fkckkr3ya8</a></p>
<p><b>Keywords</b>: Natural language processing, large language models, multi-step reasoning
</p><p><b>Compressor summary</b>: The authors examine transformer LLMs' ability to perform complex tasks requiring multi-step reasoning and find that these models rely on subgraph matching instead of developing problem-solving skills, which may limit their performance in more challenging tasks.</p><hr><h3>Direct Preference-based Policy Optimization without Reward Modeling</h3>
<p>Gaon An, Junhyeok Lee, Xingdong Zuo, Norio Kosaka, Kyung-Min Kim, Hyun Oh Song</p>
<p><a href='https://openreview.net/forum?id=FkAwlqBuyO'>https://openreview.net/forum?id=FkAwlqBuyO</a></p>
<p><b>Keywords</b>: Preference-based reinforcement learning, Contrastive learning, Offline reinforcement learning, RLHF
</p><p><b>Compressor summary</b>: The paper proposes a preference-based reinforcement learning (PbRL) method that uses contrastive learning to directly learn from human preferences without requiring a reward model.</p><hr><h3>A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm</h3>
<p>Haizhou Shi, Hao Wang</p>
<p><a href='https://openreview.net/forum?id=FiClXlUqA7'>https://openreview.net/forum?id=FiClXlUqA7</a></p>
<p><b>Keywords</b>: Domain Incremental Learning, Continual Learning, Theory
</p><p><b>Compressor summary</b>: The paper proposes a unified framework for domain incremental learning that adapts to different domains with memory, improves generalization error bounds, and outperforms existing methods on various datasets.</p><hr><h3>A Metadata-Driven Approach to Understand Graph Neural Networks</h3>
<p>Ting Wei Li, Qiaozhu Mei, Jiaqi Ma</p>
<p><a href='https://openreview.net/forum?id=FgakGFpll1'>https://openreview.net/forum?id=FgakGFpll1</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Metadata-Driven Analysis, Gini Coefficient of Degree Distribution
</p><p><b>Compressor summary</b>: This paper proposes a metadata-driven approach to analyze how graph data properties affect the performance of Graph Neural Networks (GNNs) using regression analysis, theoretical analysis, and experiments.</p><hr><h3>Gradient-Based Feature Learning under Structured Data</h3>
<p>Alireza Mousavi-Hosseini, Denny Wu, Taiji Suzuki, Murat A Erdogdu</p>
<p><a href='https://openreview.net/forum?id=Fe8PxP2F2p'>https://openreview.net/forum?id=Fe8PxP2F2p</a></p>
<p><b>Keywords</b>: feature learning, neural networks, single-index model, gradient descent
</p><p><b>Compressor summary</b>: The paper studies how adding structure to the input data affects gradient-based learning of single index models and shows that appropriate normalization and exploiting the alignment between the input covariance and the target can improve performance.</p><hr><h3>On the Trade-off of Intra-/Inter-class Diversity for Supervised Pre-training</h3>
<p>Jieyu Zhang, Bohan Wang, Zhengyu Hu, Pang Wei Koh, Alexander Ratner</p>
<p><a href='https://openreview.net/forum?id=Fe6fDq65aZ'>https://openreview.net/forum?id=Fe6fDq65aZ</a></p>
<p><b>Keywords</b>: data-centric study, supervised pretraining, transfer learning
</p><p><b>Compressor summary</b>: The paper studies how the balance between intra-class and inter-class diversity in pre-training datasets affects downstream performance, and shows that the optimal class-to-sample ratio is independent of the pre-training dataset size.</p><hr><h3>Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning</h3>
<p>Yihang Yao, Zuxin Liu, Zhepeng Cen, Jiacheng Zhu, Wenhao Yu, Tingnan Zhang, Ding Zhao</p>
<p><a href='https://openreview.net/forum?id=FdtdjQpAwJ'>https://openreview.net/forum?id=FdtdjQpAwJ</a></p>
<p><b>Keywords</b>: Safe Reinforcement Learning, Conditioned Reinforcement Learning, Multi-task Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper proposes a new framework, CCPO, that learns safe policies in reinforcement learning that can adapt to changing safety constraints without retraining and achieve good performance.</p><hr><h3>PaintSeg: Painting Pixels for Training-free Segmentation</h3>
<p>Xiang Li, Chung-Ching Lin, Yinpeng Chen, Zicheng Liu, Jinglu Wang, Rita Singh, Bhiksha Raj</p>
<p><a href='https://openreview.net/forum?id=FdsS51iif3'>https://openreview.net/forum?id=FdsS51iif3</a></p>
<p><b>Keywords</b>: Prompt-guided Segmentation, Generative models, Training-free
</p><p><b>Compressor summary</b>: PaintSeg is an unsupervised method that uses adversarial masked contrastive painting to create contrast between original and painted images, advancing the target segmentation mask towards ground truth without supervision or training.</p><hr><h3>Mnemosyne: Learning to Train Transformers with Transformers</h3>
<p>Deepali Jain, Krzysztof Marcin Choromanski, Kumar Avinava Dubey, Sumeet Singh, Vikas Sindhwani, Tingnan Zhang, Jie Tan</p>
<p><a href='https://openreview.net/forum?id=Fdfyga5i0A'>https://openreview.net/forum?id=Fdfyga5i0A</a></p>
<p><b>Keywords</b>: learnable optimizers, Transformers, efficient attention, spatio-temporal attention
</p><p><b>Compressor summary</b>: Mnemosyne is a new learnable optimizer that can train neural networks without task-specific tuning, performs well compared to popular LSTM optimizers, and has space complexity comparable to hand-designed optimizers.</p><hr><h3>Towards In-context Scene Understanding</h3>
<p>Ivana Balazevic, David Steiner, Nikhil Parthasarathy, Relja Arandjelovic, Olivier J Henaff</p>
<p><a href='https://openreview.net/forum?id=FasIQqsJhe'>https://openreview.net/forum?id=FasIQqsJhe</a></p>
<p><b>Keywords</b>: transfer learning, adaptation, self-supervised learning, contrastive learning, scene understanding, representation learning, in-context learning, vision transformers
</p><p><b>Compressor summary</b>: The paragraph discusses a new approach called Hummingbird, which enables in-context learning for computer vision tasks such as semantic segmentation and depth estimation using nearest neighbor retrieval and attention-based pretraining.</p><hr><h3>StyleGAN knows Normal, Depth, Albedo, and More</h3>
<p>Anand Bhattad, Daniel McKee, Derek Hoiem, David Forsyth</p>
<p><a href='https://openreview.net/forum?id=FYqqvQdXhZ'>https://openreview.net/forum?id=FYqqvQdXhZ</a></p>
<p><b>Keywords</b>: Generative models, StyleGAN, Depth, Normals, Segmentation, Intrinsic Images, Albedo, Shading
</p><p><b>Compressor summary</b>: This paper shows how StyleGAN can generate different types of intrinsic images easily and demonstrates its advantages over existing methods in terms of quality, robustness, and efficiency.</p><hr><h3>Episodic Multi-Task Learning with Heterogeneous Neural Processes</h3>
<p>Jiayi Shen, Xiantong Zhen, Cheems Wang, Marcel Worring</p>
<p><a href='https://openreview.net/forum?id=FXU4aR2uif'>https://openreview.net/forum?id=FXU4aR2uif</a></p>
<p><b>Keywords</b>: data-insufficiency problem, episodic training, multi-task learning and neural processes
</p><p><b>Compressor summary</b>: The paper introduces HNPs, a method that uses meta-knowledge, heterogeneous information, and task-relatedness to adapt to novel tasks in episodic multi-task learning with limited data, and shows its superior performance over baselines.</p><hr><h3>Non-stationary Experimental Design under Linear Trends</h3>
<p>David Simchi-Levi, Chonghuan Wang, Zeyu Zheng</p>
<p><a href='https://openreview.net/forum?id=FV4ngfUlY0'>https://openreview.net/forum?id=FV4ngfUlY0</a></p>
<p><b>Keywords</b>: Adaptive Experimental Design, Non-stationary, Online Learning, Treatment Effect
</p><p><b>Compressor summary</b>: The paper proposes an efficient design for non-stationary experiments with linear trends, aiming to estimate the dynamic treatment effect and minimize welfare loss, while highlighting the challenge and trade-off between these objectives.</p><hr><h3>Token-Scaled Logit Distillation for Ternary Weight Generative Language Models</h3>
<p>Minsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong Chang, Wonyong Sung, Jungwook Choi</p>
<p><a href='https://openreview.net/forum?id=FUnEkOkodU'>https://openreview.net/forum?id=FUnEkOkodU</a></p>
<p><b>Keywords</b>: Generative Language Model, Quantization, QAT, Knowledge Distillation, Causal Attention, Language Modeling
</p><p><b>Compressor summary</b>: The paper proposes a new method for training large generative language models with less memory and maintaining accuracy in various tasks.</p><hr><h3>Markovian Sliced Wasserstein Distances: Beyond Independent Projections</h3>
<p>Khai Nguyen, Tongzheng Ren, Nhat Ho</p>
<p><a href='https://openreview.net/forum?id=FT2q2B4cKZ'>https://openreview.net/forum?id=FT2q2B4cKZ</a></p>
<p><b>Keywords</b>: Sliced Wasserstein, Generative Models, Optimal Transport
</p><p><b>Compressor summary</b>: The Markovian sliced Wasserstein (MSW) distance is a new family of sliced Wasserstein distances that imposes a first-order Markov structure on projecting directions, improving metricity, computational efficiency, and effectiveness over previous methods.</p><hr><h3>Streaming PCA for Markovian Data</h3>
<p>Syamantak Kumar, Purnamrita Sarkar</p>
<p><a href='https://openreview.net/forum?id=FQGRkwmRzm'>https://openreview.net/forum?id=FQGRkwmRzm</a></p>
<p><b>Keywords</b>: Streaming PCA, Markov Chain, Mixing, Oja's algorithm
</p><p><b>Compressor summary</b>: The paper proposes a new method to estimate the top eigenvector of a covariance matrix using Oja's algorithm on data streams sampled from a Markov chain without downsampling.</p><hr><h3>BCDiff: Bidirectional Consistent Diffusion for Instantaneous Trajectory Prediction</h3>
<p>Rongqing Li, Changsheng Li, Dongchun Ren, Guangyi Chen, Ye Yuan, Guoren Wang</p>
<p><a href='https://openreview.net/forum?id=FOFJmR1oxt'>https://openreview.net/forum?id=FOFJmR1oxt</a></p>
<p><b>Keywords</b>: Trajectory prediction, instantaneous observation
</p><p><b>Compressor summary</b>: BCDiff is a novel framework for predicting pedestrian trajectories from instantaneous observations, using two coupled diffusion models that mutually guide each other to generate more accurate predictions.</p><hr><h3>Hyperbolic VAE via Latent Gaussian Distributions</h3>
<p>Seunghyuk Cho, Juyong Lee, Dongwoo Kim</p>
<p><a href='https://openreview.net/forum?id=FNn4zibGvw'>https://openreview.net/forum?id=FNn4zibGvw</a></p>
<p><b>Keywords</b>: Hyperbolic space, VAE, Distribution on hyperbolic space, Hierarchical representation learning, Reinforcement Learning
</p><p><b>Compressor summary</b>: The GM-VAE is a new model that uses Gaussian distributions to represent data in a latent space shaped like a hyperbolic space, which improves density estimation and reinforcement learning tasks.</p><hr><h3>Core-sets for Fair and Diverse Data Summarization</h3>
<p>Sepideh Mahabadi, Stojan Trajanovski</p>
<p><a href='https://openreview.net/forum?id=FM8thAWqiO'>https://openreview.net/forum?id=FM8thAWqiO</a></p>
<p><b>Keywords</b>: Constrained Diversity Maximization, Fairness, Data Summarization, Core-sets, Approximation Algorithms
</p><p><b>Compressor summary</b>: The paragraph discusses algorithms that maximize diversity in partitioned sets under fairness constraints and shows their effectiveness in summarizing timed messages for a communication platform.</p><hr><h3>FedL2P: Federated Learning to Personalize</h3>
<p>Royson Lee, Minyoung Kim, Da Li, Xinchi Qiu, Timothy Hospedales, Ferenc Huszár, Nicholas Donald Lane</p>
<p><a href='https://openreview.net/forum?id=FM81CI68Iz'>https://openreview.net/forum?id=FM81CI68Iz</a></p>
<p><b>Keywords</b>: federated learning; meta-learning; hyperparameter optimization
</p><p><b>Compressor summary</b>: The paper proposes a federated meta-learning approach to learn personalization strategies for different clients in federated learning problems, using meta-nets to adjust batch-norm and learning rate parameters based on local data statistics.</p><hr><h3>Scaling Riemannian Diffusion Models</h3>
<p>Aaron Lou, Minkai Xu, Adam Farris, Stefano Ermon</p>
<p><a href='https://openreview.net/forum?id=FLTg8uA5xI'>https://openreview.net/forum?id=FLTg8uA5xI</a></p>
<p><b>Keywords</b>: Diffusion Models, Geometric Deep Learning, Manifolds, Numerical Algorithms
</p><p><b>Compressor summary</b>: The authors improve Riemannian diffusion models by leveraging symmetry and computationally efficient approximations, enabling their application to high dimensional tasks and reducing representation collapse.</p><hr><h3>Meta-learning families of plasticity rules in recurrent spiking networks using simulation-based inference</h3>
<p>Basile Confavreux, Poornima Ramesh, Pedro J. Goncalves, Jakob H. Macke, Tim P. Vogels</p>
<p><a href='https://openreview.net/forum?id=FLFasCFJNo'>https://openreview.net/forum?id=FLFasCFJNo</a></p>
<p><b>Keywords</b>: synaptic plasticity, spiking network, meta-learning, computational neuroscience
</p><p><b>Compressor summary</b>: The authors develop a simulation-based inference method (filter SBI) that allows them to infer complex and co-active plasticity rules in spiking networks, which can be used for deeper insights into brain function.</p><hr><h3>Pseudo-Likelihood Inference</h3>
<p>Theo Gruner, Boris Belousov, Fabio Muratore, Daniel Palenicek, Jan Peters</p>
<p><a href='https://openreview.net/forum?id=FIv84qGPFT'>https://openreview.net/forum?id=FIv84qGPFT</a></p>
<p><b>Keywords</b>: simulation-based inference, approximate Bayesian computation
</p><p><b>Compressor summary</b>: Pseudo-Likelihood Inference (PLI) is a new method that combines neural approximation with Approximate Bayesian Computation (ABC) to perform better on high-dimensional Bayesian system identification tasks, especially when more data is available.</p><hr><h3>On the spectral bias of two-layer linear networks</h3>
<p>Aditya Vardhan Varre, Maria-Luiza Vladarean, Loucas Pillaud-Vivien, Nicolas Flammarion</p>
<p><a href='https://openreview.net/forum?id=FFdrXkm3Cz'>https://openreview.net/forum?id=FFdrXkm3Cz</a></p>
<p><b>Keywords</b>: linear networks, spectral bias, low rank, singular values, mirror flow
</p><p><b>Compressor summary</b>: The paper analyzes how two-layer neural networks with linear activations and gradient flow training behave differently depending on initialization scale, revealing that low-rank structures arise in small scale regimes.</p><hr><h3>C-Disentanglement: Discovering Causally-Independent Generative Factors under  an Inductive Bias of Confounder</h3>
<p>Xiaoyu Liu, Jiaxin Yuan, Bang An, Yuancheng Xu, Yifan Yang, Furong Huang</p>
<p><a href='https://openreview.net/forum?id=FFOYWUpBca'>https://openreview.net/forum?id=FFOYWUpBca</a></p>
<p><b>Keywords</b>: causal disentanglement, causal generative process, generative factors, confounder, inductive bias, disentanglement, causal inference
</p><p><b>Compressor summary</b>: The paper proposes C-Disentanglement, a framework that incorporates expert knowledge to identify causally disentangled factors in latent space, improving data generation and generalization.</p><hr><h3>Squared Neural Families: A New Class of Tractable Density Models</h3>
<p>Russell Tsuchida, Cheng Soon Ong, Dino Sejdinovic</p>
<p><a href='https://openreview.net/forum?id=FDzQQTPqEJ'>https://openreview.net/forum?id=FDzQQTPqEJ</a></p>
<p><b>Keywords</b>: probabilistic modelling; density estimation; exponential family;
</p><p><b>Compressor summary</b>: SNEFYs are a new type of probability distributions that can model complex data with flexible and tractable density models, and are applicable to many machine learning problems.</p><hr><h3>Anytime-Competitive Reinforcement Learning with Policy Prior</h3>
<p>Jianyi Yang, Pengfei Li, Tongxin Li, Adam Wierman, Shaolei Ren</p>
<p><a href='https://openreview.net/forum?id=FCwfZj1bQl'>https://openreview.net/forum?id=FCwfZj1bQl</a></p>
<p><b>Keywords</b>: Markov Decision Process, Constrained Reinforcement Learning, Anytime Competitive Constraints
</p><p><b>Compressor summary</b>: The paper introduces ACRL, a new algorithm for A-CMDP that guarantees cost bounds and optimizes expected reward, with experiments showing its effectiveness in carbon-intelligent computing.</p><hr><h3>Directed Cyclic Graph for Causal Discovery from Multivariate Functional Data</h3>
<p>Saptarshi Roy, Raymond K. W. Wong, Yang Ni</p>
<p><a href='https://openreview.net/forum?id=FCwF5431IY'>https://openreview.net/forum?id=FCwF5431IY</a></p>
<p><b>Keywords</b>: Causal Embedding, Causal Discovery, Multivariate Functional Data, Directed Cyclic Graph, Causal Structure Learning, Bayesian Inference
</p><p><b>Compressor summary</b>: The article presents a functional linear structural equation model for learning causal structures from multivariate functional data, which preserves causal information in a low-dimensional space and can be inferred using a Bayesian framework.</p><hr><h3>Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections</h3>
<p>Talia Konkle, George A. Alvarez</p>
<p><a href='https://openreview.net/forum?id=FCIj5KMn2m'>https://openreview.net/forum?id=FCIj5KMn2m</a></p>
<p><b>Keywords</b>: convolutional neural networks, steerability, computer vision
</p><p><b>Compressor summary</b>: The paragraph discusses how adding long-range modulatory pathways to vision models can improve image recognition, robustness, and alignment, as well as allow for goal-directed visual encoding using output space vectors.</p><hr><h3>Variational Monte Carlo on a Budget — Fine-tuning pre-trained Neural Wavefunctions</h3>
<p>Michael Scherbela, Leon Gerard, Philipp Grohs</p>
<p><a href='https://openreview.net/forum?id=FBNyccPfAu'>https://openreview.net/forum?id=FBNyccPfAu</a></p>
<p><b>Keywords</b>: Computational Physics, Machine Learning for Science, Quantum Monte Carlo, Fermionic Neural Networks
</p><p><b>Compressor summary</b>: The authors propose a deep learning-based method for quantum chemistry that uses pre-training and fine-tuning to achieve high accuracy with less computation than conventional methods.</p><hr><h3>A Privacy-Friendly Approach to Data Valuation</h3>
<p>Jiachen T. Wang, Yuqing Zhu, Yu-Xiang Wang, Ruoxi Jia, Prateek Mittal</p>
<p><a href='https://openreview.net/forum?id=FAZ3i0hvm0'>https://openreview.net/forum?id=FAZ3i0hvm0</a></p>
<p><b>Keywords</b>: Data Valuation, Differential Privacy
</p><p><b>Compressor summary</b>: The paper introduces TKNN-Shapley, a privacy-friendly variant of KNN-Shapley, which is a method for data valuation in machine learning models, and shows that it offers better trade-offs between privacy and utility than the original method.</p><hr><h3>Debiased and Denoised Entity Recognition from Distant Supervision</h3>
<p>Haobo Wang, Yiwen Dong, Ruixuan Xiao, Fei Huang, Gang Chen, Junbo Zhao</p>
<p><a href='https://openreview.net/forum?id=FAGY52HbyV'>https://openreview.net/forum?id=FAGY52HbyV</a></p>
<p><b>Keywords</b>: Distant Supervision; Named Entity-Recognition; Biased Learning
</p><p><b>Compressor summary</b>: DesERT is a novel self-training framework that addresses two types of biases in distant supervision for named entity recognition and achieves state-of-the-art performance on five benchmark datasets.</p><hr><h3>Toward Better PAC-Bayes Bounds for Uniformly Stable Algorithms</h3>
<p>Sijia Zhou, Yunwen Lei, Ata Kaban</p>
<p><a href='https://openreview.net/forum?id=F6j16Qr6Vk'>https://openreview.net/forum?id=F6j16Qr6Vk</a></p>
<p><b>Keywords</b>: PAC-Bayesian Bounds, Uniform Stability, Generalization Analysis
</p><p><b>Compressor summary</b>: The paper improves bounds for stable randomized algorithms using concentration of weakly dependent variables and introduces a sub-exponential stability parameter assumption, applicable to stochastic gradient descent and randomized coordinate descent.</p><hr><h3>Decision Tree for Locally Private Estimation with Public Data</h3>
<p>Yuheng Ma, Han Zhang, Yuchao Cai, Hanfang Yang</p>
<p><a href='https://openreview.net/forum?id=F5FVsfCxt8'>https://openreview.net/forum?id=F5FVsfCxt8</a></p>
<p><b>Keywords</b>: Local differential privacy, non-parametric regression, decision tree, public data
</p><p><b>Compressor summary</b>: The paper introduces a new algorithm called Locally differentially Private Decision Tree (LPDT) for enhancing private estimation using public data, which improves convergence rates and performance compared to existing methods.</p><hr><h3>GRAND-SLAMIN’ Interpretable Additive Modeling with Structural Constraints</h3>
<p>Shibal Ibrahim, Gabriel Isaac Afriat, Kayhan Behdin, Rahul Mazumder</p>
<p><a href='https://openreview.net/forum?id=F5DYsAc7Rt'>https://openreview.net/forum?id=F5DYsAc7Rt</a></p>
<p><b>Keywords</b>: Generalized additive models, component selection, hierarchy, interpretability
</p><p><b>Compressor summary</b>: The paper introduces GRAND-SLAMIN, a framework for learning generalized additive models (GAMs) with sparse interactions and structural constraints using differentiable optimization and novel prediction bounds.</p><hr><h3>Invariant Anomaly Detection under Distribution Shifts: A Causal Perspective</h3>
<p>João B. S. Carvalho, Mengtao Zhang, Robin Geyer, Carlos Cotrini, Joachim M. Buhmann</p>
<p><a href='https://openreview.net/forum?id=F1mv2L7Rkb'>https://openreview.net/forum?id=F1mv2L7Rkb</a></p>
<p><b>Keywords</b>: anomaly detection, causal inference, distribution shifts
</p><p><b>Compressor summary</b>: The paper proposes a new regularization term for anomaly detection that improves its performance under distribution shifts, using tools from causal inference and ensuring invariant representations.</p><hr><h3>GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces</h3>
<p>Josephine Lamp, Mark Derdzinski, Christopher Hannemann, Joost Van der Linden, Lu Feng, Tianhao Wang, David Evans</p>
<p><a href='https://openreview.net/forum?id=Eysb8t3MJ5'>https://openreview.net/forum?id=Eysb8t3MJ5</a></p>
<p><b>Keywords</b>: Synthetic Data, Time Series, Generative Adversarial Networks, Differential Privacy, Glucose, Diabetes
</p><p><b>Compressor summary</b>: The paper introduces GlucoSynth, a novel GAN framework that generates private and high-quality synthetic glucose traces by preserving the relationships among glucose events and temporal dynamics.</p><hr><h3>Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures</h3>
<p>Runa Eschenhagen, Alexander Immer, Richard E Turner, Frank Schneider, Philipp Hennig</p>
<p><a href='https://openreview.net/forum?id=Ex3oJEKS53'>https://openreview.net/forum?id=Ex3oJEKS53</a></p>
<p><b>Keywords</b>: deep learning, second-order, optimization, natural gradient, fisher, gauss-newton, k-fac, weight-sharing
</p><p><b>Compressor summary</b>: The paper proposes two flavors of K-FAC for linear weight-sharing layers in neural networks and shows that they can speed up training and reduce computational costs, especially for deep vision tasks.</p><hr><h3>Spiking PointNet: Spiking Neural Networks for Point Clouds</h3>
<p>Dayong Ren, Zhe Ma, Yuanpei Chen, Weihang Peng, Xiaode Liu, Yuhan Zhang, Yufei Guo</p>
<p><a href='https://openreview.net/forum?id=Ev2XuqvJCy'>https://openreview.net/forum?id=Ev2XuqvJCy</a></p>
<p><b>Keywords</b>: Spiking Neural Networks, Point Clouds
</p><p><b>Compressor summary</b>: The paper introduces Spiking PointNet, a spiking neural network model for efficient 3D point cloud recognition, overcoming the challenges of SNNs' optimization and PointNet's memory and computation cost.</p><hr><h3>OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding</h3>
<p>Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, Hao Su</p>
<p><a href='https://openreview.net/forum?id=Eu4Kkefq7p'>https://openreview.net/forum?id=Eu4Kkefq7p</a></p>
<p><b>Keywords</b>: 3d, shape understanding, open-world understanding, zero-shot 3D classification, vision-language model
</p><p><b>Compressor summary</b>: OpenShape is a method that learns joint representations of text, image, and point clouds to enable open-world 3D shape understanding and achieve superior zero-shot classification performance on several benchmarks.</p><hr><h3>Logarithmic Bayes Regret Bounds</h3>
<p>Alexia Atsidakou, Branislav Kveton, Sumeet Katariya, Constantine Caramanis, sujay sanghavi</p>
<p><a href='https://openreview.net/forum?id=ErAP8kF4tG'>https://openreview.net/forum?id=ErAP8kF4tG</a></p>
<p><b>Keywords</b>: Bayesian bandits, logarithmic regret bounds, multi-armed bandits, linear bandits
</p><p><b>Compressor summary</b>: The paper presents new upper bounds for finite-time logarithmic Bayes regret in Bayesian bandits and applies simple and general techniques to linear bandits.</p><hr><h3>Does Invariant Graph Learning via Environment Augmentation Learn Invariance?</h3>
<p>Yongqiang Chen, Yatao Bian, Kaiwen Zhou, Binghui Xie, Bo Han, James Cheng</p>
<p><a href='https://openreview.net/forum?id=EqpR9Vtt13'>https://openreview.net/forum?id=EqpR9Vtt13</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Out-of-Distribution Generalization, Invariant Learning
</p><p><b>Compressor summary</b>: The paper proposes a new framework, GALA, to learn invariant graph representations by incorporating an assistant model that detects environment changes and identifies maximally invariant subgraphs for out-of-distribution generalization.</p><hr><h3>Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning</h3>
<p>Arnaud Robert, Ciara Pike-Burke, Aldo A. Faisal</p>
<p><a href='https://openreview.net/forum?id=EqnZqrbFrc'>https://openreview.net/forum?id=EqnZqrbFrc</a></p>
<p><b>Keywords</b>: Hierarchical Reinforcement Learning, Sample Complexity
</p><p><b>Compressor summary</b>: The paper derives a lower bound on the sample complexity for goal-conditioned Hierarchical Reinforcement Learning (HRL) algorithms and proposes a new algorithm that leverages hierarchical decompositions, which is empirically validated on various tasks.</p><hr><h3>Unbounded Differentially Private Quantile and Maximum Estimation</h3>
<p>David Durfee</p>
<p><a href='https://openreview.net/forum?id=Eq9AFZlAjt'>https://openreview.net/forum?id=Eq9AFZlAjt</a></p>
<p><b>Keywords</b>: Differential privacy, Theory, Spars Vector Technique, Quantile
</p><p><b>Compressor summary</b>: The paper presents an efficient and accurate method for computing differential privacy using $\texttt{AboveThreshold}$ subroutine on unbounded data with improved privacy guarantees.</p><hr><h3>Reconciling Competing Sampling Strategies of Network Embedding</h3>
<p>Yuchen Yan, Baoyu Jing, Lihui Liu, Ruijie Wang, Jinning Li, Tarek Abdelzaher, Hanghang Tong</p>
<p><a href='https://openreview.net/forum?id=EoDpq18R30'>https://openreview.net/forum?id=EoDpq18R30</a></p>
<p><b>Keywords</b>: Network embedding
</p><p><b>Compressor summary</b>: The paper studies the trade-off between discrimination and monotonicity properties for node embeddings under different sampling strategies and proposes a new model (SENSEI) that improves network embedding performance.</p><hr><h3>Transfer learning for atomistic simulations using GNNs and kernel mean embeddings</h3>
<p>John Isak Texas Falk, Luigi Bonati, Pietro Novelli, Michele Parrinello, massimiliano pontil</p>
<p><a href='https://openreview.net/forum?id=Enzew8XujO'>https://openreview.net/forum?id=Enzew8XujO</a></p>
<p><b>Keywords</b>: GNN, Mean Embedding, Kernels, Atomistic Simulations, OCP, Transfer Learning, Molecular Dynamics, Kernel Ridge Regression, Neural Networks
</p><p><b>Compressor summary</b>: The authors propose a method to learn interatomic potentials using graph neural networks and kernel mean embeddings, which improves accuracy and interpretability for atomistic simulations of catalytic processes.</p><hr><h3>Honesty Is the Best Policy: Defining and Mitigating AI Deception</h3>
<p>Francis Rhys Ward, Francesca Toni, Francesco Belardinelli, Tom Everitt</p>
<p><a href='https://openreview.net/forum?id=EmxpDiPgRu'>https://openreview.net/forum?id=EmxpDiPgRu</a></p>
<p><b>Keywords</b>: Deception, Causality, Game Theory
</p><p><b>Compressor summary</b>: The paper introduces a formal definition of deception in structural causal games, based on philosophy literature, and provides graphical criteria for detecting and mitigating deception in AI systems like reinforcement learners and language models.</p><hr><h3>Conditional Mutual Information for Disentangled Representations in Reinforcement Learning</h3>
<p>Mhairi Dunion, Trevor McInroe, Kevin Sebastian Luck, Josiah P. Hanna, Stefano V Albrecht</p>
<p><a href='https://openreview.net/forum?id=EmYWJsyad4'>https://openreview.net/forum?id=EmYWJsyad4</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Representation Learning, Disentanglement
</p><p><b>Compressor summary</b>: The paper proposes a method to teach reinforcement learning agents to separate and represent correlated features in their latent representation, which improves their ability to generalize when the environment changes or when deployed in the real world.</p><hr><h3>ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation</h3>
<p>Yasheng SUN, Yifan Yang, Houwen Peng, Yifei Shen, Yuqing Yang, Han Hu, Lili Qiu, Hideki Koike</p>
<p><a href='https://openreview.net/forum?id=EmOIP3t9nk'>https://openreview.net/forum?id=EmOIP3t9nk</a></p>
<p><b>Keywords</b>: Image Manipulation, Visual Instruction
</p><p><b>Compressor summary</b>: The paper proposes ImageBrush, a novel image manipulation method that learns visual instructions from transformation images to accurately edit images without using language descriptions.</p><hr><h3>Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models</h3>
<p>Peter Hase, Mohit Bansal, Been Kim, Asma Ghandeharioun</p>
<p><a href='https://openreview.net/forum?id=EldbUlZtbd'>https://openreview.net/forum?id=EldbUlZtbd</a></p>
<p><b>Keywords</b>: localization, model editing, mechanistic interpretability, language models
</p><p><b>Compressor summary</b>: The paper finds that editing facts in language models does not always follow the localization conclusions from representation denoising and suggests that a better understanding of model behavior may not be helpful for editing tasks.</p><hr><h3>Recaptured Raw Screen Image and Video Demoiréing via Channel and Spatial Modulations</h3>
<p>Huanjing Yue, Yijia Cheng, Xin Liu, Jingyu Yang</p>
<p><a href='https://openreview.net/forum?id=EkcO9tHm6S'>https://openreview.net/forum?id=EkcO9tHm6S</a></p>
<p><b>Keywords</b>: Raw image demoiréing, raw video demoiréing, video demoiréing dataset
</p><p><b>Compressor summary</b>: The authors propose a new network for removing moiré patterns from raw images and videos, which outperforms existing methods and introduce a new dataset with an alignment method.</p><hr><h3>Equivariant Neural Operator Learning with Graphon Convolution</h3>
<p>Chaoran Cheng, Jian Peng</p>
<p><a href='https://openreview.net/forum?id=EjiA3uWpnc'>https://openreview.net/forum?id=EjiA3uWpnc</a></p>
<p><b>Keywords</b>: Neural Operator Learning, Spectral Graph Theory, Graphon
</p><p><b>Compressor summary</b>: The paragraph describes a new model called InfGCN that uses a combination of continuous and discrete graph structures to learn mappings between 3D functions while preserving equivariance, and shows it performs better than existing architectures on electron density datasets.</p><hr><h3>Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics</h3>
<p>Leon Klein, Andrew Y. K. Foong, Tor Erlend Fjelde, Bruno Kacper Mlodozeniec, Marc Brockschmidt, Sebastian Nowozin, Frank Noe, Ryota Tomioka</p>
<p><a href='https://openreview.net/forum?id=EjMLpTgvKH'>https://openreview.net/forum?id=EjMLpTgvKH</a></p>
<p><b>Keywords</b>: Molecular Dynamics, Normalizing Flows, MCMC
</p><p><b>Compressor summary</b>: Timewarp is a method that uses a flow trained on MD trajectories to make large steps in time, enabling efficient simulation of processes like binding and folding over longer timescales for different molecular systems.</p><hr><h3>Information-guided Planning: An Online Approach for Partially Observable Problems</h3>
<p>Matheus Aparecido Do Carmo Alves, Amokh Varma, Yehia Elkhatib, Leandro Soriano Marcolino</p>
<p><a href='https://openreview.net/forum?id=EjG2G1PT2v'>https://openreview.net/forum?id=EjG2G1PT2v</a></p>
<p><b>Keywords</b>: Information-guided planning, Planning under uncertainty, Sequential decision making
</p><p><b>Compressor summary</b>: IB-POMCP is an online planning algorithm that uses entropy estimates of the world belief to guide a tree search process and improve decision-making in partially observable environments, outperforming existing methods in reward, time, and convergence.</p><hr><h3>Generative Modelling of Stochastic Actions with Arbitrary Constraints in Reinforcement Learning</h3>
<p>Changyu Chen, Ramesha Karunasena, Thanh Hong Nguyen, Arunesh Sinha, Pradeep Varakantham</p>
<p><a href='https://openreview.net/forum?id=Ehzj9F2Kmj'>https://openreview.net/forum?id=Ehzj9F2Kmj</a></p>
<p><b>Keywords</b>: Action constrained reinforcement learning, Normalizing flow, Generative modelling
</p><p><b>Compressor summary</b>: The authors propose a method for solving large discrete action space problems in Reinforcement Learning with validity constraints by using a conditional normalizing flow to represent a stochastic policy and an invalid action rejection technique to update the base policy.</p><hr><h3>f-Policy Gradients: A General Framework for Goal-Conditioned RL using f-Divergences</h3>
<p>Siddhant Agarwal, Ishan Durugkar, Peter Stone, Amy Zhang</p>
<p><a href='https://openreview.net/forum?id=EhhPtGsVAv'>https://openreview.net/forum?id=EhhPtGsVAv</a></p>
<p><b>Keywords</b>: Goal Conditioned Reinforcement Learning, Shaping Rewards, Reward Design
</p><p><b>Compressor summary</b>: The paper proposes $f$-Policy Gradients, a method to encourage exploration in sparse reward RL problems by minimizing the f-divergence between the agent's state visitation distribution and the goal, which can lead to an optimal policy.</p><hr><h3>SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models</h3>
<p>Xiaosong Ma, Jie ZHANG, Song Guo, Wenchao Xu</p>
<p><a href='https://openreview.net/forum?id=EhdNQiOWgQ'>https://openreview.net/forum?id=EhdNQiOWgQ</a></p>
<p><b>Keywords</b>: Test-Time Adaptation, Prompt Learning, Unsupervised Representation Learning
</p><p><b>Compressor summary</b>: SwapPrompt is a novel framework for test-time adaptation that leverages self-supervised contrastive learning to improve the performance of pre-trained vision-language models on unseen test domains, achieving state-of-the-art results and comparable performance with supervised methods.</p><hr><h3>Training Private Models That Know What They Don’t Know</h3>
<p>Stephan Rabanser, Anvith Thudi, Abhradeep Guha Thakurta, Krishnamurthy Dj Dvijotham, Nicolas Papernot</p>
<p><a href='https://openreview.net/forum?id=EgCjf1vjMB'>https://openreview.net/forum?id=EgCjf1vjMB</a></p>
<p><b>Keywords</b>: differential privacy, selective classification, selective prediction, abstain option, reject option, uncertainty quantification, misclassification detection
</p><p><b>Compressor summary</b>: The authors investigate selective classifiers under differential privacy and find that some methods are ineffective due to increased privacy leakage, while a recent approach works well; however, performance degrades with lower privacy levels.</p><hr><h3>STREAMER: Streaming Representation Learning and Event Segmentation in a Hierarchical Manner</h3>
<p>Ramy Mounir, Sujal Vijayaraghavan, Sudeep Sarkar</p>
<p><a href='https://openreview.net/forum?id=EfTMRQn00d'>https://openreview.net/forum?id=EfTMRQn00d</a></p>
<p><b>Keywords</b>: predictive learning, hierarchical event segmentation, self-supervised learning, streaming processing, perceptual inputs, biologically-plausible.
</p><p><b>Compressor summary</b>: The paragraph describes STREAMER, a self-supervised architecture for hierarchical representation learning and segmentation of streaming inputs, which is evaluated on the egocentric EPIC-KITCHENS dataset for temporal event segmentation.</p><hr><h3>Speculative Decoding with Big Little Decoder</h3>
<p>Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. Mahoney, Amir Gholami, Kurt Keutzer</p>
<p><a href='https://openreview.net/forum?id=EfMyf9MC3t'>https://openreview.net/forum?id=EfMyf9MC3t</a></p>
<p><b>Keywords</b>: Transformer, efficient inference, efficient model, decoding
</p><p><b>Compressor summary</b>: BiLD improves text generation efficiency by using a small model for autoregressive tasks and a large model for refining predictions non-autoregressively, with two policies to coordinate them.</p><hr><h3>Isometric Quotient Variational Auto-Encoders for Structure-Preserving Representation Learning</h3>
<p>In Huh, changwook jeong, Jae Myung Choe, Young-Gu Kim, Dae Sin Kim</p>
<p><a href='https://openreview.net/forum?id=EdgPb3ngR4'>https://openreview.net/forum?id=EdgPb3ngR4</a></p>
<p><b>Keywords</b>: representation learning, auto-encoders, geometry, symmetry
</p><p><b>Compressor summary</b>: The paper introduces IQVAEs, a novel auto-encoding framework that learns symmetry-preserving representations of data manifolds embedded in high-dimensional spaces using variational auto-encoders.</p><hr><h3>RRHF: Rank Responses to Align Language Models with Human Feedback</h3>
<p>Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, Fei Huang</p>
<p><a href='https://openreview.net/forum?id=EdIGMCHk4l'>https://openreview.net/forum?id=EdIGMCHk4l</a></p>
<p><b>Keywords</b>: Large Language Model, Human Alignment
</p><p><b>Compressor summary</b>: RRHF is a novel learning paradigm for aligning large language models with human preferences using ranking loss and sampling responses from various sources, outperforming PPO in simplicity, efficiency, and performance.</p><hr><h3>Better Correlation and Robustness: A Distribution-Balanced Self-Supervised Learning Framework for Automatic Dialogue Evaluation</h3>
<p>Peiwen Yuan, Xinglin Wang, Jiayi Shi, Bin Sun, Yiwei Li, Kan Li</p>
<p><a href='https://openreview.net/forum?id=Ecv1GMiXSk'>https://openreview.net/forum?id=Ecv1GMiXSk</a></p>
<p><b>Keywords</b>: Natural language process, Automatic dialog evaluation
</p><p><b>Compressor summary</b>: BCR is a distribution-balanced self-supervised learning framework that improves the correlation and robustness of turn-level dialogue evaluation models using coherence-balanced training signals and a novel adaptive loss function.</p><hr><h3>Mobilizing Personalized Federated Learning in Infrastructure-Less and Heterogeneous Environments via Random Walk Stochastic ADMM</h3>
<p>Ziba Parsons, Fei Dou, Houyi Du, Zheng Song, Jin Lu</p>
<p><a href='https://openreview.net/forum?id=EcmqyXekuP'>https://openreview.net/forum?id=EcmqyXekuP</a></p>
<p><b>Keywords</b>: Mobilized Federated Networks, Personalized Federated Learning, Random Walk, Stochastic ADMM
</p><p><b>Compressor summary</b>: The paper proposes a novel FL approach for isolated nodes with data heterogeneity and wireless links, using Random Walk SADMM optimization algorithm that improves convergence, accuracy, and communication efficiency.</p><hr><h3>Optimal Treatment Allocation for Efficient Policy Evaluation in Sequential Decision Making</h3>
<p>Ting Li, Chengchun Shi, Jianing Wang, Fan Zhou, Hongtu Zhu</p>
<p><a href='https://openreview.net/forum?id=EcReRm7q9p'>https://openreview.net/forum?id=EcReRm7q9p</a></p>
<p><b>Keywords</b>: Average treatment effect, Experimental design, Off-policy evaluation, Optimal treatment allocation
</p><p><b>Compressor summary</b>: The paper proposes three optimal allocation strategies for A/B testing in dynamic settings to minimize variance and improve accuracy of treatment effect estimators using off-policy evaluation methods.</p><hr><h3>Learning Time-Invariant Representations for Individual Neurons from Population Dynamics</h3>
<p>Lu Mi, Trung Le, Tianxing He, Eli Shlizerman, Uygar Sümbül</p>
<p><a href='https://openreview.net/forum?id=EcN3l6Xmnx'>https://openreview.net/forum?id=EcN3l6Xmnx</a></p>
<p><b>Keywords</b>: population dynamics, neuronal representation, calcium imaging, cell types
</p><p><b>Compressor summary</b>: The authors propose a self-supervised learning method to assign stable representations to neurons based on population activity and improve predictions of gene expression and neuronal identity.</p><hr><h3>Disentangled Wasserstein Autoencoder for T-Cell Receptor Engineering</h3>
<p>Tianxiao Li, Hongyu Guo, Filippo Grazioli, Mark Gerstein, Martin Renqiang Min</p>
<p><a href='https://openreview.net/forum?id=Eb74zfBkWa'>https://openreview.net/forum?id=Eb74zfBkWa</a></p>
<p><b>Keywords</b>: protein engineering, disentangled representation, T cell receptor
</p><p><b>Compressor summary</b>: The proposed method uses a Wasserstein autoencoder to automatically identify functional residues in proteins and edit them without affecting the overall structure, improving efficiency and quality for protein engineering applications like T-cell receptor modification.</p><hr><h3>Context-lumpable stochastic bandits</h3>
<p>Chung-Wei Lee, Qinghua Liu, Yasin Abbasi-Yadkori, Chi Jin, Tor Lattimore, Csaba Szepesvari</p>
<p><a href='https://openreview.net/forum?id=EY7Hpj8Ok6'>https://openreview.net/forum?id=EY7Hpj8Ok6</a></p>
<p><b>Keywords</b>: Contextual bandits, low-rank bandits, latent bandits, clustering bandits, stochastic bandit problems, context-lumpable bandits
</p><p><b>Compressor summary</b>: The paper proposes an algorithm for a contextual bandit problem where the learner groups the contexts by similarity and achieves near-optimal sample complexity and minimax regret.</p><hr><h3>Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks</h3>
<p>Ryan Sullivan, Akarsh Kumar, Shengyi Huang, John P Dickerson, Joseph Suarez</p>
<p><a href='https://openreview.net/forum?id=EY4OHikuBm'>https://openreview.net/forum?id=EY4OHikuBm</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Proximal Policy Optimization, Reward Normalization
</p><p><b>Compressor summary</b>: The paper applies DreamerV3's tricks to PPO and shows that they don't generally improve PPO, but have some specific cases where they work well.</p><hr><h3>Generative Neural Fields by Mixtures of Neural Implicit Functions</h3>
<p>Tackgeun You, Mijeong Kim, Jungtaek Kim, Bohyung Han</p>
<p><a href='https://openreview.net/forum?id=EWNtYvepJh'>https://openreview.net/forum?id=EWNtYvepJh</a></p>
<p><b>Keywords</b>: generative neural fields; implicit neural representation; model averaging
</p><p><b>Compressor summary</b>: The proposed method learns implicit neural representations and their coefficients to enlarge the capacity of generative neural fields, improving efficiency and effectiveness in generating diverse data.</p><hr><h3>Better Private Linear Regression Through Better Private Feature Selection</h3>
<p>Travis Dick, Jennifer Gillenwater, Matthew Joseph</p>
<p><a href='https://openreview.net/forum?id=EUiIbwV379'>https://openreview.net/forum?id=EUiIbwV379</a></p>
<p><b>Keywords</b>: differential privacy, linear regression, sparse, feature selection, kendall
</p><p><b>Compressor summary</b>: The authors propose a differentially private feature selection method based on Kendall rank correlation to broaden the applicability of private linear regression algorithms in high-dimensional problems.</p><hr><h3>SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models</h3>
<p>Ziyi Wu, Jingyu Hu, Wuyue Lu, Igor Gilitschenski, Animesh Garg</p>
<p><a href='https://openreview.net/forum?id=ETk6cfS3vk'>https://openreview.net/forum?id=ETk6cfS3vk</a></p>
<p><b>Keywords</b>: Unsupervised object-centric learning, diffusion model, generative modeling
</p><p><b>Compressor summary</b>: SlotDiffusion is a new object-centric Latent Diffusion Model that improves visual generation and object manipulation in images and videos by enhancing slot-to-image decoding.</p><hr><h3>Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation</h3>
<p>Chaofan Ma, Yuhuan Yang, Chen Ju, Fei Zhang, Ya Zhang, Yanfeng Wang</p>
<p><a href='https://openreview.net/forum?id=ESEM1lNoeS'>https://openreview.net/forum?id=ESEM1lNoeS</a></p>
<p><b>Keywords</b>: Open-Vocabulary Semantic Segmentation, Attributes, Decomposition and Aggregation
</p><p><b>Compressor summary</b>: The paragraph describes a novel method for semantic segmentation that uses attribute decomposition and aggregation to handle ambiguous or incomplete textual category names, inspired by human cognition.</p><hr><h3>PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation</h3>
<p>Zhaoxi Chen, Fangzhou Hong, Haiyi Mei, Guangcong Wang, Lei Yang, Ziwei Liu</p>
<p><a href='https://openreview.net/forum?id=ESCafo3oD5'>https://openreview.net/forum?id=ESCafo3oD5</a></p>
<p><b>Keywords</b>: neural rendering, 3D generative model, diffusion model, volumetric primitives, 3D human generation
</p><p><b>Compressor summary</b>: PrimDiffusion is a diffusion-based framework for generating 3D humans using volumetric primitives, which enables efficient, high-quality, and flexible rendering.</p><hr><h3>H3T: Efficient Integration of Memory Optimization and Parallelism for Large-scale Transformer Training</h3>
<p>Yuzhong Wang, Xu Han, Weilin Zhao, Guoyang Zeng, Zhiyuan Liu, Maosong Sun</p>
<p><a href='https://openreview.net/forum?id=ES32O8mBK3'>https://openreview.net/forum?id=ES32O8mBK3</a></p>
<p><b>Keywords</b>: ML System, Parallelism Learning, Memory Optimization, Data Parallelism, Model Parallelism, Parameter Parallelism, ZeRO, Rematerialization, Checkpointing, Tensor Offloading, Dynamic Programming
</p><p><b>Compressor summary</b>: The paper proposes a framework called H3T that automatically integrates memory optimization and parallelism to improve the efficiency of training large Transformer-based models, achieving significant speedups and reduced memory overhead.</p><hr><h3>Byzantine-Tolerant Methods for Distributed Variational Inequalities</h3>
<p>Nazarii Tupitsa, Abdulla Jasem Almansoori, Yanlin Wu, Martin Takáč, Karthik Nandakumar, Samuel Horváth, Eduard Gorbunov</p>
<p><a href='https://openreview.net/forum?id=ER0bcYXvvo'>https://openreview.net/forum?id=ER0bcYXvvo</a></p>
<p><b>Keywords</b>: byzantine robustness, variational inequalities, min-max problems
</p><p><b>Compressor summary</b>: The authors present new methods for solving Byzantine-robust distributed variational inequalities, improving upon existing work and providing empirical evidence.</p><hr><h3>AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models</h3>
<p>Yuancheng Wang, Zeqian Ju, Xu Tan, Lei He, Zhizheng Wu, Jiang Bian, sheng zhao</p>
<p><a href='https://openreview.net/forum?id=EO1KuHoR0V'>https://openreview.net/forum?id=EO1KuHoR0V</a></p>
<p><b>Keywords</b>: audio editing, text-to-audio generation, diffusion models
</p><p><b>Compressor summary</b>: AUDIT is an instruction-guided audio editing model that uses diffusion and denoising processes, triplet training data, and only needs edit instructions as input to achieve state-of-the-art results in various audio editing tasks.</p><hr><h3>Neural Graph Generation from Graph Statistics</h3>
<p>Kiarash Zahirnia, Yaochen Hu, Mark Coates, Oliver Schulte</p>
<p><a href='https://openreview.net/forum?id=EI6BHFKA5p'>https://openreview.net/forum?id=EI6BHFKA5p</a></p>
<p><b>Keywords</b>: Graph Generation, Local Differential Privacy, Graph Statistics, Latent Adjacency Matrix
</p><p><b>Compressor summary</b>: The text describes a new method for learning a deep graph generative model from aggregate statistics without using the graph adjacency matrix, which can protect privacy and generate realistic graphs.</p><hr><h3>A Long $N$-step Surrogate Stage Reward for Deep Reinforcement Learning</h3>
<p>Junmin Zhong, Ruofan Wu, Jennie Si</p>
<p><a href='https://openreview.net/forum?id=EGfYnTyEGv'>https://openreview.net/forum?id=EGfYnTyEGv</a></p>
<p><b>Keywords</b>: Deep reinforcement learning, Reward Estimation
</p><p><b>Compressor summary</b>: The paper introduces a new stage reward estimator, LNSS, for deep RL that mitigates high variance problem and improves performance in continuous control tasks using various baseline algorithms.</p><hr><h3>LayoutPrompter: Awaken the Design Ability of Large Language Models</h3>
<p>Jiawei Lin, Jiaqi Guo, Shizhao Sun, Zijiang James Yang, Jian-Guang Lou, Dongmei Zhang</p>
<p><a href='https://openreview.net/forum?id=EF56cv8B3b'>https://openreview.net/forum?id=EF56cv8B3b</a></p>
<p><b>Keywords</b>: graphic design, graphic layout, large language models, in-context learning
</p><p><b>Compressor summary</b>: The paper introduces LayoutPrompter, which uses large language models to generate layouts efficiently and versatilely without training or fine-tuning.</p><hr><h3>Harnessing the power of choices in decision tree learning</h3>
<p>Guy Blanc, Jane Lange, Chirag Pabbaraju, Colin Sullivan, Li-Yang Tan, Mo Tiwari</p>
<p><a href='https://openreview.net/forum?id=EEtJTfvNZx'>https://openreview.net/forum?id=EEtJTfvNZx</a></p>
<p><b>Keywords</b>: Decision Trees, Decision Tree Learning, Top-$k$, ID3, Greedy Algorithms
</p><p><b>Compressor summary</b>: Top-$k$ is a simple generalization of decision tree learning algorithms that considers multiple best attributes for splitting and shows improved accuracy, scalability, and applicability compared to existing methods.</p><hr><h3>Auditing Fairness by Betting</h3>
<p>Ben Chugg, Santiago Cortes-Gomez, Bryan Wilder, Aaditya Ramdas</p>
<p><a href='https://openreview.net/forum?id=EEVpt3dJQj'>https://openreview.net/forum?id=EEVpt3dJQj</a></p>
<p><b>Keywords</b>: fairness, auditing, sequential analysis, martingales, testing by betting
</p><p><b>Compressor summary</b>: The authors propose efficient, sequential, and probabilistic methods for auditing the fairness of classification and regression models in real-world systems using inferential and game-theoretic frameworks.</p><hr><h3>PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification</h3>
<p>Qianli Shen, Wai Hoh Tang, Zhun Deng, Apostolos Psaros, Kenji Kawaguchi</p>
<p><a href='https://openreview.net/forum?id=EETqXXdqkI'>https://openreview.net/forum?id=EETqXXdqkI</a></p>
<p><b>Keywords</b>: physics-informed learning, uncertainty quantification, deep learning
</p><p><b>Compressor summary</b>: The paper presents PICProp, a bi-level optimization method for estimating confidence intervals in physics-informed learning without strong assumptions.</p><hr><h3>Bandit Task Assignment with Unknown Processing Time</h3>
<p>Shinji Ito, Daisuke Hatano, Hanna Sumita, Kei Takemura, Takuro Fukunaga, Naonori Kakimura, Ken-Ichi Kawarabayashi</p>
<p><a href='https://openreview.net/forum?id=EE1Uiu3Ryb'>https://openreview.net/forum?id=EE1Uiu3Ryb</a></p>
<p><b>Keywords</b>: bandit, combinatorial semi-bandits, bandits with badget
</p><p><b>Compressor summary</b>: The paper introduces a new bandit task assignment problem with processing time and combinatorial constraints, and proposes an UCB-based algorithm with phased updates that achieves near-optimal regret bounds.</p><hr><h3>Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution</h3>
<p>Ying Wang, Tim G. J. Rudner, Andrew Gordon Wilson</p>
<p><a href='https://openreview.net/forum?id=ECvtxmVP0x'>https://openreview.net/forum?id=ECvtxmVP0x</a></p>
<p><b>Keywords</b>: Interpretability, Attribution Maps, Information Bottleneck, Multi-Modal Learning, Vision-Language Pretrained Models
</p><p><b>Compressor summary</b>: M2IB is a method to improve interpretability of vision-language models like CLIP by compressing irrelevant information and preserving relevant features, without requiring ground truth labels.</p><hr><h3>Prototype-based Aleatoric Uncertainty Quantification for Cross-modal Retrieval</h3>
<p>Hao Li, Jingkuan Song, Lianli Gao, Xiaosu Zhu, Heng Tao Shen</p>
<p><a href='https://openreview.net/forum?id=ECRgBK6sk1'>https://openreview.net/forum?id=ECRgBK6sk1</a></p>
<p><b>Keywords</b>: multimodal learning, cross-modal retrieval, robust learning, uncertainty
</p><p><b>Compressor summary</b>: The paper introduces a new framework to quantify and reduce uncertainty in cross-modal retrieval methods by using learnable prototypes and Subjective Logic Theory.</p><hr><h3>Zero-Regret Performative Prediction Under Inequality Constraints</h3>
<p>Wenjing YAN, Xuanyu Cao</p>
<p><a href='https://openreview.net/forum?id=ECBK3TVmZl'>https://openreview.net/forum?id=ECBK3TVmZl</a></p>
<p><b>Keywords</b>: Performative prediction, decision-dependent distribution, inequality constraints, primal-dual algorithm.
</p><p><b>Compressor summary</b>: The paper studies how to optimize performative prediction problems with inequality constraints using a robust primal-dual framework and proposes an adaptive algorithm that achieves low regret and constraint violations.</p><hr><h3>Learning from Visual Observation via Offline Pretrained State-to-Go Transformer</h3>
<p>Bohan Zhou, Ke Li, Jiechuan Jiang, Zongqing Lu</p>
<p><a href='https://openreview.net/forum?id=E58gaxJN1d'>https://openreview.net/forum?id=E58gaxJN1d</a></p>
<p><b>Keywords</b>: Learning from Observations, Offline Learning from Visual Observations, State-to-Go Transformer
</p><p><b>Compressor summary</b>: The paper proposes a two-stage framework that uses a pretrained State-to-Go Transformer to predict latent transitions from visual observation and provide intrinsic rewards for reinforcement learning tasks, achieving performance comparable to policy learned from environmental rewards in some cases.</p><hr><h3>On the Asymptotic Learning Curves of Kernel Ridge Regression under Power-law Decay</h3>
<p>Yicheng Li, Haobo Zhang, Qian Lin</p>
<p><a href='https://openreview.net/forum?id=E4P5kVSKlT'>https://openreview.net/forum?id=E4P5kVSKlT</a></p>
<p><b>Keywords</b>: generalization, reproducing kernel Hilbert space, bias-variance trade-off
</p><p><b>Compressor summary</b>: The paper studies the learning curve of kernel ridge regression under realistic assumptions and shows how it relates to the choice of parameters, source condition, and noise, with implications for the benign overfitting phenomenon in over-parametrized neural networks.</p><hr><h3>Strategic Distribution Shift of Interacting Agents via Coupled Gradient Flows</h3>
<p>Lauren E Conger, Franca Hoffman, Eric Mazumdar, Lillian J Ratliff</p>
<p><a href='https://openreview.net/forum?id=E3ZUEaeFYS'>https://openreview.net/forum?id=E3ZUEaeFYS</a></p>
<p><b>Keywords</b>: distribution shift, partial differential equations
</p><p><b>Compressor summary</b>: The paper proposes a new framework to analyze how learning algorithms and their environments influence each other over time, accounting for complex dynamics and proving its convergence in different settings.</p><hr><h3>Reward Imputation with Sketching for Contextual Batched Bandits</h3>
<p>Xiao Zhang, Ninglu Shao, Zihua Si, Jun Xu, Wenhan Wang, Hanjing Su, Ji-Rong Wen</p>
<p><a href='https://openreview.net/forum?id=E2zoGTkTbW'>https://openreview.net/forum?id=E2zoGTkTbW</a></p>
<p><b>Keywords</b>: batched bandit, sketching, reward imputation, regret bound, ridge regression
</p><p><b>Compressor summary</b>: SPUIR is an efficient batched bandit algorithm that imputes unobserved rewards using sketching and achieves low regret bounds in both theoretical and practical settings.</p><hr><h3>VeriX: Towards Verified Explainability of Deep Neural Networks</h3>
<p>Min Wu, Haoze Wu, Clark Barrett</p>
<p><a href='https://openreview.net/forum?id=E2TJI6CKm0'>https://openreview.net/forum?id=E2TJI6CKm0</a></p>
<p><b>Keywords</b>: trustworthy machine learning, deep neural networks, explainability, interpretability, formal methods, automated verification
</p><p><b>Compressor summary</b>: VeriX is a system that creates optimal robust explanations and counterfactuals for machine learning models using constraint solving and feature-level sensitivity ranking.</p><hr><h3>Federated Conditional Stochastic Optimization</h3>
<p>Xidong Wu, Jianhui Sun, Zhengmian Hu, Junyi Li, Aidong Zhang, Heng Huang</p>
<p><a href='https://openreview.net/forum?id=E0Gw1uz7lU'>https://openreview.net/forum?id=E0Gw1uz7lU</a></p>
<p><b>Keywords</b>: Federated Learning, Conditional Stochastic Optimization, Nonconvex Optimization
</p><p><b>Compressor summary</b>: The paper introduces FCSG and FCSG-M, two new federated learning algorithms for nonconvex conditional stochastic optimization, and Acc-FCSG-M, an accelerated version with lower complexity, which are tested and validated on various tasks.</p><hr><h3>Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder</h3>
<p>Michael Bereket, Theofanis Karaletsos</p>
<p><a href='https://openreview.net/forum?id=DzaCE00jGV'>https://openreview.net/forum?id=DzaCE00jGV</a></p>
<p><b>Keywords</b>: Disentagled representation learning, VAE, generative models, sparse mechanism shift, perturbation modeling, cellular modeling
</p><p><b>Compressor summary</b>: SAMS-VAE is a novel generative model that combines compositionality, disentanglement, and interpretability for perturbation models in drug discovery, and shows strong performance and correlations to known biological mechanisms.</p><hr><h3>Convergence Analysis of Sequential Federated Learning on Heterogeneous Data</h3>
<p>Yipeng Li, Xinchen Lyu</p>
<p><a href='https://openreview.net/forum?id=Dxhv8Oja2V'>https://openreview.net/forum?id=Dxhv8Oja2V</a></p>
<p><b>Keywords</b>: Federated Learning, Convergence analysis
</p><p><b>Compressor summary</b>: The paper analyzes the convergence theory of sequential federated learning (SFL) for non-convex objectives on heterogeneous data and shows that SFL performs better than parallel federated learning (PFL) in cross-device settings.</p><hr><h3>Distribution Learnability and Robustness</h3>
<p>Shai Ben-David, Alex Bie, Gautam Kamath, Tosca Lechner</p>
<p><a href='https://openreview.net/forum?id=Dx99y3okbL'>https://openreview.net/forum?id=Dx99y3okbL</a></p>
<p><b>Keywords</b>: robustness, distribution learning
</p><p><b>Compressor summary</b>: The paper investigates how different types of contamination affect the relationship between learnability and robust learnability in distribution learning, and discusses implications for other settings like compression and privacy.</p><hr><h3>Curvature Filtrations for Graph Generative Model Evaluation</h3>
<p>Joshua Southern, Jeremy Wayland, Michael M. Bronstein, Bastian Rieck</p>
<p><a href='https://openreview.net/forum?id=Dt71xKyabn'>https://openreview.net/forum?id=Dt71xKyabn</a></p>
<p><b>Keywords</b>: Curvature, topology, persistent homology, graph learning, generative model, machine learning, geometric deep learning
</p><p><b>Compressor summary</b>: The authors propose using graph curvature and topological data analysis to create effective ways of assessing graph generative models.</p><hr><h3>Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes</h3>
<p>Connor Toups, Rishi Bommasani, Kathleen Creel, Sarah H Bana, Dan Jurafsky, Percy Liang</p>
<p><a href='https://openreview.net/forum?id=Ds8iLujo3p'>https://openreview.net/forum?id=Ds8iLujo3p</a></p>
<p><b>Keywords</b>: homogenous outcomes, societal impact of ML, deployed ML, systemic failure
</p><p><b>Compressor summary</b>: The authors propose a new approach called ecosystem-level analysis to study the societal impact of machine learning models by considering their collective effects on users, and they apply it to three modalities (text, images, speech) and find that systemic failure is common and not reduced by individual model improvements.</p><hr><h3>Faster Query Times for Fully Dynamic $k$-Center Clustering with Outliers</h3>
<p>Leyla Biabani, Annika Hennes, Morteza Monemizadeh, Melanie Schmidt</p>
<p><a href='https://openreview.net/forum?id=Ds7Vd83HlC'>https://openreview.net/forum?id=Ds7Vd83HlC</a></p>
<p><b>Keywords</b>: $k$-center clustering, outliers, dynamic algorithms
</p><p><b>Compressor summary</b>: The paragraph describes a problem of finding the optimal set of centers for a point cloud with some outliers, and presents an efficient data structure that can handle dynamic updates and provide approximate solutions in sublinear time.</p><hr><h3>The Impact of Positional Encoding on Length Generalization in Transformers</h3>
<p>Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan, Payel Das, Siva Reddy</p>
<p><a href='https://openreview.net/forum?id=Drrl2gcjzl'>https://openreview.net/forum?id=Drrl2gcjzl</a></p>
<p><b>Keywords</b>: Transformers, Positional Encoding, Length Generalization
</p><p><b>Compressor summary</b>: The paper compares five positional encoding methods for decoder-only Transformers and finds that no explicit encoding is better than others for length generalization in downstream tasks.</p><hr><h3>Gaussian Differential Privacy on Riemannian Manifolds</h3>
<p>Yangdi Jiang, Xiaotian Chang, Yi Liu, Lei Ding, Linglong Kong, Bei Jiang</p>
<p><a href='https://openreview.net/forum?id=DrIZZwEZtM'>https://openreview.net/forum?id=DrIZZwEZtM</a></p>
<p><b>Keywords</b>: Differential Privacy, Gaussian Differential Privacy, Differential Geometry, Riemannian Manifold, Homogeneous Riemannian Manifold, Frechet Mean
</p><p><b>Compressor summary</b>: The authors develop a new method for preserving privacy in data analysis on curved spaces using a special type of probability distribution that accounts for the geometry of the space.</p><hr><h3>Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment</h3>
<p>Carsten Tim Lüth, Till J. Bungert, Lukas Klein, Paul F Jaeger</p>
<p><a href='https://openreview.net/forum?id=Dqn715Txgl'>https://openreview.net/forum?id=Dqn715Txgl</a></p>
<p><b>Keywords</b>: Active Learning, Evaluation, Study
</p><p><b>Compressor summary</b>: The paper discusses the challenges of evaluating active learning methods, proposes a framework to address these issues, and presents an empirical study to demonstrate its usefulness.</p><hr><h3>Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards</h3>
<p>Mengfan Xu, Diego Klabjan</p>
<p><a href='https://openreview.net/forum?id=DqfdhM64LI'>https://openreview.net/forum?id=DqfdhM64LI</a></p>
<p><b>Keywords</b>: decentralized multi-agent MAB, heterogeneous light-tailed and heavy-tailed rewards, time dependent random graphs
</p><p><b>Compressor summary</b>: The paper proposes a novel algorithm for decentralized multi-agent bandit problems with time-varying graphs and varying rewards, achieving near-optimal regret bounds with high probability.</p><hr><h3>Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning</h3>
<p>Matthias Gerstgrasser, Tom Danino, Sarah Keren</p>
<p><a href='https://openreview.net/forum?id=DpuphOgJqh'>https://openreview.net/forum?id=DpuphOgJqh</a></p>
<p><b>Keywords</b>: multi-agent reinforcement learning, reinforcement learning, deep q learning, cooperative ai
</p><p><b>Compressor summary</b>: Selective Multi-Agent Prioritized Experience Relay is a new method for multi-agent reinforcement learning where agents share a small number of relevant experiences to improve learning efficiency and performance.</p><hr><h3>Statistically Valid Variable Importance Assessment through Conditional Permutations</h3>
<p>Ahmad Chamma, Denis Engemann, Bertrand Thirion</p>
<p><a href='https://openreview.net/forum?id=DoE3esTIEM'>https://openreview.net/forum?id=DoE3esTIEM</a></p>
<p><b>Keywords</b>: Interpretability, Variable Importance, Machine Learning, Deep Learning, Statistical Inference
</p><p><b>Compressor summary</b>: The authors propose CPI, a new method to assess variable importance in machine learning models, which overcomes the limitations of standard permutation importance by providing accurate type-I error control and performs well on benchmarks and real-world data analysis.</p><hr><h3>On the Implicit Bias of Linear Equivariant Steerable Networks</h3>
<p>Ziyu Chen, Wei Zhu</p>
<p><a href='https://openreview.net/forum?id=DnVjDRLwVu'>https://openreview.net/forum?id=DnVjDRLwVu</a></p>
<p><b>Keywords</b>: implicit bias, equivariant steerable networks, data augmentation, margin, generalization bound
</p><p><b>Compressor summary</b>: The text discusses how gradient flow helps linear equivariant steerable networks find group-invariant classifiers with a maximum margin, and shows that they are equivalent to data augmentation under certain conditions.</p><hr><h3>Approximation-Generalization Trade-offs under (Approximate) Group Equivariance</h3>
<p>Mircea Petrache, Shubhendu Trivedi</p>
<p><a href='https://openreview.net/forum?id=DnO6LTQ77U'>https://openreview.net/forum?id=DnO6LTQ77U</a></p>
<p><b>Keywords</b>: Equivariance, Invariance, Generalization, Equivariant Neural Networks, Approximation Error
</p><p><b>Compressor summary</b>: The paper investigates how incorporating task-specific symmetries in machine learning models improves generalization and how to handle approximate or partial symmetries between models and data distributions.</p><hr><h3>Data-Centric Learning from Unlabeled Graphs with Diffusion Model</h3>
<p>Gang Liu, Eric Inae, Tong Zhao, Jiaxin Xu, Tengfei Luo, Meng Jiang</p>
<p><a href='https://openreview.net/forum?id=DmakwvCJ7l'>https://openreview.net/forum?id=DmakwvCJ7l</a></p>
<p><b>Keywords</b>: Graph property prediction, Molecular property prediction, Diffusion model, Unlabeled data, Data augmentation, Transfer learning
</p><p><b>Compressor summary</b>: The paper proposes a method to use unlabeled graphs for property prediction tasks by generating task-specific graph examples and labels with a diffusion model.</p><hr><h3>Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization</h3>
<p>Kaiyue Wen, Zhiyuan Li, Tengyu Ma</p>
<p><a href='https://openreview.net/forum?id=Dkmpa6wCIx'>https://openreview.net/forum?id=Dkmpa6wCIx</a></p>
<p><b>Keywords</b>: Sharpness, Flatness, Generalization, Generalization Bound, SAM
</p><p><b>Compressor summary</b>: The paper investigates how flatness (sharpness) of two-layer ReLU neural networks relates to generalization and finds that it depends on data distribution and architecture, suggesting that sharpness minimization alone does not guarantee better generalization.</p><hr><h3>How to Scale Your EMA</h3>
<p>Dan Busbridge, Jason Ramapuram, Pierre Ablin, Tatiana Likhomanenko, Eeshan Gunesh Dhekane, Xavier Suau, Russell Webb</p>
<p><a href='https://openreview.net/forum?id=DkeeXVdQyu'>https://openreview.net/forum?id=DkeeXVdQyu</a></p>
<p><b>Keywords</b>: Optimization, scaling rules, EMA, exponential moving average, self-supervised learning, pseudo-labelling, semi-supervised learning, BYOL, distillation, speech, vision
</p><p><b>Compressor summary</b>: The paper proposes a scaling rule for optimization with model EMA that preserves training dynamics across batch sizes and improves the performance of various machine learning methods, including pseudo-labeling and SSL.</p><hr><h3>Optimal Rates for Bandit Nonstochastic Control</h3>
<p>Y. Jennifer Sun, Stephen Newman, Elad Hazan</p>
<p><a href='https://openreview.net/forum?id=DkKHSsmVuA'>https://openreview.net/forum?id=DkKHSsmVuA</a></p>
<p><b>Keywords</b>: Bandit control, online learning
</p><p><b>Compressor summary</b>: The paper proposes an algorithm for optimal control under adversarial conditions that achieves sublinear regret and introduces a novel memory-based technique for bandit convex optimization.</p><hr><h3>NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning</h3>
<p>Yun Yi, Haokui Zhang, Rong Xiao, Nannan Wang, Xiaoyu Wang</p>
<p><a href='https://openreview.net/forum?id=DjX2Nr15kY'>https://openreview.net/forum?id=DjX2Nr15kY</a></p>
<p><b>Keywords</b>: Transformer, Graph Neural Network, neural network encoding, representation learning, neural architecture search, neural network deployment
</p><p><b>Compressor summary</b>: The paper proposes a modified Transformer-based model that can learn efficient representations from both cell-structured and entire networks by incorporating inductive representation learning capability of GNNs and enhancing Transformer's ability to work with graph structures, achieving better performance in predicting latency and accuracy.</p><hr><h3>Automatic Integration for Spatiotemporal Neural Point Processes</h3>
<p>Zihao Zhou, Rose Yu</p>
<p><a href='https://openreview.net/forum?id=Deb1yP1zMN'>https://openreview.net/forum?id=Deb1yP1zMN</a></p>
<p><b>Keywords</b>: spatiotemporal modeling, neural point processes, integration method
</p><p><b>Compressor summary</b>: The paper introduces `Auto-STPP`, a novel method for efficiently integrating flexible spatiotemporal neural point processes using a decomposable parametrization and ProdNet, which outperforms existing methods in recovering complex intensity functions from irregular events.</p><hr><h3>Sketchy: Memory-efficient Adaptive Regularization with Frequent Directions</h3>
<p>Vladimir Feinberg, Xinyi Chen, Y. Jennifer Sun, Rohan Anil, Elad Hazan</p>
<p><a href='https://openreview.net/forum?id=DeZst6dKyi'>https://openreview.net/forum?id=DeZst6dKyi</a></p>
<p><b>Keywords</b>: online convex optimization, deep learning, matrix sketching, frequent directions
</p><p><b>Compressor summary</b>: The paper proposes a low-rank sketching approach using Frequent Directions (FD) to reduce memory and compute requirements of matrix preconditioners in deep learning training tasks, achieving similar performance to Shampoo and Adam with less memory.</p><hr><h3>Uncovering Prototypical Knowledge for Weakly Open-Vocabulary Semantic Segmentation</h3>
<p>Fei Zhang, Tianfei Zhou, Boyang Li, Hao He, Chaofan Ma, Tianjiao Zhang, Jiangchao Yao, Ya Zhang, Yanfeng Wang</p>
<p><a href='https://openreview.net/forum?id=DdViWdxCTs'>https://openreview.net/forum?id=DdViWdxCTs</a></p>
<p><b>Keywords</b>: Weakly (Text-based) Open-Vocabulary Semantic Segmentation, Vision-Language Pretraining, Prototypical Knowledge
</p><p><b>Compressor summary</b>: This paper introduces a new method for semantic segmentation of arbitrary classes using image-text pairs, which improves the performance by incorporating explicit supervision and multi-modal regularization for group tokens.</p><hr><h3>How2comm: Communication-Efficient and Collaboration-Pragmatic Multi-Agent Perception</h3>
<p>Dingkang Yang, Kun Yang, Yuzheng Wang, Jing Liu, Zhi Xu, Rongbin Yin, Peng Zhai, Lihua Zhang</p>
<p><a href='https://openreview.net/forum?id=Dbaxm9ujq6'>https://openreview.net/forum?id=Dbaxm9ujq6</a></p>
<p><b>Keywords</b>: Collaborative perception, multi-agent communication
</p><p><b>Compressor summary</b>: How2comm is a collaborative perception framework for driving scenarios that balances performance and communication bandwidth by using mutual information-aware communication, spatial-channel filtering, flow-guided delay compensation, and a collaboration transformer to handle various noises.</p><hr><h3>AiluRus: A Scalable ViT Framework for Dense Prediction</h3>
<p>Jin Li, Yaoming Wang, XIAOPENG ZHANG, Bowen Shi, Dongsheng Jiang, Chenglin Li, Wenrui Dai, Hongkai Xiong, Qi Tian</p>
<p><a href='https://openreview.net/forum?id=DVm0xxaEq1'>https://openreview.net/forum?id=DVm0xxaEq1</a></p>
<p><b>Keywords</b>: vision transformer, dense prediction
</p><p><b>Compressor summary</b>: The paper proposes an adaptive resolution method for vision transformers to speed up dense prediction tasks like semantic segmentation or object detection by clustering tokens based on importance and merging adjacent ones.</p><hr><h3>RoboCLIP: One Demonstration is Enough to Learn Robot Policies</h3>
<p>Sumedh Anand Sontakke, Jesse Zhang, Séb Arnold, Karl Pertsch, Erdem Biyik, Dorsa Sadigh, Chelsea Finn, Laurent Itti</p>
<p><a href='https://openreview.net/forum?id=DVlawv2rSI'>https://openreview.net/forum?id=DVlawv2rSI</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Vision and Language Models
</p><p><b>Compressor summary</b>: RoboCLIP is an online imitation learning method that uses VLMs to generate rewards from a single video or text demonstration, enabling high zero-shot performance in robot manipulation tasks without manual reward design or domain matching.</p><hr><h3>Chasing Fairness Under Distribution Shift: A Model Weight Perturbation Approach</h3>
<p>Zhimeng Jiang, Xiaotian Han, Hongye Jin, Guanchu Wang, Rui Chen, Na Zou, Xia Hu</p>
<p><a href='https://openreview.net/forum?id=DVjyq5eCAD'>https://openreview.net/forum?id=DVjyq5eCAD</a></p>
<p><b>Keywords</b>: Model Weight Perturbation, fairness, distribution shift
</p><p><b>Compressor summary</b>: The paper proposes a method called robust fairness regularization (RFR) that improves algorithmic fairness under distribution shifts by considering the worst case weight perturbation for each sensitive attribute group.</p><hr><h3>R-divergence for Estimating Model-oriented Distribution Discrepancy</h3>
<p>Zhilin Zhao, Longbing Cao</p>
<p><a href='https://openreview.net/forum?id=DVWIA9v9Jm'>https://openreview.net/forum?id=DVWIA9v9Jm</a></p>
<p><b>Keywords</b>: non-IID, Distribution Discrepancy, Data Divergence, Two-sample Test
</p><p><b>Compressor summary</b>: R-divergence is a method to measure how similar or different two datasets' probability distributions are by learning a minimum hypothesis on mixed data and comparing their expected risks.</p><hr><h3>Causal Interpretation of Self-Attention in Pre-Trained Transformers</h3>
<p>Raanan Yehezkel Rohekar, Yaniv Gurwicz, Shami Nisimov</p>
<p><a href='https://openreview.net/forum?id=DS4rKySlYC'>https://openreview.net/forum?id=DS4rKySlYC</a></p>
<p><b>Keywords</b>: Self-Attention, Causal Discovery, Reasoning, Explainability, Zero-shot, Transformer
</p><p><b>Compressor summary</b>: The authors propose a way to understand self-attention in Transformers as a causal structure over input symbols, allowing them to use pre-trained models for zero-shot causal discovery and provide explanations for the outcomes of two tasks.</p><hr><h3>ZoomTrack: Target-aware Non-uniform Resizing for Efficient Visual Tracking</h3>
<p>Yutong Kou, Jin Gao, Bing Li, Gang Wang, Weiming Hu, Yizheng Wang, Liang Li</p>
<p><a href='https://openreview.net/forum?id=DQgTewaKzt'>https://openreview.net/forum?id=DQgTewaKzt</a></p>
<p><b>Keywords</b>: Visual tracking, non-uniform resizing, HVS-inspired processing
</p><p><b>Compressor summary</b>: The paper proposes a method to improve the speed and performance of trackers by non-uniformly resizing the input image, allowing for better target detection despite smaller input size.</p><hr><h3>Transfer Learning with Affine Model Transformation</h3>
<p>Shunya Minami, Kenji Fukumizu, Yoshihiro Hayashi, Ryo Yoshida</p>
<p><a href='https://openreview.net/forum?id=DPeBX79eNz'>https://openreview.net/forum?id=DPeBX79eNz</a></p>
<p><b>Keywords</b>: Machine learning, Transfer learning
</p><p><b>Compressor summary</b>: The paper introduces a new transfer learning regression method called affine model transfer that has theoretical properties and outperforms existing methods in scenarios with scarce data.</p><hr><h3>A Theory of Unsupervised Translation Motivated by Understanding Animal Communication</h3>
<p>Shafi Goldwasser, David Gruber, Adam Tauman Kalai, Orr Paradise</p>
<p><a href='https://openreview.net/forum?id=DP2lioYIYl'>https://openreview.net/forum?id=DP2lioYIYl</a></p>
<p><b>Keywords</b>: Theory, Unsupervised Machine Translation
</p><p><b>Compressor summary</b>: The paragraph discusses a theoretical framework for analyzing unsupervised machine translation (UMT) between two languages with different structures, and how it could potentially help understand animal communication systems.</p><hr><h3>Off-Policy Evaluation for Human Feedback</h3>
<p>Qitong Gao, Ge Gao, Juncheng Dong, Vahid Tarokh, Min Chi, Miroslav Pajic</p>
<p><a href='https://openreview.net/forum?id=DOdaV0Hqdy'>https://openreview.net/forum?id=DOdaV0Hqdy</a></p>
<p><b>Keywords</b>: Off-policy evaluation (OPE), Variational latent model for trajectory representation learning, Reinforcement learning and OPE for adaptive neurostimulation
</p><p><b>Compressor summary</b>: The paper proposes an OPEHF framework for estimating human feedback signals in RL using offline data, which outperforms existing OPE methods in three real-world and simulation experiments.</p><hr><h3>Weakly Coupled Deep Q-Networks</h3>
<p>Ibrahim El Shar, Daniel R. Jiang</p>
<p><a href='https://openreview.net/forum?id=DNubFPV5Dy'>https://openreview.net/forum?id=DNubFPV5Dy</a></p>
<p><b>Keywords</b>: Reinforcement learning, Deep Reinforcement Learning, Weakly Coupled MDPs
</p><p><b>Compressor summary</b>: The paper introduces WCDQN, a reinforcement learning algorithm that improves performance in WCMDPs by training multiple subagents and combining their solutions to guide the main agent towards optimality.</p><hr><h3>Crystal Structure Prediction by Joint Equivariant Diffusion</h3>
<p>Rui Jiao, Wenbing Huang, Peijia Lin, Jiaqi Han, Pin Chen, Yutong Lu, Yang Liu</p>
<p><a href='https://openreview.net/forum?id=DNdN26m2Jk'>https://openreview.net/forum?id=DNdN26m2Jk</a></p>
<p><b>Keywords</b>: crystal structure prediction, equivariant graph neural networks, diffusion generative models
</p><p><b>Compressor summary</b>: DiffCSP is a novel diffusion model that leverages fractional coordinates and equivariant denoising to efficiently predict crystal structures with high accuracy.</p><hr><h3>On the Stability-Plasticity Dilemma in Continual Meta-Learning: Theory and Algorithm</h3>
<p>Qi CHEN, Changjian Shui, Ligong Han, Mario Marchand</p>
<p><a href='https://openreview.net/forum?id=DNHGKeOhLl'>https://openreview.net/forum?id=DNHGKeOhLl</a></p>
<p><b>Keywords</b>: continual meta-learning; transfer learning; stability-plasticity dilemma;
</p><p><b>Compressor summary</b>: The paragraph describes Continual Meta-Learning (CML), which aims to balance stability and plasticity in learning from non-i.i.d. tasks, and proposes a novel algorithm that adapts meta-parameter and learning rate to environment change.</p><hr><h3>Optimal Convergence Rate for Exact Policy Mirror Descent in Discounted Markov Decision Processes</h3>
<p>Emmeran Johnson, Ciara Pike-Burke, Patrick Rebeschini</p>
<p><a href='https://openreview.net/forum?id=DKHEkP7Idx'>https://openreview.net/forum?id=DKHEkP7Idx</a></p>
<p><b>Keywords</b>: Reinforcement Learning Theory, Policy Mirror Descent, Policy Gradient
</p><p><b>Compressor summary</b>: The paper introduces and analyzes Policy Mirror Descent (PMD), a general family of algorithms in reinforcement learning that achieves the optimal convergence rate and step-size when using exact policy evaluation.</p><hr><h3>Don’t just prune by magnitude! Your mask topology is a secret weapon</h3>
<p>Duc N.M Hoang, Souvik Kundu, Shiwei Liu, Zhangyang Wang</p>
<p><a href='https://openreview.net/forum?id=DIBcdjWV7k'>https://openreview.net/forum?id=DIBcdjWV7k</a></p>
<p><b>Keywords</b>: Pruning at Initialization, Pruning at Training, LTH, DST, Ramanujan, graph
</p><p><b>Compressor summary</b>: This paper explores how the connectivity of a deep network's architecture as a graph affects its performance, and proposes a new pruning method that maximizes the distance between sparse subnetworks and their original initialization.</p><hr><h3>First Order Stochastic Optimization with Oblivious Noise</h3>
<p>Ilias Diakonikolas, Sushrut Karmalkar, Jongho Park, Christos Tzamos</p>
<p><a href='https://openreview.net/forum?id=DI6KQhgqUr'>https://openreview.net/forum?id=DI6KQhgqUr</a></p>
<p><b>Keywords</b>: Oblivious noise, Robust Statistics, Heavy-tailed Stochastic Optimization, Approximate Gradients, Inexact Gradients
</p><p><b>Compressor summary</b>: The study explores stochastic optimization with oblivious noise and presents an efficient list-decodable learner that finds a close candidate or a single solution depending on the noise level.</p><hr><h3>Zero-One Laws of Graph Neural Networks</h3>
<p>Sam Adam-Day, Theodor-Mihai Iliant, Ismail Ilkan Ceylan</p>
<p><a href='https://openreview.net/forum?id=DGmxTUCHYs'>https://openreview.net/forum?id=DGmxTUCHYs</a></p>
<p><b>Keywords</b>: graph neural networks, graph convolutional networks, zero-one law, expressivity, asymptotic behavior
</p><p><b>Compressor summary</b>: The paper analyzes how graph neural networks (GNNs) represent and extrapolate information on large graphs, finding theoretical and empirical limitations to their performance.</p><hr><h3>Propagating Knowledge Updates to LMs Through Distillation</h3>
<p>Shankar Padmanabhan, Yasumasa Onoe, Michael JQ Zhang, Greg Durrett, Eunsol Choi</p>
<p><a href='https://openreview.net/forum?id=DFaGf3O7jf'>https://openreview.net/forum?id=DFaGf3O7jf</a></p>
<p><b>Keywords</b>: Knowledge editing, NLP, Distillation, deep learning, fine-tuning
</p><p><b>Compressor summary</b>: The authors propose a method for updating language models with new entity definitions and show that their approach enables better inferences based on these definitions without sacrificing performance in other contexts.</p><hr><h3>DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data</h3>
<p>Stephanie Fu, Netanel Yakir Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, Phillip Isola</p>
<p><a href='https://openreview.net/forum?id=DEiNSfh1k7'>https://openreview.net/forum?id=DEiNSfh1k7</a></p>
<p><b>Keywords</b>: perceptual similarity, foundation model, perception, computer vision, image metric
</p><p><b>Compressor summary</b>: The paper introduces DreamSim, a new perceptual similarity metric that assesses images holistically by capturing mid-level similarities in layout, object pose, and semantic content, and shows its effectiveness on retrieval and reconstruction tasks using a new dataset of human similarity judgments.</p><hr><h3>Coupled Reconstruction of Cortical Surfaces by Diffeomorphic Mesh Deformation</h3>
<p>Hao Zheng, Hongming Li, Yong Fan</p>
<p><a href='https://openreview.net/forum?id=DEC7NxDJLh'>https://openreview.net/forum?id=DEC7NxDJLh</a></p>
<p><b>Keywords</b>: Brain MRIs, cortical surface reconstruction, deep learning
</p><p><b>Compressor summary</b>: The authors propose a new deep learning framework that reconstructs cortical surfaces from brain MRIs by jointly estimating the midthickness surface and diffeomorphic flows to optimize and deform it into inner, outer, and midthickness surfaces while accounting for topological correctness and estimating cortical thickness.</p><hr><h3>Discrete-Smoothness in Online Algorithms with Predictions</h3>
<p>Yossi Azar, Debmalya Panigrahi, Noam Touitou</p>
<p><a href='https://openreview.net/forum?id=DDmH3H78iJ'>https://openreview.net/forum?id=DDmH3H78iJ</a></p>
<p><b>Keywords</b>: Online, Predictions, Learning-augmented, Facility Location, Set Cover
</p><p><b>Compressor summary</b>: The paper proposes discrete-smooth algorithms for online covering problems, such as facility location and set cover, which balance consistency, robustness, and smoothness in machine-learning predictions.</p><hr><h3>Brant: Foundation Model for Intracranial Neural Signal</h3>
<p>Daoze Zhang, Zhizhang Yuan, Yang Yang, Junru Chen, Jingjing Wang, Yafeng Li</p>
<p><a href='https://openreview.net/forum?id=DDkl9vaJyE'>https://openreview.net/forum?id=DDkl9vaJyE</a></p>
<p><b>Keywords</b>: Foundation model, Brain signal, Pretraining, Medicine
</p><p><b>Compressor summary</b>: Brant is a pre-trained foundation model that learns powerful representations of intracranial neural signals, achieving state-of-the-art performance on different brain signal tasks.</p><hr><h3>ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP</h3>
<p>Lu Yan, ZHUO ZHANG, Guanhong Tao, Kaiyuan Zhang, Xuan Chen, Guangyu Shen, Xiangyu Zhang</p>
<p><a href='https://openreview.net/forum?id=DD0QJvPbTD'>https://openreview.net/forum?id=DD0QJvPbTD</a></p>
<p><b>Keywords</b>: NLP, backdoor attack, fuzzing
</p><p><b>Compressor summary</b>: The paper proposes a method to detect stealthy backdoor attacks in NLP models by paraphrasing inputs to remove triggers and comparing model predictions before and after paraphrasing.</p><hr><h3>Payoff-based Learning with Matrix Multiplicative Weights in Quantum Games</h3>
<p>Kyriakos Lotidis, Panayotis Mertikopoulos, Nicholas Bambos, Jose Blanchet</p>
<p><a href='https://openreview.net/forum?id=DCIsNIUCV7'>https://openreview.net/forum?id=DCIsNIUCV7</a></p>
<p><b>Keywords</b>: quantum games, Matrix Multiplicative Weights, zero-sum games, Nash equilibrium
</p><p><b>Compressor summary</b>: The paper presents new methods for learning in quantum games and other semidefinite games with minimal information feedback, using zeroth-order gradient samplers and achieving different convergence rates depending on the information framework.</p><hr><h3>Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation</h3>
<p>Aniket Das, Dheeraj Mysore Nagaraj</p>
<p><a href='https://openreview.net/forum?id=DBz9E5aZey'>https://openreview.net/forum?id=DBz9E5aZey</a></p>
<p><b>Keywords</b>: Stein Variational Gradient Descent, Variational Inference, Sampling
</p><p><b>Compressor summary</b>: The paper introduces virtual particles to improve the efficiency and convergence rate of Stein Variational Gradient Descent (SVGD), a particle-based variational inference algorithm, in the finite-particle regime.</p><hr><h3>Brain-like Flexible Visual Inference by Harnessing Feedback Feedforward Alignment</h3>
<p>Tahereh Toosi, Elias Issa</p>
<p><a href='https://openreview.net/forum?id=DBlkX8Nczr'>https://openreview.net/forum?id=DBlkX8Nczr</a></p>
<p><b>Keywords</b>: Visual inference, Bio-plausible learning algorithm, Feedback connections, Visual imagery, Occlusions, Noise
</p><p><b>Compressor summary</b>: The paper proposes a learning algorithm, Feedback-Feedforward Alignment (FFA), that aligns feedforward and feedback pathways in neural networks to co-optimize tasks and enable emergent visual inference functions similar to those in natural vision.</p><hr><h3>Optimal Transport for Treatment Effect Estimation</h3>
<p>Hao Wang, Jiajun Fan, Zhichao Chen, Haoxuan Li, Weiming Liu, Tianqiao Liu, Quanyu Dai, Yichao Wang, Zhenhua Dong, Ruiming Tang</p>
<p><a href='https://openreview.net/forum?id=DAdfU1ASLb'>https://openreview.net/forum?id=DAdfU1ASLb</a></p>
<p><b>Keywords</b>: treatment effect estimation, optimal transport, wasserstein, causal inference, counterfactual
</p><p><b>Compressor summary</b>: The paper proposes a new method called Entire Space CounterFactual Regression (ESCFR) to better estimate individual treatment effects from observational data by addressing two common issues in existing methods.</p><hr><h3>A Smooth Binary Mechanism for Efficient Private Continual Observation</h3>
<p>Joel Daniel Andersson, Rasmus Pagh</p>
<p><a href='https://openreview.net/forum?id=DAKAkMhjSR'>https://openreview.net/forum?id=DAKAkMhjSR</a></p>
<p><b>Keywords</b>: differential privacy, continual observation, binary mechanism
</p><p><b>Compressor summary</b>: The paper proposes an efficient alternative method for releasing differentially private estimates based on a dataset that evolves over time, improving on previous methods in terms of noise distribution and computation time.</p><hr><h3>MGDD: A Meta Generator for Fast Dataset Distillation</h3>
<p>Songhua Liu, Xinchao Wang</p>
<p><a href='https://openreview.net/forum?id=D9CMRR5Lof'>https://openreview.net/forum?id=D9CMRR5Lof</a></p>
<p><b>Keywords</b>: Dataset Distillation, Dataset Condensation, Efficient Learning, Conditional Generation, Meta Learning
</p><p><b>Compressor summary</b>: The paper introduces a faster and more flexible dataset distillation method using a generative approach and meta-learning, achieving significant speedup and improvement over existing techniques.</p><hr><h3>A One-Size-Fits-All Approach to Improving Randomness in Paper Assignment</h3>
<p>Yixuan Even Xu, Steven Jecmen, Zimeng Song, Fei Fang</p>
<p><a href='https://openreview.net/forum?id=D94QKZA7UP'>https://openreview.net/forum?id=D94QKZA7UP</a></p>
<p><b>Keywords</b>: peer review, randomized paper assignment, mitigating malicious behavior, convex optimization
</p><p><b>Compressor summary</b>: The paper proposes a practical method for randomized paper assignment in peer review processes that balances various considerations like expertise, robustness, diversity, and anonymity.</p><hr><h3>Fairness-guided Few-shot Prompting for Large Language Models</h3>
<p>Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, Bingzhe Wu</p>
<p><a href='https://openreview.net/forum?id=D8oHQ2qSTj'>https://openreview.net/forum?id=D8oHQ2qSTj</a></p>
<p><b>Keywords</b>: large language models, prompts, classification
</p><p><b>Compressor summary</b>: The paper introduces a metric to measure predictive bias of prompts for in-context learning and proposes a search strategy to find near-optimal prompts, improving the performance on various downstream tasks.</p><hr><h3>On Transfer of Adversarial Robustness from Pretraining to Downstream Tasks</h3>
<p>Laura Fee Nern, Harsh Raj, Maurice Georgi, Yash Sharma</p>
<p><a href='https://openreview.net/forum?id=D8nAMRRCLS'>https://openreview.net/forum?id=D8nAMRRCLS</a></p>
<p><b>Keywords</b>: Machine Learning Theory, Transfer Learning, Adversarial Robustness
</p><p><b>Compressor summary</b>: The study shows how to measure and improve the robustness of linear predictors on downstream tasks by using representations with desired robustness properties.</p><hr><h3>Optimal Transport Model Distributional Robustness</h3>
<p>Van-Anh Nguyen, Trung Le, Anh Tuan Bui, Thanh-Toan Do, Dinh Phung</p>
<p><a href='https://openreview.net/forum?id=D7LdL2SCCi'>https://openreview.net/forum?id=D7LdL2SCCi</a></p>
<p><b>Keywords</b>: Distributional Robustness, Sharpness-aware, SAM
</p><p><b>Compressor summary</b>: The paper proposes an optimal transport-based distributional robustness framework for training deep learning models that can be applied to single, ensemble, or Bayesian models and shows improved performance in various experiments.</p><hr><h3>Flow Matching for Scalable Simulation-Based Inference</h3>
<p>Jonas Bernhard Wildberger, Maximilian Dax, Simon Buchholz, Stephen R Green, Jakob H. Macke, Bernhard Schölkopf</p>
<p><a href='https://openreview.net/forum?id=D2cS6SoYlP'>https://openreview.net/forum?id=D2cS6SoYlP</a></p>
<p><b>Keywords</b>: simulation-based inference, likelihood-free inference, machine learning for physical sciences
</p><p><b>Compressor summary</b>: FMPE is a continuous normalizing flows technique for simulation-based inference that offers flexibility, exact density evaluation, fast training, and scalability, and outperforms discrete flows methods in gravitational-wave inference.</p><hr><h3>Temporal Dynamic Quantization for Diffusion Models</h3>
<p>Junhyuk So, Jungwon Lee, Daehyun Ahn, Hyungjun Kim, Eunhyeok Park</p>
<p><a href='https://openreview.net/forum?id=D1sECc9fiG'>https://openreview.net/forum?id=D1sECc9fiG</a></p>
<p><b>Keywords</b>: deep learning optimization, quantization, diffusion model, generative model
</p><p><b>Compressor summary</b>: The paper introduces a novel quantization method for diffusion models that adapts to time steps, improving performance without increasing computation cost.</p><hr><h3>Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning</h3>
<p>Yihua Zhang, Yimeng Zhang, Aochuan Chen, Jinghan Jia, Jiancheng Liu, Gaowen Liu, Mingyi Hong, Shiyu Chang, Sijia Liu</p>
<p><a href='https://openreview.net/forum?id=D0MII7rP3R'>https://openreview.net/forum?id=D0MII7rP3R</a></p>
<p><b>Keywords</b>: Dataset pruning, transfer learning
</p><p><b>Compressor summary</b>: The authors propose new dataset pruning methods for transfer learning that reduce data usage by up to 80% without compromising performance on downstream tasks, leading to significant speed-ups during pretraining.</p><hr><h3>Optimal cross-learning for contextual bandits with unknown context distributions</h3>
<p>Jon Schneider, Julian Zimmert</p>
<p><a href='https://openreview.net/forum?id=CzkOzKWpMa'>https://openreview.net/forum?id=CzkOzKWpMa</a></p>
<p><b>Keywords</b>: bandits, first-price auction, sleeping bandits, contextual bandits
</p><p><b>Compressor summary</b>: The paper proposes an efficient algorithm for designing contextual bandit algorithms with a nearly tight regret bound, which removes correlations between estimation of the unknown context distribution and the actions played by the algorithm.</p><hr><h3>Coneheads: Hierarchy Aware Attention</h3>
<p>Albert Tseng, Tao Yu, Toni J.B. Liu, Christopher De Sa</p>
<p><a href='https://openreview.net/forum?id=CzAFnfwbGd'>https://openreview.net/forum?id=CzAFnfwbGd</a></p>
<p><b>Keywords</b>: Hyperbolic Entailment Cones, Hyperbolic Space, Entailment Cones, Attention, Dot Product, Hierarchy, Transformers
</p><p><b>Compressor summary</b>: Cone attention is a new attention mechanism for neural networks that uses hyperbolic cones to model the hierarchy between data points, improving performance over dot product attention and requiring fewer parameters.</p><hr><h3>Exploring and Interacting with the Set of Good Sparse Generalized Additive Models</h3>
<p>Chudi Zhong, Zhi Chen, Jiachang Liu, Margo Seltzer, Cynthia Rudin</p>
<p><a href='https://openreview.net/forum?id=CzAAbKOHQW'>https://openreview.net/forum?id=CzAAbKOHQW</a></p>
<p><b>Keywords</b>: Interpretability, human-model interaction, generalized additive model, Rashomon set
</p><p><b>Compressor summary</b>: The text discusses how approximating the Rashomon set of models allows for better interaction between machine learning models and domain experts, and presents algorithms to efficiently approximate this set for sparse, generalized additive models with fixed support sets.</p><hr><h3>Neural Priming for Sample-Efficient Adaptation</h3>
<p>Matthew Wallingford, Vivek Ramanujan, Alex Fang, Aditya Kusupati, Roozbeh Mottaghi, Aniruddha Kembhavi, Ludwig Schmidt, Ali Farhadi</p>
<p><a href='https://openreview.net/forum?id=Cyn1PvuZsB'>https://openreview.net/forum?id=Cyn1PvuZsB</a></p>
<p><b>Keywords</b>: Transfer Learning, Distribution Shift, Test-Time Training
</p><p><b>Compressor summary</b>: Neural Priming is a technique that adapts pretrained models to new tasks and distributions using recalled data from previous training, improving accuracy in various benchmarks.</p><hr><h3>SALSA VERDE: a machine learning attack on LWE with sparse small secrets</h3>
<p>Cathy Yuanchen Li, Emily Wenger, Zeyuan Allen-Zhu, Francois Charton, Kristin E. Lauter</p>
<p><a href='https://openreview.net/forum?id=CxzCoFDeQf'>https://openreview.net/forum?id=CxzCoFDeQf</a></p>
<p><b>Keywords</b>: machine learning, cryptography, cryptanalysis
</p><p><b>Compressor summary</b>: VERDE is an improved machine learning attack on the Learning with Errors problem, which can recover sparse secrets and work with larger dimensions and smaller moduli, while using less time and power.</p><hr><h3>Neural Circuits for Fast Poisson Compressed Sensing in the Olfactory Bulb</h3>
<p>Jacob A Zavatone-Veth, Paul Masset, William Lingxiao Tong, Joseph Zak, Venkatesh N Murthy, Cengiz Pehlevan</p>
<p><a href='https://openreview.net/forum?id=Cxn1FpnNvG'>https://openreview.net/forum?id=Cxn1FpnNvG</a></p>
<p><b>Keywords</b>: Olfaction, Bayesian inference, neural circuits, normative models, population geometry
</p><p><b>Compressor summary</b>: The proposed rate-based Poisson compressed sensing circuit model for the olfactory bulb accurately detects tens of odors within a single sniff and performs uncertainty estimation by matching receptor properties with the geometry of the neural code.</p><hr><h3>Non-Asymptotic Analysis of a UCB-based Top Two Algorithm</h3>
<p>Marc Jourdan, Rémy Degenne</p>
<p><a href='https://openreview.net/forum?id=CxjmYRP9Ji'>https://openreview.net/forum?id=CxjmYRP9Ji</a></p>
<p><b>Keywords</b>: multi-armed bandits, best-arm identification, Gaussian bandits, Top Two algorithm, fixed confidence, finite confidence
</p><p><b>Compressor summary</b>: The paper derives the first non-asymptotic upper bound on the expected sample complexity of a Top Two bandit algorithm with any error level, and proposes a UCB-based algorithm that has both theoretical guarantees and good empirical performance.</p><hr><h3>Diffusion Probabilistic Models for Structured Node Classification</h3>
<p>Hyosoon Jang, Seonghyun Park, Sangwoo Mo, Sungsoo Ahn</p>
<p><a href='https://openreview.net/forum?id=CxUuCydMDU'>https://openreview.net/forum?id=CxUuCydMDU</a></p>
<p><b>Keywords</b>: diffusion model, graph neural network, structured prediction, node classification
</p><p><b>Compressor summary</b>: The paper proposes a new framework for structured node classification on graphs that considers label dependencies and uses a diffusion probabilistic model to learn from partially labeled data.</p><hr><h3>Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization</h3>
<p>Jameel Hassan Abdul Samadh, Hanan Gani, Noor Hazim Hussein, Muhammad Uzair Khattak, Muzammal Naseer, Fahad Khan, Salman Khan</p>
<p><a href='https://openreview.net/forum?id=CusNOTRkQw'>https://openreview.net/forum?id=CusNOTRkQw</a></p>
<p><b>Keywords</b>: Vision-Language models, Prompt Learning, Test-Time Adaptation
</p><p><b>Compressor summary</b>: The paper proposes a method to improve zero-shot generalization of vision-language models using prompt tuning to align test sample statistics with source data, achieving better results than existing techniques on domain generalization benchmarks.</p><hr><h3>Online POMDP Planning with Anytime Deterministic Guarantees</h3>
<p>Moran Barenboim, Vadim Indelman</p>
<p><a href='https://openreview.net/forum?id=Cupr2yTFSx'>https://openreview.net/forum?id=Cupr2yTFSx</a></p>
<p><b>Keywords</b>: POMDPs, Planning under uncertainty, Robotics
</p><p><b>Compressor summary</b>: The authors propose a method to find a simplified solution for planning under uncertainty in POMDPs that has deterministic guarantees and can be integrated with existing solvers.</p><hr><h3>Convolution Monge Mapping Normalization for learning on sleep data</h3>
<p>Theo Gnassounou, Rémi Flamary, Alexandre Gramfort</p>
<p><a href='https://openreview.net/forum?id=CuHymkHRus'>https://openreview.net/forum?id=CuHymkHRus</a></p>
<p><b>Keywords</b>: Neuroscience, Domain adaptation, Optimal Transport
</p><p><b>Compressor summary</b>: In many machine learning applications on signals and biomedical data, especially electroencephalogram (EEG), one major challenge is the variability of the data across subjects, sessions, and hardware devices. In this work, we propose a new method called Convolutional Monge Mapping Normalization ($\texttt{CMMN}$), which consists in filtering the signals in order to adapt their power spectrum density (PSD) to a Wasserstein barycenter estimated on training data. $\texttt{CMMN}$ relies on novel closed-form solutions for optimal transport mappings and barycenters and provides individual test time adaptation to new data without needing to retrain a prediction model. Numerical experiments on sleep EEG data show that $\texttt{CMMN}$ leads to significant and consistent performance gains independent from the neural network architecture when adapting between subjects, sessions, and even datasets collected with different hardware. Notably our performance gain is on par with much more numerically intensive Domain Adaptation (DA) methods and can be used in conjunction with those for even better performances.</p><hr><h3>V-InFoR: A Robust Graph Neural Networks Explainer for Structurally Corrupted Graphs</h3>
<p>Senzhang Wang, Jun Yin, Chaozhuo Li, Xing Xie, Jianxin Wang</p>
<p><a href='https://openreview.net/forum?id=CtXXOaxDw7'>https://openreview.net/forum?id=CtXXOaxDw7</a></p>
<p><b>Keywords</b>: Explainable AI, Graph Neural Networks, Machine Learning
</p><p><b>Compressor summary</b>: V-InfoR is a robust graph neural network (GNN) explainer that uses variational inference to extract graph representations and graph information bottleneck optimization to generate explanations for corrupted graphs.</p><hr><h3>Saving 100x Storage: Prototype Replay for Reconstructing Training Sample Distribution in Class-Incremental Semantic Segmentation</h3>
<p>Jinpeng Chen, Runmin Cong, Yuxuan LUO, Horace Ip, Sam Kwong</p>
<p><a href='https://openreview.net/forum?id=Ct0zPIe3xs'>https://openreview.net/forum?id=Ct0zPIe3xs</a></p>
<p><b>Keywords</b>: Continual learning, Class-incremental semantic segmentation, Prototype replay
</p><p><b>Compressor summary</b>: STAR is a method for semantic segmentation that addresses class imbalance by storing compact prototypes and repeating background pixels, achieving better performance with less storage than previous methods.</p><hr><h3>Imitation Learning from Vague Feedback</h3>
<p>Xin-Qiang Cai, Yu-Jie Zhang, Chao-Kai Chiang, Masashi Sugiyama</p>
<p><a href='https://openreview.net/forum?id=CswEebv5Hn'>https://openreview.net/forum?id=CswEebv5Hn</a></p>
<p><b>Keywords</b>: imitation learning, vague feedback, risk rewriting, mixture propotion estimation
</p><p><b>Compressor summary</b>: The paragraph discusses imitation learning from human feedback, which aims to train agents using relative comparisons of demonstrations, and proposes a method for handling vague feedback in this setting.</p><hr><h3>List and Certificate Complexities in Replicable Learning</h3>
<p>Peter Dixon, A. Pavan, Jason Vander Woude, N V Vinodchandran</p>
<p><a href='https://openreview.net/forum?id=Cs9ea2Gbgx'>https://openreview.net/forum?id=Cs9ea2Gbgx</a></p>
<p><b>Keywords</b>: Replicability, learning algorithms, sample complexity, PAC Learning
</p><p><b>Compressor summary</b>: The authors study replicable learning algorithms and design optimal list and certificate complexities for estimating coin biases and learning hypothesis classes with nonadaptive statistical queries, while minimizing sample complexity.</p><hr><h3>Finding Local Minima Efficiently in Decentralized Optimization</h3>
<p>Wenhan Xian, Heng Huang</p>
<p><a href='https://openreview.net/forum?id=CruxS0C0LS'>https://openreview.net/forum?id=CruxS0C0LS</a></p>
<p><b>Keywords</b>: second-order optimality, decentralized optimization
</p><p><b>Compressor summary</b>: The paper introduces PEDESTAL, a new decentralized stochastic algorithm for nonconvex optimization that achieves second-order optimality with theoretical guarantees and matches state-of-the-art results in centralized methods.</p><hr><h3>On the Relationship Between Relevance and Conflict in Online Social Link Recommendations</h3>
<p>Yanbang Wang, Jon Kleinberg</p>
<p><a href='https://openreview.net/forum?id=CrpL8mGa0Q'>https://openreview.net/forum?id=CrpL8mGa0Q</a></p>
<p><b>Keywords</b>: social networks, spectral analysis, link recommendation, polarization and conflict
</p><p><b>Compressor summary</b>: The paper analyzes how link recommendations in online social networks affect relevance and conflict, finding that some algorithms reduce conflict more effectively than others.</p><hr><h3>HiNeRV: Video Compression with Hierarchical Encoding-based Neural Representation</h3>
<p>Ho Man Kwan, Ge Gao, Fan Zhang, Andy Gower, David Bull</p>
<p><a href='https://openreview.net/forum?id=CpoS56pYnU'>https://openreview.net/forum?id=CpoS56pYnU</a></p>
<p><b>Keywords</b>: Video compression, Implicit neural representations
</p><p><b>Compressor summary</b>: The paper introduces HiNeRV, a new Implicit Neural Representation for video compression that combines light weight layers with hierarchical positional encodings and achieves significant improvement over existing methods.</p><hr><h3>Newton–Cotes Graph Neural Networks: On the Time Evolution of Dynamic Systems</h3>
<p>Lingbing Guo, Weiqing Wang, Zhuo Chen, Ningyu Zhang, Zequn Sun, Yixuan Lai, Qiang Zhang, Huajun Chen</p>
<p><a href='https://openreview.net/forum?id=CnvZ7FIyAD'>https://openreview.net/forum?id=CnvZ7FIyAD</a></p>
<p><b>Keywords</b>: Equivariant Graph Neural Networks, Molecular Dynamics, N-body System, Human Motion
</p><p><b>Compressor summary</b>: The paper introduces a new method for predicting the future state of system dynamics using velocity estimations and Newton–Cotes formulas, which improves upon existing GNNs-based approaches.</p><hr><h3>Reusable Slotwise Mechanisms</h3>
<p>Trang Nguyen, Amin Mansouri, Kanika Madan, Nguyen Duy Khuong, Kartik Ahuja, Dianbo Liu, Yoshua Bengio</p>
<p><a href='https://openreview.net/forum?id=CniUitfEY3'>https://openreview.net/forum?id=CniUitfEY3</a></p>
<p><b>Keywords</b>: Out-of-Distribution Generalization, Slotwise, Visual Reasoning, Video Prediction, Reusable Mechanism, Dynamics modeling
</p><p><b>Compressor summary</b>: The paper proposes a framework called Reusable Slotwise Mechanisms (RSM) that models object dynamics using communication among slots and a modular architecture, achieving improved robustness, generalization, and performance in various tasks.</p><hr><h3>When Visual Prompt Tuning Meets Source-Free Domain Adaptive Semantic Segmentation</h3>
<p>Xinhong Ma, Yiming Wang, Hao Liu, Tianyu Guo, Yunhe Wang</p>
<p><a href='https://openreview.net/forum?id=ChGGbmTNgE'>https://openreview.net/forum?id=ChGGbmTNgE</a></p>
<p><b>Keywords</b>: unsupervised domain adaptation, semantic segmentation, visual prompt tuning
</p><p><b>Compressor summary</b>: The text introduces a new method (Uni-UVPT) for adapting a semantic segmentation model to an unlabeled target domain without using the source data, by using a lightweight prompt adapter and a pseudo-label correction strategy.</p><hr><h3>PAPR: Proximity Attention Point Rendering</h3>
<p>Yanshu Zhang, Shichong Peng, Seyed Alireza Moazenipourasil, Ke Li</p>
<p><a href='https://openreview.net/forum?id=CgJJvuLjec'>https://openreview.net/forum?id=CgJJvuLjec</a></p>
<p><b>Keywords</b>: point cloud learning, point cloud rendering
</p><p><b>Compressor summary</b>: PAPR is a novel point-based scene representation and differentiable renderer that learns accurate scene geometry and texture using a parsimonious set of points and demonstrates practical applications.</p><hr><h3>Smooth, exact rotational symmetrization for deep learning on point clouds</h3>
<p>Sergey Pozdnyakov, Michele Ceriotti</p>
<p><a href='https://openreview.net/forum?id=CdSRFn1fVe'>https://openreview.net/forum?id=CdSRFn1fVe</a></p>
<p><b>Keywords</b>: geometric deep learning, point clouds, equivariance, machine learning potentials, GNN, transformer, atomic-scale modeling
</p><p><b>Compressor summary</b>: The authors propose a method to add rotational symmetry to point-cloud models for chemical and materials modeling, and introduce the Point Edge Transformer (PET) architecture that achieves state-of-the-art performance on molecular and solid datasets.</p><hr><h3>Spuriosity Didn’t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features</h3>
<p>Cian Eastwood, Shashank Singh, Andrei Liviu Nicolicioiu, Marin Vlastelica, Julius von Kügelgen, Bernhard Schölkopf</p>
<p><a href='https://openreview.net/forum?id=Cc2fjBBlBD'>https://openreview.net/forum?id=Cc2fjBBlBD</a></p>
<p><b>Keywords</b>: invariant prediction, spurious correlations, out-of-distribution generalization, domain generalization, domain adaptation, test-time domain adaptation
</p><p><b>Compressor summary</b>: The paper proposes a method to improve performance in test domain by using pseudo-labels based on stable features, which are conditionally independent from unstable ones given the label.</p><hr><h3>In-Context Impersonation Reveals Large Language Models' Strengths and Biases</h3>
<p>Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, Zeynep Akata</p>
<p><a href='https://openreview.net/forum?id=CbsJ53LdKc'>https://openreview.net/forum?id=CbsJ53LdKc</a></p>
<p><b>Keywords</b>: large language models, impersonation, vision language models, reasoning
</p><p><b>Compressor summary</b>: The text explores how LLMs can take on different roles by assuming personas based on social identity or domain expertise, and how this affects their performance in various tasks, revealing both strengths and biases.</p><hr><h3>MMGP: a Mesh Morphing Gaussian Process-based machine learning method for regression of physical problems under nonparametrized geometrical variability</h3>
<p>Fabien Casenave, Brian Staber, Xavier Roynard</p>
<p><a href='https://openreview.net/forum?id=Ca78M3awPw'>https://openreview.net/forum?id=Ca78M3awPw</a></p>
<p><b>Keywords</b>: Gaussian process, mesh morphing, mesh parametrization, finite element interpolation, simulation, physics, predictive uncertainties, nonparametrized geometries
</p><p><b>Compressor summary</b>: The authors propose a machine learning method that uses mesh morphing, dimensionality reduction, and Gaussian processes to model physical phenomena in industrial designs without relying on graph neural networks, providing predictive uncertainties and handling large meshes efficiently.</p><hr><h3>Survival Permanental Processes for Survival Analysis with Time-Varying Covariates</h3>
<p>Hideaki Kim</p>
<p><a href='https://openreview.net/forum?id=CYCzfXn6cZ'>https://openreview.net/forum?id=CYCzfXn6cZ</a></p>
<p><b>Keywords</b>: survival analysis, temporal point process, Bayesian estimation, permanental process, representer theorem, kernel method
</p><p><b>Compressor summary</b>: The paper proposes a non-parametric Bayesian survival model for analyzing nonlinear dependence on time-varying covariates using a permanental process, which is fast and accurate.</p><hr><h3>DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models</h3>
<p>XiMing Xing, Chuang Wang, Haitao Zhou, Jing Zhang, Qian Yu, Dong Xu</p>
<p><a href='https://openreview.net/forum?id=CY1xatvEQj'>https://openreview.net/forum?id=CY1xatvEQj</a></p>
<p><b>Keywords</b>: Sketch; Vector Sketch; Sketch Generation; Diffusion Models
</p><p><b>Compressor summary</b>: The paper introduces DiffSketcher, an algorithm that creates vectorized free-hand sketches from natural language input using a pre-trained text-to-image diffusion model and optimizing Bézier curves with a modified loss function.</p><hr><h3>PETAL: Physics Emulation Through Averaged Linearizations for Solving Inverse Problems</h3>
<p>Jihui Jin, Etienne Ollivier, Richard Touret, Matthew McKinley, Karim Sabra, Justin Romberg</p>
<p><a href='https://openreview.net/forum?id=CXrRMfs5eY'>https://openreview.net/forum?id=CXrRMfs5eY</a></p>
<p><b>Keywords</b>: Inverse Problems, Neural Adjoint, Hybrid Machine Learning, Physics
</p><p><b>Compressor summary</b>: The paragraph discusses a new method for recovering an underlying signal using a learned weighted average model that incorporates physics-based linearizations, which improves accuracy and provides better gradient information for the inverse problem of ocean acoustic tomography.</p><hr><h3>SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs</h3>
<p>Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Patrick Murphy, Alexander G Hauptmann, Lu Jiang</p>
<p><a href='https://openreview.net/forum?id=CXPUg86A1D'>https://openreview.net/forum?id=CXPUg86A1D</a></p>
<p><b>Keywords</b>: multimodal, generation, large language model
</p><p><b>Compressor summary</b>: The paper presents SPAE, a technique that allows frozen large language models (LLMs) to perform image understanding and generation tasks by converting raw pixels to lexical tokens.</p><hr><h3>Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</h3>
<p>Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, Hannaneh Hajishirzi</p>
<p><a href='https://openreview.net/forum?id=CSbGXyCswu'>https://openreview.net/forum?id=CSbGXyCswu</a></p>
<p><b>Keywords</b>: Language Model, Reinforcement Learning with Human Feedback, Long-Form Text Generation
</p><p><b>Compressor summary</b>: The paper proposes Fine-Grained RLHF, a framework that uses reward functions based on fine-grained human feedback to train language models for better text generation.</p><hr><h3>Online Clustering of Bandits with Misspecified User Models</h3>
<p>Zhiyong Wang, Jize Xie, Xutong Liu, Shuai Li, John C.S. Lui</p>
<p><a href='https://openreview.net/forum?id=CQuRzAgjg9'>https://openreview.net/forum?id=CQuRzAgjg9</a></p>
<p><b>Keywords</b>: online clustering of bandits
</p><p><b>Compressor summary</b>: This paper introduces two robust clustering of bandits algorithms that can handle misspecified user models, achieving near-optimal regret bounds and outperforming existing methods in simulations.</p><hr><h3>Unbiased learning of deep generative models with structured discrete representations</h3>
<p>Harry Bendekgey, Gabriel Hope, Erik B. Sudderth</p>
<p><a href='https://openreview.net/forum?id=CQqBt46FUD'>https://openreview.net/forum?id=CQqBt46FUD</a></p>
<p><b>Keywords</b>: Generative Models, Graphical Models, Variational Inference, Amortized Inference
</p><p><b>Compressor summary</b>: The authors propose a new generative model that combines graphical models and deep learning, handle multimodal uncertainty with latent variables, and improve optimization techniques to learn interpretable discrete data representations.</p><hr><h3>Counting Distinct Elements in the Turnstile Model with Differential Privacy under Continual Observation</h3>
<p>Palak Jain, Iden Kalemaj, Sofya Raskhodnikova, Satchit Sivakumar, Adam Smith</p>
<p><a href='https://openreview.net/forum?id=CQ38aC92WY'>https://openreview.net/forum?id=CQ38aC92WY</a></p>
<p><b>Keywords</b>: distinct elements, differential privacy, continual release, turnstile streams
</p><p><b>Compressor summary</b>: The paper discusses privacy challenges and error bounds for systems that learn from data streams with inserts and deletions, and proposes a new item-level differentially private mechanism with low additive error for streams with low maximum flippancy.</p><hr><h3>Norm-based Generalization Bounds for Sparse Neural Networks</h3>
<p>Tomer Galanti, Mengjia Xu, Liane Galanti, Tomaso Poggio</p>
<p><a href='https://openreview.net/forum?id=COPzNA10hZ'>https://openreview.net/forum?id=COPzNA10hZ</a></p>
<p><b>Keywords</b>: generalization bounds, convolution, rademacher, generalization, sparsity
</p><p><b>Compressor summary</b>: The paper develops norm-based generalization bounds for sparse ReLU neural networks that account for their sparse structure and convolutional filters, leading to tighter bounds than previous methods and better estimation of generalization.</p><hr><h3>Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback</h3>
<p>Jaskirat Singh, Liang Zheng</p>
<p><a href='https://openreview.net/forum?id=CLjBBd8u2j'>https://openreview.net/forum?id=CLjBBd8u2j</a></p>
<p><b>Keywords</b>: Text-to-Image Generation
</p><p><b>Compressor summary</b>: The paper proposes a decompositional approach to evaluate and improve text-to-image alignment, using a novel score that measures how well each assertion in a complex caption aligns with the generated image.</p><hr><h3>A Theory of Transfer-Based Black-Box Attacks: Explanation and Implications</h3>
<p>Yanbo Chen, Weiwei Liu</p>
<p><a href='https://openreview.net/forum?id=CJY7NEXVwC'>https://openreview.net/forum?id=CJY7NEXVwC</a></p>
<p><b>Keywords</b>: Learning Theory
</p><p><b>Compressor summary</b>: This paper proposes a theoretical model that explains the transferability of adversarial examples in black-box attacks and shows that it depends on the curvature of the data manifold.</p><hr><h3>Differentiable Random Partition Models</h3>
<p>Thomas M. Sutter, Alain Ryser, Joram Liebeskind, Julia E Vogt</p>
<p><a href='https://openreview.net/forum?id=CJWQGDwa6u'>https://openreview.net/forum?id=CJWQGDwa6u</a></p>
<p><b>Keywords</b>: random partition model, continuous relaxation, reparameterization, generative models, vae, representation learning, weak supervision, variational clustering, deep learning
</p><p><b>Compressor summary</b>: The paper proposes a novel method for inferring partitions in machine learning problems that allows gradient-based optimization and shows its versatility on three different challenging experiments.</p><hr><h3>Automatic Grouping for Efficient Cooperative Multi-Agent Reinforcement Learning</h3>
<p>Yifan Zang, Jinmin He, Kai Li, Haobo Fu, QIANG FU, Junliang Xing, Jian Cheng</p>
<p><a href='https://openreview.net/forum?id=CGj72TyGJy'>https://openreview.net/forum?id=CGj72TyGJy</a></p>
<p><b>Keywords</b>: MARL, Cooperative Multi-Agent Reinforcement Learning, Coordination and Cooperation, Automatic Grouping, Group-Wise Learning
</p><p><b>Compressor summary</b>: The paper introduces GoMARL, a novel multi-agent reinforcement learning approach that learns automatic grouping for efficient cooperation in natural systems.</p><hr><h3>Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization</h3>
<p>Yuxin Guo, Shijie Ma, Hu Su, Zhiqing Wang, Yuhao Zhao, Wei Zou, Siyang Sun, Yun Zheng</p>
<p><a href='https://openreview.net/forum?id=CFhpBJ8eZ5'>https://openreview.net/forum?id=CFhpBJ8eZ5</a></p>
<p><b>Keywords</b>: audio-visual learning, audio-visual source localization, semi-supervised learning
</p><p><b>Compressor summary</b>: The paper proposes a semi-supervised learning framework called Dual Mean-Teacher for audio-visual source localization, which improves localization accuracy by using two teachers to generate high-quality pseudo-labels and outperforms state-of-the-art methods with limited labeled data.</p><hr><h3>Adversarially Robust Learning with Uncertain Perturbation Sets</h3>
<p>Tosca Lechner, Vinayak Pathak, Ruth Urner</p>
<p><a href='https://openreview.net/forum?id=CFQBcz7k8n'>https://openreview.net/forum?id=CFQBcz7k8n</a></p>
<p><b>Keywords</b>: adversarially robust learning
</p><p><b>Compressor summary</b>: The paper studies a new setting of learning with imperfect knowledge of adversarial perturbation sets and proposes methods to improve robustness against them.</p><hr><h3>Meet in the Middle: A New Pre-training Paradigm</h3>
<p>Anh Tuan Nguyen, Nikos Karampatziakis, Weizhu Chen</p>
<p><a href='https://openreview.net/forum?id=CEk6JK71Mb'>https://openreview.net/forum?id=CEk6JK71Mb</a></p>
<p><b>Keywords</b>: language modeling, pre-training, deep learning, NLP
</p><p><b>Compressor summary</b>: The paper introduces a new language model pre-training method called Meet in the Middle (MIM) that trains models in both left-to-right and right-to-left directions, improving data efficiency and performance on generation and infilling tasks.</p><hr><h3>Refined Mechanism Design for Approximately Structured Priors via Active Regression</h3>
<p>Christos Boutsikas, Petros Drineas, Marios Mertzanidis, Alexandros Psomas, Paritosh Verma</p>
<p><a href='https://openreview.net/forum?id=CDTifMbUNc'>https://openreview.net/forum?id=CDTifMbUNc</a></p>
<p><b>Keywords</b>: mechanism design, revenue maximization, randomized linear algebra, active regression
</p><p><b>Compressor summary</b>: This paper proposes a method to design revenue-maximizing mechanisms for sellers with many items and strategic bidders, using topic models and active learning techniques from randomized linear algebra.</p><hr><h3>Video Dynamics Prior: An Internal Learning Approach for Robust Video Enhancements</h3>
<p>Gaurav Shrivastava, Ser-Nam Lim, Abhinav Shrivastava</p>
<p><a href='https://openreview.net/forum?id=CCq73CGMyV'>https://openreview.net/forum?id=CCq73CGMyV</a></p>
<p><b>Keywords</b>: Computational Photography, Deep Internal Learning, low-level vision, video denoising, video super-resolution, video frame interpolation, video inpainting
</p><p><b>Compressor summary</b>: The paper proposes a robust framework for low-level vision tasks that learns from corrupted test sequences and uses a novel spatial pyramid loss to handle noise and degradation in videos, achieving state-of-the-art results on several tasks.</p><hr><h3>Equivariant Neural Simulators for Stochastic Spatiotemporal Dynamics</h3>
<p>Koen Minartz, Yoeri Poels, Simon Martinus Koop, Vlado Menkovski</p>
<p><a href='https://openreview.net/forum?id=CCVsGbhFdj'>https://openreview.net/forum?id=CCVsGbhFdj</a></p>
<p><b>Keywords</b>: stochastic simulation, equivariance, dynamical systems, probabilistic simulation, generative models
</p><p><b>Compressor summary</b>: Equivariant Probabilistic Neural Simulation (EPNS) is a framework for simulating stochastic systems with improved accuracy, efficiency, and stability by incorporating domain symmetries into probabilistic neural networks.</p><hr><h3>Neural Data Transformer 2: Multi-context Pretraining for Neural Spiking Activity</h3>
<p>Joel Ye, Jennifer L Collinger, Leila Wehbe, Robert Gaunt</p>
<p><a href='https://openreview.net/forum?id=CBBtMnlTGq'>https://openreview.net/forum?id=CBBtMnlTGq</a></p>
<p><b>Keywords</b>: Pretraining, Scaling Laws, Neuroscience, Brain-computer interfaces
</p><p><b>Compressor summary</b>: NDT2 is a spatiotemporal Transformer that uses unsupervised pretraining to learn rich representations from neural spiking data across sessions, subjects, and tasks for iBCI control.</p><hr><h3>Information Maximization Perspective of Orthogonal Matching Pursuit with Applications to Explainable AI</h3>
<p>Aditya Chattopadhyay, Ryan Pilgrim, Rene Vidal</p>
<p><a href='https://openreview.net/forum?id=CAF4CnUblx'>https://openreview.net/forum?id=CAF4CnUblx</a></p>
<p><b>Keywords</b>: Information Maximization, Sparse Coding, Orthogonal Matching Pursuit, Explainable AI, Information Pursuit
</p><p><b>Compressor summary</b>: The paper explores using Orthogonal Matching Pursuit (OMP) as an alternative to Information Pursuit (IP) for greedily selecting queries and shows how IP with random projections can almost reduce to OMP, leading to a simple explainable AI algorithm that encodes images as sparse combinations of semantically meaningful dictionary atoms.</p><hr><h3>Pointwise uncertainty quantification for sparse variational Gaussian process regression with a Brownian motion prior</h3>
<p>Luke Travis, Kolyan Ray</p>
<p><a href='https://openreview.net/forum?id=CA8tMQiscx'>https://openreview.net/forum?id=CA8tMQiscx</a></p>
<p><b>Keywords</b>: Gaussian process, sparse variational Bayes, uncertainty quantification, theoretical guarantees
</p><p><b>Compressor summary</b>: The paper investigates how well a specific type of Gaussian process prior performs in estimating and quantifying uncertainty, and under what conditions it is accurate or misleading.</p><hr><h3>Bicriteria Multidimensional Mechanism Design with Side Information</h3>
<p>Siddharth Prasad, Nina Balcan, Tuomas Sandholm</p>
<p><a href='https://openreview.net/forum?id=C9wlNF1Ooj'>https://openreview.net/forum?id=C9wlNF1Ooj</a></p>
<p><b>Keywords</b>: mechanism design, revenue maximization, welfare maximization, side information, weakest competitors, algorithms with predictions, learning-augmented algorithms
</p><p><b>Compressor summary</b>: The paper presents a new method for multidimensional mechanism design using side information, which can generate high social welfare and revenue while accounting for errors and varying quality of the input data.</p><hr><h3>Causal discovery from observational and interventional data across multiple environments</h3>
<p>Adam Li, Amin Jaber, Elias Bareinboim</p>
<p><a href='https://openreview.net/forum?id=C9wTM5xyw2'>https://openreview.net/forum?id=C9wTM5xyw2</a></p>
<p><b>Keywords</b>: causal inference, causal discovery, transportability, multi-domain learning
</p><p><b>Compressor summary</b>: The paper proposes an approach to learn causal structures in non-Markovian systems using data from multiple domains, by connecting interventional distributions with graphical criteria and introducing a new causal discovery algorithm called S-FCI.</p><hr><h3>Fast Projected Newton-like Method for Precision Matrix Estimation under Total Positivity</h3>
<p>Jian-Feng CAI, José Vinícius De Miranda Cardoso, Daniel P. Palomar, Jiaxi Ying</p>
<p><a href='https://openreview.net/forum?id=C9cgwmJ8Pt'>https://openreview.net/forum?id=C9cgwmJ8Pt</a></p>
<p><b>Keywords</b>: MTP2, Total Positivity, Generalized graph Laplacian, Precision matrix estimation, Nonnegative partial correlations
</p><p><b>Compressor summary</b>: The paper proposes a new algorithm for estimating precision matrices in Gaussian distributions with specific properties, which is faster and theoretically proven to converge.</p><hr><h3>Conservative Offline Policy Adaptation in Multi-Agent Games</h3>
<p>Chengjie Wu, Pingzhong Tang, Jun Yang, Yujing Hu, Tangjie Lv, Changjie Fan, Chongjie Zhang</p>
<p><a href='https://openreview.net/forum?id=C8pvL8Qbfa'>https://openreview.net/forum?id=C8pvL8Qbfa</a></p>
<p><b>Keywords</b>: reinforcement learning, opponent exploitation, multi-agent
</p><p><b>Compressor summary</b>: The paper introduces a novel approach for offline policy adaptation in multi-agent games using conservative reinforcement learning that can exploit weaknesses or enable cooperation without online interaction.</p><hr><h3>Towards Label-free Scene Understanding by Vision Foundation Models</h3>
<p>Runnan Chen, Youquan Liu, Lingdong Kong, Nenglun Chen, Xinge ZHU, Yuexin Ma, Tongliang Liu, Wenping Wang</p>
<p><a href='https://openreview.net/forum?id=C8JdyM7B8I'>https://openreview.net/forum?id=C8JdyM7B8I</a></p>
<p><b>Keywords</b>: label-free, scene understanding
</p><p><b>Compressor summary</b>: The paper explores using CLIP and SAM for label-free scene understanding and proposes a novel method to supervise 2D and 3D networks with noisy labels from these models, improving performance on several datasets.</p><hr><h3>Self-Consistent Velocity Matching of Probability Flows</h3>
<p>Lingxiao Li, Samuel Hurault, Justin Solomon</p>
<p><a href='https://openreview.net/forum?id=C6fvJ2RfsL'>https://openreview.net/forum?id=C6fvJ2RfsL</a></p>
<p><b>Keywords</b>: JKO, mass-conservation, PDE, Fokker-Planck, scalable, discretization-free, neural ODE
</p><p><b>Compressor summary</b>: The authors propose a scalable framework for solving mass-conserving PDEs using an iterative formulation with a biased gradient estimator that overcomes computational challenges and outperforms existing methods.</p><hr><h3>Dynamic Sparsity Is Channel-Level Sparsity Learner</h3>
<p>Lu Yin, Gen Li, Meng Fang, Li Shen, Tianjin Huang, Zhangyang Wang, Vlado Menkovski, Xiaolong Ma, Mykola Pechenizkiy, Shiwei Liu</p>
<p><a href='https://openreview.net/forum?id=C6IIwFHWkF'>https://openreview.net/forum?id=C6IIwFHWkF</a></p>
<p><b>Keywords</b>: dynamic sparsity, dynamic sparse training, sparse training
</p><p><b>Compressor summary</b>: Chase is a new dynamic sparse training method that converts unstructured sparsity to GPU-friendly channel-level sparsity, improving inference speedup without sacrificing accuracy.</p><hr><h3>Clustering the Sketch: Dynamic Compression for Embedding Tables</h3>
<p>Henry Tsang, Thomas Dybdahl Ahle</p>
<p><a href='https://openreview.net/forum?id=C4rRqkXFyC'>https://openreview.net/forum?id=C4rRqkXFyC</a></p>
<p><b>Keywords</b>: Embedding table compression, Clustering and sketching, Memory-efficient training
</p><p><b>Compressor summary</b>: CCE combines clustering-based compression with dynamic methods to efficiently represent large embedding tables for recommendation systems, and has theoretical convergence guarantees.</p><hr><h3>On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective</h3>
<p>Mathieu Serrurier, Franck Mamalet, Thomas FEL, Louis Béthune, Thibaut Boissin</p>
<p><a href='https://openreview.net/forum?id=ByDy2mlkig'>https://openreview.net/forum?id=ByDy2mlkig</a></p>
<p><b>Keywords</b>: 1-Lipschitz neural network, explicability
</p><p><b>Compressor summary</b>: The paper shows that 1-Lipschitz neural networks, trained with optimal transportation problem's dual loss, produce saliency maps with low noise and high XAI properties, aligning well with human explanations and allowing counterfactual explanations that reveal the direction of transportation plans.</p><hr><h3>Lookup Table meets Local Laplacian Filter: Pyramid Reconstruction Network for Tone Mapping</h3>
<p>Feng Zhang, Ming Tian, Zhiqiang Li, Bin Xu, Qingbo Lu, Changxin Gao, Nong Sang</p>
<p><a href='https://openreview.net/forum?id=BxqPN7KuQS'>https://openreview.net/forum?id=BxqPN7KuQS</a></p>
<p><b>Keywords</b>: tone mapping; learnable local laplacian filter; laplacian pyramid; 3D lookup table
</p><p><b>Compressor summary</b>: The paper proposes a novel strategy for converting HDR to LDR images using image-adaptive 3D LUTs and local Laplacian filters learned from annotated data, achieving better tone mapping and edge preservation than existing methods.</p><hr><h3>Self-Evaluation Guided Beam Search for Reasoning</h3>
<p>Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, Qizhe Xie</p>
<p><a href='https://openreview.net/forum?id=Bw82hwg5Q3'>https://openreview.net/forum?id=Bw82hwg5Q3</a></p>
<p><b>Keywords</b>: Large Language Models, Multistep Reasoning, Stochastic Beam Search, LLM Self-Evaluation
</p><p><b>Compressor summary</b>: The paper proposes a decoding algorithm that integrates self-evaluation guidance to improve the reasoning quality of large language models and outperforms baselines on various benchmarks.</p><hr><h3>Improving the Knowledge Gradient Algorithm</h3>
<p>Le Yang, Siyang Gao, Chin Pang Ho</p>
<p><a href='https://openreview.net/forum?id=BvslVXlUvF'>https://openreview.net/forum?id=BvslVXlUvF</a></p>
<p><b>Keywords</b>: best arm identification, knowledge gradient, asymptotic optimality, convergence rate
</p><p><b>Compressor summary</b>: The improved knowledge gradient (iKG) algorithm is a better version of the knowledge gradient (KG) algorithm for identifying the best arm in multiple-arm bandit problems, as it is asymptotically optimal and can handle more variants of the problem.</p><hr><h3>Incomplete Multimodality-Diffused Emotion Recognition</h3>
<p>Yuanzhi Wang, Yong Li, Zhen Cui</p>
<p><a href='https://openreview.net/forum?id=BuGFwUS9B3'>https://openreview.net/forum?id=BuGFwUS9B3</a></p>
<p><b>Keywords</b>: Multimodal emotion recognition, Incomplete multimodalities
</p><p><b>Compressor summary</b>: IMDer is a method to improve multimodal emotion recognition by recovering missing modalities using a score-based diffusion model and embedding available modalities as conditions for semantic alignment.</p><hr><h3>Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression</h3>
<p>Allan Raventos, Mansheej Paul, Feng Chen, Surya Ganguli</p>
<p><a href='https://openreview.net/forum?id=BtAz4a5xDg'>https://openreview.net/forum?id=BtAz4a5xDg</a></p>
<p><b>Keywords</b>: in-context learning, Bayesian inference, transformers, task diversity, emergence
</p><p><b>Compressor summary</b>: The study investigates how task diversity during pretraining affects transformers' ability to learn new linear regression tasks without updating weights, finding a threshold beyond which they can optimally solve unseen tasks.</p><hr><h3>Optimizing Prompts for Text-to-Image Generation</h3>
<p>Yaru Hao, Zewen Chi, Li Dong, Furu Wei</p>
<p><a href='https://openreview.net/forum?id=BsZNWXD3a1'>https://openreview.net/forum?id=BsZNWXD3a1</a></p>
<p><b>Keywords</b>: prompt adaptation, automatic prompt engineering, text-to-image generation
</p><p><b>Compressor summary</b>: Prompt adaptation is a framework that automatically improves text-to-image models by adapting user input to model preferences using supervised fine-tuning and reinforcement learning, resulting in better images that match user intentions.</p><hr><h3>Augmenting Language Models with Long-Term Memory</h3>
<p>Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei</p>
<p><a href='https://openreview.net/forum?id=BryMFPQ4L6'>https://openreview.net/forum?id=BryMFPQ4L6</a></p>
<p><b>Keywords</b>: large language models, long-term memory, long-text modeling and understanding, residual side-network, in-context learning
</p><p><b>Compressor summary</b>: The LongMem framework allows large language models to store and use long past contexts, improving performance on long-context modeling tasks and memory-augmented in-context learning.</p><hr><h3>SANFlow: Semantic-Aware Normalizing Flow for Anomaly Detection</h3>
<p>Daehyun Kim, Sungyong Baik, Tae Hyun Kim</p>
<p><a href='https://openreview.net/forum?id=BqZ70BEtuW'>https://openreview.net/forum?id=BqZ70BEtuW</a></p>
<p><b>Keywords</b>: Anomaly Detection, Visual Anomaly Detection, Computer Vision, Normalizing Flow, Anomaly Localization
</p><p><b>Compressor summary</b>: The authors propose a method to improve anomaly detection using normalizing flow to map different features of an image to different distributions, enhancing discriminability between normal and abnormal data.</p><hr><h3>Designing Robust Transformers using Robust Kernel Density Estimation</h3>
<p>Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, Nhat Ho</p>
<p><a href='https://openreview.net/forum?id=BqTv1Mtuhu'>https://openreview.net/forum?id=BqTv1Mtuhu</a></p>
<p><b>Keywords</b>: Transformers, Kernel Density Estimation, Robustness
</p><p><b>Compressor summary</b>: The paper proposes new transformer architectures that are more robust to adversarial attacks and data contamination by adapting classical kernel density estimation methods to the self-attention mechanism.</p><hr><h3>A Computationally Efficient Sparsified Online Newton Method</h3>
<p>Fnu Devvrit, Sai Surya Duvvuri, Rohan Anil, Vineet Gupta, Cho-Jui Hsieh, Inderjit S Dhillon</p>
<p><a href='https://openreview.net/forum?id=BopG5dhH7L'>https://openreview.net/forum?id=BopG5dhH7L</a></p>
<p><b>Keywords</b>: Optimization, Second order methods; Deep Learning
</p><p><b>Compressor summary</b>: The paper introduces Sparsified Online Newton (SONew), a memory-efficient second-order algorithm that improves convergence and performance for deep neural network training using structured sparsity patterns.</p><hr><h3>Noise-Adaptive Thompson Sampling for Linear Contextual Bandits</h3>
<p>Ruitu Xu, Yifei Min, Tianhao Wang</p>
<p><a href='https://openreview.net/forum?id=BnV2M2WFaY'>https://openreview.net/forum?id=BnV2M2WFaY</a></p>
<p><b>Keywords</b>: Linear Contextual Bandit, Thompson Sampling, Noise-Adaptive
</p><p><b>Compressor summary</b>: The paper proposes a noise-adaptive Thompson sampling-style algorithm for linear contextual bandits with heteroscedastic noise that achieves better regret bounds in both constant-variance and deterministic scenarios.</p><hr><h3>Koopman Kernel Regression</h3>
<p>Petar Bevanda, Max Beier, Armin Lederer, Stefan Georg Sosnowski, Eyke Hüllermeier, Sandra Hirche</p>
<p><a href='https://openreview.net/forum?id=BmIW6U0rz8'>https://openreview.net/forum?id=BmIW6U0rz8</a></p>
<p><b>Keywords</b>: Kernel Methods, Regression, Statistical Learning Theory, Koopman Operator, Mode Decomposition, Dynamical Systems, Supervised Learning
</p><p><b>Compressor summary</b>: The paragraph discusses a new method called Koopman Kernel Regression (KKR) that uses linear time-invariant equations to simplify complex forecasts, providing better learning guarantees and performance than existing approaches.</p><hr><h3>Learning to Discover Skills through Guidance</h3>
<p>Hyunseung Kim, Byungkun Lee, Hojoon Lee, Dongyoon Hwang, Sejik Park, Kyushik Min, Jaegul Choo</p>
<p><a href='https://openreview.net/forum?id=Bkrmr9LjeI'>https://openreview.net/forum?id=Bkrmr9LjeI</a></p>
<p><b>Keywords</b>: Unsupervised skill discovery, Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper proposes a new unsupervised skill discovery algorithm that improves exploration by selecting a guide skill and guiding other skills to follow it while maximizing their discriminability in unexplored states, and shows its effectiveness in challenging environments.</p><hr><h3>How many samples are needed to leverage smoothness?</h3>
<p>Vivien Cabannes, Stefano Vigogna</p>
<p><a href='https://openreview.net/forum?id=BklIgOO76D'>https://openreview.net/forum?id=BklIgOO76D</a></p>
<p><b>Keywords</b>: Statistical learning, breaking the curse, kernel methods
</p><p><b>Compressor summary</b>: This paper studies how smoothness helps overcome the curse of dimensionality in statistical learning and explores the impact of constants and transient periods on generalization error.</p><hr><h3>A normative theory of social conflict</h3>
<p>Sergey A. Shuvaev, Evgeny M Amelchenko, Dmitry Smagin, Natalia Kudryavtseva, Grigori Enikolopov, Alexei A. Koulakov</p>
<p><a href='https://openreview.net/forum?id=BkQM8huiIc'>https://openreview.net/forum?id=BkQM8huiIc</a></p>
<p><b>Keywords</b>: neuroscience, decision-making, normative modeling, game theory, Bayesian methods, POMDP, inverse rational control, belief, theory of mind
</p><p><b>Compressor summary</b>: The study uses neural data from mice to develop a Theory of Mind model explaining how they form beliefs about strength and behavior in social conflict situations.</p><hr><h3>Participatory Personalization in Classification</h3>
<p>Hailey James, Chirag Nagpal, Katherine A Heller, Berk Ustun</p>
<p><a href='https://openreview.net/forum?id=Bj1QSgiBPP'>https://openreview.net/forum?id=Bj1QSgiBPP</a></p>
<p><b>Keywords</b>: healthcare, algorithmic fairness, data privacy, classification, interpretability
</p><p><b>Compressor summary</b>: The authors propose participatory systems, which allow individuals to opt into personalization of prediction models using their group attributes, improving both performance and privacy.</p><hr><h3>Zero-shot causal learning</h3>
<p>Hamed Nilforoshan, Michael Moor, Yusuf H Roohani, Yining Chen, Anja Šurina, Michihiro Yasunaga, Sara Oblak, Jure Leskovec</p>
<p><a href='https://openreview.net/forum?id=BfQJrIiOZC'>https://openreview.net/forum?id=BfQJrIiOZC</a></p>
<p><b>Keywords</b>: causal inference, CATE, CATE estimation, causal machine learning, causal ML, heterogenous treatment effects, causality, potential outcomes, treatment effect
</p><p><b>Compressor summary</b>: CaML is a framework that predicts personalized effects of novel interventions using meta-learning and individual features, outperforming existing methods in medical and cell-line datasets.</p><hr><h3>MG-ViT: A Multi-Granularity Method for Compact and Efficient Vision Transformers</h3>
<p>Yu Zhang, Yepeng Liu, Duoqian Miao, Qi Zhang, Yiwei Shi, Liang Hu</p>
<p><a href='https://openreview.net/forum?id=Bf6WFWNCUP'>https://openreview.net/forum?id=Bf6WFWNCUP</a></p>
<p><b>Keywords</b>: Efficient AI, Vision Transformer, Image Classification, Multi-Granularity, Three-Way Decisions
</p><p><b>Compressor summary</b>: The paper introduces a multi-granularity strategy for compressing Vision Transformers that reduces computational cost without sacrificing performance on classification, detection, and segmentation tasks.</p><hr><h3>The Contextual Lasso: Sparse Linear Models via Deep Neural Networks</h3>
<p>Ryan Thompson, Amir Dezfouli, Robert Kohn</p>
<p><a href='https://openreview.net/forum?id=BdvCo8RVlx'>https://openreview.net/forum?id=BdvCo8RVlx</a></p>
<p><b>Keywords</b>: feature selection, sparsity, sparse regression, varying coefficients, deep learning
</p><p><b>Compressor summary</b>: The contextual lasso is a new method for creating interpretable sparse linear models that adapt to different contexts using a deep neural network and a novel lasso regularizer.</p><hr><h3>LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images</h3>
<p>Viraj Uday Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay, Judy Hoffman</p>
<p><a href='https://openreview.net/forum?id=BbIxB4xnbq'>https://openreview.net/forum?id=BbIxB4xnbq</a></p>
<p><b>Keywords</b>: image classification, robustness, guided diffusion models, counterfactuals
</p><p><b>Compressor summary</b>: The paper introduces an algorithm to create realistic test images based on language input to stress-test visual models and reveal hidden biases.</p><hr><h3>Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets</h3>
<p>Anirudhan Badrinath, Yannis Flet-Berliac, Allen Nie, Emma Brunskill</p>
<p><a href='https://openreview.net/forum?id=BYywOFbRFz'>https://openreview.net/forum?id=BYywOFbRFz</a></p>
<p><b>Keywords</b>: offline reinforcement learning, reinforcement learning via supervised learning, behavioral cloning
</p><p><b>Compressor summary</b>: The Waypoint Transformer is a new reinforcement learning method that improves offline learning by using automatically-generated waypoints to guide the agent's trajectory.</p><hr><h3>Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization</h3>
<p>Xiangsen Wang, Haoran Xu, Yinan Zheng, Xianyuan Zhan</p>
<p><a href='https://openreview.net/forum?id=BXQtgwA2n0'>https://openreview.net/forum?id=BXQtgwA2n0</a></p>
<p><b>Keywords</b>: Offline reinforcement learning; multi-agent reinforcement learning; multi-agent cooperation
</p><p><b>Compressor summary</b>: OMIGA is a new offline multi-agent RL algorithm with implicit global-to-local value regularization that outperforms existing methods on various MuJoCo and StarCraft II tasks.</p><hr><h3>Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields</h3>
<p>Seungtae Nam, Daniel Rho, Jong Hwan Ko, Eunbyung Park</p>
<p><a href='https://openreview.net/forum?id=BW6nZf7TnK'>https://openreview.net/forum?id=BW6nZf7TnK</a></p>
<p><b>Keywords</b>: Novel view synthesis, Neural radiance fields
</p><p><b>Compressor summary</b>: mip-Grid is a novel approach that improves grid-based neural radiance field methods by integrating anti-aliasing techniques, achieving fast training speed and high rendering quality.</p><hr><h3>From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader</h3>
<p>Weiwen Xu, Xin Li, Wenxuan Zhang, Meng Zhou, Wai Lam, Luo Si, Lidong Bing</p>
<p><a href='https://openreview.net/forum?id=BVN9Kgvwzv'>https://openreview.net/forum?id=BVN9Kgvwzv</a></p>
<p><b>Keywords</b>: Machine Reading Comprehension, Pre-training, Natural Language Understanding
</p><p><b>Compressor summary</b>: PMR is a novel method that improves machine reading comprehension models by retrofitting pre-trained masked language models using Wikipedia data and a Wiki Anchor Extraction task, leading to better performance, explainability, and versatility in various extraction and classification tasks.</p><hr><h3>Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing</h3>
<p>Nived Rajaraman, Fnu Devvrit, Aryan Mokhtari, Kannan Ramchandran</p>
<p><a href='https://openreview.net/forum?id=BTRcVP7ZJn'>https://openreview.net/forum?id=BTRcVP7ZJn</a></p>
<p><b>Keywords</b>: Greedy Pruning; Matrix Sensing; Lasso regularization
</p><p><b>Compressor summary</b>: The paper investigates the theory behind pruning and fine-tuning methods for reducing model complexity in overparameterized matrix sensing problems, and shows that pruning columns with low norms leads to good generalization.</p><hr><h3>BQ-NCO: Bisimulation Quotienting for Efficient Neural Combinatorial Optimization</h3>
<p>Darko Drakulic, Sofia Michel, Florian Mai, Arnaud Sors, Jean-Marc Andreoli</p>
<p><a href='https://openreview.net/forum?id=BRqlkTDvvm'>https://openreview.net/forum?id=BRqlkTDvvm</a></p>
<p><b>Keywords</b>: Combinatorial Optimization, Markov Decision Processes, Bisimulation, Policy Learning, Out-of-Distribution Generalization, Routing Problems, TSP, CVRP, KP.
</p><p><b>Compressor summary</b>: The paper proposes a method for solving combinatorial optimization problems using Markov decision processes and bisimulation quotienting to exploit symmetries, achieving state-of-the-art results and better out-of-distribution generalization.</p><hr><h3>Passive learning of active causal strategies in agents and language models</h3>
<p>Andrew Kyle Lampinen, Stephanie C.Y. Chan, Ishita Dasgupta, Andrew Joo Hun Nam, Jane X Wang</p>
<p><a href='https://openreview.net/forum?id=BRpi8YAfac'>https://openreview.net/forum?id=BRpi8YAfac</a></p>
<p><b>Keywords</b>: passive; causal; offline; agency; language models
</p><p><b>Compressor summary</b>: The paragraph discusses how passively-trained agents can learn generalizable strategies for determining and using causal structures by intervening at test time, and how natural language explanations can further enhance their performance in complex environments.</p><hr><h3>Optimal privacy guarantees for a relaxed threat model: Addressing sub-optimal adversaries in differentially private machine learning</h3>
<p>Georgios Kaissis, Alexander Ziller, Stefan Kolek, Anneliese Riess, Daniel Rueckert</p>
<p><a href='https://openreview.net/forum?id=BRSgVw85Mc'>https://openreview.net/forum?id=BRSgVw85Mc</a></p>
<p><b>Keywords</b>: Differential Privacy, Membership Inference Attack, Hypothesis Testing, Data Reconstruction Attack, Security
</p><p><b>Compressor summary</b>: The paper studies how private machine learning models can be attacked by less powerful adversaries with partial data, and suggests ways to set noise levels based on this risk.</p><hr><h3>Identifiable Contrastive Learning with Automatic Feature Importance Discovery</h3>
<p>Qi Zhang, Yifei Wang, Yisen Wang</p>
<p><a href='https://openreview.net/forum?id=BQA7wR2KBF'>https://openreview.net/forum?id=BQA7wR2KBF</a></p>
<p><b>Keywords</b>: Self-supervised Learning, Contrastive Learning, Identifiability, Representation Learning
</p><p><b>Compressor summary</b>: The paper introduces triCL, a contrastive learning method that uses a 3-factor contrast to learn more interpretable and identifiable features by using a learnable diagonal matrix $S$ that captures the importance of each feature.</p><hr><h3>Uncovering and Quantifying Social Biases in Code Generation</h3>
<p>Yan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji Zhang, Daoguang Zan, Jian-Guang Lou, Pin-Yu Chen, Tsung-Yi Ho</p>
<p><a href='https://openreview.net/forum?id=BOP5McdqGy'>https://openreview.net/forum?id=BOP5McdqGy</a></p>
<p><b>Keywords</b>: Social bias, code fairness
</p><p><b>Compressor summary</b>: This paper investigates the social bias problem in automatic code generation tools like Copilot and proposes a method to construct code prompts that reveal and measure these biases.</p><hr><h3>Fast Model DeBias with Machine Unlearning</h3>
<p>Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, YANG FENG, Joey Tianyi Zhou, Jian Wu, Zuozhu Liu</p>
<p><a href='https://openreview.net/forum?id=BL9Pc7xsdX'>https://openreview.net/forum?id=BL9Pc7xsdX</a></p>
<p><b>Keywords</b>: Model Debias, Bias Mitigation, Machine Unlearning, Counterfactual Fairness
</p><p><b>Compressor summary</b>: The paper proposes FMD, an efficient method for identifying, evaluating, and removing biases in deep neural networks using counterfactuals and machine unlearning.</p><hr><h3>Outlier-Robust Gromov-Wasserstein for Graph Data</h3>
<p>Lemin Kong, Jiajin Li, Jianheng Tang, Anthony Man-Cho So</p>
<p><a href='https://openreview.net/forum?id=BKAFLUcpBS'>https://openreview.net/forum?id=BKAFLUcpBS</a></p>
<p><b>Keywords</b>: Gromov Wasserstein, Robust Optimization, Nonconvex Optimization
</p><p><b>Compressor summary</b>: The paper introduces a new robust version of Gromov-Wasserstein distance called RGW that minimizes the impact of outliers in graph learning tasks using efficient algorithms and validates its effectiveness on real data.</p><hr><h3>Retaining Beneficial Information from Detrimental Data for Neural Network Repair</h3>
<p>Long-Kai Huang, Peilin Zhao, Junzhou Huang, Sinno Pan</p>
<p><a href='https://openreview.net/forum?id=BJ1vOqh3hJ'>https://openreview.net/forum?id=BJ1vOqh3hJ</a></p>
<p><b>Keywords</b>: Model Repair; Fine-tuning
</p><p><b>Compressor summary</b>: The paper proposes a new approach to repair deep learning models by identifying and separating harmful data from beneficial data using a clean set and enhancing the model's performance.</p><hr><h3>Recommender Systems with Generative Retrieval</h3>
<p>Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan Hulikal Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q. Tran, Jonah Samost, Maciej Kula, Ed H. Chi, Maheswaran Sathiamoorthy</p>
<p><a href='https://openreview.net/forum?id=BJ0fQUU32w'>https://openreview.net/forum?id=BJ0fQUU32w</a></p>
<p><b>Keywords</b>: Recommender Systems, Generative Retrieval, Vector Quantization
</p><p><b>Compressor summary</b>: The paper proposes a generative retrieval approach using semantically meaningful Semantic IDs for items, which improves recommender system performance and generalization.</p><hr><h3>Flow: Per-instance Personalized Federated Learning</h3>
<p>Kunjal Panchal, Sunav Choudhary, Nisarg Parikh, Lijun Zhang, Hui Guan</p>
<p><a href='https://openreview.net/forum?id=BI031mw7iS'>https://openreview.net/forum?id=BI031mw7iS</a></p>
<p><b>Keywords</b>: federated learning, personalization, statistical heterogeneity, dynamic routing
</p><p><b>Compressor summary</b>: Flow is a per-instance personalization FL algorithm that adapts dynamic models to each client's data distributions and instances, improving clients' accuracy.</p><hr><h3>OKRidge: Scalable Optimal k-Sparse Ridge Regression</h3>
<p>Jiachang Liu, Sam Rosen, Chudi Zhong, Cynthia Rudin</p>
<p><a href='https://openreview.net/forum?id=BHxsP5fSHv'>https://openreview.net/forum?id=BHxsP5fSHv</a></p>
<p><b>Keywords</b>: Sparse Ridge Regression, Dynamical Systems
</p><p><b>Compressor summary</b>: The authors propose OKRidge, a fast algorithm for identifying sparse governing equations in nonlinear dynamical systems using a novel lower bound calculation and beam search warm-starting.</p><hr><h3>An Improved Relaxation for Oracle-Efficient Adversarial Contextual Bandits</h3>
<p>Kiarash Banihashem, MohammadTaghi Hajiaghayi, Suho Shin, Max Springer</p>
<p><a href='https://openreview.net/forum?id=BHZsJ2sTkG'>https://openreview.net/forum?id=BHZsJ2sTkG</a></p>
<p><b>Keywords</b>: contextual bandits, adversarial bandits, oracle-efficient online learning
</p><p><b>Compressor summary</b>: The paper presents a new algorithm for adversarial contextual bandits that improves existing regret bounds and makes fewer calls to an offline optimization oracle per round.</p><hr><h3>ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings</h3>
<p>Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu</p>
<p><a href='https://openreview.net/forum?id=BHXsb69bSx'>https://openreview.net/forum?id=BHXsb69bSx</a></p>
<p><b>Keywords</b>: large language model, tool learning
</p><p><b>Compressor summary</b>: The authors propose ToolkenGPT, which enables large language models to master tools by using vector embeddings of tools that are plugged into the LLM head and executed during text generation.</p><hr><h3>Regularity as Intrinsic Reward for Free Play</h3>
<p>Cansu Sancaktar, Justus Piater, Georg Martius</p>
<p><a href='https://openreview.net/forum?id=BHHrX3CRE1'>https://openreview.net/forum?id=BHHrX3CRE1</a></p>
<p><b>Keywords</b>: Intrinsic Motivation, Reinforcement Learning, Model-based Planning, Regularity, Manipulation, Zero-shot Generalization, Unsupervised Exploration
</p><p><b>Compressor summary</b>: We propose regularity as a novel reward signal for intrinsically-motivated reinforcement learning. Taking inspiration from child development, we postulate that striving for structure and order helps guide exploration towards a subspace of tasks that are not favored by naive uncertainty-based intrinsic rewards. Our generalized formulation of Regularity as Intrinsic Reward (RaIR) allows us to operationalize it within model-based reinforcement learning. In a synthetic environment, we showcase the plethora of structured patterns that can emerge from pursuing this regularity objective. We also demonstrate the strength of our method in a multi-object robotic manipulation environment. We incorporate RaIR into free play and use it to complement the model’s epistemic uncertainty as an intrinsic reward. Doing so, we witness the autonomous construction of towers and other regular structures during free play, which leads to a substantial improvement in zero-shot downstream task performance on assembly tasks.</p><hr><h3>Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning</h3>
<p>Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang</p>
<p><a href='https://openreview.net/forum?id=BGvkwZEGt7'>https://openreview.net/forum?id=BGvkwZEGt7</a></p>
<p><b>Keywords</b>: Large language models, Bayesian explanation, in-context learning
</p><p><b>Compressor summary</b>: The study examines in-context learning using Bayesian methods and proposes an algorithm to select optimal demonstrations for few-shot text classification, improving performance on real-world datasets.</p><hr><h3>Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic model</h3>
<p>Shiwei Liu, Tian Zhu, Milong Ren, Yu Chungong, Dongbo Bu, Haicang Zhang</p>
<p><a href='https://openreview.net/forum?id=BGP5Vjt93A'>https://openreview.net/forum?id=BGP5Vjt93A</a></p>
<p><b>Keywords</b>: Riemannian Diffusion Probabilistic Model, Mutation, Protein-protein binding
</p><p><b>Compressor summary</b>: SidechainDiff is a new method that uses unlabelled experimental data to predict how mutations affect protein-protein binding by learning side-chain conformations and structural context representations.</p><hr><h3>Score-based Source Separation with Applications to Digital Communication Signals</h3>
<p>Tejas Jayashankar, Gary C.F. Lee, Alejandro Lancho, Amir Weiss, Yury Polyanskiy, Gregory Wornell</p>
<p><a href='https://openreview.net/forum?id=BFGQQKicuu'>https://openreview.net/forum?id=BFGQQKicuu</a></p>
<p><b>Keywords</b>: Diffusion models, score-based models, source separation, digital communications, maximum a posteriori (MAP) estimation, alpha-posterior, Gaussian smoothing, score distillation sampling, radio frequency systems, interference mitigation
</p><p><b>Compressor summary</b>: The authors propose a new method for separating mixed signals using diffusion-based generative models that improves bit error rate in radio-frequency systems and has applications beyond conditional sampling.</p><hr><h3>HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork</h3>
<p>Bipasha Sen, Gaurav Singh, Aditya Agarwal, Rohith Agaram, Madhava Krishna, Srinath Sridhar</p>
<p><a href='https://openreview.net/forum?id=BExDjNDYkN'>https://openreview.net/forum?id=BExDjNDYkN</a></p>
<p><b>Keywords</b>: neural radiance field, hypernetwork, multi-hash encoding, NeRF
</p><p><b>Compressor summary</b>: HyP-NeRF is a method for learning generalizable NeRF priors using hypernetworks that improves quality and enables multiple downstream tasks.</p><hr><h3>TensorNet: Cartesian Tensor Representations for Efficient Learning of Molecular Potentials</h3>
<p>Guillem Simeon, Gianni De Fabritiis</p>
<p><a href='https://openreview.net/forum?id=BEHlPdBZ2e'>https://openreview.net/forum?id=BEHlPdBZ2e</a></p>
<p><b>Keywords</b>: Neural network interatomic potentials, Equivariant graph neural network, Message passing neural network
</p><p><b>Compressor summary</b>: TensorNet is a novel neural network architecture that uses Cartesian tensor representations to efficiently model molecular systems with reduced computational cost and improved performance.</p><hr><h3>Multi-Object Representation Learning via Feature Connectivity and Object-Centric Regularization</h3>
<p>Alex Foo, Wynne Hsu, Mong-Li Lee</p>
<p><a href='https://openreview.net/forum?id=BDno5qWEFh'>https://openreview.net/forum?id=BDno5qWEFh</a></p>
<p><b>Keywords</b>: Object-Centric Learning, Multi-Object Representation Learning
</p><p><b>Compressor summary</b>: The paragraph describes a new method for discovering object-centered representations from images that improves robustness, sample efficiency, and interpretability, and outperforms existing methods on various types of real-world images.</p><hr><h3>Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models</h3>
<p>Alvin Heng, Harold Soh</p>
<p><a href='https://openreview.net/forum?id=BC1IJdsuYB'>https://openreview.net/forum?id=BC1IJdsuYB</a></p>
<p><b>Keywords</b>: generative models, forgetting
</p><p><b>Compressor summary</b>: Selective Amnesia is a technique that allows controlling the forgetting of specific concepts in deep generative models to prevent misuse for harmful or inappropriate content.</p><hr><h3>Subspace Identification for Multi-Source Domain Adaptation</h3>
<p>Zijian Li, Ruichu Cai, Guangyi Chen, Boyang Sun, Zhifeng Hao, Kun Zhang</p>
<p><a href='https://openreview.net/forum?id=BACQLWQW8u'>https://openreview.net/forum?id=BACQLWQW8u</a></p>
<p><b>Keywords</b>: Domain Adaptation, Identification
</p><p><b>Compressor summary</b>: The paper proposes a new method for transferring knowledge between multiple labeled source domains to an unlabeled target domain using subspace identification and variational inference, which works better than existing methods.</p><hr><h3>Sampling from Structured Log-Concave Distributions via a Soft-Threshold Dikin Walk</h3>
<p>Oren Mangoubi, Nisheeth K Vishnoi</p>
<p><a href='https://openreview.net/forum?id=BA7NHAzbpO'>https://openreview.net/forum?id=BA7NHAzbpO</a></p>
<p><b>Keywords</b>: Logconcave sampling, Dikin walk, Markov chain Monte Carlo, interior point methods
</p><p><b>Compressor summary</b>: The paper presents a new Dikin walk algorithm to sample from a log-concave distribution defined by a Lipschitz or smooth convex function on a bounded polytope, with improved efficiency and acceptance ratio compared to previous works.</p><hr><h3>CamoPatch: An Evolutionary Strategy for Generating Camoflauged Adversarial Patches</h3>
<p>Phoenix Neale Williams, Ke Li</p>
<p><a href='https://openreview.net/forum?id=B94G0MXWQX'>https://openreview.net/forum?id=B94G0MXWQX</a></p>
<p><b>Keywords</b>: Evolutionary Strategy, Adversarial Attack, Adversarial Patches, Computational Art, Computer Vision
</p><p><b>Compressor summary</b>: Deep neural networks (DNNs) have demonstrated vulnerabilities to adversarial examples, which raises concerns about their reliability in safety-critical applications. While the majority of existing methods generate adversarial examples by making small modifications to the entire image, recent research has proposed a practical alternative known as adversarial patches. Adversarial patches have shown to be highly effective in causing DNNs to misclassify by distorting a localized area (patch) of the image. However, existing methods often produce clearly visible distortions since they do not consider the visibility of the patch. To address this, we propose a novel method for constructing adversarial patches that approximates the appearance of the area it covers. We achieve this by using a set of semi-transparent, RGB-valued circles, drawing inspiration from the computational art community. We utilize an evolutionary strategy to optimize the properties of each shape, and employ a simulated annealing approach to optimize the patch's location. Our approach achieves better or comparable performance to state-of-the-art methods on ImageNet DNN classifiers while achieving a lower $l_2$ distance from the original image. By minimizing the visibility of the patch, this work further highlights the vulnerabilities of DNNs to adversarial patches.</p><hr><h3>Optimization and Bayes: A Trade-off for Overparameterized Neural Networks</h3>
<p>Zhengmian Hu, Heng Huang</p>
<p><a href='https://openreview.net/forum?id=B7QkdEnjL9'>https://openreview.net/forum?id=B7QkdEnjL9</a></p>
<p><b>Keywords</b>: Bayesian learning, Generalization
</p><p><b>Compressor summary</b>: The paper introduces TansBL, which combines ERM and Bayesian learning for neural networks, balancing generalization and computational complexity.</p><hr><h3>An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient</h3>
<p>Yudong Luo, Guiliang Liu, Pascal Poupart, Yangchen Pan</p>
<p><a href='https://openreview.net/forum?id=B7QRV4XXiK'>https://openreview.net/forum?id=B7QRV4XXiK</a></p>
<p><b>Keywords</b>: risk-averse RL, mean-variance RL
</p><p><b>Compressor summary</b>: The paper proposes a new risk measure for Reinforcement Learning, called Gini deviation, which addresses the limitations of variance-based methods and improves policy learning.</p><hr><h3>POMDP Planning for Object Search in Partially Unknown Environment</h3>
<p>Yongbo Chen, Hanna Kurniawati</p>
<p><a href='https://openreview.net/forum?id=B6qZdrGRpm'>https://openreview.net/forum?id=B6qZdrGRpm</a></p>
<p><b>Keywords</b>: robotics, Partially Observable Markov Decision Process (POMDP), object search
</p><p><b>Compressor summary</b>: The paper proposes a method for mobile robots to search for objects in complex environments using a POMDP formulation, a perception module, and a planning algorithm that improves performance over baseline approaches.</p><hr><h3>A Batch-to-Online Transformation under Random-Order Model</h3>
<p>Jing Dong, Yuichi Yoshida</p>
<p><a href='https://openreview.net/forum?id=B6HSIgvyJ3'>https://openreview.net/forum?id=B6HSIgvyJ3</a></p>
<p><b>Keywords</b>: online learning, random model setting
</p><p><b>Compressor summary</b>: The framework transforms offline approximation algorithms into online ones with low $\epsilon$-approximate regret and applies to various problems like clustering, matrix approximation, and regression.</p><hr><h3>Universal Gradient Descent Ascent Method for Nonconvex-Nonconcave Minimax Optimization</h3>
<p>Taoli Zheng, Linglingzhi Zhu, Anthony Man-Cho So, Jose Blanchet, Jiajin Li</p>
<p><a href='https://openreview.net/forum?id=B6FihisDBl'>https://openreview.net/forum?id=B6FihisDBl</a></p>
<p><b>Keywords</b>: Nonconvex-Nonconcave Minimax Optimization, Limit Cycle
</p><p><b>Compressor summary</b>: The paper proposes a new optimization algorithm, DS-GDA, that works for various nonconvex-nonconcave problems and achieves convergence with better complexity than existing methods.</p><hr><h3>Communication-Efficient Federated Bilevel Optimization with Global and Local Lower Level Problems</h3>
<p>Junyi Li, Feihu Huang, Heng Huang</p>
<p><a href='https://openreview.net/forum?id=B5XwENgy0T'>https://openreview.net/forum?id=B5XwENgy0T</a></p>
<p><b>Keywords</b>: Federated Learning, Bilevel Optimization
</p><p><b>Compressor summary</b>: The paper proposes FedBiOAcc, a communication-efficient algorithm for Federated Bilevel Optimization problems with good theoretical and empirical performance.</p><hr><h3>Online Nonstochastic Model-Free Reinforcement Learning</h3>
<p>Udaya Ghai, Arushi Gupta, Wenhan Xia, Karan Singh, Elad Hazan</p>
<p><a href='https://openreview.net/forum?id=B5LpWAaBVA'>https://openreview.net/forum?id=B5LpWAaBVA</a></p>
<p><b>Keywords</b>: Control, Reinforcement Learning, Online Learning, Regret Minimization, Bandit Linear Control
</p><p><b>Compressor summary</b>: The authors propose a novel policy class based on disturbance signals and introduce efficient algorithms for optimizing them, improving robustness in model-free reinforcement learning agents facing adversarial or dynamic environments.</p><hr><h3>Optimal Time Complexities of Parallel Stochastic Optimization Methods Under a Fixed Computation Model</h3>
<p>Alexander Tyurin, Peter Richtárik</p>
<p><a href='https://openreview.net/forum?id=B4xF1wfQnF'>https://openreview.net/forum?id=B4xF1wfQnF</a></p>
<p><b>Keywords</b>: nonconvex optimization, convex optimization, parallel methods, asynchronous methods, lower bounds
</p><p><b>Compressor summary</b>: The paper introduces a new protocol for parallel optimization methods with an unbiased stochastic gradient oracle, establishes minimax complexities, and reveals implications for asynchronous optimization methods.</p><hr><h3>Grammar Prompting for Domain-Specific Language Generation with  Large Language Models</h3>
<p>Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A. Saurous, Yoon Kim</p>
<p><a href='https://openreview.net/forum?id=B4tkwuzeiY'>https://openreview.net/forum?id=B4tkwuzeiY</a></p>
<p><b>Keywords</b>: semantic parsing, large language models, PDDL, AI planning, molecule generation, data efficiency, grammar-based learning
</p><p><b>Compressor summary</b>: Grammar prompting helps large language models generate outputs for structured languages by using external knowledge and domain-specific constraints from a grammar in BNF during in-context learning.</p><hr><h3>Fast Approximation of Similarity Graphs with Kernel Density Estimation</h3>
<p>Peter Macgregor, He Sun</p>
<p><a href='https://openreview.net/forum?id=B4G87Bq5wA'>https://openreview.net/forum?id=B4G87Bq5wA</a></p>
<p><b>Keywords</b>: similarity graphs, spectral clustering
</p><p><b>Compressor summary</b>: The paper proposes a new algorithm to efficiently construct a sparse similarity graph that preserves cluster structure for data points in high dimensions using kernel density estimation.</p><hr><h3>Recurrent Temporal Revision Graph Networks</h3>
<p>YIZHOU CHEN, Anxiang Zeng, Qingtao Yu, Kerui Zhang, Cao Yuanpeng, Kangle Wu, Guangda Huzhang, Han Yu, Zhiming Zhou</p>
<p><a href='https://openreview.net/forum?id=B3UDx1rNOy'>https://openreview.net/forum?id=B3UDx1rNOy</a></p>
<p><b>Keywords</b>: temporal graph, temporal graph network, temporal graph model expressiveness, continuous-time dynamic graph
</p><p><b>Compressor summary</b>: The paragraph discusses a novel framework for temporal neighbor aggregation that uses recurrent neural networks to integrate information from all historical neighbors, improving accuracy and expressiveness in graph networks.</p><hr><h3>Counterfactually Comparing Abstaining Classifiers</h3>
<p>Yo Joong Choe, Aditya Gangrade, Aaditya Ramdas</p>
<p><a href='https://openreview.net/forum?id=B2DEcj4a7i'>https://openreview.net/forum?id=B2DEcj4a7i</a></p>
<p><b>Keywords</b>: abstaining classifiers, black-box model evaluation, causal inference, missing data
</p><p><b>Compressor summary</b>: The paper proposes a method to evaluate abstaining classifiers by treating their abstentions as missing data and estimating their counterfactual performance.</p><hr><h3>Parameterizing Context: Unleashing the Power of Parameter-Efficient Fine-Tuning and In-Context Tuning for Continual Table Semantic Parsing</h3>
<p>Yongrui Chen, Shenyu Zhang, Guilin Qi, Xinnan Guo</p>
<p><a href='https://openreview.net/forum?id=B01uiWhjpc'>https://openreview.net/forum?id=B01uiWhjpc</a></p>
<p><b>Keywords</b>: semantic parsing, continual learning, few-shot learning
</p><p><b>Compressor summary</b>: This paper introduces a novel method combining parameter-efficient fine-tuning and in-context tuning for training a continual table semantic parser that overcomes challenges like catastrophic forgetting and limited supervision.</p><hr><h3>Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model</h3>
<p>Valentyn Melnychuk, Dennis Frauen, Stefan Feuerriegel</p>
<p><a href='https://openreview.net/forum?id=AygwZzdCM0'>https://openreview.net/forum?id=AygwZzdCM0</a></p>
<p><b>Keywords</b>: causal inference, counterfactual inference, partial identification, sensitivity model, normalizing flows, causal machine learning
</p><p><b>Compressor summary</b>: The paper proposes a novel sensitivity model and deep generative model to relax assumptions and achieve partial counterfactual identification of continuous outcomes in structural causal models.</p><hr><h3>Model-Free Reinforcement Learning with the Decision-Estimation Coefficient</h3>
<p>Dylan J Foster, Noah Golowich, Jian Qian, Alexander Rakhlin, Ayush Sekhari</p>
<p><a href='https://openreview.net/forum?id=Ay3WvSrtpO'>https://openreview.net/forum?id=Ay3WvSrtpO</a></p>
<p><b>Keywords</b>: Decision making, learning theory, bandits, reinforcement learning theory, online learning, decision-estimation coefficient
</p><p><b>Compressor summary</b>: The paper proposes a method to improve interactive decision making by combining a meta-algorithm with a specialized estimation technique, achieving better guarantees and accommodating more lenient notions of estimation error.</p><hr><h3>ProteinNPT: Improving protein property prediction and design with non-parametric transformers</h3>
<p>Pascal Notin, Ruben Weitzman, Debora Susan Marks, Yarin Gal</p>
<p><a href='https://openreview.net/forum?id=AwzbQVuDBk'>https://openreview.net/forum?id=AwzbQVuDBk</a></p>
<p><b>Keywords</b>: Non-parametric transformers, protein design, protein property prediction, fitness prediction, Bayesian optimization, ProteinGym
</p><p><b>Compressor summary</b>: ProteinNPT is a non-parametric transformer variant for protein sequence optimization that outperforms existing methods and enables iterative protein design in multi-task settings with label scarcity.</p><hr><h3>Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics Alignment with Diffusion Model</h3>
<p>Yule Wang, Zijing Wu, Chengrui Li, Anqi Wu</p>
<p><a href='https://openreview.net/forum?id=AuXd54odxm'>https://openreview.net/forum?id=AuXd54odxm</a></p>
<p><b>Keywords</b>: Neural distribution alignment, Diffusion model, Neuroscience, Neural decoding
</p><p><b>Compressor summary</b>: The proposed ERDiff method aligns neural signals across domains by using a diffusion model to preserve spatio-temporal structures in latent dynamics, resulting in better alignment quality and neural decoding performance.</p><hr><h3>Calibrate and Boost Logical Expressiveness of GNN Over Multi-Relational and Temporal Graphs</h3>
<p>Yeyuan Chen, Dingmin Wang</p>
<p><a href='https://openreview.net/forum?id=AtHJ7TLheF'>https://openreview.net/forum?id=AtHJ7TLheF</a></p>
<p><b>Keywords</b>: Knowledge Graphs, First-Order Logic, Temporal Knowledge Graph, Graph Neural Networks
</p><p><b>Compressor summary</b>: The paper analyzes the expressiveness of Graph Neural Networks (GNNs) for Boolean node classification over multi-relational graphs, proposes a graph transformation technique to improve GNNs' capabilities, and tests their approach empirically.</p><hr><h3>FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning</h3>
<p>Dipam Goswami, Yuyang Liu, Bartłomiej Twardowski, Joost van de Weijer</p>
<p><a href='https://openreview.net/forum?id=Asx5eDqFZl'>https://openreview.net/forum?id=Asx5eDqFZl</a></p>
<p><b>Keywords</b>: Continual Learning, Class-Incremental Learning
</p><p><b>Compressor summary</b>: The paper proposes a prototypical network approach for class-incremental learning that uses anisotropic Mahalanobis distance and feature covariance modeling, achieving state-of-the-art results without updating the backbone network.</p><hr><h3>Neural Lyapunov Control for Discrete-Time Systems</h3>
<p>Junlin Wu, Andrew Clark, Yiannis Kantaros, Yevgeniy Vorobeychik</p>
<p><a href='https://openreview.net/forum?id=ArRycLMoUg'>https://openreview.net/forum?id=ArRycLMoUg</a></p>
<p><b>Keywords</b>: nonlinear systems, Lyapunov stability, neural Lyapunov control
</p><p><b>Compressor summary</b>: The authors propose a novel method for learning neural Lyapunov control in discrete-time systems using mixed-integer linear programming, sublevel sets computation, and gradient-based counterexample finding, achieving faster and better results on four benchmarks than existing approaches.</p><hr><h3>Resetting the Optimizer in Deep RL: An Empirical Study</h3>
<p>Kavosh Asadi, Rasool Fakoor, Shoham Sabach</p>
<p><a href='https://openreview.net/forum?id=AnFUgNC3Yc'>https://openreview.net/forum?id=AnFUgNC3Yc</a></p>
<p><b>Keywords</b>: Deep Reinforcement Learning, Rainbow Algorithm, Atari benchmark, Adam Optimizer
</p><p><b>Compressor summary</b>: The text discusses how resetting internal parameters of optimization algorithms, such as Adam, can improve the performance of deep reinforcement learning on Atari games.</p><hr><h3>Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness</h3>
<p>Jacy Reese Anthis, Victor Veitch</p>
<p><a href='https://openreview.net/forum?id=AmwgBjXqc3'>https://openreview.net/forum?id=AmwgBjXqc3</a></p>
<p><b>Keywords</b>: causal graphs, causality, counterfactual fairness, domain generalization, fairness, robustness, machine learning, artificial intelligence
</p><p><b>Compressor summary</b>: The paper proposes a causal context approach to relate counterfactual fairness, robust prediction, and group fairness in data-generating processes and shows how counterfactual fairness is equivalent to different group fairness metrics in various scenarios.</p><hr><h3>You Only Condense Once: Two Rules for Pruning Condensed Datasets</h3>
<p>Yang He, Lingao Xiao, Joey Tianyi Zhou</p>
<p><a href='https://openreview.net/forum?id=AlTyimRsLf'>https://openreview.net/forum?id=AlTyimRsLf</a></p>
<p><b>Keywords</b>: Dataset Condensation, Dataset Pruning
</p><p><b>Compressor summary</b>: YOCO is a method to adjust the size of a condensed dataset for efficient on-device training using simple rules and improving accuracy.</p><hr><h3>Phase diagram of early training dynamics in deep neural networks: effect of the learning rate, depth, and width</h3>
<p>Dayal Singh Kalra, Maissam Barkeshli</p>
<p><a href='https://openreview.net/forum?id=Al9yglQGKj'>https://openreview.net/forum?id=Al9yglQGKj</a></p>
<p><b>Keywords</b>: Optimization dynamics, Phase diagrams, learning rate transition, Catapult effect
</p><p><b>Compressor summary</b>: The paper studies how the learning rate, depth, and width of deep neural networks affect their optimization dynamics and the sharpness of the loss landscape during training with stochastic gradient descent.</p><hr><h3>Towards Anytime Classification in Early-Exit Architectures by Enforcing Conditional Monotonicity</h3>
<p>Metod Jazbec, James Urquhart Allingham, Dan Zhang, Eric Nalisnick</p>
<p><a href='https://openreview.net/forum?id=Akslsk891N'>https://openreview.net/forum?id=Akslsk891N</a></p>
<p><b>Keywords</b>: anytime algorithms, early-exit neural networks, conditional monotonicity, anytime uncertainty
</p><p><b>Compressor summary</b>: The paper proposes a modification to early-exit neural networks that allows them to provide better predictions as computation time increases, enabling anytime predictive modeling with these architectures.</p><hr><h3>Strategic Data Sharing between Competitors</h3>
<p>Nikita Tsoy, Nikola Konstantinov</p>
<p><a href='https://openreview.net/forum?id=AkK3S2spZs'>https://openreview.net/forum?id=AkK3S2spZs</a></p>
<p><b>Keywords</b>: Incentives, collaborative learning, federated learning, game theory, competition, oligopolistic markets, strategic behavior, Nash equilibrium
</p><p><b>Compressor summary</b>: The paper presents a framework to analyze how data sharing affects machine learning model quality and profitability in collaborative learning settings, and finds that market conditions and task difficulty influence collaboration incentives.</p><hr><h3>A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability</h3>
<p>Zijie Geng, Xijun Li, Jie Wang, Xiao Li, Yongdong Zhang, Feng Wu</p>
<p><a href='https://openreview.net/forum?id=AiEipk1X0c'>https://openreview.net/forum?id=AiEipk1X0c</a></p>
<p><b>Keywords</b>: Learning to Optimize, Machine Learning for Combinatorial Optimization, Mixed-Integer Linear Programming, Graph Generation
</p><p><b>Compressor summary</b>: G2MILP is a deep generative framework that creates realistic and novel mixed-integer linear programs (MILPs) from bipartite graphs without prior expert knowledge, improving MILP solver performance under limited data availability.</p><hr><h3>Optimal Excess Risk Bounds for Empirical Risk Minimization on $p$-Norm Linear Regression</h3>
<p>Ayoub El Hanchi, Murat A Erdogdu</p>
<p><a href='https://openreview.net/forum?id=Ah2Q8mLH96'>https://openreview.net/forum?id=Ah2Q8mLH96</a></p>
<p><b>Keywords</b>: Excess risk bounds, Linear regression, Lp-norm, Fast rates
</p><p><b>Compressor summary</b>: The paper investigates how many samples are needed for different p-norms in linear regression and proves bounds on the performance of empirical risk minimization.</p><hr><h3>Multi-task Representation Learning for Pure Exploration in Bilinear Bandits</h3>
<p>Subhojyoti Mukherjee, Qiaomin Xie, Josiah P. Hanna, Robert D Nowak</p>
<p><a href='https://openreview.net/forum?id=AfC8PVQZ9z'>https://openreview.net/forum?id=AfC8PVQZ9z</a></p>
<p><b>Keywords</b>: Linear Bandits, Experimental design, Pure Exploration, Representation Learning
</p><p><b>Compressor summary</b>: The paper proposes GOBLIN, a multi-task representation learning algorithm for bilinear bandits that reduces exploration time and improves sample complexity by sharing a common low-dimensional linear representation across tasks.</p><hr><h3>To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis</h3>
<p>Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You</p>
<p><a href='https://openreview.net/forum?id=Af5GvIj3T5'>https://openreview.net/forum?id=Af5GvIj3T5</a></p>
<p><b>Keywords</b>: Large Language Model, Transformer Scaling, Foundation Model Pre-training
</p><p><b>Compressor summary</b>: The study investigates how repeating pre-training data affects large language models, causing overfitting and multi-epoch degradation, and explores factors contributing to it, as well as regularization techniques and hyper-parameter tuning methods.</p><hr><h3>No-Regret Online Prediction with Strategic Experts</h3>
<p>Omid Sadeghi, Maryam Fazel</p>
<p><a href='https://openreview.net/forum?id=AesN5bYnJr'>https://openreview.net/forum?id=AesN5bYnJr</a></p>
<p><b>Keywords</b>: incentive-compatible, online prediction with expert advice, forecasting
</p><p><b>Compressor summary</b>: The authors study a generalization of online binary prediction with expert advice where experts act strategically and aim to maximize their influence, and they design algorithms that encourage truthful reporting and achieve sublinear regret.</p><hr><h3>A Unified Solution for Privacy and Communication Efficiency in Vertical Federated Learning</h3>
<p>Ganyu Wang, Bin Gu, Qingsong Zhang, Xiang Li, Boyu Wang, Charles Ling</p>
<p><a href='https://openreview.net/forum?id=AYiRHZirD2'>https://openreview.net/forum?id=AYiRHZirD2</a></p>
<p><b>Keywords</b>: Vertical Federated Learning, Zeroth Order Optimization, Communication Efficiency, Privacy
</p><p><b>Compressor summary</b>: The paper proposes a cascaded hybrid optimization approach for Vertical Federated Learning that improves convergence and privacy protection by using a zeroth-order gradient on the output layer and a first-order gradient elsewhere, with experimental results showing its advantages over existing methods.</p><hr><h3>Two Heads are Better Than One: A Simple Exploration Framework for Efficient Multi-Agent Reinforcement Learning</h3>
<p>Jiahui Li, Kun Kuang, Baoxiang Wang, Xingchen Li, Fei Wu, Jun Xiao, Long Chen</p>
<p><a href='https://openreview.net/forum?id=AYLlZMmUbo'>https://openreview.net/forum?id=AYLlZMmUbo</a></p>
<p><b>Keywords</b>: multi-agent reinforcement learning, influence-based exploration
</p><p><b>Compressor summary</b>: COIN is an exploration method for cooperative multi-agent reinforcement learning that combines curiosity-based and influence-based intrinsic rewards to effectively explore large and complex state spaces.</p><hr><h3>Every Parameter Matters: Ensuring the Convergence of Federated Learning with Dynamic Heterogeneous Models Reduction</h3>
<p>Hanhan Zhou, Tian Lan, Guru Prasadh Venkataramani, Wenbo Ding</p>
<p><a href='https://openreview.net/forum?id=AWpWaub6nf'>https://openreview.net/forum?id=AWpWaub6nf</a></p>
<p><b>Keywords</b>: Federated Learning, Optimization, Deep Learning
</p><p><b>Compressor summary</b>: The paper proposes a framework for heterogeneous FL algorithms with online model extraction and proves their convergence under certain conditions for IID and non-IID data, while introducing the concept of minimum coverage index to improve efficiency.</p><hr><h3>Enhancing User Intent Capture in Session-Based Recommendation with Attribute Patterns</h3>
<p>Xin Liu, Zheng Li, Yifan Gao, Jingfeng Yang, Tianyu Cao, Zhengyang Wang, Bing Yin, Yangqiu Song</p>
<p><a href='https://openreview.net/forum?id=AV3iZlDrzF'>https://openreview.net/forum?id=AV3iZlDrzF</a></p>
<p><b>Keywords</b>: session-based recommendation, representation learning, pattern mining
</p><p><b>Compressor summary</b>: FAPAT is a new method for session-based recommendation in E-commerce that uses attribute transition graphs and frequent attribute patterns to capture user intents and outperforms existing methods by 4.5% on average.</p><hr><h3>A Reduction-based Framework for Sequential Decision Making with Delayed Feedback</h3>
<p>Yunchang Yang, Han Zhong, Tianhao Wu, Bin Liu, Liwei Wang, Simon Shaolei Du</p>
<p><a href='https://openreview.net/forum?id=AT6NaLPwy0'>https://openreview.net/forum?id=AT6NaLPwy0</a></p>
<p><b>Keywords</b>: sequential decision making, delay, reinforcement learning
</p><p><b>Compressor summary</b>: The paper presents a framework to improve sample efficiency in sequential decision making with stochastic delays using different multi-batched algorithms.</p><hr><h3>Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models</h3>
<p>Sivan Doveh, Assaf Arbelle, Sivan Harary, Roei Herzig, Donghyun Kim, Paola Cascante-Bonilla, Amit Alfassy, Rameswar Panda, Raja Giryes, Rogerio Feris, Shimon Ullman, Leonid Karlinsky</p>
<p><a href='https://openreview.net/forum?id=ARrwf7Ev2T'>https://openreview.net/forum?id=ARrwf7Ev2T</a></p>
<p><b>Keywords</b>: computer vision, deep learning, vision and language models
</p><p><b>Compressor summary</b>: The paper identifies two factors that limit VL models' compositional reasoning performance and proposes a fine-tuning approach to improve it on a standard paired VL dataset, leading to significant performance increase.</p><hr><h3>Differentiable Sampling of Categorical Distributions Using the CatLog-Derivative Trick</h3>
<p>Lennert De Smet, Emanuele Sansone, Pedro Zuidberg Dos Martires</p>
<p><a href='https://openreview.net/forum?id=AQyqxXctsN'>https://openreview.net/forum?id=AQyqxXctsN</a></p>
<p><b>Keywords</b>: gradient estimation, categorical random variables, probability theory, discrete distributions
</p><p><b>Compressor summary</b>: The CatLog-Derivative trick improves upon the Log-Derivative trick by accounting for the discrete nature of categorical distributions, leading to a more efficient and unbiased gradient estimator called IndeCateR.</p><hr><h3>When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment</h3>
<p>Tianwei Ni, Michel Ma, Benjamin Eysenbach, Pierre-Luc Bacon</p>
<p><a href='https://openreview.net/forum?id=APGXBNkt6h'>https://openreview.net/forum?id=APGXBNkt6h</a></p>
<p><b>Keywords</b>: Memory-based RL, Transformers, Credit Assignment, Online RL, Model-free RL
</p><p><b>Compressor summary</b>: The paper investigates why Transformer architecture works well for reinforcement learning problems involving long-term dependencies, finding that it enhances memory capability but not credit assignment.</p><hr><h3>Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment</h3>
<p>Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, Gal Chechik</p>
<p><a href='https://openreview.net/forum?id=AOKU4nRw1W'>https://openreview.net/forum?id=AOKU4nRw1W</a></p>
<p><b>Keywords</b>: syntax, diffusion, stable diffusion, attribute, attention
</p><p><b>Compressor summary</b>: SynGen improves text-to-image generation by using syntax to guide cross-attention maps and maintain correct associations between entities and their visual attributes.</p><hr><h3>3D-Aware Visual Question Answering about Parts, Poses and Occlusions</h3>
<p>Xingrui Wang, Wufei Ma, Zhuowan Li, Adam Kortylewski, Alan Yuille</p>
<p><a href='https://openreview.net/forum?id=AMIJEupsNq'>https://openreview.net/forum?id=AMIJEupsNq</a></p>
<p><b>Keywords</b>: VQA, reasoning, 3D scene understanding, analysis-by-synthesis, neural modular network, neuro-symbolic reasoning
</p><p><b>Compressor summary</b>: The paragraph introduces the task of 3D-aware VQA, which involves answering questions about the 3D structure of visual scenes, and presents a new dataset (Super-CLEVR-3D) and a novel model (PO3D-VQA) to address this challenge.</p><hr><h3>Boosting Adversarial Transferability by Achieving Flat Local Maxima</h3>
<p>Zhijin Ge, Xiaosen Wang, Hongying Liu, Fanhua Shang, Yuanyuan Liu</p>
<p><a href='https://openreview.net/forum?id=AKAMNDe2Sw'>https://openreview.net/forum?id=AKAMNDe2Sw</a></p>
<p><b>Keywords</b>: Adversarial attack, Adversarial transferability, Black-box Attack
</p><p><b>Compressor summary</b>: The authors propose a method to generate adversarial examples that transfer better by optimizing the gradient norm and using random sampling and averaging of gradients, achieving improved transferability on both normal and adversarial models.</p><hr><h3>Fantastic Robustness Measures: The Secrets of Robust Generalization</h3>
<p>Hoki Kim, Jinseong Park, Yujin Choi, Jaewook Lee</p>
<p><a href='https://openreview.net/forum?id=AGVBqJuL0T'>https://openreview.net/forum?id=AGVBqJuL0T</a></p>
<p><b>Keywords</b>: Adversarial Robustness, Generalization, Measures
</p><p><b>Compressor summary</b>: The study analyzes how different measures of robust generalization relate to adversarial examples in various settings using over 1,300 models trained on CIFAR-10 and more than 100 models from RobustBench.</p><hr><h3>Active Bipartite Ranking</h3>
<p>James Cheshire, Vincent Laurent, Stephan Clémençon</p>
<p><a href='https://openreview.net/forum?id=AGMVzMGcGP'>https://openreview.net/forum?id=AGMVzMGcGP</a></p>
<p><b>Keywords</b>: bipartite ranking, multi armed bandits, active learning
</p><p><b>Compressor summary</b>: The paper presents an active learning framework for bipartite ranking and proposes a new algorithm called active-rank that minimizes the distance between the ROC curve of the ranking function and the optimal one.</p><hr><h3>DIFFER:Decomposing Individual Reward for Fair Experience Replay in Multi-Agent Reinforcement Learning</h3>
<p>Xunhan Hu, Jian Zhao, Wengang Zhou, Ruili Feng, Houqiang Li</p>
<p><a href='https://openreview.net/forum?id=AG9A7Ae9r3'>https://openreview.net/forum?id=AG9A7Ae9r3</a></p>
<p><b>Keywords</b>: Experience Replay; Reinforcement Learning; Multi-Agent System
</p><p><b>Compressor summary</b>: The paper proposes DIFFER, a framework to decompose individual rewards in cooperative multi-agent reinforcement learning for more efficient and fair learning.</p><hr><h3>Training Energy-Based Normalizing Flow with Score-Matching Objectives</h3>
<p>Chen-Hao Chao, Wei-Fang Sun, Yen-Chang Hsu, Zsolt Kira, Chun-Yi Lee</p>
<p><a href='https://openreview.net/forum?id=AALLvnv95q'>https://openreview.net/forum?id=AALLvnv95q</a></p>
<p><b>Keywords</b>: flow-based models, score-matching methods
</p><p><b>Compressor summary</b>: The paper introduces EBFlow, a flow-based modeling method that connects flow- and energy-based models, optimizes with score-matching for faster training and better performance than existing methods.</p><hr><h3>Universal Online Learning with Gradient Variations: A Multi-layer Online Ensemble Approach</h3>
<p>Yu-Hu Yan, Peng Zhao, Zhi-Hua Zhou</p>
<p><a href='https://openreview.net/forum?id=AA1xrgAP5z'>https://openreview.net/forum?id=AA1xrgAP5z</a></p>
<p><b>Keywords</b>: online learning
</p><p><b>Compressor summary</b>: The paper proposes an online convex optimization method that adapts to different function types and can achieve problem-dependent guarantees, while using only one gradient query per round.</p><hr><h3>NAS-X: Neural Adaptive Smoothing via Twisting</h3>
<p>Dieterich Lawson, Michael Y. Li, Scott Linderman</p>
<p><a href='https://openreview.net/forum?id=A9mHph8GJk'>https://openreview.net/forum?id=A9mHph8GJk</a></p>
<p><b>Keywords</b>: sequence models, probabilistic inference, reweighted wake-sleep, sequential monte carlo, smoothing, mechanistic models
</p><p><b>Compressor summary</b>: NAS-X is a method for approximating inferences and model learning in complex sequential latent variable models using neural adaptive smoothing and twisting, which improves performance over previous methods in various applications, including neuronal dynamics.</p><hr><h3>AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix</h3>
<p>Yun Yue, Zhiling Ye, Jiadi Jiang, Yongchao Liu, Ke Zhang</p>
<p><a href='https://openreview.net/forum?id=A954O4tDmU'>https://openreview.net/forum?id=A954O4tDmU</a></p>
<p><b>Keywords</b>: adaptive optimizer, gradient difference, auto switch, AGD
</p><p><b>Compressor summary</b>: AGD is a new optimizer that combines a dynamic preconditioning matrix and an auto-switching function between SGD and adaptive optimizers, improving generalization performance on NLP, CV, and RecSys datasets.</p><hr><h3>On the Importance of Feature Separability in Predicting Out-Of-Distribution Error</h3>
<p>RENCHUNZI XIE, Hongxin Wei, Lei Feng, Yuzhou Cao, Bo An</p>
<p><a href='https://openreview.net/forum?id=A86JTXllHa'>https://openreview.net/forum?id=A86JTXllHa</a></p>
<p><b>Keywords</b>: Machine Learning, Uncertainty Estimation
</p><p><b>Compressor summary</b>: The paper proposes a dataset-level score based on feature dispersion to estimate test accuracy under distribution shift, showing that inter-class dispersion is strongly correlated with model accuracy on out-of-distribution data.</p><hr><h3>Image Captioners Are Scalable Vision Learners Too</h3>
<p>Michael Tschannen, Manoj Kumar, Andreas Peter Steiner, Xiaohua Zhai, Neil Houlsby, Lucas Beyer</p>
<p><a href='https://openreview.net/forum?id=A7feCufBhL'>https://openreview.net/forum?id=A7feCufBhL</a></p>
<p><b>Keywords</b>: contrastive learning, CLIP, CapPa, Cap, vision-language, image captioning, visual representation learning, weakly supervised learning, VLM, multimodal learning, VQA, image classification
</p><p><b>Compressor summary</b>: The paper compares image captioning and contrastive pretraining for vision backbones, finding that captioning alone produces competitive or better results on various tasks.</p><hr><h3>On Convergence of Polynomial Approximations to the Gaussian Mixture Entropy</h3>
<p>Caleb Dahlke, Jason Pacheco</p>
<p><a href='https://openreview.net/forum?id=A7ESFTMJWs'>https://openreview.net/forum?id=A7ESFTMJWs</a></p>
<p><b>Keywords</b>: entropy, Gaussian mixture model, uncertainty quantification, approximate inference
</p><p><b>Compressor summary</b>: The paper proposes a new Taylor series for Gaussian mixture model entropy that converges to the true value and improves accuracy over previous methods.</p><hr><h3>One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization</h3>
<p>Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, Hao Su</p>
<p><a href='https://openreview.net/forum?id=A6X9y8n4sT'>https://openreview.net/forum?id=A6X9y8n4sT</a></p>
<p><b>Keywords</b>: single image reconstruction, 3d generation, mesh reconstruction, diffusion models
</p><p><b>Compressor summary</b>: The authors propose a novel method that generates a 360-degree 3D textured mesh from a single image using a view-conditioned 2D diffusion model and an SDF-based neural surface reconstruction method, achieving better geometry, faster runtime, and supporting the text-to-3D task.</p><hr><h3>Generalized Semi-Supervised Learning via Self-Supervised Feature Adaptation</h3>
<p>Jiachen Liang, RuiBing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin CHEN</p>
<p><a href='https://openreview.net/forum?id=A6PRwRjI8V'>https://openreview.net/forum?id=A6PRwRjI8V</a></p>
<p><b>Keywords</b>: semi-supervised learning, self-supervised learning
</p><p><b>Compressor summary</b>: The paper proposes a new semi-supervised learning setting where unlabeled data has a different feature distribution than labeled data, and introduces SSFA, a framework that adapts the model features to improve pseudo-label quality.</p><hr><h3>Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control</h3>
<p>Chao Li, Chen GONG, Qiang He, Xinwen Hou</p>
<p><a href='https://openreview.net/forum?id=A6JDQDv7Nt'>https://openreview.net/forum?id=A6JDQDv7Nt</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Ensemble Exploration, Control Tasks
</p><p><b>Compressor summary</b>: TEEN is a new ensemble RL algorithm that enhances diversity and performance of sub-policies in complex sequential decision-making problems.</p><hr><h3>Effective Bayesian Heteroscedastic Regression with Deep Neural Networks</h3>
<p>Alexander Immer, Emanuele Palumbo, Alexander Marx, Julia E Vogt</p>
<p><a href='https://openreview.net/forum?id=A6EquH0enk'>https://openreview.net/forum?id=A6EquH0enk</a></p>
<p><b>Keywords</b>: Heteroscedastic Regression, Marginal Likelihood, Bayesian Neural Networks, Uncertainty Estimaton, Model Selection, Laplace Approximation
</p><p><b>Compressor summary</b>: The authors propose an efficient method to handle uncertainties in complex regression problems using neural networks and Gaussian processes, which improves generalization and requires no hyperparameter tuning.</p><hr><h3>Combinatorial Group Testing with Selfish Agents</h3>
<p>Giorgos Chionas, Dariusz Rafal Kowalski, Piotr Krysta</p>
<p><a href='https://openreview.net/forum?id=A5yMv7XPuA'>https://openreview.net/forum?id=A5yMv7XPuA</a></p>
<p><b>Keywords</b>: Combinatorial Group Testing, Adversarial Equilibrium, Contention Resolution, selfish agents, learning time, adaptive learning algorithms
</p><p><b>Compressor summary</b>: The paper presents a game-theoretic approach to Combinatorial Group Testing with selfish agents and shows how adaptive strategies can achieve near-optimal learning time depending on whether the number of active agents is known or not.</p><hr><h3>Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration</h3>
<p>Zhihan Liu, Miao Lu, Wei Xiong, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, Zhaoran Wang</p>
<p><a href='https://openreview.net/forum?id=A57UMlUJdc'>https://openreview.net/forum?id=A57UMlUJdc</a></p>
<p><b>Keywords</b>: reinforcement learning; online learning; game
</p><p><b>Compressor summary</b>: MEX is an easy-to-implement RL framework that balances exploration and exploitation automatically and outperforms baselines in various MuJoCo environments.</p><hr><h3>Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors</h3>
<p>Yong Liu, Chenyu Li, Jianmin Wang, Mingsheng Long</p>
<p><a href='https://openreview.net/forum?id=A4zzxu82a7'>https://openreview.net/forum?id=A4zzxu82a7</a></p>
<p><b>Keywords</b>: Time series forecasting, Deep learning
</p><p><b>Compressor summary</b>: Koopa is a novel Koopman forecaster that uses modern theory to handle non-stationary time series with complex dynamics, achieving competitive performance while reducing training time and memory.</p><hr><h3>Oracle Complexity of Single-Loop Switching Subgradient Methods for Non-Smooth Weakly Convex Functional Constrained Optimization</h3>
<p>Yankun Huang, Qihang Lin</p>
<p><a href='https://openreview.net/forum?id=A383wMho4h'>https://openreview.net/forum?id=A383wMho4h</a></p>
<p><b>Keywords</b>: Constrained optimization, first-order method, non-smooth optimization, non-convex optimization
</p><p><b>Compressor summary</b>: The paper analyzes the oracle complexity of the switching subgradient method for solving non-convex constrained optimization problems with weakly convex or convex constraint functions, and shows that it outperforms double-loop methods in terms of simplicity and efficiency.</p><hr><h3>Tuning Multi-mode Token-level Prompt Alignment across Modalities</h3>
<p>Dongsheng Wang, Miaoge Li, Xinyang Liu, MingSheng Xu, Bo Chen, Hanwang Zhang</p>
<p><a href='https://openreview.net/forum?id=A253n2EXCd'>https://openreview.net/forum?id=A253n2EXCd</a></p>
<p><b>Keywords</b>: Multi-modal prompt learning; Optimal transport
</p><p><b>Compressor summary</b>: The text describes a new method for improving vision-language models by tuning multiple prompts across modalities using optimal transportation, which enhances semantic alignment and leads to better image recognition performance.</p><hr><h3>Accelerating Molecular Graph Neural Networks via Knowledge Distillation</h3>
<p>Filip Ekström Kelvinius, Dimitar Georgiev, Artur Toshev, Johannes Gasteiger</p>
<p><a href='https://openreview.net/forum?id=A18PgVSUgf'>https://openreview.net/forum?id=A18PgVSUgf</a></p>
<p><b>Keywords</b>: GNN, graph neural networks, knowledge distillation, molecules, molecular simulations
</p><p><b>Compressor summary</b>: The paper explores using knowledge distillation to accelerate molecular graph neural networks without sacrificing predictive accuracy in energy and force prediction tasks.</p><hr><h3>k-Median Clustering via Metric Embedding: Towards Better Initialization with Differential Privacy</h3>
<p>Chenglin Fan, Ping Li, Xiaoyun Li</p>
<p><a href='https://openreview.net/forum?id=9zV2OXCrVF'>https://openreview.net/forum?id=9zV2OXCrVF</a></p>
<p><b>Keywords</b>: privacy, clustering
</p><p><b>Compressor summary</b>: The paper introduces a new initialization method for k-median problem in metric spaces called HST, which produces better and more efficient initial centers than existing methods, and can be extended to generate private initial centers with differential privacy.</p><hr><h3>Three Iterations of (d − 1)-WL Test Distinguish Non Isometric Clouds of d-dimensional Points</h3>
<p>Valentino delle Rose, Alexander Kozachinskiy, Cristobal Rojas, Mircea Petrache, Pablo Barcelo</p>
<p><a href='https://openreview.net/forum?id=9yhYcjsdab'>https://openreview.net/forum?id=9yhYcjsdab</a></p>
<p><b>Keywords</b>: euclidean graphs, point clouds, WL test, graph neural networks
</p><p><b>Compressor summary</b>: The Weisfeiler-Lehman test can distinguish point clouds in Euclidean space using a limited number of iterations and is complete for three dimensions or higher.</p><hr><h3>MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation</h3>
<p>Marco Bellagente, Manuel Brack, Hannah Benita Teufel, Felix Friedrich, Björn Deiseroth, Constantin Eichenberg, Andrew Dai, Robert John Nicholas Baldock, Souradeep Nanda, Koen Oostermeijer, Andres Felipe Cruz-Salinas, Patrick Schramowski, Kristian Kersting, Samuel Weinbach</p>
<p><a href='https://openreview.net/forum?id=9ych3krqP0'>https://openreview.net/forum?id=9ych3krqP0</a></p>
<p><b>Keywords</b>: diffusion, image generation, multimodal
</p><p><b>Compressor summary</b>: MultiFusion is a system that enables complex and nuanced image generation from multiple modalities and languages by fusing pre-trained models without extensive training.</p><hr><h3>Probabilistic Inference in Reinforcement Learning Done Right</h3>
<p>Jean Tarbouriech, Tor Lattimore, Brendan O'Donoghue</p>
<p><a href='https://openreview.net/forum?id=9yQ2aaArDn'>https://openreview.net/forum?id=9yQ2aaArDn</a></p>
<p><b>Keywords</b>: Reinforcement learning, Bayesian inference, Exploration
</p><p><b>Compressor summary</b>: The paper proposes VAPOR, a new variational Bayesian approach for Reinforcement learning that efficiently approximates the posterior probability of state-action optimality and leads to better exploration policies.</p><hr><h3>Diversify Your Vision Datasets with Automatic Diffusion-based Augmentation</h3>
<p>Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E. Gonzalez, Trevor Darrell</p>
<p><a href='https://openreview.net/forum?id=9wrYfqdrwk'>https://openreview.net/forum?id=9wrYfqdrwk</a></p>
<p><b>Keywords</b>: data augmentation, diffusion, vision and language
</p><p><b>Compressor summary</b>: ALIA uses natural language descriptions and large vision models to generate diverse and visually consistent image augmentations for fine-grained classification tasks, improving performance on domain generalization and contextual bias.</p><hr><h3>Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?</h3>
<p>Yutong He, Xinmeng Huang, Kun Yuan</p>
<p><a href='https://openreview.net/forum?id=9v6gpFTfCM'>https://openreview.net/forum?id=9v6gpFTfCM</a></p>
<p><b>Keywords</b>: Communication Compression, Distributed Optimization, Unbiased Compression, Optimal Complexity
</p><p><b>Compressor summary</b>: The paper investigates when unbiased communication compression reduces total communication cost in distributed optimization, showing that it can achieve this under certain conditions, and provides theoretical and empirical evidence for its effectiveness.</p><hr><h3>Dynamic Regret of Adversarial Linear Mixture MDPs</h3>
<p>Long-Fei Li, Peng Zhao, Zhi-Hua Zhou</p>
<p><a href='https://openreview.net/forum?id=9tUjsRLjf2'>https://openreview.net/forum?id=9tUjsRLjf2</a></p>
<p><b>Keywords</b>: dynamic regret, adversarial MDPs, linear mixture MDPs, policy optimization
</p><p><b>Compressor summary</b>: The paper proposes a new algorithm for reinforcement learning in adversarial MDPs with unknown transition kernels, achieving near-optimal dynamic regret.</p><hr><h3>Gradient-Free Kernel Stein Discrepancy</h3>
<p>Matthew A Fisher, Chris J. Oates</p>
<p><a href='https://openreview.net/forum?id=9rmwPAjk9O'>https://openreview.net/forum?id=9rmwPAjk9O</a></p>
<p><b>Keywords</b>: Bayesian, discrepancy, kernel, sampling, Stein's method
</p><p><b>Compressor summary</b>: The paper proposes non-canonical Stein discrepancies, which don't need derivatives, for posterior approximation in complex models, with convergence guarantees and applications in sampling and variational inference.</p><hr><h3>Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning</h3>
<p>Guanlin Liu, Lifeng Lai</p>
<p><a href='https://openreview.net/forum?id=9qlJGjO7bA'>https://openreview.net/forum?id=9qlJGjO7bA</a></p>
<p><b>Keywords</b>: adversarial attacks; multi agent reinforcement learning;
</p><p><b>Compressor summary</b>: The paper investigates how adversarial attacks affect multi-agent reinforcement learning and proposes a mixed attack strategy with both action poisoning and reward poisoning that can efficiently target MARL agents.</p><hr><h3>Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models</h3>
<p>Beier Zhu, Kaihua Tang, Qianru Sun, Hanwang Zhang</p>
<p><a href='https://openreview.net/forum?id=9qG6cMGUWk'>https://openreview.net/forum?id=9qG6cMGUWk</a></p>
<p><b>Keywords</b>: Foundation Model, Class Bias, Generalized Logit Adjustment
</p><p><b>Compressor summary</b>: The paper proposes a method called Generalized Logit Adjustment (GLA) to reduce biases in foundation models like CLIP and improve their performance on various tasks.</p><hr><h3>NeRF-IBVS: Visual Servo Based on NeRF for Visual Localization and Navigation</h3>
<p>Yuanze Wang, Yichao Yan, Dianxi Shi, Wenhan Zhu, Jianqiang Xia, Tan Jeff, Songchang Jin, KE GAO, XIAOBO LI, Xiaokang Yang</p>
<p><a href='https://openreview.net/forum?id=9pLaDXX8m3'>https://openreview.net/forum?id=9pLaDXX8m3</a></p>
<p><b>Keywords</b>: NeRF, Image-Based Visual Servoing (IBVS), visual localization, visual navigation
</p><p><b>Compressor summary</b>: The paper proposes a novel visual localization method that uses few posed images with coarse pseudo-3D labels from NeRF to train a coordinate regression network, estimate a coarse pose with PNP, optimize the pose with image-based visual servo, and provide navigation prior for navigation without custom markers or depth sensors.</p><hr><h3>Geometric Transformer with Interatomic Positional Encoding</h3>
<p>Yusong Wang, Shaoning Li, Tong Wang, Bin Shao, Nanning Zheng, Tie-Yan Liu</p>
<p><a href='https://openreview.net/forum?id=9o6KQrklrE'>https://openreview.net/forum?id=9o6KQrklrE</a></p>
<p><b>Keywords</b>: Geometric Deep Learning, Molecular Modeling, Positional Encoding
</p><p><b>Compressor summary</b>: The paper proposes Geoformer, a novel geometric Transformer that uses Interatomic Positional Encoding to effectively model molecular structures for various property prediction and outperforms existing algorithms on benchmark datasets.</p><hr><h3>Beyond Invariance: Test-Time Label-Shift Adaptation for Addressing "Spurious" Correlations</h3>
<p>Qingyao Sun, Kevin Patrick Murphy, Sayna Ebrahimi, Alexander D'Amour</p>
<p><a href='https://openreview.net/forum?id=9mJXDcr17V'>https://openreview.net/forum?id=9mJXDcr17V</a></p>
<p><b>Keywords</b>: Distribution shift, Spurious correlation, Group robustness
</p><p><b>Compressor summary</b>: The paper proposes a method (TTLSA) to correct for label shifts in test data that includes additional meta-data labels, and shows improved performance on various datasets.</p><hr><h3>Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and Motion Estimation</h3>
<p>Jia-Xing Zhong, Ta-Ying Cheng, Yuhang He, Kai Lu, Kaichen Zhou, Andrew Markham, Niki Trigoni</p>
<p><a href='https://openreview.net/forum?id=9lygTqLdWn'>https://openreview.net/forum?id=9lygTqLdWn</a></p>
<p><b>Keywords</b>: Dynamic Point Cloud Analytics, Multi-body Motion
</p><p><b>Compressor summary</b>: The authors propose a novel method for segmenting and estimating motion in 3D scenes without category information, using an unsupervised training strategy and lightweight architecture.</p><hr><h3>RangePerception: Taming LiDAR Range View for Efficient and Accurate 3D Object Detection</h3>
<p>Yeqi BAI, Ben Fei, Youquan Liu, Tao MA, Yuenan Hou, Botian Shi, Yikang LI</p>
<p><a href='https://openreview.net/forum?id=9kFQEJSyCM'>https://openreview.net/forum?id=9kFQEJSyCM</a></p>
<p><b>Keywords</b>: 3D Detection, Autonomous Driving
</p><p><b>Compressor summary</b>: RangePerception is an efficient and accurate RV-based 3D object detection framework that addresses two challenges in existing methods, achieving higher performance and speed than BEV-based methods.</p><hr><h3>Hypervolume Maximization: A Geometric View of Pareto Set Learning</h3>
<p>Xiaoyuan Zhang, Xi Lin, Bo Xue, Yifan Chen, Qingfu Zhang</p>
<p><a href='https://openreview.net/forum?id=9ieV1hnuva'>https://openreview.net/forum?id=9ieV1hnuva</a></p>
<p><b>Keywords</b>: multiobjective optimization;multitask learning;hypervolume maximization;Pareto set learning
</p><p><b>Compressor summary</b>: The paper proposes a new neural network-based method for modeling the entire Pareto set in multiobjective optimization and shows its effectiveness on several problems.</p><hr><h3>Rewrite Caption Semantics: Bridging Semantic Gaps for Language-Supervised Semantic Segmentation</h3>
<p>Yun Xing, Jian Kang, Aoran Xiao, Jiahao Nie, Ling Shao, Shijian Lu</p>
<p><a href='https://openreview.net/forum?id=9iafshF7s3'>https://openreview.net/forum?id=9iafshF7s3</a></p>
<p><b>Keywords</b>: language-supervised semantic segmentation, vision-language pre-training
</p><p><b>Compressor summary</b>: CoCu is a method that uses CLIP to find and incorporate missing visual concepts from image-text pairs into pre-training, improving zero-shot recognition and language-supervised segmentation performance.</p><hr><h3>(Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy</h3>
<p>Elan Rosenfeld, Saurabh Garg</p>
<p><a href='https://openreview.net/forum?id=9i8MD9btc8'>https://openreview.net/forum?id=9i8MD9btc8</a></p>
<p><b>Keywords</b>: accuracy estimation, error bounds, distribution shift, unsupervised domain adaptation
</p><p><b>Compressor summary</b>: The paper proposes a new bound on the error of deep neural networks under distribution shift using unlabeled test data, which is simpler, more accurate, and easier to evaluate than previous methods.</p><hr><h3>Django: Detecting Trojans in Object Detection Models via Gaussian Focus Calibration</h3>
<p>Guangyu Shen, Siyuan Cheng, Guanhong Tao, Kaiyuan Zhang, Yingqi Liu, Shengwei An, Shiqing Ma, Xiangyu Zhang</p>
<p><a href='https://openreview.net/forum?id=9fb975Au9G'>https://openreview.net/forum?id=9fb975Au9G</a></p>
<p><b>Keywords</b>: backdoor detection; object detection;
</p><p><b>Compressor summary</b>: The paper proposes Django, a backdoor detection framework for object detection models that uses dynamic Gaussian weighting to prioritize vulnerable boxes and improve trigger inversion efficiency.</p><hr><h3>DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics</h3>
<p>Kaiwen Zheng, Cheng Lu, Jianfei Chen, Jun Zhu</p>
<p><a href='https://openreview.net/forum?id=9fWKExmKa0'>https://openreview.net/forum?id=9fWKExmKa0</a></p>
<p><b>Keywords</b>: diffusion models, fast sampling, ODE solver
</p><p><b>Compressor summary</b>: The paper proposes a new fast ODE solver for DPMs that improves sampling efficiency and sample quality, especially with fewer function evaluations or large guidance scales.</p><hr><h3>Inner Product-based Neural Network Similarity</h3>
<p>Wei Chen, Zichen Miao, Qiang Qiu</p>
<p><a href='https://openreview.net/forum?id=9eneYFIGKq'>https://openreview.net/forum?id=9eneYFIGKq</a></p>
<p><b>Keywords</b>: Neural Network Similarity, Filter Subspace
</p><p><b>Compressor summary</b>: The paper proposes a new method to quickly compare neural network models by simplifying their representation to filter subspace distance, which is efficient and robust.</p><hr><h3>Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions</h3>
<p>Dongjie Wang, Meng Xiao, Min Wu, pengfei wang, Yuanchun Zhou, Yanjie Fu</p>
<p><a href='https://openreview.net/forum?id=9dp35y5C0p'>https://openreview.net/forum?id=9dp35y5C0p</a></p>
<p><b>Keywords</b>: Feature Transformation, Autoregressive Generation, Continuous Space Optimization
</p><p><b>Compressor summary</b>: The authors propose a new method to optimize feature transformation as a continuous space optimization task, and demonstrate its effectiveness and robustness through experiments and case studies.</p><hr><h3>Diffusion Representation for Asymmetric Kernels via Magnetic Transform</h3>
<p>Mingzhen He, FAN He, Ruikai Yang, Xiaolin Huang</p>
<p><a href='https://openreview.net/forum?id=9cQzO3rXgR'>https://openreview.net/forum?id=9cQzO3rXgR</a></p>
<p><b>Keywords</b>: Asymmetric kernels, diffusion maps, magnetic transform, dimension reduction
</p><p><b>Compressor summary</b>: The paper introduces MagDM, a new diffusion map technique that can handle asymmetric data using the magnetic transform, and shows its effectiveness on synthetic and real-world datasets.</p><hr><h3>Learning threshold neurons via edge of stability</h3>
<p>Kwangjun Ahn, Sebastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, Yi Zhang</p>
<p><a href='https://openreview.net/forum?id=9cQ6kToLnJ'>https://openreview.net/forum?id=9cQ6kToLnJ</a></p>
<p><b>Keywords</b>: Gradient descent, edge of stability, generalization
</p><p><b>Compressor summary</b>: The paper analyzes how large learning rates affect two-layer neural networks and shows that using a high learning rate can improve generalization by enabling the training of threshold-like neurons.</p><hr><h3>Learning Space-Time Continuous Latent Neural PDEs from Partially Observed States</h3>
<p>Valerii Iakovlev, Markus Heinonen, Harri Lähdesmäki</p>
<p><a href='https://openreview.net/forum?id=9cF6RUwMe7'>https://openreview.net/forum?id=9cF6RUwMe7</a></p>
<p><b>Keywords</b>: neural, PDEs, neural PDEs, partial observations, space time continuous
</p><p><b>Compressor summary</b>: The paper presents a new neural network model that learns partial differential equations from noisy and partial observations without relying on regular grids, achieving high performance and data efficiency.</p><hr><h3>Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality</h3>
<p>Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, Jun Zhu</p>
<p><a href='https://openreview.net/forum?id=9XieH21Tlf'>https://openreview.net/forum?id=9XieH21Tlf</a></p>
<p><b>Keywords</b>: Continual Learning, Catastrophic Forgetting, Pre-training, Prompt Tuning
</p><p><b>Compressor summary</b>: The paper proposes Hierarchical Decomposition (HiDe-)Prompt, an approach that improves the performance of continual learning with self-supervised pre-training by optimizing hierarchical components with task-specific prompts and representation statistics.</p><hr><h3>Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering</h3>
<p>Noah Hollmann, Samuel Müller, Frank Hutter</p>
<p><a href='https://openreview.net/forum?id=9WSxQZ9mG7'>https://openreview.net/forum?id=9WSxQZ9mG7</a></p>
<p><b>Keywords</b>: AutoML, AutoDS, Automated Feature Engineering, LLM Code Generation, Tabular Data, Feature Engineering, Automated Data Science, Automated Machine Learning
</p><p><b>Compressor summary</b>: The authors present CAAFE, a feature engineering method that uses large language models to generate semantically meaningful features for tabular datasets based on their descriptions, improving performance and interpretability in automated machine learning systems.</p><hr><h3>Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models</h3>
<p>Andrew Luo, Margaret Marie Henderson, Leila Wehbe, Michael J. Tarr</p>
<p><a href='https://openreview.net/forum?id=9VqMaSjf7U'>https://openreview.net/forum?id=9VqMaSjf7U</a></p>
<p><b>Keywords</b>: neuroscience, brain, fmri, generative models, diffusion models, image synthesis, visual cortex
</p><p><b>Compressor summary</b>: The authors present BrainDiVE, a data-driven approach to explore the fine-grained functional organization of the brain's higher visual cortex by synthesizing images that activate specific brain regions, revealing novel subdivisions and functional differences within these regions.</p><hr><h3>LuminAIRe: Illumination-Aware Conditional Image Repainting for Lighting-Realistic Generation</h3>
<p>Jiajun Tang, Haofeng Zhong, Shuchen Weng, Boxin Shi</p>
<p><a href='https://openreview.net/forum?id=9UxUTGCteW'>https://openreview.net/forum?id=9UxUTGCteW</a></p>
<p><b>Keywords</b>: Illumination, Image Generation, Conditional Image Repainting
</p><p><b>Compressor summary</b>: The paper introduces a new task (LuminAIRe) and a dataset (Car-LuminAIRe) to improve conditional image repainting methods by using estimated 3D geometry and physically-based illumination rendering.</p><hr><h3>Diffused Task-Agnostic Milestone Planner</h3>
<p>Mineui Hong, Minjae Kang, Songhwai Oh</p>
<p><a href='https://openreview.net/forum?id=9Tx2znbyTm'>https://openreview.net/forum?id=9Tx2znbyTm</a></p>
<p><b>Keywords</b>: Multi-task decision-making, Offline reinforcement learning, Planning, Diffusion model
</p><p><b>Compressor summary</b>: The paper proposes a diffusion-based generative sequence model to plan milestones in a latent space for long-term planning, vision-based control, and multi-task decision-making, achieving superior results on various tasks.</p><hr><h3>Mean-field Langevin dynamics: Time-space discretization, stochastic gradient, and variance reduction</h3>
<p>Taiji Suzuki, Denny Wu, Atsushi Nitanda</p>
<p><a href='https://openreview.net/forum?id=9STYRIVx6u'>https://openreview.net/forum?id=9STYRIVx6u</a></p>
<p><b>Keywords</b>: mean-field regime, interacting particle system, propagation of chaos, Neural network optimization, MMD minimization, kernel stein discrepancy
</p><p><b>Compressor summary</b>: The paper proposes a framework to analyze the MFLD method for training two-layer neural networks and shows its convergence rate and applicability to various learning problems and gradient estimators.</p><hr><h3>Intervention Generalization: A View from Factor Graph Models</h3>
<p>Gecia Bravo-Hermsdorff, David Watson, Jialin Yu, Jakob Zeitler, Ricardo Silva</p>
<p><a href='https://openreview.net/forum?id=9S8oVumknA'>https://openreview.net/forum?id=9S8oVumknA</a></p>
<p><b>Keywords</b>: Causality, experimental design
</p><p><b>Compressor summary</b>: The paper proposes a method to generalize causal inference from past experiments to novel conditions using factor graph models that abstract away unmeasured confounding and feedback mechanisms.</p><hr><h3>Test-time Training for Matching-based Video Object Segmentation</h3>
<p>Juliette Bertrand, Giorgos Kordopatis-Zilos, Yannis Kalantidis, Giorgos Tolias</p>
<p><a href='https://openreview.net/forum?id=9QsdPQlWiE'>https://openreview.net/forum?id=9QsdPQlWiE</a></p>
<p><b>Keywords</b>: VOS, video object segmentation, test-time training, test-time adaptation
</p><p><b>Compressor summary</b>: The paper proposes test-time training strategies for video object segmentation tasks under distribution shifts, such as video corruptions and style transfers, and introduces a new test set with extreme distribution shifts.</p><hr><h3>Robust Learning with Progressive Data Expansion Against Spurious Correlation</h3>
<p>Yihe Deng, Yu Yang, Baharan Mirzasoleiman, Quanquan Gu</p>
<p><a href='https://openreview.net/forum?id=9QEVJ9qm46'>https://openreview.net/forum?id=9QEVJ9qm46</a></p>
<p><b>Keywords</b>: spurious correlation, robustness, robust learning
</p><p><b>Compressor summary</b>: The paper analyzes how two-layer convolutional neural networks can be influenced by non-generalizable features and proposes a new algorithm called PDE that improves robustness and performance on various tasks.</p><hr><h3>Incentives in Federated Learning: Equilibria, Dynamics, and Mechanisms for Welfare Maximization</h3>
<p>Aniket Murhekar, Zhuowen Yuan, Bhaskar Ray Chaudhury, Bo Li, Ruta Mehta</p>
<p><a href='https://openreview.net/forum?id=9OqezkNxnX'>https://openreview.net/forum?id=9OqezkNxnX</a></p>
<p><b>Keywords</b>: Federated learning, Nash equilibrium, Mechanism design, Welfare maximization
</p><p><b>Compressor summary</b>: The paper proposes a budget-balanced mechanism to improve the welfare of agents participating in collaborative federated learning by ensuring optimal trade-offs between learning payoff and data sharing costs.</p><hr><h3>Loss Decoupling for Task-Agnostic Continual Learning</h3>
<p>Yan-Shuo Liang, Wu-Jun Li</p>
<p><a href='https://openreview.net/forum?id=9Oi3YxIBSa'>https://openreview.net/forum?id=9Oi3YxIBSa</a></p>
<p><b>Keywords</b>: Continual Learning, stability, plasticity
</p><p><b>Compressor summary</b>: LODE is a method for task-agnostic continual learning that separates the objectives of distinguishing new classes from old and between different new classes, achieving better stability and plasticity than existing replay-based methods.</p><hr><h3>Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks</h3>
<p>Haoyi Duan, Yan Xia, Mingze Zhou, Li Tang, Jieming Zhu, Zhou Zhao</p>
<p><a href='https://openreview.net/forum?id=9MwidIH4ea'>https://openreview.net/forum?id=9MwidIH4ea</a></p>
<p><b>Keywords</b>: audio-visual, multi-modal prompt, clip, cross-modal attention
</p><p><b>Compressor summary</b>: The paper proposes a novel DG-SCT attention mechanism that uses audio and visual modalities as soft prompts to dynamically adjust the parameters of pre-trained models for multi-modal tasks, achieving state-of-the-art results and few-shot/zero-shot performance.</p><hr><h3>Optimal Transport-Guided Conditional Score-Based Diffusion Model</h3>
<p>Xiang Gu, Liwei Yang, Jian Sun, Zongben Xu</p>
<p><a href='https://openreview.net/forum?id=9Muli2zoFn'>https://openreview.net/forum?id=9Muli2zoFn</a></p>
<p><b>Keywords</b>: optimal transport, diffusion probabilistic model, conditional score-based model, unpaired super-resolution, image-to-image translation
</p><p><b>Compressor summary</b>: OTCS is a novel model for conditional generation of target data using unpaired or partially paired data based on optimal transport, and it achieves effective data transport and image translation.</p><hr><h3>The Behavior and Convergence of Local Bayesian Optimization</h3>
<p>Kaiwen Wu, Kyurae Kim, Roman Garnett, Jacob R. Gardner</p>
<p><a href='https://openreview.net/forum?id=9KtX12YmA7'>https://openreview.net/forum?id=9KtX12YmA7</a></p>
<p><b>Keywords</b>: Bayesian optimization, convergence rates
</p><p><b>Compressor summary</b>: The text discusses the benefits of using local optimization strategies in Bayesian optimization for high-dimensional problems, and provides a rigorous analysis of a recent algorithm's performance and convergence rates.</p><hr><h3>Provable Advantage of Curriculum Learning on Parity Targets with Mixed Inputs</h3>
<p>Emmanuel Abbe, Elisabetta Cornacchia, Aryo Lotfi</p>
<p><a href='https://openreview.net/forum?id=9Ihu0VBOTq'>https://openreview.net/forum?id=9Ihu0VBOTq</a></p>
<p><b>Keywords</b>: curriculum learning, parities, time complexity, sample complexity, neural networks, SGD
</p><p><b>Compressor summary</b>: The text explains that curriculum learning, starting with simpler tasks before more complex ones, can help neural networks learn parities faster than traditional methods, especially when the data is a mix of sparse and dense inputs.</p><hr><h3>Spike-driven Transformer</h3>
<p>Man Yao, JiaKui Hu, Zhaokun Zhou, Li Yuan, Yonghong Tian, Bo XU, Guoqi Li</p>
<p><a href='https://openreview.net/forum?id=9FmolyOHi5'>https://openreview.net/forum?id=9FmolyOHi5</a></p>
<p><b>Keywords</b>: Spiking Neural Networks; Transformer; Neuromorphic Computing; Event-driven; Linear Attention
</p><p><b>Compressor summary</b>: The paper proposes a spike-driven Transformer that uses event-driven and binary spike communication to achieve energy efficiency and state-of-the-art performance in the spiking neural network domain.</p><hr><h3>Should We Learn Most Likely Functions or Parameters?</h3>
<p>Shikai Qiu, Tim G. J. Rudner, Sanyam Kapoor, Andrew Gordon Wilson</p>
<p><a href='https://openreview.net/forum?id=9EndFTDiqh'>https://openreview.net/forum?id=9EndFTDiqh</a></p>
<p><b>Keywords</b>: Function-Space Modeling, Maximum A Posteriori Estimation, Generalization
</p><p><b>Compressor summary</b>: The paper explores how estimating the most likely function implied by a model and data can improve predictive performance, but warns of potential pathological solutions when using neural networks.</p><hr><h3>Scale-teaching: Robust Multi-scale Training for Time Series Classification with Noisy Labels</h3>
<p>Zhen Liu, Peitian Ma, Dongliang Chen, Wenbin Pei, Qianli Ma</p>
<p><a href='https://openreview.net/forum?id=9D0fELXbrg'>https://openreview.net/forum?id=9D0fELXbrg</a></p>
<p><b>Keywords</b>: time series classification, deep neural networks, noisy labels
</p><p><b>Compressor summary</b>: The paper proposes a deep learning approach called Scale-teaching to handle noisy labels in time series data by using multiple DNNs, cross-scale fusion, and label propagation.</p><hr><h3>Accelerating Motion Planning via Optimal Transport</h3>
<p>An Thai Le, Georgia Chalvatzaki, Armin Biess, Jan Peters</p>
<p><a href='https://openreview.net/forum?id=9B9J8X23LK'>https://openreview.net/forum?id=9B9J8X23LK</a></p>
<p><b>Keywords</b>: Motion Planning, Trajectory Optimization, Optimal Transport
</p><p><b>Compressor summary</b>: MPOT is a gradient-free method that optimizes smooth trajectories over nonlinear costs using the Sinkhorn Step and regular polytopes, outperforming other motion planners in various problems.</p><hr><h3>AmadeusGPT: a natural language interface for interactive animal behavioral analysis</h3>
<p>Shaokai Ye, Jessy Lauer, Mu Zhou, Alexander Mathis, Mackenzie W Mathis</p>
<p><a href='https://openreview.net/forum?id=9AcG3Tsyoq'>https://openreview.net/forum?id=9AcG3Tsyoq</a></p>
<p><b>Keywords</b>: ChatGPT, GPT3.5, GPT4, behavioral analysis, LLMs, human-AI interaction, behavioral neuroscience
</p><p><b>Compressor summary</b>: The paragraph introduces AmadeusGPT, a natural language interface that converts descriptions of animal behaviors into machine code using large-language models and a dual-memory mechanism, improving the efficiency and accuracy of behavior analysis.</p><hr><h3>Scenario Diffusion: Controllable Driving Scenario Generation With Diffusion</h3>
<p>Ethan Pronovost, Meghana Reddy Ganesina, Noureldin Hendy, Zeyu Wang, Andres Morales, Kai Wang, Nicholas Roy</p>
<p><a href='https://openreview.net/forum?id=99MHSB98yZ'>https://openreview.net/forum?id=99MHSB98yZ</a></p>
<p><b>Keywords</b>: Deep Learning, (Other) Applications, (Other) Machine Learning Topics
</p><p><b>Compressor summary</b>: Scenario Diffusion generates diverse and controllable synthetic traffic scenarios for autonomous vehicles using latent diffusion, object detection, and trajectory regression.</p><hr><h3>Generalization bounds for neural ordinary differential equations and deep residual networks</h3>
<p>Pierre Marion</p>
<p><a href='https://openreview.net/forum?id=992vogTP1L'>https://openreview.net/forum?id=992vogTP1L</a></p>
<p><b>Keywords</b>: residual neural networks, neural ODEs, generalization bound
</p><p><b>Compressor summary</b>: The paper develops a generalization bound for continuous-in-time parameterized ODEs, which includes time-dependent neural ODEs and deep residual networks, by using a Lipschitz-based argument that relates the magnitude of weight matrix differences to generalization.</p><hr><h3>Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration</h3>
<p>Dongyoung Kim, Jinwoo Shin, Pieter Abbeel, Younggyo Seo</p>
<p><a href='https://openreview.net/forum?id=97E3YXvcFM'>https://openreview.net/forum?id=97E3YXvcFM</a></p>
<p><b>Keywords</b>: Reinforcement Learning, State Entropy, Exploration
</p><p><b>Compressor summary</b>: The paper proposes a new exploration technique for reinforcement learning that considers the value of states when maximizing state entropy to prevent bias and improve performance in various tasks.</p><hr><h3>Towards Data-Algorithm Dependent Generalization: a Case Study on Overparameterized Linear Regression</h3>
<p>Jing Xu, Jiaye Teng, Yang Yuan, Andrew C Yao</p>
<p><a href='https://openreview.net/forum?id=966yOmwk6d'>https://openreview.net/forum?id=966yOmwk6d</a></p>
<p><b>Keywords</b>: data-algorithm dependent generalization analysis, overparameterized linear regression
</p><p><b>Compressor summary</b>: The paper proposes a new concept called data-algorithm compatibility to analyze generalization in overparameterized models by considering the entire training trajectory instead of just the final model.</p><hr><h3>Generalizable One-shot 3D Neural Head Avatar</h3>
<p>Xueting Li, Shalini De Mello, Sifei Liu, Koki Nagano, Umar Iqbal, Jan Kautz</p>
<p><a href='https://openreview.net/forum?id=95q46MpBGZ'>https://openreview.net/forum?id=95q46MpBGZ</a></p>
<p><b>Keywords</b>: Neural Radiance Field, Portrait Reconstruction and Animation
</p><p><b>Compressor summary</b>: The authors present a method that can reconstruct and animate realistic 3D head avatars from single-view images, generalizing to different identities and capturing details beyond the face. The method uses three branches with tri-planes and volumetric rendering for high fidelity results.</p><hr><h3>Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power</h3>
<p>Junru Zhou, Jiarui Feng, Xiyuan Wang, Muhan Zhang</p>
<p><a href='https://openreview.net/forum?id=94rKFkcm56'>https://openreview.net/forum?id=94rKFkcm56</a></p>
<p><b>Keywords</b>: Cycle counting, graph neural networks
</p><p><b>Compressor summary</b>: The paper introduces $d$-Distance-Restricted FWL(2) GNNs, a novel class of graph neural networks that can efficiently count certain graph substructures like cycles by exploiting graph sparsity and avoiding expensive preprocessing steps.</p><hr><h3>UltraRE: Enhancing RecEraser for Recommendation Unlearning via Error Decomposition</h3>
<p>Yuyuan Li, Chaochao Chen, Yizhao Zhang, Weiming Liu, Lingjuan Lyu, Xiaolin Zheng, Dan Meng, Jun Wang</p>
<p><a href='https://openreview.net/forum?id=93NLxUojvc'>https://openreview.net/forum?id=93NLxUojvc</a></p>
<p><b>Keywords</b>: recommendation unlearning, machine unlearning, recommender systems, ensemble learning
</p><p><b>Compressor summary</b>: UltraRE is a new framework that enhances RecEraser's recommendation unlearning by addressing redundancy, relevance, and combination losses.</p><hr><h3>Self-supervised Object-Centric Learning for Videos</h3>
<p>Görkay Aydemir, Weidi Xie, Fatma Guney</p>
<p><a href='https://openreview.net/forum?id=919tWtJPXe'>https://openreview.net/forum?id=919tWtJPXe</a></p>
<p><b>Keywords</b>: Unsupervised Object Discovery, Unsupervised Video Object Segmentation, Object-Centric Learning, Unsupervised Video Multi Object Segmentation
</p><p><b>Compressor summary</b>: The paper presents an unsupervised method for segmenting multiple objects in real-world video sequences using object-centric learning and a masking strategy.</p><hr><h3>Distribution-Free Statistical Dispersion Control for Societal Applications</h3>
<p>Zhun Deng, Thomas P Zollo, Jake Snell, Toniann Pitassi, Richard Zemel</p>
<p><a href='https://openreview.net/forum?id=917crxqJdA'>https://openreview.net/forum?id=917crxqJdA</a></p>
<p><b>Keywords</b>: societal dispersion, distribution-free uncertainty quantification
</p><p><b>Compressor summary</b>: The paragraph discusses the importance of controlling the dispersion of loss distribution in machine learning models for high-stakes applications and proposes a new framework to handle various statistical functionals.</p><hr><h3>GUST: Combinatorial Generalization by Unsupervised Grouping with Neuronal Coherence</h3>
<p>Hao Zheng, Hui Lin, Rong Zhao</p>
<p><a href='https://openreview.net/forum?id=90O5cvFZkZ'>https://openreview.net/forum?id=90O5cvFZkZ</a></p>
<p><b>Keywords</b>: neuronal coherence, combinatorial generalization, perceptual grouping, unsupervised learning
</p><p><b>Compressor summary</b>: GUST is an iterative neural network architecture that mimics neuronal coherence in the human brain to learn how to group sensory information effectively and create symbolic representations of scenes.</p><hr><h3>Training neural operators to preserve invariant measures of chaotic attractors</h3>
<p>Ruoxi Jiang, Peter Y. Lu, Elena Orlova, Rebecca Willett</p>
<p><a href='https://openreview.net/forum?id=8xx0pyMOW1'>https://openreview.net/forum?id=8xx0pyMOW1</a></p>
<p><b>Keywords</b>: Neural operators, contrastive learning, optimal transport, chaotic attractors, invariant measures
</p><p><b>Compressor summary</b>: The paper proposes new methods for training neural networks to forecast chaotic systems over long time horizons while preserving their statistical properties using optimal transport or contrastive learning.</p><hr><h3>TMT-VIS: Taxonomy-aware Multi-dataset Joint Training for Video Instance Segmentation</h3>
<p>rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, Hengshuang Zhao</p>
<p><a href='https://openreview.net/forum?id=8xTOtxinMH'>https://openreview.net/forum?id=8xTOtxinMH</a></p>
<p><b>Keywords</b>: taxonomy-aware, multiple-datasets, video instance segementation
</p><p><b>Compressor summary</b>: The paper proposes TMT-VIS, a model that uses taxonomy information to improve video instance segmentation performance on multiple datasets.</p><hr><h3>OpenMask3D: Open-Vocabulary 3D Instance Segmentation</h3>
<p>Ayça Takmaz, Elisabetta Fedele, Robert Sumner, Marc Pollefeys, Federico Tombari, Francis Engelmann</p>
<p><a href='https://openreview.net/forum?id=8vuDHCxrmy'>https://openreview.net/forum?id=8vuDHCxrmy</a></p>
<p><b>Keywords</b>: open-world, open-vocabulary, 3D vision, point cloud, instance segmentation, 3D instance segmentation
</p><p><b>Compressor summary</b>: OpenMask3D is a method for open-vocabulary 3D instance segmentation that uses multi-view fusion of CLIP-based image embeddings to separate multiple objects in scenes.</p><hr><h3>Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts</h3>
<p>Eduard Tulchinskii, Kristian Kuznetsov, Kushnareva Laida, Daniil Cherniavskii, Sergey Nikolenko, Evgeny Burnaev, Serguei Barannikov, Irina Piontkovskaya</p>
<p><a href='https://openreview.net/forum?id=8uOZ0kNji6'>https://openreview.net/forum?id=8uOZ0kNji6</a></p>
<p><b>Keywords</b>: generated texts detection, intrinsic dimension, TDA, Persistent Homology, ChatGPT
</p><p><b>Compressor summary</b>: The authors propose a new method to distinguish between natural and AI-generated texts based on the intrinsic dimensionality of text embeddings, which is lower for AI-generated texts and stable across different languages and domains.</p><hr><h3>Constant Approximation for Individual Preference Stable Clustering</h3>
<p>Anders Aamand, Justin Y. Chen, Allen Liu, Sandeep Silwal, Pattara Sukprasert, Ali Vakilian, Fred Zhang</p>
<p><a href='https://openreview.net/forum?id=8rDbUoYc0p'>https://openreview.net/forum?id=8rDbUoYc0p</a></p>
<p><b>Keywords</b>: clustering, fairness, approximation algorithms
</p><p><b>Compressor summary</b>: The paper introduces IP stability as a new clustering objective, shows that an O(1)-IP stable clustering always exists for general metrics, and provides efficient algorithms for various generalizations of IP stability.</p><hr><h3>One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation</h3>
<p>Zhiwei Hao, Jianyuan Guo, Kai Han, Yehui Tang, Han Hu, Yunhe Wang, Chang Xu</p>
<p><a href='https://openreview.net/forum?id=8qePPvL1VY'>https://openreview.net/forum?id=8qePPvL1VY</a></p>
<p><b>Keywords</b>: knowledge distillation, feature distillation, heterogeneous architectures
</p><p><b>Compressor summary</b>: The paper proposes OFA-KD, a framework for distilling knowledge between heterogeneous models that improves performance by aligning features and adaptively enhancing the target.</p><hr><h3>FLSL: Feature-level Self-supervised Learning</h3>
<p>Qing Su, Anton Netchaev, Hai Li, Shihao Ji</p>
<p><a href='https://openreview.net/forum?id=8pOBo5NgTQ'>https://openreview.net/forum?id=8pOBo5NgTQ</a></p>
<p><b>Keywords</b>: Transformer, ViT, Dense Prediction, Self-supervised Learning, Mean Shift, Self-attention, Representation learning
</p><p><b>Compressor summary</b>: The paper proposes a new SSL method, FLSL, that uses transformer for joint embedding and clustering, which improves dense prediction tasks like object detection and segmentation.</p><hr><h3>Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells</h3>
<p>Rylan Schaeffer, Mikail Khona, Tzuhsuan Ma, Cristobal Eyzaguirre, Sanmi Koyejo, Ila R Fiete</p>
<p><a href='https://openreview.net/forum?id=8ox2vrQiTF'>https://openreview.net/forum?id=8ox2vrQiTF</a></p>
<p><b>Keywords</b>: self-supervised learning, unsupervised learning, grid cells, neuroscience, systems neuroscience, representation learning
</p><p><b>Compressor summary</b>: The authors propose a new self-supervised learning framework to synthesize multi-periodic grid cells in deep recurrent neural networks, drawing from dynamical systems, coding theory, function optimization and supervised deep learning approaches.</p><hr><h3>Sparse Deep Learning for Time Series Data: Theory and Applications</h3>
<p>Mingxuan Zhang, Yan Sun, Faming Liang</p>
<p><a href='https://openreview.net/forum?id=8niGwlkLAX'>https://openreview.net/forum?id=8niGwlkLAX</a></p>
<p><b>Keywords</b>: Sparse Deep Learning, Uncertainty Quantification, Model Compression, Variable Selection, Dependent Data
</p><p><b>Compressor summary</b>: The paper studies sparse deep learning with dependent data, showing that sparse recurrent neural networks can be consistently estimated and outperform existing methods in uncertainty quantification and model compression for time series data.</p><hr><h3>Towards Stable Backdoor Purification through Feature Shift Tuning</h3>
<p>Rui Min, Zeyu Qin, Li Shen, Minhao Cheng</p>
<p><a href='https://openreview.net/forum?id=8muKbaAgsh'>https://openreview.net/forum?id=8muKbaAgsh</a></p>
<p><b>Keywords</b>: Backdoor Defense, Model-tuning
</p><p><b>Compressor summary</b>: The paper proposes Feature Shift Tuning (FST), a method to disentangle backdoor and clean features for tuning-based backdoor purification, which performs well against diverse attack scenarios with low tuning costs.</p><hr><h3>Investigating how ReLU-networks encode symmetries</h3>
<p>Georg Bökman, Fredrik Kahl</p>
<p><a href='https://openreview.net/forum?id=8lbFwpebeu'>https://openreview.net/forum?id=8lbFwpebeu</a></p>
<p><b>Keywords</b>: loss landscape, network merging, linear mode connectivity, equivariance, group convolutional neural network, permutation, group, symmetry, invariance, weight space ensembling
</p><p><b>Compressor summary</b>: The authors study how neural networks' equivariance properties affect their layers and propose a conjecture related to the recent permutation conjecture, while providing experiments to support their findings.</p><hr><h3>When Does Optimizing a Proper Loss Yield Calibration?</h3>
<p>Jarosław Błasiok, Parikshit Gopalan, Lunjia Hu, Preetum Nakkiran</p>
<p><a href='https://openreview.net/forum?id=8kyIChWsAG'>https://openreview.net/forum?id=8kyIChWsAG</a></p>
<p><b>Keywords</b>: calibration, deep learning, theory, optimization
</p><p><b>Compressor summary</b>: This work studies under what conditions optimizing a proper loss function over a restricted family of predictors yields calibrated models, and provides a rigorous answer by introducing the concept of local optimality.</p><hr><h3>Closing the Computational-Statistical Gap in Best Arm Identification for Combinatorial Semi-bandits</h3>
<p>Ruo-Chun Tzeng, Po-An Wang, Alexandre Proutiere, Chi-Jen Lu</p>
<p><a href='https://openreview.net/forum?id=8jg8z3ASiw'>https://openreview.net/forum?id=8jg8z3ASiw</a></p>
<p><b>Keywords</b>: best-arm identification; combinatorial semi-bandit; no-regret learning;
</p><p><b>Compressor summary</b>: Perturbed Frank-Wolfe Sampling (P-FWS) is an algorithm that efficiently identifies the best arm in combinatorial semi-bandits by solving an optimization problem with a single iteration of the Frank-Wolfe algorithm and leveraging structural properties.</p><hr><h3>A Riemannian Exponential Augmented Lagrangian Method for Computing the Projection Robust Wasserstein Distance</h3>
<p>Bo Jiang, Ya-Feng Liu</p>
<p><a href='https://openreview.net/forum?id=8hKCNVqrlf'>https://openreview.net/forum?id=8hKCNVqrlf</a></p>
<p><b>Keywords</b>: Barzilai-Borwein method, exponential augmented Lagrangian, inexact gradient, Stiefel manifold, Sinkhorn iteration, Wasserstein distance
</p><p><b>Compressor summary</b>: The paper proposes a new method (REALM) to compute the projection robust Wasserstein distance more efficiently and accurately than existing methods, by using an inexact Riemannian Barzilai-Borwein method with Sinkhorn iteration.</p><hr><h3>Uniform-in-Time Wasserstein Stability Bounds for (Noisy) Stochastic Gradient Descent</h3>
<p>Lingjiong Zhu, Mert Gurbuzbalaban, Anant Raj, Umut Simsekli</p>
<p><a href='https://openreview.net/forum?id=8fLatmFQgF'>https://openreview.net/forum?id=8fLatmFQgF</a></p>
<p><b>Keywords</b>: Algorithmic stability, SGD, Wasserstein distance
</p><p><b>Compressor summary</b>: The paper introduces a unified proof technique for deriving Wasserstein stability bounds for stochastic optimization algorithms like SGD, showing that ergodicity is crucial for time-uniform bounds and extending previous results to more general cases.</p><hr><h3>Accurate Interpolation for Scattered Data through Hierarchical Residual Refinement</h3>
<p>Shizhe Ding, Boyang Xia, Dongbo Bu</p>
<p><a href='https://openreview.net/forum?id=8d9wVXri89'>https://openreview.net/forum?id=8d9wVXri89</a></p>
<p><b>Keywords</b>: Interpolation algorithm, scattered data, deep learning, residual learning
</p><p><b>Compressor summary</b>: HINT is a novel neural network-based interpolation method that uses residuals on observed points to guide target function estimation and hierarchical local constraints in correlation modeling to improve interpolation accuracy.</p><hr><h3>Provably Efficient Offline Reinforcement Learning in Regular Decision Processes</h3>
<p>Roberto Cipollone, Anders Jonsson, Alessandro Ronca, Mohammad Sadegh Talebi</p>
<p><a href='https://openreview.net/forum?id=8bQc7oRnjm'>https://openreview.net/forum?id=8bQc7oRnjm</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Offline Reinforcement Learning, Regular Decision Processes, Sample complexity, Automata
</p><p><b>Compressor summary</b>: The paper introduces RegORL, an algorithm that combines automata learning and offline RL to learn near-optimal policies in unknown episodic RDPs using pre-collected data without further exploration.</p><hr><h3>Convex and Non-convex Optimization Under Generalized Smoothness</h3>
<p>Haochuan Li, Jian Qian, Yi Tian, Alexander Rakhlin, Ali Jadbabaie</p>
<p><a href='https://openreview.net/forum?id=8aunGrXdkl'>https://openreview.net/forum?id=8aunGrXdkl</a></p>
<p><b>Keywords</b>: Optimization, Convergence, Generalized smoothness
</p><p><b>Compressor summary</b>: The paper proposes a generalized non-uniform smoothness condition for optimization problems, which enables convergence rates for various methods in both convex and non-convex settings without gradient clipping or heavy-tailed noise restrictions.</p><hr><h3>No Change, No Gain: Empowering Graph Neural Networks with Expected Model Change Maximization for Active Learning</h3>
<p>Zixing Song, Yifei Zhang, Irwin King</p>
<p><a href='https://openreview.net/forum?id=8aDG51pxFc'>https://openreview.net/forum?id=8aDG51pxFc</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Expected Model Change Maximization
</p><p><b>Compressor summary</b>: The paper proposes an active learning method for graph neural networks that uses the Expected Model Change Maximization principle to improve prediction performance on unlabeled data.</p><hr><h3>On the Convergence of Encoder-only Shallow Transformers</h3>
<p>Yongtao Wu, Fanghui Liu, Grigorios Chrysos, Volkan Cevher</p>
<p><a href='https://openreview.net/forum?id=8ZveVHfmIE'>https://openreview.net/forum?id=8ZveVHfmIE</a></p>
<p><b>Keywords</b>: Transformer, convergence, scaling, initialization, over-parameterization
</p><p><b>Compressor summary</b>: The paper develops a global convergence theory for shallow Transformers with encoder-only architecture under realistic settings and analyzes the impact of architectures, initialization, scaling, and softmax on their performance.</p><hr><h3>A Unified Discretization Framework for Differential Equation Approach with Lyapunov Arguments for Convex Optimization</h3>
<p>Kansei Ushiyama, Shun Sato, Takayasu Matsuo</p>
<p><a href='https://openreview.net/forum?id=8YN62t19AW'>https://openreview.net/forum?id=8YN62t19AW</a></p>
<p><b>Keywords</b>: Convex optimization, Numerical analysis, Ordinary differential equations, Convergence estimate
</p><p><b>Compressor summary</b>: This paper introduces a concept called weak discrete gradient (wDG) that allows transitioning from continuous to discrete optimization methods in the differential equation approach for convex optimization, simplifying analysis and enabling faster convergence rates.</p><hr><h3>Mitigating the Effect of Incidental Correlations on Part-based Learning</h3>
<p>Gaurav Bhatt, Deepayan Das, Leonid Sigal, Vineeth N. Balasubramanian</p>
<p><a href='https://openreview.net/forum?id=8Xn3D9OtqI'>https://openreview.net/forum?id=8Xn3D9OtqI</a></p>
<p><b>Keywords</b>: part-based learning, interpretability, few-shot learning, vision transformers
</p><p><b>Compressor summary</b>: The study proposes two regularization methods to improve interpretability and generalization of part-based representations in intelligent systems by reducing incidental background correlations.</p><hr><h3>Near-Optimal $k$-Clustering in the Sliding Window Model</h3>
<p>David Woodruff, Peilin Zhong, Samson Zhou</p>
<p><a href='https://openreview.net/forum?id=8XRMbNAP6Z'>https://openreview.net/forum?id=8XRMbNAP6Z</a></p>
<p><b>Keywords</b>: clustering, streaming algorithms, sliding window model
</p><p><b>Compressor summary</b>: The paper presents the first algorithm that achieves near-optimal clustering in the sliding window model and develops an online coreset data structure for this purpose.</p><hr><h3>MixFormerV2: Efficient Fully Transformer Tracking</h3>
<p>Yutao Cui, Tianhui Song, Gangshan Wu, Limin Wang</p>
<p><a href='https://openreview.net/forum?id=8WvYAycmDJ'>https://openreview.net/forum?id=8WvYAycmDJ</a></p>
<p><b>Keywords</b>: Efficient Tracking, Fully Transformer, Distillation, Model Pruning
</p><p><b>Compressor summary</b>: The paper proposes MixFormerV2, a transformer-based tracker that uses four special prediction tokens and a new distillation-based model reduction paradigm to achieve high accuracy and efficiency on both GPU and CPU platforms.</p><hr><h3>The Grand Illusion: The Myth of Software Portability and Implications for ML Progress.</h3>
<p>Fraser Mince, Dzung Dinh, Jonas Kgomo, Neil Thompson, Sara Hooker</p>
<p><a href='https://openreview.net/forum?id=8VTbfVfAfI'>https://openreview.net/forum?id=8VTbfVfAfI</a></p>
<p><b>Keywords</b>: hardware, software, meta study, portability
</p><p><b>Compressor summary</b>: The lack of portability in popular ML frameworks across different hardware types hinders exploratory research and innovation in machine learning due to significant function loss and performance slowdown.</p><hr><h3>Learning Shared Safety Constraints from Multi-task Demonstrations</h3>
<p>Konwoo Kim, Gokul Swamy, Zuxin Liu, Ding Zhao, Sanjiban Choudhury, Steven Wu</p>
<p><a href='https://openreview.net/forum?id=8U31BCquNF'>https://openreview.net/forum?id=8U31BCquNF</a></p>
<p><b>Keywords</b>: constraints, inverse reinforcement learning, safe reinforcement learning
</p><p><b>Compressor summary</b>: The paper proposes a method to learn safety constraints from expert demonstrations for robotic tasks, using inverse reinforcement learning and diverse demonstrations to avoid overly conservative constraints.</p><hr><h3>Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination</h3>
<p>Yuchen BAI, Jean-Baptiste Durand, Grégoire Laurent Vincent, Florence Forbes</p>
<p><a href='https://openreview.net/forum?id=8SUtvEZCF2'>https://openreview.net/forum?id=8SUtvEZCF2</a></p>
<p><b>Keywords</b>: UAV, Deep Learning, Semantic Segmentation, Lidar, Class Imbalance, Point Cloud
</p><p><b>Compressor summary</b>: Lidar technology helps monitor forests and track climate change, but identifying leaf points from wood points in UAV data is challenging; a new neural network model based on Pointnet ++ architecture uses point geometry only to overcome these issues.</p><hr><h3>Real-World Image Super-Resolution as Multi-Task Learning</h3>
<p>Wenlong Zhang, Xiaohui Li, Guangyuan SHI, Xiangyu Chen, Yu Qiao, Xiaoyun Zhang, Xiao-Ming Wu, Chao Dong</p>
<p><a href='https://openreview.net/forum?id=8SCz56sUGP'>https://openreview.net/forum?id=8SCz56sUGP</a></p>
<p><b>Keywords</b>: Image super-resolution
</p><p><b>Compressor summary</b>: The paper proposes a multi-task learning approach to improve real-world image super-resolution by grouping similar degradation tasks together and training them separately, which reduces task competition and enhances performance.</p><hr><h3>Data-driven Optimal Filtering for Linear Systems with Unknown Noise Covariances</h3>
<p>Shahriar Talebi, Amirhossein Taghvaei, Mehran Mesbahi</p>
<p><a href='https://openreview.net/forum?id=8S9Fbee743'>https://openreview.net/forum?id=8S9Fbee743</a></p>
<p><b>Keywords</b>: Optimal filtering, data-driven control, stochastic optimization, learning
</p><p><b>Compressor summary</b>: The paper studies how to learn the best filtering policy for a linear system with unknown noise matrices using noisy data, and proposes a method that minimizes prediction error, has good convergence properties, and scales well with problem size.</p><hr><h3>New Bounds for Hyperparameter Tuning of Regression Problems Across Instances</h3>
<p>Nina Balcan, Anh Tuan Nguyen, Dravyansh Sharma</p>
<p><a href='https://openreview.net/forum?id=8QGukmdAbh'>https://openreview.net/forum?id=8QGukmdAbh</a></p>
<p><b>Keywords</b>: Elastic Net, logistic regression, data-driven algorithm design, learning theory, regularization
</p><p><b>Compressor summary</b>: This paper studies how many samples are needed to tune regularization parameters in linear and logistic regressions with different constraints, and provides new upper and lower bounds for their validation loss functions.</p><hr><h3>Rethinking Gauss-Newton for learning over-parameterized models</h3>
<p>Michael Arbel, Romain Menegaux, Pierre Wolinski</p>
<p><a href='https://openreview.net/forum?id=8Oukmqfek2'>https://openreview.net/forum?id=8Oukmqfek2</a></p>
<p><b>Keywords</b>: implicit bias, gauss newton
</p><p><b>Compressor summary</b>: The paper analyzes how Gauss Newton's method optimizes over-parameterized networks, finding that it converges faster than gradient descent but has an implicit bias that affects generalization.</p><hr><h3>Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models</h3>
<p>Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, Kimin Lee</p>
<p><a href='https://openreview.net/forum?id=8OTPepXzeh'>https://openreview.net/forum?id=8OTPepXzeh</a></p>
<p><b>Keywords</b>: Diffusion models, RLHF
</p><p><b>Compressor summary</b>: The authors propose using online reinforcement learning to fine-tune text-to-image diffusion models, achieving better alignment and image quality than supervised fine-tuning methods.</p><hr><h3>Few-Shot Class-Incremental Learning via Training-Free Prototype Calibration</h3>
<p>Qi-Wei Wang, Da-Wei Zhou, Yi-Kai Zhang, De-Chuan Zhan, Han-Jia Ye</p>
<p><a href='https://openreview.net/forum?id=8NAxGDdf7H'>https://openreview.net/forum?id=8NAxGDdf7H</a></p>
<p><b>Keywords</b>: Few-Shot Class-Incremental Learning, Continual Learning, Class-Incremental Learning
</p><p><b>Compressor summary</b>: The paper introduces a new strategy (TEEN) to improve few-shot class-incremental learning by enhancing the discriminability of new classes using weighted prototypes, and shows its effectiveness on standard benchmarks and few-shot learning tasks.</p><hr><h3>Bootstrapping Vision-Language Learning with Decoupled Language Pre-training</h3>
<p>Yiren Jian, Chongyang Gao, Soroush Vosoughi</p>
<p><a href='https://openreview.net/forum?id=8Kch0ILfQH'>https://openreview.net/forum?id=8Kch0ILfQH</a></p>
<p><b>Keywords</b>: vision-language pretraining, multi-modal learning, uni-modal auxiliary learning
</p><p><b>Compressor summary</b>: The Prompt-Transformer (P-Former) is a model that predicts optimal prompts for language models to align with visual features, improving performance and reducing the need for image-text pairs in vision-language pre-training.</p><hr><h3>3D Indoor Instance Segmentation in an Open-World</h3>
<p>Mohamed El Amine Boudjoghra, Salwa K. Al Khatib, Jean Lahoud, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Fahad Khan</p>
<p><a href='https://openreview.net/forum?id=8JsbdJjRvY'>https://openreview.net/forum?id=8JsbdJjRvY</a></p>
<p><b>Keywords</b>: open-world, 3d instance segmentation
</p><p><b>Compressor summary</b>: The paper presents a new method for 3D indoor instance segmentation that can handle unknown classes and improve its performance using pseudo-labels and realistic open-world scenarios.</p><hr><h3>DAC-DETR: Divide the Attention Layers and Conquer</h3>
<p>Zhengdong Hu, Yifan Sun, Jingdong Wang, Yi Yang</p>
<p><a href='https://openreview.net/forum?id=8JMexYVcXB'>https://openreview.net/forum?id=8JMexYVcXB</a></p>
<p><b>Keywords</b>: deep learning, computer vision, object detection, transformer
</p><p><b>Compressor summary</b>: The paper presents a method called Divide-And-Conquer DETR (DAC-DETR) that improves object detection by separating cross-attention and self-attention layers in the decoder, leading to better performance on popular benchmarks.</p><hr><h3>Systematic Visual Reasoning through Object-Centric Relational Abstraction</h3>
<p>Taylor Whittington Webb, Shanka Subhra Mondal, Jonathan Cohen</p>
<p><a href='https://openreview.net/forum?id=8JCZe7QrPy'>https://openreview.net/forum?id=8JCZe7QrPy</a></p>
<p><b>Keywords</b>: relational reasoning, object-centric representations, abstract rule learning, relational inductive biases, systematic generalization
</p><p><b>Compressor summary</b>: The Object-Centric Relational Abraction (OCRA) model combines object-centric and relational abstraction approaches to achieve strong systematic generalization in complex visual tasks.</p><hr><h3>Exploring Loss Functions for Time-based Training Strategy in Spiking Neural Networks</h3>
<p>Yaoyu Zhu, Wei Fang, Xiaodong Xie, Tiejun Huang, Zhaofei Yu</p>
<p><a href='https://openreview.net/forum?id=8IvW2k5VeA'>https://openreview.net/forum?id=8IvW2k5VeA</a></p>
<p><b>Keywords</b>: Spiking neural networks, Spike encoding, Time-based training
</p><p><b>Compressor summary</b>: The paper proposes an enhanced counting loss for training spiking neural networks (SNNs) with time-based schemes, which better utilizes temporal information and improves performance on most datasets.</p><hr><h3>Efficient Subgame Refinement for Extensive-form Games</h3>
<p>Zhenxing Ge, Zheng Xu, Tianyu Ding, Wenbin Li, Yang Gao</p>
<p><a href='https://openreview.net/forum?id=8HzOyg1ngp'>https://openreview.net/forum?id=8HzOyg1ngp</a></p>
<p><b>Keywords</b>: Subgame solving, extensive-form game, imperfect information
</p><p><b>Compressor summary</b>: The GS2 framework uses a generation function to identify important subgame nodes, reducing their size and improving performance in large imperfect information games.</p><hr><h3>Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning</h3>
<p>Jialong Wu, Haoyu Ma, Chaoyi Deng, Mingsheng Long</p>
<p><a href='https://openreview.net/forum?id=8GuEVzAUQS'>https://openreview.net/forum?id=8GuEVzAUQS</a></p>
<p><b>Keywords</b>: Model-based reinforcement learning, world model, pre-training
</p><p><b>Compressor summary</b>: The paper proposes Contextualized World Models to pre-train world models with in-the-wild videos for efficient learning of visual control tasks in reinforcement learning.</p><hr><h3>Conservative State Value Estimation for Offline Reinforcement Learning</h3>
<p>Liting Chen, Jie Yan, Zhengdao Shao, Lu Wang, Qingwei Lin, Saravan Rajmohan, Thomas Moscibroda, Dongmei Zhang</p>
<p><a href='https://openreview.net/forum?id=8GSCaoFot9'>https://openreview.net/forum?id=8GSCaoFot9</a></p>
<p><b>Keywords</b>: Offline Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper proposes a new approach called Conservative State Value Estimation (CSVE) for offline reinforcement learning, which improves state value estimation by penalizing out-of-distribution states and performs well in classic continual control tasks of D4RL.</p><hr><h3>Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling</h3>
<p>Jack Jewson, Sahra Ghalebikesabi, Christopher C. Holmes</p>
<p><a href='https://openreview.net/forum?id=8FbuHeVU7D'>https://openreview.net/forum?id=8FbuHeVU7D</a></p>
<p><b>Keywords</b>: differential privacy, beta-divergence, posterior sampling, generalised Bayesian inference
</p><p><b>Compressor summary</b>: $\beta$D-Bayes is a new method that allows more accurate and private estimation for various types of statistical models without altering the original data or model.</p><hr><h3>Exploiting Connections between Lipschitz Structures for Certifiably Robust Deep Equilibrium Models</h3>
<p>Aaron J Havens, Alexandre Araujo, Siddharth Garg, Farshad Khorrami, Bin Hu</p>
<p><a href='https://openreview.net/forum?id=8F3Lutda7R'>https://openreview.net/forum?id=8F3Lutda7R</a></p>
<p><b>Keywords</b>: Deep equilibrium models, Lipschitz networks, certifiable robustness
</p><p><b>Compressor summary</b>: The paper explores how different deep equilibrium models can be reparameterized as Lipschitz-bounded equilibrium networks, which could improve their certified robustness.</p><hr><h3>Analyzing Generalization of Neural Networks through Loss Path Kernels</h3>
<p>Yilan Chen, Wei Huang, Hao Wang, Charlotte Loh, Akash Srivastava, Lam M. Nguyen, Tsui-Wei Weng</p>
<p><a href='https://openreview.net/forum?id=8Ba7VJ7xiM'>https://openreview.net/forum?id=8Ba7VJ7xiM</a></p>
<p><b>Keywords</b>: generalization, deep learning theory, neural tangent kernel, neural architecture search
</p><p><b>Compressor summary</b>: This paper introduces a new kernel called loss path kernel, which measures data similarity using loss gradient agreement along gradient flow paths, and shows its application in deriving tight generalization bounds for neural networks and guiding neural architecture search.</p><hr><h3>Label-efficient Segmentation via Affinity Propagation</h3>
<p>Wentong Li, Yuqian Yuan, Song Wang, Wenyu Liu, Dongqi Tang, Jian liu, Jianke Zhu, Lei Zhang</p>
<p><a href='https://openreview.net/forum?id=8BPzLxF9p5'>https://openreview.net/forum?id=8BPzLxF9p5</a></p>
<p><b>Keywords</b>: Computer Vision, Segmentation, Weakly-supervised Learning
</p><p><b>Compressor summary</b>: The paper proposes an affinity propagation method that uses local and global pairwise affinity terms to generate accurate soft pseudo labels for weakly-supervised segmentation tasks, reducing the need for pixel-wise labeling and improving performance on three types of tasks.</p><hr><h3>Towards Automated Circuit Discovery for Mechanistic Interpretability</h3>
<p>Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, Adrià Garriga-Alonso</p>
<p><a href='https://openreview.net/forum?id=89ia77nZ8u'>https://openreview.net/forum?id=89ia77nZ8u</a></p>
<p><b>Keywords</b>: Mechanistic Interpretability, Pruning, Science of Deep Learning, AI Safety
</p><p><b>Compressor summary</b>: The paper presents a systematic method to interpret transformer models and automates one step of the process using novel algorithms, demonstrating their effectiveness on GPT-2 Small.</p><hr><h3>Tame a Wild Camera: In-the-Wild Monocular Camera Calibration</h3>
<p>Shengjie Zhu, Abhinav Kumar, Masa Hu, Xiaoming Liu</p>
<p><a href='https://openreview.net/forum?id=898RcRYWCg'>https://openreview.net/forum?id=898RcRYWCg</a></p>
<p><b>Keywords</b>: Monocular Camera Calibration; Camera Pose Estimation; Image Editing
</p><p><b>Compressor summary</b>: The paper proposes a method to calibrate intrinsic camera parameters using monocular 3D priors from depthmaps and surface normals, which can improve 3D sensing tasks and applications.</p><hr><h3>Out-of-distribution Detection Learning with Unreliable Out-of-distribution Sources</h3>
<p>Haotian Zheng, Qizhou Wang, Zhen Fang, Xiaobo Xia, Feng Liu, Tongliang Liu, Bo Han</p>
<p><a href='https://openreview.net/forum?id=87Qnneer8l'>https://openreview.net/forum?id=87Qnneer8l</a></p>
<p><b>Keywords</b>: out-of-distribution detection
</p><p><b>Compressor summary</b>: The paper proposes ATOL, a data generation-based learning method for OOD detection that uses an auxiliary task to relieve mistaken OOD generation and improve reliability.</p><hr><h3>Embracing the chaos: analysis and diagnosis of numerical instability in variational flows</h3>
<p>Zuheng Xu, Trevor Campbell</p>
<p><a href='https://openreview.net/forum?id=87Nu9SagB7'>https://openreview.net/forum?id=87Nu9SagB7</a></p>
<p><b>Keywords</b>: variational flow, numerical instability, shadowing property
</p><p><b>Compressor summary</b>: The paper examines how numerical instability affects variational flows' reliability in sampling, density evaluation, and ELBO estimation, and proposes a diagnostic procedure to validate results from unstable flows.</p><hr><h3>Geometry-Informed Neural Operator for Large-Scale 3D PDEs</h3>
<p>Zongyi Li, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Prakash Otta, Mohammad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, Anima Anandkumar</p>
<p><a href='https://openreview.net/forum?id=86dXbqT5Ua'>https://openreview.net/forum?id=86dXbqT5Ua</a></p>
<p><b>Keywords</b>: partial differential equation, computational fluid dynamics, neural operator
</p><p><b>Compressor summary</b>: GINO is an efficient method to learn solutions for large-scale PDEs with varying geometries, achieving significant speed-up and accuracy improvements over existing methods on 3D fluid simulation.</p><hr><h3>Nearest Neighbour with Bandit Feedback</h3>
<p>Stephen Pasteris, Chris Hicks, Vasilios Mavroudis</p>
<p><a href='https://openreview.net/forum?id=86ADcKOHAw'>https://openreview.net/forum?id=86ADcKOHAw</a></p>
<p><b>Keywords</b>: Nearest Neighbours, Contextual Bandits
</p><p><b>Compressor summary</b>: The paper presents an efficient algorithm for contextual bandits that adapts the nearest neighbour rule and handles adversarial settings with minimal assumptions, providing regret bounds and application to online classification.</p><hr><h3>Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP</h3>
<p>Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, Liang-Chieh Chen</p>
<p><a href='https://openreview.net/forum?id=83LJRUzXWj'>https://openreview.net/forum?id=83LJRUzXWj</a></p>
<p><b>Keywords</b>: open-vocabulary panoptic segmentation, panoptic segmentation, vision and language
</p><p><b>Compressor summary</b>: The authors propose a single-stage framework for open-vocabulary segmentation using a frozen convolutional CLIP backbone that simplifies the existing two-stage pipeline, achieves better accuracy-cost trade-off, and outperforms previous methods on various benchmarks.</p><hr><h3>Expanding Small-Scale Datasets with Guided Imagination</h3>
<p>Yifan Zhang, Daquan Zhou, Bryan Hooi, Kai Wang, Jiashi Feng</p>
<p><a href='https://openreview.net/forum?id=82HeVCqsfh'>https://openreview.net/forum?id=82HeVCqsfh</a></p>
<p><b>Keywords</b>: Dataset Expansion, Guided Imagination
</p><p><b>Compressor summary</b>: The paper proposes a Guided Imagination Framework that uses generative models to create new labeled data for dataset expansion, improving model accuracy on various image and medical datasets.</p><hr><h3>One-step differentiation of iterative algorithms</h3>
<p>Jerome Bolte, Edouard Pauwels, Samuel Vaiter</p>
<p><a href='https://openreview.net/forum?id=81snFfE3vR'>https://openreview.net/forum?id=81snFfE3vR</a></p>
<p><b>Keywords</b>: automatic differentiation, implicit differentiation, super-linear algorithms, bilevel optimization.
</p><p><b>Compressor summary</b>: One-step differentiation is an easy and efficient method for derivative estimation that works well for fast optimization algorithms and has theoretical and practical implications in bilevel optimization.</p><hr><h3>Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow Shrink Trees</h3>
<p>Bryan Andrews, Joseph Ramsey, Ruben Sanchez Romero, Jazmin Camchong, Erich Kummerfeld</p>
<p><a href='https://openreview.net/forum?id=80g3Yqlo1a'>https://openreview.net/forum?id=80g3Yqlo1a</a></p>
<p><b>Keywords</b>: Causal Discovery, Directed Acyclic Graphs, DAGs, fMRI, Graphical Models, High Dimension, Densely Connected
</p><p><b>Compressor summary</b>: BOSS and GSTs are efficient algorithms for learning directed acyclic graphs (DAGs) from highly connected variables, achieving high accuracy and fast execution time in various conditions, and applied to fMRI data analysis.</p><hr><h3>A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories</h3>
<p>Kai Yan, Alex Schwing, Yu-Xiong Wang</p>
<p><a href='https://openreview.net/forum?id=805CW5w2CY'>https://openreview.net/forum?id=805CW5w2CY</a></p>
<p><b>Keywords</b>: offline Imitation learning, learning from observations, positive-unlabeled learning
</p><p><b>Compressor summary</b>: TAILO is a novel imitation learning method that uses a discriminator to scale weights for weighted behavior cloning based on expert states, improving robustness and performance, especially when learning from incomplete trajectories.</p><hr><h3>LD2: Scalable Heterophilous Graph Neural Network with Decoupled Embeddings</h3>
<p>Ningyi Liao, Siqiang Luo, Xiang Li, Jieming Shi</p>
<p><a href='https://openreview.net/forum?id=7zkFc9TGKz'>https://openreview.net/forum?id=7zkFc9TGKz</a></p>
<p><b>Keywords</b>: Graph neural networks, Scalability, Heterophilous Graphs, Non-Homophily
</p><p><b>Compressor summary</b>: LD2 is a scalable heterophilous GNN that simplifies the learning process by decoupling graph propagation and generating expressive embeddings prior to training, achieving optimal time complexity and memory footprint, and improving speed and efficiency on large-scale graphs.</p><hr><h3>The Target-Charging Technique for Privacy Analysis across Interactive Computations</h3>
<p>Edith Cohen, Xin Lyu</p>
<p><a href='https://openreview.net/forum?id=7yjsYrajlt'>https://openreview.net/forum?id=7yjsYrajlt</a></p>
<p><b>Keywords</b>: Differential Privacy; Adaptive Composition; Sparse Vector Technique
</p><p><b>Compressor summary</b>: TCT is a framework that improves privacy in interactive settings by charging a small overhead for computations that hit their targets, while keeping most others essentially free.</p><hr><h3>A Theory of Multimodal Learning</h3>
<p>Zhou Lu</p>
<p><a href='https://openreview.net/forum?id=7xlrdSOm3g'>https://openreview.net/forum?id=7xlrdSOm3g</a></p>
<p><b>Keywords</b>: Multimodal Learning
</p><p><b>Compressor summary</b>: The paper presents a theory explaining how multimodal machine learning can generalize better than unimodal learning, especially when there is diversity and connection among different sensory inputs.</p><hr><h3>HyTrel: Hypergraph-enhanced  Tabular Data Representation Learning</h3>
<p>Pei Chen, Soumajyoti Sarkar, Leonard Lausen, Balasubramaniam Srinivasan, Sheng Zha, Ruihong Huang, George Karypis</p>
<p><a href='https://openreview.net/forum?id=7vqlzODS28'>https://openreview.net/forum?id=7vqlzODS28</a></p>
<p><b>Keywords</b>: Tabular Language Model, Tabular Representation Learning, Pretraining, Tabular Data, Table, Hypergraph
</p><p><b>Compressor summary</b>: HyTrel is a tabular language model that uses hypergraphs to capture structural properties of tabular data and improve performance on downstream tasks.</p><hr><h3>Federated Learning with Manifold Regularization and Normalized Update Reaggregation</h3>
<p>Xuming An, Li Shen, Han Hu, Yong Luo</p>
<p><a href='https://openreview.net/forum?id=7uPnuoYqac'>https://openreview.net/forum?id=7uPnuoYqac</a></p>
<p><b>Keywords</b>: federated learning; manifold regularization; update reaggregation
</p><p><b>Compressor summary</b>: FedMRUR is a novel federated learning framework that uses hyperbolic graph manifolds and a new optimizer to reduce model inconsistency and improve convergence speed, achieving state-of-the-art results.</p><hr><h3>DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning</h3>
<p>Wenxuan Bao, Francesco Pittaluga, Vijay Kumar b g, Vincent Bindschaedler</p>
<p><a href='https://openreview.net/forum?id=7rm3OcASkg'>https://openreview.net/forum?id=7rm3OcASkg</a></p>
<p><b>Keywords</b>: differential privacy, deep learning, data augmentation
</p><p><b>Compressor summary</b>: The paper proposes two new data augmentation techniques for differentially private learning that improve performance and are compatible with privacy constraints.</p><hr><h3>ExPT: Synthetic Pretraining for Few-Shot Experimental Design</h3>
<p>Tung Nguyen, Sudhanshu Agrawal, Aditya Grover</p>
<p><a href='https://openreview.net/forum?id=7qfkImn0dL'>https://openreview.net/forum?id=7qfkImn0dL</a></p>
<p><b>Keywords</b>: experimental design, few-shot, black-box optimization, synthetic pretraining, in-context learning, transformer
</p><p><b>Compressor summary</b>: The paper introduces Experiment Pretrained Transformers (ExPT), a foundation model for few-shot experimental design that uses synthetic pretraining and in-context learning to generate optimal input designs with limited labeled data.</p><hr><h3>MeGraph: Capturing Long-Range Interactions by Alternating Local and Hierarchical Aggregation on Multi-Scaled Graph Hierarchy</h3>
<p>Honghua Dong, Jiawei Xu, Yu Yang, Rui Zhao, Shiwen Wu, Chun Yuan, Xiu Li, Chris J. Maddison, Lei Han</p>
<p><a href='https://openreview.net/forum?id=7p5YWe8GqG'>https://openreview.net/forum?id=7p5YWe8GqG</a></p>
<p><b>Keywords</b>: Long-Range Interactions, Hierachical Structure, Multi-Scale, Graph Pooling, Graph Neural Networks(GNNs)
</p><p><b>Compressor summary</b>: MeGraph is a model that combines local and hierarchical information in a multi-scale graph hierarchy to better capture long-range interactions and outperforms existing methods on various benchmarks.</p><hr><h3>Dis-inhibitory neuronal circuits can control the sign of synaptic plasticity</h3>
<p>Julian Rossbroich, Friedemann Zenke</p>
<p><a href='https://openreview.net/forum?id=7otRtfrRqo'>https://openreview.net/forum?id=7otRtfrRqo</a></p>
<p><b>Keywords</b>: Credit assignment, hebbian plasticity, inhibitory microcircuits, bio-plausible learning
</p><p><b>Compressor summary</b>: The paragraph discusses a microcircuit model that shows how error signals can be encoded in top-down synapses and influence Hebbian learning, resolving the discrepancy between functional models and experimental observations.</p><hr><h3>Energy-Efficient Scheduling with Predictions</h3>
<p>Eric Balkanski, Noemie Perivier, Clifford Stein, Hao-Ting Wei</p>
<p><a href='https://openreview.net/forum?id=7ntySBR3Ey'>https://openreview.net/forum?id=7ntySBR3Ey</a></p>
<p><b>Keywords</b>: Scheduling, algorithms with predictions, speed scaling, energy minimization
</p><p><b>Compressor summary</b>: The paper proposes a learning-augmented algorithmic framework for energy-efficient scheduling that improves performance when predictions are accurate and maintains bounded competitive ratios regardless of prediction error, as shown by empirical results on real and synthetic datasets.</p><hr><h3>AMAG: Additive, Multiplicative and Adaptive Graph Neural Network For Forecasting Neuron Activity</h3>
<p>Jingyuan Li, Leo Scholl, Trung Le, Pavithra Rajeswaran, Amy L Orsborn, Eli Shlizerman</p>
<p><a href='https://openreview.net/forum?id=7ntI4kcoqG'>https://openreview.net/forum?id=7ntI4kcoqG</a></p>
<p><b>Keywords</b>: Neuroscience and Cognitive Science, Neural Activity Forecasting, Graph Neural Network
</p><p><b>Compressor summary</b>: The text describes a model (AMAG) that uses deep learning and neural network interactions to forecast neural activity, capturing temporal causality and improving performance on synthetic and real neural data from macaques.</p><hr><h3>A Sublinear-Time Spectral Clustering Oracle with Improved Preprocessing Time</h3>
<p>Ranran Shen, Pan Peng</p>
<p><a href='https://openreview.net/forum?id=7nXaoclHed'>https://openreview.net/forum?id=7nXaoclHed</a></p>
<p><b>Keywords</b>: Sublinear-time algorithms, Spectral Clustering, Graph Clustering, Random Walks
</p><p><b>Compressor summary</b>: The paper proposes an efficient spectral clustering algorithm for graphs with strong clusterability, allowing sublinear-time preprocessing and queries, while handling some noise and deviation from ideal conditions.</p><hr><h3>Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects</h3>
<p>Tianhang Cheng, Wei-Chiu Ma, Kaiyu Guan, Antonio Torralba, Shenlong Wang</p>
<p><a href='https://openreview.net/forum?id=7irm2VJARb'>https://openreview.net/forum?id=7irm2VJARb</a></p>
<p><b>Keywords</b>: 3d reconstruction, inverse rendering, pose estimation, single view reconstruction, nerf, duplicates
</p><p><b>Compressor summary</b>: The paragraph introduces Structure from Duplicates (SfD), a novel inverse graphics framework that reconstructs geometry, material, and illumination from a single image containing multiple identical objects, using them as a robust prior for single-image inverse graphics and proposing an in-plane rotation-robust Structure from Motion formulation for joint 6-DoF object pose estimation.</p><hr><h3>A Theory of Link Prediction via Relational Weisfeiler-Leman on Knowledge Graphs</h3>
<p>Xingyue Huang, Miguel Romero Orth, Ismail Ilkan Ceylan, Pablo Barcelo</p>
<p><a href='https://openreview.net/forum?id=7hLlZNrkt5'>https://openreview.net/forum?id=7hLlZNrkt5</a></p>
<p><b>Keywords</b>: graph neural networks, knowledge graphs, expressivity, logical characterization
</p><p><b>Compressor summary</b>: The authors analyze and compare different graph neural network models for link prediction on knowledge graphs and provide a logical characterization of their expressive power based on relational Weisfeiler-Leman algorithms.</p><hr><h3>Physics-Driven ML-Based Modelling for Correcting Inverse Estimation</h3>
<p>Ruiyuan Kang, Tingting Mu, Panos Liatsis, Dimitrios Kyritsis</p>
<p><a href='https://openreview.net/forum?id=7h1YaSGaHS'>https://openreview.net/forum?id=7h1YaSGaHS</a></p>
<p><b>Keywords</b>: Failure detection, Physical evaluation, Network-based optimization, Generative model, Hybrid surrogate model
</p><p><b>Compressor summary</b>: The authors propose GEESE, a method to detect and correct failed machine learning estimations in engineering problems using simulations and optimization, outperforming existing approaches.</p><hr><h3>Unsupervised Optical Flow Estimation with Dynamic Timing Representation for Spike Camera</h3>
<p>Lujie Xia, Ziluo Ding, Rui Zhao, Jiyuan Zhang, Lei Ma, Zhaofei Yu, Tiejun Huang, Ruiqin Xiong</p>
<p><a href='https://openreview.net/forum?id=7gbjsgcN5p'>https://openreview.net/forum?id=7gbjsgcN5p</a></p>
<p><b>Keywords</b>: Optical flow, unsupervised learning, spike camera
</p><p><b>Compressor summary</b>: The authors propose a dynamic timing representation for spike streams, an unsupervised learning method for optical flow estimation, and a synthetic validation dataset called SSES for evaluating their approach in autonomous driving scenarios.</p><hr><h3>Multi-Agent Learning with Heterogeneous Linear Contextual Bandits</h3>
<p>Anh Do, Thanh Nguyen-Tang, Raman Arora</p>
<p><a href='https://openreview.net/forum?id=7f6vH3mmhr'>https://openreview.net/forum?id=7f6vH3mmhr</a></p>
<p><b>Keywords</b>: Multi-agent, Bandits, Cooperative
</p><p><b>Compressor summary</b>: The paper proposes H-LINUCB, a novel distributed learning algorithm for linear contextual bandits, which optimally cooperates to minimize group regret when agents have knowledge of task similarity or dissimilarity.</p><hr><h3>Information Maximizing Curriculum: A Curriculum-Based Approach for Learning Versatile Skills</h3>
<p>Denis Blessing, Onur Celik, Xiaogang Jia, Moritz Reuss, Maximilian Xiling Li, Rudolf Lioutikov, Gerhard Neumann</p>
<p><a href='https://openreview.net/forum?id=7eW6NzSE4g'>https://openreview.net/forum?id=7eW6NzSE4g</a></p>
<p><b>Keywords</b>: Imitation Learning, Verstile Skill Learning, Curriculum Learning
</p><p><b>Compressor summary</b>: The paper proposes a curriculum-based imitation learning method that uses weighted data and mixture of experts policy with maximum entropy objective to learn versatile policies from multimodal human demonstrations.</p><hr><h3>Random-Access Infinite Context Length for Transformers</h3>
<p>Amirkeivan Mohtashami, Martin Jaggi</p>
<p><a href='https://openreview.net/forum?id=7eHn64wOVy'>https://openreview.net/forum?id=7eHn64wOVy</a></p>
<p><b>Keywords</b>: large language models, memory, context length
</p><p><b>Compressor summary</b>: The paper proposes a new attention mechanism called landmark attention that allows Transformers to handle longer contexts without compromising flexibility or memory requirements.</p><hr><h3>Certification of Distributional Individual Fairness</h3>
<p>Matthew Robert Wicker, Vihari Piratla, Adrian Weller</p>
<p><a href='https://openreview.net/forum?id=7cnMLZvTy9'>https://openreview.net/forum?id=7cnMLZvTy9</a></p>
<p><b>Keywords</b>: Fairness, Individual Fairness, Deep Learning, Certification, Trustworthy ML
</p><p><b>Compressor summary</b>: The authors propose a novel method for certifying individual fairness in neural networks that is more computationally efficient and scalable than previous approaches, allowing them to analyze larger models and handle distributional shifts.</p><hr><h3>Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis</h3>
<p>Mitchell Ostrow, Adam Joseph Eisen, Leo Kozachkov, Ila R Fiete</p>
<p><a href='https://openreview.net/forum?id=7blSUMwe7R'>https://openreview.net/forum?id=7blSUMwe7R</a></p>
<p><b>Keywords</b>: Computational Neuroscience, Neural Data Analysis, Statistical Shape Metrics, Representational Similarity Analysis, Recurrent Neural Networks, Dynamical Systems
</p><p><b>Compressor summary</b>: The paper introduces Dynamical Similarity Analysis (DSA), a novel method to compare recurrent neural networks based on their dynamics, which can distinguish different computations and learning rules that are not possible with standard geometric approaches.</p><hr><h3>Bias in Evaluation Processes: An Optimization-Based Model</h3>
<p>L. Elisa Celis, Amit Kumar, Anay Mehrotra, Nisheeth K Vishnoi</p>
<p><a href='https://openreview.net/forum?id=7b4oobeB4w'>https://openreview.net/forum?id=7b4oobeB4w</a></p>
<p><b>Keywords</b>: bias, evaluation, maximum entropy, selection
</p><p><b>Compressor summary</b>: The paragraph discusses a model that explains how biases arise in evaluation processes, such as hiring or admissions, due to resource-information trade-offs and risk-averseness, and shows how this model can be used to study and reduce these biases.</p><hr><h3>Generalized equivalences between subsampling and ridge regularization</h3>
<p>Pratik Patil, Jin-Hong Du</p>
<p><a href='https://openreview.net/forum?id=7aoVQkNmQ6'>https://openreview.net/forum?id=7aoVQkNmQ6</a></p>
<p><b>Keywords</b>: subsampling, ridge regularization, asymptotic equivalences, proportional asymptotics
</p><p><b>Compressor summary</b>: The text shows that subsampling and ridge regularization have similar effects on ensemble ridge estimators, and provides methods to find the equivalent parameters for both.</p><hr><h3>Information Theoretic Lower Bounds for Information Theoretic Upper Bounds</h3>
<p>Roi Livni</p>
<p><a href='https://openreview.net/forum?id=7anW5TWbCJ'>https://openreview.net/forum?id=7anW5TWbCJ</a></p>
<p><b>Keywords</b>: Learning Theory
</p><p><b>Compressor summary</b>: The paper explores how mutual information relates to generalization in stochastic convex optimization and finds that existing information-theoretic bounds do not fully capture the performance of some learning algorithms.</p><hr><h3>Bounding the Invertibility of Privacy-preserving Instance Encoding using Fisher Information</h3>
<p>Kiwan Maeng, Chuan Guo, Sanjay Kariyappa, G. Edward Suh</p>
<p><a href='https://openreview.net/forum?id=7ZQiucQu2u'>https://openreview.net/forum?id=7ZQiucQu2u</a></p>
<p><b>Keywords</b>: privacy, instance encoding, split learning
</p><p><b>Compressor summary</b>: The paper introduces a new measure for evaluating the privacy of instance encoding based on Fisher information, which can bound its invertibility both theoretically and empirically.</p><hr><h3>Joint processing of linguistic properties in brains and language models</h3>
<p>SUBBA REDDY OOTA, Manish Gupta, Mariya Toneva</p>
<p><a href='https://openreview.net/forum?id=7WeCyYy9TL'>https://openreview.net/forum?id=7WeCyYy9TL</a></p>
<p><b>Keywords</b>: Linguistic properties, fMRI, probing tasks, cognitive neuroscience, language models, NLP
</p><p><b>Compressor summary</b>: The correspondence between human brain processing and language models is affected by specific linguistic properties, particularly syntactic ones, which are crucial for maintaining alignment with fMRI recordings.</p><hr><h3>Masked Image Residual Learning for Scaling Deeper Vision Transformers</h3>
<p>Guoxi Huang, Hongtao Fu, Adrian G. Bors</p>
<p><a href='https://openreview.net/forum?id=7WTA298wts'>https://openreview.net/forum?id=7WTA298wts</a></p>
<p><b>Keywords</b>: self-supervised learning, vision transformer, masked image modeling
</p><p><b>Compressor summary</b>: MIRL is a self-supervised learning framework that improves the training of deeper ViTs by teaching them to recover masked image residuals, leading to better accuracy and generalization on downstream tasks.</p><hr><h3>Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport</h3>
<p>Jaemoo Choi, Jaewoong Choi, Myungjoo Kang</p>
<p><a href='https://openreview.net/forum?id=7WQt1J13ex'>https://openreview.net/forum?id=7WQt1J13ex</a></p>
<p><b>Keywords</b>: Optimal Transport, Generative modeling, Generative adversarial network
</p><p><b>Compressor summary</b>: The paper proposes a new generative model based on Unbalanced Optimal Transport, which improves robustness, stability, and convergence compared to traditional Optimal Transport methods, achieving lower FID scores on image generation tasks.</p><hr><h3>A State Representation for Diminishing Rewards</h3>
<p>Ted Moskovitz, Samo Hromadka, Ahmed Touati, Diana L Borsa, Maneesh Sahani</p>
<p><a href='https://openreview.net/forum?id=7Uix1eQZ8z'>https://openreview.net/forum?id=7Uix1eQZ8z</a></p>
<p><b>Keywords</b>: reinforcement learning, successor features, successor representation, neuroscience
</p><p><b>Compressor summary</b>: The paper introduces a new state representation, the $\lambda$ representation (λR), that helps agents adapt to shifting reward priorities in multitask reinforcement learning and natural behaviors like foraging.</p><hr><h3>On student-teacher deviations in distillation: does it pay to disobey?</h3>
<p>Vaishnavh Nagarajan, Aditya Krishna Menon, Srinadh Bhojanapalli, Hossein Mobahi, Sanjiv Kumar</p>
<p><a href='https://openreview.net/forum?id=7UdVPRmpif'>https://openreview.net/forum?id=7UdVPRmpif</a></p>
<p><b>Keywords</b>: knowledge distillation, regularization, understanding, underfitting, theory
</p><p><b>Compressor summary</b>: The paper investigates why students trained with knowledge distillation may overconfident but still outperform their teachers, and shows that this is due to an exaggerated bias of gradient descent that improves generalization.</p><hr><h3>Order Matters in the Presence of Dataset Imbalance for Multilingual Learning</h3>
<p>Dami Choi, Derrick Xin, Hamid Dadkhahi, Justin Gilmer, Ankush Garg, Orhan Firat, Chih-Kuan Yeh, Andrew M. Dai, Behrooz Ghorbani</p>
<p><a href='https://openreview.net/forum?id=7RMGI4slcb'>https://openreview.net/forum?id=7RMGI4slcb</a></p>
<p><b>Keywords</b>: Multitask Optimization, Multilingual, Pre-training, Language Models, Language Sampling, Low Resource Languages, Overfitting
</p><p><b>Compressor summary</b>: This paper investigates how pre-training on high-resource tasks and fine-tuning on a mix of high/low-resource tasks improves multi-task learning with data imbalance, and demonstrates its effectiveness in neural machine translation and multi-lingual language modeling.</p><hr><h3>Tempo Adaptation in Non-stationary Reinforcement Learning</h3>
<p>Hyunin Lee, Yuhao Ding, Jongmin Lee, Ming Jin, Javad Lavaei, Somayeh Sojoudi</p>
<p><a href='https://openreview.net/forum?id=7R8noSP4vL'>https://openreview.net/forum?id=7R8noSP4vL</a></p>
<p><b>Keywords</b>: Non-stationary RL, Reinforcement Learning
</p><p><b>Compressor summary</b>: The text proposes a method to synchronize time between an agent and a changing environment in reinforcement learning, improving policy performance.</p><hr><h3>Statistical and Computational Trade-off in Multi-Agent Multi-Armed Bandits</h3>
<p>Filippo Vannella, Alexandre Proutiere, Jaeseong Jeong</p>
<p><a href='https://openreview.net/forum?id=7PJ6LaIOO4'>https://openreview.net/forum?id=7PJ6LaIOO4</a></p>
<p><b>Keywords</b>: Multi-Agent Multi-Armed Bandits, Multi-Armed Bandits, Regret Minimization
</p><p><b>Compressor summary</b>: The paper presents a regret minimization algorithm for MAMABs with factor graph rewards, which approximates a lower bound using Mean Field techniques and shows its performance in radio communications networks.</p><hr><h3>Dual Self-Awareness Value Decomposition Framework without Individual Global Max for Cooperative MARL</h3>
<p>Zhiwei Xu, Bin Zhang, Dapeng Li, Guangchong Zhou, Zeren Zhang, Guoliang Fan</p>
<p><a href='https://openreview.net/forum?id=7LtzqnfuOs'>https://openreview.net/forum?id=7LtzqnfuOs</a></p>
<p><b>Keywords</b>: Multi-Agent Reinforcement Learning, Individual Global Max
</p><p><b>Compressor summary</b>: The paper introduces a new value decomposition method for multi-agent reinforcement learning that rejects the Individual Global Max principle and uses dual self-awareness to solve the credit assignment problem and improve exploration.</p><hr><h3>Bounding training data reconstruction in DP-SGD</h3>
<p>Jamie Hayes, Borja Balle, Saeed Mahloujifar</p>
<p><a href='https://openreview.net/forum?id=7LZ4tZrYlx'>https://openreview.net/forum?id=7LZ4tZrYlx</a></p>
<p><b>Keywords</b>: Differential privacy, reconstruction
</p><p><b>Compressor summary</b>: The text discusses how privacy levels in DP-SGD affect the risk of training data reconstruction and suggests that less noise may improve utility while emphasizing that the standard privacy guarantee may not reflect the actual risk.</p><hr><h3>Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning</h3>
<p>Francesca Bartolucci, Emmanuel de Bezenac, Bogdan Raonic, Roberto Molinaro, Siddhartha Mishra, Rima Alaifari</p>
<p><a href='https://openreview.net/forum?id=7LSEkvEGCM'>https://openreview.net/forum?id=7LSEkvEGCM</a></p>
<p><b>Keywords</b>: Operator Learning, Neural Operators, PDEs, Frame theory, Sampling theory
</p><p><b>Compressor summary</b>: The paper proposes ReNO, a framework to address issues in neural operator learning by measuring inconsistency between neural operators and their discrete representations using the concept of operator aliasing.</p><hr><h3>Adaptive Contextual Perception: How To Generalize To New Backgrounds and Ambiguous Objects</h3>
<p>Zhuofan Ying, Peter Hase, Mohit Bansal</p>
<p><a href='https://openreview.net/forum?id=7JuReDmGSL'>https://openreview.net/forum?id=7JuReDmGSL</a></p>
<p><b>Keywords</b>: Computer vision, out-of-distribution generalization, representational geometry
</p><p><b>Compressor summary</b>: The paper investigates how biological vision systems adapt to context for out-of-distribution generalization and proposes new augmentation methods based on representational geometry analysis and causal intervention.</p><hr><h3>Long-Term Fairness with Unknown Dynamics</h3>
<p>Tongxin Yin, Reilly Raab, Mingyan Liu, Yang Liu</p>
<p><a href='https://openreview.net/forum?id=7INd5Yu9ET'>https://openreview.net/forum?id=7INd5Yu9ET</a></p>
<p><b>Keywords</b>: Long-term Fairness, Dynamics, Reinforcement Learning
</p><p><b>Compressor summary</b>: The paper proposes an online reinforcement learning approach for achieving long-term fairness in policy decisions affecting human populations, with algorithmic solutions that adapt to unknown dynamics and prove probabilistic bounds on loss and fairness violations.</p><hr><h3>AbDiffuser: full-atom generation of in-vitro functioning antibodies</h3>
<p>Karolis Martinkus, Jan Ludwiczak, WEI-CHING LIANG, Julien Lafrance-Vanasse, Isidro Hotzel, Arvind Rajpal, Yan Wu, Kyunghyun Cho, Richard Bonneau, Vladimir Gligorijevic, Andreas Loukas</p>
<p><a href='https://openreview.net/forum?id=7GyYpomkEa'>https://openreview.net/forum?id=7GyYpomkEa</a></p>
<p><b>Keywords</b>: antibody generation, diffusion, equivariance
</p><p><b>Compressor summary</b>: AbDiffuser is a new diffusion model that generates antibody structures and sequences by using novel protein representation, physics-based constraints, and strong diffusion priors, achieving high in silico and in vitro performance.</p><hr><h3>Change point detection and inference in multivariate non-parametric models under mixing conditions</h3>
<p>Carlos Misael Madrid Padilla, Haotian Xu, Daren Wang, OSCAR HERNAN MADRID PADILLA, Yi Yu</p>
<p><a href='https://openreview.net/forum?id=7Fb2lCwS76'>https://openreview.net/forum?id=7Fb2lCwS76</a></p>
<p><b>Keywords</b>: Multivariate; Nonparametric; Change point inference; short range dependence; Long-run variance; Confidence interval.
</p><p><b>Compressor summary</b>: The paper analyzes how to find changes in the underlying distributions of multivariate time series with smooth densities and unknown change points.</p><hr><h3>Exact Optimality of Communication-Privacy-Utility Tradeoffs in Distributed Mean Estimation</h3>
<p>Berivan Isik, Wei-Ning Chen, Ayfer Ozgur, Tsachy Weissman, Albert No</p>
<p><a href='https://openreview.net/forum?id=7ETbK9lQd7'>https://openreview.net/forum?id=7ETbK9lQd7</a></p>
<p><b>Keywords</b>: distributed mean estimation, privacy, compression, communication, federated analytics.
</p><p><b>Compressor summary</b>: The paper explores how to estimate a population mean with privacy guarantees and shared randomness, proposing an exact-optimal mechanism based on a rotationally symmetric codebook and $k$-closest encoding.</p><hr><h3>AVIS: Autonomous Visual Information Seeking with Large Language Model Agent</h3>
<p>Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David A Ross, Cordelia Schmid, Alireza Fathi</p>
<p><a href='https://openreview.net/forum?id=7EMphtUgCI'>https://openreview.net/forum?id=7EMphtUgCI</a></p>
<p><b>Keywords</b>: large language model, visual question answering, dynamic decision making, Tool augmented LLM
</p><p><b>Compressor summary</b>: AVIS is a framework that uses an LLM to strategize the use of external tools and analyze their outputs to answer complex visual questions by following a user study-informed sequence of actions.</p><hr><h3>Defending against Data-Free Model Extraction by  Distributionally Robust Defensive Training</h3>
<p>Zhenyi Wang, Li Shen, Tongliang Liu, Tiehang Duan, Yanjun Zhu, Donglin Zhan, David Doermann, Mingchen Gao</p>
<p><a href='https://openreview.net/forum?id=7DZAVpOoAK'>https://openreview.net/forum?id=7DZAVpOoAK</a></p>
<p><b>Keywords</b>: Data-Free Model Extraction; Defense
</p><p><b>Compressor summary</b>: MeCo is a defense method against data-free model cloning that randomizes inputs to confuse attackers and protect the original model's utility.</p><hr><h3>Learning Energy-based Model via Dual-MCMC Teaching</h3>
<p>Jiali Cui, Tian Han</p>
<p><a href='https://openreview.net/forum?id=7962B4nXX7'>https://openreview.net/forum?id=7962B4nXX7</a></p>
<p><b>Keywords</b>: Energy-based model, MCMC, Joint-training, Generator model
</p><p><b>Compressor summary</b>: The paper proposes a joint learning framework for energy-based models that uses a complementary generator model to improve MCMC sampling and avoid bias in learning.</p><hr><h3>Language Model Tokenizers Introduce Unfairness Between Languages</h3>
<p>Aleksandar Petrov, Emanuele La Malfa, Philip Torr, Adel Bibi</p>
<p><a href='https://openreview.net/forum?id=78yDLKi95p'>https://openreview.net/forum?id=78yDLKi95p</a></p>
<p><b>Keywords</b>: LLM, language model, tokenizer, multilingual, language, fairness
</p><p><b>Compressor summary</b>: The paper argues that current language models are not fair to all languages and suggests using multilingually fair subword tokenizers for better performance.</p><hr><h3>IDEA: An Invariant Perspective for Efficient Domain Adaptive Image Retrieval</h3>
<p>Haixin Wang, Hao Wu, Jinan Sun, Shikun Zhang, Chong Chen, Xian-Sheng Hua, Xiao Luo</p>
<p><a href='https://openreview.net/forum?id=77i6itptQW'>https://openreview.net/forum?id=77i6itptQW</a></p>
<p><b>Keywords</b>: domain adaption, binary descriptor, causal inference
</p><p><b>Compressor summary</b>: The paper proposes an IDEA model that learns to generate discriminative hash codes using causal and non-causal features, while minimizing the impact of non-causal effects on domain invariance for cross-domain retrieval.</p><hr><h3>Fragment-based Pretraining and Finetuning on Molecular Graphs</h3>
<p>Kha-Dinh Luong, Ambuj Singh</p>
<p><a href='https://openreview.net/forum?id=77Nq1KjmLl'>https://openreview.net/forum?id=77Nq1KjmLl</a></p>
<p><b>Keywords</b>: Self-supervised Learning, Graph Neural Network, Molecule
</p><p><b>Compressor summary</b>: The authors propose GraphFP, a pretraining method for GNNs that leverages fragment-level information and improves performance on molecular property prediction tasks.</p><hr><h3>Hierarchical clustering with dot products recovers hidden tree structure</h3>
<p>Annie Gray, Alexander Modell, Patrick Rubin-Delanchy, Nick Whiteley</p>
<p><a href='https://openreview.net/forum?id=75v88kyyko'>https://openreview.net/forum?id=75v88kyyko</a></p>
<p><b>Keywords</b>: agglomerative clustering, generative model, graphical model, hierarchical clustering, high-dimensional data
</p><p><b>Compressor summary</b>: The paper proposes a simple variant of agglomerative clustering that better recovers hierarchical structure in data using maximum average dot product and shows its advantages over other methods.</p><hr><h3>No-Regret Learning in Dynamic Competition with Reference Effects Under Logit Demand</h3>
<p>Mengzi Amy Guo, Donghao Ying, Javad Lavaei, Zuo-Jun Shen</p>
<p><a href='https://openreview.net/forum?id=75Mxzfoeq7'>https://openreview.net/forum?id=75Mxzfoeq7</a></p>
<p><b>Keywords</b>: no-regret learning, price competition, reference effect, last-iterate convergence
</p><p><b>Compressor summary</b>: The paper proposes an algorithm for two firms to learn a stable equilibrium in a dynamic price competition with opaque information using online projected gradient ascent and stationary Nash equilibrium concepts.</p><hr><h3>Smoothing the Landscape Boosts the Signal for SGD: Optimal Sample Complexity for Learning Single Index Models</h3>
<p>Alex Damian, Eshaan Nichani, Rong Ge, Jason D. Lee</p>
<p><a href='https://openreview.net/forum?id=73XPopmbXH'>https://openreview.net/forum?id=73XPopmbXH</a></p>
<p><b>Keywords</b>: statistical learning, learning theory, single index model, gradient descent, stochastic gradient descent
</p><p><b>Compressor summary</b>: The paper studies how many samples are needed to learn a single index model with respect to an isotropic Gaussian distribution using online SGD, and shows that $n \gtrsim d^{k^\star/2}$ samples suffice, closing the gap between upper and lower bounds.</p><hr><h3>Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL</h3>
<p>Yang Yue, Rui Lu, Bingyi Kang, Shiji Song, Gao Huang</p>
<p><a href='https://openreview.net/forum?id=71P7ugOGCV'>https://openreview.net/forum?id=71P7ugOGCV</a></p>
<p><b>Keywords</b>: Offline RL, Theory
</p><p><b>Compressor summary</b>: The authors study the cause of Q-value estimation divergence in offline RL, propose a new metric to measure it, and introduce LayerNorm as a solution to improve extrapolation behavior and achieve better performance.</p><hr><h3>VPGTrans: Transfer Visual Prompt Generator across LLMs</h3>
<p>Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, Tat-Seng Chua</p>
<p><a href='https://openreview.net/forum?id=716PvHoDct'>https://openreview.net/forum?id=716PvHoDct</a></p>
<p><b>Keywords</b>: Visual Prompt Generator, Efficient Transfer, Multimodality
</p><p><b>Compressor summary</b>: This paper explores transferring visual prompt generators (VPG) between multimodal large language models (MLLMs) to reduce training costs and presents a two-stage transfer framework called VPGTrans that achieves significant efficiency improvements.</p><hr><h3>Causes and Effects of Unanticipated Numerical Deviations in Neural Network Inference Frameworks</h3>
<p>Alexander Schlögl, Nora Hofer, Rainer Böhme</p>
<p><a href='https://openreview.net/forum?id=6zyFgr1b8Q'>https://openreview.net/forum?id=6zyFgr1b8Q</a></p>
<p><b>Keywords</b>: machine learning, security, reproducibility, forensics
</p><p><b>Compressor summary</b>: The study investigates numerical deviations in CNN inference results across different platforms due to hardware-specific optimizations, such as SIMD use on CPUs and convolution algorithms on GPUs.</p><hr><h3>Weitzman's Rule for Pandora's Box with Correlations</h3>
<p>Evangelia Gergatsouli, Christos Tzamos</p>
<p><a href='https://openreview.net/forum?id=6wBkT2ndDu'>https://openreview.net/forum?id=6wBkT2ndDu</a></p>
<p><b>Keywords</b>: pandora's box, stochastic optimization, discrete optimization, learning from samples, algorithms under uncertainty
</p><p><b>Compressor summary</b>: The paper studies Pandora's Box problem with correlated value distributions and shows that Weitzman's rule works for both independent and correlated cases, improving approximation guarantees and simplifying the algorithm.</p><hr><h3>PUe: Biased Positive-Unlabeled Learning Enhancement by Causal Inference</h3>
<p>Xutao Wang, Hanting Chen, Tianyu Guo, Yunhe Wang</p>
<p><a href='https://openreview.net/forum?id=6vtZIoxZoJ'>https://openreview.net/forum?id=6vtZIoxZoJ</a></p>
<p><b>Keywords</b>: PU learning, causal inference, semi-supervised learning
</p><p><b>Compressor summary</b>: The paper proposes a PU learning enhancement algorithm using causal inference to improve accuracy on non-uniform label distribution datasets.</p><hr><h3>Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks</h3>
<p>Ziyi Huang, Henry Lam, Haofeng Zhang</p>
<p><a href='https://openreview.net/forum?id=6vnwhzRinw'>https://openreview.net/forum?id=6vnwhzRinw</a></p>
<p><b>Keywords</b>: frequentist uncertainty, epistemic uncertainty, procedural variability, confidence intervals, batching, cheap bootstrap
</p><p><b>Compressor summary</b>: The paragraph discusses a new method to quantify and remove uncertainty in deep learning models using a single auxiliary network and light-computation resampling methods.</p><hr><h3>Explaining Predictive Uncertainty with Information Theoretic Shapley Values</h3>
<p>David Watson, Joshua O'Hara, Niek Tax, Richard Mudd, Ido Guy</p>
<p><a href='https://openreview.net/forum?id=6rabAZhCRS'>https://openreview.net/forum?id=6rabAZhCRS</a></p>
<p><b>Keywords</b>: Explainable AI, interpretable ML, feature attributions, information theory, Shapley values
</p><p><b>Compressor summary</b>: The authors propose a method to explain predictive uncertainty using Shapley values, which are related to information theory and conditional independence testing, and have applications in various tasks.</p><hr><h3>Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?</h3>
<p>Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Yecheng Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Tingfan Wu, Jay Vakil, Pieter Abbeel, Jitendra Malik, Dhruv Batra, Yixin Lin, Oleksandr Maksymets, Aravind Rajeswaran, Franziska Meier</p>
<p><a href='https://openreview.net/forum?id=6qLzQeFGio'>https://openreview.net/forum?id=6qLzQeFGio</a></p>
<p><b>Keywords</b>: representation learning, pre-training, foundation models, embodied AI, reinforcement learning, imitation learning
</p><p><b>Compressor summary</b>: The paper evaluates various pre-trained visual representations for Embodied AI tasks using CortexBench, a new benchmark, and shows that adapting a large model to specific tasks improves performance.</p><hr><h3>Bayesian Optimization with Cost-varying Variable Subsets</h3>
<p>Sebastian Shenghong Tay, Chuan-Sheng Foo, Daisuke Urano, Richalynn Leong, Bryan Kian Hsiang Low</p>
<p><a href='https://openreview.net/forum?id=6oiux75UDj'>https://openreview.net/forum?id=6oiux75UDj</a></p>
<p><b>Keywords</b>: Bayesian optimization, Gaussian processes
</p><p><b>Compressor summary</b>: The paper proposes a novel algorithm for Bayesian optimization with cost-varying variable subsets, which balances informativeness and costs, and shows its superior performance over baselines.</p><hr><h3>Learning the Efficient Frontier</h3>
<p>Philippe Chatigny, Ivan Sergienko, Ryan Ferguson, Jordan Weir, Maxime Bergeron</p>
<p><a href='https://openreview.net/forum?id=6lnoUqFd5R'>https://openreview.net/forum?id=6lnoUqFd5R</a></p>
<p><b>Keywords</b>: Efficient Frontier, Convex Optimization, Resource Allocation, Constrainted Optimization, Finance
</p><p><b>Compressor summary</b>: NeuralEF is a framework that uses neural networks to quickly and accurately predict the optimal portfolio for a given risk level in the efficient frontier problem, considering heterogeneous constraints and variable inputs.</p><hr><h3>Mask Propagation for Efficient Video Semantic Segmentation</h3>
<p>Yuetian Weng, Mingfei Han, Haoyu He, Mingjie Li, Lina Yao, Xiaojun Chang, Bohan Zhuang</p>
<p><a href='https://openreview.net/forum?id=6ljXBlojde'>https://openreview.net/forum?id=6ljXBlojde</a></p>
<p><b>Keywords</b>: Video Semantic Segmentation; Inference Efficiency
</p><p><b>Compressor summary</b>: The paper proposes MPVSS, an efficient mask propagation framework for video semantic segmentation that reduces computational costs by reusing predictions from key frames and warping them to non-key frames using learned queries.</p><hr><h3>Language Model Alignment with Elastic Reset</h3>
<p>Michael Noukhovitch, Samuel Lavoie, Florian Strub, Aaron Courville</p>
<p><a href='https://openreview.net/forum?id=6lgugutkin'>https://openreview.net/forum?id=6lgugutkin</a></p>
<p><b>Keywords</b>: reinforcement learning from human feedback (rlhf), language
</p><p><b>Compressor summary</b>: Elastic Reset is a new algorithm that reduces reward hacking and language drift in fine-tuning language models with reinforcement learning by periodically resetting the online model to an exponentially moving average of itself.</p><hr><h3>Towards A Richer 2D Understanding of Hands at Scale</h3>
<p>Tianyi Cheng, Dandan Shan, Ayda Sultan Hassen, Richard Ely Locke Higgins, David Fouhey</p>
<p><a href='https://openreview.net/forum?id=6ldTxwhgtP'>https://openreview.net/forum?id=6ldTxwhgtP</a></p>
<p><b>Keywords</b>: human-object interaction; hand object detection; hand detection
</p><p><b>Compressor summary</b>: The new model produces detailed outputs about hand interactions, using large-scale annotations from multiple datasets to improve AI understanding of hands in contact with objects.</p><hr><h3>AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning</h3>
<p>Mohammadamin Tavakoli, Pierre Baldi, Ann Marie Carlton, Yinting Chiu, Alexander Shmakov, David Van Vranken</p>
<p><a href='https://openreview.net/forum?id=6kRQTPEVip'>https://openreview.net/forum?id=6kRQTPEVip</a></p>
<p><b>Keywords</b>: Chemistry, Reactions, Contrastive, Radical, Graph
</p><p><b>Compressor summary</b>: The new reaction predictor system, RMechRP, uses contrastive learning and mechanistic pathways to provide accurate and interpretable predictions of radical reactions, overcoming limitations of existing deep learning-based methods that rely on US patents data.</p><hr><h3>Adversarially Robust Distributed Count Tracking via Partial Differential Privacy</h3>
<p>Zhongzheng Xiong, Xiaoyi Zhu, Zengfeng Huang</p>
<p><a href='https://openreview.net/forum?id=6kINNTYQcm'>https://openreview.net/forum?id=6kINNTYQcm</a></p>
<p><b>Keywords</b>: Distributed Tracking, Adaptive Robustness, Differential Privacy, Generalization
</p><p><b>Compressor summary</b>: The paper studies how to track a function of items received by multiple sites communicating with a central server, considering adaptive adversaries and extending the differential privacy framework for optimal communication.</p><hr><h3>Synthetic Experience Replay</h3>
<p>Cong Lu, Philip J. Ball, Yee Whye Teh, Jack Parker-Holder</p>
<p><a href='https://openreview.net/forum?id=6jNQ1AY1Uf'>https://openreview.net/forum?id=6jNQ1AY1Uf</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Diffusion Models, Synthetic Data, Sample-Efficient RL
</p><p><b>Compressor summary</b>: The authors propose Synthetic Experience Replay (SynthER), a method that uses generative models to upsample an agent's collected experience, improving the performance of deep reinforcement learning agents in both offline and online settings with limited data.</p><hr><h3>The Exact Sample Complexity Gain from Invariances for Kernel Regression</h3>
<p>Behrooz Tahmasebi, Stefanie Jegelka</p>
<p><a href='https://openreview.net/forum?id=6iouUxI45W'>https://openreview.net/forum?id=6iouUxI45W</a></p>
<p><b>Keywords</b>: invariances, manifolds, sample complexity
</p><p><b>Compressor summary</b>: The paper studies how encoding invariances into models reduces sample complexity for kernel ridge regression on compact manifolds with group action-invariant target functions, and presents a minimax optimal rate using differential geometry.</p><hr><h3>Statistical Analysis of Quantum State Learning Process in Quantum Neural Networks</h3>
<p>Hao-Kai Zhang, Chenghong Zhu, Mingrui Jing, Xin Wang</p>
<p><a href='https://openreview.net/forum?id=6gcY0MGNhj'>https://openreview.net/forum?id=6gcY0MGNhj</a></p>
<p><b>Keywords</b>: quantum neural networks, quantum state learning, quantum computing, quantum machine learning, quantum optimization
</p><p><b>Compressor summary</b>: The paper develops a no-go theorem for learning an unknown quantum state with quantum neural networks (QNNs) and shows that local minima become more difficult to avoid as the qubit count increases, limiting the learnability and scalability of QNNs.</p><hr><h3>When is Agnostic Reinforcement Learning Statistically Tractable?</h3>
<p>Zeyu Jia, Gene Li, Alexander Rakhlin, Ayush Sekhari, Nathan Srebro</p>
<p><a href='https://openreview.net/forum?id=6gWpJ0IExE'>https://openreview.net/forum?id=6gWpJ0IExE</a></p>
<p><b>Keywords</b>: Agnostic Reinforcement Learning, Sample Complexity, Learning Theory, Complexity Measure
</p><p><b>Compressor summary</b>: The paper studies the number of interactions needed to learn an approximately optimal policy for unknown MDPs using a new complexity measure called spanning capacity, and introduces a new algorithm called POPLER that enables efficient online RL with certain conditions.</p><hr><h3>Optimal and Fair Encouragement Policy Evaluation and Learning</h3>
<p>Angela Zhou</p>
<p><a href='https://openreview.net/forum?id=6fuZs3ibGA'>https://openreview.net/forum?id=6fuZs3ibGA</a></p>
<p><b>Keywords</b>: causal inference, fairness in machine learning, algorithmic fairness, criminal justice, policy learning, off-policy evaluation
</p><p><b>Compressor summary</b>: The paragraph discusses how to optimize treatment recommendations for individuals who may not follow them, considering factors like who benefits most and fairness, using a new online learning algorithm.</p><hr><h3>Conformalized matrix completion</h3>
<p>Yu Gui, Rina Barber, Cong Ma</p>
<p><a href='https://openreview.net/forum?id=6f320HfMeS'>https://openreview.net/forum?id=6f320HfMeS</a></p>
<p><b>Keywords</b>: matrix completion, conformal inference, uncertainty quantification
</p><p><b>Compressor summary</b>: The paper proposes a new method, conformalized matrix completion (cmc), which predicts missing entries in data matrices without requiring a low-rank assumption and provides valid prediction intervals using conformal prediction framework.</p><hr><h3>An Efficient End-to-End Training Approach for Zero-Shot Human-AI Coordination</h3>
<p>Xue Yan, Jiaxian Guo, Xingzhou Lou, Jun Wang, Haifeng Zhang, Yali Du</p>
<p><a href='https://openreview.net/forum?id=6ePsuwXUwf'>https://openreview.net/forum?id=6ePsuwXUwf</a></p>
<p><b>Keywords</b>: Zero-Shot Coordination, Human-AI coordination, Training Efficiency, Partner Modeling
</p><p><b>Compressor summary</b>: E3T is an efficient and diverse agent for zero-shot human-AI coordination that uses a mixture of ego policy and random policy, partner modeling, and end-to-end training to improve collaboration efficiency and performance.</p><hr><h3>Balancing Risk and Reward: A Batched-Bandit Strategy for Automated Phased Release</h3>
<p>Yufan Li, Jialiang Mao, Iavor Bojinov</p>
<p><a href='https://openreview.net/forum?id=6e86TccKyQ'>https://openreview.net/forum?id=6e86TccKyQ</a></p>
<p><b>Keywords</b>: bandit algorithms, online learning, causality, Bayesian inference
</p><p><b>Compressor summary</b>: The paper proposes an algorithm that automatically determines the release percentage of new products or updates in phased releases, balancing risk and ramp-up speed.</p><hr><h3>Offline Imitation Learning with Variational Counterfactual Reasoning</h3>
<p>Zexu Sun, Bowei He, Jinxin Liu, Xu Chen, Chen Ma, Shuai Zhang</p>
<p><a href='https://openreview.net/forum?id=6d9Yxttb3w'>https://openreview.net/forum?id=6d9Yxttb3w</a></p>
<p><b>Keywords</b>: offline imitaion learning, counterfactual reasoning, data augmentation
</p><p><b>Compressor summary</b>: OILCA is a framework for offline imitation learning that uses counterfactual data augmentation to generate expert data and improve the agent's generalization ability.</p><hr><h3>Globally injective and bijective neural operators</h3>
<p>Takashi Furuya, Michael Anthony Puthawala, Matti Lassas, Maarten V. de Hoop</p>
<p><a href='https://openreview.net/forum?id=6cc69ArD3O'>https://openreview.net/forum?id=6cc69ArD3O</a></p>
<p><b>Keywords</b>: Deep Learning, Operator Learning, Functional Analysis, Injectivity, Bijectivity, Universal approximation
</p><p><b>Compressor summary</b>: This paper investigates when neural networks can learn injective and surjective operators between function spaces and their applications in uncertainty quantification and inverse problems.</p><hr><h3>Strategic Classification under Unknown Personalized Manipulation</h3>
<p>Han Shao, Avrim Blum, Omar Montasser</p>
<p><a href='https://openreview.net/forum?id=6cJKcIxPck'>https://openreview.net/forum?id=6cJKcIxPck</a></p>
<p><b>Keywords</b>: strategic classification, mistake bound in online learning, PAC learning
</p><p><b>Compressor summary</b>: The study analyzes how strategic feature manipulation affects learning in classification problems with different levels of information available to the learner, focusing on ball and non-ball manipulations.</p><hr><h3>HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception</h3>
<p>Junkun Yuan, Xinyu Zhang, Hao Zhou, Jian Wang, Zhongwei Qiu, Zhiyin Shao, Shaofeng Zhang, Sifan Long, Kun Kuang, Kun Yao, Junyu Han, Errui Ding, Lanfen Lin, Fei Wu, Jingdong Wang</p>
<p><a href='https://openreview.net/forum?id=6XPPfZkhKi'>https://openreview.net/forum?id=6XPPfZkhKi</a></p>
<p><b>Keywords</b>: human centric perception, masked image modeling, structural-aware pre-training
</p><p><b>Compressor summary</b>: The paper introduces HAP, a pre-training method for human-centric perception tasks that uses masked image modeling with human structure priors to improve performance on various benchmarks.</p><hr><h3>DELTA: Diverse Client Sampling for Fasting Federated Learning</h3>
<p>Lin Wang, Yongxin Guo, Tao Lin, Xiaoying Tang</p>
<p><a href='https://openreview.net/forum?id=6XC5iKqRVm'>https://openreview.net/forum?id=6XC5iKqRVm</a></p>
<p><b>Keywords</b>: federated learning, client sampling
</p><p><b>Compressor summary</b>: DELTA is an unbiased sampling scheme for Federated Learning that considers client diversity and local variance to improve convergence and reduce variance caused by partial client participation.</p><hr><h3>Adversarial Model for Offline Reinforcement Learning</h3>
<p>Mohak Bhardwaj, Tengyang Xie, Byron Boots, Nan Jiang, Ching-An Cheng</p>
<p><a href='https://openreview.net/forum?id=6UCMa0Qgej'>https://openreview.net/forum?id=6UCMa0Qgej</a></p>
<p><b>Keywords</b>: model based, offline, reinforcement learning, adversarial training
</p><p><b>Compressor summary</b>: ARMOR is a novel model-based offline RL framework that learns policies to improve upon any reference policy, optimizing for worst-case performance and being robust to hyperparameters.</p><hr><h3>Preference-grounded Token-level Guidance for Language Model Fine-tuning</h3>
<p>Shentao Yang, Shujian Zhang, Congying Xia, Yihao Feng, Caiming Xiong, Mingyuan Zhou</p>
<p><a href='https://openreview.net/forum?id=6SRE9GZ9s6'>https://openreview.net/forum?id=6SRE9GZ9s6</a></p>
<p><b>Keywords</b>: Preference Learning, Training Guidance Learning, Language Model Fine-tuning, Text Sequence Generation
</p><p><b>Compressor summary</b>: The paper proposes a method to align language models with preferences by iterating between grounding preferences into token-level training and improving the LM with learned guidance, using a framework that extends pairwise-preference learning and two minimalist learning objectives.</p><hr><h3>Robust Lipschitz Bandits to Adversarial Corruptions</h3>
<p>Yue Kang, Cho-Jui Hsieh, Thomas Chun Man Lee</p>
<p><a href='https://openreview.net/forum?id=6RiqluMFNz'>https://openreview.net/forum?id=6RiqluMFNz</a></p>
<p><b>Keywords</b>: bandits
</p><p><b>Compressor summary</b>: The paper introduces robust Lipschitz bandit algorithms that can handle adaptive adversaries corrupting stochastic rewards up to a budget $C$, achieving sub-linear regret even when $C$ is unknown, and provides lower bounds and experiments.</p><hr><h3>ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text Translation</h3>
<p>Chenyang Le, Yao Qian, Long Zhou, Shujie LIU, Yanmin Qian, Michael Zeng, Xuedong Huang</p>
<p><a href='https://openreview.net/forum?id=6Qx7G1xrAk'>https://openreview.net/forum?id=6Qx7G1xrAk</a></p>
<p><b>Keywords</b>: end-to-end speech to text translation, cross-modality learning, joint speech and language training
</p><p><b>Compressor summary</b>: ComSL is a data-efficient speech-language model that combines cross-modality learning and multi-task learning to achieve state-of-the-art performance in speech-to-text translation across 21 languages.</p><hr><h3>Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantisation.</h3>
<p>Chris Subia-Waud, Srinandan Dasmahapatra</p>
<p><a href='https://openreview.net/forum?id=6Odmtoek02'>https://openreview.net/forum?id=6Odmtoek02</a></p>
<p><b>Keywords</b>: Quantization, compression, bayesian neural networks, accelerators
</p><p><b>Compressor summary</b>: The paper proposes a probabilistic method to cluster neural network weights based on their position-specific uncertainty, improving compression and accuracy in large models.</p><hr><h3>TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery</h3>
<p>Jialin Chen, Zhitao Ying</p>
<p><a href='https://openreview.net/forum?id=6OOgw4boZI'>https://openreview.net/forum?id=6OOgw4boZI</a></p>
<p><b>Keywords</b>: Explainability, Temporal Graph Neural Network
</p><p><b>Compressor summary</b>: TempME is a novel approach that uncovers the most pivotal temporal motifs guiding the prediction of temporal graph neural networks, improving their explainability and trustworthiness.</p><hr><h3>Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees</h3>
<p>Sharan Vaswani, Amirreza Kazemi, Reza Babanezhad Harikandeh, Nicolas Le Roux</p>
<p><a href='https://openreview.net/forum?id=6MQ5cheYDZ'>https://openreview.net/forum?id=6MQ5cheYDZ</a></p>
<p><b>Keywords</b>: Decision-aware reinforcement learning, Actor-Critic algorithm, Off-policy updates, General function approximation, Theoretical guarantees
</p><p><b>Compressor summary</b>: The paper proposes a joint objective for training actor and critic in reinforcement learning that improves policy improvement and uses surrogate functions similar to TRPO and PPO.</p><hr><h3>Polyhedron Attention Module: Learning Adaptive-order Interactions</h3>
<p>Tan Zhu, Fei Dou, Xinyu Wang, Jin Lu, Jinbo Bi</p>
<p><a href='https://openreview.net/forum?id=6JrckqCxtl'>https://openreview.net/forum?id=6JrckqCxtl</a></p>
<p><b>Keywords</b>: Feature interaction modeling, model interpretation framework, adptive-order interaction, piece-wise polynomial
</p><p><b>Compressor summary</b>: PAM is a new module that allows neural networks to model nonlinear feature interactions adaptively using polyhedrons, improving predictive performance and interpretability.</p><hr><h3>Learning World Models with Identifiable Factorization</h3>
<p>Yu-Ren Liu, Biwei Huang, Zhengmao Zhu, Honglong Tian, Mingming Gong, Yang Yu, Kun Zhang</p>
<p><a href='https://openreview.net/forum?id=6JJq5TW9Mc'>https://openreview.net/forum?id=6JJq5TW9Mc</a></p>
<p><b>Keywords</b>: Model-based Reinforcement Learning; Causal Representation Learning;
</p><p><b>Compressor summary</b>: IFactor is a framework that models four categories of latent state variables to efficiently represent information in high-dimensional, noisy, and non-stationary environments for reinforcement learning.</p><hr><h3>Hierarchical Randomized Smoothing</h3>
<p>Yan Scholten, Jan Schuchardt, Aleksandar Bojchevski, Stephan Günnemann</p>
<p><a href='https://openreview.net/forum?id=6IhNHKyuJO'>https://openreview.net/forum?id=6IhNHKyuJO</a></p>
<p><b>Keywords</b>: Adversarial Robustness, Robustness Certification, Randomized Smoothing, Graph Neural Networks
</p><p><b>Compressor summary</b>: Hierarchical randomized smoothing adds targeted noise to subsets of entities in complex objects, improving robustness while maintaining accuracy in classification tasks.</p><hr><h3>Certified Minimax Unlearning with Generalization Rates and Deletion Capacity</h3>
<p>Jiaqi Liu, Jian Lou, Zhan Qin, Kui Ren</p>
<p><a href='https://openreview.net/forum?id=6H8Md75kAw'>https://openreview.net/forum?id=6H8Md75kAw</a></p>
<p><b>Keywords</b>: machine unlearning, machin learning privacy, minimax learning, certified removal
</p><p><b>Compressor summary</b>: The paper proposes a new certified machine unlearning algorithm for minimax models that uses a total-Hessian-based complete Newton update and Gaussian mechanism, and achieves better generalization and deletion capacity than existing methods.</p><hr><h3>Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective</h3>
<p>Yuzheng Hu, Ruicheng Xian, Qilong Wu, Qiuling Fan, Lang Yin, Han Zhao</p>
<p><a href='https://openreview.net/forum?id=6EqUpqMnwl'>https://openreview.net/forum?id=6EqUpqMnwl</a></p>
<p><b>Keywords</b>: multi-task learning, scalarization, Pareto front
</p><p><b>Compressor summary</b>: The paper investigates whether scalarization can fully explore the Pareto front for linear multi-task learning models and finds that it is inherently incapable of doing so, especially for balanced trade-offs between tasks.</p><hr><h3>LinkerNet: Fragment Poses and Linker Co-Design with 3D Equivariant Diffusion</h3>
<p>Jiaqi Guan, Xingang Peng, PeiQi Jiang, Yunan Luo, Jian Peng, Jianzhu Ma</p>
<p><a href='https://openreview.net/forum?id=6EaLIw3W7c'>https://openreview.net/forum?id=6EaLIw3W7c</a></p>
<p><b>Keywords</b>: Linker design, generative models
</p><p><b>Compressor summary</b>: The paper proposes a new approach to design stable drug-candidate molecules for targeted protein degradation by jointly learning fragment poses and linker structures using a 3D equivariant diffusion model.</p><hr><h3>DDF-HO: Hand-Held Object Reconstruction via Conditional Directed Distance Field</h3>
<p>Chenyangguang Zhang, Yan Di, Ruida Zhang, Guangyao Zhai, Fabian Manhardt, Federico Tombari, Xiangyang Ji</p>
<p><a href='https://openreview.net/forum?id=6EDHfVHicP'>https://openreview.net/forum?id=6EDHfVHicP</a></p>
<p><b>Keywords</b>: hand-held object reconstruction, directed distance field, human-object interaction
</p><p><b>Compressor summary</b>: The proposed DDF-HO method uses Directed Distance Fields to represent shapes and capture hand-object interactions more effectively than existing Signed Distance Field methods, achieving better performance on synthetic and real-world datasets.</p><hr><h3>In-Context Learning Unlocked for Diffusion Models</h3>
<p>Zhendong Wang, Yifan Jiang, Yadong Lu, yelong shen, Pengcheng He, Weizhu Chen, Zhangyang Wang, Mingyuan Zhou</p>
<p><a href='https://openreview.net/forum?id=6BZS2EAkns'>https://openreview.net/forum?id=6BZS2EAkns</a></p>
<p><b>Keywords</b>: diffusion models, in-context learning
</p><p><b>Compressor summary</b>: Prompt Diffusion is a framework that enables in-context learning in diffusion-based generative models using vision-language prompts and a trained diffusion model, achieving high-quality results on various tasks and unseen vision tasks.</p><hr><h3>Causal Fairness for Outcome Control</h3>
<p>Drago Plecko, Elias Bareinboim</p>
<p><a href='https://openreview.net/forum?id=6AAbWSF6Qg'>https://openreview.net/forum?id=6AAbWSF6Qg</a></p>
<p><b>Keywords</b>: Fair Machine Learning, Causal Inference, Decision-Making
</p><p><b>Compressor summary</b>: The paper studies outcome control, where automated systems optimize an outcome variable while ensuring fairness and equity by considering sensitive attributes like gender, race, and religion.</p><hr><h3>Logarithmic-Regret Quantum Learning Algorithms for Zero-Sum Games</h3>
<p>Minbo Gao, Zhengfeng Ji, Tongyang Li, Qisheng Wang</p>
<p><a href='https://openreview.net/forum?id=69dAz94zPv'>https://openreview.net/forum?id=69dAz94zPv</a></p>
<p><b>Keywords</b>: Online learning, quantum computing, zero-sum games, linear programming, optimistic multiplicative weight update
</p><p><b>Compressor summary</b>: The paper presents a quantum algorithm that finds an approximate Nash equilibrium for zero-sum games quickly and with succinct outputs, using a novel quantum multi-sampling technique.</p><hr><h3>Counterfactual Memorization in Neural Language Models</h3>
<p>Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, Nicholas Carlini</p>
<p><a href='https://openreview.net/forum?id=67o9UQgTD0'>https://openreview.net/forum?id=67o9UQgTD0</a></p>
<p><b>Keywords</b>: Memorization, Language Models
</p><p><b>Compressor summary</b>: The paper proposes a method to study and quantify language model memorization by examining how predictions change when certain documents are removed from training data.</p><hr><h3>Revisit the Power of Vanilla Knowledge Distillation: from Small Scale to Large Scale</h3>
<p>Zhiwei Hao, Jianyuan Guo, Kai Han, Han Hu, Chang Xu, Yunhe Wang</p>
<p><a href='https://openreview.net/forum?id=67MTWzhEOn'>https://openreview.net/forum?id=67MTWzhEOn</a></p>
<p><b>Keywords</b>: knowledge distillation, small-data pitfall, vanilla kd
</p><p><b>Compressor summary</b>: The paper argues that using larger datasets and stronger data augmentation techniques can improve knowledge distillation approaches for limited-capacity architectures, and demonstrates this with state-of-the-art results on ImageNet.</p><hr><h3>A Competitive Algorithm for Agnostic Active Learning</h3>
<p>Yihan Zhou, Eric Price</p>
<p><a href='https://openreview.net/forum?id=66XhNDahk6'>https://openreview.net/forum?id=66XhNDahk6</a></p>
<p><b>Keywords</b>: active learning, binary classification, competitive ratio
</p><p><b>Compressor summary</b>: The paper proposes a new agnostic active learning algorithm that is competitive with the optimal algorithm and has a logarithmic overhead in the query complexity, and shows that improving this is NP-hard.</p><hr><h3>Computational Complexity of Learning Neural Networks: Smoothness and Degeneracy</h3>
<p>Amit Daniely, Nathan Srebro, Gal Vardi</p>
<p><a href='https://openreview.net/forum?id=65aDEXIhih'>https://openreview.net/forum?id=65aDEXIhih</a></p>
<p><b>Keywords</b>: Learning neural networks, Computational complexity, Hardness of learning, Smoothed analysis, Degenerate weights
</p><p><b>Compressor summary</b>: The paper investigates if assumptions about Gaussian inputs and non-degenerate weights are enough to learn deeper neural networks efficiently, and finds that they are not for depth-$3$ ReLU networks.</p><hr><h3>No-Regret Learning with Unbounded Losses: The Case of Logarithmic Pooling</h3>
<p>Eric Neyman, Tim Roughgarden</p>
<p><a href='https://openreview.net/forum?id=639RkUOmW8'>https://openreview.net/forum?id=639RkUOmW8</a></p>
<p><b>Keywords</b>: Logarithmic pooling, online learning, no-regret learning, calibrated experts, online mirror descent, prediction with expert advice
</p><p><b>Compressor summary</b>: The paper proposes a novel algorithm for learning expert weights in an online adversarial setting, where experts' forecasts are consistent and calibrated, to minimize log loss using logarithmic pooling and achieve low regret.</p><hr><h3>Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning</h3>
<p>Jianzhun Shao, Yun Qu, Chen Chen, Hongchang Zhang, Xiangyang Ji</p>
<p><a href='https://openreview.net/forum?id=62zmO4mv8X'>https://openreview.net/forum?id=62zmO4mv8X</a></p>
<p><b>Keywords</b>: multi-agent reinforcement learning, offline reinforcement learning
</p><p><b>Compressor summary</b>: The paper proposes CounterFactual Conservative Q-Learning (CFCQL), a new multi-agent offline RL algorithm that estimates conservative values for each agent separately, improving performance and reducing the impact of out-of-distribution actions and value overestimation.</p><hr><h3>MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining</h3>
<p>Jacob Portes, Alexander R Trott, Sam Havens, DANIEL KING, Abhinav Venigalla, Moin Nadeem, Nikhil Sardana, Daya Khudia, Jonathan Frankle</p>
<p><a href='https://openreview.net/forum?id=5zipcfLC2Z'>https://openreview.net/forum?id=5zipcfLC2Z</a></p>
<p><b>Keywords</b>: BERT, Pretraining, Efficiency, FlashAttention, ALiBi
</p><p><b>Compressor summary</b>: MosaicBERT is a fast and efficient BERT-style encoder architecture that allows for low-cost pretraining, enabling researchers to customize their own NLP models.</p><hr><h3>No Representation Rules Them All in Category Discovery</h3>
<p>Sagar Vaze, Andrea Vedaldi, Andrew Zisserman</p>
<p><a href='https://openreview.net/forum?id=5ytypAqAsR'>https://openreview.net/forum?id=5ytypAqAsR</a></p>
<p><b>Keywords</b>: Category discovery, semi-supervised learning, self-supervised learning, classification
</p><p><b>Compressor summary</b>: The paper introduces Clevr-4, a synthetic dataset for Generalized Category Discovery, and proposes a novel method called $\mu$GCD that leverages consistent findings from representation learning to outperform existing approaches on both synthetic and real data.</p><hr><h3>Many-body Approximation for Non-negative Tensors</h3>
<p>Kazu Ghalamkari, Mahito Sugiyama, Yoshinobu Kawahara</p>
<p><a href='https://openreview.net/forum?id=5yedZXV7wt'>https://openreview.net/forum?id=5yedZXV7wt</a></p>
<p><b>Keywords</b>: Tensor decomposition, Energy based model, Tensor networks
</p><p><b>Compressor summary</b>: The paper introduces many-body approximation, a new method for decomposing non-negative tensors that avoids problems with traditional methods by using energy-based modeling and global optimization.</p><hr><h3>Amortized Reparametrization: Efficient and Scalable Variational Inference for Latent SDEs</h3>
<p>Kevin Course, Prasanth B. Nair</p>
<p><a href='https://openreview.net/forum?id=5yZiP9fZNv'>https://openreview.net/forum?id=5yZiP9fZNv</a></p>
<p><b>Keywords</b>: variational inference, differential equations, dynamical systems, neural ordinary differential equations, latent stochastic differential equations
</p><p><b>Compressor summary</b>: The paragraph describes a novel method for inferring latent stochastic differential equations with low computational cost and memory usage, using amortization and a reparametrization technique.</p><hr><h3>StreamNet: Memory-Efficient Streaming Tiny Deep Learning Inference on the Microcontroller</h3>
<p>Hong Sheng Zheng, Yu-Yuan Liu, Chen-Fong Hsu, Tsung Tai Yeh</p>
<p><a href='https://openreview.net/forum?id=5t5u8PQa2T'>https://openreview.net/forum?id=5t5u8PQa2T</a></p>
<p><b>Keywords</b>: TinyML models, edge AIs, Microcontroller
</p><p><b>Compressor summary</b>: StreamNet is a method that improves the performance and memory efficiency of deploying TinyML models on low-power MCUs using 1D and 2D streaming processing and smart parameter selection.</p><hr><h3>Module-wise Training of Neural Networks via the Minimizing Movement Scheme</h3>
<p>Skander Karkar, Ibrahim Ayed, Emmanuel de Bezenac, patrick gallinari</p>
<p><a href='https://openreview.net/forum?id=5sV53leJCv'>https://openreview.net/forum?id=5sV53leJCv</a></p>
<p><b>Keywords</b>: Deep learning, greedy layerwise training, memory, optimal transport
</p><p><b>Compressor summary</b>: The paper introduces TRGL, a module-wise regularization method for neural networks that improves accuracy and reduces memory usage in limited memory settings.</p><hr><h3>Composing Parameter-Efficient Modules with Arithmetic Operation</h3>
<p>Jinghan Zhang, Shiqi Chen, Junteng Liu, Junxian He</p>
<p><a href='https://openreview.net/forum?id=5r3e27I9Gy'>https://openreview.net/forum?id=5r3e27I9Gy</a></p>
<p><b>Keywords</b>: Parameter-efficient fine-tuning, module composition
</p><p><b>Compressor summary</b>: The paper proposes a method to compose different parameter-efficient modules using linear arithmetic operations in the weight space for various tasks, improving their performance.</p><hr><h3>Pairwise Causality Guided Transformers for Event Sequences</h3>
<p>Xiao Shou, Debarun Bhattacharjya, Tian Gao, Dharmashankar Subramanian, Oktie Hassanzadeh, Kristin Bennett</p>
<p><a href='https://openreview.net/forum?id=5q8xovQF7r'>https://openreview.net/forum?id=5q8xovQF7r</a></p>
<p><b>Keywords</b>: temporal event sequences, causal inference, transformer, causal knowledge graph
</p><p><b>Compressor summary</b>: The paper proposes a new method to improve transformer-based models' performance in predicting temporal event sequences using pairwise causal knowledge, and shows its effectiveness in several experiments and applications.</p><hr><h3>Compression with Bayesian Implicit Neural Representations</h3>
<p>Zongyu Guo, Gergely Flamich, Jiajun He, Zhibo Chen, José Miguel Hernández-Lobato</p>
<p><a href='https://openreview.net/forum?id=5otj6QKUMI'>https://openreview.net/forum?id=5otj6QKUMI</a></p>
<p><b>Keywords</b>: Neural Compression, Implicit Neural Representation, Relative Entropy Coding, Bayesian Neural Network
</p><p><b>Compressor summary</b>: The authors propose a new method to compress data by overfitting neural networks to their functional representation and encoding the weights using relative entropy coding, which improves rate-distortion performance and allows adjusting trade-offs for different network architectures.</p><hr><h3>Cross-Scale MAE: A Tale of Multiscale Exploitation in Remote Sensing</h3>
<p>Maofeng Tang, Andrei Liviu Cozma, Konstantinos Georgiou, Hairong Qi</p>
<p><a href='https://openreview.net/forum?id=5oEVdOd6TV'>https://openreview.net/forum?id=5oEVdOd6TV</a></p>
<p><b>Keywords</b>: Remote Sensting, Self-Supervised Learning
</p><p><b>Compressor summary</b>: The paper introduces Cross-Scale MAE, a self-supervised model that learns consistent and meaningful representations for remote sensing image analysis using scale augmentation, cross-scale consistency constraints, contrastive and generative losses, and xFormers library.</p><hr><h3>LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference</h3>
<p>Hongwu Peng, Ran Ran, Yukui Luo, Jiahui Zhao, Shaoyi Huang, Kiran Thorat, Tong Geng, Chenghong Wang, Xiaolin Xu, Wujie Wen, Caiwen Ding</p>
<p><a href='https://openreview.net/forum?id=5loV5tVzsY'>https://openreview.net/forum?id=5loV5tVzsY</a></p>
<p><b>Keywords</b>: Privacy-Preserving Machine Learning, efficient private inference, machine learning as a service, homomorphic encryption, non-linear pruning, ST-GCN
</p><p><b>Compressor summary</b>: LinGCN is a framework that optimizes the performance of homomorphic encryption-based Graph Convolution Networks by reducing multiplication depth and improving latency, accuracy, and scalability for encrypted inference in applications like personal healthcare and financial systems.</p><hr><h3>Evolving Standardization for Continual Domain Generalization over Temporal Drift</h3>
<p>Mixue Xie, Shuang Li, Longhui Yuan, Chi Harold Liu, Zehui Dai</p>
<p><a href='https://openreview.net/forum?id=5hVXbiEGXB'>https://openreview.net/forum?id=5hVXbiEGXB</a></p>
<p><b>Keywords</b>: domain generalization, sequential learning, temporal drift, feature standardization
</p><p><b>Compressor summary</b>: EvoS is a method for continual domain generalization over temporal drift that uses multi-scale attention to learn evolving feature distribution patterns and standardize features with generated statistics, enabling models to adapt to gradually shifting data distributions in real-world applications.</p><hr><h3>A Cross-Moment Approach for Causal Effect Estimation</h3>
<p>Yaroslav Kivva, Saber Salehkaleybar, Negar Kiyavash</p>
<p><a href='https://openreview.net/forum?id=5gz7npbQ6Z'>https://openreview.net/forum?id=5gz7npbQ6Z</a></p>
<p><b>Keywords</b>: Causal inference, Difference-in-Difference, Structural causal models, Potential outcome, Proxy learning
</p><p><b>Compressor summary</b>: The paper proposes a new method to estimate the causal effect of a treatment on an outcome using cross moments, when only one proxy variable is available, and shows its advantages over existing methods that require restrictive assumptions or two proxy variables.</p><hr><h3>Quantification of Uncertainty with Adversarial Models</h3>
<p>Kajetan Schweighofer, Lukas Aichberger, Mykyta Ielanskyi, Günter Klambauer, Sepp Hochreiter</p>
<p><a href='https://openreview.net/forum?id=5eu00pcLWa'>https://openreview.net/forum?id=5eu00pcLWa</a></p>
<p><b>Keywords</b>: uncertainty, uncertainty quantification, predictive uncertainty, epistemic uncertainty, out of distribution, mc dropout, deep ensembles, sg-mcmc, adversarial model, adversarial model search, imagenet
</p><p><b>Compressor summary</b>: QUAM is a new method to estimate epistemic uncertainty better than current methods by focusing on regions where the product of divergence function and posterior is large, which corresponds to adversarial models.</p><hr><h3>Replicability in Reinforcement Learning</h3>
<p>Amin Karbasi, Grigoris Velegkas, Lin Yang, Felix Zhou</p>
<p><a href='https://openreview.net/forum?id=5cPz5hrjy6'>https://openreview.net/forum?id=5cPz5hrjy6</a></p>
<p><b>Keywords</b>: Theory, Reinforcement Learning Theory, Statistical Learning Theory, Reproducibility, Replicability
</p><p><b>Compressor summary</b>: The authors study replicability in reinforcement learning algorithms and provide efficient algorithms with theoretical guarantees for various settings.</p><hr><h3>The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs</h3>
<p>Laura Eline Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rocktäschel, Edward Grefenstette</p>
<p><a href='https://openreview.net/forum?id=5bWW9Eop7l'>https://openreview.net/forum?id=5bWW9Eop7l</a></p>
<p><b>Keywords</b>: large language models, pragmatics, natural language processing, communication, conversation, implicature, language model fine-tuning
</p><p><b>Compressor summary</b>: The paragraph discusses a study on whether large language models can understand implicatures, a type of inference based on beliefs and knowledge, and finds that instruction-tuned models perform better than others in this task.</p><hr><h3>Approximately Equivariant Graph Networks</h3>
<p>Ningyuan Teresa Huang, Ron Levie, Soledad Villar</p>
<p><a href='https://openreview.net/forum?id=5aeyKAZr0L'>https://openreview.net/forum?id=5aeyKAZr0L</a></p>
<p><b>Keywords</b>: graph neural networks, equivariant machine learning, symmetry, generalization, statistical learning
</p><p><b>Compressor summary</b>: The paper explores how active symmetries in GNNs affect their performance on various tasks and proposes a bias-variance formula to choose an appropriate symmetry group for each task.</p><hr><h3>Self-Adaptive Motion Tracking against On-body Displacement of Flexible Sensors</h3>
<p>Chengxu Zuo, Jiawei Fang, Shihui Guo, Yipeng Qin</p>
<p><a href='https://openreview.net/forum?id=5ZMBiS1uMq'>https://openreview.net/forum?id=5ZMBiS1uMq</a></p>
<p><b>Keywords</b>: motion tracking, flexible sensor, on-body displacement, deep learning, domain adaptation
</p><p><b>Compressor summary</b>: The authors propose a self-adaptive motion tracking network to deal with the challenges posed by flexible sensors' displacement on human body during different sessions, using a light-weight learnable layer, a Fourier-encoded LSTM network, and a novel sequence discrepancy loss.</p><hr><h3>Understanding Contrastive Learning via Distributionally Robust Optimization</h3>
<p>Junkang Wu, Jiawei Chen, Jiancan Wu, Wentao Shi, Xiang Wang, Xiangnan He</p>
<p><a href='https://openreview.net/forum?id=5XshcizH9w'>https://openreview.net/forum?id=5XshcizH9w</a></p>
<p><b>Keywords</b>: contrastive learning, distributionally robust optimization, mutual information
</p><p><b>Compressor summary</b>: The study shows that contrastive learning is robust to sampling bias, explains its connection to distributionally robust optimization and mutual information, and proposes a new loss function to improve performance and convergence.</p><hr><h3>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</h3>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik R Narasimhan</p>
<p><a href='https://openreview.net/forum?id=5Xc1ecxO1h'>https://openreview.net/forum?id=5Xc1ecxO1h</a></p>
<p><b>Keywords</b>: large language model, general problem solving, heuristic search, reasoning, planning, decision making
</p><p><b>Compressor summary</b>: The Tree of Thoughts framework improves language models' problem-solving abilities by enabling them to explore, plan, and evaluate multiple reasoning paths in tasks requiring non-trivial planning or search.</p><hr><h3>Characteristic Circuits</h3>
<p>Zhongjie Yu, Martin Trapp, Kristian Kersting</p>
<p><a href='https://openreview.net/forum?id=5W7cXno10k'>https://openreview.net/forum?id=5W7cXno10k</a></p>
<p><b>Keywords</b>: Characteristic Circuit, Characteristic Function, Probabilistic Circuit, Heterogeneous Data, Density Estimation
</p><p><b>Compressor summary</b>: Characteristic circuits are a new type of probabilistic model that can learn high-dimensional distributions over different types of data and perform efficient inference, even when no closed-form density function is available.</p><hr><h3>Replicable Clustering</h3>
<p>Hossein Esfandiari, Amin Karbasi, Vahab Mirrokni, Grigoris Velegkas, Felix Zhou</p>
<p><a href='https://openreview.net/forum?id=5VQFAvUHcd'>https://openreview.net/forum?id=5VQFAvUHcd</a></p>
<p><b>Keywords</b>: Theory, Clustering Theory, Statistical Learning Theory, Reproducibility, Replicability
</p><p><b>Compressor summary</b>: The authors propose replicable algorithms for statistical clustering problems like $k$-medians, $k$-means, and $k$-centers by using approximation routines for their combinatorial versions in a black-box manner.</p><hr><h3>Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance</h3>
<p>Jonathan Crabbé, Mihaela van der Schaar</p>
<p><a href='https://openreview.net/forum?id=5UwnKSgY6u'>https://openreview.net/forum?id=5UwnKSgY6u</a></p>
<p><b>Keywords</b>: interpretability, explainability, robustness, invariance, equivariance, geometric deep learning
</p><p><b>Compressor summary</b>: The paper proposes a framework to measure and improve the robustness of interpretability methods for neural networks that are invariant under specific symmetry groups.</p><hr><h3>Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing</h3>
<p>Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, Joost van de Weijer</p>
<p><a href='https://openreview.net/forum?id=5UXXhVI08r'>https://openreview.net/forum?id=5UXXhVI08r</a></p>
<p><b>Keywords</b>: Diffusion Models; Text-guided Image Edit; Textual Inversion; Localization
</p><p><b>Compressor summary</b>: Dynamic Prompt Learning (DPL) improves text-to-image generation by focusing cross-attention maps on correct nouns in the prompt, enabling fine-grained image editing without unwanted changes.</p><hr><h3>Uni3DETR: Unified 3D Detection Transformer</h3>
<p>Zhenyu Wang, Ya-Li Li, Xi Chen, Hengshuang Zhao, Shengjin Wang</p>
<p><a href='https://openreview.net/forum?id=5UOYGfobhC'>https://openreview.net/forum?id=5UOYGfobhC</a></p>
<p><b>Keywords</b>: 3d object detection, unified object detection, point clouds
</p><p><b>Compressor summary</b>: Uni3DETR is a unified 3D detector that works well for both indoor and outdoor scenes by using a detection transformer with point-voxel interaction and a mixture of query points.</p><hr><h3>Variational Inference with Gaussian Score Matching</h3>
<p>Chirag Modi, Robert M. Gower, Charles Margossian, Yuling Yao, David Blei, Lawrence K. Saul</p>
<p><a href='https://openreview.net/forum?id=5TTV5IZnLL'>https://openreview.net/forum?id=5TTV5IZnLL</a></p>
<p><b>Keywords</b>: Variational Inference, score matching, KL projection, polyak stepsize
</p><p><b>Compressor summary</b>: The authors propose a new variational inference method called Gaussian score matching VI (GSM-VI), which uses the principle of score matching and is faster and equally or more accurate than black box variational inference (BBVI) for various Bayesian inference problems.</p><hr><h3>Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training</h3>
<p>Rie Johnson, Tong Zhang</p>
<p><a href='https://openreview.net/forum?id=5SIz31OGFV'>https://openreview.net/forum?id=5SIz31OGFV</a></p>
<p><b>Keywords</b>: Deep neural network training, Generalization gap, Empirical study
</p><p><b>Compressor summary</b>: The paper analyzes how inconsistency and instability of model outputs affect the generalization gap in deep neural networks, and shows that reducing inconsistency improves performance.</p><hr><h3>Smoothed Analysis of Sequential Probability Assignment</h3>
<p>Alankrita Bhatt, Nika Haghtalab, Abhishek Shetty</p>
<p><a href='https://openreview.net/forum?id=5R9bZlpZKj'>https://openreview.net/forum?id=5R9bZlpZKj</a></p>
<p><b>Keywords</b>: Online learning, Log loss, Information theory, Smoothed Analysis, Beyond worst case analysis, Oracle Efficient Online Learning
</p><p><b>Compressor summary</b>: The study explores smoothed analysis for sequential probability assignment problem with contexts, leading to optimal fast rates and efficient algorithms for minimax learning.</p><hr><h3>Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models</h3>
<p>Zhong Yi Wan, Ricardo Baptista, Anudhyan Boral, Yi-Fan Chen, John Anderson, Fei Sha, Leonardo Zepeda-Nunez</p>
<p><a href='https://openreview.net/forum?id=5NxJuc0T1P'>https://openreview.net/forum?id=5NxJuc0T1P</a></p>
<p><b>Keywords</b>: optimal transport, probabilistic diffusion models, statistical downscaling
</p><p><b>Compressor summary</b>: The paper presents a two-stage probabilistic framework that uses unpaired data to downscale low-resolution numerical simulations of fluid flow to high-resolution data, while preserving the physical statistics and addressing the core difficulties in weather and climate modeling.</p><hr><h3>Global Optimality in Bivariate Gradient-based DAG Learning</h3>
<p>Chang Deng, Kevin Bello, Pradeep Kumar Ravikumar, Bryon Aragam</p>
<p><a href='https://openreview.net/forum?id=5MG5C5aS6m'>https://openreview.net/forum?id=5MG5C5aS6m</a></p>
<p><b>Keywords</b>: global optimization, nonconvex optimization, graphical models, directed acyclic graphs, structure learning
</p><p><b>Compressor summary</b>: The paper shows how a simple optimization method can find the best solution for learning an acyclic graphical model from data without getting stuck in spurious solutions.</p><hr><h3>Fast Bellman Updates for Wasserstein Distributionally Robust MDPs</h3>
<p>Zhuodong Yu, Ling Dai, Shaohang Xu, Siyang Gao, Chin Pang Ho</p>
<p><a href='https://openreview.net/forum?id=5La4Y8BnQw'>https://openreview.net/forum?id=5La4Y8BnQw</a></p>
<p><b>Keywords</b>: Markov decision processes, distributionally robust optimization
</p><p><b>Compressor summary</b>: The paper introduces a fast algorithm for solving distributionally robust MDPs with Wasserstein ambiguity sets, which reduces computational cost and improves performance over existing methods.</p><hr><h3>GeoTMI: Predicting Quantum Chemical Property with Easy-to-Obtain Geometry via Positional Denoising</h3>
<p>Hyeonsu Kim, Jeheon Woo, SEONGHWAN KIM, Seokhyun Moon, Jun Hyeong Kim, Woo Youn Kim</p>
<p><a href='https://openreview.net/forum?id=5JcKKRX2iH'>https://openreview.net/forum?id=5JcKKRX2iH</a></p>
<p><b>Keywords</b>: Mutual information, Easy-to-obtain geometry, Denoising, 3D Graph neural network, OC20
</p><p><b>Compressor summary</b>: GeoTMI is a new framework that uses denoising to predict quantum chemical properties accurately using easy-to-obtain corrupted geometries.</p><hr><h3>Computational Guarantees for Doubly Entropic Wasserstein Barycenters</h3>
<p>Tomas Vaskevicius, Lénaïc Chizat</p>
<p><a href='https://openreview.net/forum?id=5HahZRA0fy'>https://openreview.net/forum?id=5HahZRA0fy</a></p>
<p><b>Keywords</b>: Wasserstein barycenters, entropic penalization, optimal transport, Sinkhorn's algorithm
</p><p><b>Compressor summary</b>: The paper presents an algorithm for computing doubly regularized Wasserstein barycenters with convergence guarantees and extends it to handle discrete point clouds using Monte Carlo sampling.</p><hr><h3>PAC Learning Linear Thresholds from Label Proportions</h3>
<p>Anand Paresh Brahmbhatt, Rishi Saket, Aravindan Raghuveer</p>
<p><a href='https://openreview.net/forum?id=5Gw9YkJkFF'>https://openreview.net/forum?id=5Gw9YkJkFF</a></p>
<p><b>Keywords</b>: PAC learning, Learning from label proportions, Linear thresholds
</p><p><b>Compressor summary</b>: The paper presents efficient algorithms for learning linear threshold functions from label proportions using Gaussian distributions and shows their effectiveness through experiments.</p><hr><h3>Strong and Precise Modulation of Human Percepts via Robustified ANNs</h3>
<p>Guy Gaziv, Michael J. Lee, James J. DiCarlo</p>
<p><a href='https://openreview.net/forum?id=5GmTI4LNqX'>https://openreview.net/forum?id=5GmTI4LNqX</a></p>
<p><b>Keywords</b>: Vision, Object Recognition, Human, Primate, Ventral Stream, Adversarial Examples, Behavior Modulation, Behavioral Alignment
</p><p><b>Compressor summary</b>: The paragraph discusses how robustified artificial neural networks (ANNs) can disrupt and manipulate human visual perception by finding low-norm image perturbations, while human percepts remain stable in the same regime.</p><hr><h3>Diffusion Model for Graph Inverse Problems: Towards Effective Source Localization on Complex Networks</h3>
<p>Xin Yan, Hui Fang, Qiang He</p>
<p><a href='https://openreview.net/forum?id=5Fr8Nwi5KF'>https://openreview.net/forum?id=5Fr8Nwi5KF</a></p>
<p><b>Keywords</b>: Diffusion Model, Graph Inverse Problems, Source Localization, Information Diffusion
</p><p><b>Compressor summary</b>: The paper proposes a probabilistic model called DDMSL to locate the sources and reconstruct the paths of information diffusion over complex networks, using Markov chains and a reversible residual network.</p><hr><h3>Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective</h3>
<p>Zeyuan Yin, Eric Xing, Zhiqiang Shen</p>
<p><a href='https://openreview.net/forum?id=5Fgdk3hZpb'>https://openreview.net/forum?id=5Fgdk3hZpb</a></p>
<p><b>Keywords</b>: dataset condensation and distillation, ImageNet Scale
</p><p><b>Compressor summary</b>: The SRe$^2$L framework creates smaller synthetic data to train models efficiently on different dataset scales and resolutions, achieving state-of-the-art results with fast speed and less memory consumption.</p><hr><h3>Provable Guarantees for Neural Networks via Gradient Feature Learning</h3>
<p>Zhenmei Shi, Junyi Wei, Yingyu Liang</p>
<p><a href='https://openreview.net/forum?id=5F04bU79eK'>https://openreview.net/forum?id=5F04bU79eK</a></p>
<p><b>Keywords</b>: neural networks, gradient descent, feature learning, provable guarantees, theoretical analysis
</p><p><b>Compressor summary</b>: This paper presents a unified analysis framework for two-layer neural networks trained by gradient descent, which explains their ability to learn features from gradients and applies to various problems, revealing new insights into network learning.</p><hr><h3>Adaptive Normalization for Non-stationary Time Series Forecasting: A Temporal Slice Perspective</h3>
<p>Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, Enhong Chen</p>
<p><a href='https://openreview.net/forum?id=5BqDSw8r5j'>https://openreview.net/forum?id=5BqDSw8r5j</a></p>
<p><b>Keywords</b>: Time series forecasting, deep learning, normalization
</p><p><b>Compressor summary</b>: The paragraph describes a new method called SAN that improves time series forecasting by adaptively normalizing and denormalizing data at the slice level, accounting for non-stationary changes in statistical properties.</p><hr><h3>A Spectral Theory of Neural Prediction and Alignment</h3>
<p>Abdulkadir Canatar, Jenelle Feather, Albert Wakhloo, SueYeon Chung</p>
<p><a href='https://openreview.net/forum?id=5B1ZK60jWn'>https://openreview.net/forum?id=5B1ZK60jWn</a></p>
<p><b>Keywords</b>: computational neuroscience, neural manifolds, neuro-AI, statistical physics, representational geometry
</p><p><b>Compressor summary</b>: The authors analyze how different deep neural networks predict visual cortical activity using a theoretical framework that relates generalization error to spectral properties, and introduce geometrical measures to interpret the neural prediction error.</p><hr><h3>Common Ground in Cooperative Communication</h3>
<p>Xiaoran Hao, Yash Jhaveri, Patrick Shafto</p>
<p><a href='https://openreview.net/forum?id=5AMa9fiyJq'>https://openreview.net/forum?id=5AMa9fiyJq</a></p>
<p><b>Keywords</b>: Cooperative Communication, Common Ground, Bayesian Theory
</p><p><b>Compressor summary</b>: The paragraph discusses a new theory of cooperative communication that accounts for different levels of common ground, connects it to variational autoencoding, and tests it with simulations.</p><hr><h3>Efficient Meta Neural Heuristic for Multi-Objective Combinatorial Optimization</h3>
<p>Jinbiao Chen, Jiahai Wang, Zizhen Zhang, Zhiguang Cao, Te Ye, Siyuan Chen</p>
<p><a href='https://openreview.net/forum?id=593fc38lhN'>https://openreview.net/forum?id=593fc38lhN</a></p>
<p><b>Keywords</b>: neural heuristic, meta learning, deep reinforcement learning, multi-objective combinatorial optimization
</p><p><b>Compressor summary</b>: The proposed efficient meta neural heuristic (EMNH) uses a multi-task model for parallel learning and a hierarchical method for fine-tuning to solve MOCOPs faster and better than existing neural heuristics.</p><hr><h3>Bayesian Metric Learning for Uncertainty Quantification in Image Retrieval</h3>
<p>Frederik Rahbæk Warburg, Marco Miani, Silas Brack, Søren Hauberg</p>
<p><a href='https://openreview.net/forum?id=58XMiu8kot'>https://openreview.net/forum?id=58XMiu8kot</a></p>
<p><b>Keywords</b>: Laplace approximation, metric learning, uncertainty quantification, weight posterior, bayesian
</p><p><b>Compressor summary</b>: The paper introduces a Bayesian encoder for metric learning using the Laplace Approximation, which improves uncertainty, out-of-distribution detection, and predictive performance.</p><hr><h3>Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement</h3>
<p>Hui Yuan, Kaixuan Huang, Chengzhuo Ni, Minshuo Chen, Mengdi Wang</p>
<p><a href='https://openreview.net/forum?id=58HwnnEdtF'>https://openreview.net/forum?id=58HwnnEdtF</a></p>
<p><b>Keywords</b>: Theory, Diffusion Model, Reward Optimization, Low-dimensional Data, Distribution estimation
</p><p><b>Compressor summary</b>: The authors propose a method to generate samples with desired properties using a reward function and a conditional diffusion model, and show its effectiveness in theory and practice.</p><hr><h3>Language Models are Weak Learners</h3>
<p>Hariharan Manikandan, Yiding Jiang, J Zico Kolter</p>
<p><a href='https://openreview.net/forum?id=559NJBfN20'>https://openreview.net/forum?id=559NJBfN20</a></p>
<p><b>Keywords</b>: language model, prompting, tabular data, summarization, boosting, adaboost
</p><p><b>Compressor summary</b>: The paper demonstrates that large language models can act as weak learners in a boosting algorithm for tabular data, achieving better results than traditional tree-based methods and few-shot learning.</p><hr><h3>GeoPhy: Differentiable Phylogenetic Inference via Geometric Gradients of Tree Topologies</h3>
<p>Takahiro Mimori, Michiaki Hamada</p>
<p><a href='https://openreview.net/forum?id=54z8M7NTbJ'>https://openreview.net/forum?id=54z8M7NTbJ</a></p>
<p><b>Keywords</b>: phylogenetic inference, variational inference, control variates, hyperbolic space
</p><p><b>Compressor summary</b>: GeoPhy is a new method for phylogenetic inference that uses continuous geometric spaces to represent tree topologies and improve variational Bayesian models without restricting possible tree candidates.</p><hr><h3>Quantifying the Cost of Learning in Queueing Systems</h3>
<p>Daniel Freund, Thodoris Lykouris, Wentao Weng</p>
<p><a href='https://openreview.net/forum?id=54hYifmQZU'>https://openreview.net/forum?id=54hYifmQZU</a></p>
<p><b>Keywords</b>: bandits, learning, queueing systems, optimal control
</p><p><b>Compressor summary</b>: The paper introduces the Cost of Learning in Queueing (CLQ), a metric to measure how much parameter uncertainty affects queue length in queueing systems, and develops a unified analysis framework for CLQ that combines Lyapunov and bandit analysis.</p><hr><h3>Towards a fuller understanding of neurons with Clustered Compositional Explanations</h3>
<p>Biagio La Rosa, Leilani H. Gilpin, Roberto Capobianco</p>
<p><a href='https://openreview.net/forum?id=51PLYhMFWz'>https://openreview.net/forum?id=51PLYhMFWz</a></p>
<p><b>Keywords</b>: compositional explanations, network dissection, explainable artificial intelligence, interpretability
</p><p><b>Compressor summary</b>: Clustered Compositional Explanations is a method for broadening the spectrum of neuron behavior approximated by logical formulas of concepts, by combining existing techniques with clustering and a novel search heuristic.</p><hr><h3>Recovering Simultaneously Structured Data via Non-Convex Iteratively Reweighted Least Squares</h3>
<p>Christian Kümmerle, Johannes Maly</p>
<p><a href='https://openreview.net/forum?id=50hs53Zb3w'>https://openreview.net/forum?id=50hs53Zb3w</a></p>
<p><b>Keywords</b>: low-rank models, sparsity, iteratively reweighted least squares, non-convex optimization, quadratic convergence, simultaneously structured data
</p><p><b>Compressor summary</b>: The paper introduces an algorithm (IRLS) for recovering data with multiple structures (row-sparsity and low-rank) from linear observations, which has better performance and sample complexity than existing methods.</p><hr><h3>A Unified Detection Framework for Inference-Stage Backdoor Defenses</h3>
<p>Xun Xian, Ganghua Wang, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, Jie Ding</p>
<p><a href='https://openreview.net/forum?id=4zWEyYGGfI'>https://openreview.net/forum?id=4zWEyYGGfI</a></p>
<p><b>Keywords</b>: Backdoor attacks, Backdoor Defense, Security for AI
</p><p><b>Compressor summary</b>: The study proposes a framework for detecting backdoor attacks in machine learning models that guarantees few false positives and maximizes the accuracy of identifying poisoned samples.</p><hr><h3>On Proper Learnability between Average- and Worst-case Robustness</h3>
<p>Vinod Raman, UNIQUE SUBEDI, Ambuj Tewari</p>
<p><a href='https://openreview.net/forum?id=4yXnnCK3r9'>https://openreview.net/forum?id=4yXnnCK3r9</a></p>
<p><b>Keywords</b>: Adversarial Robustness, PAC Learning
</p><p><b>Compressor summary</b>: The authors study how relaxing the robust loss in adversarial learning can improve sample complexity and learnability, while showing that some existing relaxations still require more than finite VC dimension for proper learning.</p><hr><h3>Attention as Implicit Structural Inference</h3>
<p>Ryan Singh, Christopher Buckley</p>
<p><a href='https://openreview.net/forum?id=4xckZu4MPG'>https://openreview.net/forum?id=4xckZu4MPG</a></p>
<p><b>Keywords</b>: Attention, Structural Inference, Variational Inference, Predictive Coding, Graphical Models
</p><p><b>Compressor summary</b>: The paragraph discusses how attention mechanisms in Transformers can be viewed as inference over adjacency structures in graphical models, unifying different attentional architectures and suggesting potential modifications and generalizations.</p><hr><h3>Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias</h3>
<p>Zhongwei Wan, Che Liu, Mi Zhang, Jie Fu, Benyou Wang, Sibo Cheng, Lei Ma, César Quilodrán-Casas, Rossella Arcucci</p>
<p><a href='https://openreview.net/forum?id=4vpsQdRBlK'>https://openreview.net/forum?id=4vpsQdRBlK</a></p>
<p><b>Keywords</b>: Medical Vision Langauge Pretraining, Cross-lingual, Language bias
</p><p><b>Compressor summary</b>: This paper introduces Med-UniC, a framework to integrate medical data from English and Spanish, using Cross-Lingual Text Alignment Regularization (CTR) to reduce community bias and improve performance in various medical vision-language tasks.</p><hr><h3>Unsupervised Behavior Extraction via Random Intent Priors</h3>
<p>Hao Hu, Yiqin Yang, Jianing Ye, Ziqing Mai, Chongjie Zhang</p>
<p><a href='https://openreview.net/forum?id=4vGVQVz5KG'>https://openreview.net/forum?id=4vGVQVz5KG</a></p>
<p><b>Keywords</b>: offline RL, reward-free, behavior extraction
</p><p><b>Compressor summary</b>: UBER is an unsupervised method that uses diverse pseudo-rewards from random neural networks to extract useful behaviors from offline reward-free datasets and improve online RL performance.</p><hr><h3>HiBug: On Human-Interpretable Model Debug</h3>
<p>Muxi Chen, YU LI, Qiang Xu</p>
<p><a href='https://openreview.net/forum?id=4sDHLxKb1L'>https://openreview.net/forum?id=4sDHLxKb1L</a></p>
<p><b>Keywords</b>: model debugging, error slice discovery
</p><p><b>Compressor summary</b>: HiBug is a framework that uses pre-trained models like chatGPT to automatically discover and explain model bugs in computer vision tasks by identifying underperforming data slices and suggesting human-understandable attributes.</p><hr><h3>Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback</h3>
<p>TaeHo Yoon, Kibeom Myoung, Keon Lee, Jaewoong Cho, Albert No, Ernest K. Ryu</p>
<p><a href='https://openreview.net/forum?id=4qG2RKuZaA'>https://openreview.net/forum?id=4qG2RKuZaA</a></p>
<p><b>Keywords</b>: Generative models, Diffusion probabilistic models, Controlled generation, Human Feedback, RLHF
</p><p><b>Compressor summary</b>: The paper proposes censored image generation using a pre-traired diffusion model and minimal human feedback, which is very efficient in preventing undesirable images.</p><hr><h3>Energy-Based Models for Anomaly Detection: A Manifold Diffusion Recovery Approach</h3>
<p>Sangwoong Yoon, Young-Uk Jin, Yung-Kyun Noh, Frank C. Park</p>
<p><a href='https://openreview.net/forum?id=4nSDDokpfK'>https://openreview.net/forum?id=4nSDDokpfK</a></p>
<p><b>Keywords</b>: Energy-based Models, Anomaly Detection, Generative Models, Out-of-Distribution Detection, Recovery Likelihood
</p><p><b>Compressor summary</b>: The proposed algorithm MPDR trains energy-based models for anomaly detection by perturbing data points along low-dimensional manifolds and generating near-manifold negative samples.</p><hr><h3>Stable Vectorization of Multiparameter Persistent Homology using Signed Barcodes as Measures</h3>
<p>David Loiseaux, Luis Scoccola, Mathieu Carrière, Magnus Bakke Botnan, Steve Oudot</p>
<p><a href='https://openreview.net/forum?id=4mwORQjAim'>https://openreview.net/forum?id=4mwORQjAim</a></p>
<p><b>Keywords</b>: topological data analysis, multiparameter persistent homology, kernel methods, optimal transport
</p><p><b>Compressor summary</b>: The paper introduces a new method for stable vectorization of multiparameter persistent homology descriptors using signed Radon measures and demonstrates its effectiveness on different data sets.</p><hr><h3>Online Pricing for Multi-User Multi-Item Markets</h3>
<p>Yigit Efe Erginbas, Thomas Courtade, Kannan Ramchandran, Soham Rajesh Phade</p>
<p><a href='https://openreview.net/forum?id=4mXYJzoPhf'>https://openreview.net/forum?id=4mXYJzoPhf</a></p>
<p><b>Keywords</b>: revenue, price, offer, online
</p><p><b>Compressor summary</b>: The study proposes online algorithms to price items efficiently and learn user valuations from feedback, and compares their performance under three user valuation models.</p><hr><h3>Multi-Modal Inverse Constrained Reinforcement Learning from a Mixture of Demonstrations</h3>
<p>Guanren Qiao, Guiliang Liu, Pascal Poupart, zhiqiang xu</p>
<p><a href='https://openreview.net/forum?id=4mPiqh4pLb'>https://openreview.net/forum?id=4mPiqh4pLb</a></p>
<p><b>Keywords</b>: Inverse Constrained Reinforcement Learning, Learning from Demonstrations, Muti-Modal Learning
</p><p><b>Compressor summary</b>: MMICRL is a new algorithm that can learn multiple constraints from diverse expert agents by using flow-based density estimation and contrastive learning, leading to better imitation policies and behavior diversity.</p><hr><h3>Operator Learning with Neural Fields: Tackling PDEs on General Geometries</h3>
<p>Louis Serrano, Lise Le Boudec, Armand Kassaï Koupaï, Thomas X Wang, Yuan Yin, Jean-Noël Vittaut, patrick gallinari</p>
<p><a href='https://openreview.net/forum?id=4jEjq5nhg1'>https://openreview.net/forum?id=4jEjq5nhg1</a></p>
<p><b>Keywords</b>: PDEs, Physics, Operator Learning, Deep Learning, Spatiotemporal
</p><p><b>Compressor summary</b>: CORAL is a new method that uses coordinate-based networks to solve PDEs on any spatial sampling and geometry, achieving robust performance across multiple resolutions and problem domains.</p><hr><h3>Train 'n Trade: Foundations of Parameter Markets</h3>
<p>Tzu-Heng Huang, Harit Vishwakarma, Frederic Sala</p>
<p><a href='https://openreview.net/forum?id=4iV26fZPUD'>https://openreview.net/forum?id=4iV26fZPUD</a></p>
<p><b>Keywords</b>: Parameter Market, Pricing, Efficient Model Training
</p><p><b>Compressor summary</b>: The authors propose a framework for trading model parameters as commodities, which could improve large-scale model training and create value for agents.</p><hr><h3>Data-Dependent Bounds for Online Portfolio Selection Without Lipschitzness and Smoothness</h3>
<p>Chung-En Tsai, Ying-Ting Lin, Yen-Huan Li</p>
<p><a href='https://openreview.net/forum?id=4iTAUsyisM'>https://openreview.net/forum?id=4iTAUsyisM</a></p>
<p><b>Keywords</b>: Online portfolio selection, small-loss bound, gradual-variation bound, second-order bound, optimistic FTRL with self-concordant regularizers
</p><p><b>Compressor summary</b>: The paper presents new regret bounds for online portfolio selection that depend on the data and apply to non-Lipschitz, non-smooth losses, using novel smoothness characterizations and analysis techniques.</p><hr><h3>Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification</h3>
<p>Neel Guha, Mayee F Chen, Kush Bhatia, Azalia Mirhoseini, Frederic Sala, Christopher Re</p>
<p><a href='https://openreview.net/forum?id=4iMpwAlza1'>https://openreview.net/forum?id=4iMpwAlza1</a></p>
<p><b>Keywords</b>: language models, prompting, embeddings, weak supervision
</p><p><b>Compressor summary</b>: Embroid is a method that improves language models' prompt-based learning by using consistency between predictions for similar samples to generate corrected predictions without additional labeled data.</p><hr><h3>AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback</h3>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori Hashimoto</p>
<p><a href='https://openreview.net/forum?id=4hturzLcKX'>https://openreview.net/forum?id=4hturzLcKX</a></p>
<p><b>Keywords</b>: Instruction-Following, Reinforcement Learning from Human Feedback, Artificial General Intelligence, Large Language Models
</p><p><b>Compressor summary</b>: AlpacaFarm is a simulator for training large language models with human feedback, which is cheaper, more trustworthy, and easier to evaluate than real-world data.</p><hr><h3>Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation</h3>
<p>Zhongqi Yue, Qianru Sun, Hanwang Zhang</p>
<p><a href='https://openreview.net/forum?id=4hYIxI8ds0'>https://openreview.net/forum?id=4hYIxI8ds0</a></p>
<p><b>Keywords</b>: unsupervised domain adaptation, transfer learning
</p><p><b>Compressor summary</b>: The paper proposes ICON, a method for unsupervised domain adaptation that removes spurious correlations between features by learning an invariant classifier that is consistent with labels in the source domain and clusters in the target domain.</p><hr><h3>Recovering from Out-of-sample States via Inverse Dynamics in Offline Reinforcement Learning</h3>
<p>Ke Jiang, Jia-Yu Yao, Xiaoyang Tan</p>
<p><a href='https://openreview.net/forum?id=4gLWjSaw4o'>https://openreview.net/forum?id=4gLWjSaw4o</a></p>
<p><b>Keywords</b>: Offline reinforcement learning, state distributional shift, state recovery, inverse dynamics model
</p><p><b>Compressor summary</b>: The paper proposes a method to improve offline reinforcement learning by using an inverse dynamics model to guide actions that recover the state distribution, leading to better performance on benchmark tasks.</p><hr><h3>Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning</h3>
<p>Hanlin Zhu, Paria Rashidinejad, Jiantao Jiao</p>
<p><a href='https://openreview.net/forum?id=4e0NJbkkd8'>https://openreview.net/forum?id=4e0NJbkkd8</a></p>
<p><b>Keywords</b>: offline RL, actor-critic, l_2 single-policy concentrability, average bellman error
</p><p><b>Compressor summary</b>: A-Crab is an offline reinforcement learning algorithm that uses importance sampling and actor-critic framework, achieving optimal convergence rates and outperforming existing methods in complex environments with insufficient data coverage.</p><hr><h3>Likelihood Ratio Confidence Sets for Sequential Decision Making</h3>
<p>Nicolas Emmenegger, Mojmir Mutny, Andreas Krause</p>
<p><a href='https://openreview.net/forum?id=4anryczeED'>https://openreview.net/forum?id=4anryczeED</a></p>
<p><b>Keywords</b>: confidence sets, uncertainty quantification, bandits, active learning, testing
</p><p><b>Compressor summary</b>: The paper proposes a new likelihood-based method to construct valid uncertainty estimates for sequential decision-making algorithms that works well for problems with well-specified likelihoods and can handle different noise distributions and estimators.</p><hr><h3>What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement.</h3>
<p>Yotam Alexander, Nimrod De La Vega, Noam Razin, Nadav Cohen</p>
<p><a href='https://openreview.net/forum?id=4aIpgq1nuI'>https://openreview.net/forum?id=4aIpgq1nuI</a></p>
<p><b>Keywords</b>: Deep Learning, Locally Connected Neural Networks, Data Distributions, Quantum Entanglement, Tensor Networks
</p><p><b>Compressor summary</b>: The authors apply quantum physics concepts to study the suitability of locally connected neural networks for different data distributions, and propose a preprocessing method based on low quantum entanglement.</p><hr><h3>Breaking the Communication-Privacy-Accuracy Tradeoff with $f$-Differential Privacy</h3>
<p>Richeng Jin, Zhonggen Su, Caijun Zhong, Zhaoyang Zhang, Tony Quek, Huaiyu Dai</p>
<p><a href='https://openreview.net/forum?id=4ZaPpVDjGQ'>https://openreview.net/forum?id=4ZaPpVDjGQ</a></p>
<p><b>Keywords</b>: Differential privacy, federated data analytics, discrete valued-mechanism, distributed mean estimation
</p><p><b>Compressor summary</b>: The paper explores how discrete-valued mechanisms with finite output space can provide local differential privacy guarantees for federated data analytics while also improving communication efficiency and accuracy.</p><hr><h3>Learning Sample Difficulty from Pre-trained Models for Reliable Prediction</h3>
<p>Peng Cui, Dan Zhang, Zhijie Deng, Yinpeng Dong, Jun Zhu</p>
<p><a href='https://openreview.net/forum?id=4WPhXYMK6N'>https://openreview.net/forum?id=4WPhXYMK6N</a></p>
<p><b>Keywords</b>: uncertainty calibration, sample difficulty, reliable prediction
</p><p><b>Compressor summary</b>: The authors propose a method to use large pre-trained models to improve the reliability and calibration of downstream models by incorporating sample difficulty into entropy regularization, leading to better accuracy and uncertainty estimation.</p><hr><h3>Structured State Space Models for In-Context Reinforcement Learning</h3>
<p>Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Nicolaus Foerster, Satinder Singh, Feryal Behbahani</p>
<p><a href='https://openreview.net/forum?id=4W9FVg1j6I'>https://openreview.net/forum?id=4W9FVg1j6I</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Meta-Learning, State Space Models
</p><p><b>Compressor summary</b>: The authors propose a modified S4 model for reinforcement learning tasks, which initializes and resets the hidden state in parallel, resulting in faster speeds and better performance than Transformers and RNNs on various tasks.</p><hr><h3>Adaptive Selective Sampling for Online Prediction with Experts</h3>
<p>Rui M. Castro, Fredrik Hellström, Tim van Erven</p>
<p><a href='https://openreview.net/forum?id=4VAF3d5jNg'>https://openreview.net/forum?id=4VAF3d5jNg</a></p>
<p><b>Keywords</b>: Online learning, prediction with experts, selective sampling, active learning
</p><p><b>Compressor summary</b>: The text describes a method for online prediction with expert advice that uses selective sampling to reduce label queries while still achieving optimal regret guarantees in some cases.</p><hr><h3>Training-free Diffusion Model Adaptation for Variable-Sized Text-to-Image Synthesis</h3>
<p>Zhiyu Jin, Xuli Shen, Bin Li, Xiangyang Xue</p>
<p><a href='https://openreview.net/forum?id=4ULTSBBY4U'>https://openreview.net/forum?id=4ULTSBBY4U</a></p>
<p><b>Keywords</b>: Text-to-Image Synthesis, Variable-Sized Image Synthesis, Entropy
</p><p><b>Compressor summary</b>: The paper proposes a scaling factor for diffusion models to handle various image sizes and aspect ratios, improving visual fidelity, quality, and alignment without extra training.</p><hr><h3>MuSe-GNN: Learning Unified Gene Representation From Multimodal Biological Graph Data</h3>
<p>Tianyu Liu, Yuge Wang, Zhitao Ying, Hongyu Zhao</p>
<p><a href='https://openreview.net/forum?id=4UCktT9XZx'>https://openreview.net/forum?id=4UCktT9XZx</a></p>
<p><b>Keywords</b>: Multimodal Learning; Representation Learning; Graph Neural Network; Similarity Learning; Contrastive Learning; Computational Biology and Bioinformatics; Single-cell genomics
</p><p><b>Compressor summary</b>: The novel Multimodal Similarity Learning Graph Neural Network model combines multimodal machine learning and deep graph neural networks to learn gene representations from diverse data, enabling better analysis of gene functions and related biomedical phenomena.</p><hr><h3>Reversible and irreversible bracket-based dynamics for deep graph neural networks</h3>
<p>Anthony Gruber, Kookjin Lee, Nathaniel Trask</p>
<p><a href='https://openreview.net/forum?id=4SoTUaTK8N'>https://openreview.net/forum?id=4SoTUaTK8N</a></p>
<p><b>Keywords</b>: graph neural networks, structure preserving machine learning, neural ordinary differential equations, hamiltonian dynamics, metriplectic dynamics
</p><p><b>Compressor summary</b>: This paper introduces new graph neural networks (GNNs) based on physics principles that conserve energy or generate dissipation, and explains their performance in relation to reversibility and irreversibility.</p><hr><h3>Reference-Based POMDPs</h3>
<p>Edward Kim, Yohan Karunanayake, Hanna Kurniawati</p>
<p><a href='https://openreview.net/forum?id=4Sn2vUs0zA'>https://openreview.net/forum?id=4Sn2vUs0zA</a></p>
<p><b>Keywords</b>: POMDP, planning under uncertainty, long horizon
</p><p><b>Compressor summary</b>: The paper proposes a modified POMDP problem called Reference-Based POMDP, which balances expected reward and similarity to a reference policy, and shows that it improves long-horizon navigation problems compared to the standard POMDP approach.</p><hr><h3>Cal-DETR: Calibrated Detection Transformer</h3>
<p>Muhammad Akhtar Munir, Salman Khan, Muhammad Haris Khan, Mohsen Ali, Fahad Khan</p>
<p><a href='https://openreview.net/forum?id=4SkPTD6XNP'>https://openreview.net/forum?id=4SkPTD6XNP</a></p>
<p><b>Keywords</b>: Model Calibration, Object Detection, Detection Transformers, Uncertainty
</p><p><b>Compressor summary</b>: The paragraph discusses a new mechanism, Cal-DETR, for calibrating uncertainty in DNN-based object detectors using train-time approaches, which improves their performance and reliability in safety-critical applications.</p><hr><h3>Binary Classification with Confidence Difference</h3>
<p>Wei Wang, Lei Feng, Yuchen Jiang, Gang Niu, Min-Ling Zhang, Masashi Sugiyama</p>
<p><a href='https://openreview.net/forum?id=4RoD1o7yq6'>https://openreview.net/forum?id=4RoD1o7yq6</a></p>
<p><b>Keywords</b>: Weakly supervised learning, binary classification, unbiased risk estimator
</p><p><b>Compressor summary</b>: The paper proposes a new weakly supervised binary classification method called ConfDiff classification, which uses confidence differences between unlabeled data pairs instead of pointwise labeling confidence, and shows that it improves model performance and mitigates overfitting issues.</p><hr><h3>CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography</h3>
<p>Jiwen Yu, Xuanyu Zhang, Youmin Xu, Jian Zhang</p>
<p><a href='https://openreview.net/forum?id=4R2Y5B12jm'>https://openreview.net/forum?id=4R2Y5B12jm</a></p>
<p><b>Keywords</b>: Diffusion models, image steganography, Stable Diffusion, coverless steganography
</p><p><b>Compressor summary</b>: The authors propose a new image steganography method, CRoSS, that uses diffusion models to improve security and natural robustness without additional training.</p><hr><h3>High Precision Causal Model Evaluation with Conditional Randomization</h3>
<p>Chao Ma, Cheng Zhang</p>
<p><a href='https://openreview.net/forum?id=4PkBhz18in'>https://openreview.net/forum?id=4PkBhz18in</a></p>
<p><b>Keywords</b>: causality, causal inference, causal model evaluation
</p><p><b>Compressor summary</b>: The pairs estimator is a new method to improve causal model evaluation in real-world experiments by reducing variance and achieving near-RCT performance.</p><hr><h3>Generalization in the Face of Adaptivity: A Bayesian Perspective</h3>
<p>Moshe Shenfeld, Katrina Ligett</p>
<p><a href='https://openreview.net/forum?id=4L9g1jUDtO'>https://openreview.net/forum?id=4L9g1jUDtO</a></p>
<p><b>Keywords</b>: Differential Privacy, Adaptive Data Analysis
</p><p><b>Compressor summary</b>: The paper proposes a novel approach to prevent overfitting in adaptive data analysis by using noise-addition algorithms and introducing a new data-dependent stability notion.</p><hr><h3>Object-centric Learning with Cyclic Walks between Parts and Whole</h3>
<p>Ziyu Wang, Mike Zheng Shou, Mengmi Zhang</p>
<p><a href='https://openreview.net/forum?id=4L3RfWnDzL'>https://openreview.net/forum?id=4L3RfWnDzL</a></p>
<p><b>Keywords</b>: object representation learning, slot attention, object-centric, contrastive random walks
</p><p><b>Compressor summary</b>: The authors propose a method to learn object-centric representations from scenes using cyclic walks between perceptual features and object entities, which enables reasoning abilities in both humans and machines.</p><hr><h3>FIRAL: An Active Learning Algorithm for Multinomial Logistic Regression</h3>
<p>Youguang Chen, George Biros</p>
<p><a href='https://openreview.net/forum?id=4L2OlXhiTM'>https://openreview.net/forum?id=4L2OlXhiTM</a></p>
<p><b>Keywords</b>: statistical learning, active learning, logistic regression, regret minimization
</p><p><b>Compressor summary</b>: The paper studies how to use pool-based active learning with multinomial logistic regression for multiclass classification, proves theoretical bounds on excess risk using Fisher Information Ratio (FIR), proposes a regret minimization algorithm based on FIR, and shows experimental results that beat other methods on synthetic and real datasets.</p><hr><h3>Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms</h3>
<p>Peiyao Xiao, Hao Ban, Kaiyi Ji</p>
<p><a href='https://openreview.net/forum?id=4Ks8RPcXd9'>https://openreview.net/forum?id=4Ks8RPcXd9</a></p>
<p><b>Keywords</b>: Multi-objective optimization, multi-task leaning, stochastic algorithms, convergence and complexity, Pareto stationarity
</p><p><b>Compressor summary</b>: The paper proposes SDMGrad, a stochastic optimization method that improves sample complexity and convergence for multi-objective machine learning problems with multiple criteria and tasks.</p><hr><h3>When Does Confidence-Based Cascade Deferral Suffice?</h3>
<p>Wittawat Jitkrittum, Neha Gupta, Aditya Krishna Menon, Harikrishna Narasimhan, Ankit Singh Rawat, Sanjiv Kumar</p>
<p><a href='https://openreview.net/forum?id=4KZhZJSPYU'>https://openreview.net/forum?id=4KZhZJSPYU</a></p>
<p><b>Keywords</b>: cascades, deferral rules, adaptive computation, model confidence
</p><p><b>Compressor summary</b>: The paper investigates when confidence-based deferral in cascade classifiers may fail and proposes alternative deferral strategies for such scenarios.</p><hr><h3>On the Variance, Admissibility, and Stability of Empirical Risk Minimization</h3>
<p>Gil Kur, Eli Putterman, Alexander Rakhlin</p>
<p><a href='https://openreview.net/forum?id=4KV2xLeqPN'>https://openreview.net/forum?id=4KV2xLeqPN</a></p>
<p><b>Keywords</b>: empirical risk minimization, bias-variance decomposition, admissibility
</p><p><b>Compressor summary</b>: The paper proves that ERM's suboptimality is due to its bias, not variance, and provides proofs and extensions for various settings and models, as well as stability results and a discussion on the irregular loss landscape of ERM.</p><hr><h3>Effectively Learning Initiation Sets in Hierarchical Reinforcement Learning</h3>
<p>Akhil Bagaria, Ben M Abbatematteo, Omer Gottesman, Matt Corsaro, Sreehari Rammohan, George Konidaris</p>
<p><a href='https://openreview.net/forum?id=4JCVw8oMlf'>https://openreview.net/forum?id=4JCVw8oMlf</a></p>
<p><b>Keywords</b>: hierarchical reinforcment learning
</p><p><b>Compressor summary</b>: The paragraph discusses the challenges of learning options in hierarchical reinforcement learning, especially the issue of learning initiation sets, and proposes a new method that uses off-policy value estimation and classification to improve performance on various tasks.</p><hr><h3>Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions</h3>
<p>Samantha Chen, Yusu Wang</p>
<p><a href='https://openreview.net/forum?id=4JB42GBxGs'>https://openreview.net/forum?id=4JB42GBxGs</a></p>
<p><b>Keywords</b>: neural networks, Wasserstein distance, universal approximation, optimal transport
</p><p><b>Compressor summary</b>: The paper proposes a neural network architecture that approximates Wasserstein distance between point sets with low complexity, independent of the input size, and shows its superior performance over other models.</p><hr><h3>Learning to Receive Help: Intervention-Aware Concept Embedding Models</h3>
<p>Mateo Espinosa Zarlenga, Katherine M. Collins, Krishnamurthy Dj Dvijotham, Adrian Weller, Zohreh Shams, Mateja Jamnik</p>
<p><a href='https://openreview.net/forum?id=4ImZxqmT1K'>https://openreview.net/forum?id=4ImZxqmT1K</a></p>
<p><b>Keywords</b>: Explainable Artificial Intelligence, Concept Bottleneck Models, Concept-based Explainability, Interpretability, XAI, Concept Interventions
</p><p><b>Compressor summary</b>: IntCEMs are a new type of neural architecture that allows users to improve the model's performance by correcting mispredicted concepts during training, based on an learned intervention policy.</p><hr><h3>Removing Hidden Confounding in Recommendation: A Unified Multi-Task Learning Approach</h3>
<p>Haoxuan Li, Kunhan Wu, Chunyuan Zheng, Yanghao Xiao, Hao Wang, Zhi Geng, Fuli Feng, Xiangnan He, Peng Wu</p>
<p><a href='https://openreview.net/forum?id=4IWJZjbRFj'>https://openreview.net/forum?id=4IWJZjbRFj</a></p>
<p><b>Keywords</b>: Debiased recommender system, Multi-task learning, Causal inference
</p><p><b>Compressor summary</b>: The paper analyzes the limitations of existing debiasing methods for recommender systems and proposes a new multi-task learning approach to address hidden confounding.</p><hr><h3>Block-Coordinate Methods and Restarting for Solving Extensive-Form Games</h3>
<p>Darshan Chakrabarti, Jelena Diakonikolas, Christian Kroer</p>
<p><a href='https://openreview.net/forum?id=4AmJVaJ78I'>https://openreview.net/forum?id=4AmJVaJ78I</a></p>
<p><b>Keywords</b>: extensive-form games, first-order methods, coordinate descent
</p><p><b>Compressor summary</b>: The paper presents a new cyclic coordinate-descent method for solving sequential games that exploits the recursive structure of dilated regularizers, achieving fast convergence and sometimes outperforming state-of-the-art algorithms.</p><hr><h3>On the Exploitability of Instruction Tuning</h3>
<p>Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, Tom Goldstein</p>
<p><a href='https://openreview.net/forum?id=4AQ4Fnemox'>https://openreview.net/forum?id=4AQ4Fnemox</a></p>
<p><b>Keywords</b>: Trustworthy machine learning, Large language models, Supervised fine-tuning, instruction tuning
</p><p><b>Compressor summary</b>: The paper introduces AutoPoison, a tool that can change the behavior of large language models by injecting specific examples into their training data.</p><hr><h3>Direct Diffusion Bridge using Data Consistency for Inverse Problems</h3>
<p>Hyungjin Chung, Jeongsol Kim, Jong Chul Ye</p>
<p><a href='https://openreview.net/forum?id=497CevPdOg'>https://openreview.net/forum?id=497CevPdOg</a></p>
<p><b>Keywords</b>: Diffusion models, Inverse problems, Diffusion bridge
</p><p><b>Compressor summary</b>: The paper proposes a new method called data Consistent DDB that improves the performance of Direct Diffusion Bridges for solving inverse problems by ensuring data consistency and is open-sourced.</p><hr><h3>Lightweight Vision Transformer with Bidirectional Interaction</h3>
<p>Qihang Fan, Huaibo Huang, Xiaoqiang Zhou, Ran He</p>
<p><a href='https://openreview.net/forum?id=492Hfmgejy'>https://openreview.net/forum?id=492Hfmgejy</a></p>
<p><b>Keywords</b>: Vision Transformer, Lightweight Vision Backbone, Convolution Neural Network
</p><p><b>Compressor summary</b>: The paper proposes FASA, a mechanism for vision transformers that adaptively extracts local and global information and their bidirectional interaction, leading to improved performance and efficiency in multiple vision tasks.</p><hr><h3>Zeroth-Order Methods for Nondifferentiable, Nonconvex, and Hierarchical Federated Optimization</h3>
<p>Yuyang Qiu, Uday Shanbhag, Farzad Yousefian</p>
<p><a href='https://openreview.net/forum?id=46x3zvYCyQ'>https://openreview.net/forum?id=46x3zvYCyQ</a></p>
<p><b>Keywords</b>: Federated Learning, Nonsmooth Optimization, Nonconvex Optimization, Bilevel Optimization
</p><p><b>Compressor summary</b>: The paper proposes a framework for communication-efficient decentralized training that handles nonconvex optimization, bilevel optimization, and minimax problems in federated learning using randomized smoothing and implicit zeroth-order methods.</p><hr><h3>Unsupervised Protein-Ligand Binding Energy Prediction via Neural Euler's Rotation Equation</h3>
<p>Wengong Jin, Siranush Sarkizova, Xun Chen, Nir Hacohen, Caroline Uhler</p>
<p><a href='https://openreview.net/forum?id=46gYakmj4e'>https://openreview.net/forum?id=46gYakmj4e</a></p>
<p><b>Keywords</b>: Energy-based Models, Denoising Score Matching, Equivariant Neural Networks
</p><p><b>Compressor summary</b>: The paper proposes a new generative modeling approach for predicting binding affinity of antibodies, using a rotation prediction network that models forces between protein and ligand atoms.</p><hr><h3>Accelerated On-Device Forward Neural Network Training with Module-Wise Descending Asynchronism</h3>
<p>Xiaohan Zhao, Hualin Zhang, Zhouyuan Huo, Bin Gu</p>
<p><a href='https://openreview.net/forum?id=45RBLZBJid'>https://openreview.net/forum?id=45RBLZBJid</a></p>
<p><b>Keywords</b>: asynchronous algorithm, one-device learning, forward gradient descent, directional derivative, forward algorithms
</p><p><b>Compressor summary</b>: The paper introduces AsyncFGD, an asynchronous framework for on-device learning that reduces memory usage and improves hardware efficiency by decoupling dependencies and utilizing stale parameters.</p><hr><h3>A Unified Framework for U-Net Design and Analysis</h3>
<p>Christopher Williams, Fabian Falck, George Deligiannidis, Christopher C. Holmes, Arnaud Doucet, Saifuddin Syed</p>
<p><a href='https://openreview.net/forum?id=43ruO2fMjq'>https://openreview.net/forum?id=43ruO2fMjq</a></p>
<p><b>Keywords</b>: U-Net, ResNet, Multi-ResNet, Generalised U-Net, Wavelets, Diffusion models, Generative modelling, PDE Modelling, Image Segmentation
</p><p><b>Compressor summary</b>: The paper presents a framework for analysing and designing general U-Net architectures, proposes Multi-ResNets with a simplified encoder and novel U-Net designs that incorporate function constraints and data geometry, and shows their improved performance in various tasks.</p><hr><h3>The Pick-to-Learn Algorithm: Empowering Compression for Tight Generalization Bounds and Improved Post-training Performance</h3>
<p>Dario Paccagnan, Marco Campi, Simone Garatti</p>
<p><a href='https://openreview.net/forum?id=40L3viVWQN'>https://openreview.net/forum?id=40L3viVWQN</a></p>
<p><b>Keywords</b>: Statistical learning theory, Compression theory, Generalization bounds
</p><p><b>Compressor summary</b>: The authors propose Pick-to-Learn, a meta-algorithm that improves compression properties and generalization bounds for learning algorithms on MNIST and a synthetic regression problem.</p><hr><h3>Uncoupled and Convergent Learning in Two-Player Zero-Sum Markov Games with Bandit Feedback</h3>
<p>Yang Cai, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng</p>
<p><a href='https://openreview.net/forum?id=3xSwxlB0fd'>https://openreview.net/forum?id=3xSwxlB0fd</a></p>
<p><b>Keywords</b>: two-player zero-sum Markov game, last-iterate convergence, path convergence, learning in games
</p><p><b>Compressor summary</b>: The paper proposes an efficient and rational algorithm for learning in two-player zero-sum Markov games with various feedback types and convergence rates, outperforming previous methods in terms of assumptions and synchronization.</p><hr><h3>Estimating Noise Correlations Across Continuous Conditions With Wishart Processes</h3>
<p>Amin Nejatbakhsh, Isabel Garon, Alex H Williams</p>
<p><a href='https://openreview.net/forum?id=3ucmcMzCXD'>https://openreview.net/forum?id=3ucmcMzCXD</a></p>
<p><b>Keywords</b>: Noise Correlations, Wishart Process, Variational Inference
</p><p><b>Compressor summary</b>: The authors propose a new method to estimate the covariance structure of neural noise across naturalistic behaviors using Wishart process models, which can provide information on signal fidelity and noise correlations in unseen conditions.</p><hr><h3>The Quantization Model of Neural Scaling</h3>
<p>Eric J Michaud, Ziming Liu, Uzay Girit, Max Tegmark</p>
<p><a href='https://openreview.net/forum?id=3tbTw2ga8K'>https://openreview.net/forum?id=3tbTw2ga8K</a></p>
<p><b>Keywords</b>: scaling laws, emergence, language models, science of deep learning
</p><p><b>Compressor summary</b>: The Quantization Model explains neural scaling laws by describing how network abilities are divided into discrete chunks and learned in order of decreasing use frequency.</p><hr><h3>Projection Regret: Reducing Background Bias for Novelty Detection via Diffusion Models</h3>
<p>Sungik Choi, Hankook Lee, Honglak Lee, Moontae Lee</p>
<p><a href='https://openreview.net/forum?id=3qHlPqzjM1'>https://openreview.net/forum?id=3qHlPqzjM1</a></p>
<p><b>Keywords</b>: Novelty detection, out-of-distribution detection, consistency models, diffusion models, score-based generative models
</p><p><b>Compressor summary</b>: The paper proposes Projection Regret (PR), a novelty detection method for diffusion models that mitigates background bias and outperforms existing methods.</p><hr><h3>ReHLine: Regularized Composite ReLU-ReHU Loss Minimization with Linear Computation and Linear Convergence</h3>
<p>Ben Dai, Yixuan Qiu</p>
<p><a href='https://openreview.net/forum?id=3pEBW2UPAD'>https://openreview.net/forum?id=3pEBW2UPAD</a></p>
<p><b>Keywords</b>: coordinate descent, linear convergence, primal-dual methods, empirical risk minimization, linear constraints, quantile regression
</p><p><b>Compressor summary</b>: ReHLine is a novel algorithm for minimizing regularized ERMs with convex piecewise linear-quadratic loss functions and optional linear constraints, which handles diverse problems and has provable linear convergence rate and linear computational complexity.</p><hr><h3>DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models</h3>
<p>Tao Yang, Yuwang Wang, Yan Lu, Nanning Zheng</p>
<p><a href='https://openreview.net/forum?id=3ofe0lpwQP'>https://openreview.net/forum?id=3ofe0lpwQP</a></p>
<p><b>Keywords</b>: Diffusion Probabilistic Model, Disentangled representation
</p><p><b>Compressor summary</b>: The authors propose a new task called disentanglement of diffusion probabilistic models (DPMs), which aims to discover and represent inherent factors behind observations, and introduce an unsupervised approach named DisDiff that achieves this task for the first time.</p><hr><h3>RECESS Vaccine for Federated Learning: Proactive Defense Against Model Poisoning Attacks</h3>
<p>Haonan Yan, Wenjing Zhang, Qian Chen, Xiaoguang Li, Wenhai Sun, HUI LI, Xiaodong Lin</p>
<p><a href='https://openreview.net/forum?id=3n8PNUdvSg'>https://openreview.net/forum?id=3n8PNUdvSg</a></p>
<p><b>Keywords</b>: Federated Learning, Model Poisoning Attacks, Proactive Detection, Robust Aggregation, Benign Outlier Identification
</p><p><b>Compressor summary</b>: RECESS is a novel defense for federated learning against model poisoning attacks that uses trust scoring based aggregation and proactive querying to detect and protect against malicious clients.</p><hr><h3>Beyond probability partitions: Calibrating neural networks with semantic aware grouping</h3>
<p>Jia-Qi Yang, De-Chuan Zhan, Le Gan</p>
<p><a href='https://openreview.net/forum?id=3kitbpEZZO'>https://openreview.net/forum?id=3kitbpEZZO</a></p>
<p><b>Keywords</b>: Uncertainty calibration, Deep neural networks
</p><p><b>Compressor summary</b>: The paper introduces Partitioned Calibration Error (PCE) as a generalized definition of calibration error in deep networks, showing that accurate models should be calibrated across any data partition, and proposes a method to learn semantic-aware partitioning functions for improved calibration.</p><hr><h3>PERFOGRAPH: A Numerical Aware Program Graph Representation for Performance Optimization and Program Analysis</h3>
<p>Ali TehraniJamsaz, Quazi Ishtiaque Mahmud, Le Chen, Nesreen K. Ahmed, Ali Jannesari</p>
<p><a href='https://openreview.net/forum?id=3jAsfo8x8k'>https://openreview.net/forum?id=3jAsfo8x8k</a></p>
<p><b>Keywords</b>: program representation, graph representation, program analysis, graph neural networks, performance optimization
</p><p><b>Compressor summary</b>: PERFOGRAPH is a novel graph-based program representation that captures numerical information and aggregate data structure, improving machine learning methods' ability to reason about programs and achieving state-of-the-art results in various applications.</p><hr><h3>Learning Interpretable Low-dimensional Representation via Physical Symmetry</h3>
<p>Xuanjie Liu, Daniel Chin, Yichen Huang, Gus Xia</p>
<p><a href='https://openreview.net/forum?id=3iSj4l8ZGT'>https://openreview.net/forum?id=3iSj4l8ZGT</a></p>
<p><b>Keywords</b>: Physics Symmetry, Time series data, Self-supervised Learning, Representation Augmentation
</p><p><b>Compressor summary</b>: The study proposes using physical symmetry as a self-consistency constraint for learning interpretable music representations, leading to a linear pitch factor and representation augmentation.</p><hr><h3>Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes</h3>
<p>Ali Younis, Erik B. Sudderth</p>
<p><a href='https://openreview.net/forum?id=3gxiOEf2D6'>https://openreview.net/forum?id=3gxiOEf2D6</a></p>
<p><b>Keywords</b>: particle, filter, mixture, belief propagation, nonparametric, deep learning, generative, discriminative, graphical model, multiple modes, mutli-modal
</p><p><b>Compressor summary</b>: The paragraph describes a novel particle filter method that uses deep neural networks to learn nonparametric representations of uncertainty in latent object states from arbitrary observations, improving tracking and localization performance.</p><hr><h3>QuantSR: Accurate Low-bit Quantization for Efficient Image Super-Resolution</h3>
<p>Haotong Qin, Yulun Zhang, Yifu Ding, Yifan liu, Xianglong Liu, Martin Danelljan, Fisher Yu</p>
<p><a href='https://openreview.net/forum?id=3gamyee9Yh'>https://openreview.net/forum?id=3gamyee9Yh</a></p>
<p><b>Keywords</b>: Super Resolution, Model Quantization, Deep Learning
</p><p><b>Compressor summary</b>: The paper proposes a novel quantized image super-resolution network called QuantSR that achieves accurate and efficient processing under low-bit quantization using the Redistribution-driven Learnable Quantizer (RLQ) and Depth-dynamic Quantized Architecture (DQA).</p><hr><h3>Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals</h3>
<p>Tam Minh Nguyen, Tan Minh Nguyen, Richard Baraniuk</p>
<p><a href='https://openreview.net/forum?id=3fd776zKmo'>https://openreview.net/forum?id=3fd776zKmo</a></p>
<p><b>Keywords</b>: transformers, self-attention, total variation, nonlocal functionals, over-smoothing
</p><p><b>Compressor summary</b>: NeuTRENO is a new type of transformer model that reduces token uniformity by penalizing the difference between input and output tokens in self-attention layers.</p><hr><h3>SoTTA: Robust Test-Time Adaptation on Noisy Data Streams</h3>
<p>Taesik Gong, Yewon Kim, Taeckyung Lee, Sorn Chottananurak, Sung-Ju Lee</p>
<p><a href='https://openreview.net/forum?id=3bdXag2rUd'>https://openreview.net/forum?id=3bdXag2rUd</a></p>
<p><b>Keywords</b>: test-time adaptation, domain adaptation, deep learning, machine learning
</p><p><b>Compressor summary</b>: The paper introduces SoTTA, a novel test-time adaptation algorithm that is robust to noisy data by using high-confidence uniform-class sampling and entropy-sharpness minimization.</p><hr><h3>A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks</h3>
<p>Sara Babakniya, Zalan Fabian, Chaoyang He, Mahdi Soltanolkotabi, Salman Avestimehr</p>
<p><a href='https://openreview.net/forum?id=3b9sqxCW1x'>https://openreview.net/forum?id=3b9sqxCW1x</a></p>
<p><b>Keywords</b>: federated learning, class incremental learning, generative models, data-free, continual learning
</p><p><b>Compressor summary</b>: The paper proposes a framework for federated class incremental learning that uses generative models to synthesize data from past distributions and train the model without requesting data or storing old information from clients, addressing privacy and resource limitations in federated learning.</p><hr><h3>Volume Feature Rendering for Fast Neural Radiance Field Reconstruction</h3>
<p>Kang Han, Wei Xiang, Lu Yu</p>
<p><a href='https://openreview.net/forum?id=3aVZhMfsyz'>https://openreview.net/forum?id=3aVZhMfsyz</a></p>
<p><b>Keywords</b>: neural rendering, volume rendering, view synthesis, 3D reconstruction
</p><p><b>Compressor summary</b>: NeRFs use neural networks to generate realistic images from multiple views, but this can be slow due to many color network evaluations. This paper proposes a new method called VFR that combines queries into one vector and uses a larger color network for better quality with less training time.</p><hr><h3>Deep learning with kernels through RKHM and the Perron-Frobenius operator</h3>
<p>Yuka Hashimoto, Masahiro Ikeda, Hachem Kadri</p>
<p><a href='https://openreview.net/forum?id=3ZrGmenVM2'>https://openreview.net/forum?id=3ZrGmenVM2</a></p>
<p><b>Keywords</b>: kernel method. generalization bound. C*-algebra. Perron-Frobenius operator and Koopman operator.
</p><p><b>Compressor summary</b>: The paper introduces deep RKHM, a deep learning framework for kernel methods using reproducing kernel Hilbert module and Perron-Frobenius operator, with a new generalization bound and insights on benign overfitting.</p><hr><h3>ReTR: Modeling Rendering Via Transformer for Generalizable Neural Surface Reconstruction</h3>
<p>Yixun Liang, Hao He, Ying-Cong Chen</p>
<p><a href='https://openreview.net/forum?id=3ZICE99e6n'>https://openreview.net/forum?id=3ZICE99e6n</a></p>
<p><b>Keywords</b>: 3D vision, 3D reconstruction, Generalizable Neural Surface Reconstruction
</p><p><b>Compressor summary</b>: The paper introduces ReTR, a novel framework that uses transformer architecture to improve neural surface reconstruction by redesigning the rendering process with a learnable meta-ray token and cross-attention mechanism, resulting in more accurate and confident surface assessment.</p><hr><h3>Learning Large Graph Property Prediction via Graph Segment Training</h3>
<p>Kaidi Cao, Phitchaya Mangpo Phothilimthana, Sami Abu-El-Haija, Dustin Zelle, Yanqi Zhou, Charith Mendis, Jure Leskovec, Bryan Perozzi</p>
<p><a href='https://openreview.net/forum?id=3YDukx2cpr'>https://openreview.net/forum?id=3YDukx2cpr</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Graph Property Prediction
</p><p><b>Compressor summary</b>: The Graph Segment Training framework uses a divide-and-conquer approach to learn large graph property prediction with constant memory, and introduces techniques to handle input distribution shift and stale embeddings.</p><hr><h3>A General Framework for Equivariant Neural Networks on Reductive Lie Groups</h3>
<p>Ilyes Batatia, Mario Geiger, Jose M Munoz, Tess Smidt, Lior Silberman, Christoph Ortner</p>
<p><a href='https://openreview.net/forum?id=3XStpETaO8'>https://openreview.net/forum?id=3XStpETaO8</a></p>
<p><b>Keywords</b>: equivariance, point clouds, machine learning, particle physics
</p><p><b>Compressor summary</b>: The paper introduces a new neural network architecture that respects symmetries in various scientific fields, such as high energy physics and computer vision, using reductive Lie groups.</p><hr><h3>Birth of a Transformer: A Memory Viewpoint</h3>
<p>Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, Leon Bottou</p>
<p><a href='https://openreview.net/forum?id=3X2EbBLNsk'>https://openreview.net/forum?id=3X2EbBLNsk</a></p>
<p><b>Keywords</b>: transformers, language models, deep learning theory, interpretability
</p><p><b>Compressor summary</b>: The paragraph discusses how transformers learn from global and context-specific information and the importance of understanding their internal mechanisms for reliability.</p><hr><h3>Learning via Wasserstein-Based High Probability Generalisation Bounds</h3>
<p>Paul Viallard, Maxime Haddouche, Umut Simsekli, Benjamin Guedj</p>
<p><a href='https://openreview.net/forum?id=3Wrolscjbx'>https://openreview.net/forum?id=3Wrolscjbx</a></p>
<p><b>Keywords</b>: Wasserstein, PAC-Bayes, Generalisation Bound, Algorithm
</p><p><b>Compressor summary</b>: The paper proposes new generalization bounds for PAC-Bayesian learning using Wasserstein distance, which are stronger than previous ones and can be used in structural risk minimization.</p><hr><h3>Why Does Sharpness-Aware Minimization Generalize Better Than SGD?</h3>
<p>Zixiang Chen, Junkai Zhang, Yiwen Kou, Xiangning Chen, Cho-Jui Hsieh, Quanquan Gu</p>
<p><a href='https://openreview.net/forum?id=3WAnGWLpSQ'>https://openreview.net/forum?id=3WAnGWLpSQ</a></p>
<p><b>Keywords</b>: Sharpness Aware Algorithm, Deep Learning Theory
</p><p><b>Compressor summary</b>: The paper explores why Sharpness-Aware Minimization (SAM) works better than Stochastic Gradient Descent (SGD) for certain neural network settings, especially in preventing overfitting and noise learning.</p><hr><h3>DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization</h3>
<p>Hua Wang, Sheng Gao, Huanyu Zhang, Weijie J Su, Milan Shen</p>
<p><a href='https://openreview.net/forum?id=3Py8A1j5N3'>https://openreview.net/forum?id=3Py8A1j5N3</a></p>
<p><b>Keywords</b>: Differential Privacy, Hyperparameter Tuning, Deep Learning
</p><p><b>Compressor summary</b>: DP-HyPO is a novel framework for private and adaptive hyperparameter optimization that bridges the gap between private and non-private methods, providing a thorough differential privacy analysis and showing its effectiveness on various real-world datasets.</p><hr><h3>From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces</h3>
<p>Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, Kristina Toutanova</p>
<p><a href='https://openreview.net/forum?id=3PjCt4kmRx'>https://openreview.net/forum?id=3PjCt4kmRx</a></p>
<p><b>Keywords</b>: instruction following, web tasks, user interface tasks, vision and language, representation learning, reinforcement learning, imitation learning, tree search, language grounding, web agents, computer control
</p><p><b>Compressor summary</b>: The paper proposes pixel-based agents that use keyboard and mouse actions to interact with GUIs and shows they can perform better than humans on GUI instruction following tasks.</p><hr><h3>Undirected Probabilistic Model for Tensor Decomposition</h3>
<p>Zerui Tao, Toshihisa Tanaka, Qibin Zhao</p>
<p><a href='https://openreview.net/forum?id=3NWWgB2SuF'>https://openreview.net/forum?id=3NWWgB2SuF</a></p>
<p><b>Keywords</b>: Tensor decomposition, tensor completion, probabilistic methods
</p><p><b>Compressor summary</b>: The paper introduces a flexible tensor decomposition framework using energy-based models and neural networks to learn underlying structures and distributions without prior assumptions, and proposes a variational objective for efficient training.</p><hr><h3>CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society</h3>
<p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem</p>
<p><a href='https://openreview.net/forum?id=3IyL2XWDkG'>https://openreview.net/forum?id=3IyL2XWDkG</a></p>
<p><b>Keywords</b>: Communicative Agents, Large Language Models, AI Society, Role-Playing, Society of Mind
</p><p><b>Compressor summary</b>: This paper introduces role-playing, a framework for enabling autonomous cooperation among chat-based agents using inception prompting, and demonstrates its potential for studying multi-agent systems.</p><hr><h3>DFRD: Data-Free Robustness Distillation for Heterogeneous Federated Learning</h3>
<p>Kangyang Luo, Shuai Wang, Yexuan Fu, Xiang Li, Yunshi Lan, Ming Gao</p>
<p><a href='https://openreview.net/forum?id=3H9QH1v6U9'>https://openreview.net/forum?id=3H9QH1v6U9</a></p>
<p><b>Keywords</b>: Federated Learning, Data Heterogeneity, Model Heterogeneity, Data-Free Distillation
</p><p><b>Compressor summary</b>: DFRD is a new federated learning method that uses data-free knowledge distillation and a conditional generator to learn a robust global model from privately decentralized clients.</p><hr><h3>Post Hoc Explanations of Language Models Can Improve Language Models</h3>
<p>Satyapriya Krishna, Jiaqi Ma, Dylan Z Slack, Asma Ghandeharioun, Sameer Singh, Himabindu Lakkaraju</p>
<p><a href='https://openreview.net/forum?id=3H37XciUEv'>https://openreview.net/forum?id=3H37XciUEv</a></p>
<p><b>Keywords</b>: Machine Learning Explainability, Large Language Models
</p><p><b>Compressor summary</b>: AMPLIFY is a framework that uses post hoc explanations to generate rationales for large language models, improving their performance on complex tasks without human involvement.</p><hr><h3>The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning</h3>
<p>Artyom Gadetsky, Maria Brbic</p>
<p><a href='https://openreview.net/forum?id=3GpIeVYw8X'>https://openreview.net/forum?id=3GpIeVYw8X</a></p>
<p><b>Keywords</b>: unsupervised learning, deep learning, generalization, self-supervised learning, clustering
</p><p><b>Compressor summary</b>: HUME is a simple framework that can infer human labels on a dataset without supervision, using linear classifiers on top of pretrained representations, and achieves state-of-the-art performance on several image classification benchmarks.</p><hr><h3>Addressing Negative Transfer in Diffusion Models</h3>
<p>Hyojun Go, Jinyoung Kim, Yunsung Lee, Seunghyun Lee, Shinhyeok Oh, Hyeongdon Moon, Seungtaek Choi</p>
<p><a href='https://openreview.net/forum?id=3G2ec833mW'>https://openreview.net/forum?id=3G2ec833mW</a></p>
<p><b>Keywords</b>: Diffusion Models, Multi-Task Learning
</p><p><b>Compressor summary</b>: The paper analyzes diffusion training in multi-task learning, identifies challenges like negative transfer and task affinity, and proposes interval clustering to improve it.</p><hr><h3>Neural Frailty Machine: Beyond proportional hazard assumption in neural survival regressions</h3>
<p>Ruofan Wu, Jiawei Qiao, Mingzhe Wu, Wen Yu, Ming Zheng, Tengfei LIU, Tianyi Zhang, Weiqiang Wang</p>
<p><a href='https://openreview.net/forum?id=3Fc9gnR0fa'>https://openreview.net/forum?id=3Fc9gnR0fa</a></p>
<p><b>Keywords</b>: Survival Analysis, Theory, Semiparametric statistics
</p><p><b>Compressor summary</b>: The paper introduces neural frailty machine (NFM), a neural model for survival regressions that extends the proportional hazard assumption and uses multiplicative frailty, with theoretical guarantees and empirical results showing its effectiveness.</p><hr><h3>GLIME: General, Stable and Local LIME Explanation</h3>
<p>Zeren Tan, Yang Tian, Jian Li</p>
<p><a href='https://openreview.net/forum?id=3FJaFElIVN'>https://openreview.net/forum?id=3FJaFElIVN</a></p>
<p><b>Keywords</b>: Explanation, LIME, Stability, Local fidelity, Interpretability
</p><p><b>Compressor summary</b>: The paper proposes \textsc{Glime}, a framework that improves the stability and local fidelity of explanations for black-box machine learning models by modifying LIME's sampling approach and formulation.</p><hr><h3>Posthoc privacy guarantees for collaborative inference with modified Propose-Test-Release</h3>
<p>Abhishek Singh, Praneeth Vepakomma, Vivek Sharma, Ramesh Raskar</p>
<p><a href='https://openreview.net/forum?id=3DMDNwd7ND'>https://openreview.net/forum?id=3DMDNwd7ND</a></p>
<p><b>Keywords</b>: privacy, deep learning, neural networks, adversarial learning, reconstruction guarantees, collaborative inference, MLaaS
</p><p><b>Compressor summary</b>: The paper proposes a new framework to provide formal privacy guarantees for cloud-based machine learning inference using local sensitivity and extends an existing method for neural network queries.</p><hr><h3>HotBEV: Hardware-oriented Transformer-based Multi-View 3D Detector for BEV Perception</h3>
<p>Peiyan Dong, Zhenglun Kong, Xin Meng, Pinrui Yu, Yifan Gong, Geng Yuan, Hao Tang, Yanzhi Wang</p>
<p><a href='https://openreview.net/forum?id=3Cj67k38st'>https://openreview.net/forum?id=3Cj67k38st</a></p>
<p><b>Keywords</b>: Multi-view 3D detection, Hardware efficiency, Autonomous driving
</p><p><b>Compressor summary</b>: This paper proposes a latency-aware model design for bird's-eye-view detection in autonomous driving systems, using efficient building operators and a hardware-oriented backbone to achieve significant speedups while maintaining accuracy.</p><hr><h3>On the Last-iterate Convergence in Time-varying Zero-sum Games: Extra Gradient Succeeds where Optimism Fails</h3>
<p>Yi Feng, Hu Fu, Qun Hu, Ping Li, Ioannis Panageas, bo peng, Xiao Wang</p>
<p><a href='https://openreview.net/forum?id=3CJOaJugMG'>https://openreview.net/forum?id=3CJOaJugMG</a></p>
<p><b>Keywords</b>: zero sum game, time-varying game, optimistic gradient, extra gradient, momentum method
</p><p><b>Compressor summary</b>: The paper investigates how OGDA and EG perform in time-varying bilinear zero-sum games and finds that they have different last-iterate behaviors.</p><hr><h3>Schema-learning and rebinding as mechanisms of in-context learning and emergence</h3>
<p>Sivaramakrishnan Swaminathan, Antoine Dedieu, Rajkumar Vasudeva Raju, Murray Shanahan, Miguel Lazaro-Gredilla, Dileep George</p>
<p><a href='https://openreview.net/forum?id=3AreDQZ8eO'>https://openreview.net/forum?id=3AreDQZ8eO</a></p>
<p><b>Keywords</b>: mechanistic interpretability, in-context learning, emergence, large language models
</p><p><b>Compressor summary</b>: This paper proposes an interpretable alternative method for in-context learning using causal graphs and shows how it works at different levels of complexity.</p><hr><h3>Time-uniform confidence bands for the CDF under nonstationarity</h3>
<p>Paul Mineiro, Steven R Howard</p>
<p><a href='https://openreview.net/forum?id=39cFjnRpYm'>https://openreview.net/forum?id=39cFjnRpYm</a></p>
<p><b>Keywords</b>: off-policy evaluation, anytime-valid
</p><p><b>Compressor summary</b>: The paper proposes bounds on the distribution of a sequence of real-valued random variables and their importance-weighted extension for counterfactual distributions in randomized experiments.</p><hr><h3>Im-Promptu: In-Context Composition from Image Prompts</h3>
<p>Bhishma Dedhia, Michael Chang, Jake Snell, Thomas L. Griffiths, Niraj Jha</p>
<p><a href='https://openreview.net/forum?id=38o372YoYt'>https://openreview.net/forum?id=38o372YoYt</a></p>
<p><b>Keywords</b>: in-context learning, compositionality, generative models
</p><p><b>Compressor summary</b>: This paper explores how analogical reasoning can help in-context visual learners generalize to new tasks and domains, and introduces a meta-learning framework called Im-Promptu that trains agents with different levels of compositional granularity.</p><hr><h3>Fairness Aware Counterfactuals for Subgroups</h3>
<p>Loukas Kavouras, Konstantinos Tsopelas, Giorgos Giannopoulos, Dimitris Sacharidis, Eleni Psaroudaki, Nikolaos Theologitis, Dimitrios Rontogiannis, Dimitris Fotakis, Ioannis Emiris</p>
<p><a href='https://openreview.net/forum?id=38dQv3OwN3'>https://openreview.net/forum?id=38dQv3OwN3</a></p>
<p><b>Keywords</b>: subgroup fairness, recourse, counterfactual explanations
</p><p><b>Compressor summary</b>: FACTS is a framework to audit subgroup fairness using counterfactual explanations that consider individual and group-level difficulties in achieving desired outcomes while being efficient and explainable.</p><hr><h3>Explore to Generalize in Zero-Shot RL</h3>
<p>Ev Zisselman, Itai Lavie, Daniel Soudry, Aviv Tamar</p>
<p><a href='https://openreview.net/forum?id=37cADkATD0'>https://openreview.net/forum?id=37cADkATD0</a></p>
<p><b>Keywords</b>: Reinforcement Learning, Generalization, State Space Maximum Entropy Exploration
</p><p><b>Compressor summary</b>: ExpGen is a reinforcement learning algorithm that uses exploration to achieve zero-shot generalization on unseen tasks, outperforming previous invariance-based methods and achieving state-of-the-art results on ProcGen challenge domains.</p><hr><h3>Representational Strengths and Limitations of Transformers</h3>
<p>Clayton Sanford, Daniel Hsu, Matus Telgarsky</p>
<p><a href='https://openreview.net/forum?id=36DxONZ9bA'>https://openreview.net/forum?id=36DxONZ9bA</a></p>
<p><b>Keywords</b>: self-attention, approximation theory, communication complexity
</p><p><b>Compressor summary</b>: The text discusses the benefits and drawbacks of attention layers in transformers, focusing on their representation power and complexity in different scenarios, using proof techniques that highlight communication complexity and sparse averaging as a key attention task.</p><hr><h3>Equivariant Spatio-Temporal Attentive Graph Networks to Simulate Physical Dynamics</h3>
<p>Liming Wu, Zhichao Hou, Jirui Yuan, Yu Rong, Wenbing Huang</p>
<p><a href='https://openreview.net/forum?id=35nFSbEBks'>https://openreview.net/forum?id=35nFSbEBks</a></p>
<p><b>Keywords</b>: Equivariance, Spatio-Temporal GNNs, Physical Dynamics
</p><p><b>Compressor summary</b>: The paper introduces ESTAG, an equivariant spatio-temporal graph neural network that uses a novel Equivariant Discrete Fourier Transform and forward attention to simulate non-Markovian physical systems.</p><hr><h3>Idempotent Learned Image Compression with Right-Inverse</h3>
<p>Yanghao Li, Tongda Xu, Yan Wang, Jingjing Liu, Ya-Qin Zhang</p>
<p><a href='https://openreview.net/forum?id=35dOU92OJM'>https://openreview.net/forum?id=35dOU92OJM</a></p>
<p><b>Keywords</b>: learned image compression, idempotent compression, right-inverse
</p><p><b>Compressor summary</b>: The paper proposes a new idempotent image compression codec using blocked convolution and null-space enhancement, which achieves state-of-the-art performance and has less quality decay after re-compression.</p><hr><h3>Evolving Connectivity for Recurrent Spiking Neural Networks</h3>
<p>Guan Wang, Yuhao Sun, Sijie Cheng, Sen Song</p>
<p><a href='https://openreview.net/forum?id=30o4ARmfC3'>https://openreview.net/forum?id=30o4ARmfC3</a></p>
<p><b>Keywords</b>: neuromorphic computing, spiking neural networks, evolutionary algorithms, inference-only approach, hardware-friendly, robotic locomotion tasks
</p><p><b>Compressor summary</b>: The evolving connectivity (EC) framework trains recurrent spiking neural networks without gradients by optimizing connection probability distributions with natural evolution strategies, achieving comparable performance to deep neural networks and outperforming gradient-trained RSNNs on robotic locomotion tasks.</p><hr><h3>Mutual Information Regularized Offline Reinforcement Learning</h3>
<p>Xiao Ma, Bingyi Kang, Zhongwen Xu, Min Lin, Shuicheng YAN</p>
<p><a href='https://openreview.net/forum?id=2z8noau98f'>https://openreview.net/forum?id=2z8noau98f</a></p>
<p><b>Keywords</b>: Mutual Information, Offline Reinforcement Learning
</p><p><b>Compressor summary</b>: MISA is a novel framework for offline RL that maximizes mutual information between states and actions to improve policy evaluation and constrain the policy improvement direction.</p><hr><h3>A Diffusion-Model of Joint Interactive Navigation</h3>
<p>Matthew Niedoba, Jonathan Wilder Lavington, Yunpeng Liu, Vasileios Lioutas, Justice Sefas, Xiaoxuan Liang, Dylan Green, Setareh Dabiri, Berend Zwartsenberg, Adam Scibior, Frank Wood</p>
<p><a href='https://openreview.net/forum?id=2yXExAl0FW'>https://openreview.net/forum?id=2yXExAl0FW</a></p>
<p><b>Keywords</b>: Diffusion Models, Trajecotry Forecasting, Autonomous Vehicles, Motion Forecasting, Simulation
</p><p><b>Compressor summary</b>: The paper introduces DJINN, a diffusion method that generates realistic traffic scenarios for autonomous vehicle simulations using past, present, or future state observations.</p><hr><h3>Compressed Video Prompt Tuning</h3>
<p>Bing Li, Jiaxin Chen, Xiuguo Bao, Di Huang</p>
<p><a href='https://openreview.net/forum?id=2vADOf3K00'>https://openreview.net/forum?id=2vADOf3K00</a></p>
<p><b>Keywords</b>: Compressed video, Action Recognition, Prompt Tuning
</p><p><b>Compressor summary</b>: The paper introduces Compressed Video Prompt Tuning (CVPT), a novel approach to adapt pre-trained raw video models to compressed video understanding tasks, using conditional prompts and cross-modal complementary blocks to improve efficiency and accuracy.</p><hr><h3>Training on Foveated Images Improves Robustness to Adversarial Attacks</h3>
<p>Muhammad A Shah, Aqsa Kashaf, Bhiksha Raj</p>
<p><a href='https://openreview.net/forum?id=2tfG9QaFA7'>https://openreview.net/forum?id=2tfG9QaFA7</a></p>
<p><b>Keywords</b>: adversarial robustness, computer vision, biologically-inspired, retina, blurring
</p><p><b>Compressor summary</b>: The authors propose a method called RBlur that simulates peripheral vision blurring and color desaturation to improve the robustness of deep neural networks against various types of image corruptions, including adversarial attacks.</p><hr><h3>What Do Deep Saliency Models Learn about Visual Attention?</h3>
<p>Shi Chen, Ming Jiang, Qi Zhao</p>
<p><a href='https://openreview.net/forum?id=2rq4LwwjfE'>https://openreview.net/forum?id=2rq4LwwjfE</a></p>
<p><b>Keywords</b>: Saliency prediction, human attention, low-level vision
</p><p><b>Compressor summary</b>: The paper introduces a method to analyze how deep saliency models predict human visual attention by breaking down their implicit features into interpretable components related to semantics and applying it to different scenarios.</p><hr><h3>Double Auctions with Two-sided Bandit Feedback</h3>
<p>Soumya Basu, Abishek Sankararaman</p>
<p><a href='https://openreview.net/forum?id=2nTpPxJ5Bs'>https://openreview.net/forum?id=2nTpPxJ5Bs</a></p>
<p><b>Keywords</b>: Double Auction, Markets, Bandits, Regret
</p><p><b>Compressor summary</b>: The paper studies how buyers and sellers can efficiently learn their valuations and discover prices in Double Auction markets with bandit feedback, and shows that certain regrets are unachievable.</p><hr><h3>Density of States Prediction of Crystalline Materials via Prompt-guided Multi-Modal Transformer</h3>
<p>Namkyeong Lee, Heewoong Noh, Sungwon Kim, Dongmin Hyun, Gyoung S. Na, Chanyoung Park</p>
<p><a href='https://openreview.net/forum?id=2lWh1G1W1I'>https://openreview.net/forum?id=2lWh1G1W1I</a></p>
<p><b>Keywords</b>: ML4Materials, AI4Science, Graph Neural Networks
</p><p><b>Compressor summary</b>: The paper proposes a multi-modal transformer that predicts density of states (DOS) in crystalline materials by considering both energy levels and the relationships between atoms, and shows its effectiveness on two types of DOS.</p><hr><h3>Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization</h3>
<p>Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo Lee</p>
<p><a href='https://openreview.net/forum?id=2jUKhUrBxP'>https://openreview.net/forum?id=2jUKhUrBxP</a></p>
<p><b>Keywords</b>: Large Language Models, Parameter-Efficient Fine-Tuning, Neural Network Quantization
</p><p><b>Compressor summary</b>: PEQA is a method that combines parameter-efficient fine-tuning and quantization to reduce memory demands and computational costs of large language models while maintaining or improving their performance.</p><hr><h3>A Combinatorial Algorithm for Approximating the Optimal Transport in the Parallel and MPC Settings</h3>
<p>Nathaniel Lahn, Sharath Raghvendra, Kaiyi Zhang</p>
<p><a href='https://openreview.net/forum?id=2izFpGERjU'>https://openreview.net/forum?id=2izFpGERjU</a></p>
<p><b>Keywords</b>: Optimal Transport, Combinatorial Optimization
</p><p><b>Compressor summary</b>: The paper introduces a parallel combinatorial algorithm to compute an additive epsilon-approximation of the optimal transport distance with improved complexity and speed compared to existing methods.</p><hr><h3>Neuro-symbolic Learning Yielding Logical Constraints</h3>
<p>Zenan Li, Yunpeng Huang, Zhaoyu Li, Yuan Yao, Jingwei Xu, Taolue Chen, Xiaoxing Ma, Jian Lu</p>
<p><a href='https://openreview.net/forum?id=2ioRi2uwLR'>https://openreview.net/forum?id=2ioRi2uwLR</a></p>
<p><b>Keywords</b>: Neuro-symbolic learning, logical constraint learning, symbol grounding, difference-of-convex relaxation
</p><p><b>Compressor summary</b>: The paper presents a framework that integrates neural network training, symbol grounding, and logical constraint synthesis for end-to-end learning of neuro-symbolic systems, using techniques to relax and maintain logical constraints.</p><hr><h3>Estimating Propensity for Causality-based Recommendation without Exposure Data</h3>
<p>Zhongzhou Liu, Yuan Fang, Min Wu</p>
<p><a href='https://openreview.net/forum?id=2hhIDEHhkk'>https://openreview.net/forum?id=2hhIDEHhkk</a></p>
<p><b>Keywords</b>: recommendation systems, causal effect, propensity score, propensity estimation
</p><p><b>Compressor summary</b>: The paper proposes PropCare, a framework that estimates exposure and propensity from interaction data for causality-based recommendation systems without additional input.</p><hr><h3>What is the Inductive Bias of Flatness Regularization? A Study of Deep Matrix Factorization Models</h3>
<p>Khashayar Gatmiry, Zhiyuan Li, Tengyu Ma, Sashank J. Reddi, Stefanie Jegelka, Ching-Yao Chuang</p>
<p><a href='https://openreview.net/forum?id=2hQ7MBQApp'>https://openreview.net/forum?id=2hQ7MBQApp</a></p>
<p><b>Keywords</b>: Sharpness minimization, Deep learning, Matrix factorization, Deep linear networks, Implicit bias, SGD, Trace of Hessian regularizer
</p><p><b>Compressor summary</b>: This paper investigates how minimizing the sharpness of the loss function improves generalization in deep linear networks trained with certain measurements, and shows that it is equivalent to minimizing a matrix norm.</p><hr><h3>Mode Connectivity in Auction Design</h3>
<p>Christoph Hertrich, Yixin Tao, László A. Végh</p>
<p><a href='https://openreview.net/forum?id=2gn9WFlqJ4'>https://openreview.net/forum?id=2gn9WFlqJ4</a></p>
<p><b>Keywords</b>: Differentiable Economics, Mechanism Design, Neural Network Theory, Mode Connectivity, RochetNet
</p><p><b>Compressor summary</b>: The paper analyzes mode connectivity of neural networks for optimal auction design and provides the first theoretical justification for their empirical success in this area.</p><hr><h3>TopP&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models</h3>
<p>Pum Jun Kim, Yoojin Jang, Jisu Kim, Jaejun Yoo</p>
<p><a href='https://openreview.net/forum?id=2gUCMr6fDY'>https://openreview.net/forum?id=2gUCMr6fDY</a></p>
<p><b>Keywords</b>: GAN, Evaluation, Support Estimation
</p><p><b>Compressor summary</b>: TopP&R is a new and better way to evaluate generative models by measuring the significance and confidence of features in samples, unlike existing metrics that rely on unreliable support estimates.</p><hr><h3>Effective Targeted Attacks for Adversarial Self-Supervised Learning</h3>
<p>Minseon Kim, Hyeonjeong Ha, Sooel Son, Sung Ju Hwang</p>
<p><a href='https://openreview.net/forum?id=2f0dlMZlNb'>https://openreview.net/forum?id=2f0dlMZlNb</a></p>
<p><b>Keywords</b>: Adversarial self supervised learning, targeted attack, self supervised learning, contrastive learning, positive mining
</p><p><b>Compressor summary</b>: The paragraph discusses a new method for improving adversarial robustness in unsupervised learning by using positive mining for targeted attacks instead of general adversarial attacks.</p><hr><h3>Foundation Model is Efficient Multimodal Multitask Model Selector</h3>
<p>Fanqing Meng, Wenqi Shao, zhanglin peng, Chonghe Jiang, Kaipeng Zhang, Yu Qiao, Ping Luo</p>
<p><a href='https://openreview.net/forum?id=2ep5PXEZiw'>https://openreview.net/forum?id=2ep5PXEZiw</a></p>
<p><b>Keywords</b>: transfer learning, model selection, foundation model
</p><p><b>Compressor summary</b>: The paper proposes EMMS, a fast and effective method to estimate the transferability of pre-trained neural networks on multiple multi-modal tasks without fine-tuning them, using large-scale foundation models and unified label embeddings.</p><hr><h3>Learning Invariant Representations with a Nonparametric Nadaraya-Watson Head</h3>
<p>Alan Q. Wang, Minh Nguyen, Mert R. Sabuncu</p>
<p><a href='https://openreview.net/forum?id=2ePf1sBgLU'>https://openreview.net/forum?id=2ePf1sBgLU</a></p>
<p><b>Keywords</b>: Invariant representations, causality, domain generalization
</p><p><b>Compressor summary</b>: The paper proposes a nonparametric method using Nadaraya-Watson heads to learn invariant representations across different environments, which can improve domain generalization in machine learning models.</p><hr><h3>Probabilistic Exponential Integrators</h3>
<p>Nathanael Bosch, Philipp Hennig, Filip Tronarp</p>
<p><a href='https://openreview.net/forum?id=2dx5MNs2Ip'>https://openreview.net/forum?id=2dx5MNs2Ip</a></p>
<p><b>Keywords</b>: Probabilistic numerics, differential equations, exponential integrators, Kalman filters, Gaussian processes
</p><p><b>Compressor summary</b>: The paper presents a new class of probabilistic integrators for dynamical systems that improve stability and efficiency in stiff problems by including fast, linear dynamics in the prior, and generalizing them to non-linear systems using piece-wise semi-linearity.</p><hr><h3>Bilevel Coreset Selection in Continual Learning: A New Formulation and Algorithm</h3>
<p>Jie Hao, Kaiyi Ji, Mingrui Liu</p>
<p><a href='https://openreview.net/forum?id=2dtU9ZbgSN'>https://openreview.net/forum?id=2dtU9ZbgSN</a></p>
<p><b>Keywords</b>: Coreset Selection, Continual Learning, Bilevel Optimization
</p><p><b>Compressor summary</b>: The paper proposes a new bilevel formulation for coreset selection in rehearsal-based continual learning, using a novel regularizer and an efficient optimization algorithm that outperforms baselines.</p><hr><h3>Efficient Online Clustering with Moving Costs</h3>
<p>Dimitris Christou, EFSTRATIOS PANTELEIMON SKOULAKIS, Volkan Cevher</p>
<p><a href='https://openreview.net/forum?id=2doqt9r0r0'>https://openreview.net/forum?id=2doqt9r0r0</a></p>
<p><b>Keywords</b>: Online Learning, Regret Analysis, Clustering, k-Median
</p><p><b>Compressor summary</b>: The paper proposes an online learning algorithm for clustering facilities with moving costs that achieves logarithmic regret and beats the best fixed solution's average cost by a factor of poly-log(n).</p><hr><h3>Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing</h3>
<p>Shuyao Li, Yu Cheng, Ilias Diakonikolas, Jelena Diakonikolas, Rong Ge, Stephen Wright</p>
<p><a href='https://openreview.net/forum?id=2ccH4zjKVs'>https://openreview.net/forum?id=2ccH4zjKVs</a></p>
<p><b>Keywords</b>: low rank matrix sensing, non-convex optimization, high-dimensional robust statistics, second-order optimization, statistical query model
</p><p><b>Compressor summary</b>: The paper proposes a general framework to find approximate second-order stationary points in nonconvex optimization with dimension-independent accuracy guarantees, and applies it to low rank matrix sensing with robustness to data corruption.</p><hr><h3>Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots</h3>
<p>Ruixiang Tang, Jiayi Yuan, Yiming Li, Zirui Liu, Rui Chen, Xia Hu</p>
<p><a href='https://openreview.net/forum?id=2cYxNWNzk3'>https://openreview.net/forum?id=2cYxNWNzk3</a></p>
<p><b>Keywords</b>: Backdoor Defense, Honeypot
</p><p><b>Compressor summary</b>: The study proposes an honeypot module to absorb backdoor information and prevent backdoor attacks during fine-tuning of pretrained language models, achieving substantial reduction in attack success rate.</p><hr><h3>Towards Accelerated Model Training via Bayesian Data Selection</h3>
<p>Zhijie Deng, Peng Cui, Jun Zhu</p>
<p><a href='https://openreview.net/forum?id=2bRG4Hj8qd'>https://openreview.net/forum?id=2bRG4Hj8qd</a></p>
<p><b>Keywords</b>: data selection, training acceleration, probabilistic modeling, Bayesian methods
</p><p><b>Compressor summary</b>: The paper proposes an efficient algorithm that selects data based on its impact on generalization loss, using Bayesian treatment and zero-shot predictors, and shows improved training efficiency on noisy and imbalanced datasets compared to existing methods.</p><hr><h3>Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning</h3>
<p>Cristina Menghini, Andrew Delworth, Stephen Bach</p>
<p><a href='https://openreview.net/forum?id=2b9aY2NgXE'>https://openreview.net/forum?id=2b9aY2NgXE</a></p>
<p><b>Keywords</b>: vision-language models, prompt-tuning, pseudolabels, self-training
</p><p><b>Compressor summary</b>: The study explores using zero-shot pseudolabels and prompt tuning strategies to enhance CLIP's performance on image classification tasks without requiring additional labeled data.</p><hr><h3>Implicit Convolutional Kernels for Steerable CNNs</h3>
<p>Maksim Zhdanov, Nico Hoffmann, Gabriele Cesa</p>
<p><a href='https://openreview.net/forum?id=2YtdxqvdjX'>https://openreview.net/forum?id=2YtdxqvdjX</a></p>
<p><b>Keywords</b>: equivariance; group convolutions; implicit kernels; physical simulations
</p><p><b>Compressor summary</b>: The paper introduces a flexible way to implement Steerable CNNs using MLPs to parameterize G-steerable kernels, which generalizes to different groups G and performs well on diverse tasks.</p><hr><h3>CLIP-OGD: An Experimental Design for Adaptive Neyman Allocation in Sequential Experiments</h3>
<p>Jessica Dai, Paula Gradu, Christopher Harshaw</p>
<p><a href='https://openreview.net/forum?id=2Xqvk2KVAq'>https://openreview.net/forum?id=2Xqvk2KVAq</a></p>
<p><b>Keywords</b>: causal inference, randomized experiments, online optimization
</p><p><b>Compressor summary</b>: The paper studies how to create adaptive designs for causal inference that are almost as efficient as non-adaptive ones and proposes new performance measures and an example design that performs well.</p><hr><h3>Structured Federated Learning through Clustered Additive Modeling</h3>
<p>Jie Ma, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang</p>
<p><a href='https://openreview.net/forum?id=2XT3UpOv48'>https://openreview.net/forum?id=2XT3UpOv48</a></p>
<p><b>Keywords</b>: Federated Learning
</p><p><b>Compressor summary</b>: The paper proposes Clustered Additive Modeling (CAM) for heterogeneous federated learning, which combines global and cluster models to capture shared features and prevent "clustering collapse."</p><hr><h3>Scaling laws for language encoding models in fMRI</h3>
<p>Richard Antonello, Aditya Vaidya, Alexander Huth</p>
<p><a href='https://openreview.net/forum?id=2W4LxJbgec'>https://openreview.net/forum?id=2W4LxJbgec</a></p>
<p><b>Keywords</b>: Encoding Models, Language Models, Neuroscience, Scaling Laws
</p><p><b>Compressor summary</b>: Larger language models like OPT and LLaMA can better predict brain responses to natural language using fMRI, with performance scaling logarithmically with model size and nearing a theoretical maximum for certain brain areas.</p><hr><h3>Revisiting Implicit Differentiation for Learning Problems in Optimal Control</h3>
<p>Ming Xu, Timothy L Molloy, Stephen Gould</p>
<p><a href='https://openreview.net/forum?id=2URr3mkagy'>https://openreview.net/forum?id=2URr3mkagy</a></p>
<p><b>Keywords</b>: implicit differentiation, bi-level optimization; constrained learning and control; safe learning for control
</p><p><b>Compressor summary</b>: The paper presents a new method for solving non-convex optimal control problems using the implicit function theorem, which improves scalability, numerical stability, and parallelization compared to previous methods.</p><hr><h3>On the Connection between Pre-training Data Diversity and Fine-tuning Robustness</h3>
<p>Vivek Ramanujan, Thao Nguyen, Sewoong Oh, Ali Farhadi, Ludwig Schmidt</p>
<p><a href='https://openreview.net/forum?id=2SScUiWUbn'>https://openreview.net/forum?id=2SScUiWUbn</a></p>
<p><b>Keywords</b>: robustness, out-of-distribution shifts, finetuning, pretraining
</p><p><b>Compressor summary</b>: The main factor affecting the robustness of fine-tuned models after pre-training is data quantity, while other factors like label space, image diversity, and domain have limited impact.</p><hr><h3>DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method</h3>
<p>Ahmed Khaled, Konstantin Mishchenko, Chi Jin</p>
<p><a href='https://openreview.net/forum?id=2RQhgx1WLA'>https://openreview.net/forum?id=2RQhgx1WLA</a></p>
<p><b>Keywords</b>: normalized gradient descent, gradient descent, adagrad, adaptive optimization, parameter-free, smooth optimization, convex optimization, edge of stability
</p><p><b>Compressor summary</b>: DoWG is a new gradient-based optimizer that works efficiently and universally without parameters and achieves this by using a distance-based weighted average of gradients.</p><hr><h3>Group Robust Classification Without Any Group Information</h3>
<p>Christos Tsirigotis, Joao Monteiro, Pau Rodriguez, David Vazquez, Aaron Courville</p>
<p><a href='https://openreview.net/forum?id=2OcNWFHFpk'>https://openreview.net/forum?id=2OcNWFHFpk</a></p>
<p><b>Keywords</b>: out-of-distribution generalization, robustness, fairness, spurious correlations, systematic generalization, model selection
</p><p><b>Compressor summary</b>: The paper proposes a new bias-unsupervised method to improve group robustness in ERM by using pretrained self-supervised models and logit adjustment training loss, addressing the limitations of existing approaches that depend on group information or bias labels.</p><hr><h3>CoLLAT: On Adding Fine-grained Audio Understanding to Language Models using Token-Level Locked-Language Tuning</h3>
<p>Amila Silva, Spencer Whitehead, Chris Lengerich, Hugh James Leather</p>
<p><a href='https://openreview.net/forum?id=2NncD8AaFK'>https://openreview.net/forum?id=2NncD8AaFK</a></p>
<p><b>Keywords</b>: Audio Understanding, Contrastive Learning, Audio-Language Grounding
</p><p><b>Compressor summary</b>: The proposed method $CoLLAT$ enhances audio understanding and provides fine-grained audio grounding using a novel pretraining objective, achieving state-of-the-art results in various downstream tasks.</p><hr><h3>Segment Anything in 3D with NeRFs</h3>
<p>Jiazhong Cen, Zanwei Zhou, Jiemin Fang, chen yang, Wei Shen, Lingxi Xie, Dongsheng Jiang, XIAOPENG ZHANG, Qi Tian</p>
<p><a href='https://openreview.net/forum?id=2NkGfA66Ne'>https://openreview.net/forum?id=2NkGfA66Ne</a></p>
<p><b>Keywords</b>: Segmentation, NeRF, 3D segmentation
</p><p><b>Compressor summary</b>: SA3D is an efficient method that uses SAM and NeRF to segment 3D objects from 2D images with minimal manual input.</p><hr><h3>Gaussian Membership Inference Privacy</h3>
<p>Tobias Leemann, Martin Pawelczyk, Gjergji Kasneci</p>
<p><a href='https://openreview.net/forum?id=2NUFe4TZMS'>https://openreview.net/forum?id=2NUFe4TZMS</a></p>
<p><b>Keywords</b>: Privacy, Membership Inference Attacks
</p><p><b>Compressor summary</b>: The paper introduces $f$-MIP, a novel privacy notion for machine learning models that considers realistic adversaries and offers better utility, and proposes $\mu$-GMIP, an enhancement of $f$-MIP with added noise to gradient updates.</p><hr><h3>Unifying Predictions of Deterministic and Stochastic Physics in Mesh-reduced Space with Sequential Flow Generative Model</h3>
<p>Luning Sun, Xu Han, Han Gao, Jian-Xun Wang, Liping Liu</p>
<p><a href='https://openreview.net/forum?id=2JtwuJtoa0'>https://openreview.net/forum?id=2JtwuJtoa0</a></p>
<p><b>Keywords</b>: AI4Science, Fluid Dynamics, Generative Models, Graph Neural Network
</p><p><b>Compressor summary</b>: The paper proposes a new model that combines generative and sequential networks to accurately predict both deterministic and stochastic dynamical systems, using an autoencoder and a conditional normalizing flow model.</p><hr><h3>Constructing Non-isotropic Gaussian Diffusion Model Using Isotropic Gaussian Diffusion Model for Image Editing</h3>
<p>Xi Yu, Xiang Gu, Haozhi Liu, Jian Sun</p>
<p><a href='https://openreview.net/forum?id=2Ibp83esmb'>https://openreview.net/forum?id=2Ibp83esmb</a></p>
<p><b>Keywords</b>: score-based diffusion model, non-isotropic Gaussian diffusion model, image editing
</p><p><b>Compressor summary</b>: The Non-isotropic Gaussian Diffusion Model (NGDM) is a novel technique for image editing that uses different noises and diffusion times for different pixels, achieving high quality results while preserving the source image.</p><hr><h3>A General Theory of Correct, Incorrect, and Extrinsic Equivariance</h3>
<p>Dian Wang, Xupeng Zhu, Jung Yeon Park, Mingxi Jia, Guanang Su, Robert Platt, Robin Walters</p>
<p><a href='https://openreview.net/forum?id=2FMJtNDLeE'>https://openreview.net/forum?id=2FMJtNDLeE</a></p>
<p><b>Keywords</b>: Equivariance, Deep Learning, Error Bound, Symmetry
</p><p><b>Compressor summary</b>: The paper presents a theory to analyze and quantify various types of equivariance in neural networks when the ground truth function is only partially symmetric, and studies its impact on model error.</p><hr><h3>ViSt3D: Video Stylization with 3D CNN</h3>
<p>Ayush Pande, Gaurav Sharma</p>
<p><a href='https://openreview.net/forum?id=2EiqizElGO'>https://openreview.net/forum?id=2EiqizElGO</a></p>
<p><b>Keywords</b>: Video style transfer
</p><p><b>Compressor summary</b>: The paper presents a new approach to video stylization using 3D CNN that disentangles motion and appearance, stylizes the appearance, and then adds back the motion, as well as introducing a new dataset for training and testing video stylization networks.</p><hr><h3>Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning</h3>
<p>Ahmadreza Moradipari, Mohammad Pedramfar, Modjtaba Shokrian Zini, Vaneet Aggarwal</p>
<p><a href='https://openreview.net/forum?id=2EVTB1idyR'>https://openreview.net/forum?id=2EVTB1idyR</a></p>
<p><b>Keywords</b>: Thompson Sampling, Reinforcement Learning, Bayesian Regret
</p><p><b>Compressor summary</b>: The paper shows new Bayesian regret bounds for Thompson Sampling in various reinforcement learning scenarios, with improved upper bounds for the information ratio.</p><hr><h3>Any-to-Any Generation via Composable Diffusion</h3>
<p>Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, Mohit Bansal</p>
<p><a href='https://openreview.net/forum?id=2EDqbSCnmF'>https://openreview.net/forum?id=2EDqbSCnmF</a></p>
<p><b>Keywords</b>: Generative AI, Diffusion Model, Multimodal Generation, Audio-Video Generation
</p><p><b>Compressor summary</b>: CoDi is a novel generative model that can create any combination of language, image, video, or audio from any input modalities, using a unique composable generation strategy and multimodal space alignment.</p><hr><h3>Learning to Compress Prompts with Gist Tokens</h3>
<p>Jesse Mu, Xiang Lisa Li, Noah Goodman</p>
<p><a href='https://openreview.net/forum?id=2DtxPCL3T5'>https://openreview.net/forum?id=2DtxPCL3T5</a></p>
<p><b>Keywords</b>: language models, instruction finetuning, prompt compression, distillation, context distillation, prompting, soft prompting, efficiency
</p><p><b>Compressor summary</b>: The paper proposes gisting, a method that trains language models to compress prompts into smaller sets of tokens for compute efficiency and other benefits.</p><hr><h3>Navigating Data Heterogeneity in Federated Learning: A Semi-Supervised Approach for Object Detection</h3>
<p>Taehyeon Kim, Eric Lin, Junu Lee, Christian Lau, Vaikkunth Mugunthan</p>
<p><a href='https://openreview.net/forum?id=2D7ou48q0E'>https://openreview.net/forum?id=2D7ou48q0E</a></p>
<p><b>Keywords</b>: Federated Learning, Semi-Supervised Learning, Object Detection
</p><p><b>Compressor summary</b>: The paper introduces FedSTO, a semi-supervised federated object detection framework for autonomous driving that uses selective training and orthogonality regularization to handle data shift and limited labeled data.</p><hr><h3>Discriminative Calibration: Check Bayesian Computation from Simulations and Flexible Classifier</h3>
<p>Yuling Yao, Justin Domke</p>
<p><a href='https://openreview.net/forum?id=2Cmdh5z6ph'>https://openreview.net/forum?id=2Cmdh5z6ph</a></p>
<p><b>Keywords</b>: simulation based calibration, simulation based inference, Bayesian computation, diagnostics, classifier two-sample test, likelihood-free
</p><p><b>Compressor summary</b>: The paper proposes a classification approach that learns test statistics from data to improve Bayesian calibration accuracy and provide interpretable divergence measures, which can be used for simulation-based or traditional inference methods.</p><hr><h3>Characterizing the Optimal $0-1$ Loss for Multi-class Classification with a Test-time Attacker</h3>
<p>Sihui Dai, Wenxin Ding, Arjun Nitin Bhagoji, Daniel Cullina, Haitao Zheng, Ben Y. Zhao, Prateek Mittal</p>
<p><a href='https://openreview.net/forum?id=2CRaOpEKWh'>https://openreview.net/forum?id=2CRaOpEKWh</a></p>
<p><b>Keywords</b>: adversarial robustness, graph theory, fundamental bounds
</p><p><b>Compressor summary</b>: The paper develops a framework to find information-theoretic lower bounds on robust loss for multi-class classifiers under test-time attacks and compares them with state-of-the-art methods.</p><hr><h3>DOSE: Diffusion Dropout with Adaptive Prior for Speech Enhancement</h3>
<p>Wenxin Tai, Yue Lei, Fan Zhou, Goce Trajcevski, Ting Zhong</p>
<p><a href='https://openreview.net/forum?id=2C2WZfCfo9'>https://openreview.net/forum?id=2C2WZfCfo9</a></p>
<p><b>Keywords</b>: speech enhancement, diffusion models, adaptive prior, dropout, generalization
</p><p><b>Compressor summary</b>: The paper proposes DOSE, a model-agnostic method that uses two techniques to incorporate condition information into DDPMs for speech enhancement, improving speech quality and stability.</p><hr><h3>ISP: Multi-Layered Garment Draping with Implicit Sewing Patterns</h3>
<p>Ren Li, Benoît Guillard, Pascal Fua</p>
<p><a href='https://openreview.net/forum?id=2BrHBj1Puu'>https://openreview.net/forum?id=2BrHBj1Puu</a></p>
<p><b>Keywords</b>: garment modeling, draping, deformation, human body modeling
</p><p><b>Compressor summary</b>: The paper introduces a new model for draping garments on human body models that can handle multi-layered clothing and various poses, using a combination of 2D and 3D parameterization.</p><hr><h3>Solving Inverse Physics Problems with Score Matching</h3>
<p>Benjamin Holzschuh, Simona Vegetti, Nils Thuerey</p>
<p><a href='https://openreview.net/forum?id=2BpoGPSDCR'>https://openreview.net/forum?id=2BpoGPSDCR</a></p>
<p><b>Keywords</b>: inverse problems, diffusion models, learned corrections, score matching
</p><p><b>Compressor summary</b>: The authors propose a method to solve inverse problems in physics using diffusion models that combines an approximate inverse simulator and a learned correction function, achieving high accuracy and temporal stability.</p><hr><h3>Learning Functional Transduction</h3>
<p>Mathieu Chalvidal, Thomas Serre, Rufin VanRullen</p>
<p><a href='https://openreview.net/forum?id=2BFZ8cPIf6'>https://openreview.net/forum?id=2BFZ8cPIf6</a></p>
<p><b>Keywords</b>: Meta-learning, Neural Operators, Kernel methods, In-context learning
</p><p><b>Compressor summary</b>: The paper proposes a hybrid approach that combines transductive and inductive methods using vector-valued Reproducing Kernel Banach Spaces to create a meta-learned neural approximator called Transducer, which can efficiently capture new functional relationships and model physical systems with little data.</p><hr><h3>Sharpness-Aware Minimization Leads to Low-Rank Features</h3>
<p>Maksym Andriushchenko, Dara Bahri, Hossein Mobahi, Nicolas Flammarion</p>
<p><a href='https://openreview.net/forum?id=29WbraPk8U'>https://openreview.net/forum?id=29WbraPk8U</a></p>
<p><b>Keywords</b>: sharpness-aware minimization, low-rank features, understanding feature learning
</p><p><b>Compressor summary</b>: Sharpness-aware minimization (SAM) reduces feature rank by pruning activations across various neural network architectures and objectives.</p><hr><h3>Improving Graph Matching with Positional Reconstruction Encoder-Decoder Network</h3>
<p>Yixiao Zhou, Ruiqi Jia, Hongxiang Lin, Hefeng Quan, Yumeng Zhao, Xiaoqing Lyu</p>
<p><a href='https://openreview.net/forum?id=28RTu9MOT6'>https://openreview.net/forum?id=28RTu9MOT6</a></p>
<p><b>Keywords</b>: Graph Matching, Positional Encoding
</p><p><b>Compressor summary</b>: The paper introduces a new graph matching method, PREGM, that leverages spatial information of keypoints to establish correspondence between keypoint sets in images using a positional reconstruction encoder-decoder (PR-EnDec).</p><hr><h3>Going beyond persistent homology using persistent homology</h3>
<p>Johanna Emilia Immonen, Amauri H Souza, Vikas Garg</p>
<p><a href='https://openreview.net/forum?id=27TdrEvqLD'>https://openreview.net/forum?id=27TdrEvqLD</a></p>
<p><b>Keywords</b>: graph representation learning, topological deep learning, persistent homology, graph neural networks
</p><p><b>Compressor summary</b>: The text introduces a new concept of color-separating sets to improve the representation of graphs using persistent homology and proposes RePHINE, a model that combines vertex- and edge-level information, achieving better results than existing methods on graph classification tasks.</p><hr><h3>Noether Embedding: Efficient Learning of Temporal Regularities</h3>
<p>Chi Gao, Zidong Zhou, Luping Shi</p>
<p><a href='https://openreview.net/forum?id=27CRbwewyb'>https://openreview.net/forum?id=27CRbwewyb</a></p>
<p><b>Keywords</b>: Schema Learning, Temporal Regularity, Event Embedding
</p><p><b>Compressor summary</b>: The authors propose Noether Embedding (NE), an efficient method to learn temporal regularities from event embeddings, which outperforms existing methods in detecting and querying valid temporal patterns.</p><hr><h3>General Munchausen Reinforcement Learning with Tsallis Kullback-Leibler Divergence</h3>
<p>Lingwei Zhu, Zheng Chen, Matthew Kyle Schlegel, Martha White</p>
<p><a href='https://openreview.net/forum?id=26qqUHi9XF'>https://openreview.net/forum?id=26qqUHi9XF</a></p>
<p><b>Keywords</b>: reinforcement learning, entropy regularization, Tsallis KL divergence
</p><p><b>Compressor summary</b>: The paper proposes a new policy optimization approach in reinforcement learning using Tsallis KL divergence, which generalizes the standard KL divergence and can improve performance in Atari games.</p><hr><h3>Beyond Normal: On the Evaluation of Mutual Information Estimators</h3>
<p>Paweł Czyż, Frederic Grabowski, Julia E Vogt, Niko Beerenwinkel, Alexander Marx</p>
<p><a href='https://openreview.net/forum?id=25vRtG56YH'>https://openreview.net/forum?id=25vRtG56YH</a></p>
<p><b>Keywords</b>: Mutual Information, Invariance, Benchmark, Geometric Machine Learning
</p><p><b>Compressor summary</b>: The paper introduces a diverse family of distributions with known ground-truth mutual information and a language-independent benchmarking platform for evaluating mutual information estimators in various settings.</p><hr><h3>CAPro: Webly Supervised Learning with Cross-modality Aligned Prototypes</h3>
<p>Yulei Qin, Xingyu Chen, Yunhang Shen, Chaoyou Fu, Yun Gu, Ke Li, Xing Sun, Rongrong Ji</p>
<p><a href='https://openreview.net/forum?id=25HiFHPcXg'>https://openreview.net/forum?id=25HiFHPcXg</a></p>
<p><b>Keywords</b>: webly supervised learning, representation learning, visual-semantic alignment, collective bootstrapping
</p><p><b>Compressor summary</b>: CAPro is a method that combats label noise in webly supervised learning by using textual prototypes, visual feature enhancement, and collective bootstrapping to learn semantically aligned visual representations.</p><hr><h3>DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative Diffusion Models</h3>
<p>Tsun-Hsuan Wang, Juntian Zheng, Pingchuan Ma, Yilun Du, Byungchul Kim, Andrew Everett Spielberg, Joshua B. Tenenbaum, Chuang Gan, Daniela Rus</p>
<p><a href='https://openreview.net/forum?id=1zo4iioUEs'>https://openreview.net/forum?id=1zo4iioUEs</a></p>
<p><b>Keywords</b>: soft robot, diffusion model, co-design
</p><p><b>Compressor summary</b>: The paper introduces DiffuseBot, a model that generates soft robot morphologies for various tasks by combining physics simulation with diffusion models.</p><hr><h3>Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions</h3>
<p>Duligur Ibeling, Thomas Icard</p>
<p><a href='https://openreview.net/forum?id=1zKRwh5Rl2'>https://openreview.net/forum?id=1zKRwh5Rl2</a></p>
<p><b>Keywords</b>: potential outcomes framework, structural causal model, causal inference, logic, probability, graphical causal models, causal abstraction, causal machine learning
</p><p><b>Compressor summary</b>: The paper clarifies how Rubin causal models and structural causal models relate and shows that all Rubin models can be represented by structural models, despite some differences in their principles.</p><hr><h3>Computing a human-like reaction time metric from stable recurrent vision models</h3>
<p>Lore Goetschalckx, Lakshmi Narasimhan Govindarajan, Alekh Karkada Ashok, Aarit Ahuja, David Sheinberg, Thomas Serre</p>
<p><a href='https://openreview.net/forum?id=1xPsn2gCOe'>https://openreview.net/forum?id=1xPsn2gCOe</a></p>
<p><b>Keywords</b>: alignment, RNNs, reaction times, equilibrium dynamics, perceptual grouping, decision making
</p><p><b>Compressor summary</b>: The authors present a method to measure reaction times in recurrent vision models, which helps align them with human visual decision-making patterns across different tasks.</p><hr><h3>Hierarchical VAEs provide a normative account of motion processing in the primate brain</h3>
<p>Hadi Vafaii, Jacob L. Yates, Daniel A. Butts</p>
<p><a href='https://openreview.net/forum?id=1wOkHN9JK8'>https://openreview.net/forum?id=1wOkHN9JK8</a></p>
<p><b>Keywords</b>: NeuroAI, VAE, Dorsal stream, Hierarchical Bayesian Inference
</p><p><b>Compressor summary</b>: The paper evaluates a new hierarchical VAE model for motion perception tasks and shows how it aligns with brain function in interpreting causal relationships between stimuli and neuronal responses.</p><hr><h3>Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition</h3>
<p>Samuel Dooley, Rhea Sanjay Sukthanker, John P Dickerson, Colin White, Frank Hutter, Micah Goldblum</p>
<p><a href='https://openreview.net/forum?id=1vzF4zWQ1E'>https://openreview.net/forum?id=1vzF4zWQ1E</a></p>
<p><b>Keywords</b>: Bias Mitigation, Fairness, Facial Recognition
</p><p><b>Compressor summary</b>: The paragraph discusses how biases in face recognition systems are inherent to neural network architectures, and presents a new method for finding fairer architecture designs that outperforms existing methods on accuracy and fairness.</p><hr><h3>Unexpected Improvements to Expected Improvement for Bayesian Optimization</h3>
<p>Sebastian Ament, Sam Daulton, David Eriksson, Maximilian Balandat, Eytan Bakshy</p>
<p><a href='https://openreview.net/forum?id=1vyAG6j9PE'>https://openreview.net/forum?id=1vyAG6j9PE</a></p>
<p><b>Keywords</b>: Bayesian Optimization, Gaussian Process, Multi-Objective Optimization
</p><p><b>Compressor summary</b>: LogEI is a new acquisition function family that is easier to optimize numerically than EI and its variants, improving their performance in Bayesian optimization.</p><hr><h3>Boosting with Tempered Exponential Measures</h3>
<p>Richard Nock, Ehsan Amid, Manfred K Warmuth</p>
<p><a href='https://openreview.net/forum?id=1vvsIJtnnr'>https://openreview.net/forum?id=1vvsIJtnnr</a></p>
<p><b>Keywords</b>: Boosting, optimization, exponential families
</p><p><b>Compressor summary</b>: The paper introduces $t$-AdaBoost, a generalization of AdaBoost that uses tempered exponential measures to improve convergence rates and bound leveraging coefficients for decision tree induction.</p><hr><h3>Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture</h3>
<p>Galen Pogoncheff, Jacob Granley, Michael Beyeler</p>
<p><a href='https://openreview.net/forum?id=1uirUsR9E7'>https://openreview.net/forum?id=1uirUsR9E7</a></p>
<p><b>Keywords</b>: NeuroAI, Neuroscience, Visual Stream, Convolutional Neural Networks, Biologically inspired deep learning
</p><p><b>Compressor summary</b>: The authors improve CNNs by incorporating neuroscience-derived components to better explain V1 neural activity and properties, advancing NeuroAI research.</p><hr><h3>Topological Obstructions and How to Avoid Them</h3>
<p>Babak Esmaeili, Robin Walters, Heiko Zimmermann, Jan-Willem van de Meent</p>
<p><a href='https://openreview.net/forum?id=1tviRBNxI9'>https://openreview.net/forum?id=1tviRBNxI9</a></p>
<p><b>Keywords</b>: representation learning, variational autoencoders, homeomorphism, topological, equivariant, lie groups, normalizing flows
</p><p><b>Compressor summary</b>: The paper studies the challenges of training models with geometric latent spaces and proposes a new flow-based model that improves interpretability and generalization by mapping data to multimodal distributions over geometric spaces.</p><hr><h3>Decorate3D: Text-Driven High-Quality Texture Generation for Mesh Decoration in the Wild</h3>
<p>Yanhui Guo, Xinxin Zuo, Peng Dai, Juwei Lu, Xiaolin Wu, Li Cheng, Youliang Yan, Songcen Xu, Xiaofei Wu</p>
<p><a href='https://openreview.net/forum?id=1recIOnzOF'>https://openreview.net/forum?id=1recIOnzOF</a></p>
<p><b>Keywords</b>: Texture Generation, Text-Driven, 3D-Consistent Editing, Neural Radiance Field
</p><p><b>Compressor summary</b>: Decorate3D is a method that uses neural networks to create and edit 3D objects from images, allowing users to edit or generate textures with high quality.</p><hr><h3>Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation</h3>
<p>Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, LINGMING ZHANG</p>
<p><a href='https://openreview.net/forum?id=1qvx610Cu7'>https://openreview.net/forum?id=1qvx610Cu7</a></p>
<p><b>Keywords</b>: LLM4Code, ChatGPT, Automated Test Generation
</p><p><b>Compressor summary</b>: EvalPlus is a framework that uses automatic test input generation to rigorously evaluate the functional correctness of code synthesized by large language models, revealing previously undetected errors and improving programming benchmarks.</p><hr><h3>Energy Discrepancies: A Score-Independent Loss for Energy-Based Models</h3>
<p>Tobias Schröder, Zijing Ou, Jen Ning Lim, Yingzhen Li, Sebastian Josef Vollmer, Andrew Duncan</p>
<p><a href='https://openreview.net/forum?id=1qFnxhdbxg'>https://openreview.net/forum?id=1qFnxhdbxg</a></p>
<p><b>Keywords</b>: Energy-based models, statistical discrepancy, latent-variable model, density estimation
</p><p><b>Compressor summary</b>: Energy Discrepancy is a new loss function for training energy-based models that is faster, more accurate, and has theoretical guarantees compared to existing methods.</p><hr><h3>Large Language Models are Visual Reasoning Coordinators</h3>
<p>Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, Ziwei Liu</p>
<p><a href='https://openreview.net/forum?id=1q0feiJ2i4'>https://openreview.net/forum?id=1q0feiJ2i4</a></p>
<p><b>Keywords</b>: visual reasoning, large language models
</p><p><b>Compressor summary</b>: The paper introduces Cola, a method that uses a large language model (LLM) to coordinate multiple vision-language models (VLMs) for better visual reasoning by enabling natural language communication between them.</p><hr><h3>Uncertainty-Aware Instance Reweighting for Off-Policy Learning</h3>
<p>Xiaoying Zhang, Junpu Chen, Hongning Wang, Hong Xie, Yang Liu, John C.S. Lui, Hang Li</p>
<p><a href='https://openreview.net/forum?id=1pWNhmbllE'>https://openreview.net/forum?id=1pWNhmbllE</a></p>
<p><b>Keywords</b>: off-policy learning, uncertainty
</p><p><b>Compressor summary</b>: The paragraph discusses off-policy learning, a procedure used in applications like search engines and recommender systems, and proposes a new estimator (UIPS) that models uncertainty to improve the learning process.</p><hr><h3>Alternating Updates for Efficient Transformers</h3>
<p>Cenk Baykal, Dylan J Cutler, Nishanth Dikkala, Nikhil Ghosh, Rina Panigrahy, Xin Wang</p>
<p><a href='https://openreview.net/forum?id=1p6teT6F73'>https://openreview.net/forum?id=1p6teT6F73</a></p>
<p><b>Keywords</b>: efficiency, efficient transformers
</p><p><b>Compressor summary</b>: AltUp is a method that increases a model's capacity by widening token embeddings without increasing latency much, and it can be combined with other techniques for even better results.</p><hr><h3>Online Convex Optimization with Unbounded Memory</h3>
<p>Raunak Kumar, Sarah Dean, Robert Kleinberg</p>
<p><a href='https://openreview.net/forum?id=1osmdAfD4P'>https://openreview.net/forum?id=1osmdAfD4P</a></p>
<p><b>Keywords</b>: online learning, online convex optimization, online linear control
</p><p><b>Compressor summary</b>: This paper introduces Online Convex Optimization with Unbounded Memory (OCO-UM), a generalization of OCO that accounts for long-term dependence on past decisions, and provides upper and lower bounds on policy regret.</p><hr><h3>Energy Guided Diffusion for Generating Neurally Exciting Images</h3>
<p>Paweł A. Pierzchlewicz, Konstantin Friedrich Willeke, Arne Nix, Pavithra Elumalai, Kelli Restivo, Tori Shinn, Cate Nealley, Gabrielle Rodriguez, Saumil Patel, Katrin Franke, Andreas S. Tolias, Fabian H. Sinz</p>
<p><a href='https://openreview.net/forum?id=1moStpWGUj'>https://openreview.net/forum?id=1moStpWGUj</a></p>
<p><b>Keywords</b>: most exciting inputs, diffusion models, energy guidance, attention, macaque V4
</p><p><b>Compressor summary</b>: The authors propose a new method called Energy Guidance (EGG) for generating most exciting inputs (MEIs) that improve predictions of neuronal activity in macaque V4, reduce computational costs, and can be used for other tasks like studying invariances.</p><hr><h3>Adjustable Robust Reinforcement Learning for Online 3D Bin Packing</h3>
<p>Yuxin Pan, Yize Chen, Fangzhen Lin</p>
<p><a href='https://openreview.net/forum?id=1mdTYi1jAW'>https://openreview.net/forum?id=1mdTYi1jAW</a></p>
<p><b>Keywords</b>: online 3D bin packing problem, combinatorial optimization problem, reinforcement learning
</p><p><b>Compressor summary</b>: The paragraph discusses a new reinforcement learning framework, AR2L, which balances average and worst-case performance for solving online 3D bin packing problems, using a permutation-based attacker to evaluate its robustness.</p><hr><h3>Exploring the Optimal Choice for Generative Processes in Diffusion Models: Ordinary vs Stochastic Differential Equations</h3>
<p>Yu Cao, Jingrun Chen, Yixin Luo, Xiang ZHOU</p>
<p><a href='https://openreview.net/forum?id=1mJQq6zYaE'>https://openreview.net/forum?id=1mJQq6zYaE</a></p>
<p><b>Keywords</b>: diffusion models; stochastic differential equations; score-based generative models; asymptotic analysis
</p><p><b>Compressor summary</b>: The paper analyzes when ODE or SDE models are more suitable for computer vision tasks by studying error accumulation in two limiting scenarios and comparing numerical results with different data distributions.</p><hr><h3>Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding</h3>
<p>George Ma, Yifei Wang, Yisen Wang</p>
<p><a href='https://openreview.net/forum?id=1mAYtdoYw6'>https://openreview.net/forum?id=1mAYtdoYw6</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Positional Encoding, Spectral Embedding, Laplacian Eigenvectors
</p><p><b>Compressor summary</b>: The paper proposes Laplacian Canonization, a minimal and efficient pre-processing method for graph embedding that enhances the invariance properties of spectral embedding techniques.</p><hr><h3>Exponentially Convergent Algorithms for Supervised Matrix Factorization</h3>
<p>Joowon Lee, Hanbaek Lyu, Weixin Yao</p>
<p><a href='https://openreview.net/forum?id=1kgK0r8PGg'>https://openreview.net/forum?id=1kgK0r8PGg</a></p>
<p><b>Keywords</b>: Supervised matrix factorization, multi-objective optimization, global convergence, linear convergence, statistical estimation
</p><p><b>Compressor summary</b>: The paper proposes a new method and efficient algorithm for supervised matrix factorization that improves feature extraction and classification tasks in high-dimensional data, such as identifying cancer-related gene groups.</p><hr><h3>Implicit Transfer Operator Learning: Multiple Time-Resolution Models for Molecular Dynamics</h3>
<p>Mathias Schreiner, Ole Winther, Simon Olsson</p>
<p><a href='https://openreview.net/forum?id=1kZx7JiuA2'>https://openreview.net/forum?id=1kZx7JiuA2</a></p>
<p><b>Keywords</b>: AI4Science, Molecular Dynamics, equivariant neural networks, stochastic dynamics
</p><p><b>Compressor summary</b>: Implicit Transfer Operator Learning is a framework to learn surrogates for molecular dynamics simulations with multiple time-resolutions, enabling faster and more accurate modeling of molecular systems using coarse representations.</p><hr><h3>Neural Sculpting: Uncovering hierarchically modular task structure in neural networks through pruning and network analysis</h3>
<p>Shreyas Malakarjun Patil, Loizos Michael, Constantine Dovrolis</p>
<p><a href='https://openreview.net/forum?id=1jhmWkZGy6'>https://openreview.net/forum?id=1jhmWkZGy6</a></p>
<p><b>Keywords</b>: Neural networks, Hierarchical modularity, Pruning, Sparsity
</p><p><b>Compressor summary</b>: The paragraph discusses a method to identify the hierarchical structure of sub-functions in a task using deep neural networks, focusing on the domain of Boolean functions and two vision tasks from the MNIST dataset.</p><hr><h3>Towards Efficient Image Compression Without Autoregressive Models</h3>
<p>Muhammad Salman Ali, Yeongwoong Kim, Maryam Qamar, Sung-Chang Lim, Donghyun Kim, Chaoning Zhang, Sung-Ho Bae, Hui Yong Kim</p>
<p><a href='https://openreview.net/forum?id=1ihGy9vAIg'>https://openreview.net/forum?id=1ihGy9vAIg</a></p>
<p><b>Keywords</b>: Image Compression, Correlation
</p><p><b>Compressor summary</b>: The paper proposes a novel method to improve learned image compression by introducing a correlation loss that reduces discrepancies between latent features and the assumed distribution, achieving similar performance with significantly less computational complexity.</p><hr><h3>Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation</h3>
<p>Hongcheng Wang, Andy Guan Hong Chen, Xiaoqi Li, Mingdong Wu, Hao Dong</p>
<p><a href='https://openreview.net/forum?id=1hZwxBgQ3G'>https://openreview.net/forum?id=1hZwxBgQ3G</a></p>
<p><b>Keywords</b>: Visual Navigation, Demand-Driven Navigation
</p><p><b>Compressor summary</b>: The paper introduces Demand-driven Navigation (DDN), a method that allows agents to navigate using the user's demand, not just object names, and leverages common sense knowledge and visual features for better navigation performance.</p><hr><h3>Momentum Provably Improves Error Feedback!</h3>
<p>Ilyas Fatkhullin, Alexander Tyurin, Peter Richtárik</p>
<p><a href='https://openreview.net/forum?id=1h92PmnKov'>https://openreview.net/forum?id=1h92PmnKov</a></p>
<p><b>Keywords</b>: Heavy-ball momentum, Polyak momentum, Error feedback, Federated Learning, Distributed Optimization, Stochastic optimization, Nonconvex optimization
</p><p><b>Compressor summary</b>: The paper proposes a simple fix that improves error feedback algorithms for distributed machine learning by applying Polyak's momentum to the latest version, EF21, and shows theoretical and practical benefits.</p><hr><h3>Reliable learning in challenging environments</h3>
<p>Nina Balcan, Steve Hanneke, Rattana Pukdee, Dravyansh Sharma</p>
<p><a href='https://openreview.net/forum?id=1h7Uh9zUXc'>https://openreview.net/forum?id=1h7Uh9zUXc</a></p>
<p><b>Keywords</b>: Reliable machine learning, adversarial robustness, distribution shift, theory
</p><p><b>Compressor summary</b>: The paper proposes a reliable learner with optimal guarantees in challenging test-time environments like adversarial attacks and distribution shifts, and demonstrates strong performance on examples.</p><hr><h3>Exact Verification of ReLU Neural Control Barrier Functions</h3>
<p>Hongchao Zhang, Junlin Wu, Yevgeniy Vorobeychik, Andrew Clark</p>
<p><a href='https://openreview.net/forum?id=1h2TAUEfc4'>https://openreview.net/forum?id=1h2TAUEfc4</a></p>
<p><b>Keywords</b>: Safety, Neural Barrier Function, Verification
</p><p><b>Compressor summary</b>: The paper proposes a method to verify the safety of neural control barrier functions with ReLU activation for nonlinear systems using piecewise linear segments and interval bound propagation.</p><hr><h3>Learning Unseen Modality Interaction</h3>
<p>Yunhua Zhang, Hazel Doughty, Cees G. M. Snoek</p>
<p><a href='https://openreview.net/forum?id=1g0A9kE8Id'>https://openreview.net/forum?id=1g0A9kE8Id</a></p>
<p><b>Keywords</b>: Multimodal Learning
</p><p><b>Compressor summary</b>: The paper proposes a method to learn from different modalities without assuming all combinations are available during training and shows its effectiveness on various tasks like video classification, robot state regression, and multimedia retrieval.</p><hr><h3>A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes</h3>
<p>Han Zhong, Tong Zhang</p>
<p><a href='https://openreview.net/forum?id=1bTG4sJ7tN'>https://openreview.net/forum?id=1bTG4sJ7tN</a></p>
<p><b>Keywords</b>: policy optimization, adversarial lienar MDPs, RL theory
</p><p><b>Compressor summary</b>: The paper proposes an optimistic PPO variant for linear MDPs and proves a state-of-the-art regret bound, while introducing novel algorithm design and analysis techniques.</p><hr><h3>Incentivized Communication for Federated Bandits</h3>
<p>Zhepei Wei, Chuanhao Li, Haifeng Xu, Hongning Wang</p>
<p><a href='https://openreview.net/forum?id=1aQivXgZKj'>https://openreview.net/forum?id=1aQivXgZKj</a></p>
<p><b>Keywords</b>: contextual bandit, federated learning, incentive mechanism
</p><p><b>Compressor summary</b>: The paper introduces a new federated bandit learning problem that considers self-interested clients and proposes a near-optimal incentivized communication protocol called Inc-FedUCB.</p><hr><h3>Unified Lower Bounds for Interactive High-dimensional Estimation under Information Constraints</h3>
<p>Jayadev Acharya, Clement Louis Canonne, Ziteng Sun, Himanshu Tyagi</p>
<p><a href='https://openreview.net/forum?id=1ZzG6td0el'>https://openreview.net/forum?id=1ZzG6td0el</a></p>
<p><b>Keywords</b>: statistical estimation; interactivity; local differential privacy; communication constraint
</p><p><b>Compressor summary</b>: The paper develops a framework to derive lower bounds on distributed parameter estimation under various constraints and for different types of distributions using interactive protocols and shows its versatility and effectiveness.</p><hr><h3>Convolutional State Space Models for Long-Range Spatiotemporal Modeling</h3>
<p>Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott Linderman, Wonmin Byeon</p>
<p><a href='https://openreview.net/forum?id=1ZvEtnrHS1'>https://openreview.net/forum?id=1ZvEtnrHS1</a></p>
<p><b>Keywords</b>: spatiotemporal modeling, ConvLSTM, RNN, state spaces, SSM, S4, S5, long-range dependencies, video prediction
</p><p><b>Compressor summary</b>: ConvS5 is a fast and efficient model that combines ConvLSTM tensor modeling with state space methods for long-range spatiotemporal sequence generation.</p><hr><h3>Langevin Quasi-Monte Carlo</h3>
<p>Sifan Liu</p>
<p><a href='https://openreview.net/forum?id=1YEF6TA8Di'>https://openreview.net/forum?id=1YEF6TA8Di</a></p>
<p><b>Keywords</b>: Completely uniformly distributed; log-concave sampling; low-discrepancy; MCMC;
</p><p><b>Compressor summary</b>: LMC can reduce estimation error by using quasi-random samples from CUD sequences for generating Gaussian perturbations.</p><hr><h3>Tanh Works Better with Asymmetry</h3>
<p>Dongjin Kim, Woojeong Kim, Suhyun Kim</p>
<p><a href='https://openreview.net/forum?id=1WpmOipyYI'>https://openreview.net/forum?id=1WpmOipyYI</a></p>
<p><b>Keywords</b>: Batch Normalization, Activation Functions, Saturation, Sparsity
</p><p><b>Compressor summary</b>: Swapping the order of Batch Normalization and bounded activation functions like Tanh improves performance due to increased asymmetry, sparsity, and a modified Tanh function with consistent asymmetry.</p><hr><h3>Robust Knowledge Transfer in Tiered Reinforcement Learning</h3>
<p>Jiawei Huang, Niao He</p>
<p><a href='https://openreview.net/forum?id=1WMdoiVMov'>https://openreview.net/forum?id=1WMdoiVMov</a></p>
<p><b>Keywords</b>: Reinforcement Learning Theory, Transfer RL, Tiered RL
</p><p><b>Compressor summary</b>: The paper explores how to transfer knowledge between different tasks in parallel reinforcement learning, focusing on a condition called Optimal Value Dominance and proposing novel algorithms for near-optimal performance.</p><hr><h3>Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds</h3>
<p>Naoki Nishikawa, Yuichi Ike, Kenji Yamanishi</p>
<p><a href='https://openreview.net/forum?id=1TJaITmK2Q'>https://openreview.net/forum?id=1TJaITmK2Q</a></p>
<p><b>Keywords</b>: point cloud, persistence homology, isometry-invariant networks, filtration learning
</p><p><b>Compressor summary</b>: The paper proposes a framework that learns an adaptive filtration for point clouds using neural networks and shows its effectiveness in enhancing machine learning accuracy for various applications.</p><hr><h3>CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs</h3>
<p>Guangyao Zhai, Evin Pinar Örnek, Shun-Cheng Wu, Yan Di, Federico Tombari, Nassir Navab, Benjamin Busam</p>
<p><a href='https://openreview.net/forum?id=1SF2tiopYJ'>https://openreview.net/forum?id=1SF2tiopYJ</a></p>
<p><b>Keywords</b>: Scene Graph, Scene Synthesis, Diffusion Model, Graph Convolution Network
</p><p><b>Compressor summary</b>: CommonScenes is a model that converts scene graphs into realistic 3D scenes with controllable objects, using variational auto-encoder and latent diffusion, and improves generation consistency, quality, and diversity compared to other methods.</p><hr><h3>Single-Stage Visual Query Localization in Egocentric Videos</h3>
<p>Hanwen Jiang, Santhosh Kumar Ramakrishnan, Kristen Grauman</p>
<p><a href='https://openreview.net/forum?id=1SAzP7W43j'>https://openreview.net/forum?id=1SAzP7W43j</a></p>
<p><b>Keywords</b>: Visual Query Localization, Egocentric Video, Spatial-Temporal Correspondence, Episodic Memory
</p><p><b>Compressor summary</b>: VQLoC is a single-stage framework that uses joint query-to-frame and frame-to-frame correspondences to perform visual query Localization on long-form egocentric videos faster and more accurately than prior methods.</p><hr><h3>Gaussian Partial Information Decomposition: Bias Correction and Application to High-dimensional Data</h3>
<p>Praveen Venkatesh, Corbett Bennett, Sam Gale, Tamina K. Ramirez, Greggory Heller, Severine Durand, Shawn R Olsen, Stefan Mihalas</p>
<p><a href='https://openreview.net/forum?id=1PnSOKQKvq'>https://openreview.net/forum?id=1PnSOKQKvq</a></p>
<p><b>Keywords</b>: partial information decomposition, estimation, bias, inter-area interaction, neuroscience
</p><p><b>Compressor summary</b>: The paper presents a new method for efficiently computing partial information decompositions on multivariate Gaussian distributions and evaluates its performance on simulated and real mouse brain data.</p><hr><h3>Budgeting Counterfactual for Offline RL</h3>
<p>Yao Liu, Pratik Chaudhari, Rasool Fakoor</p>
<p><a href='https://openreview.net/forum?id=1MUxtSBUox'>https://openreview.net/forum?id=1MUxtSBUox</a></p>
<p><b>Keywords</b>: reinforcement learning, offline reinforcement learning, counterfactual reasoning
</p><p><b>Compressor summary</b>: The paper proposes a method for offline reinforcement learning that uses dynamic programming to limit counterfactual reasoning and extrapolation errors, achieving better performance than existing approaches on D4RL benchmarks.</p><hr><h3>Re-Think and Re-Design Graph Neural Networks in Spaces of Continuous Graph Diffusion Functionals</h3>
<p>Tingting Dan, Jiaqi Ding, Ziquan Wei, Shahar Z Kovalsky, Minjeong Kim, Won Hwa Kim, Guorong Wu</p>
<p><a href='https://openreview.net/forum?id=1M8nDkUU9b'>https://openreview.net/forum?id=1M8nDkUU9b</a></p>
<p><b>Keywords</b>: graph neural networks (GNNs), total variation (TV), Euler–Lagrange equation, calculus of variations, over-smoothing, min-max optimization
</p><p><b>Compressor summary</b>: The paper presents a general framework for enhancing graph neural networks (GNNs) based on continuous diffusion and variational analysis, which improves their ability to model long-range dependencies and global patterns in graphs, as well as overcomes the over-smoothing problem. The paper also introduces a novel GAN to predict spreading flows in graphs using neural transport equation.</p><hr><h3>Blurred-Dilated Method for Adversarial Attacks</h3>
<p>Yang Deng, Weibin Wu, Jianping Zhang, Zibin Zheng</p>
<p><a href='https://openreview.net/forum?id=1JlAV2paGu'>https://openreview.net/forum?id=1JlAV2paGu</a></p>
<p><b>Keywords</b>: Transferable adversarial example
</p><p><b>Compressor summary</b>: The paper proposes a new method called Blurred-Dilated (BD) that modifies the source model to generate adversarial examples for DNNs in black-box settings, improving transferability and effectiveness against other target models.</p><hr><h3>Banana: Banach Fixed-Point Network for Pointcloud Segmentation with Inter-Part Equivariance</h3>
<p>Congyue Deng, Jiahui Lei, Bokui Shen, Kostas Daniilidis, Leonidas Guibas</p>
<p><a href='https://openreview.net/forum?id=1IOU2329Za'>https://openreview.net/forum?id=1IOU2329Za</a></p>
<p><b>Keywords</b>: 3D deep learning, equivariant network, pointcloud segmentation, multi-body system
</p><p><b>Compressor summary</b>: Banana is a Banach fixed-point network that enables equivariant segmentation by co-evolving part assignment and per-part SE(3)-equivariance in complex systems.</p><hr><h3>Learning and Collusion in Multi-unit Auctions</h3>
<p>Simina Branzei, Mahsa Derakhshan, Negin Golrezaei, Yanjun Han</p>
<p><a href='https://openreview.net/forum?id=1HKJ3lPz6m'>https://openreview.net/forum?id=1HKJ3lPz6m</a></p>
<p><b>Keywords</b>: multi-unit auctions, repeated auctions, online learning, collusion, games and learning, lower bounds, multiplicative weight updates, bandit learning
</p><p><b>Compressor summary</b>: The paper studies repeated multi-unit auctions with uniform pricing for CO2 emissions licenses, and analyzes their efficiency, regret, and collusion-proneness.</p><hr><h3>On Computing Pairwise Statistics with Local Differential Privacy</h3>
<p>Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Adam Sealfon</p>
<p><a href='https://openreview.net/forum?id=1GxKVprbwM'>https://openreview.net/forum?id=1GxKVprbwM</a></p>
<p><b>Keywords</b>: differential privacy, local differential privacy, pairwise statistics
</p><p><b>Compressor summary</b>: The authors develop methods to compute pairwise statistics of user inputs while preserving differential privacy in a local model setting.</p><hr><h3>Adapting Neural Link Predictors for Data-Efficient Complex Query Answering</h3>
<p>Erik Arakelyan, Pasquale Minervini, Daniel Daza, Michael Cochez, Isabelle Augenstein</p>
<p><a href='https://openreview.net/forum?id=1G7CBp8o7L'>https://openreview.net/forum?id=1G7CBp8o7L</a></p>
<p><b>Keywords</b>: complex query answering, neural link prediction, knowledge graph embeddings, knowledge graphs, relational learning, adapters
</p><p><b>Compressor summary</b>: CQD$^{\mathcal{A}}$ is a score adaptation model that improves the accuracy of complex query answering on incomplete knowledge graphs by recalibrating neural link prediction scores with minimal parameter increase and supporting negation reasoning.</p><hr><h3>How a Student becomes a Teacher: learning and forgetting through Spectral methods</h3>
<p>Lorenzo Giambagli, Lorenzo Buffoni, Lorenzo Chicchi, Duccio Fanelli</p>
<p><a href='https://openreview.net/forum?id=1FVmMlifl7'>https://openreview.net/forum?id=1FVmMlifl7</a></p>
<p><b>Keywords</b>: Network Slimming, Spectral Analysis, Node Pruning, Teacher-Student
</p><p><b>Compressor summary</b>: The paper proposes a new optimization method for student networks in machine learning that allows identifying and isolating an invariant subnetwork that matches the complexity of the teacher network, without degrading performance.</p><hr><h3>Large language models transition from integrating across position-yoked, exponential windows to structure-yoked, power-law windows</h3>
<p>David Skrill, Samuel Victor Norman-Haignere</p>
<p><a href='https://openreview.net/forum?id=1EYKYJeZtR'>https://openreview.net/forum?id=1EYKYJeZtR</a></p>
<p><b>Keywords</b>: language modeling, temporal integration, transformers, timescales, model interpretation
</p><p><b>Compressor summary</b>: The paragraph discusses how language models integrate linguistic information over time and how their integration patterns resemble those of human brains, with structure-dependent exponential and power-law windows across different layers.</p><hr><h3>HeadSculpt: Crafting 3D Head Avatars with Text</h3>
<p>Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang, Kwan-Yee K. Wong</p>
<p><a href='https://openreview.net/forum?id=1DmP6ySKYq'>https://openreview.net/forum?id=1DmP6ySKYq</a></p>
<p><b>Keywords</b>: 3D generative model, head avatar, diffusion models, neural rendering
</p><p><b>Compressor summary</b>: The paragraph discusses a new method called HeadSculpt that can create and edit high-quality 3D head avatars from textual descriptions by incorporating 3D awareness and fine-grained editing capabilities.</p><hr><h3>Cascading Contextual Assortment Bandits</h3>
<p>Hyunjun Choi, Rajan Udwani, Min-hwan Oh</p>
<p><a href='https://openreview.net/forum?id=1DTCoyAFiV'>https://openreview.net/forum?id=1DTCoyAFiV</a></p>
<p><b>Keywords</b>: cascade bandit, assortment bandit, upper confidence bound, exploration and exploitation, combinatorial optimization
</p><p><b>Compressor summary</b>: The text introduces a new combinatorial bandit model called cascading contextual assortment bandit and two UCB-based algorithms for it that improve existing regret bounds and dependence on problem constants.</p><hr><h3>Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations</h3>
<p>Hammaad Adam, Fan Yin, Mary Hu, Neil Tenenholtz, Lorin Crawford, Lester Mackey, Allison Koenecke</p>
<p><a href='https://openreview.net/forum?id=1CpVHL10fh'>https://openreview.net/forum?id=1CpVHL10fh</a></p>
<p><b>Keywords</b>: Randomized experiments, heterogeneous effects, causal machine learning, fairness, sequential testing, clinical trials, A/B testing
</p><p><b>Compressor summary</b>: The paper introduces CLASH, a machine learning method to stop randomized experiments early when the treatment harms different groups of participants unevenly.</p><hr><h3>PoET: A generative model of protein families as sequences-of-sequences</h3>
<p>Timothy Fei Truong Jr, Tristan Bepler</p>
<p><a href='https://openreview.net/forum?id=1CJ8D7P8RZ'>https://openreview.net/forum?id=1CJ8D7P8RZ</a></p>
<p><b>Keywords</b>: protein fitness prediction, transformer, retrieval, language model, MSA, generative model, protein engineering
</p><p><b>Compressor summary</b>: PoET is a generative model that learns to create related proteins from any family by using a Transformer layer and attention mechanism, improving variant function prediction and sequence generation.</p><hr><h3>De novo Drug Design using Reinforcement Learning with Multiple GPT Agents</h3>
<p>Xiuyuan Hu, Guoqing Liu, Yang Zhao, Hao Zhang</p>
<p><a href='https://openreview.net/forum?id=1B6YKnHYBb'>https://openreview.net/forum?id=1B6YKnHYBb</a></p>
<p><b>Keywords</b>: De novo drug design, Molecular generation, Multi-agent reinforcement learning, GPT
</p><p><b>Compressor summary</b>: The paper introduces MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents for diverse molecular generation in drug design, and shows its effectiveness in designing SARS-CoV-2 inhibitors.</p><hr><h3>Task-aware Distributed Source Coding under Dynamic Bandwidth</h3>
<p>Po-han Li, Sravan Kumar Ankireddy, Ruihan Zhao, Hossein Nourkhiz Mahjoub, Ehsan Moradi Pari, ufuk topcu, Sandeep P. Chinchali, Hyeji Kim</p>
<p><a href='https://openreview.net/forum?id=1A4ZqTmnye'>https://openreview.net/forum?id=1A4ZqTmnye</a></p>
<p><b>Keywords</b>: Data Compression, Distributed Source Coding, Semantic Communication, Multi-sensor Networks, Bandwidth Allocation, Information Theory
</p><p><b>Compressor summary</b>: The authors propose a novel distributed compression framework, NDPCA, that learns low-rank task representations and flexibly adapts to varying available bandwidth in multi-sensor networks, improving performance on tasks like robotic arm manipulation and object detection.</p><hr><h3>Instructing Goal-Conditioned Reinforcement Learning Agents with Temporal Logic Objectives</h3>
<p>Wenjie Qiu, Wensen Mao, He Zhu</p>
<p><a href='https://openreview.net/forum?id=19AgWnmyoV'>https://openreview.net/forum?id=19AgWnmyoV</a></p>
<p><b>Keywords</b>: Goal-Conditioned Reinforcement Learning, Linear Temporal Logic
</p><p><b>Compressor summary</b>: The paper proposes a new method for teaching reinforcement learning agents to follow complex temporal logic instructions without extra training, using a simple technique that works with any regular expression.</p><hr><h3>Deciphering Spatio-Temporal Graph Forecasting: A Causal Lens and Treatment</h3>
<p>Yutong Xia, Yuxuan Liang, Haomin Wen, Xu Liu, Kun Wang, Zhengyang Zhou, Roger Zimmermann</p>
<p><a href='https://openreview.net/forum?id=17Zkztjlgt'>https://openreview.net/forum?id=17Zkztjlgt</a></p>
<p><b>Keywords</b>: Spatio-temporal forecasting
</p><p><b>Compressor summary</b>: CaST is a novel framework that uses causal treatments to improve Spatio-Temporal Graph Neural Networks' forecasting performance and address temporal out-of-distribution issues and dynamic spatial causation.</p><hr><h3>Towards Understanding the Dynamics of Gaussian-Stein Variational Gradient Descent</h3>
<p>Tianle Liu, Promit Ghosal, Krishna Balasubramanian, Natesh S. Pillai</p>
<p><a href='https://openreview.net/forum?id=14ZM7FfPx8'>https://openreview.net/forum?id=14ZM7FfPx8</a></p>
<p><b>Keywords</b>: Stein variational gradient descent, Gaussian variational inference, Rates of Convergence
</p><p><b>Compressor summary</b>: SVGD is a nonparametric sampling algorithm whose Gaussian variant has theoretical properties and practical advantages, including convergence to the target distribution in certain cases.</p><hr><h3>Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction</h3>
<p>Yangqing Fu, Ming Sun, Buqing Nie, Yue Gao</p>
<p><a href='https://openreview.net/forum?id=0zeLTZAqaJ'>https://openreview.net/forum?id=0zeLTZAqaJ</a></p>
<p><b>Keywords</b>: reinforcement learning, mento carlo tree search, state abstraction
</p><p><b>Compressor summary</b>: The PTSA algorithm improves the efficiency of MCTS-based algorithms like AlphaGo and MuZero by reducing the search space and providing theoretical guarantees, leading to faster training on various tasks.</p><hr><h3>Fine-Grained Theoretical Analysis of Federated Zeroth-Order Optimization</h3>
<p>Jun Chen, Hong Chen, Bin Gu, Hao Deng</p>
<p><a href='https://openreview.net/forum?id=0ycX03sMAT'>https://openreview.net/forum?id=0ycX03sMAT</a></p>
<p><b>Keywords</b>: Federated zeroth-order optimization, stability analysis, theoretical guarantee, non-convex optimization, sub-Weibull distribution
</p><p><b>Compressor summary</b>: The paper develops a theory for FedZO, a black-box optimization method that combines zeroth-order optimization and federated learning, by analyzing its stability, generalization error bound, and convergence rate in both synchronous and asynchronous settings.</p><hr><h3>On the Convergence of No-Regret Learning Dynamics in Time-Varying Games</h3>
<p>Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, Tuomas Sandholm</p>
<p><a href='https://openreview.net/forum?id=0x2Ou3xHbH'>https://openreview.net/forum?id=0x2Ou3xHbH</a></p>
<p><b>Keywords</b>: no-regret learning, optimistic gradient descent, time-varying games, dynamic regret
</p><p><b>Compressor summary</b>: The paper studies how optimistic gradient descent learns in time-varying multiagent settings and provides convergence bounds for equilibrium gaps and second-order variations.</p><hr><h3>Lovász Principle for Unsupervised Graph Representation Learning</h3>
<p>Ziheng Sun, Chris Ding, Jicong Fan</p>
<p><a href='https://openreview.net/forum?id=0vdEHDwamk'>https://openreview.net/forum?id=0vdEHDwamk</a></p>
<p><b>Keywords</b>: Lovász Number, graph-level representation learning, unsupervised learning, semi-supervised learning
</p><p><b>Compressor summary</b>: The paper introduces a new graph representation method, Lovász principle, based on graph theory concepts that can be improved with neural networks and outperforms existing methods in some tasks.</p><hr><h3>The Adversarial Consistency of Surrogate Risks for Binary Classification</h3>
<p>Natalie Frank, Jonathan Niles-Weed</p>
<p><a href='https://openreview.net/forum?id=0uARg5G04K'>https://openreview.net/forum?id=0uARg5G04K</a></p>
<p><b>Keywords</b>: Adversarial learning, surrogate risks, optimal transport
</p><p><b>Compressor summary</b>: The paper investigates which surrogate losses can replace the original loss function without changing the minimizing sequences for robust binary classification under adversarial attacks.</p><hr><h3>Two-Stage Predict+Optimize for MILPs with Unknown Parameters in Constraints</h3>
<p>Xinyi HU, Jasper C.H. Lee, Jimmy H.M. Lee</p>
<p><a href='https://openreview.net/forum?id=0tnhFpyWjb'>https://openreview.net/forum?id=0tnhFpyWjb</a></p>
<p><b>Keywords</b>: Constraint optimization, Predict+Optimize
</p><p><b>Compressor summary</b>: The paper introduces Two-Stage Predict+Optimize, a new framework for end-to-end training of supervised learning models to handle unknown parameters in both optimization objectives and constraints, with improved prediction performance over existing methods.</p><hr><h3>Collaborative Score Distillation for Consistent Visual Editing</h3>
<p>Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, Jinwoo Shin</p>
<p><a href='https://openreview.net/forum?id=0tEjORCGFD'>https://openreview.net/forum?id=0tEjORCGFD</a></p>
<p><b>Keywords</b>: Score Distillation Sampling, Diffusion model, Editing
</p><p><b>Compressor summary</b>: Collaborative Score Distillation (CSD) is a novel method that uses Stein Variational Gradient Descent to achieve consistent visual synthesis across multiple images for various editing tasks.</p><hr><h3>Transformer-based Planning for Symbolic Regression</h3>
<p>Parshin Shojaee, Kazem Meidani, Amir Barati Farimani, Chandan K. Reddy</p>
<p><a href='https://openreview.net/forum?id=0rVXQEeFEL'>https://openreview.net/forum?id=0rVXQEeFEL</a></p>
<p><b>Keywords</b>: Symbolic Regression, Transformers, Planning, Deep Learning
</p><p><b>Compressor summary</b>: TPSR is a new method for symbolic regression that combines transformers with Monte Carlo Tree Search, improving equation accuracy, complexity, extrapolation, and robustness.</p><hr><h3>Convex-Concave Zero-Sum Stochastic Stackelberg Games</h3>
<p>Denizalp Goktas, Arjun Prakash, Amy Greenwald</p>
<p><a href='https://openreview.net/forum?id=0rEJx5QAxt'>https://openreview.net/forum?id=0rEJx5QAxt</a></p>
<p><b>Keywords</b>: Stackelberg games, Equilibrium Computation, Policy Gradient
</p><p><b>Compressor summary</b>: The paper proposes policy gradient methods for solving zero-sum stochastic Stackelberg games with noisy gradients, proves convergence to Stackelberg equilibrium for convex-concave games, and shows that this approach leads to safer and more effective solutions for reach-avoid problems.</p><hr><h3>Graph Contrastive Learning with Stable and Scalable Spectral Encoding</h3>
<p>Deyu Bo, Yuan Fang, Yang Liu, Chuan Shi</p>
<p><a href='https://openreview.net/forum?id=0kz5RmHxmE'>https://openreview.net/forum?id=0kz5RmHxmE</a></p>
<p><b>Keywords</b>: Graph Contrastive Learning, Spectral Embedding
</p><p><b>Compressor summary</b>: EigenMLP is a spectral encoder for graph contrastive learning that is stable, scalable, and invariant to transformations, while Sp$^{2}$GCL fuses spatial and spectral views for better representation learning.</p><hr><h3>Model Sparsity Can Simplify Machine Unlearning</h3>
<p>Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, Sijia Liu</p>
<p><a href='https://openreview.net/forum?id=0jZH883i34'>https://openreview.net/forum?id=0jZH883i34</a></p>
<p><b>Keywords</b>: Machine unlearning, model pruning
</p><p><b>Compressor summary</b>: The text introduces model sparsification via weight pruning as a novel approach to improve machine unlearning efficiency and performance in various scenarios, such as defending against backdoor attacks and enhancing transfer learning.</p><hr><h3>Described Object Detection: Liberating Object Detection with Flexible Expressions</h3>
<p>Chi Xie, Zhao Zhang, Yixuan Wu, Feng Zhu, Rui Zhao, Shuang Liang</p>
<p><a href='https://openreview.net/forum?id=0hwq2vOHT4'>https://openreview.net/forum?id=0hwq2vOHT4</a></p>
<p><b>Keywords</b>: open-vocabulary object detection, referring expression comprehension, multi-modal detection
</p><p><b>Compressor summary</b>: The paper introduces a new task called Described Object Detection (DOD) that extends Open-Vocabulary object Detection (OVD) and Referring Expression Comprehension (REC) by using more flexible language expressions, and presents a new dataset ($D^3$) to evaluate it.</p><hr><h3>Rethinking Semi-Supervised Imbalanced Node Classification from Bias-Variance Decomposition</h3>
<p>Liang Yan, Gengchen Wei, Chen Yang, Shengzhong Zhang, Zengfeng Huang</p>
<p><a href='https://openreview.net/forum?id=0gvtoxhvMY'>https://openreview.net/forum?id=0gvtoxhvMY</a></p>
<p><b>Keywords</b>: Imbalanced Node Classification, Bias-Variance Decomposition, Graph Neural Networks
</p><p><b>Compressor summary</b>: The paper proposes a method to handle class imbalance in graph neural networks using data augmentation, variance decomposition, and regularization, and shows its effectiveness on several benchmark datasets.</p><hr><h3>Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message Passing</h3>
<p>Jung Yeon Park, Lawson L.S. Wong, Robin Walters</p>
<p><a href='https://openreview.net/forum?id=0eXniewIvr'>https://openreview.net/forum?id=0eXniewIvr</a></p>
<p><b>Keywords</b>: message passing, dynamics, mesh, symmetry, equivariance
</p><p><b>Compressor summary</b>: The paragraph discusses a new gauge equivariant architecture using nonlinear message passing for modeling surface PDEs on meshes that leverages surface geometry and outperforms existing methods for complex and nonlinear dynamics.</p><hr><h3>A Finite-Particle Convergence Rate for Stein Variational Gradient Descent</h3>
<p>Jiaxin Shi, Lester Mackey</p>
<p><a href='https://openreview.net/forum?id=0eRDQQK2TW'>https://openreview.net/forum?id=0eRDQQK2TW</a></p>
<p><b>Keywords</b>: Stein Variational Gradient Descent, SVGD, variational inference, sampling, optimization, Stein's method
</p><p><b>Compressor summary</b>: The paper studies SVGD's convergence rate and shows it has an order of ${1/}{\sqrt{\log\log n}}$ when the target distribution is sub-Gaussian with a Lipschitz score.</p><hr><h3>SAMoSSA:  Multivariate Singular Spectrum Analysis with Stochastic Autoregressive Noise</h3>
<p>Abdullah Omar Alomar, Munther A. Dahleh, Sean Mann, Devavrat Shah</p>
<p><a href='https://openreview.net/forum?id=0e4eiXoUn5'>https://openreview.net/forum?id=0e4eiXoUn5</a></p>
<p><b>Keywords</b>: Time series, System Identification, Singular Spectrum Analysis
</p><p><b>Compressor summary</b>: The paper proposes a two-stage algorithm, SAMoSSA, that combines multivariate Singular Spectrum Analysis and Autoregressive models to estimate non-stationary trends and stationary noise in time series data, and provides theoretical guarantees for its forecasting performance.</p><hr><h3>Score-based Generative Models with Lévy Processes</h3>
<p>Eunbi Yoon, Keehun Park, Sungwoong Kim, Sungbin Lim</p>
<p><a href='https://openreview.net/forum?id=0Wp3VHX0Gm'>https://openreview.net/forum?id=0Wp3VHX0Gm</a></p>
<p><b>Keywords</b>: Generative Model, Score-based Method, Lévy processes
</p><p><b>Compressor summary</b>: The Lévy-Itō Model (LIM) is a novel score-based generative model that uses heavy-tailed Lévy processes to overcome the limitations of Brownian motion, achieving faster and more diverse sampling with high fidelity on various image datasets.</p><hr><h3>No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions</h3>
<p>Tiancheng Jin, Junyan Liu, Chloé Rouyer, William Chang, Chen-Yu Wei, Haipeng Luo</p>
<p><a href='https://openreview.net/forum?id=0WLMVDdvDF'>https://openreview.net/forum?id=0WLMVDdvDF</a></p>
<p><b>Keywords</b>: reinforcement Learning, best of both worlds, MDP, robust RL, adversarial corruption
</p><p><b>Compressor summary</b>: The paper presents algorithms that can learn in adversarial Markov Decision Processes with smoothly increasing regret depending on the degree of maliciousness of the adversary, while handling adversarial transitions and losses.</p><hr><h3>Improved Frequency Estimation Algorithms with and without Predictions</h3>
<p>Anders Aamand, Justin Y. Chen, Huy Nguyen, Sandeep Silwal, Ali Vakilian</p>
<p><a href='https://openreview.net/forum?id=0VcvYQ3uPh'>https://openreview.net/forum?id=0VcvYQ3uPh</a></p>
<p><b>Keywords</b>: learning-augmented algorithms, algorithms with predictions, data-driven algorithms, sublinear, streaming, frequency estimation, sketching
</p><p><b>Compressor summary</b>: The paper proposes a novel frequency estimation algorithm that outperforms existing methods without predictions and further improves with heavy-hitter predictions.</p><hr><h3>A Fast and Accurate Estimator for Large Scale Linear Model via Data Averaging</h3>
<p>Rui Wang, Yanyan Ouyang, Panpan Yu, Wangli Xu</p>
<p><a href='https://openreview.net/forum?id=0Tq1RGJBid'>https://openreview.net/forum?id=0Tq1RGJBid</a></p>
<p><b>Keywords</b>: Big data, Data averaging, Order statistic, Sampling method, Sketching method.
</p><p><b>Compressor summary</b>: This paper studies how to estimate linear models with large data sets and varying dimensions, using sketching techniques and a new averaging-based method that improves upon existing methods in terms of speed and accuracy.</p><hr><h3>Efficient Diffusion Policies For Offline Reinforcement Learning</h3>
<p>Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, Shuicheng YAN</p>
<p><a href='https://openreview.net/forum?id=0P6uJtndWu'>https://openreview.net/forum?id=0P6uJtndWu</a></p>
<p><b>Keywords</b>: Offline Reinforcement Learning, Diffusion Models
</p><p><b>Compressor summary</b>: Efficient diffusion policy (EDP) improves offline reinforcement learning by addressing the limitations of diffusion models and enabling compatibility with maximum likelihood-based RL algorithms, achieving state-of-the-art results on D4RL benchmark tasks.</p><hr><h3>Pruning vs Quantization: Which is Better?</h3>
<p>Andrey Kuzmin, Markus Nagel, Mart Van Baalen, Arash Behboodi, Tijmen Blankevoort</p>
<p><a href='https://openreview.net/forum?id=0OU1ZXXxs5'>https://openreview.net/forum?id=0OU1ZXXxs5</a></p>
<p><b>Keywords</b>: Neural network quantization, neural network pruning, magnitude pruning, post-training quantization, quantization-aware training
</p><p><b>Compressor summary</b>: This paper compares neural network quantization and pruning techniques for compressing deep neural networks and shows that quantization usually performs better than pruning, except in rare cases where extreme compression is needed.</p><hr><h3>Improved Communication Efficiency in Federated Natural Policy Gradient via ADMM-based Gradient Updates</h3>
<p>Guangchen Lan, Han Wang, James Anderson, Christopher Brinton, Vaneet Aggarwal</p>
<p><a href='https://openreview.net/forum?id=0ORqsMY6OL'>https://openreview.net/forum?id=0ORqsMY6OL</a></p>
<p><b>Keywords</b>: reinforcement learning, federated learning
</p><p><b>Compressor summary</b>: FedNPG-ADMM is a framework that reduces communication complexity in federated reinforcement learning by using the alternating direction method of multipliers (ADMM) to approximate global policy gradient directions efficiently.</p><hr><h3>SaVeNet: A Scalable Vector Network for Enhanced Molecular Representation Learning</h3>
<p>Sarp Aykent, Tian Xia</p>
<p><a href='https://openreview.net/forum?id=0OImBCFsdf'>https://openreview.net/forum?id=0OImBCFsdf</a></p>
<p><b>Keywords</b>: geometric deep learning, molecule property prediction, geometric representation learning
</p><p><b>Compressor summary</b>: The SaVeNet framework is a more efficient and effective way to learn geometric features in molecules, overcoming challenges like computational inefficiency and limited generalizability.</p><hr><h3>Gaussian Mixture Solvers for Diffusion Models</h3>
<p>Hanzhong Allan Guo, Cheng Lu, Fan Bao, Tianyu Pang, Shuicheng YAN, Chao Du, Chongxuan Li</p>
<p><a href='https://openreview.net/forum?id=0NuseeBuB4'>https://openreview.net/forum?id=0NuseeBuB4</a></p>
<p><b>Keywords</b>: Diffusion models, SDE-based solver, Gaussian mixture, Stroke-based synthesis
</p><p><b>Compressor summary</b>: The paragraph introduces a new method called Gaussian Mixture Solvers (GMS) for improving sampling efficiency and quality in diffusion models, especially for image generation and stroke-based synthesis tasks.</p><hr><h3>ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection</h3>
<p>Zhongzhan Huang, Pan Zhou, Shuicheng YAN, Liang Lin</p>
<p><a href='https://openreview.net/forum?id=0N73P8pH2l'>https://openreview.net/forum?id=0N73P8pH2l</a></p>
<p><b>Keywords</b>: Diffusion Model, Stable Training, Network architectures
</p><p><b>Compressor summary</b>: The authors analyze the instability of UNet in diffusion models due to its long skip connects and propose a coefficient scaling framework that improves stability and robustness, leading to faster training.</p><hr><h3>Universal Prompt Tuning for Graph Neural Networks</h3>
<p>Taoran Fang, Yunchao Mercer Zhang, Yang Yang, Chunping Wang, Lei CHEN</p>
<p><a href='https://openreview.net/forum?id=0LmWBhIYLi'>https://openreview.net/forum?id=0LmWBhIYLi</a></p>
<p><b>Keywords</b>: graph neural networks, prompt tuning
</p><p><b>Compressor summary</b>: The paper introduces Graph Prompt Feature (GPF), a universal prompt-based tuning method for graph neural networks that works under any pre-training strategy and outperforms fine-tuning and specialized methods in various scenarios.</p><hr><h3>Dynamic Non-monotone Submodular Maximization</h3>
<p>Kiarash Banihashem, Leyla Biabani, Samira Goudarzi, MohammadTaghi Hajiaghayi, Peyman Jabbarzade, Morteza Monemizadeh</p>
<p><a href='https://openreview.net/forum?id=0K1ZTfHZ0N'>https://openreview.net/forum?id=0K1ZTfHZ0N</a></p>
<p><b>Keywords</b>: Non-monotone submodular maximization, dynamic algorithm, oracle query, video summarization
</p><p><b>Compressor summary</b>: The paper presents the first dynamic algorithms to solve non-monotone submodular maximization under cardinality constraints and demonstrates their effectiveness on real-world data sets.</p><hr><h3>Multi-Agent Meta-Reinforcement Learning: Sharper Convergence Rates with Task Similarity</h3>
<p>Weichao Mao, Haoran Qiu, Chen Wang, Hubertus Franke, Zbigniew Kalbarczyk, Ravi Iyer, Tamer Basar</p>
<p><a href='https://openreview.net/forum?id=0Iw2dLh8uq'>https://openreview.net/forum?id=0Iw2dLh8uq</a></p>
<p><b>Keywords</b>: Reinforcement learning, game theory, multi-agent systems, meta-learning
</p><p><b>Compressor summary</b>: This paper explores how meta-learning can improve multi-agent reinforcement learning in various game settings and provides theoretical and empirical evidence for its benefits.</p><hr><h3>Efficient Potential-based Exploration in Reinforcement Learning using Inverse Dynamic Bisimulation Metric</h3>
<p>YIMING WANG, Ming Yang, Renzhi Dong, Binbin Sun, Furui Liu, Leong Hou U</p>
<p><a href='https://openreview.net/forum?id=0FhKURbTyF'>https://openreview.net/forum?id=0FhKURbTyF</a></p>
<p><b>Keywords</b>: Reinforcement learning, reward shaping, potential-based exploration, inverse dynamic bisimulation metric
</p><p><b>Compressor summary</b>: LIBERTY is a potential-based exploration bonus for deep RL that uses bisimulation metric to measure state discrepancy, enhancing agent's discovery of novel states and improving training efficiency.</p><hr><h3>AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation</h3>
<p>Tong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, yelong shen, Jian Jiao, Juntao Li, zhongyu wei, Jian Guo, Nan Duan, Weizhu Chen</p>
<p><a href='https://openreview.net/forum?id=0EG6qUQ4xE'>https://openreview.net/forum?id=0EG6qUQ4xE</a></p>
<p><b>Keywords</b>: text generation, diffusion model, auto-regression, sequential dependency
</p><p><b>Compressor summary</b>: AR-Diffusion is a new diffusion model for text generation that uses dynamic denoising steps to account for the sequential dependency of natural language and achieves faster and better results than existing models on various tasks.</p><hr><h3>Adaptive Uncertainty Estimation via High-Dimensional Testing on Latent Representations</h3>
<p>Tsai Hor Chan, Kin Wai Lau, Jiajun Shen, Guosheng Yin, Lequan Yu</p>
<p><a href='https://openreview.net/forum?id=0DpKUzl1Se'>https://openreview.net/forum?id=0DpKUzl1Se</a></p>
<p><b>Keywords</b>: Bayesian deep learning, high-dimensional testing, uncertainty estimation, out-of-distribution detection
</p><p><b>Compressor summary</b>: The proposed framework uses data-adaptive high-dimensional hypothesis testing for uncertainty estimation in deep neural networks, without retraining the feature encoder, and improves performance on OOD detection and task-specific prediction.</p><hr><h3>Partial Label Learning with Dissimilarity Propagation guided Candidate Label Shrinkage</h3>
<p>Yuheng Jia, Fuchao Yang, Yongqiang Dong</p>
<p><a href='https://openreview.net/forum?id=0CbmvZPBGB'>https://openreview.net/forum?id=0CbmvZPBGB</a></p>
<p><b>Keywords</b>: partial label learning, dissimilarity propagation, candidate label shrinkage
</p><p><b>Compressor summary</b>: The paper proposes a method to disambiguate candidate labels in partial label learning using adversarial similarity and dissimilarity matrices, which improves performance and has theoretical guarantees.</p><hr><h3>Gaussian Process Probes (GPP) for Uncertainty-Aware Probing</h3>
<p>Zi Wang, Alexander Ku, Jason Michael Baldridge, Thomas L. Griffiths, Been Kim</p>
<p><a href='https://openreview.net/forum?id=0BwB03qA5T'>https://openreview.net/forum?id=0BwB03qA5T</a></p>
<p><b>Keywords</b>: Interpretability, probing, Bayesian, Gaussian process, transparency
</p><p><b>Compressor summary</b>: Gaussian process probes (GPP) is a framework that allows measuring uncertainty about concepts represented by models using a distribution over classifiers without needing access to training data or model details, which can be applied to any pre-trained model and helps understand and evaluate their capabilities.</p><hr><h3>Stochastic Multi-armed Bandits: Optimal Trade-off among Optimality, Consistency, and Tail Risk</h3>
<p>David Simchi-Levi, Zeyu Zheng, Feng Zhu</p>
<p><a href='https://openreview.net/forum?id=0BfQT652sC'>https://openreview.net/forum?id=0BfQT652sC</a></p>
<p><b>Keywords</b>: multi-armed bandit, worst-case optimality, instance-dependent consistency, light-tailed risk
</p><p><b>Compressor summary</b>: The paper studies how to design policies for the stochastic multi-armed bandit problem with three desired properties: worst-case optimality, instance-dependent consistency, and light-tailed risk, and proposes a novel policy that achieves optimal trade-offs among them.</p><hr><h3>Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models</h3>
<p>Guillermo Ortiz-Jimenez, Alessandro Favero, Pascal Frossard</p>
<p><a href='https://openreview.net/forum?id=0A9f2jZDGW'>https://openreview.net/forum?id=0A9f2jZDGW</a></p>
<p><b>Keywords</b>: model editing, transfer learning, neural tangent kernel, vision-language pre-training, deep learning science
</p><p><b>Compressor summary</b>: Task arithmetic improves vision-language model performance by leveraging weight disentanglement and linearizing the model's tangent space.</p><hr><h3>Online Ad Procurement in Non-stationary Autobidding Worlds</h3>
<p>Jason Cheuk Nam Liang, Haihao Lu, Baoyu Zhou</p>
<p><a href='https://openreview.net/forum?id=09bZyE9tfp'>https://openreview.net/forum?id=09bZyE9tfp</a></p>
<p><b>Keywords</b>: autobidding, online advertising, bandit online convex optimization, constrained optimization
</p><p><b>Compressor summary</b>: The paragraph describes an online learning framework for optimizing ad platform lever decisions in a realistic bandit feedback environment with non-stationary procurement outcomes, and presents a primal-dual algorithm that achieves low regret across various scenarios.</p><hr><h3>On the Powerfulness of Textual Outlier Exposure for Visual OoD Detection</h3>
<p>Sangha Park, Jisoo Mok, Dahuin Jung, Saehyung Lee, Sungroh Yoon</p>
<p><a href='https://openreview.net/forum?id=090ORrOAPL'>https://openreview.net/forum?id=090ORrOAPL</a></p>
<p><b>Keywords</b>: Out-of-distribution detection
</p><p><b>Compressor summary</b>: The paper explores using textual outliers instead of visual ones for Out-of-Distribution (OoD) detection in neural networks, introducing new methods to generate them and showing their competitive performance on benchmarks.</p><hr><h3>Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</h3>
<p>George Stein, Jesse C. Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Leigh Ross, Valentin Villecroze, Zhaoyan Liu, Anthony L. Caterini, Eric Taylor, Gabriel Loaiza-Ganem</p>
<p><a href='https://openreview.net/forum?id=08zf7kTOoh'>https://openreview.net/forum?id=08zf7kTOoh</a></p>
<p><b>Keywords</b>: generative models, generative model evaluation, self-supervised learning, representation learning, metrics
</p><p><b>Compressor summary</b>: The authors systematically evaluate generative models using human perception of image realism and 17 modern metrics, finding that existing metrics are flawed and propose improvements and a new dataset and library for future work.</p><hr><h3>Knowledge Diffusion for Distillation</h3>
<p>Tao Huang, Yuan Zhang, Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Chang Xu</p>
<p><a href='https://openreview.net/forum?id=08hStXdT1s'>https://openreview.net/forum?id=08hStXdT1s</a></p>
<p><b>Keywords</b>: knowledge distillation, diffusion models
</p><p><b>Compressor summary</b>: The paper proposes DiffKD, a novel knowledge distillation method that uses diffusion models to denoise and match features between teacher and student models, achieving state-of-the-art performance on various tasks.</p><hr><h3>Exploiting hidden structures in non-convex games for convergence to Nash equilibrium</h3>
<p>Iosif Sakos, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Panayotis Mertikopoulos, Georgios Piliouras</p>
<p><a href='https://openreview.net/forum?id=05P1U0jk8r'>https://openreview.net/forum?id=05P1U0jk8r</a></p>
<p><b>Keywords</b>: Nash Equilibrium, Games, Gradient, Non-monotone VI, Natural Gradient, Precondition
</p><p><b>Compressor summary</b>: The paper introduces PHGD, a first-order method that exploits latent convex structures in non-cooperative games to achieve convergence under minimal assumptions.</p><hr><h3>Robustness Guarantees for Adversarially Trained Neural Networks</h3>
<p>Poorya Mianjy, Raman Arora</p>
<p><a href='https://openreview.net/forum?id=02Uc0G2Cym'>https://openreview.net/forum?id=02Uc0G2Cym</a></p>
<p><b>Keywords</b>: Adversarial training, neural networks, robustness, guarantees
</p><p><b>Compressor summary</b>: The paper proposes a method to train robust neural networks against adversarial attacks by maximizing a lower bound on the loss function using projected gradient descent and provides convergence guarantees and empirical evidence.</p><hr><h3>Complexity Matters: Rethinking the Latent Space for Generative Modeling</h3>
<p>Tianyang Hu, Fei Chen, Haonan Wang, Jiawei Li, Wenjia Wang, Jiacheng Sun, Zhenguo Li</p>
<p><a href='https://openreview.net/forum?id=00EKYYu3fD'>https://openreview.net/forum?id=00EKYYu3fD</a></p>
<p><b>Keywords</b>: generative model, latent space, distance between distributions, generative adversarial network, vqgan
</p><p><b>Compressor summary</b>: The study proposes a novel method to find the optimal latent space for generative models by considering model complexity and improves the performance of various models with less complex ones.</p><hr><h3>Finite Population Regression Adjustment and Non-asymptotic Guarantees for Treatment Effect Estimation</h3>
<p>Mehrdad Ghadiri, David Arbour, Tung Mai, Cameron N Musco, Anup Rao</p>
<p><a href='https://openreview.net/forum?id=009LK0vLcY'>https://openreview.net/forum?id=009LK0vLcY</a></p>
<p><b>Keywords</b>: regression adjustment; treatment effect estimation; average treatment effect
</p><p><b>Compressor summary</b>: The paper explores regression adjustment in finite populations, using randomized numerical linear algebra to select subsets of subjects for experiments and providing non-asymptotic accuracy bounds for estimating sample mean, individual treatment effects, and average treatment effect.</p><hr><h3>Collaborative Alignment of NLP Models</h3>
<p>Fereshte Khani, Marco Tulio Ribeiro</p>
<p><a href='https://openreview.net/forum?id=f39Q3JyoIi'>https://openreview.net/forum?id=f39Q3JyoIi</a></p>
<p><b>Keywords</b>: alignment, collaborative alignment, debugging, nlp, interference, multi-user interaction
</p><p><b>Compressor summary</b>: CoAligment is a framework that helps multiple users align NLP models with their values by learning local and global models and generating instances to resolve disagreements.</p><hr><h3>Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models</h3>
<p>Tianxiang Gao, Xiaokai Huo, Hailiang Liu, Hongyang Gao</p>
<p><a href='https://openreview.net/forum?id=Z2he2Y0MoH'>https://openreview.net/forum?id=Z2he2Y0MoH</a></p>
<p><b>Keywords</b>: Gradient descent, deep equilibrium model, Gaussian processes, kernel methods, NNGP, NTK
</p><p><b>Compressor summary</b>: The paper analyzes deep equilibrium models (DEQs), infinite-depth neural networks that converge to Gaussian processes, and shows they have non-degenerate kernels, enabling training and generalization.</p><hr><h3>Auditing for Human Expertise</h3>
<p>Rohan Alur, Loren Laine, Darrick K Li, Manish Raghavan, Devavrat Shah, Dennis Shung</p>
<p><a href='https://openreview.net/forum?id=VEpU9rFaQr'>https://openreview.net/forum?id=VEpU9rFaQr</a></p>
<p><b>Keywords</b>: hypothesis testing, human-AI complementarity, machine learning for healthcare
</p><p><b>Compressor summary</b>: The authors propose a statistical test to determine if human experts add value in high-stakes prediction tasks beyond what an algorithm can capture, using the example of admissions decisions for patients with acute gastrointestinal bleeding.</p><hr><h3>Sub-optimality of the Naive Mean Field approximation for proportional high-dimensional Linear Regression</h3>
<p>Jiaze Qiu</p>
<p><a href='https://openreview.net/forum?id=OXhymu6MeN'>https://openreview.net/forum?id=OXhymu6MeN</a></p>
<p><b>Keywords</b>: Variational Bayes; Naive Mean Field; Gaussian comparison inequalities; High-dimensional statistics; Proportional asymptotic.
</p><p><b>Compressor summary</b>: The paper derives asymptotic characterizations for the NMF approximation in high-dimensional linear regression and shows its limitations in practice.</p><hr><h3>Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability</h3>
<p>Haotian Xue, Alexandre Araujo, Bin Hu, Yongxin Chen</p>
<p><a href='https://openreview.net/forum?id=JTwxylP6U9'>https://openreview.net/forum?id=JTwxylP6U9</a></p>
<p><b>Keywords</b>: Robustness, Adversarial Samples, Diffusion Model
</p><p><b>Compressor summary</b>: The paper proposes a new method called Diff-PGD for creating realistic adversarial samples that stay close to natural images while being effective against neural networks.</p>