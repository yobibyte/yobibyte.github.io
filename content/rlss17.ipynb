{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hi!**\n",
    "\n",
    "<img src=\"pics/rlss/cake.jpg\">\n",
    "\n",
    "I was rejected from [DLSS/RLSS](https://mila.quebec/en/cours/deep-learning-summer-school-2017/) this year, but I decided not to be stressed about it, watch all the lectures and make the summary of them. I understand, that a summer school is not only about the lectures, but I don't have more. Going through the lectures and writing up will still be useful for me. It might also be useful for some of you. Let's start!\n",
    "\n",
    "Sometimes, I tried to write my impression about the lectures or the related thoughts. Sometimes, the notes are the copypaste from the slides since I find this useful and it is nice to have all the stuff in one place.\n",
    "\n",
    "The sections go in their natural order as in the schedule. You can find the slides [here](https://mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/slides/) and the videos [here](http://videolectures.net/deeplearning2017_montreal/).\n",
    "\n",
    "If you see anything wrong or misunderstood, please, email [me](vitaliykurin@gmail.com) or find me on [twitter](@y0b1byte)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: basic concepts, *Joelle Pineau*\n",
    "\n",
    "*[Slides](http://videolectures.net/site/normal_dl/tag=1137927/deeplearning2017_pineau_reinforcement_learning_01.pdf)* |*[Video](http://videolectures.net/deeplearning2017_pineau_reinforcement_learning/)*\n",
    "\n",
    "The best introduction to RL I have seen so far. Heavily recommended. Even if you already know some stuff, it will be useful for you to have a more or less whole picture of the basics.\n",
    "\n",
    "In a short introduction, Joelle Pineau mentions the recent applications of RL found in [RLDM 2017](http://rldm.org/rldm2017/) submissions:\n",
    "\n",
    "* robotics\n",
    "* video games\n",
    "* conversational systems\n",
    "* medical intervention (x_X)\n",
    "* algorithm improvement\n",
    "* improvisational theater\n",
    "* autonomous driving\n",
    "* prosthetic arm control\n",
    "* financial trading\n",
    "* query completion\n",
    "\n",
    "As you can see, the list is really large. And this is only after looking through half of the accepted papers.\n",
    "\n",
    "The lecture proceeds with the question \"When should I apply RL to my problem\".\n",
    "The answer is the combination of the following prerequisites:\n",
    "\n",
    "* your data comes in the form of trajectories (forget about the i.i.d. assumption);\n",
    "* your system needs some kind of intervention;\n",
    "* you need feedback in order to understand if you are doing well or not\n",
    "* your problem is the one requiring both learning and planning\n",
    "\n",
    "The lecturer mentions, that RL is somehow similar to Supervised Learning, but not completely.\n",
    "There are some challenges, both practical and technical:\n",
    "\n",
    "* need some environment to operate in (one of the reasons for slow progress of RL in the past)\n",
    "* you need to learn and plan at the same time using correlated samples\n",
    "* your data distribution changes along with your learning procedure (the actions you take bring an agent to different states, and it makes the learning harder)\n",
    "\n",
    "Then, Joelle gives the formal representation of an RL problem as a Markov Decision Process (MDP), defined as a tuple $\\langle S,A,T(s,a,s'), R(s,a), \\mu(s) \\rangle$, where $S$ is the set of states, $A$ is the set of actions, $T(s,a,s')$ is the transition function returning the distribution over the next states $s'$ given the current state $s$ and the action taken $a$.\n",
    "\n",
    "M in MDP stands for 'Markov', i.e. the process holds the Markov assumption: the future is independent of the past given the future. What does it mean? Our next state depends only on the current one, we do not need to know the whole history of states in order to predict the future.\n",
    "And the definition of the *state* according to Joelle is the following. A *state* is a sufficient amount of information about the world in order to predict the future.\n",
    "Sometimes in the real life, the assumption does not hold, that's true. But RL still uses it in order to reduce the complexity.\n",
    "\n",
    "What is the goal of RL? We want to maximize the reward we get for our interaction with the environment.\n",
    "We can have two options here, either the task at hand is episodic (e.g. a game episode ends when you win or lose) or continuous (e.g. balancing).\n",
    "\n",
    "Usually, the future reward flow is discounted by the coefficient $\\gamma \\in [0,1)$ (usually close to 1.\n",
    "$\\gamma$ helps to trade off the balance between preferences in the immediate reward and the future reward.\n",
    "It is often said, that we discount the reward flow due to psychological reasons: humans prefer the immediate reward to the future reward.\n",
    "But, as Joelle mentions, it is much more mathematically convenient to use the discounting.\n",
    "\n",
    "We now go to one of the most important definitions in RL -- policy function. \n",
    "Policy $\\pi$ is a function, that returns an action given the state: $\\pi(s,a) = p(a_t = a | s_t = s)$.\n",
    "And to solve an RL problem is to find a policy which maximizes the expected future reward flow: $argmax_{\\pi} E_{\\pi} [r_0 + r_1 + ... + r_T | s_0]$.\n",
    "\n",
    "The value function of a state is an expected return of a policy starting from this particular state: $V_{\\pi}(s) = E_{\\pi} [r_t + r_{t+1} + ... + r_T | s_t = s]$.\n",
    "I don't get why, but all the definitions are given without the discounting. \n",
    "Maybe it does not matter here since when we take expectations later, we will be able to keep the gammas outside of the expectations, but I'm not sure. At Sutton & Barto's book, all the definitions and derivations are given for the discounted return. \n",
    "\n",
    "In order not to mess all the terms and definitions, the lecturer gives the following slide:\n",
    "\n",
    "* Reward is a one-step numerical feedback\n",
    "* Return is the sum of rewards of the agent's trajectory\n",
    "* Value is the expected sum of rewards over the agent's trajectory\n",
    "* Utility is the numerical function representing preferences (in RL *return* $\\equiv$ *utility*)\n",
    "\n",
    "Ok, let's go to the policy evaluation. What is the value of a policy? It is just the expected immediate reward plus the expected future reward: \n",
    "$V_{\\pi}(s) = E_{\\pi}[r_t + r_{t+1} + ... + r_T | s_t = s] = E_{\\pi}[r_t] + E_{\\pi}[r_{t+1} + ... + r_{T} | s_t = s]$.\n",
    "\n",
    "Let's rewrite the expectations now: \n",
    "\n",
    "$V_{\\pi}(s) = \\sum_{a \\in A}\\pi(s,a)R(s,a) + E_{\\pi}[r_{t+1} + ... + r_T | s_t = s]$ \n",
    "\n",
    "$V_{\\pi}(s) = \\sum_{a \\in A}\\pi(s,a)R(s,a) + \\sum_{a \\in A}\\pi(s,a)\\sum_{s' \\in S}T(s,a,s')E_{\\pi}[r_{t+1} + ... + r_T | s_{t+1} = s']$ \n",
    "\n",
    "And, looking at the definition of value function, we can see, that the the last expecation on the right hand side is just the value function of the state $s'$:\n",
    "\n",
    "\n",
    "$V_{\\pi}(s) = \\sum_{a \\in A}\\pi(s,a)R(s,a) + \\sum_{a \\in A}\\pi(s,a)\\sum_{s' \\in S}T(s,a,s')V_{\\pi}(s')$ \n",
    "\n",
    "\n",
    "From here we can see, that this is a dynamic programming algorithm.\n",
    "\n",
    "The lecturer uses the formulas with discounting now:\n",
    "\n",
    "$V_{\\pi}(s) = \\sum_{a \\in A}\\pi(s,a)[R(s,a) + \\gamma \\sum_{s' \\in S}T(s,a,s')V_{\\pi}(s')]$\n",
    "\n",
    "We also have to write the equation for the state-action value function $Q$ -- the function, returning the value of a state given that we take the particular action first and then follow the policy $\\pi$:\n",
    "\n",
    "$Q_{\\pi}(s,a) = R(s,a) + \\gamma \\sum_{s' \\in S}\\big[T(s,a,s')\\sum_{a' \\in A}[\\pi(s',a')Q_{\\pi}(s',a')]\\big]$\n",
    "\n",
    "The last two formulas are the two forms of Bellman's equation.\n",
    "\n",
    "We can rewrite the first one in the matrix form $V_{\\pi} = R_{\\pi} + \\gamma T_{\\pi}V_{\\pi}$.\n",
    "It has the unique solution $V_{\\pi} = (I - \\gamma T_{\\pi})^{-1}R_{\\pi}$.\n",
    "\n",
    "Let's now assume, that we have the fixed policy, how can we evaluate it? \n",
    "Let's somehow initialize the value function $V_{\\pi}$ (with zeroes, for instance). \n",
    "On each iteration, we update the value function for each state:\n",
    "\n",
    "$V_{k+1}(s) \\leftarrow (R(s,\\pi(s)) + \\gamma \\sum_{s' \\in S}T(s,\\pi(s), s')V_k(s')$, where $k$ is the index of the iteration.\n",
    "\n",
    "We repeat it until the value function does not update anymore or the number of updates is no more than some threshold. \n",
    "There is the derivation of convergence in the slides (slide #31), but I will not write it here. I will just say, that it uses the fact, that $\\gamma < 0$ to show, that\n",
    "the norm between the current approximation and the true value function contracts to zero. \n",
    "\n",
    "We move from the fixed policy to finding the best (optimal) policy.\n",
    "The optimal value function is the highers return we can get from the state: $V^{*}(s) = max_{\\pi}V_{\\pi}(s)$. \n",
    "A policy, that achieves $V^{*}$ is called an *optimal policy* $\\pi^*$.\n",
    "For each MDP there is a unique optimal value function. **BUT** the optimal policy is not necessarily unique.\n",
    "\n",
    "Having a solution to an MDP means having either the optimal value function $V^*$ or an optimal policy $\\pi^*$.\n",
    "We are saying that since if we have one of them, we can derive the other.\n",
    "\n",
    "We have already looked at the policy evaluation algorithm for a fixed policy. But how to find the best policy? There are two related algorithms: policy iteration and value iteration. \n",
    "\n",
    "Policy iteration goes as follows:\n",
    "\n",
    "* initialize a policy somehow, random is also possible\n",
    "* Repeat\n",
    "  * Compute $V_{\\pi}$ using Policy Evaluation algorithm\n",
    "  * Compute $\\pi'$ that is greedy with respect to $V_{\\pi}$\n",
    "  \n",
    "* terminate when $\\pi = \\pi'$\n",
    "\n",
    "\n",
    "A the value iteration:\n",
    "\n",
    "* initialize the value function $V_0(s)$\n",
    "* each iteration do the update $V_{k+1}(s) = max_{a \\in A}(R(s,a) + \\gamma \\sum_{s' \\in S}T(s,a,s')V_k(s'))$\n",
    "* stop when the value function changes for a step is below some threshold\n",
    "\n",
    "\n",
    "The complexities of the algorithms are the following ($S$ is the state space size, $A$ is the action space size):\n",
    "\n",
    "* policy evaluation: $O(s)^3$\n",
    "* policy iteration: $O(S^3 + S^2A)$ per iteration\n",
    "* value iteration: $O(S^2A)$ per iteration\n",
    "\n",
    "There is an example in the slides, but I will not put it here, but it's important to go through it if you think, you're confused about all the said above.\n",
    "\n",
    "We can see from the complexities, that the algorithms get less and less feasible as our state-action space scales, however, we can try not to update all the states in value iteration, but only the important ones. \n",
    "Moreover, we can do the asynchronous updates, generating trajectories through the MDP and update the states only when they appear on a trajectory. In policy iteration, we are not forced to do one policy update after each policy evaluation. We can combine the updates and evaluations in any combination we find appropriate.\n",
    "\n",
    "For those, who do not want to read the slides and watch the lectures anymore, but want to do the hardcore research, Joelle has the 'challenges' slide. I will also put them in a list:\n",
    "\n",
    "* Designing the problem domain\n",
    "  * state representation\n",
    "  * action choice\n",
    "  * cost/reward signal\n",
    "* acquiring data for training\n",
    "  * exploration/exploitation\n",
    "  * high cost actions\n",
    "  * time-delayed cost/reward signal\n",
    "* function approximation\n",
    "* validation/confidence measures\n",
    "\n",
    "The lecture proceeds with describing on-line learning, which can be of two types, according to the lecturer:\n",
    "\n",
    "* Monte-Carlo estimate, when we use the empirical return $U(s_t)$ as a target estimate for the actual value function: $V(s_t) = V(s_t) + \\alpha(U(s_t) - V(s_t)$\n",
    "* Temporal-Difference (TD) learning: $V(s_t) = V(s_t) + \\alpha [r_{t+1} + \\gamma V(s_{t+1} - V(s_t)] \\forall t = 0,1,2,...$\n",
    "\n",
    "As Joelle mentions, online learning is highly unstable, and a lot of recent RL research focused on improving the stability of the online learning algorithms.\n",
    "\n",
    "Up to now, the lecture assumed, we are in a tabular setup when the value function and the policy can be represented as a large table. But in the real world, this approach will never work since the problems are much harder. \n",
    "We need to use the function approximations.\n",
    "Linear functions have been used for a long time. \n",
    "Recently, using neural nets as function approximators has become very popular, and we can use all the Deep Learning progress within RL, e.g. memory augmented models [1].\n",
    "\n",
    "The lecture continues with describing the on-policy/off-policy dichotomy in RL. \n",
    "As we mentioned earlier, each policy change leads to data distribution change, so, when we evaluate several policies within the same batch, we need a large batch of data and a policy which adequately covers all (s,a) combinations. \n",
    "One of the solutions to the problem is to use importance sampling attaching different weights do data collected from different policies.\n",
    "\n",
    "Exploration/exploitation dilemma goes next.\n",
    "You've definitely heard about it. \n",
    "When you have some policy, you might follow it and get your deserved return or you can try something new to achieve, possibly, more. \n",
    "Though researchers have been trying to solve the problem for a long time, it is still far from being solved.\n",
    "\n",
    "In the end, Joelle mentions the two approaches to RL: model-based and model-free. The first tries to learn the model of the environment first and do the planning later. The second, that is more successful recently, is trying to learn a policy directly using the data from the environment. As for me, I find model-based approach very cool, but it is harder to learn. It has not been so hot recently, but the research is going on and, I hope, we will see great results in the near future. \n",
    "\n",
    "There was also an interesting question from the audience about choosing the discounting coefficient $\\gamma$. \n",
    "Joelle says that before she thought, that choosing the gamma is the problem of one who chooses the domain and creates the environment.\n",
    "But the community moves further and further to the fact, that $\\gamma$ is a hyperparameter and, maybe, we should be more aggressive at the beginning of the training when our estimations are too noisy. \n",
    "There were no literature pointers in the lecture, but I wrote an email, and Joelle sent me the link [2].\n",
    "\n",
    "The lecture is over, if you want to know more, either continue to read or find more awesome resources [here]( https://github.com/aikorea/awesome-rl).\n",
    "As for me, I want to add, that the lecture is great not only because of summarizing the basics of RL, but also giving some intuitions which help to understand the concepts better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Search for RL, *Pieter Abbeel*\n",
    "\n",
    "*[Slides](http://videolectures.net/site/normal_dl/tag=1137919/deeplearning2017_abbeel_policy_search_01.pdf)* |*[Video](http://videolectures.net/deeplearning2017_abbeel_policy_search/)*\n",
    "\n",
    "\n",
    "Okay, let's move to Policy Gradient methods with Peter Abbeel.\n",
    "\n",
    "Policy $\\pi$ is mapping from states to actions: $\\pi(\\cdot;\\theta): \\mathbb{S} \\rightarrow \\mathbb{A}$, where $\\theta$ parametrises the policy.\n",
    "And RL problem is to find a policy which maximises the total expected return $\\max_{\\theta}\\mathbb{E}\\sum_{t=0}^{H}{R(s_t)|\\pi_{\\theta}}$\n",
    "$\\pi$ is often stochastic.\n",
    "Reward might be sparce in time, hence, the problem is not that easy.\n",
    "\n",
    "Policy search methods are the one which optimise the policy directly.\n",
    "Why policy optimisation then?\n",
    "\n",
    "* sometimes policy is easier to represent than, let's say, $Q$ function\n",
    "* Value function does not directly tell us what to do (*not nesessarily true if we have a greedy policy with respect to $V$*)\n",
    "* learning $Q$ is hard for continuous/high-dimensional action spaces (how solve $argmax_u{Q_{\\theta}(s,u)}$\n",
    "\n",
    "Below you can see a classiication of these methods.\n",
    "\n",
    "<img src='pics/rlss/rlss_abbeel_classification.png'>\n",
    "\n",
    "## Model based approach \n",
    "\n",
    "\n",
    "We start from the model-based approach. The MDP for the setting is on the picture.\n",
    "\n",
    "<img src='pics/rlss/rlss_abbeel_graph.png'>\n",
    "\n",
    "Okay, what is the setting here? $r_t = R(s_t), u_t = \\pi_{\\theta}(s_t), s_{t+1}=f(s_t, u_t)$, where $u_t$ is named after 'upravlenie', the word for 'control' in Russian. And the assumptions is that the transition function $f$ is know, deterministic and differentiable, reward $r$ is know, deterministic and differentiable, policy $\\pi$ is (*known???*), deterministic and differentiable.\n",
    "\n",
    "The goal, again, is to maximise the total utility (don't like abusing the notation by taking capital $U$ for utility and $u_t$ for policy, but I live it as it is in the lecture):\n",
    "\n",
    "$\\max_{\\theta}{U(\\theta)} = \\max_{\\theta}\\mathbb{E}[\\sum_{t=0}^{H}{r_t}|\\pi_0]$\n",
    "\n",
    "Let's think about the MDP above as a computational graph, let's just back propagate gradients through this (BPTT).\n",
    "\n",
    "$\\frac{\\partial U}{\\partial \\theta_i} = \\sum_{t=0}^{H}\\frac{\\partial R}{\\partial s}(s_t)\\frac{\\partial s_t}{\\partial \\theta_i}$\n",
    "\n",
    "$\\frac{\\partial s_t}{\\partial \\theta_i} = \\frac{\\partial f}{\\partial s}(s_{t-1}, u_{t-1})\\frac{\\partial u_{t-1}}{\\partial \\theta_i}+\\frac{\\partial f}{\\partial u}(s_{t-1}, u_{t-1})\\frac{\\partial u_{t-1}}{\\partial \\theta_i}$\n",
    "\n",
    "$\\frac{\\partial u_t}{\\partial \\theta_i} = \\frac{\\partial \\pi_{\\theta}}{\\partial \\theta_i}(s_t, \\theta) + \\frac{\\partial \\pi_{\\theta}}{\\partial s}(s_t, \\theta)\\frac{\\partial s_t}{\\partial \\theta_i}$\n",
    "\n",
    "Feed this to your favourite automatic differentiation package, and you are set. AGI!\n",
    "\n",
    "What happens if the environment is stochastic, i.e. $s_{t+1} = f(s_{t+1}, u_t) + \\omega_t$? Not much, just consider noise constant for the rollout in question. \n",
    "That's true, that we need to sample more rollouts to reduce the variance, but nothing changes apart from that.\n",
    "\n",
    "More generally, we apply reparametrisation trick to the dynamics of an MDP, i.e. going from $s_{t+1} = f(s_{t+1}, u_t) + \\omega_t$ to $s_{t+1} = f(s_{t+1}, u_t, w_t)$, where the latter function is not stochastic anymore.\n",
    "Computational graph will just have additional inputs $\\omega_0, \\omega_1, ..., \\omega_k$\n",
    "We can apply exactly the same trick to both reward function and policy.\n",
    "\n",
    "The whole idea in pseudocode looks like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for iter = 1,2...\n",
    "    for rollout r = 1,2...\n",
    "        sample s0, w0, w1, ..., v0, v1..., z0, z1... # w_i is dynamics noise, v_i is policy noise, z_i is reward noise\n",
    "        execute rollout\n",
    "        compute gradients via BPTT\n",
    "    average gradients\n",
    "    take gradient step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real world, we do not have access to noise, but we can learn it from rollouts (model-based RL).\n",
    "\n",
    "It's very hard to make all of these work, as Peter says.\n",
    "Stochastic Value Gradients (SVG) [3] of different flavours (SVG($\\infty$), SVG(1), SVG(0)->(D)DPG [4,5])  is one of the success stories.\n",
    "DDPG uses DQN features as replay memory and using target net to improve stability:\n",
    "$\\hat{Q}_t = r_t + \\gamma Q_{\\phi'}(s_{t+1}, \\pi_{\\theta'}(s_{t+1}))$, where $Q_{\\phi'}$ is the Polyak-averaged target function (averaged weights along several weight updates).\n",
    "\n",
    "## Model free approach\n",
    "\n",
    "What are the assumptions here?\n",
    "It turns out that there is less assumptions in comparison to the previous section. \n",
    "There are no assumptions for the transition function as well as for the reward.\n",
    "Policy $\\pi$ is (*known???* and)stochastic.\n",
    "\n",
    "### Finite Difference method\n",
    "\n",
    "$\\frac{\\partial U}{\\partial \\theta_i} = \\frac{U(\\theta+\\epsilon_je_j) + U(\\theta-\\epsilon_je_j)}{2\\epsilon}$, where $e_j$ is the one hot encoding of the parameter vector index $\\begin{align}\n",
    "    e_j &= \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           1 \\\\\n",
    "           \\vdots \\\\\n",
    "           0\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}$, where (1 is for $j$-th entry)\n",
    "\n",
    "Estimate the gradient via [finite differences](https://en.wikipedia.org/wiki/Finite_difference_method). Be careful! The noise for positive and negative perturbations should be the same!\n",
    "\n",
    "### Cross-Entropy Method\n",
    "\n",
    "Optimise the same objective (expected sum of rewards along the rollout), but consider $U$ to be a black box and ignore all the other information other than U collected during an episode.\n",
    "\n",
    "This is an evolutionary algorithm which works as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "disr_params = np.zeros(population_size)\n",
    "for iter i = 1, 2, ...\n",
    "    curr_utility = np.zeros(population_size)\n",
    "    for population member e = 1, 2, ...\n",
    "        sample parameters from the distribution over params\n",
    "        execute rollouts under current policy\n",
    "        distr_params[e] = sampled_params\n",
    "        curr_utility[e] = curr_rollout_return\n",
    "    # update the mean of the distribution over parameters\n",
    "    # where the sum is taken for top p% of the population\n",
    "    # logprob is for the distribution wi\n",
    "    new_mean = argmax(sum(logprob(params[e]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting, that given how simple the method is, it works embarassingly (according to Peter) well. Unfortunately, sample efficiency is not that great and the method struggles when problem dimensionality is too high.\n",
    "\n",
    "You can find a list of related approaches with references below:\n",
    "\n",
    "* Reward Weighted Regression (RWR) [6]\n",
    "* Policy Improvement with Path Integrals ($PI^2$) [7]\n",
    "* Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [8]\n",
    "* PoWER [9]\n",
    "\n",
    "If you have a lot of compute and do not care about sample efficiency (e.g. in cheap simulation), the methods are great.\n",
    "It turns out, that since we can easy to parallelise them, they will be the fastest in terms of the wall time Salimans, Ho, Sutskever, 2017 <REFERENCE>.\n",
    "\n",
    "## Likelihood Ratio Policy Gradients\n",
    "\n",
    "Usually, when somebody is talking about PG, they are talking about Likelihood Ratio PG. \n",
    "We modify the notation a bit to reduce clutter.\n",
    "$\\tau$ is the trajectory which includes all the state/actions for one particular rollout: $s_0, u_0, ..., s_h, u_h$.\n",
    "$R(\\tau) = \\sum_{t=0}^{H}R(s_t, u_t)$.\n",
    "The goal again is to maximise the objective $U(\\theta) = \\mathbb{E}[\\sum_{t=0}^{H}R(s_t, u_t); \\pi_{\\theta}] = \\sum_{\\tau}P(\\tau;\\theta)R(\\tau)$ with respect to $\\theta$.\n",
    "\n",
    "Let's derive the gradient of the objective using the famous log trick.\n",
    "$\\nabla_{\\theta}U(\\theta) = \\nabla_{\\theta}\\sum_{\\tau}P(\\tau; \\theta)R(\\tau) = \\sum_{\\tau}\\nabla_{\\theta}P(\\tau; \\theta)R(\\tau) = \\sum_{\\tau}\\frac{P(\\tau;\\theta)}{P(\\tau;\\theta)}\\nabla_{\\theta}P(\\tau; \\theta)R(\\tau) = \\sum_{\\tau}{P(\\tau;\\theta)}\\frac{\\nabla_{\\theta}P(\\tau;\\theta)}{P(\\tau; \\theta)}R(\\tau) \\sum_{\\tau}{P(\\tau;\\theta)}\\nabla_{\\theta}\\log{P(\\tau;\\theta)}R(\\tau)$.\n",
    "So, from one expectation, we got to another expectation of the log: $\\nabla_{\\theta}U(\\theta) = \\nabla_{\\theta}\\mathbb{E}_{\\tau}[R(\\tau)] = \\mathbb{E}_{\\tau}[\\nabla_{\\theta}\\log{P(\\tau; \\theta)}R(\\tau)]$.\n",
    "Gradient of the expectation equals to the expectation of the gradient of the log.\n",
    "Why is it useful? \n",
    "It's useful because we can empirically evaluate our expectation by just taking average of the executed rollouts:\n",
    "$\\nabla_{\\theta}(U(\\theta)) \\approx \\frac{1}{m}\\sum_{i=1}^{m}\\nabla_{\\theta}\\log{P(\\tau^{(i)},\\theta)R(\\tau^{(i)})}$.\n",
    "\n",
    "There is another way of obtaining the same result via importance sampling. \n",
    "We sample from $\\theta_{old}$ and reweight the samples using the ratio of the new and the old policies:\n",
    "\n",
    "$U(\\theta) \\mathbb{E}_{\\tau \\sim \\theta_{old}}[\\frac{P(\\tau|\\theta)}{P(\\tau|\\theta_{old})}R(\\tau)]$\n",
    "\n",
    "$\\nabla_{\\theta} U(\\theta) |_{\\theta = \\theta_{old}} = \\mathbb{E}_{\\tau \\sim \\theta_{old}}[\\frac{\\nabla_{\\theta} P(\\tau|\\theta) |_{\\theta_{old}}}{P(\\tau|\\theta_{old})}R(\\tau)] = \\mathbb{E}_{\\tau \\sim \\theta_{old}}[\\nabla_{\\theta} \\log{P(\\tau|\\theta)}|_{\\theta_{old}}R(\\tau)]$\n",
    "\n",
    "Another great thing is that all of these works even when our reward function $R$ is discontinuous and/or unknown.\n",
    "\n",
    "The intuition behind LRPG methods is that they try to increase the probability of paths with positive $R$, and decrease the probability of paths with negative $R$.\n",
    "They do not try to change the paths, this is important (in contrast to path derivatives, which try to perturb the trajectories rather than shifting probability mass).\n",
    "\n",
    "The derivations seems too magical to be true =), but this is not the only magic. Let's get rid of the dynamics in our gradient computation:\n",
    "\n",
    "$\\nabla_{\\theta}\\log{P(\\tau^{(i)}; \\theta)} = \\nabla_{\\theta}\\log{[\\prod_{t=0}^H{P(s_{t+1}^{(i)}| s_t^{(i)},u_t^{(i)})\\pi_{\\theta}(u_t^{(i)}|s_t^{(i)})}]} = \\nabla_{\\theta}\\sum_{t=0}^{H}{\\log{P(s_{t+1}^{(i)}, u_t^{(i)})}} + \\sum_{t=0}^{H}\\log{\\pi_{\\theta}(u_t^{(i)}|s_t^{(i)})} = \\nabla_{\\theta}\\sum_{t=0}^{H}\\log{\\pi_{\\theta}(u_t^{(i)}|s_t^{(i)})} = \\sum_{t=0}^{H}\\nabla_{\\theta}\\log{\\pi_{\\theta}(u_t^{(i)}|s_t^{(i)})}$\n",
    "\n",
    "Magic again! No transition model required!!!\n",
    "\n",
    "As a result of all above, $\\hat{g} = \\frac{1}{m}\\sum_{i=1}^{m}\\nabla_{\\theta}\\log{P(\\tau^{(i)}}; \\theta)R(\\tau^{(i)})$ is an unbiase estimate of the true gradient, i.e. $\\mathbb{E}[\\hat{g}] = \\nabla_{\\theta}U(\\theta)$.\n",
    "\n",
    "A caveat here is that when $R > 0$, our method will try to increase the probabilities of all paths. \n",
    "But we want to increase the probabilities only the best ones.  \n",
    "Let's introduce the baseline, which, intuitively, will try to tell us whether we're doing better than average or not.\n",
    "\n",
    "Baseline is something we deduce from our trajectory return when estimating the gradient: $\\hat{g} = \\frac{1}{m}\\sum_{i=1}^{m}\\nabla_{\\theta}\\log{P(\\tau^{(i)}}; \\theta)(R(\\tau^{(i)}) - b))$.\n",
    "One can show, that introducing a (*some particular*) baselines does not bias our gradient estimate.\n",
    "When we're doing better than average, PG increase probability of these trajectories, and vice versa.\n",
    "One of the most obvious choices for a baseline is $b = \\mathbb{E}[R(\\tau)] \\approx \\frac{1}{m}\\sum_{i=1}^{m}R(\\tau^{(i)})$.\n",
    "\n",
    "Another important moment about vanilla PG is that every action influence all the reward, though it should not.\n",
    "Let's make an action influence only future rewards by changing the summation indices:\n",
    "\n",
    "$\\hat{g} = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{t=0}^{H-1}\\nabla_{\\theta}\\log{\\pi_{\\theta}}(u_t^{(i)}|s_t^{(i)})(\\sum_{k=t}^{H-1}{R(s_k^{(i)}, u_k^{(i)}) - b(s_k^{(i)})})$\n",
    "\n",
    "What's a good choice for $b$ in this setting?\n",
    "Expected return from this time step $\\mathbb{E}[r_t + r_{t+1} + ... + r_{H-1}]$ looks like a good option.\n",
    "\n",
    "You can find the whole 'vanilla' PG pseudocode below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Initialize policy parameter theta, baseline b\n",
    "for iteration 1,2,... do\n",
    "    Collect a set of trajectories by executing the current policy\n",
    "    At each timestep in each trajectory compute\n",
    "        return = sum(discount(rewards))\n",
    "        advantage = return - b(s_t)\n",
    "    Re-fit the baseline, by minimising square norm of b(s+t) - R_t summed over all trajs and time steps\n",
    "    Update the policy, using a policy gradient estimate g_hat which is the sum of grad(logprob(a_t|s_t)*advantage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some important things which can make PG work better. \n",
    "One of these things is a step size.\n",
    "Step size is important in supervised learning, but it's even more important in RL since policy generate data we will train on the next iteration.\n",
    "And if the data is somehow wrong, we will never recover out of this unlucky situation.\n",
    "\n",
    "We can just do a simple line search $-$ trying different step sizes in the direction of the gradient and taking the best one.\n",
    "True, that we use only first order approximation here.\n",
    "Can we do better?\n",
    "Let's use a trust region idea $-$ a region, where we trust that our first order approximation.\n",
    "After that, we have to find the best point within the trust region:\n",
    "\n",
    "$\\max_{\\delta\\theta}\\hat{g}^T\\delta\\theta, s.t. KL(P(\\tau; \\theta)||P(\\tau; \\theta+\\delta\\theta)) < \\epsilon$\n",
    "\n",
    "Again, if we evaluate the KL, the environment dynamics magically cancels out again and $KL(P(\\tau; \\theta)||P(\\tau; \\theta+\\delta\\theta)) \\approx \\frac{1}{M}\\sum_{s \\text{ in rollouts under } \\theta}{KL(\\pi_{\\theta}(u|s)||\\pi_{\\theta + \\delta\\theta}(u|s))}$\n",
    "\n",
    "We can look at the second-order approximation of to KL and get $KL(\\pi_{\\theta}(u|s)||\\pi_{\\theta + \\delta\\theta}(u|s)) = \\delta\\theta^T F_{\\theta}\\delta\\theta, $ where $F_{\\theta}$ is [Fisher Information Matrix](https://en.wikipedia.org/wiki/Fisher_information).\n",
    "\n",
    "If $\\theta$ is high dimensional, it's super hard to build/invert the Fisher matrix, TRPO PAPER provides ous with machinery to compute second-order approximation more efficiently.\n",
    "\n",
    "We can improve even better when bringing value function into play. \n",
    "\n",
    "$\\hat{g} = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{t=0}^{H-1}\\nabla_{\\theta}\\log{\\pi_{\\theta}}(u_t^{(i)}|s_t^{(i)})(\\sum_{k=t}^{H-1}{R(s_k^{(i)}, u_k^{(i)}) - V^{\\pi}(s_k^{(i)})})$\n",
    "\n",
    "We need to learn $V^{\\pi}$ somehow, but RL has tools to do that.\n",
    "Moreover $R(s_k^{(i)}, u_k^{(i)})$ looks suspiciously similar to estimate of $Q^{\\pi}(s,u) = \\mathbb{E}[r_0 + r_1 + ... | s_0 = s, a_0 = a]$\n",
    "\n",
    "The variance here can be reduced by discounting and function approximation (e.g. using different TD(k)).\n",
    "A3C paper and GAE are two great examples of the approach described above.\n",
    "\n",
    "When I finished watching the lecture for the first time, I didn't really like it. \n",
    "It looked pretty cluttered to me, and it was hard to tell the things apart.\n",
    "But when I finished watching it for the 3rd time I find it quite logical, very broad and deep (not because of DL) at the same time (apart from the fact, that I did not get the slide about stochastic computational graphs at all).\n",
    "\n",
    "I also like the 'Current Frontiers' slide, where Peter Abbeel puts the latest research on the subject. I won't copypaste it here, but if you want, you can find it on slides 134-135 in the presentation.\n",
    "\n",
    "In addition, Peter provides links to RL courses as well:\n",
    "\n",
    "* [CS294-112 Deep Reinforcement Learning (UC Berkeley)](http://rll.berkeley.edu/deeprlcourse/) by Sergey Levine, John Schulman, Chelsea Finn\n",
    "* [COMPM050/COMPGI13 Reinforcement Learning (UCL)](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) by David Silver\n",
    "* [Deep\tRL Bootcamp, Berkeley, CA (August 26-27)](https://sites.google.com/view/deep-rl-bootcamp/lectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Learning, *Richard Sutton*\n",
    "\n",
    "*[Slides](http://videolectures.net/site/normal_dl/tag=1137922/deeplearning2017_sutton_td_learning_01.pdf)* |*[Video](http://videolectures.net/deeplearning2017_sutton_td_learning/)*\n",
    "\n",
    "This is not only a lecture on temporal-difference (TD) methods but a commencement speech for those who are entering the field.\n",
    "In the beginning, Rich is pondering about what's the ultimate solution to AI.\n",
    "He says that the method should scale as the amount of compute grows.\n",
    "And *prediction learning* is something that does scale.\n",
    "What is prediction learning?\n",
    "It's unsupervised supervised learning: we can get a target (e.g. just by waiting) however we do not need a human label.\n",
    "\n",
    "Let's get back to TD learning, learning a prediction from another, later, learned prediction.\n",
    "TD is the difference between two predictions.\n",
    "Otherwise, TD learning is just supervised learning.\n",
    "\n",
    "Why do you need TD learning? When your problem is a multi-step prediction problem $\\rightarrow$ something which supervised learning is not really great in, e.g. stock price prediction.\n",
    "\n",
    "Why not treat the problem above as step-by-step prediction? Rich says it's a trap and mentions POMDPs, Bayesians, control theory and compression enthusiasts x_X. Long-term predictions are hard and small errors are amplified.\n",
    "Computational costs are growing exponentially and it makes no sense the further it goes. We can't wait until the end when the target is known. Moreover, sometimes we don't even know the target.\n",
    "\n",
    "I put 'new RL notation' below. If you've already seen the second edition of the RL textbook, it's not new for you. \n",
    "Capital letters are random variables; lowercase letters are instances of these variables:\n",
    "$S_0, A_0, R_1, S_1$ are the state, action and reward respectively. $R_1$ means the reward for the transition from state $S_0$ to state $S_1$.\n",
    "\n",
    "$G_t$ is the discounted return $G_t := R_{t+1} + \\gamma R_{t+2} + ... = R_{t+1} + G_{t+1}$.  \n",
    "\n",
    "$v_{\\pi}$ is the state-value function $v_{\\pi}(s) := \\mathbb{E}_{\\pi}[G_t | S_t=s] = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s]$. Note, that this is not just the value, but the value of some policy $\\pi$.\n",
    "\n",
    "Finally, TD error is the difference $R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t})$\n",
    "\n",
    "Okay, next Rich goes to the explanation of Monte Carlo (Supervised learning as he puts it):\n",
    "\n",
    "$V(S_t) \\leftarrow V(S_t + \\alpha[G_t - V(S_t)])$\n",
    "\n",
    "Opposed to MC, the siplest TD variant takes only the first reward and uses the state-value estimation for the update:\n",
    "\n",
    "$V(S_t) \\leftarrow V(S_t + \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)])$\n",
    "\n",
    "Dynamic Programming takes the expectation here:\n",
    "\n",
    "$V(S_{t}) \\leftarrow \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma V(S_{t+1})]$\n",
    "\n",
    "TD is cool because it bootstraps and samples. Computationally, TD is also cool since it can be fully incremental and allows you to learn as you go without waiting for the final outcome.\n",
    "\n",
    "Then goes the random walk example and another didactic example when MC fails (minimising the mean-square error) and TD wins (doing certainty-equivalence estimate): MC methods are better on the history, but have a higher error on future data.\n",
    "\n",
    "The relationship between all the methods can be seen at the \"Unified View\" picture:\n",
    "\n",
    "<img src=\"pics/rlss/unified.png\">\n",
    "\n",
    "All right, we go from the state-value function $V(S)$ to the action-value function $Q(S_t,A_t)$. \n",
    "\n",
    "* For SARSA it's $Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$.\n",
    "* For Q-learning it's $Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$.\n",
    "* And, finally, for the expected SARSA: $Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_{t+1} + \\gamma \\mathbb{E} Q(S_{t+1}, A_{t+1} | S_{t+1}) - Q(S_t, A_t)]$.\n",
    "\n",
    "At the end of the lecture, prof. Sutton highlights the topics that might be of researchers interest:\n",
    "\n",
    "* off-policy prediction\n",
    "* non-linear function approximation\n",
    "* convergence theory for TD control theory\n",
    "* TD + DL?\n",
    "* Predicting something other than reward, e.g. Horde, Unreal, options\n",
    "\n",
    "This is the most unusual lecture on RL I've seen so far. Prof. Sutton looks so excited about the stuff he's doing so that I envy him sometimes =) There is a lot of personal opinion going on, but in general it's a great and interesting lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning, *Hado van Hasselt*\n",
    "\n",
    "*[Slides](http://videolectures.net/site/normal_dl/tag=1137918/deeplearning2017_van_hasselt_deep_reinforcement_01.pdf)* |*[Video](http://videolectures.net/deeplearning2017_van_hasselt_deep_reinforcement/)*\n",
    "\n",
    "\n",
    "**TBD**\n",
    "\n",
    "## Deep Control, *Nando de Freitas*\n",
    "\n",
    "*No slides yet* |*[Video](http://videolectures.net/deeplearning2017_de_freitas_deep_control/)*\n",
    "\n",
    "\n",
    "**TBD**\n",
    "\n",
    "\n",
    "\n",
    "## Theory of RL, *Csaba Szepesvári*\n",
    "\n",
    "*[Slides](http://videolectures.net/site/normal_dl/tag=1137923/deeplearning2017_szepesvari_theory_of_rl_01.pdf)* |*[Video](http://videolectures.net/deeplearning2017_szepesvari_theory_of_rl/)*\n",
    "\n",
    "\n",
    "**TBD**\n",
    "\n",
    "\n",
    "## Reinforcement Learning, *Satinder Singh*\n",
    "\n",
    "*[Slides](http://videolectures.net/site/normal_dl/tag=1129741/deeplearning2017_singh_reinforcement_learning_01.pdf)* |*[Video](http://videolectures.net/deeplearning2017_singh_reinforcement_learning/)*\n",
    "\n",
    "\n",
    "**TBD**\n",
    "\n",
    "\n",
    "## Safe RL, * Philip Thomas*\n",
    "\n",
    "*[Slides](http://videolectures.net/site/normal_dl/tag=1137917/deeplearning2017_thomas_safe_rl_01.pdf)* |*[Video](http://videolectures.net/deeplearning2017_thomas_safe_rl/)*\n",
    "\n",
    "\n",
    "**TBD**\n",
    "\n",
    "\n",
    "## Applications of bandits and recommendation systems, * Nicolas Le Roux*\n",
    "\n",
    "*[Slides](http://videolectures.net/site/normal_dl/tag=1137926/deeplearning2017_le_roux_recommendation_system_01.pdf)* |*[Video](http://videolectures.net/site/normal_dl/tag=1137926/deeplearning2017_le_roux_recommendation_system_01.pdf)*\n",
    "\n",
    "\n",
    "**TBD**\n",
    "\n",
    "\n",
    "## Cooperative Visual Dialogue with Deep RL, *Dhruv Batra & Devi Parikh*\n",
    "\n",
    "*[Slides](http://videolectures.net/site/normal_dl/tag=1137915/deeplearning2017_parikh_batra_deep_rl.pdf)* |*[Video](http://videolectures.net/deeplearning2017_parikh_batra_deep_rl/)*\n",
    "\n",
    "\n",
    "**TBD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Khan, Arbaaz, Clark Zhang, Nikolay Atanasov, Konstantinos Karydis, Vijay Kumar, and Daniel D. Lee. \"Memory Augmented Control Networks.\" arXiv preprint arXiv:1709.05706 (2017), [link](https://arxiv.org/abs/1709.05706).\n",
    "\n",
    "[2] Jiang, Nan, Alex Kulesza, Satinder Singh, and Richard Lewis. \"The dependence of effective planning horizon on model accuracy.\" In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pp. 1181-1189. International Foundation for Autonomous Agents and Multiagent Systems, 2015, [link](http://www-personal.umich.edu/~rickl/pubs/jiang-et-al-2015-aamas.pdf).\n",
    "\n",
    "[3] Heess, Nicolas, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. \"Learning continuous control policies by stochastic value gradients.\" In Advances in Neural Information Processing Systems, pp. 2944-2952. 2015, [link](http://papers.nips.cc/paper/5796-learning-continuous-control-policies-by-stochastic-value-gradients).\n",
    "\n",
    "[4] Silver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. \"Deterministic policy gradient algorithms.\" In ICML. 2014, [link](https://hal.inria.fr/hal-00938992/).\n",
    "\n",
    "[5] Lillicrap, Timothy P., Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. \"Continuous control with deep reinforcement learning.\" arXiv preprint arXiv:1509.02971 (2015)., [link](https://arxiv.org/abs/1509.02971)\n",
    "\n",
    "[6] Hachiya, Hirotaka, Jan Peters, and Masashi Sugiyama. \"Reward-weighted regression with sample reuse for direct policy search in reinforcement learning.\" Neural Computation 23, no. 11 (2011): 2798-2832, [link](https://www.researchgate.net/profile/Jan_Peters4/publication/51580421_Reward-Weighted_Regression_with_Sample_Reuse_for_Direct_Policy_Search_in_Reinforcement_Learning/links/09e4150d71d59dbc1a000000/Reward-Weighted-Regression-with-Sample-Reuse-for-Direct-Policy-Search-in-Reinforcement-Learning.pdf).\n",
    "\n",
    "[7] Theodorou, Evangelos, Jonas Buchli, and Stefan Schaal. \"A generalized path integral control approach to reinforcement learning.\" Journal of Machine Learning Research 11, no. Nov (2010): 3137-3181, [link](http://www.jmlr.org/papers/v11/theodorou10a.html).\n",
    " \n",
    "[8] Hansen, Nikolaus, Sibylle D. Müller, and Petros Koumoutsakos. \"Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES).\" Evolutionary computation 11, no. 1 (2003): 1-18, [link](https://www.mitpressjournals.org/doi/abs/10.1162/106365603321828970).\n",
    "\n",
    "[9] Kober, Jens, and Jan R. Peters. \"Policy search for motor primitives in robotics.\" In Advances in neural information processing systems, pp. 849-856. 2009, [link](http://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics).\n",
    "\n",
    "[10] Mnih, Volodymyr, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. \"Asynchronous methods for deep reinforcement learning.\" In International Conference on Machine Learning, pp. 1928-1937. 2016, [link](http://proceedings.mlr.press/v48/mniha16.pdf).\n",
    "\n",
    "[11] Schulman, John, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. \"High-dimensional continuous control using generalized advantage estimation.\" arXiv preprint arXiv:1506.02438 (2015), [link](https://arxiv.org/abs/1506.02438)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
