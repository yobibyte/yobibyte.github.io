{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/sc-research/scii.jpg\" alt=\"http://www.misucell.com/data/out/10/IMG_444005.jpg\">\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hi!**\n",
    "\n",
    "I have wanted to get into StarCraft research for a long time. Actually, right after I played StarCraft II for the first time. (I did not care about research when I tried the BroodWar at school =( ). Though, I have never done anything related apart from reading some papers on the topic. \n",
    "\n",
    "Now I feel like it’s the right time to start. What is the first step? Reading up! To make it more useful for myself and for the community, I’d like to summarize all the papers on the topic I read and make a kind of overview of what have been done already. I’ve read amazing surveys [1,2] on SCI research and consider them to be one of the best resources on the topic. However it’s kinda outdated and I would like to pick up the baton.\n",
    "\n",
    "I was thinking about writing a paper with the overview, but then decided to make it a blogpost. It’s much easier to update and make it an ongoing overview. It’s nice to have a possibility to add a new paper to the review several days after it’s published. I will start from StarCraft II, but will gradually add more work on the first version including the pioneering papers which were made by heroes who were not afraid of heavy engineering and hard hand labour (original AIIDE StarCraft competitions were run manually).\n",
    "\n",
    "I’ll start from high-level StarCraft description and then go into research. I will also put a small glossary at the bottom since the post is going to be really looooooong, and it will be handy to have the meaning of some abbreviations at one place.\n",
    "\n",
    "Let’s start our new journey! I plan to update the post around once a week, so, just drop by occasionally. And if you see something weird or you don’t see some paper worth mentioning, just drop me a [line](vitaliykurin@gmail.com) or find me on [Twitter](https://twitter.com/y0b1byte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is StarCraft\n",
    "\n",
    "StarCraft is an classical example of a Real-Time Strategy. The two opponents (in 1v1) are spawn on a map (the amount of spawn points is finite). They have to collect two type of resources: minerals and vespene gas, which can be spent on different units and upgrades for them. In order to build units one has to construct the production buildings and also upgrade them. The final goal of the game is to destroy all opponent's buildings.\n",
    "\n",
    "A player can choose one of the three available races: Zergs (weak units, but [numerous](https://www.youtube.com/watch?v=zfl279o_Cko), Protoss (heavy tech, powerful, but slower to develop) or Terrans (somewhere in the middle). Amazingly, but in StarCraft the choose of race is important and predefines your strategy which can be also numerous for each of the races.\n",
    "\n",
    "Built-in AI opponents are based on hand-craffed rules. In order to make them more competitive, the hardest ones have more resources or better vision.\n",
    "\n",
    "The game is almost deterministic with some small exceptions which can be eliminated by fixing a random seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why StarCraft is Hard?\n",
    "\n",
    "This section is mostly taken from [3].\n",
    "\n",
    "SC is much harder than all our previous achievements in battling humans in computer games. There are several reasons for that. \n",
    "\n",
    "First of all, there are two levels of partial observability: the fog of war covers some map areas and we need to scout, moreover, the game screen shows only a part of the world and only some of our units can be seen on it simultaneously.\n",
    "\n",
    "Next, the state and action spaces are tremendous, amount of units in control might be higher than a hundred.\n",
    "\n",
    "Finally, the credit assignment problem is hard as never. You either win, lose, or there is a draw and, actions at early stages of the game have long-term consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "\n",
    "This is a list of things I thought about after reading all the papers mentioned below. Short summary of a survey\n",
    "\n",
    "* The game can be divided into three levels: macro-management (constructing the buildings, keeping the track of the game in general), tactics (mid-term decisions, *not sure if the second level is popular in SC literature yet*) and micro-control (controlling units during combat).\n",
    "* Most SC-related RL research is focusing on reactive control (at least for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarCraft I research\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarCraft II research\n",
    "\n",
    "\n",
    "### StarCraft II: A New Challenge for Reinforcement Learning, 2017 [2], [code](https://github.com/deepmind/pysc2)\n",
    "\n",
    "**TLDR;**\n",
    "\n",
    "The paper introduces the StarCraft II Learning Environment, the A3C baseline which is quite good for simplified tasks and fails miserably for the full version of the game. Currently, there is no possibility to do self-play, but DeepMind is working on it. The two companies also release the dataset of human replays that is planned to be hugely enlarged.\n",
    "[Announcement blogpost](https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/).\n",
    "\n",
    "------------------------------\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"pics/sc-research/sc2le.jpg\">\n",
    "  <figcaption><tt>Source: [3].</tt></figcaption>\n",
    "</figure>\n",
    "\n",
    "Right after Atari 2600 and Go success there started rumours about DeepMind working on StarCraft. The 2016 BlizzCon had unconventional for such an event guests: DeepMind. They announced the collaboration between the two companies and then calmed down for half a year. Finally, we can get our hands dirty and play with it. Everything is free, open and can be run under GNU/Linux even in headless mode! Let’s go to the release report.\n",
    "\n",
    "Starcraft II Learning Environment (SC2LE) works as a usual RL env: an agent gets observations and the reward signal and has to take actions. One can choose to play via the raw API (BWAPI-like) or via the SCIIAPI that I will describe below.\n",
    "\n",
    "The feature space consists of spatial $N\\times M$ feature maps (e.g. fog of war of height maps) and non-spatial features such as the amount of resources or available set of actions (action set depends on the current state). Spatial features are of two types: either for minimap or for the actual screen. \n",
    "Currently, there is no possibility to play from RGB, but it will be added in the future. But, to be honest, in my opinion, we are still far from beating even the built-in AI agent from raw features. You can find more on features [here](https://github.com/deepmind/pysc2/blob/master/docs/environment.md). The whole picture looks something like this:\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-fKUyT14G-8?rel=0\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\n",
    "The two companies release a set of mini-games that were used during development as unit tests (integration tests, right? ;)). They can also be used for training some aspects of our agents: e.g. collecting resources, building the army or moving a unit to a beakon. One can build a new mini-game using the map editor.\n",
    "\n",
    "In contrast to the full version of the game, where the reward is the result of one game (1, -1 or 0 based on the outcome), mini-games have reward functions and we can easily apply our available RL approaches to them. And the authors do that, they code an A3C agent and show its performance on a variety of tasks. The agent is tested for three different architectures: Atari-net Agent (roughly, adaptation of DQN [5] architecture for the current task), FullyConv agent (to preserve the spatial structure of the input the state representation is a set of feature maps rather than a fully connected layer), and FullyConv LSTM Agent (previous + recurrence). The agent takes an action each 8-th frame (~180 APM, with human APM 30-300). \n",
    "\n",
    "The GrandMaster player baseline is still better, but for some of the tasks the A3C agent plays reasonably well. As the author note, even the mini-games pose interesting challenges to existing approaches.\n",
    "\n",
    "Though, the authors want to destroy a human SCII pro in a fair game, the environment is still a simplified version of the game. The actions are not the key presses or mouse clicks, but can be a combination of several actions: e.g. action 'select_rect()' instead of manually clicking and dragging a mouse. The simulation does not also happen in real time and an agent has an infinite amount of time to take a decision. In addition to the final 1/-1/0 reward signal, one can use the running Blizzard score which evaluates the player performance as the game progesses. Even having said that, we have to admit, that the problem is still at the border of intractability. Even in such a reduced mode, the action space is large: there are ~300 actions with 13 types of arguments. The A3C agent mentioned before is unable to beat even the easiest built-in AI agent and we're only at the beginning of our journey.\n",
    "\n",
    "Another amazing thing about this collaboration is that Blizzard is sitting on a whole shitload of human replay data which might be very very important for Learning from Demonstrations research. Currently, the dataset comprises ~65k replays, but is planned to be increased to >500k soon. \n",
    "\n",
    "The paper mentiones using 800k for the supervised learning from replays. The authors report the results for the two experiments: value prediction (what is the outcome of the game?) and policy predictions (what is the next demonstrator's action?). Using replays for training an agent to play a game was not considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSC: A Dataset for Macro-Management in StarCraft II, 2017 [6], [code](https://github.com/wuhuikai/msc)\n",
    "\n",
    "**TLDR;**\n",
    "\n",
    "Macro-management in SC is hard. Available datasets are either too small/not diverse or not standartized. The authors take the dataset from [3] (65k for now, 800k replays soon) and make a dataset for learning macro-management tasks. They hope, the dataset will be a benchmark for such kind of problems and present initial baseline results for global state evaluation and build order prediction tasks.\n",
    "\n",
    "\n",
    "------------------------------\n",
    "\n",
    "\n",
    "Playing SC end-to-end is unfeasible (at least for now), [3] shows that. Hierarchical approach looks like a promising candidate for reducing the complexity by breaking the task down into easier subproblems. *This goes in line with [3] which release a set of mini-games, each of which might be a block of a SC agent*.\n",
    "\n",
    "How do we learn macro-management? Learning from human demonstrations looks like a reasonable approach [8,9]. In order to do this, we need data. There are many similar macro-management datasets for SCI , but they are mostly designed for specific tasks or really small (e.g. [7]).\n",
    "\n",
    "[3] releases the learning environment (SC2LE) and a huge dataset (800k replays expected) of high quality where the data is standartized. The authors take all of this and transform the replays into the data for learning macro-management tasks. The dataset is easy to use (no need to replay and collect the data, the data is preprocessed already and split into train/validation/test subsets.\n",
    "\n",
    "The authors explain in detail what they have done to build the whole data pipeline, but we will not cover them here. To get all the stats, plots and more idea, what the data is, please have a look at the paper. \n",
    "\n",
    "\n",
    "All the macro actions belong to one of these four groups:\n",
    "\n",
    "* Build a building\n",
    "* Train a unit\n",
    "* Research a technique\n",
    "* Upgrade a building\n",
    "\n",
    "What can we use this data for? The authors list the potential use cases of their dataset:\n",
    "\n",
    "* mine the data to understand what do the good players do and what are the mistakes of the bad players\n",
    "* apply sequence modelling for global state evaluation or even for playing\n",
    "* model the uncertainty (fog of war -> uncertainty in making decisions) (*didn't quite get the VAE reference idea here*)\n",
    "* learning from unbalanced data (lots of NOOP actions)\n",
    "* imitation learning\n",
    "* planning \n",
    "\n",
    "Apart from analysing the dataset, and discussing its potential use cases, the authors also present the baselines for global state evaluation and build order prediction. The first is prediction an outcome of the game, starting at the particular state: predicting the probability of winning, given the current state. Since we don't have access to the state, but only observations, $p(win|s_t) \\sim p(win|o_1,o_2, ... , o_t)$, where $s_t$ is the state at the time $t$ and $o_i$ is the observation for the time step $i$.\n",
    "\n",
    "They use GRUs for the state evaluation and achieve $\\sim60\\%$ accuracy. I don't get it though, what are the reported numbers. As the authors note, the accuracy is very low at the beggining, but gets better as long as the game progresses ($75\\%-100\\%$ at the end of the game). But what is in the tables? The authors don't say much about the Build Order Prediction task either. This looks very strage in the light of the claim, that the reported numbers are expected to be baselines for these tasks in SCII.\n",
    "\n",
    "There are some other moments I don't like about the papers. For instance, the authors claim splitting into train/valid/test as one of the cool advantages about the data. That's useful, but it is not something extraordinary. Sometimes, from my point of view, the authors put something into the paper just to put it there (e.g. Algorithm 2). But in general, having another great dataset that is easy and ready for use is amazing. Hope to see more use cases soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "* **SCI**, **SCII** – StarCraft I, Starcraft II\n",
    "* **SC** – StarCraft, I use this abbreviation when the distinction is not significant\n",
    "* **RL** – Reinforcement Learning\n",
    "* **LfD** – Learning from Demonstrations \n",
    "* *some italic text* – my remarks, not author's opinion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Ontanón, Santiago, Gabriel Synnaeve, Alberto Uriarte, Florian Richoux, David Churchill, and Mike Preuss. \"A survey of real-time strategy game ai research and competition in starcraft.\" IEEE Transactions on Computational Intelligence and AI in games 5, no. 4 (2013): 293-311, [link](http://ieeexplore.ieee.org/abstract/document/6637024/).\n",
    "\n",
    "[2] Robertson, Glen, and Ian Watson. \"A review of real-time strategy game AI.\" AI Magazine 35, no. 4 (2014): 75-104, [link](https://www.aaai.org/ojs/index.php/aimagazine/article/view/2478).\n",
    "\n",
    "[3] Vinyals, Oriol, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani et al. \"StarCraft II: A New Challenge for Reinforcement Learning.\" arXiv preprint arXiv:1708.04782 (2017), [link](https://arxiv.org/abs/1708.04782).\n",
    "\n",
    "[4] Mnih, Volodymyr, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. \"Asynchronous methods for deep reinforcement learning.\" In International Conference on Machine Learning, pp. 1928-1937. 2016, [link](http://proceedings.mlr.press/v48/mniha16.pdf).\n",
    "\n",
    "[5] Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves et al. \"Human-level control through deep reinforcement learning.\" Nature 518, no. 7540 (2015): 529-533, [link](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf).\n",
    "\n",
    "[6] Wu, Huikai, Zhang, Junge, Huang, Kaiqi \"MSC: A Dataset for Macro-Management in StarCraft II\" arXiv preprint arXiv:1710.03131 (2017), [link](https://arxiv.org/abs/1710.03131).\n",
    "\n",
    "[7] Cho, Ho-Chul, Kyung-Joong Kim, and Sung-Bae Cho. \"Replay-based strategy prediction and build order adaptation for StarCraft AI bots.\" In Computational Intelligence in Games (CIG), 2013 IEEE Conference on, pp. 1-7. IEEE, 2013, [link](https://cilab.sejong.ac.kr/home/lib/exe/fetch.php?media=public:paper:cig_2013.pdf).\n",
    "\n",
    "[8] Erickson, Graham Kurtis Stephen, and Michael Buro. \"Global State Evaluation in StarCraft.\" In AIIDE. 2014, [link](https://www.aaai.org/ocs/index.php/AIIDE/AIIDE14/paper/viewFile/8996/8936).\n",
    "\n",
    "[9] Justesen, Niels, and Sebastian Risi. \"Learning macromanagement in starcraft from replays using deep learning.\" arXiv preprint arXiv:1707.03743 (2017), [link](https://arxiv.org/abs/1707.03743).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
