
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-03-13</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-03-13 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Beyond Text: Frozen Large Language Models in Visual Signal Comprehension</h3>
<p><a href='http://arxiv.org/abs/2403.07874v1'>http://arxiv.org/abs/2403.07874v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to translate images into words using a large language model without fine-tuning, enabling image comprehension and denoising tasks.</p><hr><h3>Rethinking Generative Large Language Model Evaluation for Semantic  Comprehension</h3>
<p><a href='http://arxiv.org/abs/2403.07872v1'>http://arxiv.org/abs/2403.07872v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an RWQ-Elo rating system for assessing large language models based on a two-player competitive format using real-world questions and demonstrates its advantages over MCQA.</p><hr><h3>Exploring Safety Generalization Challenges of Large Language Models via  Code</h3>
<p><a href='http://arxiv.org/abs/2403.07865v1'>http://arxiv.org/abs/2403.07865v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CodeAttack, a framework that tests the safety generalization of LLMs by transforming natural language inputs into code inputs, revealing common vulnerabilities in all studied models.</p><hr><h3>Bridging Different Language Models and Generative Vision Models for  Text-to-Image Generation</h3>
<p><a href='http://arxiv.org/abs/2403.07860v1'>http://arxiv.org/abs/2403.07860v1</a></p>
<p><b>Compressor summary</b>: The paper introduces LaVi-Bridge, a pipeline that integrates diverse language and generative vision models for text-to-image generation, improving alignment and quality.</p><hr><h3>Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias</h3>
<p><a href='http://arxiv.org/abs/2403.07857v1'>http://arxiv.org/abs/2403.07857v1</a></p>
<p><b>Compressor summary</b>: The text discusses how model-induced distribution shifts can cause performance and fairness issues in machine learning models, but also proposes a framework called algorithmic reparation to address these problems and promote equity.</p><hr><h3>Quantum Support Vector Machine for Prostate Cancer Detection: A  Performance Analysis</h3>
<p><a href='http://arxiv.org/abs/2403.07856v1'>http://arxiv.org/abs/2403.07856v1</a></p>
<p><b>Compressor summary</b>: The study uses Quantum Support Vector Machine (QSVM) to improve prostate cancer detection, achieving comparable accuracy and increased sensitivity over classical SVM.</p><hr><h3>Distilling the Knowledge in Data Pruning</h3>
<p><a href='http://arxiv.org/abs/2403.07854v1'>http://arxiv.org/abs/2403.07854v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Data pruning can reduce model size and training time but may compromise accuracy
- Knowledge distillation (KD) integrates soft predictions from a teacher network pre-trained on full data to guide the pruned student network
- KD improves pruned models across datasets, pruning methods, and pruning fractions
- There is a trade-off between the pruning factor and the optimal knowledge distillation weight
- Smaller teachers may outperform larger ones for lower pruning fractions

Summary: The paper shows that using knowledge distillation with data pruning can improve accuracy and suggests optimal parameters for different pruning regimes.</p><hr><h3>12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning</h3>
<p><a href='http://arxiv.org/abs/2403.07851v1'>http://arxiv.org/abs/2403.07851v1</a></p>
<p><b>Compressor summary</b>: O-FSCIL is a lightweight, memory-efficient method for incrementally learning new classes from few examples on resource-constrained devices.</p><hr><h3>Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining  of Explanations</h3>
<p><a href='http://arxiv.org/abs/2403.07849v1'>http://arxiv.org/abs/2403.07849v1</a></p>
<p><b>Compressor summary</b>: EEGL is an iterative algorithm that improves GNNs for node classification by using explanations from subgraph mining to obtain application-dependent features.</p><hr><h3>A Machine learning and Empirical Bayesian Approach for Predictive Buying  in B2B E-commerce</h3>
<p><a href='http://arxiv.org/abs/2403.07843v1'>http://arxiv.org/abs/2403.07843v1</a></p>
<p><b>Compressor summary</b>: Udaan, India's largest B2B ecommerce platform, uses an ensemble of machine learning models to predict and optimize customer order patterns, resulting in a significant increase in order rates.</p><hr><h3>Quantifying and Mitigating Privacy Risks for Tabular Generative Models</h3>
<p><a href='http://arxiv.org/abs/2403.07842v1'>http://arxiv.org/abs/2403.07842v1</a></p>
<p><b>Compressor summary</b>: DP-TLDM is a novel tabular synthesizer that combines an autoencoder with a latent diffusion model to generate high-quality, privacy-preserving synthetic data.</p><hr><h3>MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with  Module-wise Pruning Error Metric</h3>
<p><a href='http://arxiv.org/abs/2403.07839v1'>http://arxiv.org/abs/2403.07839v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a new pruning framework (MoPE-CLIP) for vision-language pre-trained models
- MoPE metric assesses module importance by performance decline on cross-modal tasks
- MoPE-CLIP reduces pre-training costs and achieves competitive task-specific performance

Summary:
The paper introduces a new pruning method (MoPE-CLIP) that uses a novel metric to measure the importance of modules in vision-language pre-trained models, leading to reduced pre-training costs and high task-specific performance.</p><hr><h3>The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage  Brought By Model Editing</h3>
<p><a href='http://arxiv.org/abs/2403.07825v1'>http://arxiv.org/abs/2403.07825v1</a></p>
<p><b>Compressor summary</b>: The paper proposes GORA, a new method to evaluate and measure the ripple effect in large language models, and SORA, a technique to mitigate this effect by selectively re-editing the model.</p><hr><h3>Label Dropout: Improved Deep Learning Echocardiography Segmentation  Using Multiple Datasets With Domain Shift and Partial Labelling</h3>
<p><a href='http://arxiv.org/abs/2403.07818v1'>http://arxiv.org/abs/2403.07818v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new label dropout technique to improve the robustness of deep learning models for echocardiography segmentation when trained with multiple diverse partially-labelled datasets.</p><hr><h3>Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM</h3>
<p><a href='http://arxiv.org/abs/2403.07816v1'>http://arxiv.org/abs/2403.07816v1</a></p>
<p><b>Compressor summary</b>: Branch-Train-MiX (BTX) is a method for training large language models with multiple specialized skills by branching a seed model, training experts in parallel, combining their feedforward parameters, and learning token-level routing.</p><hr><h3>Chronos: Learning the Language of Time Series</h3>
<p><a href='http://arxiv.org/abs/2403.07815v1'>http://arxiv.org/abs/2403.07815v1</a></p>
<p><b>Compressor summary</b>: Chronos is a framework for pretraining probabilistic time series models that use transformer-based language models and tokenized time series data, achieving high zero-shot performance on various forecasting tasks.</p><hr><h3>pyvene: A Library for Understanding and Improving PyTorch Models via  Interventions</h3>
<p><a href='http://arxiv.org/abs/2403.07809v1'>http://arxiv.org/abs/2403.07809v1</a></p>
<p><b>Compressor summary</b>: $	extbf{pyvene}$ is a Python library that allows customizable interventions on different PyTorch modules for various AI applications, including interpretability and robustness.</p><hr><h3>StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting</h3>
<p><a href='http://arxiv.org/abs/2403.07807v1'>http://arxiv.org/abs/2403.07807v1</a></p>
<p><b>Compressor summary</b>: StyleGaussian is a fast 3D style transfer method that uses 3D Gaussian Splatting, VGG features, and a K-nearest-neighbor-based 3D CNN to achieve high quality stylization with real-time rendering and multi-view consistency.</p><hr><h3>Beyond Memorization: The Challenge of Random Memory Access in Language  Models</h3>
<p><a href='http://arxiv.org/abs/2403.07805v1'>http://arxiv.org/abs/2403.07805v1</a></p>
<p><b>Compressor summary</b>: Key points:
- LMs are effective in NLP tasks, especially knowledge-intensive ones
- Paper investigates how LMs access their memory sequentially or randomly
- LMs can access memory sequentially but struggle with random access
- Recitation and permutation techniques improve random access
- Improved random access helps question answering

Summary:
The paper explores how LMs access their memory in different scenarios, and proposes recitation and permutation to enhance their random access, which benefits question answering.</p><hr><h3>A Fourier Transform Framework for Domain Adaptation</h3>
<p><a href='http://arxiv.org/abs/2403.07798v1'>http://arxiv.org/abs/2403.07798v1</a></p>
<p><b>Compressor summary</b>: The text proposes a Fourier method (FTF) to improve unsupervised domain adaptation by fusing low-level information from both domains and aligning multiple sources of data, achieving better generalization and performance on four benchmark datasets.</p><hr><h3>Joint Selection: Adaptively Incorporating Public Information for Private  Synthetic Data</h3>
<p><a href='http://arxiv.org/abs/2403.07797v1'>http://arxiv.org/abs/2403.07797v1</a></p>
<p><b>Compressor summary</b>: Jam-pgm is a new technique that allows graphical model-based synthetic data generation to use public data, improving quality and outperforming other methods, even with biased public data distributions.</p><hr><h3>Fine-tuning Large Language Models with Sequential Instructions</h3>
<p><a href='http://arxiv.org/abs/2403.07794v1'>http://arxiv.org/abs/2403.07794v1</a></p>
<p><b>Compressor summary</b>: The paper proposes sequential instruction tuning to improve large language models' ability to follow multiple instructions in complex tasks, and analyzes its effects on various factors.</p><hr><h3>SemCity: Semantic Scene Generation with Triplane Diffusion</h3>
<p><a href='http://arxiv.org/abs/2403.07773v1'>http://arxiv.org/abs/2403.07773v1</a></p>
<p><b>Compressor summary</b>: SemCity generates realistic outdoor scenes using a 3D diffusion model with triplane representation and manipulation, improving performance on tasks like inpainting and semantic completion.</p><hr><h3>Transforming Competition into Collaboration: The Revolutionary Role of  Multi-Agent Systems and Language Models in Modern Organizations</h3>
<p><a href='http://arxiv.org/abs/2403.07769v1'>http://arxiv.org/abs/2403.07769v1</a></p>
<p><b>Compressor summary</b>: The article discusses using large language models and multi-agent systems theory to create artificial agents that can simulate human interactions and support various organizational processes, overcoming some limitations of traditional approaches.</p><hr><h3>Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model</h3>
<p><a href='http://arxiv.org/abs/2403.07764v1'>http://arxiv.org/abs/2403.07764v1</a></p>
<p><b>Compressor summary</b>: Stable-Makeup is a novel diffusion-based method that transfers realistic and detailed makeup to user-provided faces, preserving content and structure, and showing strong robustness and generalizability for various tasks.</p><hr><h3>Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and  Image Embeddings</h3>
<p><a href='http://arxiv.org/abs/2403.07750v1'>http://arxiv.org/abs/2403.07750v1</a></p>
<p><b>Compressor summary</b>: Our method synthesizes image-text pairs using LLMs and image generation models, improving VLM training efficiency and performance on image captioning tasks.</p><hr><h3>FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese  Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.07747v1'>http://arxiv.org/abs/2403.07747v1</a></p>
<p><b>Compressor summary</b>: FineMath is a new benchmark dataset for evaluating Chinese LLMs' mathematical reasoning skills on diverse elementary school math problems with different difficulty levels.</p><hr><h3>Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified  3D Perception</h3>
<p><a href='http://arxiv.org/abs/2403.07746v1'>http://arxiv.org/abs/2403.07746v1</a></p>
<p><b>Compressor summary</b>: HyDRa is a novel camera-radar fusion architecture that improves depth prediction and 3D perception for autonomous driving in diverse conditions, achieving state-of-the-art results on nuScenes and Occ3D benchmarks.</p><hr><h3>Uncertainty Quantification with Deep Ensembles for 6D Object Pose  Estimation</h3>
<p><a href='http://arxiv.org/abs/2403.07741v1'>http://arxiv.org/abs/2403.07741v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to quantify uncertainty in multi-stage 6D object pose estimation using deep ensembles and evaluates it on SurfEmb, a top-performing approach.</p><hr><h3>DSEG-LIME -- Improving Image Explanation by Hierarchical Data-Driven  Segmentation</h3>
<p><a href='http://arxiv.org/abs/2403.07733v1'>http://arxiv.org/abs/2403.07733v1</a></p>
<p><b>Compressor summary</b>: DSEG-LIME is a new method to improve image analysis by creating more accurate and consistent explanations of complex machine learning models using data-driven segmentation.</p><hr><h3>SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and  Related Observable Overgeneration Mistakes</h3>
<p><a href='http://arxiv.org/abs/2403.07726v1'>http://arxiv.org/abs/2403.07726v1</a></p>
<p><b>Compressor summary</b>: The paper describes SHROOM, a shared task on detecting inaccurate NLG outputs from 3 tasks, and reports trends and results from 42 teams participating in it.</p><hr><h3>Balancing Fairness and Accuracy in Data-Restricted Binary Classification</h3>
<p><a href='http://arxiv.org/abs/2403.07724v1'>http://arxiv.org/abs/2403.07724v1</a></p>
<p><b>Compressor summary</b>: The paper presents a framework to analyze how data restrictions affect the accuracy and fairness of Bayesian classifiers under various scenarios and fairness definitions.</p><hr><h3>On the Last-Iterate Convergence of Shuffling Gradient Methods</h3>
<p><a href='http://arxiv.org/abs/2403.07723v1'>http://arxiv.org/abs/2403.07723v1</a></p>
<p><b>Compressor summary</b>: Shuffling gradient methods, such as Random Reshuffle and Shuffle Once, have good empirical performance but lacked theoretical guarantees until now; researchers prove last-iterate convergence rates without strong convexity.</p><hr><h3>Multi-modal Auto-regressive Modeling via Visual Words</h3>
<p><a href='http://arxiv.org/abs/2403.07720v1'>http://arxiv.org/abs/2403.07720v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper introduces Large Multi-modal Models (LMMs) that combine text and image features for various vision tasks
- The paper proposes visual words, which map visual features to text vocabulary, providing supervision information
- The paper experiments with 5 VQA tasks and shows the effectiveness of the proposed approach

Summary:
The paper presents a novel LMM that uses visual words to supervise image modelling and achieves state-of-the-art results on 5 VQA tasks.</p><hr><h3>Dynamic Graph Representation with Knowledge-aware Attention for  Histopathology Whole Slide Image Analysis</h3>
<p><a href='http://arxiv.org/abs/2403.07719v1'>http://arxiv.org/abs/2403.07719v1</a></p>
<p><b>Compressor summary</b>: The authors propose a novel dynamic graph representation algorithm for histopathological whole slide images classification that captures both instance relationships and spatial interactions using a knowledge-aware attention mechanism.</p><hr><h3>WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work  Tasks?</h3>
<p><a href='http://arxiv.org/abs/2403.07718v1'>http://arxiv.org/abs/2403.07718v1</a></p>
<p><b>Compressor summary</b>: The study measures large language models' abilities to interact with enterprise software using WorkArena benchmark and BrowserGym environment, finding promise but significant gaps and disparities.</p><hr><h3>StableToolBench: Towards Stable Large-Scale Benchmarking on Tool  Learning of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.07714v1'>http://arxiv.org/abs/2403.07714v1</a></p>
<p><b>Compressor summary</b>: StableToolBench is a benchmark for testing LLMs with external tools that uses a virtual API server, a caching system, and an automatic evaluator to ensure stability and fairness.</p><hr><h3>SSM Meets Video Diffusion Models: Efficient Video Generation with  Structured State Spaces</h3>
<p><a href='http://arxiv.org/abs/2403.07711v1'>http://arxiv.org/abs/2403.07711v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Diffusion models for video generation use attention layers but have memory limitations
- State-space models (SSMs) are proposed as alternatives with linear memory consumption
- SSMs achieve competitive results on UCF101 and MineRL Navigate datasets

Summary:
The paper proposes using state-space models for video generation instead of attention layers, which save memory and maintain performance.</p><hr><h3>Improving Reinforcement Learning from Human Feedback Using Contrastive  Rewards</h3>
<p><a href='http://arxiv.org/abs/2403.07708v1'>http://arxiv.org/abs/2403.07708v1</a></p>
<p><b>Compressor summary</b>: The authors propose a penalty term called contrastive rewards to make reward models more effective in reinforcement learning from human feedback, which improves robustness, calibration, and performance.</p><hr><h3>Fast and Simple Explainability for Point Cloud Networks</h3>
<p><a href='http://arxiv.org/abs/2403.07706v1'>http://arxiv.org/abs/2403.07706v1</a></p>
<p><b>Compressor summary</b>: FBI is a fast and simple XAI method for point cloud data that enables better understanding of the network properties, online feedback, and improved classification explainability.</p><hr><h3>Robust Synthetic-to-Real Transfer for Stereo Matching</h3>
<p><a href='http://arxiv.org/abs/2403.07705v1'>http://arxiv.org/abs/2403.07705v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to fine-tune stereo matching networks without losing their robustness to unseen domains by using pseudo labels and a dynamic framework.</p><hr><h3>Symmetric Q-learning: Reducing Skewness of Bellman Error in Online  Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2403.07704v1'>http://arxiv.org/abs/2403.07704v1</a></p>
<p><b>Compressor summary</b>: Symmetric Q-learning improves deep reinforcement learning by creating a Gaussian error distribution from skewed noise, increasing sample efficiency on continuous control tasks in MuJoCo.</p><hr><h3>CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive  Self-Supervised Transformers</h3>
<p><a href='http://arxiv.org/abs/2403.07700v1'>http://arxiv.org/abs/2403.07700v1</a></p>
<p><b>Compressor summary</b>: VoteCut is a new method that uses multiple self-supervised models to discover objects without labels and improves image segmentation with CuVLER, a zero-shot model trained on pseudo-labels.</p><hr><h3>Large, Small or Both: A Novel Data Augmentation Framework Based on  Language Models for Debiasing Opinion Summarization</h3>
<p><a href='http://arxiv.org/abs/2403.07693v1'>http://arxiv.org/abs/2403.07693v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a data augmentation framework for opinion summarization that uses both large and small language models to generate synthetic negative reviews and balance the sentiment distribution of the dataset.</p><hr><h3>Masked AutoDecoder is Effective Multi-Task Vision Generalist</h3>
<p><a href='http://arxiv.org/abs/2403.07692v1'>http://arxiv.org/abs/2403.07692v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Masked AutoDecoder (MAD), a multi-task vision generalist that uses bi-directional attention and masked sequence modeling to unify different vision tasks in parallel, achieving better performance and efficiency than autoregressive models.</p><hr><h3>Reference-free Monolithic Preference Optimization with Odds Ratio</h3>
<p><a href='http://arxiv.org/abs/2403.07691v1'>http://arxiv.org/abs/2403.07691v1</a></p>
<p><b>Compressor summary</b>: The paper introduces ORPO, a reference model-free algorithm that improves language models by fine-tuning them with odds ratios, achieving better performance than state-of-the-art models.</p><hr><h3>Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of  Neurons</h3>
<p><a href='http://arxiv.org/abs/2403.07688v1'>http://arxiv.org/abs/2403.07688v1</a></p>
<p><b>Compressor summary</b>: The paper reassesses dying neurons in deep neural networks, showing that they can be useful for structured pruning and compression, and introduces Demon Pruning, a simple and effective method to control them.</p><hr><h3>Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model  Performance and Annotation Cost</h3>
<p><a href='http://arxiv.org/abs/2403.07687v1'>http://arxiv.org/abs/2403.07687v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Current foundation models have imbalanced geographical and economic representation in their training data
- More data from underrepresented countries is needed to improve model performance and reduce annotation costs
- The paper proposes methods to identify the data to be annotated based on visual distinctiveness and similarity
- The resulting lists of countries and topics are available online

Summary:
The paper presents methods to balance model performance and annotation costs by identifying and annotating data from underrepresented countries with visually distinctive and similar topics to current foundation models' training data.</p><hr><h3>Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for  Video Adverse Weather Removal</h3>
<p><a href='http://arxiv.org/abs/2403.07684v1'>http://arxiv.org/abs/2403.07684v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for removing adverse weather conditions from videos using test-time adaptation and diffusion-based network, which improves generalization to unseen weather conditions.</p><hr><h3>MoralBERT: Detecting Moral Values in Social Discourse</h3>
<p><a href='http://arxiv.org/abs/2403.07678v1'>http://arxiv.org/abs/2403.07678v1</a></p>
<p><b>Compressor summary</b>: MoralBERT models capture moral nuances in text using annotated data from Twitter, Reddit, and Facebook, improving prediction accuracy compared to traditional methods.</p><hr><h3>Machine Learning for Soccer Match Result Prediction</h3>
<p><a href='http://arxiv.org/abs/2403.07669v1'>http://arxiv.org/abs/2403.07669v1</a></p>
<p><b>Compressor summary</b>: This chapter reviews machine learning methods for soccer match outcome prediction, highlighting current best-performing models, gaps in comparison of deep learning and Random Forest, and potential improvements in features and interpretability.</p><hr><h3>Scalable Spatiotemporal Prediction with Bayesian Neural Fields</h3>
<p><a href='http://arxiv.org/abs/2403.07657v1'>http://arxiv.org/abs/2403.07657v1</a></p>
<p><b>Compressor summary</b>: Bayesian Neural Field (BayesNF) is a statistical model combining a deep neural network and Bayesian inference for spatiotemporal data analysis, outperforming existing methods on large-scale climate and public health datasets.</p><hr><h3>Harder Tasks Need More Experts: Dynamic Routing in MoE Models</h3>
<p><a href='http://arxiv.org/abs/2403.07652v1'>http://arxiv.org/abs/2403.07652v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a dynamic expert selection method for Mixture of Experts models that adjusts the number of activated experts based on input difficulty, improving efficiency and performance.</p><hr><h3>Decomposing Disease Descriptions for Enhanced Pathology Detection: A  Multi-Aspect Vision-Language Matching Framework</h3>
<p><a href='http://arxiv.org/abs/2403.07636v1'>http://arxiv.org/abs/2403.07636v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel VLP framework that dissects disease descriptions into aspects, aligns images with them, and improves detection of known and unknown diseases using a dual-head Transformer.</p><hr><h3>CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs  for Reduced hERG Liability</h3>
<p><a href='http://arxiv.org/abs/2403.07632v1'>http://arxiv.org/abs/2403.07632v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CardioGenAI, a machine learning framework that can re-engineer drugs to reduce their potential to cause heart problems by interfering with a specific ion channel, while maintaining their effectiveness.</p><hr><h3>Hunting Attributes: Context Prototype-Aware Learning for Weakly  Supervised Semantic Segmentation</h3>
<p><a href='http://arxiv.org/abs/2403.07630v1'>http://arxiv.org/abs/2403.07630v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a method called CPAL to improve semantic segmentation by using context-aware prototypes that capture diverse object features and reduce knowledge bias between instances and contexts.</p><hr><h3>Multiple Latent Space Mapping for Compressed Dark Image Enhancement</h3>
<p><a href='http://arxiv.org/abs/2403.07622v1'>http://arxiv.org/abs/2403.07622v1</a></p>
<p><b>Compressor summary</b>: The study proposes a novel latent mapping network based on variational auto-encoder (VAE) to enhance compressed dark images while preserving texture details and avoiding compression artifacts amplification.</p><hr><h3>Smartphone region-wise image indoor localization using deep learning for  indoor tourist attraction</h3>
<p><a href='http://arxiv.org/abs/2403.07621v1'>http://arxiv.org/abs/2403.07621v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using deep learning to classify locations in smart museums and aquariums with smartphone images, achieving high precision and showing good feasibility for indoor tourism attractions.</p><hr><h3>Efficient Knowledge Deletion from Trained Models through Layer-wise  Partial Machine Unlearning</h3>
<p><a href='http://arxiv.org/abs/2403.07611v1'>http://arxiv.org/abs/2403.07611v1</a></p>
<p><b>Compressor summary</b>: The paper introduces new machine unlearning algorithms that selectively erase knowledge from trained models while preserving performance and avoiding post fine-tuning.</p><hr><h3>Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in  Text-To-Image Generation</h3>
<p><a href='http://arxiv.org/abs/2403.07605v1'>http://arxiv.org/abs/2403.07605v1</a></p>
<p><b>Compressor summary</b>: NegOpt is a novel method that optimizes negative prompt generation for text-to-image models using supervised fine-tuning and reinforcement learning, improving image quality by 25% on average.</p><hr><h3>ProPML: Probability Partial Multi-label Learning</h3>
<p><a href='http://arxiv.org/abs/2403.07603v1'>http://arxiv.org/abs/2403.07603v1</a></p>
<p><b>Compressor summary</b>: \our{} is a new probabilistic method for PML that improves performance over existing methods, especially in noisy environments.</p><hr><h3>Unified Source-Free Domain Adaptation</h3>
<p><a href='http://arxiv.org/abs/2403.07601v1'>http://arxiv.org/abs/2403.07601v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel approach called LCFD that discovers causal relationships between latent variables and model decisions for unified source-free domain adaptation, achieving state-of-the-art results.</p><hr><h3>Mondrian: On-Device High-Performance Video Analytics with Compressive  Packed Inference</h3>
<p><a href='http://arxiv.org/abs/2403.07598v1'>http://arxiv.org/abs/2403.07598v1</a></p>
<p><b>Compressor summary</b>: Mondrian is a system that improves object detection on high-resolution videos by selectively processing relevant pixels and combining them efficiently on accelerators like GPUs, achieving higher accuracy and throughput than existing methods.</p><hr><h3>MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D  Sparse Convolutions</h3>
<p><a href='http://arxiv.org/abs/2403.07593v1'>http://arxiv.org/abs/2403.07593v1</a></p>
<p><b>Compressor summary</b>: MinkUNeXt is an efficient 3D place-recognition architecture based on 3D sparse convolutions that surpasses current methods without using complex proposals like Transformers or Attention-Layers.</p><hr><h3>Accurate Spatial Gene Expression Prediction by integrating  Multi-resolution features</h3>
<p><a href='http://arxiv.org/abs/2403.07592v1'>http://arxiv.org/abs/2403.07592v1</a></p>
<p><b>Compressor summary</b>: TRIPLEX is a deep learning framework that predicts spatial gene expression from images, improving on current models and aiding in cancer diagnosis and treatment.</p><hr><h3>Robustifying and Boosting Training-Free Neural Architecture Search</h3>
<p><a href='http://arxiv.org/abs/2403.07591v1'>http://arxiv.org/abs/2403.07591v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new algorithm (RoBoT) that combines and optimizes existing training-free metrics using Bayesian optimization to improve the search performance of neural network design, especially for diverse tasks.</p><hr><h3>PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral  Convolution</h3>
<p><a href='http://arxiv.org/abs/2403.07589v1'>http://arxiv.org/abs/2403.07589v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Large kernel convnets have appealing performance but face challenges due to square complexity of convolution and proliferated parameters
- The paper proposes peripheral convolution, inspired by human vision, that reduces parameter count and complexity of convolution
- The paper also introduces PeLK, a large kernel network that outperforms modern vision Transformers and ConvNet architectures on various tasks

Summary:
The paper presents peripheral convolution, a novel CNN method based on human vision, and PeLK, a large kernel network that achieves superior performance on vision tasks with extremely large kernels.</p><hr><h3>Visual Privacy Auditing with Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2403.07588v1'>http://arxiv.org/abs/2403.07588v1</a></p>
<p><b>Compressor summary</b>: The paper studies how image reconstruction attacks on machine learning models depend on real-world image priors and suggests using diffusion models to assess privacy risks under differential privacy.</p><hr><h3>Perennial Semantic Data Terms of Use for Decentralized Web</h3>
<p><a href='http://arxiv.org/abs/2403.07587v1'>http://arxiv.org/abs/2403.07587v1</a></p>
<p><b>Compressor summary</b>: The authors propose a novel system that allows users to define and automate their data terms of use policies for decentralized web applications like Solid, ensuring better control over their data and improving privacy and usability.</p><hr><h3>LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced  Personality Detection Model</h3>
<p><a href='http://arxiv.org/abs/2403.07581v1'>http://arxiv.org/abs/2403.07581v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to detect personality traits in social media posts using a large language model, text augmentations, and contrastive learning, achieving better results than existing methods.</p><hr><h3>AACP: Aesthetics assessment of children's paintings based on  self-supervised learning</h3>
<p><a href='http://arxiv.org/abs/2403.07578v1'>http://arxiv.org/abs/2403.07578v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-supervised learning model for assessing children's paintings aesthetics, using a novel dataset with labeled attributes and outperforming other methods.</p><hr><h3>FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine  Tuning in High-resolution Medical Image Classification</h3>
<p><a href='http://arxiv.org/abs/2403.07576v1'>http://arxiv.org/abs/2403.07576v1</a></p>
<p><b>Compressor summary</b>: Fine-grained Prompt Tuning (FPT) is a novel method for medical image classification that reduces memory consumption by using a lightweight side network and fine-grained prompts to access pre-trained knowledge from large-scale models.</p><hr><h3>An Active Contour Model Driven By the Hybrid Signed Pressure Function</h3>
<p><a href='http://arxiv.org/abs/2403.07570v1'>http://arxiv.org/abs/2403.07570v1</a></p>
<p><b>Compressor summary</b>: The paper presents an active contour model with a hybrid signed pressure function that combines global and local information to improve image segmentation in complex environments.</p><hr><h3>Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource  Agglutinative Data-to-Text Generation</h3>
<p><a href='http://arxiv.org/abs/2403.07567v1'>http://arxiv.org/abs/2403.07567v1</a></p>
<p><b>Compressor summary</b>: The paper presents T2X, a new data-to-text dataset for isiXhosa, introduces the SSPG model for agglutinative languages, and evaluates various methods for generating text from data.</p><hr><h3>An Improved Strategy for Blood Glucose Control Using Multi-Step Deep  Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2403.07566v1'>http://arxiv.org/abs/2403.07566v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a novel DRL algorithm for blood glucose control that uses multi-step learning and Prioritized Experience Replay, achieving better results than benchmarks in time-in-range.</p><hr><h3>Unleashing Network Potentials for Semantic Scene Completion</h3>
<p><a href='http://arxiv.org/abs/2403.07560v1'>http://arxiv.org/abs/2403.07560v1</a></p>
<p><b>Compressor summary</b>: AMMNet is a novel framework for semantic scene completion that uses cross-modal modulation and adversarial training to improve feature learning and generalization from single-view RGB-D images.</p><hr><h3>SIFiD: Reassess Summary Factual Inconsistency Detection with LLM</h3>
<p><a href='http://arxiv.org/abs/2403.07557v1'>http://arxiv.org/abs/2403.07557v1</a></p>
<p><b>Compressor summary</b>: The study compares GPT-3.5 and GPT-4 for detecting inconsistencies in summaries and proposes SIFiD, a method to identify key sentences for inconsistency detection using LLMs.</p><hr><h3>Truth-Aware Context Selection: Mitigating the Hallucinations of Large  Language Models Being Misled by Untruthful Contexts</h3>
<p><a href='http://arxiv.org/abs/2403.07556v1'>http://arxiv.org/abs/2403.07556v1</a></p>
<p><b>Compressor summary</b>: TACS is a method to help large language models generate better text by filtering out untruthful information from the input context.</p><hr><h3>Online Continual Learning For Interactive Instruction Following Agents</h3>
<p><a href='http://arxiv.org/abs/2403.07548v1'>http://arxiv.org/abs/2403.07548v1</a></p>
<p><b>Compressor summary</b>: The text proposes two realistic scenarios for learning embodied agents, and introduces Confidence-Aware Moving Average (CAMA) method to update logits without task boundary information.</p><hr><h3>SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields</h3>
<p><a href='http://arxiv.org/abs/2403.07547v1'>http://arxiv.org/abs/2403.07547v1</a></p>
<p><b>Compressor summary</b>: SMURF is a novel method that uses Neural-ODEs to model continuous camera motion and volumetric representation for synthesizing high-fidelity views with robustness to motion blur.</p><hr><h3>MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki</h3>
<p><a href='http://arxiv.org/abs/2403.07544v1'>http://arxiv.org/abs/2403.07544v1</a></p>
<p><b>Compressor summary</b>: The MAMMOTH toolkit is a framework for training modular multilingual machine translation systems efficiently across clusters of GPUs and is publicly available online.</p><hr><h3>A Survey of Vision Transformers in Autonomous Driving: Current Trends  and Future Directions</h3>
<p><a href='http://arxiv.org/abs/2403.07542v1'>http://arxiv.org/abs/2403.07542v1</a></p>
<p><b>Compressor summary</b>: The text summarizes how visual transformer models, successful in natural language processing, are being adapted for autonomous driving tasks, such as object detection and scene recognition, due to their advantages in processing dynamic visual scenes.</p><hr><h3>LaB-GATr: geometric algebra transformers for large biomedical surface  and volume meshes</h3>
<p><a href='http://arxiv.org/abs/2403.07536v1'>http://arxiv.org/abs/2403.07536v1</a></p>
<p><b>Compressor summary</b>: LaB-GATr is a transformer neural network that can effectively learn from large-scale medical 3D models by using geometric tokenisation, sequence compression and interpolation, achieving state-of-the-art results in cardiovascular hemodynamics modelling and neurodevelopmental phenotype prediction.</p><hr><h3>Adaptive Fusion of Single-View and Multi-View Depth for Autonomous  Driving</h3>
<p><a href='http://arxiv.org/abs/2403.07535v1'>http://arxiv.org/abs/2403.07535v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new fused depth estimation system that adaptively integrates multi-view and single-view results for robustness against noisy camera poses and challenging conditions.</p><hr><h3>Open-World Semantic Segmentation Including Class Similarity</h3>
<p><a href='http://arxiv.org/abs/2403.07532v1'>http://arxiv.org/abs/2403.07532v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method for autonomous systems to identify and classify novel objects in real-world images without extra training data, enabling better decision-making in tasks like planning or mapping.</p><hr><h3>Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and  Margin Loss</h3>
<p><a href='http://arxiv.org/abs/2403.07518v1'>http://arxiv.org/abs/2403.07518v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Pseudo-OCR, an open-vocabulary text recognition framework that uses character detection and image inpainting to generate pseudo OOV training data from real images and a quality-aware margin loss to train with both real and pseudo data.</p><hr><h3>D4D: An RGBD diffusion model to boost monocular depth estimation</h3>
<p><a href='http://arxiv.org/abs/2403.07516v1'>http://arxiv.org/abs/2403.07516v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to generate realistic RGBD samples using Diffusion4D, which improves deep learning models' performance on monocular depth estimation tasks.</p><hr><h3>Uncertainty-guided Contrastive Learning for Single Source Domain  Generalisation</h3>
<p><a href='http://arxiv.org/abs/2403.07514v1'>http://arxiv.org/abs/2403.07514v1</a></p>
<p><b>Compressor summary</b>: CUDGNet is a novel model that uses contrastive learning and domain generation to improve single domain generalization and provide uncertainty estimation.</p><hr><h3>Spatiotemporal Representation Learning for Short and Long Medical Image  Time Series</h3>
<p><a href='http://arxiv.org/abs/2403.07513v1'>http://arxiv.org/abs/2403.07513v1</a></p>
<p><b>Compressor summary</b>: The paper proposes two methods for analyzing temporal patterns in medical data using deep learning, improving prognosis and diagnosis of conditions like AMD and cardiac output.</p><hr><h3>Relevance Score: A Landmark-Like Heuristic for Planning</h3>
<p><a href='http://arxiv.org/abs/2403.07510v1'>http://arxiv.org/abs/2403.07510v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel "relevance score" for heuristic planning that identifies actions or facts important for most but not all plans to achieve a goal, and shows its improved performance compared to the standard landmark-based approach on problems without clear landmarks.</p><hr><h3>MoAI: Mixture of All Intelligence for Large Language and Vision Models</h3>
<p><a href='http://arxiv.org/abs/2403.07508v1'>http://arxiv.org/abs/2403.07508v1</a></p>
<p><b>Compressor summary</b>: MoAI is a new large language and vision model that uses auxiliary computer vision information for better real-world scene understanding in zero-shot tasks.</p><hr><h3>Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement  Learning Approach</h3>
<p><a href='http://arxiv.org/abs/2403.07503v1'>http://arxiv.org/abs/2403.07503v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a mathematical expression for constrained optimal fuel consumption in hybrid electric vehicles using constrained reinforcement learning and compares two mainstream approaches, finding that Lagrangian-based methods achieve lower fuel consumption with more oscillations than variational policy optimization.</p><hr><h3>Detecting Security-Relevant Methods using Multi-label Machine Learning</h3>
<p><a href='http://arxiv.org/abs/2403.07501v1'>http://arxiv.org/abs/2403.07501v1</a></p>
<p><b>Compressor summary</b>: Dev-Assist is an IntelliJ IDEA plugin that uses multi-label machine learning to detect security-relevant methods in code and automatically configure static analysis tools with better performance than related approaches.</p><hr><h3>Block-wise LoRA: Revisiting Fine-grained LoRA for Effective  Personalization and Stylization in Text-to-Image Generation</h3>
<p><a href='http://arxiv.org/abs/2403.07500v1'>http://arxiv.org/abs/2403.07500v1</a></p>
<p><b>Compressor summary</b>: LoRA is a new technique that improves personalization and stylization in text-to-image generation by fine-tuning different blocks of a diffusion model.</p><hr><h3>Motion Mamba: Efficient and Long Sequence Motion Generation with  Hierarchical and Bidirectional Selective SSM</h3>
<p><a href='http://arxiv.org/abs/2403.07487v1'>http://arxiv.org/abs/2403.07487v1</a></p>
<p><b>Compressor summary</b>: Motion Mamba is a novel approach that combines Hierarchical Temporal Mamba and Bidirectional Spatial Mamba blocks to efficiently generate high-quality human motion sequences using state space models.</p><hr><h3>XpertAI: uncovering model strategies for sub-manifolds</h3>
<p><a href='http://arxiv.org/abs/2403.07486v1'>http://arxiv.org/abs/2403.07486v1</a></p>
<p><b>Compressor summary</b>: XpertAI is a framework that helps explain regression models by breaking them into range-specific sub-strategies and allowing precise queries as linear combinations of those sub-strategies.</p><hr><h3>A Deep Learning Approach to Diabetes Diagnosis</h3>
<p><a href='http://arxiv.org/abs/2403.07483v1'>http://arxiv.org/abs/2403.07483v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a non-invasive diabetes diagnosis method using a neural network with batch normalization, data re-sampling and balancing, which improves accuracy compared to traditional machine learning methods.</p><hr><h3>Imbalance-aware Presence-only Loss Function for Species Distribution  Modeling</h3>
<p><a href='http://arxiv.org/abs/2403.07472v1'>http://arxiv.org/abs/2403.07472v1</a></p>
<p><b>Compressor summary</b>: The study shows that using deep learning models with a balanced presence-only loss function can better model rare species in citizen science datasets, helping with biodiversity conservation.</p><hr><h3>A Comprehensive Survey of 3D Dense Captioning: Localizing and Describing  Objects in 3D Scenes</h3>
<p><a href='http://arxiv.org/abs/2403.07469v1'>http://arxiv.org/abs/2403.07469v1</a></p>
<p><b>Compressor summary</b>: Key points:
- 3D dense captioning is a vision-language bridging task that generates detailed descriptions for 3D scenes
- The paper reviews existing methods, provides a standard pipeline, introduces a taxonomy, and proposes future directions
- The aim is to facilitate research and applications in multimedia and related domains

Summary:
The paper surveys 3D dense captioning, a task that creates accurate descriptions of 3D scenes, and presents a comprehensive review of methods, tools, challenges, and opportunities.</p><hr><h3>Experimental Comparison of Ensemble Methods and Time-to-Event Analysis  Models Through Integrated Brier Score and Concordance Index</h3>
<p><a href='http://arxiv.org/abs/2403.07460v1'>http://arxiv.org/abs/2403.07460v1</a></p>
<p><b>Compressor summary</b>: The paper compares various prediction models for time-to-event analysis, evaluates their performance on three datasets, and explores how ensemble methods can improve accuracy and robustness.</p><hr><h3>A tutorial on multi-view autoencoders using the multi-view-AE library</h3>
<p><a href='http://arxiv.org/abs/2403.07456v1'>http://arxiv.org/abs/2403.07456v1</a></p>
<p><b>Compressor summary</b>: The authors propose a unified mathematical framework for multi-view autoencoders and extend the {multi-view-AE} Python library to provide consistent implementations and improve performance for modelling multi-modal data.</p><hr><h3>Proxy Methods for Domain Adaptation</h3>
<p><a href='http://arxiv.org/abs/2403.07442v1'>http://arxiv.org/abs/2403.07442v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for domain adaptation under distribution shift using proximal causal learning and proxy variables, and shows its effectiveness in two settings: Concept Bottleneck and Multi-domain.</p><hr><h3>Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A  Brain-Inspired Method for Parameter-Efficient Fine-Tuning</h3>
<p><a href='http://arxiv.org/abs/2403.07440v1'>http://arxiv.org/abs/2403.07440v1</a></p>
<p><b>Compressor summary</b>: MTLoRA is a new matrix transformation-based method for efficient fine-tuning of LPLMs that mimics brain function geometry to improve performance on NLU, NLG, and other downstream tasks.</p><hr><h3>Category-Agnostic Pose Estimation for Point Clouds</h3>
<p><a href='http://arxiv.org/abs/2403.07437v1'>http://arxiv.org/abs/2403.07437v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a geometric feature method for object pose estimation that works without category information, achieving similar results to category-based methods.</p><hr><h3>JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object  Detection</h3>
<p><a href='http://arxiv.org/abs/2403.07436v1'>http://arxiv.org/abs/2403.07436v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for event-based moving object detection that uses joint spatio-temporal reasoning and improves accuracy by 13%.</p><hr><h3>Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for  Scene Flow</h3>
<p><a href='http://arxiv.org/abs/2403.07432v1'>http://arxiv.org/abs/2403.07432v1</a></p>
<p><b>Compressor summary</b>: The text proposes a novel framework that uses an event as a bridge between RGB and LiDAR sensors for fusing cross-modal knowledge in scene flow, improving visual and motion features.</p><hr><h3>DragAnything: Motion Control for Anything using Entity Representation</h3>
<p><a href='http://arxiv.org/abs/2403.07420v1'>http://arxiv.org/abs/2403.07420v1</a></p>
<p><b>Compressor summary</b>: DragAnything is a user-friendly motion control method for any object in video generation that uses entity representation and trajectory-based interaction.</p><hr><h3>Learning-Augmented Algorithms with Explicit Predictors</h3>
<p><a href='http://arxiv.org/abs/2403.07413v1'>http://arxiv.org/abs/2403.07413v1</a></p>
<p><b>Compressor summary</b>: The paper proposes online learning algorithms tailored for specific tasks like caching and scheduling, improving performance and robustness by integrating machine learning prediction into the algorithm design.</p><hr><h3>NightHaze: Nighttime Image Dehazing via Self-Prior Learning</h3>
<p><a href='http://arxiv.org/abs/2403.07408v1'>http://arxiv.org/abs/2403.07408v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel nighttime image dehazing method using severe augmentation during training, which improves robustness to real-world degradations and achieves state-of-the-art performance.</p><hr><h3>In-context learning enables multimodal large language models to classify  cancer pathology images</h3>
<p><a href='http://arxiv.org/abs/2403.07407v1'>http://arxiv.org/abs/2403.07407v1</a></p>
<p><b>Compressor summary</b>: In-context learning allows GPT-4V, a large vision language model, to perform well on three cancer histopathology tasks with minimal data and no task-specific fine-tuning.</p><hr><h3>FeTrIL++: Feature Translation for Exemplar-Free Class-Incremental  Learning with Hill-Climbing</h3>
<p><a href='http://arxiv.org/abs/2403.07406v1'>http://arxiv.org/abs/2403.07406v1</a></p>
<p><b>Compressor summary</b>: FeTrIL++ is an improved framework for class-incremental learning that balances accuracy for new and past classes using oversampling techniques and dynamic optimization strategies.</p><hr><h3>Accelerated Inference and Reduced Forgetting: The Dual Benefits of  Early-Exit Networks in Continual Learning</h3>
<p><a href='http://arxiv.org/abs/2403.07404v1'>http://arxiv.org/abs/2403.07404v1</a></p>
<p><b>Compressor summary</b>: The study explores how early-exit networks can be adapted for continual learning, improving efficiency and performance in class-incremental settings with a simple method called Task-wise Logits Correction (TLC).</p><hr><h3>From Canteen Food to Daily Meals: Generalizing Food Recognition to More  Practical Scenarios</h3>
<p><a href='http://arxiv.org/abs/2403.07403v1'>http://arxiv.org/abs/2403.07403v1</a></p>
<p><b>Compressor summary</b>: The paper introduces two new benchmarks for food recognition from daily-life scenarios and a baseline method to improve transferability of existing methods.</p><hr><h3>Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs</h3>
<p><a href='http://arxiv.org/abs/2403.07398v1'>http://arxiv.org/abs/2403.07398v1</a></p>
<p><b>Compressor summary</b>: COM2 is a new dataset that helps language models improve their ability to reason about complex events by generating questions from a commonsense knowledge graph.</p><hr><h3>ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature  Interaction for Dense Predictions</h3>
<p><a href='http://arxiv.org/abs/2403.07392v1'>http://arxiv.org/abs/2403.07392v1</a></p>
<p><b>Compressor summary</b>: ViT-CoMer is a plain, pre-training-free, and feature-enhanced ViT backbone that improves dense prediction tasks by injecting spatial pyramid multi-receptive field convolutional features and proposing a simple CNN-Transformer bidirectional fusion interaction module.</p><hr><h3>Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from  Duplex to Monoplex IHC Images</h3>
<p><a href='http://arxiv.org/abs/2403.07389v1'>http://arxiv.org/abs/2403.07389v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for translating chromogenic immunohistochemistry images to fluorescence images using a novel training design and an auxiliary unpaired image domain, which improves segmentation performance compared to existing methods.</p><hr><h3>SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large  Language Models by Summarizing Training Trajectories of Small Models</h3>
<p><a href='http://arxiv.org/abs/2403.07384v1'>http://arxiv.org/abs/2403.07384v1</a></p>
<p><b>Compressor summary</b>: The SmallToLarge (S2L) method improves data efficiency in supervised fine-tuning for specialized domains by selecting data based on training trajectories from small models, achieving better results with less data and a smaller reference model.</p><hr><h3>Gabor-guided transformer for single image deraining</h3>
<p><a href='http://arxiv.org/abs/2403.07380v1'>http://arxiv.org/abs/2403.07380v1</a></p>
<p><b>Compressor summary</b>: The Gabformer combines CNNs and Transformers with Gabor filters to improve image deraining by enhancing local texture features and robustness to noise.</p><hr><h3>Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The  Lengths, Bends, and Dead Ends</h3>
<p><a href='http://arxiv.org/abs/2403.07379v1'>http://arxiv.org/abs/2403.07379v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new way to understand neural networks by analyzing their optimization trajectories and reveals how different optimization choices affect their behavior.</p><hr><h3>SVD-LLM: Truncation-aware Singular Value Decomposition for Large  Language Model Compression</h3>
<p><a href='http://arxiv.org/abs/2403.07378v1'>http://arxiv.org/abs/2403.07378v1</a></p>
<p><b>Compressor summary</b>: SVD-LLM is a new LLM compression method that improves over existing SVD-based methods by using truncation-aware data whitening and layer-wise model parameter update to reduce compression loss and maintain accuracy.</p><hr><h3>NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning  Disentangled Reasoning</h3>
<p><a href='http://arxiv.org/abs/2403.07376v1'>http://arxiv.org/abs/2403.07376v1</a></p>
<p><b>Compressor summary</b>: This paper proposes NavCoT, a novel strategy to train large language models for vision-and-language navigation tasks by improving their navigational reasoning and interpretability using a chain-of-thought approach.</p><hr><h3>Eliminating Cross-modal Conflicts in BEV Space for LiDAR-Camera 3D  Object Detection</h3>
<p><a href='http://arxiv.org/abs/2403.07372v1'>http://arxiv.org/abs/2403.07372v1</a></p>
<p><b>Compressor summary</b>: The ECFusion method addresses extrinsic and inherent cross-modal conflicts in 3D object detection by aligning spatial distributions and preserving objectness clues, leading to improved performance on the nuScenes dataset.</p><hr><h3>Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of  Altered Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2403.07371v1'>http://arxiv.org/abs/2403.07371v1</a></p>
<p><b>Compressor summary</b>: The study presents a new diffusion-based method for virtual try-on that preserves clothing texture, retains user identity, and is significantly faster than existing approaches.</p><hr><h3>Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized  Visual Class Discovery</h3>
<p><a href='http://arxiv.org/abs/2403.07369v1'>http://arxiv.org/abs/2403.07369v1</a></p>
<p><b>Compressor summary</b>: TextGCD is a multi-modality framework that uses visual-language models to generate descriptive texts for images and leverages textual-visual disparities for novel visual category discovery, achieving superior performance over existing methods.</p><hr><h3>Entropy is not Enough for Test-Time Adaptation: From the Perspective of  Disentangled Factors</h3>
<p><a href='http://arxiv.org/abs/2403.07366v1'>http://arxiv.org/abs/2403.07366v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new test-time adaptation method called DeYO that uses a novel confidence metric based on object shape to mitigate error accumulation in online updates.</p><hr><h3>A New Random Forest Ensemble of Intuitionistic Fuzzy Decision Trees</h3>
<p><a href='http://arxiv.org/abs/2403.07363v1'>http://arxiv.org/abs/2403.07363v1</a></p>
<p><b>Compressor summary</b>: The paper introduces intuitionistic fuzzy random forest, a new classification method that combines random forest and fuzzy logic, and shows its superior performance compared to existing algorithms.</p><hr><h3>Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine  Unlearning</h3>
<p><a href='http://arxiv.org/abs/2403.07362v1'>http://arxiv.org/abs/2403.07362v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new evaluative approach for machine unlearning that identifies the worst-case data subset to erase, using bi-level optimization and experiments on various datasets and models.</p><hr><h3>FSC: Few-point Shape Completion</h3>
<p><a href='http://arxiv.org/abs/2403.07359v1'>http://arxiv.org/abs/2403.07359v1</a></p>
<p><b>Compressor summary</b>: The Few-point Shape Completion (FSC) model uses a dual-branch feature extractor and a two-stage revision network to recover 3D shapes from very sparse point clouds, outperforming previous methods and generalizing well to different objects.</p><hr><h3>Premonition: Using Generative Models to Preempt Future Data Changes in  Continual Learning</h3>
<p><a href='http://arxiv.org/abs/2403.07356v1'>http://arxiv.org/abs/2403.07356v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to generate synthetic data based on text descriptions and use it for pre-training models, which improves their performance in continual learning tasks.</p><hr><h3>BID: Boundary-Interior Decoding for Unsupervised Temporal Action  Localization Pre-Trainin</h3>
<p><a href='http://arxiv.org/abs/2403.07354v1'>http://arxiv.org/abs/2403.07354v1</a></p>
<p><b>Compressor summary</b>: The text introduces BID, an unsupervised framework that partitions motion sequences into meaningful pre-action segments, improving action localization and understanding performance.</p><hr><h3>Graph Unlearning with Efficient Partial Retraining</h3>
<p><a href='http://arxiv.org/abs/2403.07353v1'>http://arxiv.org/abs/2403.07353v1</a></p>
<p><b>Compressor summary</b>: GraphRevoker is a novel framework that improves the unlearning process of GNNs by preserving graph properties and aggregating sub-models effectively.</p><hr><h3>KEBench: A Benchmark on Knowledge Editing for Large Vision-Language  Models</h3>
<p><a href='http://arxiv.org/abs/2403.07350v1'>http://arxiv.org/abs/2403.07350v1</a></p>
<p><b>Compressor summary</b>: The paper introduces KEBench, a new benchmark for knowledge editing in large vision-language models, with an extended metric (Portability) and improved image data quality to better evaluate model performance.</p><hr><h3>Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic  Architecture</h3>
<p><a href='http://arxiv.org/abs/2403.07347v1'>http://arxiv.org/abs/2403.07347v1</a></p>
<p><b>Compressor summary</b>: FD4MM is a new approach to reveal subtle motions in videos by separating and enhancing low-frequency motion fields and high-frequency details using sparse filters and contrastive regularization, achieving better performance and efficiency than existing methods.</p><hr><h3>Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2403.07346v1'>http://arxiv.org/abs/2403.07346v1</a></p>
<p><b>Compressor summary</b>: EvRGBHand is a novel approach for 3D hand mesh reconstruction that combines an event camera and an RGB camera to overcome each other's limitations and improve performance in challenging scenarios.</p><hr><h3>Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive  Learning</h3>
<p><a href='http://arxiv.org/abs/2403.07342v1'>http://arxiv.org/abs/2403.07342v1</a></p>
<p><b>Compressor summary</b>: ASTE is a subtask of sentiment analysis that extracts structured sentiment triplets from text, and the proposed method uses a novel tagging scheme and contrastive learning to improve performance over existing approaches and LLMs.</p><hr><h3>IM-Unpack: Training and Inference with Arbitrarily Low Precision  Integers</h3>
<p><a href='http://arxiv.org/abs/2403.07339v1'>http://arxiv.org/abs/2403.07339v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method (IM-Unpack) to represent heavy hitters in GEMM matrices with low bit-width integers by unpacking them into multiple smaller matrices, achieving efficiency gains and parity with floating point calculations.</p><hr><h3>Large Window-based Mamba UNet for Medical Image Segmentation: Beyond  Convolution and Self-attention</h3>
<p><a href='http://arxiv.org/abs/2403.07332v1'>http://arxiv.org/abs/2403.07332v1</a></p>
<p><b>Compressor summary</b>: LMa-UNet is a novel medical image segmentation method that leverages large windows and a hierarchical Mamba block to achieve efficient long-range dependency modeling with linear complexity.</p><hr><h3>Unknown Domain Inconsistency Minimization for Domain Generalization</h3>
<p><a href='http://arxiv.org/abs/2403.07329v1'>http://arxiv.org/abs/2403.07329v1</a></p>
<p><b>Compressor summary</b>: The paper proposes UDIM, a domain generalization method that minimizes loss inconsistency between source and perturbed domains by perturbing instances from the source dataset and combining it with SAM optimization.</p><hr><h3>SGE: Structured Light System Based on Gray Code with an Event Camera</h3>
<p><a href='http://arxiv.org/abs/2403.07326v1'>http://arxiv.org/abs/2403.07326v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Gray code in event-based structured light systems, enabling fast and accurate depth estimation with high-speed projection and spatio-temporal encoding.</p><hr><h3>GPT-generated Text Detection: Benchmark Dataset and Tensor-based  Detection Method</h3>
<p><a href='http://arxiv.org/abs/2403.07321v1'>http://arxiv.org/abs/2403.07321v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper introduces GRiD, a dataset for detecting ChatGPT-generated text
- The dataset contains Reddit context-prompt pairs with both human and ChatGPT responses
- GpTen is a new semi-supervised tensor-based detection method that performs well on the dataset

Summary:
The paper presents GRiD, a novel dataset to detect ChatGPT-generated text from Reddit contexts, and proposes GpTen, a semi-supervised tensor-based detection method.</p><hr><h3>Efficient Diffusion Model for Image Restoration by Residual Shifting</h3>
<p><a href='http://arxiv.org/abs/2403.07319v1'>http://arxiv.org/abs/2403.07319v1</a></p>
<p><b>Compressor summary</b>: The proposed method improves image restoration by efficiently shifting between high-quality and low-quality images using a Markov chain and a flexible noise schedule, achieving superior or comparable results to current methods with fewer sampling steps.</p><hr><h3>Knowledge Graph Large Language Model (KG-LLM) for Link Prediction</h3>
<p><a href='http://arxiv.org/abs/2403.07311v1'>http://arxiv.org/abs/2403.07311v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new method to predict multiple links in knowledge graphs using natural language processing techniques, such as chain-of-thought prompting and in-context learning, which improve the performance and generalization of large language models.</p><hr><h3>Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM  Framework with Mortality Classifier and Transformer</h3>
<p><a href='http://arxiv.org/abs/2403.07309v1'>http://arxiv.org/abs/2403.07309v1</a></p>
<p><b>Compressor summary</b>: The paper proposes POSNEGDM, a reinforcement learning framework that uses positive and negative demonstrations and individual patient characteristics to guide sepsis treatment, achieving higher survival rates than existing methods.</p><hr><h3>Verification-Aided Learning of Neural Network Barrier Functions with  Termination Guarantees</h3>
<p><a href='http://arxiv.org/abs/2403.07308v1'>http://arxiv.org/abs/2403.07308v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a holistic approach to learn barrier functions for system safety with finite-step termination guarantees, by first learning an NN basis function and then fine-tuning it with convexity and counterexamples from verification failure.</p><hr><h3>Lumen: Unleashing Versatile Vision-Centric Capabilities of Large  Multimodal Models</h3>
<p><a href='http://arxiv.org/abs/2403.07304v1'>http://arxiv.org/abs/2403.07304v1</a></p>
<p><b>Compressor summary</b>: Lumen is a large multimodal model that enhances perception capabilities by decoupling learning into task-agnostic and task-specific stages, improving performance on various visual tasks.</p><hr><h3>Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal  Storyteller</h3>
<p><a href='http://arxiv.org/abs/2403.07301v1'>http://arxiv.org/abs/2403.07301v1</a></p>
<p><b>Compressor summary</b>: LLaMS generates high-quality, multimodal stories from image streams using commonsense knowledge, textual reasoning, and story illustration.</p><hr><h3>Taming Pre-trained LLMs for Generalised Time Series Forecasting via  Cross-modal Knowledge Distillation</h3>
<p><a href='http://arxiv.org/abs/2403.07300v1'>http://arxiv.org/abs/2403.07300v1</a></p>
<p><b>Compressor summary</b>: The LLaTA framework aligns language models and time series data to improve multivariate forecasting by leveraging both static and dynamic knowledge from large language models.</p><hr><h3>Graph Data Condensation via Self-expressive Graph Structure  Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2403.07294v1'>http://arxiv.org/abs/2403.07294v1</a></p>
<p><b>Compressor summary</b>: Graph Data Condensation via Self-expressive Graph Structure Reconstruction (GCSR) is a novel framework that condenses large-scale graphs by incorporating the original graph structure and reconstructing an interpretable synthetic graph.</p>