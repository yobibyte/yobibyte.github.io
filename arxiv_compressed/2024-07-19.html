
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-07-19</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-07-19 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Training-Free Model Merging for Multi-target Domain Adaptation</h3>
<p><a href='http://arxiv.org/abs/2407.13771v1'>http://arxiv.org/abs/2407.13771v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to merge scene understanding models trained on different domains without accessing their training data, using linear model parameter merging and Gaussian prior-based buffer merging.</p><hr><h3>Addressing Imbalance for Class Incremental Learning in Medical Image  Classification</h3>
<p><a href='http://arxiv.org/abs/2407.13768v1'>http://arxiv.org/abs/2407.13768v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The text is about class incremental learning (CIL) in the medical domain, which suffers from catastrophic forgetting when training on new classes.
- The text introduces two plug-in methods to mitigate the imbalance caused by the class overlap and the distribution margin.
- The text reports better performance than state-of-the-art methods on three benchmark datasets.

Summary:
The text presents two simple methods to improve CIL in the medical domain, which address the imbalance problem due to class overlap and distribution margin, and achieve better results than existing methods.</p><hr><h3>Visual Haystacks: Answering Harder Questions About Sets of Images</h3>
<p><a href='http://arxiv.org/abs/2407.13766v1'>http://arxiv.org/abs/2407.13766v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Multi-Image Visual Question Answering (MIQA) as a new task, proposes a public benchmark called "Visual Haystacks," and presents MIRAGE, a novel framework that improves efficiency and accuracy for large multimodal models in this task.</p><hr><h3>Latent Causal Probing: A Formal Perspective on Probing with Causal  Models of Data</h3>
<p><a href='http://arxiv.org/abs/2407.13765v1'>http://arxiv.org/abs/2407.13765v1</a></p>
<p><b>Compressor summary</b>: The authors propose a formal approach using structural causal models (SCM) to analyze and design probing experiments for language models, showing how it helps understand their unsupervised learning of latent causal concepts from text.</p><hr><h3>Shape of Motion: 4D Reconstruction from a Single Video</h3>
<p><a href='http://arxiv.org/abs/2407.13764v1'>http://arxiv.org/abs/2407.13764v1</a></p>
<p><b>Compressor summary</b>: Our method reconstructs dynamic scenes from monocular videos by exploiting low-dimensional structure of 3D motion and using data-driven priors to consolidate supervisory signals.</p><hr><h3>Streetscapes: Large-scale Consistent Street View Generation Using  Autoregressive Video Diffusion</h3>
<p><a href='http://arxiv.org/abs/2407.13759v1'>http://arxiv.org/abs/2407.13759v1</a></p>
<p><b>Compressor summary</b>: The text describes a method for generating realistic long sequences of city views based on language input and map data, using video diffusion and an autoregressive framework with temporal imputation to maintain quality and consistency.</p><hr><h3>Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation  of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.13757v1'>http://arxiv.org/abs/2407.13757v1</a></p>
<p><b>Compressor summary</b>: The paper explores how retrieval-enhanced generative models are vulnerable to black-box attacks that manipulate ranking results and affect user cognition and decision-making.</p><hr><h3>Random Latent Exploration for Deep Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2407.13755v1'>http://arxiv.org/abs/2407.13755v1</a></p>
<p><b>Compressor summary</b>: Random Latent Exploration (RLE) is a new exploration technique for deep reinforcement learning that combines bonus-based and noise-based strategies, adding structured random rewards to encourage agent exploration in certain states.</p><hr><h3>Exploring Facial Biomarkers for Depression through Temporal Analysis of  Action Units</h3>
<p><a href='http://arxiv.org/abs/2407.13753v1'>http://arxiv.org/abs/2407.13753v1</a></p>
<p><b>Compressor summary</b>: The study explores using facial expressions and emotions as objective biomarkers for depression by analyzing video data of people with or without the disorder.</p><hr><h3>LogoSticker: Inserting Logos into Diffusion Models for Customized  Generation</h3>
<p><a href='http://arxiv.org/abs/2407.13752v1'>http://arxiv.org/abs/2407.13752v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Text-to-image model customization can use new concepts with few examples
- Logos are hard to learn for diffusion models due to unique patterns and textual elements
- Logo insertion task aims to insert logos into diffusion models and synthesize them in contexts
- LogoSticker is a two-phase pipeline that pre-trains actor-critic relation and learns decoupled identity of logos
- LogoSticker outperforms customization methods and DALLE~3

Summary:
LogoSticker is a novel logo insertion method that uses a two-phase pipeline to train diffusion models to synthesize logos accurately and harmoniously in various contexts, surpassing existing methods.</p><hr><h3>General Geometry-aware Weakly Supervised 3D Object Detection</h3>
<p><a href='http://arxiv.org/abs/2407.13748v1'>http://arxiv.org/abs/2407.13748v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Paper proposes a general approach for weakly supervised 3D object detection using RGB images and 2D boxes
- Approach consists of three components: prior injection, 2D projection constraint, and 3D geometry constraint
- Method achieves high-quality 3D bounding boxes with only 2D annotation on two datasets

Summary:
The paper presents a general method for weakly supervised 3D object detection that uses RGB images and 2D boxes, and applies three components to obtain accurate 3D boxes without 3D annotations.</p><hr><h3>Multi-Label Learning with Stronger Consistency Guarantees</h3>
<p><a href='http://arxiv.org/abs/2407.13746v1'>http://arxiv.org/abs/2407.13746v1</a></p>
<p><b>Compressor summary</b>: The study proposes a new surrogate loss function for multi-label learning that accounts for label correlations and has optimal consistency guarantees, as well as adapting standard classification losses to multi-label settings.</p><hr><h3>MaRINeR: Enhancing Novel Views by Matching Rendered Images with Nearby  References</h3>
<p><a href='http://arxiv.org/abs/2407.13745v1'>http://arxiv.org/abs/2407.13745v1</a></p>
<p><b>Compressor summary</b>: MaRINeR is a method that uses a nearby mapping image to improve the quality of novel views in Computer Vision and Robotics tasks by matching deep features and transferring details hierarchically.</p><hr><h3>LLMs as Function Approximators: Terminology, Taxonomy, and Questions for  Evaluation</h3>
<p><a href='http://arxiv.org/abs/2407.13744v1'>http://arxiv.org/abs/2407.13744v1</a></p>
<p><b>Compressor summary</b>: The paper discusses how natural language processing models are becoming more generalist and suggests evaluating their strengths and weaknesses based on their ability to approximate specialist functions from natural language specifications.</p><hr><h3>Optimistic Q-learning for average reward and episodic reinforcement  learning</h3>
<p><a href='http://arxiv.org/abs/2407.13743v1'>http://arxiv.org/abs/2407.13743v1</a></p>
<p><b>Compressor summary</b>: The paper introduces an optimistic Q-learning algorithm for average reward reinforcement learning with a relaxed assumption on the frequent state visiting time, and shows a regret bound of O(H^5 SâˆšAT).</p><hr><h3>Scaling Granite Code Models to 128K Context</h3>
<p><a href='http://arxiv.org/abs/2407.13739v1'>http://arxiv.org/abs/2407.13739v1</a></p>
<p><b>Compressor summary</b>: The paper presents long-context Granite code models that improve context handling up to 128K tokens using continual pretraining and instruction-tuned fine-tuning, with no performance drop on regular tasks.</p><hr><h3>Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion  Models: A Tutorial and Review</h3>
<p><a href='http://arxiv.org/abs/2407.13734v1'>http://arxiv.org/abs/2407.13734v1</a></p>
<p><b>Compressor summary</b>: The tutorial covers RL-based methods for optimizing diffusion models to generate samples that maximize desired metrics in biology applications.</p><hr><h3>Realizable $H$-Consistent and Bayes-Consistent Loss Functions for  Learning to Defer</h3>
<p><a href='http://arxiv.org/abs/2407.13732v1'>http://arxiv.org/abs/2407.13732v1</a></p>
<p><b>Compressor summary</b>: The authors study different surrogate loss functions for learning to defer and show their consistency properties under various conditions and hypothesis sets.</p><hr><h3>Baba Is AI: Break the Rules to Beat the Benchmark</h3>
<p><a href='http://arxiv.org/abs/2407.13729v1'>http://arxiv.org/abs/2407.13729v1</a></p>
<p><b>Compressor summary</b>: The text describes a new benchmark for testing multi-modal language models based on the game Baba Is You, where agents have to manipulate both objects and rules to win.</p><hr><h3>Enhanced $H$-Consistency Bounds</h3>
<p><a href='http://arxiv.org/abs/2407.13722v1'>http://arxiv.org/abs/2407.13722v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a general framework to derive better $H$-consistency bounds for surrogate losses by relaxing previous assumptions on the relationship between zero-one estimation error and surrogate loss estimation error.</p><hr><h3>HazeCLIP: Towards Language Guided Real-World Image Dehazing</h3>
<p><a href='http://arxiv.org/abs/2407.13719v1'>http://arxiv.org/abs/2407.13719v1</a></p>
<p><b>Compressor summary</b>: HazeCLIP improves real-world dehazing performance using a language-guided adaptation framework based on CLIP model's ability to distinguish between hazy and clean images.</p><hr><h3>Attention Based Simple Primitives for Open World Compositional Zero-Shot  Learning</h3>
<p><a href='http://arxiv.org/abs/2407.13715v1'>http://arxiv.org/abs/2407.13715v1</a></p>
<p><b>Compressor summary</b>: The study proposes an Open World Compositional Zero-Shot Learning model with self-attention and external knowledge to predict realistic compositions of attributes and objects.</p><hr><h3>FSP-Laplace: Function-Space Priors for the Laplace Approximation in  Bayesian Deep Learning</h3>
<p><a href='http://arxiv.org/abs/2407.13711v1'>http://arxiv.org/abs/2407.13711v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve uncertainty estimates in deep neural networks by placing a prior on function space instead of weight space, using structured and interpretable inductive biases.</p><hr><h3>Understanding Reference Policies in Direct Preference Optimization</h3>
<p><a href='http://arxiv.org/abs/2407.13709v1'>http://arxiv.org/abs/2407.13709v1</a></p>
<p><b>Compressor summary</b>: This paper explores how the choice of reference policy affects Direct Preference Optimization (DPO) for instruction fine-tuning of large language models, and provides guidance on optimal settings and similarities between reference policies and target models.</p><hr><h3>Are We Ready for Out-of-Distribution Detection in Digital Pathology?</h3>
<p><a href='http://arxiv.org/abs/2407.13708v1'>http://arxiv.org/abs/2407.13708v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates OOD detection methods in digital pathology using proper protocols and exploring advanced ML settings.</p><hr><h3>ANHALTEN: Cross-Lingual Transfer for German Token-Level Reference-Free  Hallucination Detection</h3>
<p><a href='http://arxiv.org/abs/2407.13702v1'>http://arxiv.org/abs/2407.13702v1</a></p>
<p><b>Compressor summary</b>: ANHALTEN is a new German dataset for cross-lingual transfer in reference-free hallucination detection that shows the benefits of few-shot learning with minimal annotations.</p><hr><h3>Benchmark Agreement Testing Done Right: A Guide for LLM Benchmark  Evaluation</h3>
<p><a href='http://arxiv.org/abs/2407.13696v1'>http://arxiv.org/abs/2407.13696v1</a></p>
<p><b>Compressor summary</b>: The text discusses the importance of standardized benchmark agreement testing (BAT) for evaluating language models and introduces BenchBench, a python package to facilitate BAT and improve its robustness and validity.</p><hr><h3>HPix: Generating Vector Maps from Satellite Images</h3>
<p><a href='http://arxiv.org/abs/2407.13680v1'>http://arxiv.org/abs/2407.13680v1</a></p>
<p><b>Compressor summary</b>: HPix is a new method that uses GANs to create detailed vector maps from satellite images, overcoming limitations of existing techniques.</p><hr><h3>PASTA: Controllable Part-Aware Shape Generation with Autoregressive  Transformers</h3>
<p><a href='http://arxiv.org/abs/2407.13677v1'>http://arxiv.org/abs/2407.13677v1</a></p>
<p><b>Compressor summary</b>: PASTA is a transformer-based model that can generate realistic and diverse 3D objects by composing cuboidal primitives and synthesizing high quality meshes, using various inputs and manipulating object parts.</p><hr><h3>Non-Asymptotic Uncertainty Quantification in High-Dimensional Learning</h3>
<p><a href='http://arxiv.org/abs/2407.13666v1'>http://arxiv.org/abs/2407.13666v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new data-driven approach for uncertainty quantification in regression that improves confidence intervals for both LASSO and neural network predictors by estimating bias terms from training data.</p><hr><h3>Decision Focused Causal Learning for Direct Counterfactual Marketing  Optimization</h3>
<p><a href='http://arxiv.org/abs/2407.13664v1'>http://arxiv.org/abs/2407.13664v1</a></p>
<p><b>Compressor summary</b>: DFCL is a framework that integrates machine learning and operation research for optimal budget allocation in marketing, addressing technical challenges like uncertainty, counterfactual computation, and computational cost.</p><hr><h3>CogniVoice: Multimodal and Multilingual Fusion Networks for Mild  Cognitive Impairment Assessment from Spontaneous Speech</h3>
<p><a href='http://arxiv.org/abs/2407.13660v1'>http://arxiv.org/abs/2407.13660v1</a></p>
<p><b>Compressor summary</b>: CogniVoice is a new framework that uses speech data and its textual transcriptions to detect mild cognitive impairment (MCI) and estimate mental state scores in multiple languages.</p><hr><h3>FuLG: 150B Romanian Corpus for Language Model Pretraining</h3>
<p><a href='http://arxiv.org/abs/2407.13657v1'>http://arxiv.org/abs/2407.13657v1</a></p>
<p><b>Compressor summary</b>: FuLG is a large Romanian corpus created from CommonCrawl, with a new methodology for filtering and comparing its quality to other Romanian corpora.</p><hr><h3>Weak-to-Strong Reasoning</h3>
<p><a href='http://arxiv.org/abs/2407.13647v1'>http://arxiv.org/abs/2407.13647v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a progressive learning framework that improves the reasoning capabilities of large language models using weaker models without needing external supervision or human-annotated data.</p><hr><h3>Beyond Augmentation: Empowering Model Robustness under Extreme Capture  Environments</h3>
<p><a href='http://arxiv.org/abs/2407.13640v1'>http://arxiv.org/abs/2407.13640v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a multi-mode synchronization learning (MMSL) strategy to improve person re-identification accuracy under extreme capture conditions by applying diverse data augmentation techniques without altering the original image structure.</p><hr><h3>A Comparative Study on Automatic Coding of Medical Letters with  Explainability</h3>
<p><a href='http://arxiv.org/abs/2407.13638v1'>http://arxiv.org/abs/2407.13638v1</a></p>
<p><b>Compressor summary</b>: The study applies NLP and ML to automate medical coding with explainability and light-weighted models using a public database and network models, achieving high accuracy in code prediction.</p><hr><h3>Data Alchemy: Mitigating Cross-Site Model Variability Through Test Time  Data Calibration</h3>
<p><a href='http://arxiv.org/abs/2407.13632v1'>http://arxiv.org/abs/2407.13632v1</a></p>
<p><b>Compressor summary</b>: Data Alchemy is a method to improve cross-site analysis and tumor classification in histopathology images using stain normalization and template learning, without changing network weights or requiring site-specific fine-tuning.</p><hr><h3>Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies</h3>
<p><a href='http://arxiv.org/abs/2407.13623v1'>http://arxiv.org/abs/2407.13623v1</a></p>
<p><b>Compressor summary</b>: The optimal vocabulary size depends on the compute budget and is often overlooked in language model scaling research, leading to under-fitting and suboptimal performance.</p><hr><h3>Misspecified $Q$-Learning with Sparse Linear Function Approximation:  Tight Bounds on Approximation Error</h3>
<p><a href='http://arxiv.org/abs/2407.13622v1'>http://arxiv.org/abs/2407.13622v1</a></p>
<p><b>Compressor summary</b>: The paper by Dong & Yang (2023) explores the sample complexity of obtaining an optimal policy for misspecified sparse linear bandits in reinforcement learning, showing that a novel elimination-based algorithm can achieve suboptimal guarantees with a polynomial number of samples.</p><hr><h3>Differential Privacy Mechanisms in Neural Tangent Kernel Regression</h3>
<p><a href='http://arxiv.org/abs/2407.13621v1'>http://arxiv.org/abs/2407.13621v1</a></p>
<p><b>Compressor summary</b>: The paper studies how differential privacy works in neural network learning and shows that it can protect user data while maintaining accurate predictions.</p><hr><h3>Training-free Composite Scene Generation for Layout-to-Image Synthesis</h3>
<p><a href='http://arxiv.org/abs/2407.13609v1'>http://arxiv.org/abs/2407.13609v1</a></p>
<p><b>Compressor summary</b>: This paper presents a training-free method to generate high-quality images from textual descriptions by resolving token conflicts and improving pixel relationships using attention redistribution.</p><hr><h3>dzNLP at NADI 2024 Shared Task: Multi-Classifier Ensemble with Weighted  Voting and TF-IDF Features</h3>
<p><a href='http://arxiv.org/abs/2407.13608v1'>http://arxiv.org/abs/2407.13608v1</a></p>
<p><b>Compressor summary</b>: The paper describes the dzNLP team's approach to Multi-label Country-level Dialect Identification using various machine learning techniques and achieving high precision but low recall.</p><hr><h3>Physics-guided Active Sample Reweighting for Urban Flow Prediction</h3>
<p><a href='http://arxiv.org/abs/2407.13605v1'>http://arxiv.org/abs/2407.13605v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a physics-guided neural network and a data-aware framework to improve urban flow prediction by addressing the limitations of existing physics-guided machine learning methods.</p><hr><h3>dzStance at StanceEval2024: Arabic Stance Detection based on Sentence  Transformers</h3>
<p><a href='http://arxiv.org/abs/2407.13603v1'>http://arxiv.org/abs/2407.13603v1</a></p>
<p><b>Compressor summary</b>: Sentence Transformers perform better than TF-IDF features in detecting writers' stances on COVID-19 vaccine, digital transformation, and women empowerment, as shown by the team dzStance's results in a stance detection competition.</p><hr><h3>PLANTS: A Novel Problem and Dataset for Summarization of Planning-Like  (PL) Tasks</h3>
<p><a href='http://arxiv.org/abs/2407.13597v1'>http://arxiv.org/abs/2407.13597v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new challenge in summarization called planning-like tasks that involve generating actions to achieve specific goals and proposes a dataset and a baseline method for this problem.</p><hr><h3>EarthMarker: A Visual Prompt Learning Framework for Region-level and  Point-level Remote Sensing Imagery Comprehension</h3>
<p><a href='http://arxiv.org/abs/2407.13596v1'>http://arxiv.org/abs/2407.13596v1</a></p>
<p><b>Compressor summary</b>: EarthMarker is a novel visual prompting model that improves multi-granularity remote sensing imagery interpretation by leveraging natural and RS domain-specific knowledge, cross-domain phased learning, and a new dataset called RSVP.</p><hr><h3>Mechanistically Interpreting a Transformer-based 2-SAT Solver: An  Axiomatic Approach</h3>
<p><a href='http://arxiv.org/abs/2407.13594v1'>http://arxiv.org/abs/2407.13594v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a formal framework for mechanistic interpretability of neural networks using abstract interpretation and demonstrates it on a Transformer-based model solving 2-SAT problems.</p><hr><h3>MeshFeat: Multi-Resolution Features for Neural Fields on Meshes</h3>
<p><a href='http://arxiv.org/abs/2407.13592v1'>http://arxiv.org/abs/2407.13592v1</a></p>
<p><b>Compressor summary</b>: MeshFeat is a new encoding technique for neural fields on meshes that uses multi-resolution feature grids and simplifies the mesh structure to speed up inference and maintain reconstruction quality for texture and BRDF representation.</p><hr><h3>Robust Calibration of Large Vision-Language Adapters</h3>
<p><a href='http://arxiv.org/abs/2407.13588v1'>http://arxiv.org/abs/2407.13588v1</a></p>
<p><b>Compressor summary</b>: The paper tackles miscalibration in CLIP-based model adaptation for out-of-distribution samples and proposes a simple, model-agnostic solution to scale logit ranges.</p><hr><h3>Connecting Consistency Distillation to Score Distillation for Text-to-3D  Generation</h3>
<p><a href='http://arxiv.org/abs/2407.13584v1'>http://arxiv.org/abs/2407.13584v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework to improve text-to-3D generation quality by analyzing and addressing issues like limited detail, low fidelity, and oversaturation in the generated 3D assets.</p><hr><h3>Towards Zero-Shot Multimodal Machine Translation</h3>
<p><a href='http://arxiv.org/abs/2407.13579v1'>http://arxiv.org/abs/2407.13579v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a method (ZeroMMT) to train MMT systems with multimodal English data only
- ZeroMMT adapts a text-only MT model by using visually conditioned masked language modelling and KL divergence between MMT outputs
- ZeroMMT achieves disambiguation performance comparable to state-of-the-art MMT models on CoMMuTE benchmark and can be extended to other languages
- ZeroMMT allows controlling the trade-off between disambiguation and translation fidelity without extra data

Summary:
ZeroMMT is a method to train multimodal machine translation systems using only English data, by adapting a text-only model with visual and diversity objectives. It performs well on disambiguation tasks across languages and can balance disambiguation and translation quality.</p><hr><h3>Large Language Models as Reliable Knowledge Bases?</h3>
<p><a href='http://arxiv.org/abs/2407.13578v1'>http://arxiv.org/abs/2407.13578v1</a></p>
<p><b>Compressor summary</b>: This study evaluates the reliability and effectiveness of large language models as knowledge bases using new metrics and finds that current models have significant limitations in factuality and consistency, regardless of model size or fine-tuning methods.</p><hr><h3>New Capability to Look Up an ASL Sign from a Video Example</h3>
<p><a href='http://arxiv.org/abs/2407.13571v1'>http://arxiv.org/abs/2407.13571v1</a></p>
<p><b>Compressor summary</b>: The system allows users to search for unknown ASL signs by submitting a video and receiving the most likely matches to improve ASL dictionary lookup and annotation efficiency.</p><hr><h3>dzFinNlp at AraFinNLP: Improving Intent Detection in Financial  Conversational Agents</h3>
<p><a href='http://arxiv.org/abs/2407.13565v1'>http://arxiv.org/abs/2407.13565v1</a></p>
<p><b>Compressor summary</b>: The paper introduces dzFinNlp's intent detection system for financial chatbots, using various machine learning and deep learning models, achieving high scores on a benchmark dataset.</p><hr><h3>Research on Tibetan Tourism Viewpoints information generation system  based on LLM</h3>
<p><a href='http://arxiv.org/abs/2407.13561v1'>http://arxiv.org/abs/2407.13561v1</a></p>
<p><b>Compressor summary</b>: The text discusses a study on using an AI system to improve tourism services in Tibet by generating more accurate information about the region's complex topography and historical sites.</p><hr><h3>Qalam : A Multimodal LLM for Arabic Optical Character and Handwriting  Recognition</h3>
<p><a href='http://arxiv.org/abs/2407.13559v1'>http://arxiv.org/abs/2407.13559v1</a></p>
<p><b>Compressor summary</b>: Qalam is a novel foundation model for Arabic OCR and HWR that achieves high accuracy and handles diacritics and high-resolution inputs well.</p><hr><h3>PetFace: A Large-Scale Dataset and Benchmark for Animal Identification</h3>
<p><a href='http://arxiv.org/abs/2407.13555v1'>http://arxiv.org/abs/2407.13555v1</a></p>
<p><b>Compressor summary</b>: PetFace is a large dataset for animal face identification with detailed annotations and benchmarks, helping to advance automated animal recognition methods.</p><hr><h3>On the Discriminability of Self-Supervised Representation Learning</h3>
<p><a href='http://arxiv.org/abs/2407.13541v1'>http://arxiv.org/abs/2407.13541v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes the crowding problem in self-supervised learning (SSL) features and proposes a learnable regulator called Dynamic Semantic Adjuster (DSA) to improve feature separation and aggregation for complex downstream tasks.</p><hr><h3>EnergyDiff: Universal Time-Series Energy Data Generation using Diffusion  Models</h3>
<p><a href='http://arxiv.org/abs/2407.13538v1'>http://arxiv.org/abs/2407.13538v1</a></p>
<p><b>Compressor summary</b>: EnergyDiff is a generative AI framework for creating realistic time series data for energy systems, improving on temporal dependencies and marginal distributions.</p><hr><h3>Evaluating the performance-deviation of itemKNN in RecBole and LensKit</h3>
<p><a href='http://arxiv.org/abs/2407.13531v1'>http://arxiv.org/abs/2407.13531v1</a></p>
<p><b>Compressor summary</b>: ItemKNN algorithms in RecBole and LensKit libraries were compared using four data sets, showing that RecBole performed better on most metrics until similarity matrix calculations were modified in LensKit, resulting in near-identical performance.</p><hr><h3>Discussion: Effective and Interpretable Outcome Prediction by Training  Sparse Mixtures of Linear Experts</h3>
<p><a href='http://arxiv.org/abs/2407.13526v1'>http://arxiv.org/abs/2407.13526v1</a></p>
<p><b>Compressor summary</b>: The authors propose a sparse Mixture-of-Experts model with Logistic Regressors for interpretable outcome prediction from partial process traces, selecting input features automatically during training.</p><hr><h3>Enhancing Source-Free Domain Adaptive Object Detection with  Low-confidence Pseudo Label Distillation</h3>
<p><a href='http://arxiv.org/abs/2407.13524v1'>http://arxiv.org/abs/2407.13524v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new approach called Low-confidence Pseudo Label Distillation (LPLD) to improve source-free domain adaptive object detection by better utilizing low-confidence pseudo labels from Region Proposal Network (RPN).</p><hr><h3>INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question  Answering capability of LLMs for Indic Languages</h3>
<p><a href='http://arxiv.org/abs/2407.13522v1'>http://arxiv.org/abs/2407.13522v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Indic-QA, a large context-grounded question-answering dataset for 11 Indian languages, to evaluate multilingual LLMs' performance in non-English QA tasks.</p><hr><h3>EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian  Splatting</h3>
<p><a href='http://arxiv.org/abs/2407.13520v1'>http://arxiv.org/abs/2407.13520v1</a></p>
<p><b>Compressor summary</b>: EaDeblur-GS is a method that uses event cameras and Gaussian splatting to improve 3D reconstruction from blurry images with complex motion.</p><hr><h3>Model-based Policy Optimization using Symbolic World Model</h3>
<p><a href='http://arxiv.org/abs/2407.13518v1'>http://arxiv.org/abs/2407.13518v1</a></p>
<p><b>Compressor summary</b>: The authors propose using symbolic regression to generate transition dynamics models for robotics, which improves sample efficiency and extrapolation quality in model-based reinforcement learning.</p><hr><h3>Instance Selection for Dynamic Algorithm Configuration with  Reinforcement Learning: Improving Generalization</h3>
<p><a href='http://arxiv.org/abs/2407.13513v1'>http://arxiv.org/abs/2407.13513v1</a></p>
<p><b>Compressor summary</b>: Key points:
- DAC is a challenge of setting hyperparameters for different instances
- Deep RL agents have limited generalization in DAC due to bias in training instances
- The paper proposes instance selection based on time series features to improve generalization
- Empirical evaluations show the benefits of instance selection on DAC benchmarks

Summary:
The paper introduces a method for improving generalization of deep RL agents in dynamic algorithm configuration by selecting representative training instances using time series features.</p><hr><h3>Can Open-Source LLMs Compete with Commercial Models? Exploring the  Few-Shot Performance of Current GPT Models in Biomedical Tasks</h3>
<p><a href='http://arxiv.org/abs/2407.13511v1'>http://arxiv.org/abs/2407.13511v1</a></p>
<p><b>Compressor summary</b>: The paper compares the performance of commercial and open-source large language models in a natural language processing challenge, finding that open-source models are competitive in some settings but need more data for better results.</p><hr><h3>Enhancing Biomedical Knowledge Discovery for Diseases: An End-To-End  Open-Source Framework</h3>
<p><a href='http://arxiv.org/abs/2407.13492v1'>http://arxiv.org/abs/2407.13492v1</a></p>
<p><b>Compressor summary</b>: The framework extracts disease-related knowledge from text, creating annotated datasets for Rett syndrome and Alzheimer's disease, while benchmarking and probing language models' semantic relation detection capabilities.</p><hr><h3>Combining Constraint Programming Reasoning with Large Language Model  Predictions</h3>
<p><a href='http://arxiv.org/abs/2407.13490v1'>http://arxiv.org/abs/2407.13490v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a solution to overcome the challenges in text generation by combining Constraint Programming (CP) and Machine Learning (ML), using a Large Language Model (LLM) to generate words with meaning and CP to handle structural constraints, resulting in faster and better results than standard NLP methods.</p><hr><h3>Similarity over Factuality: Are we making progress on multimodal  out-of-context misinformation detection?</h3>
<p><a href='http://arxiv.org/abs/2407.13488v1'>http://arxiv.org/abs/2407.13488v1</a></p>
<p><b>Compressor summary</b>: The study introduces MUSE, a simple but robust baseline for detecting out-of-context misinformation by comparing image-text pairs with external evidence, and shows its effectiveness on two datasets.</p><hr><h3>Attention Overflow: Language Model Input Blur during Long-Context  Missing Items Recommendation</h3>
<p><a href='http://arxiv.org/abs/2407.13481v1'>http://arxiv.org/abs/2407.13481v1</a></p>
<p><b>Compressor summary</b>: Large language models struggle to suggest missing elements in long lists due to attention overflow, which can be mitigated by iterative loops but at a cost of novelty loss.</p><hr><h3>Fixed and Adaptive Simultaneous Machine Translation Strategies Using  Adapters</h3>
<p><a href='http://arxiv.org/abs/2407.13469v1'>http://arxiv.org/abs/2407.13469v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using lightweight adapter modules in machine translation models to achieve multiple latency levels without training separate models, and demonstrates improved performance over existing methods.</p><hr><h3>End-To-End Clinical Trial Matching with Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.13463v1'>http://arxiv.org/abs/2407.13463v1</a></p>
<p><b>Compressor summary</b>: The text describes an automated system using large language models that can match cancer patients to clinical trials more accurately and efficiently than human experts.</p><hr><h3>All Roads Lead to Rome? Exploring Representational Similarities Between  Latent Spaces of Generative Image Models</h3>
<p><a href='http://arxiv.org/abs/2407.13449v1'>http://arxiv.org/abs/2407.13449v1</a></p>
<p><b>Compressor summary</b>: The study measures how similar different image generation models are by creating linear maps between their latent spaces and finds that they learn similar representations, especially for gender in CelebA models.</p><hr><h3>BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in  Vision-language Models</h3>
<p><a href='http://arxiv.org/abs/2407.13442v1'>http://arxiv.org/abs/2407.13442v1</a></p>
<p><b>Compressor summary</b>: Key points:
- VLMs use visual encoder and LLM to perceive the world
- VLMs are prone to hallucination, which reduces trustworthiness
- The authors introduce BEAF dataset and new metrics (TU, IG, SB, ID) to measure hallucination based on scene changes
- The new metrics reveal different aspects of VLM hallucination that have not been reported before

Summary:
The paper proposes a new benchmark and metrics for measuring hallucination in vision language models (VLMs), which use a visual encoder and a large language model to perceive the world. The benchmark manipulates scene information by image editing and evaluates VLMs on their ability to detect changes.</p><hr><h3>Enhancing Out-of-Vocabulary Performance of Indian TTS Systems for  Practical Applications through Low-Effort Data Strategies</h3>
<p><a href='http://arxiv.org/abs/2407.13435v1'>http://arxiv.org/abs/2407.13435v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a cheap way to collect more TTS training data for low-resource languages like Hindi and Tamil by using volunteers instead of professional voice artists, which improves out-of-vocabulary word pronunciation without affecting voice quality or in-domain performance.</p><hr><h3>Improving Out-of-Distribution Generalization of Trajectory Prediction  for Autonomous Driving via Polynomial Representations</h3>
<p><a href='http://arxiv.org/abs/2407.13431v1'>http://arxiv.org/abs/2407.13431v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new OoD testing protocol for trajectory prediction models that homogenizes datasets and tasks, introduces a polynomial-based algorithm for smaller and faster models with near SotA ID performance and improved OoD robustness, and studies the effects of two augmentation strategies on model generalization.</p><hr><h3>Towards Dynamic Feature Acquisition on Medical Time Series by Maximizing  Conditional Mutual Information</h3>
<p><a href='http://arxiv.org/abs/2407.13429v1'>http://arxiv.org/abs/2407.13429v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to train acquirers for multivariate time series using conditional mutual information to improve performance and reduce costs.</p><hr><h3>WiNet: Wavelet-based Incremental Learning for Efficient Medical Image  Registration</h3>
<p><a href='http://arxiv.org/abs/2407.13426v1'>http://arxiv.org/abs/2407.13426v1</a></p>
<p><b>Compressor summary</b>: WiNet estimates scale-wise wavelet coefficients for displacement/velocity fields using the wavelet transform, enabling fast and explainable image registration with low memory usage.</p><hr><h3>CycleMix: Mixing Source Domains for Domain Generalization in  Style-Dependent Data</h3>
<p><a href='http://arxiv.org/abs/2407.13421v1'>http://arxiv.org/abs/2407.13421v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve deep learning-based image classification by using CycleGAN to learn and disregard image styles, enhancing generalization ability.</p><hr><h3>From Words to Worlds: Compositionality for Cognitive Architectures</h3>
<p><a href='http://arxiv.org/abs/2407.13419v1'>http://arxiv.org/abs/2407.13419v1</a></p>
<p><b>Compressor summary</b>: The study examines how different factors affect the compositionality of large language models and identifies challenges for improving their abilities to learn compositional strategies.</p><hr><h3>GDDS: A Single Domain Generalized Defect Detection Frame of Open World  Scenario using Gather and Distribute Domain-shift Suppression Network</h3>
<p><a href='http://arxiv.org/abs/2407.13417v1'>http://arxiv.org/abs/2407.13417v1</a></p>
<p><b>Compressor summary</b>: The GDDS method detects surface defects on photovoltaic modules using a single domain generalized approach, improving accuracy and speed while addressing distribution shift and normalized Wasserstein distance for similarity measurement.</p><hr><h3>Correcting the Mythos of KL-Regularization: Direct Alignment without  Overparameterization via Chi-squared Preference Optimization</h3>
<p><a href='http://arxiv.org/abs/2407.13399v1'>http://arxiv.org/abs/2407.13399v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new algorithm, $\chi^2$-Preference Optimization ($\chi$PO), which improves sample-efficiency in offline language model alignment by mitigating overoptimization using the $\chi^2$-divergence.</p><hr><h3>PICASSO: A Feed-Forward Framework for Parametric Inference of CAD  Sketches via Rendering Self-Supervision</h3>
<p><a href='http://arxiv.org/abs/2407.13394v1'>http://arxiv.org/abs/2407.13394v1</a></p>
<p><b>Compressor summary</b>: PICASSO is a novel framework that can learn parametric CAD sketches from precise or hand-drawn images by using self-supervised rendering techniques and geometric cues.</p><hr><h3>GeometrySticker: Enabling Ownership Claim of Recolorized Neural Radiance  Fields</h3>
<p><a href='http://arxiv.org/abs/2407.13390v1'>http://arxiv.org/abs/2407.13390v1</a></p>
<p><b>Compressor summary</b>: GeometrySticker is a method for embedding binary messages into the geometry components of NeRF models to protect copyright, while preserving effectiveness against recolorization.</p><hr><h3>Open-World Visual Reasoning by a Neuro-Symbolic Program of Zero-Shot  Symbols</h3>
<p><a href='http://arxiv.org/abs/2407.13382v1'>http://arxiv.org/abs/2407.13382v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a neuro-symbolic approach to find object configurations in images, using first-order logic and language-vision models, and shows its applicability to real-world scenarios.</p><hr><h3>Removing cloud shadows from ground-based solar imagery</h3>
<p><a href='http://arxiv.org/abs/2407.13379v1'>http://arxiv.org/abs/2407.13379v1</a></p>
<p><b>Compressor summary</b>: The study introduces a new method using U-Net architecture and conditional GAN to remove cloud shadows from solar images for better space weather prediction.</p><hr><h3>Linear-Complexity Self-Supervised Learning for Speech Processing</h3>
<p><a href='http://arxiv.org/abs/2407.13377v1'>http://arxiv.org/abs/2407.13377v1</a></p>
<p><b>Compressor summary</b>: This paper introduces SummaryMixing, a linear-complexity context encoder for self-supervised learning that reduces pre-training time and resources while maintaining or improving performance on downstream tasks.</p><hr><h3>Any Image Restoration with Efficient Automatic Degradation Adaptation</h3>
<p><a href='http://arxiv.org/abs/2407.13372v1'>http://arxiv.org/abs/2407.13372v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a unified model that efficiently restores degraded images using joint embedding, gated reweighting, and contextualized attention.</p><hr><h3>Affordance Perception by a Knowledge-Guided Vision-Language Model with  Efficient Error Correction</h3>
<p><a href='http://arxiv.org/abs/2407.13368v1'>http://arxiv.org/abs/2407.13368v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an improved method for robots to understand and interact with objects in open world settings by combining affordance representation, vision-language models, and human feedback.</p><hr><h3>Geometric Active Exploration in Markov Decision Processes: the Benefit  of Abstraction</h3>
<p><a href='http://arxiv.org/abs/2407.13364v1'>http://arxiv.org/abs/2407.13364v1</a></p>
<p><b>Compressor summary</b>: The GAE algorithm combines AE and MDP homomorphisms to explore dynamical systems' state spaces more efficiently for scientific discovery using geometric structures.</p><hr><h3>Open Vocabulary 3D Scene Understanding via Geometry Guided  Self-Distillation</h3>
<p><a href='http://arxiv.org/abs/2407.13362v1'>http://arxiv.org/abs/2407.13362v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Geometry Guided Self-Distillation (GGSD), which improves open vocabulary 3D scene understanding by leveraging geometric priors from 2D data and enhancing representation learning with self-distillation.</p><hr><h3>Capturing Style in Author and Document Representation</h3>
<p><a href='http://arxiv.org/abs/2407.13358v1'>http://arxiv.org/abs/2407.13358v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new NLP model that learns embeddings for authors and documents with a focus on capturing their writing style, and shows its effectiveness on three datasets.</p><hr><h3>Learning-From-Mistakes Prompting for Indigenous Language Translation</h3>
<p><a href='http://arxiv.org/abs/2407.13343v1'>http://arxiv.org/abs/2407.13343v1</a></p>
<p><b>Compressor summary</b>: The paper proposes techniques to improve translation for indigenous languages with little data using large language models and specific prompting methods.</p><hr><h3>Implicit Filtering for Learning Neural Signed Distance Functions from 3D  Point Clouds</h3>
<p><a href='http://arxiv.org/abs/2407.13342v1'>http://arxiv.org/abs/2407.13342v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method to reconstruct surfaces with fine-grained details using neural signed distance functions and a non-linear implicit filter.</p><hr><h3>Learn to Memorize and to Forget: A Continual Learning Perspective of  Dynamic SLAM</h3>
<p><a href='http://arxiv.org/abs/2407.13338v1'>http://arxiv.org/abs/2407.13338v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel SLAM framework for dynamic environments that leverages continual learning, forgetting, and object identification to improve robustness.</p><hr><h3>Long-Term 3D Point Tracking By Cost Volume Fusion</h3>
<p><a href='http://arxiv.org/abs/2407.13337v1'>http://arxiv.org/abs/2407.13337v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new deep learning framework for long-term 3D point tracking that generalizes well and outperforms prior methods without test-time fine-tuning.</p><hr><h3>OAT: Object-Level Attention Transformer for Gaze Scanpath Prediction</h3>
<p><a href='http://arxiv.org/abs/2407.13335v1'>http://arxiv.org/abs/2407.13335v1</a></p>
<p><b>Compressor summary</b>: The Object-level Attention Transformer (OAT) is a model that predicts human visual search behavior by using object features and a new positional encoding to generate gaze scanpaths in cluttered scenes.</p><hr><h3>Reconstruct the Pruned Model without Any Retraining</h3>
<p><a href='http://arxiv.org/abs/2407.13331v1'>http://arxiv.org/abs/2407.13331v1</a></p>
<p><b>Compressor summary</b>: LIAR is an efficient and effective compression technique for large language models that preserves high accuracy by using linear interpolation to reconstruct pruned weights.</p><hr><h3>Why do you cite? An investigation on citation intents and  decision-making classification processes</h3>
<p><a href='http://arxiv.org/abs/2407.13329v1'>http://arxiv.org/abs/2407.13329v1</a></p>
<p><b>Compressor summary</b>: The authors propose an approach to classify citation intents using advanced Ensemble Strategies with Language Models and Explainable AI techniques, showing that section titles improve performances and providing a web application for this purpose.</p><hr><h3>Unsupervised Domain Adaptive Lane Detection via Contextual Contrast and  Aggregation</h3>
<p><a href='http://arxiv.org/abs/2407.13328v1'>http://arxiv.org/abs/2407.13328v1</a></p>
<p><b>Compressor summary</b>: DACCA is a novel method for domain-adaptive lane detection that uses cross-domain contrastive loss and feature aggregation to improve feature learning and knowledge transfer across domains, achieving superior performance on six datasets.</p><hr><h3>RISC-V RVV efficiency for ANN algorithms</h3>
<p><a href='http://arxiv.org/abs/2407.13326v1'>http://arxiv.org/abs/2407.13326v1</a></p>
<p><b>Compressor summary</b>: This study applies RISC-V's vector extension RVV to optimize common ANN algorithms for high-performance computing using a parameterized vector block model.</p><hr><h3>Fully Test-Time rPPG Estimation via Synthetic Signal-Guided Feature  Learning</h3>
<p><a href='http://arxiv.org/abs/2407.13322v1'>http://arxiv.org/abs/2407.13322v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel Test-Time Adaptation framework for remote photoplethysmography estimation, which adapts to various domain information and heart rate distributions using synthetic signals and spectral-based entropy minimization.</p><hr><h3>Sortability of Time Series Data</h3>
<p><a href='http://arxiv.org/abs/2407.13313v1'>http://arxiv.org/abs/2407.13313v1</a></p>
<p><b>Compressor summary</b>: The paper studies how dataset characteristics, such as varsortability and R2-sortability, affect the performance of causal discovery algorithms for time-dependent processes using various types of real and simulated data.</p><hr><h3>Exposure Completing for Temporally Consistent Neural High Dynamic Range  Video Rendering</h3>
<p><a href='http://arxiv.org/abs/2407.13309v1'>http://arxiv.org/abs/2407.13309v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to render high dynamic range videos from low dynamic range videos by completing missing exposure information and improving temporal consistency, resulting in state-of-the-art performance.</p><hr><h3>A Dataset and Benchmark for Shape Completion of Fruits for Agricultural  Robotics</h3>
<p><a href='http://arxiv.org/abs/2407.13304v1'>http://arxiv.org/abs/2407.13304v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a 3D shape completion dataset for agricultural vision systems using RGB-D frames and high-precision point clouds of sweet peppers in lab and greenhouse conditions.
- The dataset aims to help autonomous robotic systems estimate the complete 3D shapes of fruits despite occlusions, which is challenging due to cluttered agricultural environments.
- The paper also provides segmented RGB-D frames with camera intrinsic parameters and a public challenge on a benchmark server to evaluate shape completion approaches.

Summary:
The paper introduces a new dataset for 3D shape completion of fruits in agricultural settings, using RGB-D frames and point clouds, to enable autonomous robotic systems to harvest fruits more efficiently.</p><hr><h3>Mean Teacher based SSL Framework for Indoor Localization Using Wi-Fi  RSSI Fingerprinting</h3>
<p><a href='http://arxiv.org/abs/2407.13303v1'>http://arxiv.org/abs/2407.13303v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a semi-supervised learning framework for neural networks that uses unlabeled Wi-Fi fingerprints to improve indoor localization performance and can handle hybrid databases and continual expansion.</p><hr><h3>CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis</h3>
<p><a href='http://arxiv.org/abs/2407.13301v1'>http://arxiv.org/abs/2407.13301v1</a></p>
<p><b>Compressor summary</b>: The study proposes Chain-of-Diagnosis (CoD), a method to improve the interpretability and controllability of large language models for medical diagnosis by creating a diagnostic chain that resembles a physician's thought process.</p><hr><h3>Robust ASR Error Correction with Conservative Data Filtering</h3>
<p><a href='http://arxiv.org/abs/2407.13300v1'>http://arxiv.org/abs/2407.13300v1</a></p>
<p><b>Compressor summary</b>: Our method filters low-quality error correction data to prevent overcorrection and improve automatic speech recognition performance in out-of-domain settings using Japanese language models.</p><hr><h3>SpeciaLex: A Benchmark for In-Context Specialized Lexicon Learning</h3>
<p><a href='http://arxiv.org/abs/2407.13297v1'>http://arxiv.org/abs/2407.13297v1</a></p>
<p><b>Compressor summary</b>: SpeciaLex is a benchmark to assess language models' ability to follow specialized lexicon constraints for various tasks and audiences.</p><hr><h3>Hierarchical Stage-Wise Training of Linked Deep Neural Networks for  Multi-Building and Multi-Floor Indoor Localization Based on Wi-Fi RSSI  Fingerprinting</h3>
<p><a href='http://arxiv.org/abs/2407.13288v1'>http://arxiv.org/abs/2407.13288v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new indoor localization method using linked neural networks trained in a hierarchical stage-wise way, achieving the most accurate results with the UJIIndoorLoc database.</p><hr><h3>Collaborative real-time vision-based device for olive oil production  monitoring</h3>
<p><a href='http://arxiv.org/abs/2407.13285v1'>http://arxiv.org/abs/2407.13285v1</a></p>
<p><b>Compressor summary</b>: The paper presents a computer-vision system that detects and warns about foreign objects in an olive grinder to prevent quality issues and machinery damage.</p><hr><h3>Auditing Local Explanations is Hard</h3>
<p><a href='http://arxiv.org/abs/2407.13281v1'>http://arxiv.org/abs/2407.13281v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an auditing framework to check the consistency of machine learning algorithms' explanations, but shows that it requires a large number of queries and highlights the importance of locality in explainability.</p><hr><h3>Analyzing and Bridging the Gap between Maximizing Total Reward and  Discounted Reward in Deep Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2407.13279v1'>http://arxiv.org/abs/2407.13279v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes how using discounted reward as a proxy for total reward affects deep reinforcement learning and proposes conditions to align optimal policies of these two objectives.</p><hr><h3>Deep Time Series Models: A Comprehensive Survey and Benchmark</h3>
<p><a href='http://arxiv.org/abs/2407.13278v1'>http://arxiv.org/abs/2407.13278v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Time series are sequences of data points in discrete-time order with complex and dynamic patterns.
- Deep learning models have advanced the analysis of time series across various tasks.
- The paper reviews existing literature, introduces Time Series Library (TSLib), and evaluates 12 deep time series models on different tasks.

Summary:
The paper surveys deep learning models for time series analysis, introduces a benchmark library (TSLib) with 24 models, 30 datasets, and five tasks, and assesses 12 models on different tasks.</p><hr><h3>Mixture of Experts based Multi-task Supervise Learning from Crowds</h3>
<p><a href='http://arxiv.org/abs/2407.13268v1'>http://arxiv.org/abs/2407.13268v1</a></p>
<p><b>Compressor summary</b>: This paper introduces a new multi-task learning approach for truth inference in crowdsourcing, which improves the accuracy and effectiveness of worker behavior models by focusing on item features rather than hidden ground truth variables.</p><hr><h3>Unveiling Structural Memorization: Structural Membership Inference  Attack for Text-to-Image Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2407.13252v1'>http://arxiv.org/abs/2407.13252v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new method for protecting privacy in text-to-image models by detecting if an image was used to train them based on the preservation of image structures during the diffusion process.</p><hr><h3>Motif-Consistent Counterfactuals with Adversarial Refinement for  Graph-Level Anomaly Detection</h3>
<p><a href='http://arxiv.org/abs/2407.13251v1'>http://arxiv.org/abs/2407.13251v1</a></p>
<p><b>Compressor summary</b>: MotifCAR is a novel graph anomaly detection method that uses motifs and GANs to create realistic, valid, proximal, and sparse counterfactual graphs for improved performance.</p><hr><h3>Are Large Language Models Capable of Generating Human-Level Narratives?</h3>
<p><a href='http://arxiv.org/abs/2407.13248v1'>http://arxiv.org/abs/2407.13248v1</a></p>
<p><b>Compressor summary</b>: The paper explores how LLMs struggle with storytelling, especially creating suspense and diversity, and proposes a framework to improve their narrative skills.</p><hr><h3>PM-LLM-Benchmark: Evaluating Large Language Models on Process Mining  Tasks</h3>
<p><a href='http://arxiv.org/abs/2407.13244v1'>http://arxiv.org/abs/2407.13244v1</a></p>
<p><b>Compressor summary</b>: The paper introduces PM-LLM-Benchmark, a comprehensive benchmark for evaluating open-source large language models in process mining tasks, and discusses the challenges and limitations of such a benchmark.</p><hr><h3>NODER: Image Sequence Regression Based on Neural Ordinary Differential  Equations</h3>
<p><a href='http://arxiv.org/abs/2407.13241v1'>http://arxiv.org/abs/2407.13241v1</a></p>
<p><b>Compressor summary</b>: The paper introduces NODER, a novel framework that uses neural ODEs to model complex dynamics in medical image sequences and achieves state-of-the-art 3D image regression performance with reduced computational cost and practical applicability.</p><hr><h3>Transformers with Stochastic Competition for Tabular Data Modelling</h3>
<p><a href='http://arxiv.org/abs/2407.13238v1'>http://arxiv.org/abs/2407.13238v1</a></p>
<p><b>Compressor summary</b>: A new Transformer-based deep learning model for tabular data uses stochastic competition to promote generalization capacity and outperforms gradient boosted decision trees on various datasets.</p><hr><h3>LLM-Empowered State Representation for Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2407.13237v1'>http://arxiv.org/abs/2407.13237v1</a></p>
<p><b>Compressor summary</b>: The paper proposes LESR, a method that uses large language models to generate task-related state representation codes for better reinforcement learning performance.</p><hr><h3>Evaluating Large Language Models for Anxiety and Depression  Classification using Counseling and Psychotherapy Transcripts</h3>
<p><a href='http://arxiv.org/abs/2407.13228v1'>http://arxiv.org/abs/2407.13228v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Non-Contact Breath Rate Classification Using SVM Model and mmWave Radar  Sensor Data</h3>
<p><a href='http://arxiv.org/abs/2407.13222v1'>http://arxiv.org/abs/2407.13222v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Multimodal Label Relevance Ranking via Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2407.13221v1'>http://arxiv.org/abs/2407.13221v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Multi-sentence Video Grounding for Long Video Generation</h3>
<p><a href='http://arxiv.org/abs/2407.13219v1'>http://arxiv.org/abs/2407.13219v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>LiNR: Model Based Neural Retrieval on GPUs at LinkedIn</h3>
<p><a href='http://arxiv.org/abs/2407.13218v1'>http://arxiv.org/abs/2407.13218v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>LIDIA: Precise Liver Tumor Diagnosis on Multi-Phase Contrast-Enhanced CT  via Iterative Fusion and Asymmetric Contrastive Learning</h3>
<p><a href='http://arxiv.org/abs/2407.13217v1'>http://arxiv.org/abs/2407.13217v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>TXL-PBC: a freely accessible labeled peripheral blood cell dataset</h3>
<p><a href='http://arxiv.org/abs/2407.13214v1'>http://arxiv.org/abs/2407.13214v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Research on Image Super-Resolution Reconstruction Mechanism based on  Convolutional Neural Network</h3>
<p><a href='http://arxiv.org/abs/2407.13211v1'>http://arxiv.org/abs/2407.13211v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Transformer-based Single-Cell Language Model: A Survey</h3>
<p><a href='http://arxiv.org/abs/2407.13205v1'>http://arxiv.org/abs/2407.13205v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Adapt PointFormer: 3D Point Cloud Analysis via Adapting 2D Visual  Transformers</h3>
<p><a href='http://arxiv.org/abs/2407.13200v1'>http://arxiv.org/abs/2407.13200v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Adaptive Foundation Models for Online Decisions: HyperAgent with Fast  Incremental Uncertainty Estimation</h3>
<p><a href='http://arxiv.org/abs/2407.13195v1'>http://arxiv.org/abs/2407.13195v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Robust Multivariate Time Series Forecasting against Intra- and  Inter-Series Transitional Shift</h3>
<p><a href='http://arxiv.org/abs/2407.13194v1'>http://arxiv.org/abs/2407.13194v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Retrieval-Augmented Generation for Natural Language Processing: A Survey</h3>
<p><a href='http://arxiv.org/abs/2407.13193v1'>http://arxiv.org/abs/2407.13193v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Safe-SD: Safe and Traceable Stable Diffusion with Text Prompt Trigger  for Invisible Generative Watermarking</h3>
<p><a href='http://arxiv.org/abs/2407.13188v1'>http://arxiv.org/abs/2407.13188v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>KFD-NeRF: Rethinking Dynamic NeRF with Kalman Filter</h3>
<p><a href='http://arxiv.org/abs/2407.13185v1'>http://arxiv.org/abs/2407.13185v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>HSEmotion Team at the 7th ABAW Challenge: Multi-Task Learning and  Compound Facial Expression Recognition</h3>
<p><a href='http://arxiv.org/abs/2407.13184v1'>http://arxiv.org/abs/2407.13184v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>SpaDiT: Diffusion Transformer for Spatial Gene Expression Prediction  using scRNA-seq</h3>
<p><a href='http://arxiv.org/abs/2407.13182v1'>http://arxiv.org/abs/2407.13182v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Training-Free Large Model Priors for Multiple-in-One Image Restoration</h3>
<p><a href='http://arxiv.org/abs/2407.13181v1'>http://arxiv.org/abs/2407.13181v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>The use of the symmetric finite difference in the local binary pattern  (symmetric LBP)</h3>
<p><a href='http://arxiv.org/abs/2407.13178v1'>http://arxiv.org/abs/2407.13178v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Compressed models are NOT miniature versions of large models</h3>
<p><a href='http://arxiv.org/abs/2407.13174v1'>http://arxiv.org/abs/2407.13174v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>SciCode: A Research Coding Benchmark Curated by Scientists</h3>
<p><a href='http://arxiv.org/abs/2407.13168v1'>http://arxiv.org/abs/2407.13168v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Translate-and-Revise: Boosting Large Language Models for Constrained  Translation</h3>
<p><a href='http://arxiv.org/abs/2407.13164v1'>http://arxiv.org/abs/2407.13164v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Attenuation-Aware Weighted Optical Flow with Medium Transmission Map for  Learning-based Visual Odometry in Underwater terrain</h3>
<p><a href='http://arxiv.org/abs/2407.13159v1'>http://arxiv.org/abs/2407.13159v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>HHGT: Hierarchical Heterogeneous Graph Transformer for Heterogeneous  Graph Representation Learning</h3>
<p><a href='http://arxiv.org/abs/2407.13158v1'>http://arxiv.org/abs/2407.13158v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Preset-Voice Matching for Privacy Regulated Speech-to-Speech Translation  Systems</h3>
<p><a href='http://arxiv.org/abs/2407.13153v1'>http://arxiv.org/abs/2407.13153v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>DFMSD: Dual Feature Masking Stage-wise Knowledge Distillation for Object  Detection</h3>
<p><a href='http://arxiv.org/abs/2407.13147v1'>http://arxiv.org/abs/2407.13147v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>PG-Rainbow: Using Distributional Reinforcement Learning in Policy  Gradient Methods</h3>
<p><a href='http://arxiv.org/abs/2407.13146v1'>http://arxiv.org/abs/2407.13146v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Integrated Hardware Architecture and Device Placement Search</h3>
<p><a href='http://arxiv.org/abs/2407.13143v1'>http://arxiv.org/abs/2407.13143v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>A light-weight and efficient punctuation and word casing prediction  model for on-device streaming ASR</h3>
<p><a href='http://arxiv.org/abs/2407.13142v1'>http://arxiv.org/abs/2407.13142v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Out-of-Distribution Detection through Soft Clustering with Non-Negative  Kernel Regression</h3>
<p><a href='http://arxiv.org/abs/2407.13141v1'>http://arxiv.org/abs/2407.13141v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Image Inpainting Models are Effective Tools for Instruction-guided Image  Editing</h3>
<p><a href='http://arxiv.org/abs/2407.13139v1'>http://arxiv.org/abs/2407.13139v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>FocusDiffuser: Perceiving Local Disparities for Camouflaged Object  Detection</h3>
<p><a href='http://arxiv.org/abs/2407.13133v1'>http://arxiv.org/abs/2407.13133v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Reconfigurable Intelligent Surface Aided Vehicular Edge Computing: Joint  Phase-shift Optimization and Multi-User Power Allocation</h3>
<p><a href='http://arxiv.org/abs/2407.13123v1'>http://arxiv.org/abs/2407.13123v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>MO-EMT-NAS: Multi-Objective Continuous Transfer of Architectural  Knowledge Between Tasks from Different Datasets</h3>
<p><a href='http://arxiv.org/abs/2407.13122v1'>http://arxiv.org/abs/2407.13122v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>HPPP: Halpern-type Preconditioned Proximal Point Algorithms and  Applications to Image Restoration</h3>
<p><a href='http://arxiv.org/abs/2407.13120v1'>http://arxiv.org/abs/2407.13120v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>TrialEnroll: Predicting Clinical Trial Enrollment Success with Deep &  Cross Network and Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.13115v1'>http://arxiv.org/abs/2407.13115v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Multiobjective Vehicle Routing Optimization with Time Windows: A Hybrid  Approach Using Deep Reinforcement Learning and NSGA-II</h3>
<p><a href='http://arxiv.org/abs/2407.13113v1'>http://arxiv.org/abs/2407.13113v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>UCIP: A Universal Framework for Compressed Image Super-Resolution using  Dynamic Prompt</h3>
<p><a href='http://arxiv.org/abs/2407.13108v1'>http://arxiv.org/abs/2407.13108v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with  an Iterative Approach</h3>
<p><a href='http://arxiv.org/abs/2407.13101v1'>http://arxiv.org/abs/2407.13101v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>AlcLaM: Arabic Dialectal Language Model</h3>
<p><a href='http://arxiv.org/abs/2407.13097v1'>http://arxiv.org/abs/2407.13097v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Audio-visual Generalized Zero-shot Learning the Easy Way</h3>
<p><a href='http://arxiv.org/abs/2407.13095v1'>http://arxiv.org/abs/2407.13095v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Rethinking Video-Text Understanding: Retrieval from Counterfactually  Augmented Data</h3>
<p><a href='http://arxiv.org/abs/2407.13094v1'>http://arxiv.org/abs/2407.13094v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>On Causally Disentangled State Representation Learning for Reinforcement  Learning based Recommender Systems</h3>
<p><a href='http://arxiv.org/abs/2407.13091v1'>http://arxiv.org/abs/2407.13091v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for  Fact-Checking</h3>
<p><a href='http://arxiv.org/abs/2407.13089v1'>http://arxiv.org/abs/2407.13089v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Enhancing Temporal Action Localization: Advanced S6 Modeling with  Recurrent Mechanism</h3>
<p><a href='http://arxiv.org/abs/2407.13078v1'>http://arxiv.org/abs/2407.13078v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Dynamic Sentiment Analysis with Local Large Language Models using  Majority Voting: A Study on Factors Affecting Restaurant Evaluation</h3>
<p><a href='http://arxiv.org/abs/2407.13069v1'>http://arxiv.org/abs/2407.13069v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Krait: A Backdoor Attack Against Graph Prompt Tuning</h3>
<p><a href='http://arxiv.org/abs/2407.13068v1'>http://arxiv.org/abs/2407.13068v1</a></p>
<p><b>Compressor summary</b>: </p>