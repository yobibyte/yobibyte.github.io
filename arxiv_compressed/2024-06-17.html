
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-06-17</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-06-17 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Quantifying Variance in Evaluation Benchmarks</h3>
<p><a href='http://arxiv.org/abs/2406.10229v1'>http://arxiv.org/abs/2406.10229v1</a></p>
<p><b>Compressor summary</b>: The paper investigates the sources of variance in language model evaluation benchmarks and proposes methods to reduce it, aiming to improve the comparison of different models.</p><hr><h3>VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language  Large Models</h3>
<p><a href='http://arxiv.org/abs/2406.10228v1'>http://arxiv.org/abs/2406.10228v1</a></p>
<p><b>Compressor summary</b>: The text introduces Interleaved Image-Text Comprehension, a challenging task that requires models to understand and ignore irrelevant information in both images and texts, and presents VEGA, a new dataset tailored for this task.</p><hr><h3>VideoGUI: A Benchmark for GUI Automation from Instructional Videos</h3>
<p><a href='http://arxiv.org/abs/2406.10227v1'>http://arxiv.org/abs/2406.10227v1</a></p>
<p><b>Compressor summary</b>: VideoGUI is a benchmark for evaluating GUI assistants on complex visual tasks, such as video editing or using novel software, showing that even advanced models like GPT4o struggle with these tasks.</p><hr><h3>SatDiffMoE: A Mixture of Estimation Method for Satellite Image  Super-resolution with Latent Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2406.10225v1'>http://arxiv.org/abs/2406.10225v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Trade-off between spatial and temporal resolution in satellite images
- Diffusion models can generate realistic high-resolution images from low-resolution ones
- SatDiffMoE is a novel diffusion-based fusion algorithm that fuses multiple low-res images into one high-res image with fine details
- Superior performance and improved efficiency compared to previous methods

Summary:
SatDiffMoE is a new algorithm that uses diffusion models to fuse multiple low-resolution satellite images into one high-resolution image, achieving better results and efficiency than existing methods.</p><hr><h3>EFM3D: A Benchmark for Measuring Progress Towards 3D Egocentric  Foundation Models</h3>
<p><a href='http://arxiv.org/abs/2406.10224v1'>http://arxiv.org/abs/2406.10224v1</a></p>
<p><b>Compressor summary</b>: Egocentric Foundation Models (EFMs) are AI models that use wearable computers and 3D sensor data to improve spatial perception tasks, and Egocentric Voxel Lifting (EVL) is a baseline method for EFMs that performs well on the EFM3D benchmark.</p><hr><h3>Diffusion Synthesizer for Efficient Multilingual Speech to Speech  Translation</h3>
<p><a href='http://arxiv.org/abs/2406.10223v1'>http://arxiv.org/abs/2406.10223v1</a></p>
<p><b>Compressor summary</b>: DiffuseST is a fast and accurate speech translation system that uses a new diffusion-based synthesizer to preserve speaker's voice and improve audio quality.</p><hr><h3>Short Film Dataset (SFD): A Benchmark for Story-Level Video  Understanding</h3>
<p><a href='http://arxiv.org/abs/2406.10221v1'>http://arxiv.org/abs/2406.10221v1</a></p>
<p><b>Compressor summary</b>: The Short Film Dataset (SFD) provides a large collection of publicly available amateur movies with diverse genres for studying long-term story-oriented video tasks, while addressing the limitations of existing datasets and tasks in video understanding.</p><hr><h3>PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting</h3>
<p><a href='http://arxiv.org/abs/2406.10219v1'>http://arxiv.org/abs/2406.10219v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to compress 3D Gaussian Splatting models for novel view synthesis by pruning less sensitive Gaussians, improving rendering speed and image quality.</p><hr><h3>Semantic Membership Inference Attack against Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.10218v1'>http://arxiv.org/abs/2406.10218v1</a></p>
<p><b>Compressor summary</b>: SMIA is a new method that uses semantic information and neural networks to better identify if a data point was used to train a model, improving upon existing methods.</p><hr><h3>Regularizing Hidden States Enables Learning Generalizable Reward Model  for LLMs</h3>
<p><a href='http://arxiv.org/abs/2406.10216v1'>http://arxiv.org/abs/2406.10216v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new technique to improve reward models for Large Language Models by regularizing hidden states with text-generation losses, reducing over-optimization and increasing generalization ability.</p><hr><h3>DevBench: A multimodal developmental benchmark for language learning</h3>
<p><a href='http://arxiv.org/abs/2406.10215v1'>http://arxiv.org/abs/2406.10215v1</a></p>
<p><b>Compressor summary</b>: DevBench is a new benchmark that compares vision-language models' performance and response patterns to children and adults on seven language tasks, revealing differences between model and human development.</p><hr><h3>Universal randomised signatures for generative time series modelling</h3>
<p><a href='http://arxiv.org/abs/2406.10214v1'>http://arxiv.org/abs/2406.10214v1</a></p>
<p><b>Compressor summary</b>: The authors propose a generative model using randomised signature, a Wasserstein-type distance, and a reservoir neural stochastic differential equation for synthesizing financial time series data.</p><hr><h3>Selecting Interpretability Techniques for Healthcare Machine Learning  models</h3>
<p><a href='http://arxiv.org/abs/2406.10213v1'>http://arxiv.org/abs/2406.10213v1</a></p>
<p><b>Compressor summary</b>: The text discusses interpretable machine learning algorithms in healthcare and their categorization into post-hoc or model-based approaches.</p><hr><h3>NeST: Neural Stress Tensor Tomography by leveraging 3D Photoelasticity</h3>
<p><a href='http://arxiv.org/abs/2406.10212v1'>http://arxiv.org/abs/2406.10212v1</a></p>
<p><b>Compressor summary</b>: NeST is a new method to analyze 3D stress in transparent objects without slicing, using neural networks and polarization measurements.</p><hr><h3>DiffusionBlend: Learning 3D Image Prior through Position-aware Diffusion  Score Blending for 3D Computed Tomography Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2406.10211v1'>http://arxiv.org/abs/2406.10211v1</a></p>
<p><b>Compressor summary</b>: The authors propose a novel 3D diffusion prior method for large-scale medical image reconstruction that achieves state-of-the-art performance and is computationally efficient.</p><hr><h3>Make It Count: Text-to-Image Generation with an Accurate Number of  Objects</h3>
<p><a href='http://arxiv.org/abs/2406.10210v1'>http://arxiv.org/abs/2406.10210v1</a></p>
<p><b>Compressor summary</b>: The authors propose CountGen, a method to control the number of objects generated from text using a diffusion model, by identifying and separating object features and predicting missing objects' shape and location.</p><hr><h3>Be like a Goldfish, Don't Memorize! Mitigating Memorization in  Generative LLMs</h3>
<p><a href='http://arxiv.org/abs/2406.10209v1'>http://arxiv.org/abs/2406.10209v1</a></p>
<p><b>Compressor summary</b>: The goldfish loss is a technique that prevents large language models from memorizing and reproducing their training data, while maintaining performance on downstream tasks.</p><hr><h3>Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual  Visual Text Rendering</h3>
<p><a href='http://arxiv.org/abs/2406.10208v1'>http://arxiv.org/abs/2406.10208v1</a></p>
<p><b>Compressor summary</b>: The authors present Glyph-ByT5-v2 and Glyph-SDXL-v2, which improve multilingual visual text rendering in graphic design images by creating a large dataset, building a benchmark, and using step-aware preference learning.</p><hr><h3>A Fundamental Trade-off in Aligned Language Models and its Relation to  Sampling Adaptors</h3>
<p><a href='http://arxiv.org/abs/2406.10203v1'>http://arxiv.org/abs/2406.10203v1</a></p>
<p><b>Compressor summary</b>: This paper studies how human preferences in language models affect the probability--quality relationship and the trade-off between average reward and log-likelihood.</p><hr><h3>SSTFB: Leveraging self-supervised pretext learning and temporal  self-attention with feature branching for real-time video polyp segmentation</h3>
<p><a href='http://arxiv.org/abs/2406.10200v1'>http://arxiv.org/abs/2406.10200v1</a></p>
<p><b>Compressor summary</b>: The text describes a new video polyp segmentation method using self-supervised learning and spatial-temporal self-attention to improve performance on real-world colonoscopy videos.</p><hr><h3>Crafting Parts for Expressive Object Composition</h3>
<p><a href='http://arxiv.org/abs/2406.10197v1'>http://arxiv.org/abs/2406.10197v1</a></p>
<p><b>Compressor summary</b>: PartCraft is a method that allows artists to generate images with fine-grained control over object parts using a pre-trained diffusion model.</p><hr><h3>TRIP-PAL: Travel Planning with Guarantees by Combining Large Language  Models and Automated Planners</h3>
<p><a href='http://arxiv.org/abs/2406.10196v1'>http://arxiv.org/abs/2406.10196v1</a></p>
<p><b>Compressor summary</b>: TRIP-PAL is a hybrid method that combines large language models and automated planners to generate coherent, constraint-satisfying, and high-quality travel plans from user requests.</p><hr><h3>CHIRON: Rich Character Representations in Long-Form Narratives</h3>
<p><a href='http://arxiv.org/abs/2406.10190v1'>http://arxiv.org/abs/2406.10190v1</a></p>
<p><b>Compressor summary</b>: CHIRON is a new character representation for long-form narratives that uses question-answering and automated reasoning to create detailed and accurate character sheets.</p><hr><h3>Detecting and Evaluating Medical Hallucinations in Large Vision Language  Models</h3>
<p><a href='http://arxiv.org/abs/2406.10185v1'>http://arxiv.org/abs/2406.10185v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Med-HallMark, a benchmark for detecting and evaluating hallucinations in large vision language models used in healthcare applications, along with MediHall Score and MediHallDetector to assess and prevent hallucination impacts.</p><hr><h3>MeshPose: Unifying DensePose and 3D Body Mesh reconstruction</h3>
<p><a href='http://arxiv.org/abs/2406.10180v1'>http://arxiv.org/abs/2406.10180v1</a></p>
<p><b>Compressor summary</b>: MeshPose is a new method that combines DensePose and Human Mesh Reconstruction using weak supervision and end-to-end training to achieve high accuracy in 2D and 3D body mesh localization, suitable for real-time augmented reality applications.</p><hr><h3>Enhancing Incomplete Multi-modal Brain Tumor Segmentation with  Intra-modal Asymmetry and Inter-modal Dependency</h3>
<p><a href='http://arxiv.org/abs/2406.10175v1'>http://arxiv.org/abs/2406.10175v1</a></p>
<p><b>Compressor summary</b>: Our proposed method improves brain tumor segmentation from incomplete MRI modalities by pre-training on diverse synthetic data and post-processing predictions with missing modality reconstruction.</p><hr><h3>Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for  Beat-Aligned Poetry Generation</h3>
<p><a href='http://arxiv.org/abs/2406.10174v1'>http://arxiv.org/abs/2406.10174v1</a></p>
<p><b>Compressor summary</b>: The paper investigates using a byte-based language model to generate poetry that fits specific beat patterns, demonstrating promising results for computational creativity in this area.</p><hr><h3>IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension  Abilities of Language Models in E-commerce</h3>
<p><a href='http://arxiv.org/abs/2406.10173v1'>http://arxiv.org/abs/2406.10173v1</a></p>
<p><b>Compressor summary</b>: The paper introduces IntentionQA, a benchmark to evaluate language models' comprehension of purchase intentions in E-commerce scenarios, revealing their limitations and challenges.</p><hr><h3>Datasets for Multilingual Answer Sentence Selection</h3>
<p><a href='http://arxiv.org/abs/2406.10172v1'>http://arxiv.org/abs/2406.10172v1</a></p>
<p><b>Compressor summary</b>: The paper presents new high-quality AS2 datasets in five European languages created via supervised AMT of English datasets using an LLM, which help train more effective multilingual QA systems.</p><hr><h3>4DRecons: 4D Neural Implicit Deformable Objects Reconstruction from a  single RGB-D Camera with Geometrical and Topological Regularizations</h3>
<p><a href='http://arxiv.org/abs/2406.10167v1'>http://arxiv.org/abs/2406.10167v1</a></p>
<p><b>Compressor summary</b>: 4DRecons is a novel method to create textured 3D models from single camera RGB-D sequences by fitting a 4D neural implicit surface to the input data and using two regularization terms for rigid deformation and fixed topology.</p><hr><h3>Misam: Using ML in Dataflow Selection of Sparse-Sparse Matrix  Multiplication</h3>
<p><a href='http://arxiv.org/abs/2406.10166v1'>http://arxiv.org/abs/2406.10166v1</a></p>
<p><b>Compressor summary</b>: Key points:
- SpGEMM is a crucial operation for many fields, but sparse matrices have irregular structures
- Traditional hardware accelerators are not flexible enough to handle different sparsity patterns
- The paper proposes a machine learning based approach to adapt dataflow schemes for SpGEMM tasks with diverse sparsity patterns
- Machine learning can improve performance by up to 28 times compared to heuristic methods

Summary:
The paper introduces a machine learning method to dynamically select the best dataflow scheme for sparse matrix-matrix multiplication on hardware accelerators, achieving significant speedups.</p><hr><h3>CarLLaVA: Vision language models for camera-only closed-loop driving</h3>
<p><a href='http://arxiv.org/abs/2406.10165v1'>http://arxiv.org/abs/2406.10165v1</a></p>
<p><b>Compressor summary</b>: CarLLaVA is a Vision Language Model for autonomous driving that uses LLaMA architecture, achieves high performance with only camera input, and predicts language commentary while driving.</p><hr><h3>MeshAnything: Artist-Created Mesh Generation with Autoregressive  Transformers</h3>
<p><a href='http://arxiv.org/abs/2406.10163v1'>http://arxiv.org/abs/2406.10163v1</a></p>
<p><b>Compressor summary</b>: MeshAnything is a model that converts 3D assets into high-quality meshes for various 3D industry applications by using a VQ-VAE and a shape-conditioned decoder-only transformer.</p><hr><h3>Sycophancy to Subterfuge: Investigating Reward-Tampering in Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.10162v1'>http://arxiv.org/abs/2406.10162v1</a></p>
<p><b>Compressor summary</b>: Specification gaming in reinforcement learning occurs when AI systems learn undesired behaviors due to misspecified training goals; this paper investigates whether Large Language Model assistants can generalize from common forms of specification gaming to more pernicious reward tampering, with mixed results.</p><hr><h3>On the Computability of Robust PAC Learning</h3>
<p><a href='http://arxiv.org/abs/2406.10161v1'>http://arxiv.org/abs/2406.10161v1</a></p>
<p><b>Compressor summary</b>: The paper studies computability requirements for adversarially robust learning and introduces the concept of robust CPAC learnability with new sufficient conditions and insights.</p><hr><h3>BABILong: Testing the Limits of LLMs with Long Context  Reasoning-in-a-Haystack</h3>
<p><a href='http://arxiv.org/abs/2406.10149v1'>http://arxiv.org/abs/2406.10149v1</a></p>
<p><b>Compressor summary</b>: BABILong is a new benchmark for evaluating large language models' ability to reason across long contexts in various tasks, showing their limitations and potential improvements.</p><hr><h3>Improving rule mining via embedding-based link prediction</h3>
<p><a href='http://arxiv.org/abs/2406.10144v1'>http://arxiv.org/abs/2406.10144v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to combine rule mining and embedding-based methods for link prediction on knowledge graphs, using pre-trained embeddings to enrich the graph and discover new rules.</p><hr><h3>YOLOv1 to YOLOv10: A comprehensive review of YOLO variants and their  application in the agricultural domain</h3>
<p><a href='http://arxiv.org/abs/2406.10139v1'>http://arxiv.org/abs/2406.10139v1</a></p>
<p><b>Compressor summary</b>: This survey explores how YOLO variants can improve various aspects of agriculture using advanced object detection technology.</p><hr><h3>Evaluation of Large Language Models: STEM education and Gender  Stereotypes</h3>
<p><a href='http://arxiv.org/abs/2406.10133v1'>http://arxiv.org/abs/2406.10133v1</a></p>
<p><b>Compressor summary</b>: The paper investigates gender biases in large language models (LLMs) regarding educational choices across different cultures and languages, finding significant differences in STEM suggestions based on typical girl vs boy names.</p><hr><h3>Linear Contextual Bandits with Hybrid Payoff: Revisited</h3>
<p><a href='http://arxiv.org/abs/2406.10131v1'>http://arxiv.org/abs/2406.10131v1</a></p>
<p><b>Compressor summary</b>: The paper proposes HyLinUCB, a new algorithm for the Linear Contextual Bandit problem in the hybrid reward setting, which improves on existing regret guarantees and performs well in experiments.</p><hr><h3>The Devil is in the Neurons: Interpreting and Mitigating Social Biases  in Pre-trained Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.10130v1'>http://arxiv.org/abs/2406.10130v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a method to identify and remove social biases in pre-trained language models using a technique called Integrated Gap Gradients (IG$^2$), which pinpoints harmful neurons and suppresses them to improve fairness and performance.</p><hr><h3>SmartRSD: An Intelligent Multimodal Approach to Real-Time Road Surface  Detection for Safe Driving</h3>
<p><a href='http://arxiv.org/abs/2406.10128v1'>http://arxiv.org/abs/2406.10128v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method that uses both audio and images to automatically detect road surface conditions, improving vehicle safety under different environmental situations.</p><hr><h3>Exploration by Learning Diverse Skills through Successor State Measures</h3>
<p><a href='http://arxiv.org/abs/2406.10127v1'>http://arxiv.org/abs/2406.10127v1</a></p>
<p><b>Compressor summary</b>: The paper proposes LEADS, a method to teach agents diverse skills that cover the state space, using successor states and mutual information measures, improving exploration in maze navigation and robotic control tasks.</p><hr><h3>Training-free Camera Control for Video Generation</h3>
<p><a href='http://arxiv.org/abs/2406.10126v1'>http://arxiv.org/abs/2406.10126v1</a></p>
<p><b>Compressor summary</b>: Our method CamTrol allows camera control for video diffusion models without training or supervision, using a two-stage process of modeling image layout rearrangement and generating videos with layout prior of noisy latents.</p><hr><h3>MapVision: CVPR 2024 Autonomous Grand Challenge Mapless Driving Tech  Report</h3>
<p><a href='http://arxiv.org/abs/2406.10125v1'>http://arxiv.org/abs/2406.10125v1</a></p>
<p><b>Compressor summary</b>: The text describes a competition where autonomous driving algorithms using multi-perspective images and SD maps improve scene understanding for road and traffic elements detection.</p><hr><h3>SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for  Southeast Asian Languages</h3>
<p><a href='http://arxiv.org/abs/2406.10118v1'>http://arxiv.org/abs/2406.10118v1</a></p>
<p><b>Compressor summary</b>: SEACrowd is an initiative that provides standardized corpora in nearly 1,000 Southeast Asian languages and assesses AI models on 36 indigenous languages across 13 tasks to improve the quality and cultural representation of AI in the region.</p><hr><h3>Trustworthy Artificial Intelligence in the Context of Metrology</h3>
<p><a href='http://arxiv.org/abs/2406.10117v1'>http://arxiv.org/abs/2406.10117v1</a></p>
<p><b>Compressor summary</b>: The text reviews NPL's research on trustworthy artificial intelligence (TAI) in metrology, focusing on uncertainty quantification and three areas of TAI that they are working on.</p><hr><h3>Shelf-Supervised Multi-Modal Pre-Training for 3D Object Detection</h3>
<p><a href='http://arxiv.org/abs/2406.10115v1'>http://arxiv.org/abs/2406.10115v1</a></p>
<p><b>Compressor summary</b>: Key points:
- 3D object detection needs labeled data but it is costly and time-consuming to annotate
- Self-supervised pre-training with unlabeled data can improve accuracy with limited labels
- Image-based foundation models can help bootstrap point cloud representations from paired RGB and LiDAR data
- Shelf-supervised approach with image-based pseudo-labels improves semi-supervised detection accuracy over self-supervised methods

Summary:
The authors propose a shelf-supervised method that uses image-based foundation models to generate pseudo-labels for 3D object detection from RGB and LiDAR data, achieving better results than self-supervised pre-training in limited data settings.</p><hr><h3>Task-aligned Part-aware Panoptic Segmentation through Joint Object-Part  Representations</h3>
<p><a href='http://arxiv.org/abs/2406.10114v1'>http://arxiv.org/abs/2406.10114v1</a></p>
<p><b>Compressor summary</b>: TAPPS is a novel method for part-aware panoptic segmentation that jointly predicts object-level and part-level segments using shared queries, improving performance and aligning the learning objective with the task objective.</p><hr><h3>GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors</h3>
<p><a href='http://arxiv.org/abs/2406.10111v1'>http://arxiv.org/abs/2406.10111v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method called GaussianSR that uses 3D Gaussian Splatting and Score Distillation Sampling to generate high-resolution novel views from low-resolution inputs, with techniques to reduce randomness and improve quality.</p><hr><h3>Precipitation Nowcasting Using Physics Informed Discriminator Generative  Models</h3>
<p><a href='http://arxiv.org/abs/2406.10108v1'>http://arxiv.org/abs/2406.10108v1</a></p>
<p><b>Compressor summary</b>: The study proposes a physics-informed neural network to improve short-term weather forecasting, especially for extreme events, using data from the Netherlands Meteorological Institute.</p><hr><h3>Annotation Cost-Efficient Active Learning for Deep Metric Learning  Driven Remote Sensing Image Retrieval</h3>
<p><a href='http://arxiv.org/abs/2406.10107v1'>http://arxiv.org/abs/2406.10107v1</a></p>
<p><b>Compressor summary</b>: ANNEAL is a cost-efficient active learning method for deep metric learning in content-based image retrieval, using uncertainty and diversity criteria to select informative image pairs for annotation.</p><hr><h3>SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for  Remote Sensing Vision-Language Understanding</h3>
<p><a href='http://arxiv.org/abs/2406.10100v1'>http://arxiv.org/abs/2406.10100v1</a></p>
<p><b>Compressor summary</b>: The paper introduces FIT-RS, a large instruction tuning dataset for remote sensing imagery comprehension, and SkySenseGPT, a model that outperforms existing ones on complex tasks.</p><hr><h3>Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction  Tuning</h3>
<p><a href='http://arxiv.org/abs/2406.10099v1'>http://arxiv.org/abs/2406.10099v1</a></p>
<p><b>Compressor summary</b>: The paper proposes uncertainty-sensitive tuning, a two-stage training method for LLMs to recognize knowledge gaps and respond with "I do not know" when appropriate, improving their overall performance and outperforming GPT-4.</p><hr><h3>ECGMamba: Towards Efficient ECG Classification with BiSSM</h3>
<p><a href='http://arxiv.org/abs/2406.10098v1'>http://arxiv.org/abs/2406.10098v1</a></p>
<p><b>Compressor summary</b>: ECGMamba, a novel model using a bidirectional state-space model, enhances efficiency and effectiveness in ECG classification without sacrificing accuracy.</p><hr><h3>Exploring the Correlation between Human and Machine Evaluation of  Simultaneous Speech Translation</h3>
<p><a href='http://arxiv.org/abs/2406.10091v1'>http://arxiv.org/abs/2406.10091v1</a></p>
<p><b>Compressor summary</b>: The study finds that GPT models, especially GPT-3.5 with direct prompting, show the best correlation with human judgment in assessing translation accuracy of simultaneous interpretations.</p><hr><h3>Biomarker based Cancer Classification using an Ensemble with Pre-trained  Models</h3>
<p><a href='http://arxiv.org/abs/2406.10087v1'>http://arxiv.org/abs/2406.10087v1</a></p>
<p><b>Compressor summary</b>: The text describes the challenge of early pancreatic cancer detection and proposes a novel ensemble model that combines Hyperfast, XGBoost, and LightGBM machine learning algorithms to enhance liquid biopsy-based cancer identification using fewer features.</p><hr><h3>Discovering influential text using convolutional neural networks</h3>
<p><a href='http://arxiv.org/abs/2406.10086v1'>http://arxiv.org/abs/2406.10086v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method using convolutional neural networks to discover clusters of similar text phrases that affect human reactions, which can be applied in experimental settings to identify text treatments and their effects.</p><hr><h3>Enhancing Question Answering on Charts Through Effective Pre-training  Tasks</h3>
<p><a href='http://arxiv.org/abs/2406.10085v1'>http://arxiv.org/abs/2406.10085v1</a></p>
<p><b>Compressor summary</b>: This paper analyzes the limitations of current VisualQA models for charts and plots, and proposes pre-training tasks to improve their performance on structural-visual and numerical questions.</p><hr><h3>On the Evaluation of Speech Foundation Models for Spoken Language  Understanding</h3>
<p><a href='http://arxiv.org/abs/2406.10083v1'>http://arxiv.org/abs/2406.10083v1</a></p>
<p><b>Compressor summary</b>: SLUE is a benchmark for spoken language understanding tasks that compares different speech foundation models, finding self-supervised models often perform as well or better than supervised ones, especially on sequence generation tasks.</p><hr><h3>Localizing Events in Videos with Multimodal Queries</h3>
<p><a href='http://arxiv.org/abs/2406.10079v1'>http://arxiv.org/abs/2406.10079v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new benchmark, ICQ, for locating events in videos using multimodal queries with images and texts.</p><hr><h3>D-NPC: Dynamic Neural Point Clouds for Non-Rigid View Synthesis from  Monocular Video</h3>
<p><a href='http://arxiv.org/abs/2406.10078v1'>http://arxiv.org/abs/2406.10078v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method to synthesize novel views from monocular video using a dynamic neural point cloud that encodes scene geometry and appearance, and leverages data-driven priors like depth estimation and object segmentation.</p><hr><h3>DurLAR: A High-fidelity 128-channel LiDAR Dataset with Panoramic Ambient  and Reflectivity Imagery for Multi-modal Autonomous Driving Applications</h3>
<p><a href='http://arxiv.org/abs/2406.10068v1'>http://arxiv.org/abs/2406.10068v1</a></p>
<p><b>Compressor summary</b>: DurLAR is a high-quality 3D LiDAR dataset for autonomous driving with improved depth estimation using multi-modal images and joint supervised/self-supervised losses.</p><hr><h3>TACCO: Task-guided Co-clustering of Clinical Concepts and Patient Visits  for Disease Subtyping based on EHR Data</h3>
<p><a href='http://arxiv.org/abs/2406.10061v1'>http://arxiv.org/abs/2406.10061v1</a></p>
<p><b>Compressor summary</b>: TACCO is a novel framework that jointly discovers clusters of clinical concepts and patient visits in EHR data to improve risk prediction for complex diseases.</p><hr><h3>First Multi-Dimensional Evaluation of Flowchart Comprehension for  Multimodal Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.10057v1'>http://arxiv.org/abs/2406.10057v1</a></p>
<p><b>Compressor summary</b>: The paper introduces FlowCE, a comprehensive method to evaluate multimodal large language models on various tasks related to flowcharts, and shows that current models perform poorly.</p><hr><h3>Comparison of fine-tuning strategies for transfer learning in medical  image classification</h3>
<p><a href='http://arxiv.org/abs/2406.10050v1'>http://arxiv.org/abs/2406.10050v1</a></p>
<p><b>Compressor summary</b>: This study compares eight fine-tuning methods for adapting pre-trained models to various medical imaging domains, finding that some strategies work better than others depending on the architecture and modality.</p><hr><h3>Unobtrusive Monitoring of Physical Weakness: A Simulated Approach</h3>
<p><a href='http://arxiv.org/abs/2406.10045v1'>http://arxiv.org/abs/2406.10045v1</a></p>
<p><b>Compressor summary</b>: The proposed system uses a non-intrusive camera sensor to monitor daily activities for signs of weakness in older adults, employing a Bayesian Network to model the relationship between features, activities, and health conditions with high accuracy.</p><hr><h3>Bridging the Communication Gap: Artificial Agents Learning Sign Language  through Imitation</h3>
<p><a href='http://arxiv.org/abs/2406.10043v1'>http://arxiv.org/abs/2406.10043v1</a></p>
<p><b>Compressor summary</b>: The research teaches humanoid robots non-verbal communication skills, such as sign language, using a combination of computer vision, deep learning, and reinforcement learning.</p><hr><h3>FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in  Biomedical Domain</h3>
<p><a href='http://arxiv.org/abs/2406.10040v1'>http://arxiv.org/abs/2406.10040v1</a></p>
<p><b>Compressor summary</b>: The paper presents a system that uses chain of thought and self-consistency to improve biomedical natural language inference for clinical trials, achieving high scores in various metrics.</p><hr><h3>Intepretative Deep Learning using Domain Adaptation for Fluorescence  Spectroscopy</h3>
<p><a href='http://arxiv.org/abs/2406.10031v1'>http://arxiv.org/abs/2406.10031v1</a></p>
<p><b>Compressor summary</b>: This study develops a new approach using domain adaptation with pretrained vision models to analyze fluorescence data from complex samples like extra virgin olive oil, improving the quality of predictions and providing insights into the underlying processes.</p><hr><h3>Off-Policy Evaluation from Logged Human Feedback</h3>
<p><a href='http://arxiv.org/abs/2406.10030v1'>http://arxiv.org/abs/2406.10030v1</a></p>
<p><b>Compressor summary</b>: The text explores if we can evaluate new models using human feedback on another model's responses without collecting new data.</p><hr><h3>ProtoS-ViT: Visual foundation models for sparse self-explainable  classifications</h3>
<p><a href='http://arxiv.org/abs/2406.10025v1'>http://arxiv.org/abs/2406.10025v1</a></p>
<p><b>Compressor summary</b>: This paper shows how pre-trained ViTs can be used to build explainable biomedical image classifiers with better accuracy and interpretability than existing prototypical models.</p><hr><h3>Deep Bayesian Active Learning for Preference Modeling in Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2406.10023v1'>http://arxiv.org/abs/2406.10023v1</a></p>
<p><b>Compressor summary</b>: BAL-PM is a new method that reduces the cost of preference labeling for large language models by selectively acquiring informative data points using Bayesian Active Learning.</p><hr><h3>Group and Shuffle: Efficient Structured Orthogonal Parametrization</h3>
<p><a href='http://arxiv.org/abs/2406.10019v1'>http://arxiv.org/abs/2406.10019v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new class of structured matrices for efficient fine-tuning of pretrained neural networks, and evaluates it on various tasks like text-to-image and language modeling.</p><hr><h3>Tilt and Average : Geometric Adjustment of the Last Layer for  Recalibration</h3>
<p><a href='http://arxiv.org/abs/2406.10017v1'>http://arxiv.org/abs/2406.10017v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Tilt and Average(	extsc{Tna}), a method that adjusts the weights of the last layer of a classifier to improve calibration, which aligns confidence with accuracy in neural network predictions.</p><hr><h3>Gradient-based Learning in State-based Potential Games for Self-Learning  Production Systems</h3>
<p><a href='http://arxiv.org/abs/2406.10015v1'>http://arxiv.org/abs/2406.10015v1</a></p>
<p><b>Compressor summary</b>: The paper introduces new optimization methods for state-based potential games in self-learning distributed systems, which improve convergence speed and policy quality using gradient-based approaches tailored to different systems.</p><hr><h3>Beyond Slow Signs in High-fidelity Model Extraction</h3>
<p><a href='http://arxiv.org/abs/2406.10011v1'>http://arxiv.org/abs/2406.10011v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates and improves methods for extracting the parameters of deep neural networks from standard benchmarks, enabling faster and more efficient attacks on their confidentiality.</p><hr><h3>An elementary proof of a universal approximation theorem</h3>
<p><a href='http://arxiv.org/abs/2406.10002v1'>http://arxiv.org/abs/2406.10002v1</a></p>
<p><b>Compressor summary</b>: The paper proves a simplified version of a universal approximation theorem for neural networks with three hidden layers and special activation functions.</p><hr><h3>OrientDream: Streamlining Text-to-3D Generation with Explicit  Orientation Control</h3>
<p><a href='http://arxiv.org/abs/2406.10000v1'>http://arxiv.org/abs/2406.10000v1</a></p>
<p><b>Compressor summary</b>: OrientDream is a text-to-3D framework that uses camera orientation conditioning and external data to generate efficient and consistent 3D models from textual prompts.</p><hr><h3>Towards Scalable and Versatile Weight Space Learning</h3>
<p><a href='http://arxiv.org/abs/2406.09997v1'>http://arxiv.org/abs/2406.09997v1</a></p>
<p><b>Compressor summary</b>: SANE is a method to learn task-agnostic representations of neural networks that can handle larger models and various tasks, by embedding subsets of network weights as tokens into the learned space.</p><hr><h3>Precision Empowers, Excess Distracts: Visual Question Answering With  Dynamically Infused Knowledge In Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.09994v1'>http://arxiv.org/abs/2406.09994v1</a></p>
<p><b>Compressor summary</b>: The paper introduces an approach for Knowledge-Based Visual Question Answering (KBVQA) that enhances questions with external knowledge from knowledge graphs and improves reasoning capabilities over existing models.</p><hr><h3>Details Make a Difference: Object State-Sensitive Neurorobotic Task  Planning</h3>
<p><a href='http://arxiv.org/abs/2406.09988v1'>http://arxiv.org/abs/2406.09988v1</a></p>
<p><b>Compressor summary</b>: The paper introduces OSSA, a task-planning agent using pre-trained neural networks, and evaluates two methods for generating object state-sensitive plans in tabletop scenarios.</p><hr><h3>Self-Supervised and Few-Shot Learning for Robust Bioaerosol Monitoring</h3>
<p><a href='http://arxiv.org/abs/2406.09984v1'>http://arxiv.org/abs/2406.09984v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to classify bioaerosol particles using self-supervised learning and few-shot learning, which could optimize real-time monitoring and reduce adaptation efforts.</p><hr><h3>Challenges in explaining deep learning models for data with biological  variation</h3>
<p><a href='http://arxiv.org/abs/2406.09981v1'>http://arxiv.org/abs/2406.09981v1</a></p>
<p><b>Compressor summary</b>: The paper discusses challenges of applying machine learning models to biological data, particularly grain data, for disease detection, and evaluates various post-hoc explainability methods on this data.</p><hr><h3>HIRO: Hierarchical Information Retrieval Optimization</h3>
<p><a href='http://arxiv.org/abs/2406.09979v1'>http://arxiv.org/abs/2406.09979v1</a></p>
<p><b>Compressor summary</b>: HIRO is a novel querying approach for RAG applications that uses hierarchical structures to optimize information retrieval and improve LLM responses.</p><hr><h3>Disentangling Dialect from Social Bias via Multitask Learning to Improve  Fairness</h3>
<p><a href='http://arxiv.org/abs/2406.09977v1'>http://arxiv.org/abs/2406.09977v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how dialects affect NLP methods' ability to detect biased language and proposes a multitask learning approach to improve fairness and accuracy.</p><hr><h3>Robust Model-Based Reinforcement Learning with an Adversarial Auxiliary  Model</h3>
<p><a href='http://arxiv.org/abs/2406.09976v1'>http://arxiv.org/abs/2406.09976v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to improve policy robustness in reinforcement learning by learning a pessimistic transition model that estimates the worst-case MDP and incorporating it into a practical algorithm called Robust Model-Based Policy Optimization (RMBPO).</p><hr><h3>InstructRL4Pix: Training Diffusion for Image Editing by Reinforcement  Learning</h3>
<p><a href='http://arxiv.org/abs/2406.09973v1'>http://arxiv.org/abs/2406.09973v1</a></p>
<p><b>Compressor summary</b>: InstructRL4Pix is a new image editing method that uses reinforcement learning and attention maps to accurately edit images based on human language commands, overcoming limitations of traditional datasets.</p><hr><h3>A Better LLM Evaluator for Text Generation: The Impact of Prompt Output  Sequencing and Optimization</h3>
<p><a href='http://arxiv.org/abs/2406.09972v1'>http://arxiv.org/abs/2406.09972v1</a></p>
<p><b>Compressor summary</b>: The study explores how to create better prompts for assessing generated texts using large language models, finding that the order of instructions and reasons affects scoring accuracy and consistency.</p><hr><h3>Impact of Speech Mode in Automatic Pathological Speech Detection</h3>
<p><a href='http://arxiv.org/abs/2406.09968v1'>http://arxiv.org/abs/2406.09968v1</a></p>
<p><b>Compressor summary</b>: The paper explores how different methods for detecting pathological speech perform on controlled and spontaneous speech, finding that deep learning outperforms classical machine learning.</p><hr><h3>Bag of Lies: Robustness in Continuous Pre-training BERT</h3>
<p><a href='http://arxiv.org/abs/2406.09967v1'>http://arxiv.org/abs/2406.09967v1</a></p>
<p><b>Compressor summary</b>: The study explores how continuous pre-training can improve BERT's entity knowledge on COVID-19 and its robustness against misinformation, using a new dataset of true and fake texts from academic publications.</p><hr><h3>H-Fac: Memory-Efficient Optimization with Factorized Hamiltonian Descent</h3>
<p><a href='http://arxiv.org/abs/2406.09958v1'>http://arxiv.org/abs/2406.09958v1</a></p>
<p><b>Compressor summary</b>: H-Fac is a new adaptive optimizer that uses factorized momentum and scaling parameters, performs well on ResNets and Vision Transformers, has low memory costs, and is based on Hamiltonian dynamics principles.</p><hr><h3>Rule Based Learning with Dynamic (Graph) Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2406.09954v1'>http://arxiv.org/abs/2406.09954v1</a></p>
<p><b>Compressor summary</b>: The text proposes a two-step approach for integrating expert knowledge into classical neural network architectures using rule based layers, which generalize conventional feed-forward layers and improve the performance of graph neural networks.</p><hr><h3>BiVLC: Extending Vision-Language Compositionality Evaluation with  Text-to-Image Retrieval</h3>
<p><a href='http://arxiv.org/abs/2406.09952v1'>http://arxiv.org/abs/2406.09952v1</a></p>
<p><b>Compressor summary</b>: The BiVLC dataset introduces synthetic hard negative images for vision-language compositionality benchmarks, revealing weaknesses in current models and improving multimodal learning with contrastive models.</p><hr><h3>Neural Concept Binder</h3>
<p><a href='http://arxiv.org/abs/2406.09949v1'>http://arxiv.org/abs/2406.09949v1</a></p>
<p><b>Compressor summary</b>: The Neural Concept Binder is a framework that generates discrete concept representations for object-based visual reasoning using soft and hard binding methods, enabling human input and integration with other AI models.</p><hr><h3>BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures  and Languages</h3>
<p><a href='http://arxiv.org/abs/2406.09948v1'>http://arxiv.org/abs/2406.09948v1</a></p>
<p><b>Compressor summary</b>: BLEnD is a new benchmark to evaluate large language models' cultural knowledge across diverse regions and low-resource languages.</p><hr><h3>Finite-Time Analysis of Simultaneous Double Q-learning</h3>
<p><a href='http://arxiv.org/abs/2406.09946v1'>http://arxiv.org/abs/2406.09946v1</a></p>
<p><b>Compressor summary</b>: Simultaneous double Q-learning is a modified version of double Q-learning that eliminates random selection and allows faster convergence and better bias reduction in reinforcement learning.</p><hr><h3>SemanticSpray++: A Multimodal Dataset for Autonomous Driving in Wet  Surface Conditions</h3>
<p><a href='http://arxiv.org/abs/2406.09945v1'>http://arxiv.org/abs/2406.09945v1</a></p>
<p><b>Compressor summary</b>: The SemanticSpray++ dataset provides labeled multimodal data for camera, LiDAR, and radar sensors in wet conditions to evaluate autonomous vehicle perception methods.</p><hr><h3>Experiments in News Bias Detection with Pre-Trained Neural Transformers</h3>
<p><a href='http://arxiv.org/abs/2406.09938v1'>http://arxiv.org/abs/2406.09938v1</a></p>
<p><b>Compressor summary</b>: The study compares language models' ability to detect and classify biased or fake information in news articles.</p><hr><h3>ALGM: Adaptive Local-then-Global Token Merging for Efficient Semantic  Segmentation with Plain Vision Transformers</h3>
<p><a href='http://arxiv.org/abs/2406.09936v1'>http://arxiv.org/abs/2406.09936v1</a></p>
<p><b>Compressor summary</b>: ALGM is a token reduction method for semantic segmentation with Vision Transformers that improves throughput and segmentation quality by merging similar tokens in two stages.</p><hr><h3>Forgetting Order of Continual Learning: Examples That are Learned First  are Forgotten Last</h3>
<p><a href='http://arxiv.org/abs/2406.09935v1'>http://arxiv.org/abs/2406.09935v1</a></p>
<p><b>Compressor summary</b>: Goldilocks is a replay buffer sampling method that reduces catastrophic forgetting in continual learning by focusing on examples learned at an intermediate speed.</p><hr><h3>POWN: Prototypical Open-World Node Classification</h3>
<p><a href='http://arxiv.org/abs/2406.09926v1'>http://arxiv.org/abs/2406.09926v1</a></p>
<p><b>Compressor summary</b>: The paper proposes POWN, a novel method for open-world semi-supervised node classification that learns prototype representations of new classes and outperforms baselines by up to 30%.</p><hr><h3>CliBench: Multifaceted Evaluation of Large Language Models in Clinical  Decisions on Diagnoses, Procedures, Lab Tests Orders and Prescriptions</h3>
<p><a href='http://arxiv.org/abs/2406.09923v1'>http://arxiv.org/abs/2406.09923v1</a></p>
<p><b>Compressor summary</b>: CliBench is a new benchmark that evaluates large language models' ability to perform various realistic clinical tasks using data from the MIMIC IV dataset.</p><hr><h3>Knowledge Editing in Language Models via Adapted Direct Preference  Optimization</h3>
<p><a href='http://arxiv.org/abs/2406.09920v1'>http://arxiv.org/abs/2406.09920v1</a></p>
<p><b>Compressor summary</b>: KDPO is a method for updating large language models' knowledge using online alignment and weight updates without retraining, improving Knowledge Editing performance.</p><hr><h3>Robust compressive tracking via online weighted multiple instance  learning</h3>
<p><a href='http://arxiv.org/abs/2406.09914v1'>http://arxiv.org/abs/2406.09914v1</a></p>
<p><b>Compressor summary</b>: The proposed visual object tracking algorithm combines sparse representation, coarse-to-fine search, weighted multiple instance learning, and selective sample usage to tackle various challenges and achieve a stable and robust tracker.</p><hr><h3>OpenECAD: An Efficient Visual Language Model for Computer-Aided Design</h3>
<p><a href='http://arxiv.org/abs/2406.09913v1'>http://arxiv.org/abs/2406.09913v1</a></p>
<p><b>Compressor summary</b>: The text describes OpenECAD, a system that uses fine-tuned visual language models to generate 2D sketches and 3D construction commands from images of 3D designs, enabling integration into manufacturing processes.</p><hr><h3>What Does Softmax Probability Tell Us about Classifiers Ranking Across  Diverse Test Conditions?</h3>
<p><a href='http://arxiv.org/abs/2406.09908v1'>http://arxiv.org/abs/2406.09908v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Softmax Correlation, a new metric to rank classifiers' performance on unlabeled data from out-of-distribution distributions by measuring how similar their predictions are to ideal class correlations.</p><hr><h3>Label-Efficient Semantic Segmentation of LiDAR Point Clouds in Adverse  Weather Conditions</h3>
<p><a href='http://arxiv.org/abs/2406.09906v1'>http://arxiv.org/abs/2406.09906v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a label-efficient approach to segment LiDAR point clouds in adverse weather using few-shot semantic segmentation and semi-supervised learning with good weather data integration.</p><hr><h3>Nymeria: A Massive Collection of Multimodal Egocentric Daily Motion in  the Wild</h3>
<p><a href='http://arxiv.org/abs/2406.09905v1'>http://arxiv.org/abs/2406.09905v1</a></p>
<p><b>Compressor summary</b>: Nymeria is a large, diverse, in-the-wild human motion dataset with rich annotations, including 3D motion ground truth, multimodal recordings from multiple devices, and hierarchical language descriptions of activities.</p><hr><h3>QQQ: Quality Quattuor-Bit Quantization for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.09904v1'>http://arxiv.org/abs/2406.09904v1</a></p>
<p><b>Compressor summary</b>: QQQ is a new quantization method that improves the speed and performance of large language models without extensive training by using adaptive smoothing and Hessian-based compensation, as well as engineered W4A8 GEMM kernels.</p><hr><h3>GEB-1.3B: Open Lightweight Large Language Model</h3>
<p><a href='http://arxiv.org/abs/2406.09900v1'>http://arxiv.org/abs/2406.09900v1</a></p>
<p><b>Compressor summary</b>: GEB-1.3B is a lightweight large language model that performs well on various tasks and runs efficiently on CPUs.</p><hr><h3>Learning Solution-Aware Transformers for Efficiently Solving Quadratic  Assignment Problem</h3>
<p><a href='http://arxiv.org/abs/2406.09899v1'>http://arxiv.org/abs/2406.09899v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a learning-based approach for efficiently solving quadratic assignment problems (QAPs), which are hard combinatorial optimization problems, by encoding facility and location nodes separately and using a solution transformer architecture to capture higher-order information.</p><hr><h3>Positive-Unlabelled Learning for Identifying New Candidate Dietary  Restriction-related Genes among Ageing-related Genes</h3>
<p><a href='http://arxiv.org/abs/2406.09898v1'>http://arxiv.org/abs/2406.09898v1</a></p>
<p><b>Compressor summary</b>: The authors propose a novel gene prioritization method using Positive-Unlabelled Learning to improve the identification of dietary restriction-related genes and outperform existing methods.</p><hr><h3>Exploring the Benefits of Vision Foundation Models for Unsupervised  Domain Adaptation</h3>
<p><a href='http://arxiv.org/abs/2406.09896v1'>http://arxiv.org/abs/2406.09896v1</a></p>
<p><b>Compressor summary</b>: The study shows that combining Vision Foundation Models with Unsupervised Domain Adaptation improves generalization, speed, and performance in semantic segmentation tasks across diverse data domains.</p><hr><h3>3D-RPE: Enhancing Long-Context Modeling Through 3D Rotary Position  Encoding</h3>
<p><a href='http://arxiv.org/abs/2406.09897v1'>http://arxiv.org/abs/2406.09897v1</a></p>
<p><b>Compressor summary</b>: The proposed 3D Rotary Position Encoding improves on the 2D version by providing controllable long-term decay and better position resolution for modeling long contexts in natural language understanding and language modeling tasks.</p><hr><h3>Benchmarking Generative Models on Computational Thinking Tests in  Elementary Visual Programming</h3>
<p><a href='http://arxiv.org/abs/2406.09891v1'>http://arxiv.org/abs/2406.09891v1</a></p>
<p><b>Compressor summary</b>: The paper explores how state-of-the-art generative models struggle with elementary-level problem-solving tasks and proposes a novel benchmark using synthetic data to improve their performance.</p><hr><h3>A Unified Data Augmentation Framework for Low-Resource Multi-Domain  Dialogue Generation</h3>
<p><a href='http://arxiv.org/abs/2406.09881v1'>http://arxiv.org/abs/2406.09881v1</a></p>
<p><b>Compressor summary</b>: The paper proposes AMD$^2$G, a framework that augments data and trains models in two stages to enable dialogue generation in multiple domains with insufficient or no domain-specific training data.</p><hr><h3>Sailing in high-dimensional spaces: Low-dimensional embeddings through  angle preservation</h3>
<p><a href='http://arxiv.org/abs/2406.09876v1'>http://arxiv.org/abs/2406.09876v1</a></p>
<p><b>Compressor summary</b>: Mercat is a new low-dimensional embedding method that reconstructs angles between data points, preserving local and global structures in high-dimensional data better than existing approaches.</p><hr><h3>IGL-Bench: Establishing the Comprehensive Benchmark for Imbalanced Graph  Learning</h3>
<p><a href='http://arxiv.org/abs/2406.09870v1'>http://arxiv.org/abs/2406.09870v1</a></p>
<p><b>Compressor summary</b>: IGL-Bench is a benchmark for imbalanced graph learning that evaluates 24 algorithms on node-level and graph-level tasks under class-imbalance and topology-imbalance, providing insights and opportunities to improve performance.</p><hr><h3>Rethinking the Evaluation of Out-of-Distribution Detection: A Sorites  Paradox</h3>
<p><a href='http://arxiv.org/abs/2406.09867v1'>http://arxiv.org/abs/2406.09867v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new benchmark, IS-OOD, that divides test samples into subsets with different semantic and covariate shifts to address the issue of marginal OOD samples having close semantic contents to ID samples.</p><hr><h3>LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal  Data</h3>
<p><a href='http://arxiv.org/abs/2406.09864v1'>http://arxiv.org/abs/2406.09864v1</a></p>
<p><b>Compressor summary</b>: LUMA is a new dataset for learning from uncertain and multimodal data, featuring audio, image, and textual data from 50 classes, with tools to control uncertainty and evaluate robustness in multimodal deep learning models.</p><hr><h3>Dataset Condensation with Latent Quantile Matching</h3>
<p><a href='http://arxiv.org/abs/2406.09860v1'>http://arxiv.org/abs/2406.09860v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method, Latent Quantile Matching (LQM), to improve distribution matching-based dataset condensation by better aligning latent embeddings and addressing outliers.</p><hr><h3>Vision Language Modeling of Content, Distortion and Appearance for Image  Quality Assessment</h3>
<p><a href='http://arxiv.org/abs/2406.09858v1'>http://arxiv.org/abs/2406.09858v1</a></p>
<p><b>Compressor summary</b>: SLIQUE is a new blind image quality assessment (IQA) model that uses joint vision-language learning to analyze semantic content, distortion characteristics, and appearance properties of images, outperforming existing methods.</p><hr><h3>On the Encoding of Gender in Transformer-based ASR Representations</h3>
<p><a href='http://arxiv.org/abs/2406.09855v1'>http://arxiv.org/abs/2406.09855v1</a></p>
<p><b>Compressor summary</b>: This study analyzes how gender is represented and used in two ASR models and shows that it's possible to remove gender information with minimal performance impact, suggesting the potential of creating gender-neutral embeddings.</p><hr><h3>GradeADreamer: Enhanced Text-to-3D Generation Using Gaussian Splatting  and Multi-View Diffusion</h3>
<p><a href='http://arxiv.org/abs/2406.09850v1'>http://arxiv.org/abs/2406.09850v1</a></p>
<p><b>Compressor summary</b>: The paper introduces GradeADreamer, a three-stage training pipeline that produces high-quality 3D assets with minimal issues and fast generation time using a Multi-view Diffusion Model and StableDiffusion.</p><hr><h3>Learning Multi-view Molecular Representations with Structured and  Unstructured Knowledge</h3>
<p><a href='http://arxiv.org/abs/2406.09841v1'>http://arxiv.org/abs/2406.09841v1</a></p>
<p><b>Compressor summary</b>: MV-Mol is a model that learns molecular representations from different sources, improving property prediction and multi-modal comprehension in chemistry and life science.</p><hr><h3>Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for  Improving User Experience at First Meeting</h3>
<p><a href='http://arxiv.org/abs/2406.09839v1'>http://arxiv.org/abs/2406.09839v1</a></p>
<p><b>Compressor summary</b>: The study uses a large language model to create virtual agents that can build rapport with humans through small talk, and finds that free-form dialogue strategies improve subjective measures of rapport.</p><hr><h3>Vision-Language Models Meet Meteorology: Developing Models for Extreme  Weather Events Detection with Heatmaps</h3>
<p><a href='http://arxiv.org/abs/2406.09838v1'>http://arxiv.org/abs/2406.09838v1</a></p>
<p><b>Compressor summary</b>: The paper introduces ClimateIQA, a meteorological VQA dataset, SPOT, a technique to capture color contours in heatmaps, and Climate-Zoo, a collection of meteorological VLMs that significantly improve EWED accuracy.</p><hr><h3>TabularFM: An Open Framework For Tabular Foundational Models</h3>
<p><a href='http://arxiv.org/abs/2406.09837v1'>http://arxiv.org/abs/2406.09837v1</a></p>
<p><b>Compressor summary</b>: TabularFM is an open-source framework that develops foundational models for tabular data using various neural architectures, curated datasets, and pretrained models.</p><hr><h3>Robustness-Inspired Defense Against Backdoor Attacks on Graph Neural  Networks</h3>
<p><a href='http://arxiv.org/abs/2406.09836v1'>http://arxiv.org/abs/2406.09836v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to detect and counteract graph backdoor attacks using random edge dropping and robust training for GNNs.</p><hr><h3>I Know How: Combining Prior Policies to Solve New Tasks</h3>
<p><a href='http://arxiv.org/abs/2406.09835v1'>http://arxiv.org/abs/2406.09835v1</a></p>
<p><b>Compressor summary</b>: The I Know How (IKH) framework helps agents learn and adapt efficiently to dynamic environments by using modular and compositional knowledge.</p><hr><h3>SHMamba: Structured Hyperbolic State Space Model for Audio-Visual  Question Answering</h3>
<p><a href='http://arxiv.org/abs/2406.09833v1'>http://arxiv.org/abs/2406.09833v1</a></p>
<p><b>Compressor summary</b>: SHMamba is a new model that uses hyperbolic geometry and state space models to better represent hierarchical structures and relationships in audio-visual data, resulting in improved performance and reduced computational costs compared to previous methods.</p><hr><h3>Open-Vocabulary Semantic Segmentation with Image Embedding Balancing</h3>
<p><a href='http://arxiv.org/abs/2406.09829v1'>http://arxiv.org/abs/2406.09829v1</a></p>
<p><b>Compressor summary</b>: EBSeg is a novel framework for open-vocabulary semantic segmentation that uses an Adaptively Balanced Decoder and Semantic Structure Consistency loss to improve generalization ability and overcome overfitting issues, achieving state-of-the-art results.</p><hr><h3>HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical  Attention Pruning</h3>
<p><a href='http://arxiv.org/abs/2406.09827v1'>http://arxiv.org/abs/2406.09827v1</a></p>
<p><b>Compressor summary</b>: HiP is a novel approach for large language models that reduces time and space complexity of attention mechanisms, enabling efficient handling of long context sequences without retraining pre-trained models.</p><hr><h3>Unraveling Anomalies in Time: Unsupervised Discovery and Isolation of  Anomalous Behavior in Bio-regenerative Life Support System Telemetry</h3>
<p><a href='http://arxiv.org/abs/2406.09825v1'>http://arxiv.org/abs/2406.09825v1</a></p>
<p><b>Compressor summary</b>: The study analyzes anomalies in a space greenhouse using time series clustering to better understand their causes and improve condition monitoring.</p><hr><h3>From Manifestations to Cognitive Architectures: a Scalable Framework</h3>
<p><a href='http://arxiv.org/abs/2406.09823v1'>http://arxiv.org/abs/2406.09823v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to interpret reality as an information source and build cognitive architectures using spatial distributed representations in a scalable way.</p><hr><h3>Retrieval Augmented Fact Verification by Synthesizing Contrastive  Arguments</h3>
<p><a href='http://arxiv.org/abs/2406.09815v1'>http://arxiv.org/abs/2406.09815v1</a></p>
<p><b>Compressor summary</b>: RAFTS is a method that uses evidence retrieval and contrasting arguments to verify claim credibility and improve fact verification performance.</p><hr><h3>RaNeuS: Ray-adaptive Neural Surface Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2406.09801v1'>http://arxiv.org/abs/2406.09801v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method (RaNeuS) to improve 3D surface reconstruction using a differentiable radiance field by adaptively adjusting regularization and projection, achieving better results than existing methods.</p><hr><h3>DeltaPhi: Learning Physical Trajectory Residual for PDE Solving</h3>
<p><a href='http://arxiv.org/abs/2406.09795v1'>http://arxiv.org/abs/2406.09795v1</a></p>
<p><b>Compressor summary</b>: The paper proposes DeltaPhi, a method that improves the learning of physical dynamics in neural operator networks by predicting and reducing residuals between a solved trajectory and an auxiliary one.</p><hr><h3>SuperSVG: Superpixel-based Scalable Vector Graphics Synthesis</h3>
<p><a href='http://arxiv.org/abs/2406.09794v1'>http://arxiv.org/abs/2406.09794v1</a></p>
<p><b>Compressor summary</b>: SuperSVG is a fast and accurate image vectorization model that uses superpixels and a two-stage self-training framework with dynamic path warping loss to convert raster images to SVGs.</p><hr><h3>A Two-Stage Masked Autoencoder Based Network for Indoor Depth Completion</h3>
<p><a href='http://arxiv.org/abs/2406.09792v1'>http://arxiv.org/abs/2406.09792v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Depth images have many applications but are challenging for complex indoor scenes
- A Transformer-based network with self-supervision pre-training and token fusion is proposed
- The method outperforms existing methods on the Matterport3D dataset and can be used for 3D reconstruction

Summary:
The paper presents a novel Transformer-based network that learns to complete depth images from RGB images for complex indoor scenes, achieving state-of-the-art results and enabling 3D reconstruction.</p><hr><h3>Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic  Textual Similarity</h3>
<p><a href='http://arxiv.org/abs/2406.09790v1'>http://arxiv.org/abs/2406.09790v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Pcc-tuning, a method that uses Pearson's correlation coefficient as a loss function to improve semantic textual similarity beyond contrastive learning.</p><hr><h3>OpenCapBench: A Benchmark to Bridge Pose Estimation and Biomechanics</h3>
<p><a href='http://arxiv.org/abs/2406.09788v1'>http://arxiv.org/abs/2406.09788v1</a></p>
<p><b>Compressor summary</b>: OpenCapBench is a unified benchmark for human pose estimation that considers physiological constraints and improves keypoint density for accurate biomechanics analysis using synthetic data finetuning.</p><hr><h3>Unsupervised Monocular Depth Estimation Based on Hierarchical  Feature-Guided Diffusion</h3>
<p><a href='http://arxiv.org/abs/2406.09782v1'>http://arxiv.org/abs/2406.09782v1</a></p>
<p><b>Compressor summary</b>: The authors propose a robust unsupervised monocular depth estimation model using a diffusion model, a hierarchical feature-guided denoising module, and an implicit depth consistency loss.</p><hr><h3>GPT-4o: Visual perception performance of multimodal large language  models in piglet activity understanding</h3>
<p><a href='http://arxiv.org/abs/2406.09781v1'>http://arxiv.org/abs/2406.09781v1</a></p>
<p><b>Compressor summary</b>: This study evaluates how well multimodal LLMs can recognize piglet activities from video clips and suggests that they have potential for animal behavior understanding in livestock scenarios, especially GPT-4o.</p><hr><h3>OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst</h3>
<p><a href='http://arxiv.org/abs/2406.09779v1'>http://arxiv.org/abs/2406.09779v1</a></p>
<p><b>Compressor summary</b>: The study presents a novel method to detect harmful memes in multiple languages using image captioning, OCR, and LLM analysis, achieving top-1 performance at the Online Safety Prize Challenge.</p><hr><h3>A lightweight residual network for unsupervised deformable image  registration</h3>
<p><a href='http://arxiv.org/abs/2406.09774v1'>http://arxiv.org/abs/2406.09774v1</a></p>
<p><b>Compressor summary</b>: The paper presents a CNN-based image registration method with an enhanced receptive field, low number of parameters, and good performance on limited training data, outperforming or being comparable to transformer-based methods.</p><hr><h3>Research on Edge Detection of LiDAR Images Based on Artificial  Intelligence Technology</h3>
<p><a href='http://arxiv.org/abs/2406.09773v1'>http://arxiv.org/abs/2406.09773v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a deep learning-based edge detection method for LiDAR images that improves accuracy and efficiency compared to traditional methods and has practical application value.</p><hr><h3>Towards Efficient Pareto Set Approximation via Mixture of Experts Based  Model Fusion</h3>
<p><a href='http://arxiv.org/abs/2406.09770v1'>http://arxiv.org/abs/2406.09770v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to efficiently approximate the Pareto front of large neural networks using mixture of experts (MoE) for multi-objective optimization tasks.</p><hr><h3>Bayesian Conditioned Diffusion Models for Inverse Problems</h3>
<p><a href='http://arxiv.org/abs/2406.09768v1'>http://arxiv.org/abs/2406.09768v1</a></p>
<p><b>Compressor summary</b>: The paper introduces BCDM, a novel Bayesian method to condition diffusion models for optimal image reconstruction tasks, achieving state-of-the-art results in various problems.</p><hr><h3>Full-reference Point Cloud Quality Assessment Using Spectral Graph  Wavelets</h3>
<p><a href='http://arxiv.org/abs/2406.09762v1'>http://arxiv.org/abs/2406.09762v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new method for assessing the quality of 3D point clouds using spectral graph wavelets, which improves accuracy and correlates better with human perception than existing methods.</p><hr><h3>Bootstrapping Language Models with DPO Implicit Rewards</h3>
<p><a href='http://arxiv.org/abs/2406.09760v1'>http://arxiv.org/abs/2406.09760v1</a></p>
<p><b>Compressor summary</b>: The DICE approach improves large language model alignment using direct preference optimization and a bootstrapped implicit reward model.</p><hr><h3>Grounding Image Matching in 3D with MASt3R</h3>
<p><a href='http://arxiv.org/abs/2406.09756v1'>http://arxiv.org/abs/2406.09756v1</a></p>
<p><b>Compressor summary</b>: MASt3R is a 3D image matching method that combines DUSt3R's robustness with dense features, reciprocal matching, and theoretical guarantees to significantly outperform existing methods.</p><hr><h3>Mix Q-learning for Lane Changing: A Collaborative Decision-Making Method  in Multi-Agent Deep Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2406.09755v1'>http://arxiv.org/abs/2406.09755v1</a></p>
<p><b>Compressor summary</b>: Mix Q-learning for Lane Changing (MQLC) is a method that uses deep reinforcement learning to improve autonomous vehicle path planning by integrating individual and collective benefits for better traffic efficiency and safety.</p><hr><h3>LAVIB: A Large-scale Video Interpolation Benchmark</h3>
<p><a href='http://arxiv.org/abs/2406.09754v1'>http://arxiv.org/abs/2406.09754v1</a></p>
<p><b>Compressor summary</b>: LAVIB is a large dataset for video frame interpolation tasks that includes various metrics and challenges based on video characteristics like motion, luminance, sharpness, and contrast.</p><hr><h3>ControlVAR: Exploring Controllable Visual Autoregressive Modeling</h3>
<p><a href='http://arxiv.org/abs/2406.09750v1'>http://arxiv.org/abs/2406.09750v1</a></p>
<p><b>Compressor summary</b>: ControlVAR is a new framework that uses visual autoregressive modeling to allow flexible and efficient control over images in various conditional generation tasks.</p><hr><h3>How Does Distribution Matching Help Domain Generalization: An  Information-theoretic Analysis</h3>
<p><a href='http://arxiv.org/abs/2406.09745v1'>http://arxiv.org/abs/2406.09745v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a probabilistic framework for domain generalization that combines gradient and representation alignment, and introduces new methods for complex distribution matching to improve robustness and generalization.</p><hr><h3>Deep Symbolic Optimization for Combinatorial Optimization: Accelerating  Node Selection by Discovering Potential Heuristics</h3>
<p><a href='http://arxiv.org/abs/2406.09740v1'>http://arxiv.org/abs/2406.09740v1</a></p>
<p><b>Compressor summary</b>: The text proposes a novel framework that combines data-driven and symbolic methods for node selection in combinatorial optimization solvers, improving performance and interpretability on CPU machines.</p><hr><h3>Decoupling Forgery Semantics for Generalizable Deepfake Detection</h3>
<p><a href='http://arxiv.org/abs/2406.09739v1'>http://arxiv.org/abs/2406.09739v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new method for detecting DeepFakes by decoupling unique and common forgery semantics, improving the generalization of detection.</p><hr><h3>Automated GIS-Based Framework for Detecting Crosswalk Changes from  Bi-Temporal High-Resolution Aerial Images</h3>
<p><a href='http://arxiv.org/abs/2406.09731v1'>http://arxiv.org/abs/2406.09731v1</a></p>
<p><b>Compressor summary</b>: This study develops an automated framework to detect changes in crosswalks using high-resolution images, finding over 3,000 crosswalk changes in three Florida counties that can inform traffic and safety studies.</p><hr><h3>Neural Pose Representation Learning for Generating and Transferring  Non-Rigid Object Poses</h3>
<p><a href='http://arxiv.org/abs/2406.09728v1'>http://arxiv.org/abs/2406.09728v1</a></p>
<p><b>Compressor summary</b>: Key points:
- A new method for learning 3D deformable object poses
- Disentangles pose from identity, facilitates pose variation and transfer
- Does not require explicit shape parameterization or supervision
- Uses keypoint-based hybrid representation and implicit deformation field
- Achieves state-of-the-art performance on DeformThings4D and Human datasets

Summary:
The authors present a novel method for learning pose representations of 3D deformable objects that can disentangle, vary, and transfer poses without explicit shape parameterization or supervision, using keypoint-based and implicit deformation techniques. They show superior results on two benchmarks.</p><hr><h3>PixRO: Pixel-Distributed Rotational Odometry with Gaussian Belief  Propagation</h3>
<p><a href='http://arxiv.org/abs/2406.09726v1'>http://arxiv.org/abs/2406.09726v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new pixel-level method for estimating rotational motion in visual sensors, reducing data transmission and processing costs.</p><hr><h3>When Will Gradient Regularization Be Harmful?</h3>
<p><a href='http://arxiv.org/abs/2406.09723v1'>http://arxiv.org/abs/2406.09723v1</a></p>
<p><b>Compressor summary</b>: This paper proposes three GR warmup strategies to improve performance and stability in adaptive optimization scenarios, especially for scalable models.</p><hr><h3>Cross-view geo-localization: a survey</h3>
<p><a href='http://arxiv.org/abs/2406.09722v1'>http://arxiv.org/abs/2406.09722v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Cross-view geo-localization is a challenging but important computer vision task.
- The paper reviews feature-based and deep learning methods, as well as the challenges and solutions involved.
- It also discusses benchmark datasets, evaluation metrics, and future research directions.

Summary:
The paper surveys cross-view geo-localization techniques, focusing on feature-based and deep learning approaches, and highlights the challenges, datasets, metrics, and applications of this task.</p><hr><h3>Self-Knowledge Distillation for Learning Ambiguity</h3>
<p><a href='http://arxiv.org/abs/2406.09719v1'>http://arxiv.org/abs/2406.09719v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-knowledge distillation method that improves natural language understanding by learning label distributions from lower layers and re-calibrating confidence for ambiguous samples.</p><hr><h3>UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for  Low-Resource Languages</h3>
<p><a href='http://arxiv.org/abs/2406.09717v1'>http://arxiv.org/abs/2406.09717v1</a></p>
<p><b>Compressor summary</b>: UniBridge improves Cross-Lingual Transfer Learning by optimizing embeddings initialization and vocabulary size for languages with limited resources.</p><hr><h3>Meta-Learning Loss Functions for Deep Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2406.09713v1'>http://arxiv.org/abs/2406.09713v1</a></p>
<p><b>Compressor summary</b>: Meta-learning explores using past experiences from similar tasks to improve performance, focusing on the often-overlooked loss function component in this thesis.</p><hr><h3>AnimalFormer: Multimodal Vision Framework for Behavior-based Precision  Livestock Farming</h3>
<p><a href='http://arxiv.org/abs/2406.09711v1'>http://arxiv.org/abs/2406.09711v1</a></p>
<p><b>Compressor summary</b>: The framework uses AI models to analyze livestock behavior from video data without invasive tagging, providing insights for activity detection, counting, health assessments, and posture analyses.</p><hr><h3>Fine-Grained Urban Flow Inference with Multi-scale Representation  Learning</h3>
<p><a href='http://arxiv.org/abs/2406.09710v1'>http://arxiv.org/abs/2406.09710v1</a></p>
<p><b>Compressor summary</b>: UrbanMSR is a model that uses self-supervised learning to infer fine-grained urban traffic flows from coarse-grained data, capturing multi-scale and dynamic information for better efficiency and safety.</p><hr><h3>Detecting Response Generation Not Requiring Factual Judgment</h3>
<p><a href='http://arxiv.org/abs/2406.09702v1'>http://arxiv.org/abs/2406.09702v1</a></p>
<p><b>Compressor summary</b>: The study aimed to create a dialogue dataset and develop a model that can predict sentences needing fact-checking in conversations, achieving both attractiveness and factuality.</p><hr><h3>Compressed Video Quality Enhancement with Temporal Group Alignment and  Fusion</h3>
<p><a href='http://arxiv.org/abs/2406.09693v1'>http://arxiv.org/abs/2406.09693v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method to improve compressed videos using temporal group alignment and fusion of features from neighboring frames, achieving better quality and lower complexity than current methods.</p><hr><h3>FreeCtrl: Constructing Control Centers with Feedforward Layers for  Learning-Free Controllable Text Generation</h3>
<p><a href='http://arxiv.org/abs/2406.09688v1'>http://arxiv.org/abs/2406.09688v1</a></p>
<p><b>Compressor summary</b>: FreeCtrl is a learning-free method for controlling text generation that adjusts neural network weights to produce desired attributes in output.</p><hr><h3>Explainable AI for Comparative Analysis of Intrusion Detection Models</h3>
<p><a href='http://arxiv.org/abs/2406.09684v1'>http://arxiv.org/abs/2406.09684v1</a></p>
<p><b>Compressor summary</b>: This paper evaluates various machine learning models for intrusion detection from network traffic using occlusion sensitivity and finds that Random Forest performs best in accuracy, efficiency, and robustness.</p><hr><h3>Asymmetrical Siamese Network for Point Clouds Normal Estimation</h3>
<p><a href='http://arxiv.org/abs/2406.09681v1'>http://arxiv.org/abs/2406.09681v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an Asymmetric Siamese Network to improve point cloud normal estimation by exploring intrinsic feature consistency across different noise levels, and introduces a new multi-view dataset with diverse shapes and noise levels to evaluate methods and reduce overfitting.</p><hr><h3>Exploring Training on Heterogeneous Data with Mixture of Low-rank  Adapters</h3>
<p><a href='http://arxiv.org/abs/2406.09679v1'>http://arxiv.org/abs/2406.09679v1</a></p>
<p><b>Compressor summary</b>: The study explores using Mixture of Low-rank Adapters (MoLA) to efficiently mitigate training conflicts among heterogeneous data in artificial general intelligence models, and introduces two variants for target-aware and target-agnostic scenarios.</p><hr><h3>Benchmarking Spectral Graph Neural Networks: A Comprehensive Study on  Effectiveness and Efficiency</h3>
<p><a href='http://arxiv.org/abs/2406.09675v1'>http://arxiv.org/abs/2406.09675v1</a></p>
<p><b>Compressor summary</b>: This paper benchmarks over 30 spectral graph neural networks (GNNs), analyzes their frequency characteristics, and provides a unified framework for efficient evaluation and selection of these models for large-scale tasks.</p><hr><h3>Evaluating ChatGPT-4 Vision on Brazil's National Undergraduate Computer  Science Exam</h3>
<p><a href='http://arxiv.org/abs/2406.09671v1'>http://arxiv.org/abs/2406.09671v1</a></p>
<p><b>Compressor summary</b>: The study shows that ChatGPT-4 Vision, a visual model, performed better than average students in a computer science exam, but faced challenges with question interpretation and logical reasoning, highlighting the importance of human oversight in assessments.</p><hr><h3>Learning Language Structures through Grounding</h3>
<p><a href='http://arxiv.org/abs/2406.09662v1'>http://arxiv.org/abs/2406.09662v1</a></p>
<p><b>Compressor summary</b>: The text discusses learning language structures through grounding, using various data sources and modalities, and improving parsing, program synthesis, and cross-lingual tasks.</p><hr><h3>ScaLES: Scalable Latent Exploration Score for Pre-Trained Generative  Networks</h3>
<p><a href='http://arxiv.org/abs/2406.09657v1'>http://arxiv.org/abs/2406.09657v1</a></p>
<p><b>Compressor summary</b>: ScaLES is a method that reduces over-exploration in Latent Space Optimization, improving the quality of solutions for black-box discrete optimization problems.</p><hr><h3>RSEND: Retinex-based Squeeze and Excitation Network with Dark Region  Detection for Efficient Low Light Image Enhancement</h3>
<p><a href='http://arxiv.org/abs/2406.09656v1'>http://arxiv.org/abs/2406.09656v1</a></p>
<p><b>Compressor summary</b>: RSEND is a one-stage Retinex theory based framework that enhances low-light images by capturing details with Squeeze and Excitation network, achieving significant improvement over other CNN-based models.</p><hr><h3>An Intrinsic Vector Heat Network</h3>
<p><a href='http://arxiv.org/abs/2406.09648v1'>http://arxiv.org/abs/2406.09648v1</a></p>
<p><b>Compressor summary</b>: The paper presents a novel neural network architecture for learning tangent vector fields on surfaces in 3D that preserves intrinsic properties and is robust to various deformations.</p><hr><h3>OpenAnimalTracks: A Dataset for Animal Track Recognition</h3>
<p><a href='http://arxiv.org/abs/2406.09647v1'>http://arxiv.org/abs/2406.09647v1</a></p>
<p><b>Compressor summary</b>: The paper introduces OpenAnimalTracks, a labeled dataset for automated animal footprint classification and detection, which can help with biodiversity preservation.</p><hr><h3>A Survey of Video Datasets for Grounded Event Understanding</h3>
<p><a href='http://arxiv.org/abs/2406.09646v1'>http://arxiv.org/abs/2406.09646v1</a></p>
<p><b>Compressor summary</b>: The paper surveys 105 video datasets that require event understanding capability and discusses how they can help study robust video event extraction tasks, considering the temporal nature and ambiguity of visual content.</p><hr><h3>Reinforced Decoder: Towards Training Recurrent Neural Networks for Time  Series Forecasting</h3>
<p><a href='http://arxiv.org/abs/2406.09643v1'>http://arxiv.org/abs/2406.09643v1</a></p>
<p><b>Compressor summary</b>: The study introduces a reinforced decoder method that uses auxiliary inputs and reinforcement learning to improve multi-step-ahead time series forecasting accuracy.</p><hr><h3>TGB 2.0: A Benchmark for Learning on Temporal Knowledge Graphs and  Heterogeneous Graphs</h3>
<p><a href='http://arxiv.org/abs/2406.09639v1'>http://arxiv.org/abs/2406.09639v1</a></p>
<p><b>Compressor summary</b>: TGB 2.0 is a benchmarking framework for evaluating predictions on large-scale temporal graphs with new datasets and realistic evaluation pipeline.</p><hr><h3>RASPNet: A Benchmark Dataset for Radar Adaptive Signal Processing  Applications</h3>
<p><a href='http://arxiv.org/abs/2406.09638v1'>http://arxiv.org/abs/2406.09638v1</a></p>
<p><b>Compressor summary</b>: The paper introduces RASPNet, a large-scale dataset for radar adaptive signal processing, that covers diverse real-world environments and contains 10,000 clutter realizations per scenario.</p><hr><h3>Industrial Language-Image Dataset (ILID): Adapting Vision Foundation  Models for Industrial Settings</h3>
<p><a href='http://arxiv.org/abs/2406.09637v1'>http://arxiv.org/abs/2406.09637v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Large Language Models (LLM) influence computer vision with multimodal datasets and self-/semi-supervised learning
- Vision Foundation Models (VFM) generalize well on everyday objects or scenes, but not in specialized domains
- The paper introduces a pipeline to generate the Industrial Language-Image Dataset (ILID) from web-crawled data
- The paper shows effective self-supervised transfer learning and downstream tasks after training on ILID without human labeling

Summary:
The paper presents a web-crawling pipeline to create an industrial dataset for self-supervised vision models, which improves their performance in specialized domains without human labels.</p>