
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-09-16</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-09-16 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>INN-PAR: Invertible Neural Network for PPG to ABP Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2409.09021v1'>http://arxiv.org/abs/2409.09021v1</a></p>
<p><b>Compressor summary</b>: The text introduces an invertible neural network for non-invasive blood pressure monitoring using photoplethysmography, which improves accuracy by capturing high-frequency details and learning features across multiple scales.</p><hr><h3>An Efficient and Streaming Audio Visual Active Speaker Detection System</h3>
<p><a href='http://arxiv.org/abs/2409.09018v1'>http://arxiv.org/abs/2409.09018v1</a></p>
<p><b>Compressor summary</b>: The paper proposes two methods to reduce latency and memory usage in real-time Active Speaker Detection systems, limiting future and past context frames, and shows they perform well compared to existing models.</p><hr><h3>AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM  Agents</h3>
<p><a href='http://arxiv.org/abs/2409.09013v1'>http://arxiv.org/abs/2409.09013v1</a></p>
<p><b>Compressor summary</b>: AI-LieDar is a framework to study how large language models navigate situations where being truthful conflicts with achieving goals, showing that current models are often untruthful and hard to steer towards truthfulness.</p><hr><h3>Optimizing Rare Word Accuracy in Direct Speech Translation with a  Retrieval-and-Demonstration Approach</h3>
<p><a href='http://arxiv.org/abs/2409.09009v1'>http://arxiv.org/abs/2409.09009v1</a></p>
<p><b>Compressor summary</b>: The proposed method uses retrieved examples to improve rare word translation accuracy in direct ST models, with speech-to-speech retrieval being the most effective and robust approach.</p><hr><h3>SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear  Complexity</h3>
<p><a href='http://arxiv.org/abs/2409.09007v1'>http://arxiv.org/abs/2409.09007v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates the need for multi-layer attention in graph Transformers, proposes a simplified single-layer version (SGFormer) that scales well and requires less data, and shows its effectiveness on large graphs.</p><hr><h3>E2MoCase: A Dataset for Emotional, Event and Moral Observations in News  Articles on High-impact Legal Cases</h3>
<p><a href='http://arxiv.org/abs/2409.09001v1'>http://arxiv.org/abs/2409.09001v1</a></p>
<p><b>Compressor summary</b>: E2MoCase is a dataset that helps study how emotions, morals, and events in legal stories affect media coverage and public opinion.</p><hr><h3>PINNfluence: Influence Functions for Physics-Informed Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2409.08958v1'>http://arxiv.org/abs/2409.08958v1</a></p>
<p><b>Compressor summary</b>: The text discusses using influence functions to improve interpretability and validate physics-informed neural networks in fluid flow problems.</p><hr><h3>Pushing the boundaries of event subsampling in event-based video  classification using CNNs</h3>
<p><a href='http://arxiv.org/abs/2409.08953v1'>http://arxiv.org/abs/2409.08953v1</a></p>
<p><b>Compressor summary</b>: Event cameras can classify images with low accuracy even after heavily subsampling events, but training CNNs becomes more sensitive to hyperparameters in highly subsampled scenarios.</p><hr><h3>A Diffusion Approach to Radiance Field Relighting using  Multi-Illumination Synthesis</h3>
<p><a href='http://arxiv.org/abs/2409.08947v1'>http://arxiv.org/abs/2409.08947v1</a></p>
<p><b>Compressor summary</b>: The method creates relightable radiance fields from single-illumination data by using 2D diffusion model priors to augment the data and optimize appearance features for multi-view consistency.</p><hr><h3>DELTA: Dual Consistency Delving with Topological Uncertainty for Active  Graph Domain Adaptation</h3>
<p><a href='http://arxiv.org/abs/2409.08946v1'>http://arxiv.org/abs/2409.08946v1</a></p>
<p><b>Compressor summary</b>: DELTA is a novel approach for active graph domain adaptation that selects informative nodes and uses two subnetworks to explore topological semantics, improving performance on target graphs.</p><hr><h3>SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical  Records</h3>
<p><a href='http://arxiv.org/abs/2409.08936v1'>http://arxiv.org/abs/2409.08936v1</a></p>
<p><b>Compressor summary</b>: The SynSUM benchmark is a synthetic dataset for research on clinical information extraction and reasoning with tabular background variables and text.</p><hr><h3>Optimization and Generalization Guarantees for Weight Normalization</h3>
<p><a href='http://arxiv.org/abs/2409.08935v1'>http://arxiv.org/abs/2409.08935v1</a></p>
<p><b>Compressor summary</b>: This paper provides the first theory for optimizing and generalizing deep neural networks with weight normalization, showing how it affects convergence and uniformity in training and testing.</p><hr><h3>Latent Space Score-based Diffusion Model for Probabilistic Multivariate  Time Series Imputation</h3>
<p><a href='http://arxiv.org/abs/2409.08917v1'>http://arxiv.org/abs/2409.08917v1</a></p>
<p><b>Compressor summary</b>: The Latent Space Score-Based Diffusion Model (LSSDM) is an unsupervised learning approach for probabilistic multivariate time series imputation that projects observed values onto a low-dimensional latent space, reconstructs coarse missing data, and uses a conditional diffusion model to obtain precise imputed values with uncertainty analysis.</p><hr><h3>Affective Computing Has Changed: The Foundation Model Disruption</h3>
<p><a href='http://arxiv.org/abs/2409.08907v1'>http://arxiv.org/abs/2409.08907v1</a></p>
<p><b>Compressor summary</b>: This paper explores the potential and challenges of using foundation models for affective computing, which involves generating and analysing multimodal data related to human emotions.</p><hr><h3>Exploring Action-Centric Representations Through the Lens of  Rate-Distortion Theory</h3>
<p><a href='http://arxiv.org/abs/2409.08892v1'>http://arxiv.org/abs/2409.08892v1</a></p>
<p><b>Compressor summary</b>: The text discusses how efficient coding and rate-distortion theory can be used to understand action-oriented efficient representations in organisms' perception.</p><hr><h3>Visual Language Tracking with Multi-modal Interaction: A Robust  Benchmark</h3>
<p><a href='http://arxiv.org/abs/2409.08887v1'>http://arxiv.org/abs/2409.08887v1</a></p>
<p><b>Compressor summary</b>: The text introduces VLT-MI, a novel benchmark for visual language tracking with multi-modal interaction, which improves cognitive alignment and robustness of trackers by enabling multiple rounds of text and object updates during tracking.</p><hr><h3>Interactive Masked Image Modeling for Multimodal Object Detection in  Remote Sensing</h3>
<p><a href='http://arxiv.org/abs/2409.08885v1'>http://arxiv.org/abs/2409.08885v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Object detection in remote sensing imagery is challenging due to small and barely visible objects across diverse terrains
- Multimodal learning can integrate features from different data modalities to improve detection accuracy
- Masked Image Modeling (MIM) can be used as a pre-training technique for object detection using self-supervised learning on unlabeled data
- Conventional MIM such as MAE lacks contextual information and fine-grained details
- The paper proposes a new interactive MIM method that can establish interactions between different tokens, which is beneficial for object detection in remote sensing

Summary:
The paper introduces an interactive Masked Image Modeling method to improve object detection in remote sensing imagery by leveraging self-supervised learning on unlabeled data and establishing interactions between different tokens.</p><hr><h3>Detect Fake with Fake: Leveraging Synthetic Data-driven Representation  for Synthetic Image Detection</h3>
<p><a href='http://arxiv.org/abs/2409.08884v1'>http://arxiv.org/abs/2409.08884v1</a></p>
<p><b>Compressor summary</b>: Synthetic data helps detect fake images by training vision transformers with synthetic representation learners like SynCLR, outperforming CLIP on unseen GAN models.</p><hr><h3>Exploring the Impact of Data Quantity on ASR in Extremely Low-resource  Languages</h3>
<p><a href='http://arxiv.org/abs/2409.08872v1'>http://arxiv.org/abs/2409.08872v1</a></p>
<p><b>Compressor summary</b>: The study shows that using a data selection scheme to augment limited target language data improves automatic speech recognition for two endangered languages, Amis and Seediq.</p><hr><h3>Exploring Graph Structure Comprehension Ability of Multimodal Large  Language Models: Case Studies</h3>
<p><a href='http://arxiv.org/abs/2409.08864v1'>http://arxiv.org/abs/2409.08864v1</a></p>
<p><b>Compressor summary</b>: The study explores how using images alongside text improves large language models' ability to understand graphs.</p><hr><h3>Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with  Memoryless Stochastic Optimal Control</h3>
<p><a href='http://arxiv.org/abs/2409.08861v1'>http://arxiv.org/abs/2409.08861v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a new method for reward fine-tuning of dynamical generative models using stochastic optimal control, which improves their quality and generalization.</p><hr><h3>InstantDrag: Improving Interactivity in Drag-based Image Editing</h3>
<p><a href='http://arxiv.org/abs/2409.08857v1'>http://arxiv.org/abs/2409.08857v1</a></p>
<p><b>Compressor summary</b>: InstantDrag is an optimization-free method that enables fast, photo-realistic drag-based image editing without masks or text prompts using two networks that learn motion dynamics from real-world video datasets.</p><hr><h3>Using The Concept Hierarchy for Household Action Recognition</h3>
<p><a href='http://arxiv.org/abs/2409.08853v1'>http://arxiv.org/abs/2409.08853v1</a></p>
<p><b>Compressor summary</b>: </p><hr><h3>Kinect Calibration and Data Optimization For Anthropometric Parameters</h3>
<p><a href='http://arxiv.org/abs/2409.08847v1'>http://arxiv.org/abs/2409.08847v1</a></p>
<p><b>Compressor summary</b>: The text discusses a new method for calibrating and optimizing Microsoft Kinect sensors, which are widely used in 3D vision systems for applications such as medical and biometric fields.</p><hr><h3>AIPO: Improving Training Objective for Iterative Preference Optimization</h3>
<p><a href='http://arxiv.org/abs/2409.08845v1'>http://arxiv.org/abs/2409.08845v1</a></p>
<p><b>Compressor summary</b>: Agreement-aware Iterative Preference Optimization (AIPO) addresses the length exploitation issue in iterative preference optimization with synthetic data, achieving state-of-the-art results on various language model benchmarks.</p><hr><h3>Direct-CP: Directed Collaborative Perception for Connected and  Autonomous Vehicles via Proactive Attention</h3>
<p><a href='http://arxiv.org/abs/2409.08840v1'>http://arxiv.org/abs/2409.08840v1</a></p>
<p><b>Compressor summary</b>: Direct-CP is a system that uses RSUs to help autonomous vehicles signal their interests and focus on important areas, improving their local perception accuracy in collaborative 3D object detection tasks.</p><hr><h3>Can Kans (re)discover predictive models for Direct-Drive Laser Fusion?</h3>
<p><a href='http://arxiv.org/abs/2409.08832v1'>http://arxiv.org/abs/2409.08832v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Kolmogorov-Arnold Networks as a new method for machine learning in laser fusion, which improves prediction accuracy and interpretability compared to other approaches.</p><hr><h3>AutoIRT: Calibrating Item Response Theory Models with Automated Machine  Learning</h3>
<p><a href='http://arxiv.org/abs/2409.08823v1'>http://arxiv.org/abs/2409.08823v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a multistage fitting procedure that improves scoring accuracy and calibration for computerized adaptive tests using out-of-the-box AutoML tools.</p><hr><h3>A RAG Approach for Generating Competency Questions in Ontology  Engineering</h3>
<p><a href='http://arxiv.org/abs/2409.08820v1'>http://arxiv.org/abs/2409.08820v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to automatically generate competency questions (CQs) for ontology development using large language models (LLMs) and scientific papers as input, and evaluates its performance on two domain engineering tasks.</p><hr><h3>Your Weak LLM is Secretly a Strong Teacher for Alignment</h3>
<p><a href='http://arxiv.org/abs/2409.08813v1'>http://arxiv.org/abs/2409.08813v1</a></p>
<p><b>Compressor summary</b>: This paper shows how using a less powerful language model can generate effective feedback for aligning AI systems with human values and intentions, making alignment more scalable and sustainable.</p><hr><h3>TabKANet: Tabular Data Modelling with Kolmogorov-Arnold Network and  Transformer</h3>
<p><a href='http://arxiv.org/abs/2409.08806v1'>http://arxiv.org/abs/2409.08806v1</a></p>
<p><b>Compressor summary</b>: The study introduces TabKANet, a Transformer-based model that uses Kolmogorov-Arnold network to encode and merge numerical and categorical features for tabular data, achieving excellent results in six binary classification tasks.</p><hr><h3>Exploring SSL Discrete Tokens for Multilingual ASR</h3>
<p><a href='http://arxiv.org/abs/2409.08805v1'>http://arxiv.org/abs/2409.08805v1</a></p>
<p><b>Compressor summary</b>: This paper compares discrete tokens from self-supervised learning models for speech recognition in multiple languages and scenarios, showing improved performance and efficiency over Fbank features.</p><hr><h3>Task-Specific Data Preparation for Deep Learning to Reconstruct  Structures of Interest from Severely Truncated CBCT Data</h3>
<p><a href='http://arxiv.org/abs/2409.08800v1'>http://arxiv.org/abs/2409.08800v1</a></p>
<p><b>Compressor summary</b>: The text introduces a method to extend the field-of-view of CBCT systems using deep learning and improve their clinical applications, especially for reconstructing rib structures.</p><hr><h3>Exploring SSL Discrete Speech Features for Zipformer-based Contextual  ASR</h3>
<p><a href='http://arxiv.org/abs/2409.08797v1'>http://arxiv.org/abs/2409.08797v1</a></p>
<p><b>Compressor summary</b>: The paper shows that using discrete speech features from self-supervised learning in ASR systems improves performance, especially for cross-utterance contexts.</p><hr><h3>Optimizing Ingredient Substitution Using Large Language Models to  Enhance Phytochemical Content in Recipes</h3>
<p><a href='http://arxiv.org/abs/2409.08792v1'>http://arxiv.org/abs/2409.08792v1</a></p>
<p><b>Compressor summary</b>: The study shows how large language models can help create recipes with more phytochemicals, potentially improving health, but cautions that these benefits need clinical validation.</p><hr><h3>Electrocardiogram Report Generation and Question Answering via  Retrieval-Augmented Self-Supervised Modeling</h3>
<p><a href='http://arxiv.org/abs/2409.08788v1'>http://arxiv.org/abs/2409.08788v1</a></p>
<p><b>Compressor summary</b>: ECG-ReGen is a retrieval-based method that uses self-supervised learning and large language models to generate comprehensive reports and answer questions from electrocardiograms, potentially improving patient care.</p><hr><h3>Contactless Fingerprint Recognition Using 3D Graph Matching</h3>
<p><a href='http://arxiv.org/abs/2409.08782v1'>http://arxiv.org/abs/2409.08782v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel contactless fingerprint recognition algorithm that captures the 3D feature of contactless fingerprints and improves matching accuracy across multiple poses.</p><hr><h3>Sign Language Sense Disambiguation</h3>
<p><a href='http://arxiv.org/abs/2409.08780v1'>http://arxiv.org/abs/2409.08780v1</a></p>
<p><b>Compressor summary</b>: The text describes a project that explores how to improve sign language translation of German sign language, especially for ambiguous words, using different bodypart representations in transformer models and evaluating their impact on performance.</p><hr><h3>In-depth Analysis of Low-rank Matrix Factorisation in a Federated  Setting</h3>
<p><a href='http://arxiv.org/abs/2409.08771v1'>http://arxiv.org/abs/2409.08771v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a distributed algorithm for low-rank matrix factorization, using power initialization to improve convergence rates and reduce communication overhead.</p><hr><h3>Increasing Both Batch Size and Learning Rate Accelerates Stochastic  Gradient Descent</h3>
<p><a href='http://arxiv.org/abs/2409.08770v1'>http://arxiv.org/abs/2409.08770v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes four mini-batch SGD schedulers and shows that increasing batch size and learning rate can improve performance and minimize the full gradient norm of the empirical loss faster.</p><hr><h3>SAUC: Sparsity-Aware Uncertainty Calibration for Spatiotemporal  Prediction with Graph Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2409.08766v1'>http://arxiv.org/abs/2409.08766v1</a></p>
<p><b>Compressor summary</b>: The paper proposes SAUC, a novel framework that calibrates uncertainty in both zero and non-zero values for spatiotemporal prediction using probabilistic Graph Neural Networks and quantile approaches.</p><hr><h3>Online Network Inference from Graph-Stationary Signals with Hidden Nodes</h3>
<p><a href='http://arxiv.org/abs/2409.08760v1'>http://arxiv.org/abs/2409.08760v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for estimating unknown graph connectivity from incomplete and streaming data using stationary signals and a convex optimization problem.</p><hr><h3>Uncertainty Estimation by Density Aware Evidential Deep Learning</h3>
<p><a href='http://arxiv.org/abs/2409.08754v1'>http://arxiv.org/abs/2409.08754v1</a></p>
<p><b>Compressor summary</b>: DAEDL is a novel method for improving uncertainty estimation in deep learning by integrating feature space density and using a new parameterization, achieving state-of-the-art results on various tasks.</p><hr><h3>A Hybrid Meta-Learning and Multi-Armed Bandit Approach for  Context-Specific Multi-Objective Recommendation Optimization</h3>
<p><a href='http://arxiv.org/abs/2409.08752v1'>http://arxiv.org/abs/2409.08752v1</a></p>
<p><b>Compressor summary</b>: Juggler-MAB is a hybrid recommender system that combines meta-learning and Multi-Armed Bandits to balance multiple objectives for various stakeholders in online marketplaces.</p><hr><h3>Uncertainty and Generalizability in Foundation Models for Earth  Observation</h3>
<p><a href='http://arxiv.org/abs/2409.08744v1'>http://arxiv.org/abs/2409.08744v1</a></p>
<p><b>Compressor summary</b>: The authors study how different Foundation Models and labeling strategies affect the performance and uncertainty of estimating vegetation coverage in various areas using Sentinel satellite images.</p><hr><h3>Adaptive Sampling for Continuous Group Equivariant Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2409.08741v1'>http://arxiv.org/abs/2409.08741v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an adaptive sampling method for steerable networks that adjusts to data symmetries, improving performance, equivariance, and computational efficiency.</p><hr><h3>Multi-intent Aware Contrastive Learning for Sequential Recommendation</h3>
<p><a href='http://arxiv.org/abs/2409.08733v1'>http://arxiv.org/abs/2409.08733v1</a></p>
<p><b>Compressor summary</b>: Sequence recommendation models should consider multiple user intents instead of just one to better capture real-world scenarios.</p><hr><h3>Bridging Dynamic Factor Models and Neural Controlled Differential  Equations for Nowcasting GDP</h3>
<p><a href='http://arxiv.org/abs/2409.08732v1'>http://arxiv.org/abs/2409.08732v1</a></p>
<p><b>Compressor summary</b>: The authors propose NCDENow, a GDP nowcasting framework that combines neural controlled differential equations with dynamic factor models to handle irregular dynamics and improve prediction accuracy.</p><hr><h3>Quasimetric Value Functions with Dense Rewards</h3>
<p><a href='http://arxiv.org/abs/2409.08724v1'>http://arxiv.org/abs/2409.08724v1</a></p>
<p><b>Compressor summary</b>: The paper explores how goal-conditioned reinforcement learning can be improved with dense rewards by using a quasimetric structure in the optimal value function, leading to more efficient neural architectures and better sample complexity in robotics tasks.</p><hr><h3>Distilling Monolingual and Crosslingual Word-in-Context Representations</h3>
<p><a href='http://arxiv.org/abs/2409.08719v1'>http://arxiv.org/abs/2409.08719v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Propose a method to distil word meaning in context from masked language model
- No human-annotated corpora or parameter updates needed
- Use self-attention and auto-encoder to combine hidden layers outputs
- Perform well on monolingual and crosslingual tasks

Summary:
The study presents a method that uses self-attention and auto-encoder to extract word meaning in context from masked language models without human annotations or updates, achieving competitive results on various semantic tasks.</p><hr><h3>Layerwise Change of Knowledge in Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2409.08712v1'>http://arxiv.org/abs/2409.08712v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how deep neural networks learn and forget features during forward propagation, and tracks the changes in their interactions and generalization capacity.</p><hr><h3>L3Cube-IndicQuest: A Benchmark Questing Answering Dataset for Evaluating  Knowledge of LLMs in Indic Context</h3>
<p><a href='http://arxiv.org/abs/2409.08706v1'>http://arxiv.org/abs/2409.08706v1</a></p>
<p><b>Compressor summary</b>: The paper introduces L3Cube-IndicQuest, a question-answering benchmark dataset for evaluating regional knowledge in multilingual LLMs across 20 Indic languages and five domains.</p><hr><h3>Personalized Weight Loss Management through Wearable Devices and  Artificial Intelligence</h3>
<p><a href='http://arxiv.org/abs/2409.08700v1'>http://arxiv.org/abs/2409.08700v1</a></p>
<p><b>Compressor summary</b>: The study uses wearable devices and AI to predict weight loss in overweight people by analyzing various data sources and achieves promising results with an 84.44% accuracy rate.</p><hr><h3>Precision Aquaculture: An Integrated Computer Vision and IoT Approach  for Optimized Tilapia Feeding</h3>
<p><a href='http://arxiv.org/abs/2409.08695v1'>http://arxiv.org/abs/2409.08695v1</a></p>
<p><b>Compressor summary</b>: Key points:
- An innovative system combines computer vision and IoT for precise Tilapia feeding
- It uses real-time sensors to monitor water quality and fish size/count
- A mobile app enables remote monitoring and control
- The method could increase production up to 58 times compared to traditional farms

Summary:
The system, which combines computer vision and IoT, monitors water quality and fish size with sensors and a mobile app, and feeds Tilapia optimally, potentially boosting production by 58 times.</p><hr><h3>Autoregressive Sequence Modeling for 3D Medical Image Representation</h3>
<p><a href='http://arxiv.org/abs/2409.08691v1'>http://arxiv.org/abs/2409.08691v1</a></p>
<p><b>Compressor summary</b>: The authors propose an autoregressive pre-training method for 3D medical image representations that leverages spatial, contrast, and semantic correlations to better understand and integrate contextual information.</p><hr><h3>Redesigning graph filter-based GNNs to relax the homophily assumption</h3>
<p><a href='http://arxiv.org/abs/2409.08676v1'>http://arxiv.org/abs/2409.08676v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new GNN architecture that can handle heterophilic data by reinterpreting graph filters and improving expressiveness, permutation equivariance, and performance.</p><hr><h3>AdR-Gaussian: Accelerating Gaussian Splatting with Adaptive Radius</h3>
<p><a href='http://arxiv.org/abs/2409.08669v1'>http://arxiv.org/abs/2409.08669v1</a></p>
<p><b>Compressor summary</b>: AdR-Gaussian accelerates 3D Gaussian splatting by moving culling to the preprocess stage, using adaptive radius and load balancing to reduce overhead and increase quality.</p><hr><h3>Test-time Training for Hyperspectral Image Super-resolution</h3>
<p><a href='http://arxiv.org/abs/2409.08667v1'>http://arxiv.org/abs/2409.08667v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel self-training framework with a new network architecture and data augmentation method for hyperspectral image super-resolution, achieving significant improvements over existing methods.</p><hr><h3>Towards certifiable AI in aviation: landscape, challenges, and  opportunities</h3>
<p><a href='http://arxiv.org/abs/2409.08666v1'>http://arxiv.org/abs/2409.08666v1</a></p>
<p><b>Compressor summary</b>: The paper provides a detailed overview of formal AI certification in avionics, discussing the challenges and importance of ensuring safety and reliability in AI systems.</p><hr><h3>Online Learning Of Expanding Graphs</h3>
<p><a href='http://arxiv.org/abs/2409.08660v1'>http://arxiv.org/abs/2409.08660v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new online algorithm for learning expanding graphs from spatiotemporal signals that can handle graph growth and node dynamics.</p><hr><h3>Promoting Fairness in Link Prediction with Graph Enhancement</h3>
<p><a href='http://arxiv.org/abs/2409.08658v1'>http://arxiv.org/abs/2409.08658v1</a></p>
<p><b>Compressor summary</b>: FairLink is a method that learns a fairness-enhanced graph for link prediction, ensuring equal link probabilities between nodes from the same sensitive group and reducing bias in predictions.</p><hr><h3>Training Gradient Boosted Decision Trees on Tabular Data Containing  Label Noise for Classification Tasks</h3>
<p><a href='http://arxiv.org/abs/2409.08647v1'>http://arxiv.org/abs/2409.08647v1</a></p>
<p><b>Compressor summary</b>: This paper studies how label noise affects gradient-boosted decision trees (GBDTs) for tabular data and proposes methods to improve their performance.</p><hr><h3>CPL: Critical Planning Step Learning Boosts LLM Generalization in  Reasoning Tasks</h3>
<p><a href='http://arxiv.org/abs/2409.08642v1'>http://arxiv.org/abs/2409.08642v1</a></p>
<p><b>Compressor summary</b>: CPL uses Monte Carlo Tree Search to improve large language models' general reasoning capabilities by learning step-level planning preferences, while Step-APO enhances existing preference learning approaches for complex multi-step reasoning tasks.</p><hr><h3>Developing an Algorithm Selector for Green Configuration in Scheduling  Problems</h3>
<p><a href='http://arxiv.org/abs/2409.08641v1'>http://arxiv.org/abs/2409.08641v1</a></p>
<p><b>Compressor summary</b>: The paper presents an intelligent algorithm selection tool for the Job Shop Scheduling Problem using machine learning, which optimizes energy efficiency and production metrics by recommending the best solver for each instance.</p><hr><h3>Byzantine-Robust and Communication-Efficient Distributed Learning via  Compressed Momentum Filtering</h3>
<p><a href='http://arxiv.org/abs/2409.08640v1'>http://arxiv.org/abs/2409.08640v1</a></p>
<p><b>Compressor summary</b>: Our method improves distributed learning by using Polyak Momentum to defend against Byzantine workers and achieve better convergence results.</p><hr><h3>Utilizing Data Fingerprints for Privacy-Preserving Algorithm Selection  in Time Series Classification: Performance and Uncertainty Estimation on  Unseen Datasets</h3>
<p><a href='http://arxiv.org/abs/2409.08636v1'>http://arxiv.org/abs/2409.08636v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel data fingerprint method that helps select AI algorithms for time series classification without needing access to all data points, improving algorithm selection accuracy.</p><hr><h3>Improving Analog Neural Network Robustness: A Noise-Agnostic Approach  with Explainable Regularizations</h3>
<p><a href='http://arxiv.org/abs/2409.08633v1'>http://arxiv.org/abs/2409.08633v1</a></p>
<p><b>Compressor summary</b>: The authors propose an approach to improve noise resistance in analog neural networks by revealing and using the underlying mechanisms that reduce sensitivity to noise.</p><hr><h3>Dense Point Clouds Matter: Dust-GS for Scene Reconstruction from Sparse  Viewpoints</h3>
<p><a href='http://arxiv.org/abs/2409.08613v1'>http://arxiv.org/abs/2409.08613v1</a></p>
<p><b>Compressor summary</b>: Dust-GS is a new framework for scene synthesis that improves on 3D Gaussian Splatting by using an adaptive masking technique and working better with sparse input data.</p><hr><h3>Optimizing Item-based Marketing Promotion Efficiency in C2C Marketplace  with Dynamic Sequential Coupon Allocation Framework</h3>
<p><a href='http://arxiv.org/abs/2409.08609v1'>http://arxiv.org/abs/2409.08609v1</a></p>
<p><b>Compressor summary</b>: DSCAF is a framework that optimizes coupons for e-commerce sellers by dynamically adjusting their allocation strategies across multiple promotions to maximize ROI and sell-through rate.</p><hr><h3>Knowledge-Enhanced Facial Expression Recognition with  Emotional-to-Neutral Transformation</h3>
<p><a href='http://arxiv.org/abs/2409.08598v1'>http://arxiv.org/abs/2409.08598v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Existing FER methods use discrete labels, which are limited for emotional recognition
- Proposed a novel method that uses text embeddings to enhance facial expression representations
- Used an emotional-to-neutral transformation with a self-contrast objective
- Outperformed state-of-the-art FER methods on four datasets using different visual encoders

Summary:
The paper proposes a new FER method that leverages text embeddings and an emotional-to-neutral transformation to improve facial expression recognition, achieving superior results on four datasets.</p><hr><h3>Large Language Model Can Transcribe Speech in Multi-Talker Scenarios  with Versatile Instructions</h3>
<p><a href='http://arxiv.org/abs/2409.08596v1'>http://arxiv.org/abs/2409.08596v1</a></p>
<p><b>Compressor summary</b>: The paper explores how large language models can be used to transcribe speech in multi-talker situations using different instructions and speaker characteristics.</p><hr><h3>Optimizing 4D Lookup Table for Low-light Video Enhancement via Wavelet  Priori</h3>
<p><a href='http://arxiv.org/abs/2409.08585v1'>http://arxiv.org/abs/2409.08585v1</a></p>
<p><b>Compressor summary</b>: The proposed WaveLUT method improves low-light video enhancement by using a wavelet-based lookup table, dynamic fusion strategy, and text-driven appearance reconstruction to achieve color coherence, accurate mapping, and low latency.</p><hr><h3>ChangeChat: An Interactive Model for Remote Sensing Change Analysis via  Multimodal Instruction Tuning</h3>
<p><a href='http://arxiv.org/abs/2409.08582v1'>http://arxiv.org/abs/2409.08582v1</a></p>
<p><b>Compressor summary</b>: ChangeChat is a bitemporal vision-language model that supports interactive RS change analysis using multimodal instruction tuning and the ChangeChat-87k dataset.</p><hr><h3>Molecular Graph Representation Learning via Structural Similarity  Information</h3>
<p><a href='http://arxiv.org/abs/2409.08580v1'>http://arxiv.org/abs/2409.08580v1</a></p>
<p><b>Compressor summary</b>: The Molecular Structural Similarity Motif GNN (MSSM-GNN) is a novel method that leverages graph kernel algorithms to capture structural similarity between molecules and improve feature representation learning for property prediction.</p><hr><h3>HTR-VT: Handwritten Text Recognition with Vision Transformer</h3>
<p><a href='http://arxiv.org/abs/2409.08573v1'>http://arxiv.org/abs/2409.08573v1</a></p>
<p><b>Compressor summary</b>: Key points:
- ViT for handwritten text recognition with limited data
- Data-efficient encoder + CNN + SAM optimizer
- Span mask technique as a regularizer
- Outperforms traditional models on small datasets and sets new benchmark on LAM dataset

Summary:
The paper proposes a data-efficient ViT method for handwritten text recognition that uses an encoder, a CNN, and a SAM optimizer with span masking. It beats conventional models on small datasets and achieves the best result on the largest LAM dataset.</p><hr><h3>DiffFAS: Face Anti-Spoofing via Generative Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2409.08572v1'>http://arxiv.org/abs/2409.08572v1</a></p>
<p><b>Compressor summary</b>: The paper proposes DiffFAS, a framework to improve face anti-spoofing by addressing image quality and style shifts between domains and attack types, using diffusion-based generation of high-fidelity spoof faces.</p><hr><h3>Batch Ensemble for Variance Dependent Regret in Stochastic Bandits</h3>
<p><a href='http://arxiv.org/abs/2409.08570v1'>http://arxiv.org/abs/2409.08570v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a simple batch ensemble scheme for online RL that achieves near-optimal regret in stochastic MAB with just one parameter, the number of batches.</p><hr><h3>Hybrid-TTA: Continual Test-time Adaptation via Dynamic Domain Shift  Detection</h3>
<p><a href='http://arxiv.org/abs/2409.08566v1'>http://arxiv.org/abs/2409.08566v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Hybrid-TTA, a method that adapts to domain shifts by combining Full-Tuning and Efficient-Tuning strategies with Dynamic Domain Shift Detection and Masked Image Modeling based Adaptation.</p><hr><h3>Cracking the Code: Multi-domain LLM Evaluation on Real-World  Professional Exams in Indonesia</h3>
<p><a href='http://arxiv.org/abs/2409.08564v1'>http://arxiv.org/abs/2409.08564v1</a></p>
<p><b>Compressor summary</b>: IndoCareer is a diverse dataset for evaluating language models on vocational and professional exams in Indonesia, highlighting their challenges in local contexts like insurance and finance.</p><hr><h3>Second-order difference subspace</h3>
<p><a href='http://arxiv.org/abs/2409.08563v1'>http://arxiv.org/abs/2409.08563v1</a></p>
<p><b>Compressor summary</b>: The paper introduces the second-order difference subspace, which analyzes geometric differences between multiple subspaces in machine learning, and applies it to temporal shape analysis and biometric signal analysis.</p><hr><h3>CSS: Overcoming Pose and Scene Challenges in Crowd-Sourced 3D Gaussian  Splatting</h3>
<p><a href='http://arxiv.org/abs/2409.08562v1'>http://arxiv.org/abs/2409.08562v1</a></p>
<p><b>Compressor summary</b>: CSS is a new technique that uses crowd-sourced images to reconstruct challenging scenes with high quality and accuracy, overcoming limitations of traditional 3D methods.</p><hr><h3>Expediting and Elevating Large Language Model Reasoning via Hidden  Chain-of-Thought Decoding</h3>
<p><a href='http://arxiv.org/abs/2409.08561v1'>http://arxiv.org/abs/2409.08561v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Large language models can reason and solve problems using chain-of-thought (CoT) prompting, but it's slow and costly to generate the full CoT process.
- The proposed method compresses the CoT process through semantic alignment, using an auxiliary CoT model that learns to generate a compact representation of the thought process.
- The method achieves competitive or improved performance compared to the full CoT baseline, while providing significant speedup in decoding time.

Summary:
The paper proposes a novel approach to compress the chain-of-thought (CoT) process in large language models using semantic alignment and an auxiliary model, improving efficiency and performance in various tasks.</p><hr><h3>Fair CoVariance Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2409.08558v1'>http://arxiv.org/abs/2409.08558v1</a></p>
<p><b>Compressor summary</b>: Fair coVariance Neural Networks (FVNNs) use graph convolutions to process covariance matrices for both fair and accurate predictions in signal processing and machine learning applications.</p><hr><h3>DICS: Find Domain-Invariant and Class-Specific Features for  Out-of-Distribution Generalization</h3>
<p><a href='http://arxiv.org/abs/2409.08557v1'>http://arxiv.org/abs/2409.08557v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a DICS model to extract domain-invariant and class-specific features for deep neural networks, which improves their performance in out-of-distribution scenarios.</p><hr><h3>LLM-Powered Grapheme-to-Phoneme Conversion: Benchmark and Case Study</h3>
<p><a href='http://arxiv.org/abs/2409.08554v1'>http://arxiv.org/abs/2409.08554v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates large language models for grapheme-to-phoneme conversion and proposes methods to improve their performance without extra training or data, showing they can outperform traditional tools in Persian.</p><hr><h3>Causal GNNs: A GNN-Driven Instrumental Variable Approach for Causal  Inference in Networks</h3>
<p><a href='http://arxiv.org/abs/2409.08544v1'>http://arxiv.org/abs/2409.08544v1</a></p>
<p><b>Compressor summary</b>: CgNN is a new method that uses network structure as instrumental variables to estimate causal effects in network data, while accounting for hidden confounders and node importance.</p><hr><h3>An Efficient Privacy-aware Split Learning Framework for Satellite  Communications</h3>
<p><a href='http://arxiv.org/abs/2409.08538v1'>http://arxiv.org/abs/2409.08538v1</a></p>
<p><b>Compressor summary</b>: The text proposes DTIP, a novel framework that uses split learning and differential privacy to enhance satellite communication efficiency, accuracy, and privacy.</p><hr><h3>Integration of Mamba and Transformer -- MAT for Long-Short Range Time  Series Forecasting with Application to Weather Dynamics</h3>
<p><a href='http://arxiv.org/abs/2409.08530v1'>http://arxiv.org/abs/2409.08530v1</a></p>
<p><b>Compressor summary</b>: MAT is a new hybrid model combining Mamba and Transformer techniques to improve long-short range time series forecasting by leveraging their respective strengths in capturing dependencies and patterns.</p><hr><h3>Eir: Thai Medical Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2409.08523v1'>http://arxiv.org/abs/2409.08523v1</a></p>
<p><b>Compressor summary</b>: Eir Thai Medical LLM is a large language model that enhances medical tasks in Thai with high accuracy and clear answers for healthcare professionals and patients.</p><hr><h3>GroundingBooth: Grounding Text-to-Image Customization</h3>
<p><a href='http://arxiv.org/abs/2409.08520v1'>http://arxiv.org/abs/2409.08520v1</a></p>
<p><b>Compressor summary</b>: The paper presents GroundingBooth, a framework that generates personalized images with accurate layout alignment and identity preservation in text-to-image customization tasks, enabling the customization of multiple subjects at once.</p><hr><h3>Anytime Continual Learning for Open Vocabulary Classification</h3>
<p><a href='http://arxiv.org/abs/2409.08518v1'>http://arxiv.org/abs/2409.08518v1</a></p>
<p><b>Compressor summary</b>: The authors present a method for open vocabulary image classification that can learn from new data anytime, improve existing models, and reduce storage and computation using attention-weighted PCA compression.</p><hr><h3>Mamba-YOLO-World: Marrying YOLO-World with Mamba for Open-Vocabulary  Detection</h3>
<p><a href='http://arxiv.org/abs/2409.08513v1'>http://arxiv.org/abs/2409.08513v1</a></p>
<p><b>Compressor summary</b>: Mamba-YOLO-World improves object detection beyond predefined categories by introducing a novel feature fusion mechanism that combines speed and efficiency.</p><hr><h3>Exploiting Supervised Poison Vulnerability to Strengthen Self-Supervised  Defense</h3>
<p><a href='http://arxiv.org/abs/2409.08509v1'>http://arxiv.org/abs/2409.08509v1</a></p>
<p><b>Compressor summary</b>: VESPR is a defense mechanism that exploits supervised learning's vulnerability to poison attacks, enhancing self-supervised learning's performance on poisoned images and outperforming six previous defenses.</p><hr><h3>Identifying Human Indoor Daily Life Behavior employing Thermal Sensor  Arrays (TSAs)</h3>
<p><a href='http://arxiv.org/abs/2409.08508v1'>http://arxiv.org/abs/2409.08508v1</a></p>
<p><b>Compressor summary</b>: The study used thermal sensor arrays to monitor daily activities in households, preserving privacy and accurately detecting sleep and daily life activities.</p><hr><h3>A BERT-Based Summarization approach for depression detection</h3>
<p><a href='http://arxiv.org/abs/2409.08483v1'>http://arxiv.org/abs/2409.08483v1</a></p>
<p><b>Compressor summary</b>: The text describes using machine learning and AI to detect depression indicators from text data by analyzing interviews with virtual agents and proposes text summarization as a preprocessing technique to improve accuracy.</p><hr><h3>Risks When Sharing LoRA Fine-Tuned Diffusion Model Weights</h3>
<p><a href='http://arxiv.org/abs/2409.08482v1'>http://arxiv.org/abs/2409.08482v1</a></p>
<p><b>Compressor summary</b>: The paper explores privacy risks of fine-tuning diffusion models on personal images and shows that existing defenses fail to protect the data.</p><hr><h3>Integrating Neural Operators with Diffusion Models Improves Spectral  Representation in Turbulence Modeling</h3>
<p><a href='http://arxiv.org/abs/2409.08477v1'>http://arxiv.org/abs/2409.08477v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new method that combines neural operators with diffusion models to improve the surrogate modeling of turbulent flows by enhancing the resolution of turbulent structures and better capturing high-frequency flow dynamics.</p><hr><h3>RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense  Positive Supervision</h3>
<p><a href='http://arxiv.org/abs/2409.08475v1'>http://arxiv.org/abs/2409.08475v1</a></p>
<p><b>Compressor summary</b>: RT-DETRv3 improves real-time object detection by adding a CNN branch, self-attention perturbation, and a shared-weight decoder branch for dense positive supervision.</p><hr><h3>Rethinking Meta-Learning from a Learning Lens</h3>
<p><a href='http://arxiv.org/abs/2409.08474v1'>http://arxiv.org/abs/2409.08474v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new meta-learning method called Task Relation Learner (TRLearner) that uses task relations to calibrate optimization and reduce overfitting and underfitting issues in previous methods.</p><hr><h3>Explaining Datasets in Words: Statistical Models with Natural Language  Parameters</h3>
<p><a href='http://arxiv.org/abs/2409.08466v1'>http://arxiv.org/abs/2409.08466v1</a></p>
<p><b>Compressor summary</b>: The authors propose a framework to fit statistical models with interpretable natural language predicates, which can be applied to various problems in textual and visual domains.</p><hr><h3>Inter Observer Variability Assessment through Ordered Weighted Belief  Divergence Measure in MAGDM Application to the Ensemble Classifier Feature  Fusion</h3>
<p><a href='http://arxiv.org/abs/2409.08450v1'>http://arxiv.org/abs/2409.08450v1</a></p>
<p><b>Compressor summary</b>: The study proposes an Evidential MAGDM method that handles uncertainty and conflict among experts by assessing inter-observational variability, generating belief degrees, and constructing weighted belief and plausibility measures.</p><hr><h3>Towards Unified Facial Action Unit Recognition Framework by Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2409.08444v1'>http://arxiv.org/abs/2409.08444v1</a></p>
<p><b>Compressor summary</b>: AU-LLaVA is a new framework that uses a large language model to recognize facial expressions accurately and generate different formats of results for the same image.</p><hr><h3>CF-PRNet: Coarse-to-Fine Prototype Refining Network for Point Cloud  Completion and Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2409.08443v1'>http://arxiv.org/abs/2409.08443v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper introduces CF-PRNet, a network that reconstructs 3D shapes of fruits from partial views for agricultural tasks.
- The network uses coarse-to-fine prototype refining with scaling vectors to complete point clouds accurately.
- The network achieves high performance metrics and wins a challenge in shape completion and reconstruction of sweet peppers.

Summary:
CF-PRNet is a novel network that reconstructs 3D fruit shapes from partial views using coarse-to-fine refining with scaling vectors, and performs well in a shape completion and reconstruction challenge.</p><hr><h3>When Context Leads but Parametric Memory Follows in Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2409.08435v1'>http://arxiv.org/abs/2409.08435v1</a></p>
<p><b>Compressor summary</b>: This study examines how large language models use contextual and parametric knowledge to answer questions in consistent scenarios, finding a balance between the two and fewer hallucinations with more context.</p><hr><h3>Predictive Control and Regret Analysis of Non-Stationary MDP with  Look-ahead Information</h3>
<p><a href='http://arxiv.org/abs/2409.08434v1'>http://arxiv.org/abs/2409.08434v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an algorithm for policy design in non-stationary MDPs that uses look-ahead predictions to achieve low regret, and shows its effectiveness in simulations.</p>