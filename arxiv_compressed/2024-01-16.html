
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-01-16</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-01-16 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Machine Translation Models are Zero-Shot Detectors of Translation  Direction</h3>
<p>Michelle Wastl,Jannis Vamvas,Rico Sennrich</p>
<p><a href='http://arxiv.org/abs/2401.06769v1'>http://arxiv.org/abs/2401.06769v1</a></p>
<p><b>Compressor summary</b>: The authors propose an unsupervised method to detect translation direction of parallel text using the simplification effect in machine translation and achieve high accuracies for both NMT and human translations.</p><hr><h3>Mind Your Format: Towards Consistent Evaluation of In-Context Learning  Improvements</h3>
<p>Anton Voronov,Lena Wolf,Max Ryabinin</p>
<p><a href='http://arxiv.org/abs/2401.06766v1'>http://arxiv.org/abs/2401.06766v1</a></p>
<p><b>Compressor summary</b>: The prompt template format significantly affects large language models' in-context learning performance, and using Template Ensembles can improve their accuracy by aggregating predictions across different templates.</p><hr><h3>Seeing the roads through the trees: A benchmark for modeling spatial  dependencies with aerial imagery</h3>
<p>Caleb Robinson,Isaac Corley,Anthony Ortiz,Rahul Dodhia,Juan M. Lavista Ferres,Peyman Najafirad</p>
<p><a href='http://arxiv.org/abs/2401.06762v1'>http://arxiv.org/abs/2401.06762v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new benchmark dataset (Chesapeake Roads Spatial Context) for testing how well geospatial machine learning models understand spatial context over long distances, and shows that current models often fail at this task.</p><hr><h3>APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding</h3>
<p>Mingdao Liu,Aohan Zeng,Bowen Wang,Peng Zhang,Jie Tang,Yuxiao Dong</p>
<p><a href='http://arxiv.org/abs/2401.06761v1'>http://arxiv.org/abs/2401.06761v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a parallel auto-regressive generation method that speeds up text generation by LLMs and reduces resource consumption.</p><hr><h3>Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies</h3>
<p>Tom Kocmi,Vilém Zouhar,Christian Federmann,Matt Post</p>
<p><a href='http://arxiv.org/abs/2401.06760v1'>http://arxiv.org/abs/2401.06760v1</a></p>
<p><b>Compressor summary</b>: The paper explores how different metrics measure human-noticeable differences in machine translation quality using a new dataset and finding more stable results than p-values.</p><hr><h3>Synthetic Data Generation Framework, Dataset, and Efficient Deep Model  for Pedestrian Intention Prediction</h3>
<p>Muhammad Naveed Riaz,Maciej Wielgosz,Abel Garcia Romera,Antonio M. Lopez</p>
<p><a href='http://arxiv.org/abs/2401.06757v1'>http://arxiv.org/abs/2401.06757v1</a></p>
<p><b>Compressor summary</b>: ARCANE generates diverse synthetic datasets for pedestrian intention prediction, complementing real-world data, and PedGNN is a fast and memory-efficient model for this task.</p><hr><h3>Stylometry Analysis of Multi-authored Documents for Authorship and  Author Style Change Detection</h3>
<p>Muhammad Tayyab Zamir,Muhammad Asif Ayub,Asma Gul,Nasir Ahmad,Kashif Ahmad</p>
<p><a href='http://arxiv.org/abs/2401.06752v1'>http://arxiv.org/abs/2401.06752v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new framework that uses style analysis to detect authorship and changes in multi-authored documents, improving on existing methods with special characters and weight optimization.</p><hr><h3>The Unreasonable Effectiveness of Easy Training Data for Hard Tasks</h3>
<p>Peter Hase,Mohit Bansal,Peter Clark,Sarah Wiegreffe</p>
<p><a href='http://arxiv.org/abs/2401.06751v1'>http://arxiv.org/abs/2401.06751v1</a></p>
<p><b>Compressor summary</b>: This paper shows that current language models can generalize well from easy to hard data using simple training methods, and suggests collecting easy data instead of hard data for better performance.</p><hr><h3>Using Natural Language Inference to Improve Persona Extraction from  Dialogue in a New Domain</h3>
<p>Alexandra DeLucia,Mengjie Zhao,Yoshinori Maeda,Makoto Yoda,Keiichi Yamada,Hiromi Wakaki</p>
<p><a href='http://arxiv.org/abs/2401.06742v1'>http://arxiv.org/abs/2401.06742v1</a></p>
<p><b>Compressor summary</b>: The authors propose a natural language inference method for adapting a persona extraction model to new settings without needing new data or human annotation.</p><hr><h3>Relying on the Unreliable: The Impact of Language Models' Reluctance to  Express Uncertainty</h3>
<p>Kaitlyn Zhou,Jena D. Hwang,Xiang Ren,Maarten Sap</p>
<p><a href='http://arxiv.org/abs/2401.06730v1'>http://arxiv.org/abs/2401.06730v1</a></p>
<p><b>Compressor summary</b>: The text discusses the challenges of AI language models expressing uncertainties in their responses, which can lead to overconfidence and safety risks for users who rely on them.</p><hr><h3>Deep Manifold Graph Auto-Encoder for Attributed Graph Embedding</h3>
<p>Bozhen Hu,Zelin Zang,Jun Xia,Lirong Wu,Cheng Tan,Stan Z. Li</p>
<p><a href='http://arxiv.org/abs/2401.06727v1'>http://arxiv.org/abs/2401.06727v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method for embedding graph data in a low-dimensional space that preserves topological structure and improves stability and quality, outperforming existing approaches on various tasks.</p><hr><h3>Reframing Tax Law Entailment as Analogical Reasoning</h3>
<p>Xinrui Zou,Ming Zhang,Nathaniel Weir,Benjamin Van Durme,Nils Holzenberger</p>
<p><a href='http://arxiv.org/abs/2401.06715v1'>http://arxiv.org/abs/2401.06715v1</a></p>
<p><b>Compressor summary</b>: The paper proposes re-framing statutory reasoning as an analogy task to increase dataset size and interpretability, and improves upon existing methods by combining retrieval and analogy models.</p><hr><h3>Few-Shot Detection of Machine-Generated Text using Style Representations</h3>
<p>Rafael Rivera Soto,Kailin Koch,Aleem Khan,Barry Chen,Marcus Bishop,Nicholas Andrews</p>
<p><a href='http://arxiv.org/abs/2401.06712v1'>http://arxiv.org/abs/2401.06712v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to distinguish human from machine-written text without relying on training data from potentially abusive language models, using style features that work across different models.</p><hr><h3>Model-Free Approximate Bayesian Learning for Large-Scale Conversion  Funnel Optimization</h3>
<p>Garud Iyengar,Raghav Singal</p>
<p><a href='http://arxiv.org/abs/2401.06710v1'>http://arxiv.org/abs/2401.06710v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper studies how to optimize personalized interventions for different consumer states in marketing campaigns.
- It proposes a novel algorithm that combines attribution-based decision-making and approximate Bayesian learning.
- It shows high accuracy, interpretability, scalability, and asymptotic optimality of the algorithm on a real-world email marketing dataset.

Summary:
The paper introduces an optimal algorithm for personalized interventions in marketing campaigns that uses attribution-based decision-making and approximate Bayesian learning to learn from consumer interactions.</p><hr><h3>Reliability Analysis of Psychological Concept Extraction and  Classification in User-penned Text</h3>
<p>Muskan Garg,MSVPJ Sathvik,Amrit Chadha,Shaina Raza,Sunghwan Sohn</p>
<p><a href='http://arxiv.org/abs/2401.06709v1'>http://arxiv.org/abs/2401.06709v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new dataset for analyzing low self-esteem in Reddit posts and suggests that current models should focus more on textual cues related to self-esteem rather than triggers or consequences of mental disturbances.</p><hr><h3>Multi-Candidate Speculative Decoding</h3>
<p>Sen Yang,Shujian Huang,Xinyu Dai,Jiajun Chen</p>
<p><a href='http://arxiv.org/abs/2401.06706v1'>http://arxiv.org/abs/2401.06706v1</a></p>
<p><b>Compressor summary</b>: This paper improves speculative decoding by sampling multiple candidates and verifying them in batches, leading to higher acceptance rates for large language models.</p><hr><h3>Scalable 3D Panoptic Segmentation With Superpoint Graph Clustering</h3>
<p>Damien Robert,Hugo Raguet,Loic Landrieu</p>
<p><a href='http://arxiv.org/abs/2401.06704v1'>http://arxiv.org/abs/2401.06704v1</a></p>
<p><b>Compressor summary</b>: SuperCluster is an efficient and scalable method for panoptic segmentation of large 3D point clouds using graph clustering and superpoint adaptation, achieving state-of-the-art results on multiple datasets while being much smaller and faster than previous methods.</p><hr><h3>A Closed-form Solution for Weight Optimization in Fully-connected  Feed-forward Neural Networks</h3>
<p>Slavisa Tomic,João Pedro Matos-Carvalho,Marko Beko</p>
<p><a href='http://arxiv.org/abs/2401.06699v1'>http://arxiv.org/abs/2401.06699v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a weight optimization method for neural networks using least squares that is faster and easier to implement than existing methods.</p><hr><h3>An Experimental Design Framework for Label-Efficient Supervised  Finetuning of Large Language Models</h3>
<p>Gantavya Bhatt,Yifang Chen,Arnav M. Das,Jifan Zhang,Sang T. Truong,Stephen Mussmann,Yinglun Zhu,Jeffrey Bilmes,Simon S. Du,Kevin Jamieson,Jordan T. Ash,Robert D. Nowak</p>
<p><a href='http://arxiv.org/abs/2401.06692v1'>http://arxiv.org/abs/2401.06692v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using experimental design techniques to reduce the annotation cost and increase the label efficiency for supervised finetuning on instruction datasets for large language models.</p><hr><h3>Embedded Planogram Compliance Control System</h3>
<p>M. Erkin Yücel,Serkan Topaloğlu,Cem Ünsalan</p>
<p><a href='http://arxiv.org/abs/2401.06690v1'>http://arxiv.org/abs/2401.06690v1</a></p>
<p><b>Compressor summary</b>: The study proposes an embedded system using computer vision and deep learning techniques for planogram compliance control in retail, achieving high F1 scores and working stand-alone for up to two years with energy harvesting options.</p><hr><h3>Don't Rank, Combine! Combining Machine Translation Hypotheses Using  Quality Estimation</h3>
<p>Giorgos Vernikos,Andrei Popescu-Belis</p>
<p><a href='http://arxiv.org/abs/2401.06688v1'>http://arxiv.org/abs/2401.06688v1</a></p>
<p><b>Compressor summary</b>: QE-fusion is a method that uses quality estimation metrics to improve neural machine translation, achieving better results than existing techniques and scaling linearly with candidate diversity.</p><hr><h3>Proximal Causal Inference With Text Data</h3>
<p>Jacob M. Chen,Rohit Bhattacharya,Katherine A. Keith</p>
<p><a href='http://arxiv.org/abs/2401.06687v1'>http://arxiv.org/abs/2401.06687v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new causal inference method using text data and zero-shot models to handle unobserved confounding variables, which is novel and has low bias.</p><hr><h3>How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to  Challenge AI Safety by Humanizing LLMs</h3>
<p>Yi Zeng,Hongpeng Lin,Jingwen Zhang,Diyi Yang,Ruoxi Jia,Weiyan Shi</p>
<p><a href='http://arxiv.org/abs/2401.06373v1'>http://arxiv.org/abs/2401.06373v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using social science research to generate persuasive prompts that can jailbreak large language models, highlighting the need for better defense mechanisms.</p><hr><h3>WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual  World Knowledge</h3>
<p>Wenbin Wang,Liang Ding,Li Shen,Yong Luo,Han Hu,Dacheng Tao</p>
<p><a href='http://arxiv.org/abs/2401.06659v1'>http://arxiv.org/abs/2401.06659v1</a></p>
<p><b>Compressor summary</b>: WisdoM is a framework that uses large vision-language models to analyze images and text for sentiment analysis, incorporating contextual world knowledge and improving performance by +1.89 F1 score on average.</p><hr><h3>Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI  Benchmarks</h3>
<p>Stefan Blücher,Johanna Vielhaben,Nils Strodthoff</p>
<p><a href='http://arxiv.org/abs/2401.06654v1'>http://arxiv.org/abs/2401.06654v1</a></p>
<p><b>Compressor summary</b>: The study proposes the R-OMS score to compare occlusion strategies in XAI and the SRG measure to resolve disagreements between MIF and LIF measures.</p><hr><h3>Block Majorization Minimization with Extrapolation and Application to  $β$-NMF</h3>
<p>Le Thi Khanh Hien,Valentin Leplat,Nicolas Gillis</p>
<p><a href='http://arxiv.org/abs/2401.06646v1'>http://arxiv.org/abs/2401.06646v1</a></p>
<p><b>Compressor summary</b>: BMMe is a new method to solve multi-convex optimization problems faster by updating parameters adaptively and using extrapolation, which is shown to be efficient for nonnegative matrix factorization with different $\beta$-divergences.</p><hr><h3>SeizNet: An AI-enabled Implantable Sensor Network System for Seizure  Prediction</h3>
<p>Ali Saeizadeh,Douglas Schonholtz,Daniel Uvaydov,Raffaele Guida,Emrecan Demirors,Pedram Johari,Jorge M. Jimenez,Joseph S. Neimat,Tommaso Melodia</p>
<p><a href='http://arxiv.org/abs/2401.06644v1'>http://arxiv.org/abs/2401.06644v1</a></p>
<p><b>Compressor summary</b>: SeizNet is a closed-loop system that uses deep learning and implantable sensors to predict drug-resistant epileptic seizures with high accuracy and specificity, providing a better alternative to traditional treatments.</p><hr><h3>Effects of diversity incentives on sample diversity and downstream model  performance in LLM-based text augmentation</h3>
<p>Jan Cegin,Branislav Pecher,Jakub Simko,Ivan Srba,Maria Bielikova,Peter Brusilovsky</p>
<p><a href='http://arxiv.org/abs/2401.06643v1'>http://arxiv.org/abs/2401.06643v1</a></p>
<p><b>Compressor summary</b>: The study explores how using taboo words, hints from previous solutions, and chaining on previous solutions can increase text diversity in LLM-generated data and improve downstream models' performance.</p><hr><h3>Experimental Contexts Can Facilitate Robust Semantic Property Inference  in Language Models, but Inconsistently</h3>
<p>Kanishka Misra,Allyson Ettinger,Kyle Mahowald</p>
<p><a href='http://arxiv.org/abs/2401.06640v1'>http://arxiv.org/abs/2401.06640v1</a></p>
<p><b>Compressor summary</b>: Experimental contexts can improve language models' performance in predicting semantic properties of novel concepts, but this ability is inconsistent and relies on controlling the input examples and instructions.</p><hr><h3>OOP: Object-Oriented Programming Evaluation Benchmark for Large Language  Models</h3>
<p>Shuai Wang,Liang Ding,Li Shen,Yong Luo,Bo Du,Dacheng Tao</p>
<p><a href='http://arxiv.org/abs/2401.06628v1'>http://arxiv.org/abs/2401.06628v1</a></p>
<p><b>Compressor summary</b>: The text introduces an OOP-focused code generation benchmark and evaluation metric, pass@o, which reveals the limitations of current LLMs in OOP.</p><hr><h3>TransliCo: A Contrastive Learning Framework to Address the Script  Barrier in Multilingual Pretrained Language Models</h3>
<p>Yihong Liu,Chunlan Ma,Haotian Ye,Hinrich Schütze</p>
<p><a href='http://arxiv.org/abs/2401.06620v1'>http://arxiv.org/abs/2401.06620v1</a></p>
<p><b>Compressor summary</b>: TransliCo is a framework that fine-tunes pretrained language models to improve crosslingual transfer by contrasting sentences with their transliterations in a unified script.</p><hr><h3>Identifying Policy Gradient Subspaces</h3>
<p>Jan Schneider,Pierre Schumacher,Simon Guist,Le Chen,Daniel Häufle,Bernhard Schölkopf,Dieter Büchler</p>
<p><a href='http://arxiv.org/abs/2401.06604v1'>http://arxiv.org/abs/2401.06604v1</a></p>
<p><b>Compressor summary</b>: This paper evaluates how exploiting low-dimensional gradient subspaces can improve the training efficiency of deep policy gradient methods in reinforcement learning.</p><hr><h3>Mutual Enhancement of Large Language and Reinforcement Learning Models  through Bi-Directional Feedback Mechanisms: A Case Study</h3>
<p>Shangding Gu</p>
<p><a href='http://arxiv.org/abs/2401.06603v1'>http://arxiv.org/abs/2401.06603v1</a></p>
<p><b>Compressor summary</b>: The study proposes a teacher-student learning framework where a large language model (LLM) acts as a teacher and a reinforcement learning (RL) model serves as a student, enabling both agents to improve each other through feedback and cooperation in challenging tasks.</p><hr><h3>Every Node is Different: Dynamically Fusing Self-Supervised Tasks for  Attributed Graph Clustering</h3>
<p>Pengfei Zhu,Qian Wang,Yu Wang,Jialu Li,Qinghua Hu</p>
<p><a href='http://arxiv.org/abs/2401.06595v1'>http://arxiv.org/abs/2401.06595v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a graph clustering method that adapts the weights of self-supervised learning tasks for different nodes and fuses their embeddings, achieving state-of-the-art results.</p><hr><h3>Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained  Evaluation</h3>
<p>Seongyun Lee,Seungone Kim,Sue Hyun Park,Geewook Kim,Minjoon Seo</p>
<p><a href='http://arxiv.org/abs/2401.06591v1'>http://arxiv.org/abs/2401.06591v1</a></p>
<p><b>Compressor summary</b>: The authors propose Prometheus-Vision, an evaluator model that uses a new feedback dataset to assess long-form responses generated by Vision-Language Models based on user-defined criteria.</p><hr><h3>Mapping Transformer Leveraged Embeddings for Cross-Lingual Document  Representation</h3>
<p>Tsegaye Misikir Tashu,Eduard-Raul Kontos,Matthia Sabatelli,Matias Valdenegro-Toro</p>
<p><a href='http://arxiv.org/abs/2401.06583v1'>http://arxiv.org/abs/2401.06583v1</a></p>
<p><b>Compressor summary</b>: The study evaluates four multilingual transformer models in creating cross-lingual document representations to overcome limitations in recommending documents in different languages.</p><hr><h3>360DVD: Controllable Panorama Video Generation with 360-Degree Video  Diffusion Model</h3>
<p>Qian Wang,Weiqi Li,Chong Mou,Xinhua Cheng,Jian Zhang</p>
<p><a href='http://arxiv.org/abs/2401.06578v1'>http://arxiv.org/abs/2401.06578v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a method to generate 360-degree panoramic videos from text prompts using a modified text-to-video diffusion model and a new panorama dataset called WEB360.</p><hr><h3>Lost in the Source Language: How Large Language Models Evaluate the  Quality of Machine Translation</h3>
<p>Xu Huang,Zhirui Zhang,Xiang Geng,Yichao Du,Jiajun Chen,Shujian Huang</p>
<p><a href='http://arxiv.org/abs/2401.06568v1'>http://arxiv.org/abs/2401.06568v1</a></p>
<p><b>Compressor summary</b>: The study examines how large language models use source and reference information to evaluate translations, finding that reference information improves accuracy while source information sometimes hinders it, indicating a need for better cross-lingual capability in LLMs.</p><hr><h3>Resource-Efficient Gesture Recognition using Low-Resolution Thermal  Camera via Spiking Neural Networks and Sparse Segmentation</h3>
<p>Ali Safa,Wout Mommen,Lars Keuninckx</p>
<p><a href='http://arxiv.org/abs/2401.06563v1'>http://arxiv.org/abs/2401.06563v1</a></p>
<p><b>Compressor summary</b>: The paper presents a low-cost hand gesture recognition system using a thermal sensor and a Spiking Neural Network, achieving high accuracy and outperforming deep learning approaches.</p><hr><h3>Intention Analysis Prompting Makes Large Language Models A Good  Jailbreak Defender</h3>
<p>Yuqi Zhang,Liang Ding,Lefei Zhang,Dacheng Tao</p>
<p><a href='http://arxiv.org/abs/2401.06561v1'>http://arxiv.org/abs/2401.06561v1</a></p>
<p><b>Compressor summary</b>: The study proposes a defense strategy called Intention Analysis Prompting (IAPrompt) to reduce the harmfulness of large language models without compromising their helpfulness, by triggering their self-correct and improve ability.</p><hr><h3>A General Benchmark Framework is Dynamic Graph Neural Network Need</h3>
<p>Yusen Zhang</p>
<p><a href='http://arxiv.org/abs/2401.06559v1'>http://arxiv.org/abs/2401.06559v1</a></p>
<p><b>Compressor summary</b>: This paper argues that a unified benchmark framework is needed to accurately evaluate dynamic graph learning models and improve their performance in various applications.</p><hr><h3>Treatment-Aware Hyperbolic Representation Learning for Causal Effect  Estimation with Social Networks</h3>
<p>Ziqiang Cui,Xing Tang,Yang Qiao,Bowei He,Liang Chen,Xiuqiang He,Chen Ma</p>
<p><a href='http://arxiv.org/abs/2401.06557v1'>http://arxiv.org/abs/2401.06557v1</a></p>
<p><b>Compressor summary</b>: TAHyper is a novel method that uses hyperbolic space and treatment-aware relationship identification to estimate hidden confounders in social networks for individual treatment effect estimation.</p><hr><h3>Multimodal Learning for detecting urban functional zones using remote  sensing image and multi-semantic information</h3>
<p>Chuanji Shi,Yingying Zhang,Jiaotuan Wang,Qiqi Zhu</p>
<p><a href='http://arxiv.org/abs/2401.06550v1'>http://arxiv.org/abs/2401.06550v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an algorithm using multimodal deep learning to detect urban area-of-interest fence polygons from remote sensing images and multi-semantics data, improving accuracy for mobile Internet businesses.</p><hr><h3>Enhancing Consistency and Mitigating Bias: A Data Replay Approach for  Incremental Learning</h3>
<p>Chenyang Wang,Junjun Jiang,Xingyu Hu,Xianming Liu,Xiangyang Ji</p>
<p><a href='http://arxiv.org/abs/2401.06548v1'>http://arxiv.org/abs/2401.06548v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Deep learning systems suffer from catastrophic forgetting when learning new tasks without access to old data
- Data-free data replay methods invert samples from the classification model to reuse old data
- Existing methods ignore the inconsistency of inverted and real data, which affects performance
- The proposed CCIL method measures and reduces the data consistency problem using a novel loss function and class weight regularization

Summary:
CCIL is a new method that improves deep learning systems' continuity by measuring and reducing the inconsistency of inverting samples from the model to replay old data, which mitigates catastrophic forgetting.</p><hr><h3>Optimizing Feature Selection for Binary Classification with Noisy  Labels: A Genetic Algorithm Approach</h3>
<p>Vandad Imani,Elaheh Moradi,Carlos Sevilla-Salcedo,Vittorio Fortino,Jussi Tohka</p>
<p><a href='http://arxiv.org/abs/2401.06546v1'>http://arxiv.org/abs/2401.06546v1</a></p>
<p><b>Compressor summary</b>: The paper proposes NMFS-GA, a genetic algorithm to choose accurate and interpretable features for binary classification with noisy labels.</p><hr><h3>Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and  Outlook</h3>
<p>Ziying Song,Lin Liu,Feiyang Jia,Yadan Luo,Guoxin Zhang,Lei Yang,Li Wang,Caiyan Jia</p>
<p><a href='http://arxiv.org/abs/2401.06542v1'>http://arxiv.org/abs/2401.06542v1</a></p>
<p><b>Compressor summary</b>: The text discusses the importance of evaluating 3D object detection methods for autonomous driving in terms of accuracy, latency, and robustness against environmental variations and weather changes.</p><hr><h3>Medical Dialogue Generation via Intuitive-then-Analytical Differential  Diagnosis</h3>
<p>Kaishuai Xu,Wenjun Hou,Yi Cheng,Jian Wang,Wenjie Li</p>
<p><a href='http://arxiv.org/abs/2401.06541v1'>http://arxiv.org/abs/2401.06541v1</a></p>
<p><b>Compressor summary</b>: The IADDx framework generates medical dialogues that include a comprehensive differential diagnosis using retrieval-based intuitive association and graph-enhanced analytic reasoning, helping both clinicians and patients understand the diagnostic process.</p><hr><h3>INTERS: Unlocking the Power of Large Language Models in Search with  Instruction Tuning</h3>
<p>Yutao Zhu,Peitian Zhang,Chenghao Zhang,Yifei Chen,Binyu Xie,Zhicheng Dou,Zheng Liu,Ji-Rong Wen</p>
<p><a href='http://arxiv.org/abs/2401.06532v1'>http://arxiv.org/abs/2401.06532v1</a></p>
<p><b>Compressor summary</b>: This paper introduces INTERS, a new dataset for instruction tuning to improve large language models' performance in information retrieval tasks.</p><hr><h3>PCB-Vision: A Multiscene RGB-Hyperspectral Benchmark Dataset of Printed  Circuit Boards</h3>
<p>Elias Arbash,Margret Fuchs,Behnood Rasti,Sandra Lorenz,Pedram Ghamisi,Richard Gloaguen</p>
<p><a href='http://arxiv.org/abs/2401.06528v1'>http://arxiv.org/abs/2401.06528v1</a></p>
<p><b>Compressor summary</b>: The paper presents PCB-Vision, a dataset of RGB and hyperspectral images for analyzing electronic waste composition to improve recycling efficiency and align with the UN Sustainable Development Goals.</p><hr><h3>MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection</h3>
<p>Paloma Piot,Patricia Martín-Rodilla,Javier Parapar</p>
<p><a href='http://arxiv.org/abs/2401.06526v1'>http://arxiv.org/abs/2401.06526v1</a></p>
<p><b>Compressor summary</b>: The paper presents MetaHate, a meta-collection of datasets for studying hate speech, to help develop better models for combating it online.</p><hr><h3>Domain Adaptation for Time series Transformers using One-step  fine-tuning</h3>
<p>Subina Khanal,Seshu Tirupathi,Giulio Zizzo,Ambrish Rawat,Torben Bach Pedersen</p>
<p><a href='http://arxiv.org/abs/2401.06524v1'>http://arxiv.org/abs/2401.06524v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a one-step fine-tuning method to improve Transformers' performance in time series prediction for domains with limited data by incorporating source domain data gradually and using gradual unfreezing.</p><hr><h3>Exploring Diverse Representations for Open Set Recognition</h3>
<p>Yu Wang,Junxian Mu,Pengfei Zhu,Qinghua Hu</p>
<p><a href='http://arxiv.org/abs/2401.06521v1'>http://arxiv.org/abs/2401.06521v1</a></p>
<p><b>Compressor summary</b>: MEDAF is a discriminative model that learns diverse representations to improve open set recognition and outperforms generative models with less computational cost.</p><hr><h3>Personalized Reinforcement Learning with a Budget of Policies</h3>
<p>Dmitry Ivanov,Omer Ben-Porat</p>
<p><a href='http://arxiv.org/abs/2401.06514v1'>http://arxiv.org/abs/2401.06514v1</a></p>
<p><b>Compressor summary</b>: The paper proposes r-MDPs, a framework for balancing personalization and regulatory constraints in high-stakes fields like healthcare using deep reinforcement learning algorithms inspired by K-means clustering.</p><hr><h3>AntEval: Quantitatively Evaluating Informativeness and Expressiveness of  Agent Social Interactions</h3>
<p>Yuanzhi Liang,Linchao Zhu,Yi Yang</p>
<p><a href='http://arxiv.org/abs/2401.06509v1'>http://arxiv.org/abs/2401.06509v1</a></p>
<p><b>Compressor summary</b>: The authors propose a virtual setting using tabletop role-playing games to foster complex, context-rich interactions among agents and introduce AntEval, a framework to evaluate the informativeness and expressiveness of these interactions using novel metrics.</p><hr><h3>Frequency Masking for Universal Deepfake Detection</h3>
<p>Chandler Timm Doloriel,Ngai-Man Cheung</p>
<p><a href='http://arxiv.org/abs/2401.06506v1'>http://arxiv.org/abs/2401.06506v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel deepfake detector using frequency masking in the self-supervised pre-training phase, which outperforms existing methods.</p><hr><h3>Improving the Detection of Small Oriented Objects in Aerial Images</h3>
<p>Chandler Timm C. Doloriel,Rhandley D. Cajote</p>
<p><a href='http://arxiv.org/abs/2401.06503v1'>http://arxiv.org/abs/2401.06503v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an Attention-Points Network that improves the detection of small oriented objects in aerial images using two losses: Guided-Attention Loss and Box-Points Loss.</p><hr><h3>An investigation of structures responsible for gender bias in BERT and  DistilBERT</h3>
<p>Thibaud Leteno,Antoine Gourru,Charlotte Laclau,Christophe Gravier</p>
<p><a href='http://arxiv.org/abs/2401.06495v1'>http://arxiv.org/abs/2401.06495v1</a></p>
<p><b>Compressor summary</b>: The paper investigates gender bias in BERT and DistilBERT, finding that bias is uniformly encoded by every attention head except a few in underrepresented classes, and that distillation may increase bias.</p><hr><h3>Kun: Answer Polishment for Chinese Self-Alignment with Instruction  Back-Translation</h3>
<p>Tianyu Zheng,Shuyue Guo,Xingwei Qu,Jiawei Guo,Weixu Zhang,Xinrun Du,Chenghua Lin,Wenhao Huang,Wenhu Chen,Jie Fu,Ge Zhang</p>
<p><a href='http://arxiv.org/abs/2401.06477v1'>http://arxiv.org/abs/2401.06477v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Kun is a novel approach to create instruction-tuning datasets for LLMs without manual annotations
- It uses self-training, back-translation, and answer polishing with diverse unlabelled data sources
- It improves data retention, clarity, and reduces manual annotation costs
- It shows robustness and scalability on Yi model across various benchmarks
- It has implications for LLM applications in diverse fields

Summary:
Kun is a new method that generates instruction-tuning datasets for large language models using self-training, back-translation, and answer polishing, without manual annotations. It enhances data quality, reduces costs, and improves LLM performance across different tasks.</p><hr><h3>Self-supervised Learning of Dense Hierarchical Representations for  Medical Image Segmentation</h3>
<p>Eytan Kats,Jochen G. Hirsch,Mattias P. Heinrich</p>
<p><a href='http://arxiv.org/abs/2401.06473v1'>http://arxiv.org/abs/2401.06473v1</a></p>
<p><b>Compressor summary</b>: The paper presents a self-supervised framework for learning voxel-wise coarse-to-fine representations that balances global and local features, improves downstream tasks with limited annotations, and outperforms baselines.</p><hr><h3>A Brain-inspired Computational Model for Human-like Concept Learning</h3>
<p>Yuwei Wang,Yi Zeng</p>
<p><a href='http://arxiv.org/abs/2401.06471v1'>http://arxiv.org/abs/2401.06471v1</a></p>
<p><b>Compressor summary</b>: The study develops a computational model for concept learning based on spiking neural networks that mimic human brain mechanisms and achieves human-like concept representations.</p><hr><h3>Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning</h3>
<p>Kaiyi Zhang,Ang Lv,Yuhan Chen,Hansen Ha,Tao Xu,Rui Yan</p>
<p><a href='http://arxiv.org/abs/2401.06469v1'>http://arxiv.org/abs/2401.06469v1</a></p>
<p><b>Compressor summary</b>: Batch-ICL is an efficient and order-agnostic inference algorithm for in-context learning that consistently outperforms most example sequences and reduces computational resources.</p><hr><h3>Adapting Large Language Models for Document-Level Machine Translation</h3>
<p>Minghao Wu,Thuy-Trang Vu,Lizhen Qu,George Foster,Gholamreza Haffari</p>
<p><a href='http://arxiv.org/abs/2401.06468v1'>http://arxiv.org/abs/2401.06468v1</a></p>
<p><b>Compressor summary</b>: This paper investigates how to adapt large language models for document-level machine translation, finding that some specialized models outperform GPT-4 while others struggle with off-target translations.</p><hr><h3>PersianMind: A Cross-Lingual Persian-English Large Language Model</h3>
<p>Pedram Rostami,Ali Salemi,Mohammad Javad Dousti</p>
<p><a href='http://arxiv.org/abs/2401.06466v1'>http://arxiv.org/abs/2401.06466v1</a></p>
<p><b>Compressor summary</b>: PersianMind is an open-source bilingual large language model that performs similarly to GPT-3.5-turbo in Persian by expanding LLaMa2's vocabulary and training it on a large Persian dataset.</p><hr><h3>Sanity Checks Revisited: An Exploration to Repair the Model Parameter  Randomisation Test</h3>
<p>Anna Hedström,Leander Weber,Sebastian Lapuschkin,Marina MC Höhne</p>
<p><a href='http://arxiv.org/abs/2401.06465v1'>http://arxiv.org/abs/2401.06465v1</a></p>
<p><b>Compressor summary</b>: Smooth MPRT and Efficient MPRT are new adaptations of the Model Parameter Randomisation Test that address methodological caveats and improve reliability in eXplainable Artificial Intelligence evaluations.</p><hr><h3>AttributionScanner: A Visual Analytics System for Metadata-Free  Data-Slicing Based Model Validation</h3>
<p>Xiwei Xuan,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren</p>
<p><a href='http://arxiv.org/abs/2401.06462v1'>http://arxiv.org/abs/2401.06462v1</a></p>
<p><b>Compressor summary</b>: AttributionScanner is a Visual Analytics system that helps evaluate machine learning models on images by finding interpretable subgroups with explainable features and enabling users to fix model issues using neural network regularization.</p><hr><h3>Automated Machine Learning for Positive-Unlabelled Learning</h3>
<p>Jack D. Saunders,Alex A. Freitas</p>
<p><a href='http://arxiv.org/abs/2401.06452v1'>http://arxiv.org/abs/2401.06452v1</a></p>
<p><b>Compressor summary</b>: The text introduces two new Automated Machine Learning systems for Positive-Unlabelled learning and evaluates them along with a previous system and other methods on various datasets.</p><hr><h3>BOK-VQA: Bilingual Outside Knowledge-based Visual Question Answering via  Graph Representation Pretraining</h3>
<p>Minjun Kim,Seungwoo Song,Youhan Lee,Haneol Jang,Kyungtae Lim</p>
<p><a href='http://arxiv.org/abs/2401.06443v1'>http://arxiv.org/abs/2401.06443v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new bilingual dataset for visual question answering and proposes a framework to inject external knowledge into the system using graph embeddings.</p><hr><h3>RotationDrag: Point-based Image Editing with Rotated Diffusion Features</h3>
<p>Minxing Luo,Wentao Cheng,Jian Yang</p>
<p><a href='http://arxiv.org/abs/2401.06442v1'>http://arxiv.org/abs/2401.06442v1</a></p>
<p><b>Compressor summary</b>: This paper introduces RotationDrag, a novel point-based image editing method that improves in-plane rotation accuracy by using feature maps of rotated images, and presents RotateBench, the first benchmark to evaluate this task on real and generated images.</p><hr><h3>Improving Low-Light Image Recognition Performance Based on  Image-adaptive Learnable Module</h3>
<p>Seitaro Ono,Yuka Ogino,Takahiro Toizumi,Atsushi Ito,Masato Tsukada</p>
<p><a href='http://arxiv.org/abs/2401.06438v1'>http://arxiv.org/abs/2401.06438v1</a></p>
<p><b>Compressor summary</b>: The study presents a method to improve image recognition in low-light conditions by applying an adaptive image processing module and forecasting optimal parameters for it.</p><hr><h3>Improving Graph Convolutional Networks with Transformer Layer in  social-based items recommendation</h3>
<p>Thi Linh Hoang,Tuan Dung Pham,Viet Cuong Ta</p>
<p><a href='http://arxiv.org/abs/2401.06436v1'>http://arxiv.org/abs/2401.06436v1</a></p>
<p><b>Compressor summary</b>: The paper presents a transformer-based GCN model with an improved encoder for node embedding and attention mechanism, which outperforms GCN in predicting social network ratings.</p><hr><h3>From Automation to Augmentation: Large Language Models Elevating Essay  Scoring Landscape</h3>
<p>Changrong Xiao,Wenxing Ma,Sean Xin Xu,Kunpeng Zhang,Yufang Wang,Qi Fu</p>
<p><a href='http://arxiv.org/abs/2401.06431v1'>http://arxiv.org/abs/2401.06431v1</a></p>
<p><b>Compressor summary</b>: The study shows that large language models like GPT-4 and fine-tuned GPT-3.5 can significantly improve automated essay scoring for second-language learners by providing accurate, consistent, generalizable, and interpretable feedback, as well as assisting human graders to perform better.</p><hr><h3>Mutual Distillation Learning For Person Re-Identification</h3>
<p>Huiyuan Fu,Kuilong Cui,Chuanming Wang,Mengshi Qi,Huadong Ma</p>
<p><a href='http://arxiv.org/abs/2401.06430v1'>http://arxiv.org/abs/2401.06430v1</a></p>
<p><b>Compressor summary</b>: The paper proposes MDPR, a novel approach for person re-identification that extracts features from multiple perspectives using mutual distillation and fusion, achieving state-of-the-art results on widely used datasets.</p><hr><h3>UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer</h3>
<p>Ji Liu,Dehua Tang,Yuanxian Huang,Li Zhang,Xiaocheng Zeng,Dong Li,Mingjie Lu,Jinzhang Peng,Yu Wang,Fan Jiang,Lu Tian,Ashish Sirasao</p>
<p><a href='http://arxiv.org/abs/2401.06426v1'>http://arxiv.org/abs/2401.06426v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Traditional channel-wise pruning methods struggle to prune efficient CNN models with depth-wise convolutions and inverted residual blocks.
- The paper proposes a novel depth pruning method that uses a block pruning strategy and progressive training for the subnet, and works on vision transformer models as well.
- The method outperforms existing depth pruning methods and achieves state-of-the-art results on efficiency and performance.

Summary:
The paper presents a new depth pruning method that improves the efficiency and performance of efficient CNN models with depth-wise convolutions and inverted residual blocks, as well as vision transformer models.</p><hr><h3>Uncertainty quantification for probabilistic machine learning in earth  observation using conformal prediction</h3>
<p>Geethen Singh,Glenn Moncrieff,Zander Venter,Kerry Cawse-Nicholson,Jasper Slingsby,Tamara B Robinson</p>
<p><a href='http://arxiv.org/abs/2401.06421v1'>http://arxiv.org/abs/2401.06421v1</a></p>
<p><b>Compressor summary</b>: Conformal prediction is a model-agnostic framework for uncertainty quantification that can improve the reliability of AI systems for Earth Observation applications without requiring access to the underlying model or training dataset.</p><hr><h3>Mission: Impossible Language Models</h3>
<p>Julie Kallini,Isabel Papadimitriou,Richard Futrell,Kyle Mahowald,Christopher Potts</p>
<p><a href='http://arxiv.org/abs/2401.06416v1'>http://arxiv.org/abs/2401.06416v1</a></p>
<p><b>Compressor summary</b>: The study tests GPT-2's ability to learn synthetic impossible languages and finds that it struggles, challenging the claim that LLMs can learn any possible language.</p><hr><h3>3D Reconstruction of Interacting Multi-Person in Clothing from a Single  Image</h3>
<p>Junuk Cha,Hansol Lee,Jaewon Kim,Nhat Nguyen Bao Truong,Jae Shin Yoon,Seungryul Baek</p>
<p><a href='http://arxiv.org/abs/2401.06415v1'>http://arxiv.org/abs/2401.06415v1</a></p>
<p><b>Compressor summary</b>: The paper presents a novel pipeline to reconstruct 3D geometry of interacting people in clothing from a single image using priors for complete geometry and surface contacts, overcoming occlusion challenges.</p><hr><h3>AboutMe: Using Self-Descriptions in Webpages to Document the Effects of  English Pretraining Data Filters</h3>
<p>Li Lucy,Suchin Gururangan,Luca Soldaini,Emma Strubell,David Bamman,Lauren Klein,Jesse Dodge</p>
<p><a href='http://arxiv.org/abs/2401.06408v1'>http://arxiv.org/abs/2401.06408v1</a></p>
<p><b>Compressor summary</b>: The authors examine how different quality and language identification filters affect web text pretraining data, revealing implicit biases in data curation based on social and geographic contexts.</p><hr><h3>Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis:  A review</h3>
<p>Lingchao Mao,Hairong Wang,Leland S. Hu,Nhan L Tran,Peter D Canoll,Kristin R Swanson,Jing Li</p>
<p><a href='http://arxiv.org/abs/2401.06406v1'>http://arxiv.org/abs/2401.06406v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Machine learning can analyze multi-omics profiles and medical imaging for cancer diagnosis and prognosis
- Machine learning models face challenges such as limited labeled samples, high-dimensionality data types, heterogeneity, and interpretability
- Knowledge-informed machine learning integrates biomedical knowledge into data-driven models to improve accuracy, robustness, and interpretability
- The paper reviews different forms of knowledge representation and integration strategies for four primary data types
- The paper discusses future directions to advance cancer research through knowledge-informed machine learning

Summary:
The paper reviews how knowledge-informed machine learning, which integrates biomedical knowledge into data-driven models, can overcome challenges in cancer diagnosis and prognosis using multi-omics profiles and medical imaging.</p><hr><h3>Generalizing Visual Question Answering from Synthetic to Human-Written  Questions via a Chain of QA with a Large Language Model</h3>
<p>Taehee Kim,Yeongjae Cho,Heejun Shin,Yohan Jo,Dongmyung Shin</p>
<p><a href='http://arxiv.org/abs/2401.06400v1'>http://arxiv.org/abs/2401.06400v1</a></p>
<p><b>Compressor summary</b>: CoQAH is a new method that uses a sequence of QA interactions between a language model and a VQA model to answer human-written questions for images, achieving state-of-the-art accuracy without finetuning.</p><hr><h3>An approach for mistranslation removal from popular dataset for Indic MT  Task</h3>
<p>Sudhansu Bala Das,Leo Raphael Rodrigues,Tapas Kumar Mishra,Bidyut Kr. Patra</p>
<p><a href='http://arxiv.org/abs/2401.06398v1'>http://arxiv.org/abs/2401.06398v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an algorithm to remove mistranslations from a parallel dataset for Indian languages and evaluates its impact on neural machine translation quality.</p><hr><h3>UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World  Understanding</h3>
<p>Bowen Shi,Peisen Zhao,Zichen Wang,Yuhang Zhang,Yaoming Wang,Jin Li,Wenrui Dai,Junni Zou,Hongkai Xiong,Qi Tian,Xiaopeng Zhang</p>
<p><a href='http://arxiv.org/abs/2401.06397v1'>http://arxiv.org/abs/2401.06397v1</a></p>
<p><b>Compressor summary</b>: UMG-CLIP is a new model that improves vision-language understanding by aligning local image regions with text tokens at different levels of detail, achieving state-of-the-art performance on various tasks.</p><hr><h3>ModaVerse: Efficiently Transforming Modalities with LLMs</h3>
<p>Xinyu Wang,Bohan Zhuang,Qi Wu</p>
<p><a href='http://arxiv.org/abs/2401.06395v1'>http://arxiv.org/abs/2401.06395v1</a></p>
<p><b>Compressor summary</b>: ModaVerse is a new multi-modal language model that can understand and transform images, videos, and audio using natural language without complex latent feature alignments, making it more efficient and cost-effective.</p><hr><h3>Adaptive Data Augmentation for Aspect Sentiment Quad Prediction</h3>
<p>Wenyuan Zhang,Xinghua Zhang,Shiyao Cui,Kun Huang,Xuebin Wang,Tingwen Liu</p>
<p><a href='http://arxiv.org/abs/2401.06394v1'>http://arxiv.org/abs/2401.06394v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an Adaptive Data Augmentation framework to address data imbalance issues in aspect-based sentiment analysis by enhancing tail quad patterns and aspect categories.</p><hr><h3>SD-MVS: Segmentation-Driven Deformation Multi-View Stereo with Spherical  Refinement and EM optimization</h3>
<p>Zhenlong Yuan,Jiakai Cao,Zhaoxin Li,Hao Jiang,Zhaoqi Wang</p>
<p><a href='http://arxiv.org/abs/2401.06385v1'>http://arxiv.org/abs/2401.06385v1</a></p>
<p><b>Compressor summary</b>: SD-MVS is a new method that uses semantic segmentation, pixel deformation, and refinement techniques to reconstruct 3D models of textureless areas with high quality and efficiency.</p><hr><h3>Vehicle: Bridging the Embedding Gap in the Verification of  Neuro-Symbolic Programs</h3>
<p>Matthew L. Daggitt,Wen Kokke,Robert Atkey,Natalia Slusarz,Luca Arnaboldi,Ekaterina Komendantskaya</p>
<p><a href='http://arxiv.org/abs/2401.06379v1'>http://arxiv.org/abs/2401.06379v1</a></p>
<p><b>Compressor summary</b>: Vehicle is a tool that helps verify neural-symbolic programs by linking problem-space properties to embedding-space properties, enabling formal verification of a simple self-driving car example.</p><hr><h3>Cognitive BPM as an Equalizer: Improving Access and Efficiency for  Employees with (and without) Cognitive Disabilities</h3>
<p>Gordon Banks,Gates Bierhuizen,Katherine McCrum,Ellen Wengert</p>
<p><a href='http://arxiv.org/abs/2401.06375v1'>http://arxiv.org/abs/2401.06375v1</a></p>
<p><b>Compressor summary</b>: ProcessGPT is an AI model that helps design better business processes considering human cognitive limitations, benefiting both people with and without disabilities.</p><hr><h3>SamLP: A Customized Segment Anything Model for License Plate Detection</h3>
<p>Haoxuan Ding,Junyu Gao,Yuan Yuan,Qi Wang</p>
<p><a href='http://arxiv.org/abs/2401.06374v1'>http://arxiv.org/abs/2401.06374v1</a></p>
<p><b>Compressor summary</b>: The paper introduces SamLP, a license plate detector based on a vision foundation model (SAM) that leverages few-shot and zero-shot learning for diverse LP styles and appearances.</p><hr><h3>Graph Relation Distillation for Efficient Biomedical Instance  Segmentation</h3>
<p>Xiaoyu Liu,Yueyi Zhang,Zhiwei Xiong,Wei Huang,Bo Hu,Xiaoyan Sun,Feng Wu</p>
<p><a href='http://arxiv.org/abs/2401.06370v1'>http://arxiv.org/abs/2401.06370v1</a></p>
<p><b>Compressor summary</b>: The text proposes a graph relation distillation method for efficient biomedical instance segmentation that transfers knowledge from heavy to lightweight networks using instance and pixel relation graphs, achieving high performance with significantly reduced parameters and inference time.</p><hr><h3>An Empirical Investigation into the Effect of Parameter Choices in  Knowledge Distillation</h3>
<p>Md Arafat Sultan,Aashka Trivedi,Parul Awasthy,Avirup Sil</p>
<p><a href='http://arxiv.org/abs/2401.06356v1'>http://arxiv.org/abs/2401.06356v1</a></p>
<p><b>Compressor summary</b>: This study examines how different configuration parameters in knowledge distillation affect student performance, identifying an optimal configuration for various natural language processing tasks and student sizes.</p><hr><h3>Seek for Incantations: Towards Accurate Text-to-Image Diffusion  Synthesis through Prompt Engineering</h3>
<p>Chang Yu,Junran Peng,Xiangyu Zhu,Zhaoxiang Zhang,Qi Tian,Zhen Lei</p>
<p><a href='http://arxiv.org/abs/2401.06345v1'>http://arxiv.org/abs/2401.06345v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve diffusion models' image generation by learning proper textual descriptions using quality and semantic guidance from pre-trained models.</p><hr><h3>Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for  Human Trajectory Prediction with Hypergraph Reasoning</h3>
<p>Weizheng Wang,Le Mao,Baijian Yang,Guohua Chen,Byung-Cheol Min</p>
<p><a href='http://arxiv.org/abs/2401.06344v1'>http://arxiv.org/abs/2401.06344v1</a></p>
<p><b>Compressor summary</b>: Hyper-STTN is a hypergraph-based method for predicting crowd trajectories that captures both pair-wise and group-wise interactions using spectral convolution and multimodal transformers.</p><hr><h3>AffordanceLLM: Grounding Affordance from Vision Language Models</h3>
<p>Shengyi Qian,Weifeng Chen,Min Bai,Xiong Zhou,Zhuowen Tu,Li Erran Li</p>
<p><a href='http://arxiv.org/abs/2401.06341v1'>http://arxiv.org/abs/2401.06341v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a model that leverages large-scale vision language models to improve affordance grounding tasks, achieving better performance on in-the-wild object affordance grounding and handling unseen objects and actions.</p><hr><h3>Application Of Vision-Language Models For Assessing Osteoarthritis  Disease Severity</h3>
<p>Banafshe Felfeliyan,Yuyue Zhou,Shrimanti Ghosh,Jessica Kupper,Shaobo Liu,Abhilash Hareendranathan,Jacob L. Jaremko</p>
<p><a href='http://arxiv.org/abs/2401.06331v1'>http://arxiv.org/abs/2401.06331v1</a></p>
<p><b>Compressor summary</b>: The study explores using Vision Language Processing models to predict osteoarthritis severity from X-ray images and reports, potentially improving diagnosis and paving the way for specialized AI in medicine.</p><hr><h3>Learning from Semi-Factuals: A Debiased and Semantic-Aware Framework for  Generalized Relation Discovery</h3>
<p>Jiaxin Wang,Lingling Zhang,Jun Liu,Tianlin Guo,Wenjun Wu</p>
<p><a href='http://arxiv.org/abs/2401.06327v1'>http://arxiv.org/abs/2401.06327v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new task called GRD that involves finding novel relations or clustering instances in existing relations using semi-factual examples and proposes a framework (SFGRD) that outperforms current models.</p><hr><h3>Multi-Task Learning for Front-End Text Processing in TTS</h3>
<p>Wonjune Kang,Yun Wang,Shun Zhang,Arthur Hinsvark,Qing He</p>
<p><a href='http://arxiv.org/abs/2401.06321v1'>http://arxiv.org/abs/2401.06321v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a multi-task learning model for text-to-speech tasks that uses shared representations and pre-trained language embeddings to improve text normalization, part-of-speech tagging, and homograph disambiguation performance.</p><hr><h3>Striking a Balance in Fairness for Dynamic Systems Through Reinforcement  Learning</h3>
<p>Yaowei Hu,Jacob Lear,Lu Zhang</p>
<p><a href='http://arxiv.org/abs/2401.06318v1'>http://arxiv.org/abs/2401.06318v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an algorithmic framework to integrate fairness and reinforcement learning in dynamic systems with sequential decisions.</p><hr><h3>Video Super-Resolution Transformer with Masked Inter&Intra-Frame  Attention</h3>
<p>Xingyu Zhou,Leheng Zhang,Xiaorui Zhao,Keze Wang,Leida Li,Shuhang Gu</p>
<p><a href='http://arxiv.org/abs/2401.06312v1'>http://arxiv.org/abs/2401.06312v1</a></p>
<p><b>Compressor summary</b>: The paper proposes MIA-VSR, a feature-level masked processing framework for video super-resolution that reduces redundant computations and improves memory and computation efficiency.</p><hr><h3>Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in  Text-to-Image Generation</h3>
<p>Akshita Jha,Vinodkumar Prabhakaran,Remi Denton,Sarah Laszlo,Shachi Dave,Rida Qadri,Chandan K. Reddy,Sunipa Dev</p>
<p><a href='http://arxiv.org/abs/2401.06310v1'>http://arxiv.org/abs/2401.06310v1</a></p>
<p><b>Compressor summary</b>: This study evaluates visual stereotypes in Text-to-Image models for 135 nationality-based identity groups and finds that they are often present, offensive, and similar across different attributes.</p><hr><h3>Misconfidence-based Demonstration Selection for LLM In-Context Learning</h3>
<p>Shangqing Xu,Chao Zhang</p>
<p><a href='http://arxiv.org/abs/2401.06301v1'>http://arxiv.org/abs/2401.06301v1</a></p>
<p><b>Compressor summary</b>: In-Context Reflection (ICR) is a method that selects and refines demonstrations for large language models to improve their learning efficiency and generalization across tasks.</p>