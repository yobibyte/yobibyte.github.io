
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2023-12-05</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2023-12-05 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness</h3>
<p>Anh-Quan Cao,Angela Dai,Raoul de Charette</p>
<p><a href='http://arxiv.org/abs/2312.02158v1'>http://arxiv.org/abs/2312.02158v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new task called Panoptic Scene Completion (PSC) that adds instance-level information to the Semantic Scene Completion (SSC) task, and introduces a method to estimate uncertainties using a multi-input multi-output strategy.</p><hr><h3>Mesh-Guided Neural Implicit Field Editing</h3>
<p>Can Wang,Mingming He,Menglei Chai,Dongdong Chen,Jing Liao</p>
<p><a href='http://arxiv.org/abs/2312.02157v1'>http://arxiv.org/abs/2312.02157v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new approach that combines mesh guiding and neural implicit fields for easy editing of 3D scenes while maintaining high-quality rendering.</p><hr><h3>GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for  Real-time Human Novel View Synthesis</h3>
<p>Shunyuan Zheng,Boyao Zhou,Ruizhi Shao,Boning Liu,Shengping Zhang,Liqiang Nie,Yebin Liu</p>
<p><a href='http://arxiv.org/abs/2312.02155v1'>http://arxiv.org/abs/2312.02155v1</a></p>
<p><b>Compressor summary</b>: The GPS-Gaussian method allows real-time, high-resolution view synthesis of characters without fine-tuning or optimization by using Gaussian parameter maps and training on human scan data.</p><hr><h3>Latent Feature-Guided Diffusion Models for Shadow Removal</h3>
<p>Kangfu Mei,Luis Figueroa,Zhe Lin,Zhihong Ding,Scott Cohen,Vishal M. Patel</p>
<p><a href='http://arxiv.org/abs/2312.02156v1'>http://arxiv.org/abs/2312.02156v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using diffusion models to gradually refine shadow regions and improve texture recovery, conditioning on a learned latent feature space and fusing noise features with the network, achieving significant improvements over previous methods.</p><hr><h3>Aligning and Prompting Everything All at Once for Universal Visual  Perception</h3>
<p>Yunhang Shen,Chaoyou Fu,Peixian Chen,Mengdan Zhang,Ke Li,Xing Sun,Yunsheng Wu,Shaohui Lin,Rongrong Ji</p>
<p><a href='http://arxiv.org/abs/2312.02153v1'>http://arxiv.org/abs/2312.02153v1</a></p>
<p><b>Compressor summary</b>: The paper introduces APE, a universal visual perception model that performs diverse tasks like detection, segmentation, and grounding in images using instance-level sentence-object matching and without task-specific fine-tuning.</p><hr><h3>Steerers: A framework for rotation equivariant keypoint descriptors</h3>
<p>Georg Bökman,Johan Edstedt,Michael Felsberg,Fredrik Kahl</p>
<p><a href='http://arxiv.org/abs/2312.02152v1'>http://arxiv.org/abs/2312.02152v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to learn a linear transform called a steerer that encodes rotations in image keypoint descriptions, improving their robustness to camera rotation without sacrificing performance or runtime.</p><hr><h3>Guarding Barlow Twins Against Overfitting with Mixed Samples</h3>
<p>Wele Gedara Chaminda Bandara,Celso M. De Melo,Vishal M. Patel</p>
<p><a href='http://arxiv.org/abs/2312.02151v1'>http://arxiv.org/abs/2312.02151v1</a></p>
<p><b>Compressor summary</b>: Mixed Barlow Twins is a method to improve self-supervised learning by enhancing sample interaction and reducing feature overfitting, leading to better downstream task performance.</p><hr><h3>Readout Guidance: Learning Control from Diffusion Features</h3>
<p>Grace Luo,Trevor Darrell,Oliver Wang,Dan B Goldman,Aleksander Holynski</p>
<p><a href='http://arxiv.org/abs/2312.02150v1'>http://arxiv.org/abs/2312.02150v1</a></p>
<p><b>Compressor summary</b>: Readout Guidance is a method that uses lightweight networks to guide text-to-image diffusion models with user-defined targets, requiring fewer parameters and training samples than prior methods.</p><hr><h3>Generative Powers of Ten</h3>
<p>Xiaojuan Wang,Janne Kontkanen,Brian Curless,Steve Seitz,Ira Kemelmacher,Ben Mildenhall,Pratul Srinivasan,Dor Verbin,Aleksander Holynski</p>
<p><a href='http://arxiv.org/abs/2312.02149v1'>http://arxiv.org/abs/2312.02149v1</a></p>
<p><b>Compressor summary</b>: The text describes a method for creating images that can be zoomed in on extensively while maintaining consistency across different scales using a joint multi-scale diffusion sampling approach.</p><hr><h3>Rejuvenating image-GPT as Strong Visual Representation Learners</h3>
<p>Sucheng Ren,Zeyu Wang,Hongru Zhu,Junfei Xiao,Alan Yuille,Cihang Xie</p>
<p><a href='http://arxiv.org/abs/2312.02147v1'>http://arxiv.org/abs/2312.02147v1</a></p>
<p><b>Compressor summary</b>: The paper presents D-iGPT, a modified version of image-GPT that uses semantic tokens and predicts both visible and hidden tokens, achieving strong visual representation learning results on ImageNet-1K and other tasks.</p><hr><h3>Learning Polynomial Problems with $SL(2,\mathbb{R})$ Equivariance</h3>
<p>Hannah Lawrence,Mitchell Tong Harris</p>
<p><a href='http://arxiv.org/abs/2312.02146v1'>http://arxiv.org/abs/2312.02146v1</a></p>
<p><b>Compressor summary</b>: Neural networks can solve positivity optimization and certification problems for polynomials faster and accurately, using data-driven methods and adapting to non-compact group equivariant structures.</p><hr><h3>Repurposing Diffusion-Based Image Generators for Monocular Depth  Estimation</h3>
<p>Bingxin Ke,Anton Obukhov,Shengyu Huang,Nando Metzger,Rodrigo Caye Daudt,Konrad Schindler</p>
<p><a href='http://arxiv.org/abs/2312.02145v1'>http://arxiv.org/abs/2312.02145v1</a></p>
<p><b>Compressor summary</b>: Marigold is a method for monocular depth estimation that uses generative diffusion models to improve generalization and achieve state-of-the-art performance with synthetic training data.</p><hr><h3>Optimizing Camera Configurations for Multi-View Pedestrian Detection</h3>
<p>Yunzhong Hou,Xingjian Leng,Tom Gedeon,Liang Zheng</p>
<p><a href='http://arxiv.org/abs/2312.02144v1'>http://arxiv.org/abs/2312.02144v1</a></p>
<p><b>Compressor summary</b>: The text describes a novel solution that uses reinforcement learning and transformers to autonomously generate camera configurations for multi-view pedestrian detection systems, achieving better results than existing methods.</p><hr><h3>Competition-Level Problems Are Effective Evaluators of LLMs</h3>
<p>Yiming Huang,Zhenghao Lin,Xiao Liu,Yeyun Gong,Shuai Lu,Fangyu Lei,Yaobo Liang,Yelong Shen,Chen Lin,Nan Duan,Weizhu Chen</p>
<p><a href='http://arxiv.org/abs/2312.02143v1'>http://arxiv.org/abs/2312.02143v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates GPT-4's reasoning skills on Codeforces problems, finding a decline in performance after September 2021, suggesting data contamination and challenges for existing LLMs to solve complex reasoning tasks.</p><hr><h3>Object Recognition as Next Token Prediction</h3>
<p>Kaiyu Yue,Bor-Chun Chen,Jonas Geiping,Hengduo Li,Tom Goldstein,Ser-Nam Lim</p>
<p><a href='http://arxiv.org/abs/2312.02142v1'>http://arxiv.org/abs/2312.02142v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an efficient object recognition method using a language decoder to predict text tokens from image embeddings, with a custom attention mask and one-shot sampling for parallel label generation.</p><hr><h3>iMatching: Imperative Correspondence Learning</h3>
<p>Zitong Zhan,Dasong Gao,Yun-Jou Lin,Youjie Xia,Chen Wang</p>
<p><a href='http://arxiv.org/abs/2312.02141v1'>http://arxiv.org/abs/2312.02141v1</a></p>
<p><b>Compressor summary</b>: The paper introduces imperative learning (IL), a self-supervised scheme for training feature correspondence in computer vision, which improves performance on tasks like feature matching and pose estimation.</p><hr><h3>DiffiT: Diffusion Vision Transformers for Image Generation</h3>
<p>Ali Hatamizadeh,Jiaming Song,Guilin Liu,Jan Kautz,Arash Vahdat</p>
<p><a href='http://arxiv.org/abs/2312.02139v1'>http://arxiv.org/abs/2312.02139v1</a></p>
<p><b>Compressor summary</b>: This paper proposes Diffusion Vision Transformers (DiffiT), a hybrid hierarchical architecture with a U-shaped encoder and decoder, that uses time-dependent self-attention for efficient denoising in diffusion-based generative learning, achieving state-of-the-art results on various image synthesis tasks.</p><hr><h3>MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D  Gaussians</h3>
<p>Chandradeep Pokhariya,Ishaan N Shah,Angela Xing,Zekun Li,Kefan Chen,Avinash Sharma,Srinath Sridhar</p>
<p><a href='http://arxiv.org/abs/2312.02137v1'>http://arxiv.org/abs/2312.02137v1</a></p>
<p><b>Compressor summary</b>: MANUs is a novel method for capturing hand-object grasps using articulated 3D Gaussians, which enables accurate estimation of contacts between hands and objects, and requires tens of camera views.</p><hr><h3>BerfScene: Bev-conditioned Equivariant Radiance Fields for Infinite 3D  Scene Generation</h3>
<p>Qihang Zhang,Yinghao Xu,Yujun Shen,Bo Dai,Bolei Zhou,Ceyuan Yang</p>
<p><a href='http://arxiv.org/abs/2312.02136v1'>http://arxiv.org/abs/2312.02136v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new way to generate large-scale 3D scenes using an equivariant radiance field and a bird's-eye view map, which allows for easy manipulation of objects and smooth stitching of local scenes.</p><hr><h3>Fast View Synthesis of Casual Videos</h3>
<p>Yao-Chih Lee,Zhoutong Zhang,Kevin Blackburn-Matzen,Simon Niklaus,Jianming Zhang,Jia-Bin Huang,Feng Liu</p>
<p><a href='http://arxiv.org/abs/2312.02135v1'>http://arxiv.org/abs/2312.02135v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a fast and efficient method to synthesize novel views from monocular videos using explicit video representations, treating static and dynamic content separately.</p><hr><h3>GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single  Video via Animatable 3D Gaussians</h3>
<p>Liangxiao Hu,Hongwen Zhang,Yuxiang Zhang,Boyao Zhou,Boning Liu,Shengping Zhang,Liqiang Nie</p>
<p><a href='http://arxiv.org/abs/2312.02134v1'>http://arxiv.org/abs/2312.02134v1</a></p>
<p><b>Compressor summary</b>: GaussianAvatar creates realistic human avatars with dynamic 3D appearances from a single video using animatable 3D Gaussians and pose-dependent appearance modeling.</p><hr><h3>Style Aligned Image Generation via Shared Attention</h3>
<p>Amir Hertz,Andrey Voynov,Shlomi Fruchter,Daniel Cohen-Or</p>
<p><a href='http://arxiv.org/abs/2312.02133v1'>http://arxiv.org/abs/2312.02133v1</a></p>
<p><b>Compressor summary</b>: StyleAligned is a technique for maintaining consistent style across generated images using text-to-image models by sharing attention during the diffusion process.</p><hr><h3>Hot PATE: Private Aggregation of Distributions for Diverse Task</h3>
<p>Edith Cohen,Xin Lyu,Jelani Nelson,Tamas Sarlos,Uri Stemmer</p>
<p><a href='http://arxiv.org/abs/2312.02132v1'>http://arxiv.org/abs/2312.02132v1</a></p>
<p><b>Compressor summary</b>: Hot PATE is a method for transferring knowledge from multiple teacher models to a student model while preserving privacy, focusing on diverse tasks where there may not be a clear label for each example.</p><hr><h3>Can we truly transfer an actor's genuine happiness to avatars? An  investigation into virtual, real, posed and spontaneous faces</h3>
<p>Vitor Miguel Xavier Peres,Greice Pinho Dal Molin,Soraia Raupp Musse</p>
<p><a href='http://arxiv.org/abs/2312.02128v1'>http://arxiv.org/abs/2312.02128v1</a></p>
<p><b>Compressor summary</b>: The research evaluates Ekman's action units in real and virtual human faces, posed and spontaneous, to find differences and similarities that can help various fields of knowledge.</p><hr><h3>SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM</h3>
<p>Nikhil Keetha,Jay Karhade,Krishna Murthy Jatavallabhula,Gengshan Yang,Sebastian Scherer,Deva Ramanan,Jonathon Luiten</p>
<p><a href='http://arxiv.org/abs/2312.02126v1'>http://arxiv.org/abs/2312.02126v1</a></p>
<p><b>Compressor summary</b>: The paper introduces SplaTAM, a method that uses 3D Gaussians to enable dense SLAM with a single unposed monocular RGB-D camera, improving performance in pose estimation, map construction, and novel-view synthesis while allowing real-time rendering.</p><hr><h3>TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and  Advanced Decoding Techniques</h3>
<p>Amir Panahandeh,Hanie Asemi,Esmail Nourani</p>
<p><a href='http://arxiv.org/abs/2312.02125v1'>http://arxiv.org/abs/2312.02125v1</a></p>
<p><b>Compressor summary</b>: The study trains a Persian classical poetry generation model using a transformer architecture on a specialized dataset without pretraining, and proposes a novel decoding method to enhance coherence and meaningfulness in the generated poetry.</p><hr><h3>VerA: Versatile Anonymization Fit for Clinical Facial Images</h3>
<p>Majed El Helou,Doruk Cetin,Petar Stamenkovic,Fabio Zund</p>
<p><a href='http://arxiv.org/abs/2312.02124v1'>http://arxiv.org/abs/2312.02124v1</a></p>
<p><b>Compressor summary</b>: VerA is a versatile facial image anonymization method that preserves semantic areas for medical intervention and works well on before-and-after results, outperforming or matching existing methods in regular images.</p><hr><h3>Magicoder: Source Code Is All You Need</h3>
<p>Yuxiang Wei,Zhe Wang,Jiawei Liu,Yifeng Ding,Lingming Zhang</p>
<p><a href='http://arxiv.org/abs/2312.02120v1'>http://arxiv.org/abs/2312.02120v1</a></p>
<p><b>Compressor summary</b>: Magicoder is an open-source LLM for code that uses OSS-Instruct to generate diverse and realistic instruction data from open-source code snippets, resulting in high performance on coding benchmarks.</p><hr><h3>Tree of Attacks: Jailbreaking Black-Box LLMs Automatically</h3>
<p>Anay Mehrotra,Manolis Zampetakis,Paul Kassianik,Blaine Nelson,Hyrum Anderson,Yaron Singer,Amin Karbasi</p>
<p><a href='http://arxiv.org/abs/2312.02119v1'>http://arxiv.org/abs/2312.02119v1</a></p>
<p><b>Compressor summary</b>: The paper introduces TAP, an automated method that uses tree-of-thoughts reasoning to generate jailbreaks for LLMs with only black-box access and minimal queries, outperforming previous methods.</p><hr><h3>When it Rains, it Pours: Modeling Media Storms and the News Ecosystem</h3>
<p>Benjamin Litterer,David Jurgens,Dallas Card</p>
<p><a href='http://arxiv.org/abs/2312.02118v1'>http://arxiv.org/abs/2312.02118v1</a></p>
<p><b>Compressor summary</b>: The authors develop a method to identify media storms from online news articles and study their characteristics and effects on media coverage and agenda setting.</p><hr><h3>GIVT: Generative Infinite-Vocabulary Transformers</h3>
<p>Michael Tschannen,Cian Eastwood,Fabian Mentzer</p>
<p><a href='http://arxiv.org/abs/2312.02116v1'>http://arxiv.org/abs/2312.02116v1</a></p>
<p><b>Compressor summary</b>: The paper introduces GIVT, which are transformers that generate real-valued vector sequences from infinite vocabularies, and shows their applications in image generation and other tasks.</p><hr><h3>TriDeNT: Triple Deep Network Training for Privileged Knowledge  Distillation in Histopathology</h3>
<p>Lucas Farndale,Robert Insall,Ke Yuan</p>
<p><a href='http://arxiv.org/abs/2312.02111v1'>http://arxiv.org/abs/2312.02111v1</a></p>
<p><b>Compressor summary</b>: TriDeNT is a self-supervised method that uses additional data unavailable during inference to improve computational pathology models' performance on various tasks.</p><hr><h3>ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder  and Explicit Adaptation</h3>
<p>Dar-Yen Chen,Hamish Tennent,Ching-Wen Hsu</p>
<p><a href='http://arxiv.org/abs/2312.02109v1'>http://arxiv.org/abs/2312.02109v1</a></p>
<p><b>Compressor summary</b>: ArtAdapter is a new text-to-image framework that transfers high-level artistic elements from text to image with unprecedented fidelity and no content borrowing, outperforming existing methods.</p><hr><h3>Learning Pseudo-Labeler beyond Noun Concepts for Open-Vocabulary Object  Detection</h3>
<p>Sunghun Kang,Junbum Cha,Jonghwan Mun,Byungseok Roh,Chang D. Yoo</p>
<p><a href='http://arxiv.org/abs/2312.02103v1'>http://arxiv.org/abs/2312.02103v1</a></p>
<p><b>Compressor summary</b>: The paper presents PLAC, a method to learn image-to-text mapping for arbitrary concepts in open-vocabulary object detection.</p><hr><h3>Single-sample versus case-control sampling scheme for Positive Unlabeled  data: the story of two scenarios</h3>
<p>Jan Mielniczuk,Adam Wawrzeńczyk</p>
<p><a href='http://arxiv.org/abs/2312.02095v1'>http://arxiv.org/abs/2312.02095v1</a></p>
<p><b>Compressor summary</b>: The paper compares different classifiers for positive unlabeled data in case-control and single-sample scenarios and shows that their performance can vary significantly depending on the scenario.</p><hr><h3>Physics simulation capabilities of LLMs</h3>
<p>Mohamad Ali-Dib,Kristen Menou</p>
<p><a href='http://arxiv.org/abs/2312.02091v1'>http://arxiv.org/abs/2312.02091v1</a></p>
<p><b>Compressor summary</b>: Key points:
- LLMs can solve physics problems and code, but struggle with PhD-level computational physics problems
- The paper evaluates LLMs on 50 original problems in different domains using various packages
- GPT4 fails most problems, but produces mostly correct lines with some physics and coding errors

Summary:
The paper tests the ability of large language models to solve complex physics problems using code packages and finds that GPT4 fails most of them, but produces mostly correct lines with some physics and coding errors.</p><hr><h3>VideoSwap: Customized Video Subject Swapping with Interactive Semantic  Point Correspondence</h3>
<p>Yuchao Gu,Yipin Zhou,Bichen Wu,Licheng Yu,Jia-Wei Liu,Rui Zhao,Jay Zhangjie Wu,David Junhao Zhang,Mike Zheng Shou,Kevin Tang</p>
<p><a href='http://arxiv.org/abs/2312.02087v1'>http://arxiv.org/abs/2312.02087v1</a></p>
<p><b>Compressor summary</b>: The VideoSwap framework uses semantic point correspondences instead of dense correspondences for shape-preserving video subject swapping with user-friendly interactions.</p><hr><h3>Deep Set Neural Networks for forecasting asynchronous bioprocess  timeseries</h3>
<p>Maxim Borisyak,Stefan Born,Peter Neubauer,Nicolás Cruz-Bournazou</p>
<p><a href='http://arxiv.org/abs/2312.02079v1'>http://arxiv.org/abs/2312.02079v1</a></p>
<p><b>Compressor summary</b>: The authors propose a deep learning method that can handle sparse and irregular time series from bio-process data without needing imputation or alignment procedures, and demonstrate its effectiveness in forecasting tasks.</p><hr><h3>A Glitch in the Matrix? Locating and Detecting Language Model Grounding  with Fakepedia</h3>
<p>Giovanni Monea,Maxime Peyrard,Martin Josifoski,Vishrav Chaudhary,Jason Eisner,Emre Kıcıman,Hamid Palangi,Barun Patra,Robert West</p>
<p><a href='http://arxiv.org/abs/2312.02073v1'>http://arxiv.org/abs/2312.02073v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Fakepedia, a dataset to study how large language models ground their knowledge in contradictory situations, and analyzes the differences between GPT-4-turbo and Mistral-7B in this context.</p><hr><h3>GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</h3>
<p>Shenhan Qian,Tobias Kirschstein,Liam Schoneveld,Davide Davoli,Simon Giebenhain,Matthias Nießner</p>
<p><a href='http://arxiv.org/abs/2312.02069v1'>http://arxiv.org/abs/2312.02069v1</a></p>
<p><b>Compressor summary</b>: GaussianAvatars is a new method for creating realistic and controllable 3D head avatars using Gaussian splats and a parametric morphable face model.</p><hr><h3>Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?</h3>
<p>Donya Rooein,Amanda Cercas Curry,Dirk Hovy</p>
<p><a href='http://arxiv.org/abs/2312.02065v1'>http://arxiv.org/abs/2312.02065v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates the readability of answers generated by large language models (LLMs) for science questions targeting different age groups and education levels, finding that LLMs need improvement to better adapt to diverse audiences in educational settings.</p><hr><h3>DUCK: Distance-based Unlearning via Centroid Kinematics</h3>
<p>Marco Cotogni,Jacopo Bonato,Luigi Sabetta,Francesco Pelosin,Alessandro Nicolosi</p>
<p><a href='http://arxiv.org/abs/2312.02052v1'>http://arxiv.org/abs/2312.02052v1</a></p>
<p><b>Compressor summary</b>: DUCK is a new unlearning algorithm that uses metric learning to remove specific samples and achieve state-of-the-art performance in ensuring privacy in AI models.</p><hr><h3>TimeChat: A Time-sensitive Multimodal Large Language Model for Long  Video Understanding</h3>
<p>Shuhuai Ren,Linli Yao,Shicheng Li,Xu Sun,Lu Hou</p>
<p><a href='http://arxiv.org/abs/2312.02051v1'>http://arxiv.org/abs/2312.02051v1</a></p>
<p><b>Compressor summary</b>: TimeChat is a multimodal language model that can understand long videos by processing their visual and temporal information and following instructions from users.</p><hr><h3>GFS: Graph-based Feature Synthesis for Prediction over Relational  Databases</h3>
<p>Han Zhang,Quan Gan,David Wipf,Weinan Zhang</p>
<p><a href='http://arxiv.org/abs/2312.02037v1'>http://arxiv.org/abs/2312.02037v1</a></p>
<p><b>Compressor summary</b>: The paragraph describes a novel framework called Graph-based Feature Synthesis (GFS) that uses heterogeneous graphs to train machine learning models on multi-table relational databases without feature engineering, preserving the data's inherent relationships and structure.</p><hr><h3>Implicit Learning of Scene Geometry from Poses for Global Localization</h3>
<p>Mohammad Altillawi,Shile Li,Sai Manoj Prakhya,Ziyuan Liu,Joan Serrat</p>
<p><a href='http://arxiv.org/abs/2312.02029v1'>http://arxiv.org/abs/2312.02029v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a learning method for global visual localization that uses minimal pose labels to learn 3D scene geometry and improve pose estimation accuracy using rigid alignment and additional learning constraints.</p><hr><h3>VLTSeg: Simple Transfer of CLIP-Based Vision-Language Representations  for Domain Generalized Semantic Segmentation</h3>
<p>Christoph Hümmer,Manuel Schwonberg,Liangwei Zhong,Hu Cao,Alois Knoll,Hanno Gottschalk</p>
<p><a href='http://arxiv.org/abs/2312.02021v1'>http://arxiv.org/abs/2312.02021v1</a></p>
<p><b>Compressor summary</b>: VLTSeg is a vision-language method that improves domain generalization in semantic segmentation by using CLIP and EVA-CLIP encoders, outperforming previous approaches on several benchmarks.</p><hr><h3>Action Inference by Maximising Evidence: Zero-Shot Imitation from  Observation with World Models</h3>
<p>Xingyuan Zhang,Philip Becker-Ehmck,Patrick van der Smagt,Maximilian Karl</p>
<p><a href='http://arxiv.org/abs/2312.02019v1'>http://arxiv.org/abs/2312.02019v1</a></p>
<p><b>Compressor summary</b>: The paper introduces AIME, a method that enables agents to learn new behaviors by imitating expert demonstrations without needing further training or environment interactions.</p><hr><h3>ColonNeRF: Neural Radiance Fields for High-Fidelity Long-Sequence  Colonoscopy Reconstruction</h3>
<p>Yufei Shi,Beijia Lu,Jia-Wei Liu,Ming Li,Mike Zheng Shou</p>
<p><a href='http://arxiv.org/abs/2312.02015v1'>http://arxiv.org/abs/2312.02015v1</a></p>
<p><b>Compressor summary</b>: The ColonNeRF framework uses neural rendering to reconstruct the entire colon in a piecewise manner, addressing challenges like shape dissimilarity, geometry complexity, and sparse views for accurate long-sequence colonoscopy reconstruction.</p><hr><h3>Optimal Data Generation in Multi-Dimensional Parameter Spaces, using  Bayesian Optimization</h3>
<p>M. R. Mahani,Igor A. Nechepurenko,Yasmin Rahimof,Andreas Wicht</p>
<p><a href='http://arxiv.org/abs/2312.02012v1'>http://arxiv.org/abs/2312.02012v1</a></p>
<p><b>Compressor summary</b>: The text proposes a method for creating a minimal yet informative database using Bayesian optimization and Gaussian process regression to train accurate machine learning models with less data points.</p><hr><h3>Towards Learning a Generalist Model for Embodied Navigation</h3>
<p>Duo Zheng,Shijia huang,Lin Zhao,Yiwu Zhong,Liwei Wang</p>
<p><a href='http://arxiv.org/abs/2312.02010v1'>http://arxiv.org/abs/2312.02010v1</a></p>
<p><b>Compressor summary</b>: The paper introduces NaviLLM, a generalist AI model for embodied navigation that adapts large language models to various tasks using schema-based instructions and achieves state-of-the-art performance and generalizability.</p><hr><h3>Language-only Efficient Training of Zero-shot Composed Image Retrieval</h3>
<p>Geonmo Gu,Sanghyuk Chun,Wonjae Kim,Yoohoon Kang,Sangdoo Yun</p>
<p><a href='http://arxiv.org/abs/2312.01998v1'>http://arxiv.org/abs/2312.01998v1</a></p>
<p><b>Compressor summary</b>: The proposed LinCIR framework trains a composed image retrieval model using only text datasets with a self-supervision technique called self-masking projection, achieving high performance on four benchmarks and outperforming some supervised methods.</p><hr><h3>A Generative Self-Supervised Framework using Functional Connectivity in  fMRI Data</h3>
<p>Jungwon Choi,Seongho Keum,EungGu Yun,Byung-Hoon Kim,Juho Lee</p>
<p><a href='http://arxiv.org/abs/2312.01994v1'>http://arxiv.org/abs/2312.01994v1</a></p>
<p><b>Compressor summary</b>: The authors propose a generative self-supervised learning method for graph neural networks to improve accuracy and interpretability in modeling dynamic functional connectivity from fMRI data, addressing challenges such as high data cost and limited generalization.</p><hr><h3>Information Modified K-Nearest Neighbor</h3>
<p>Mohammad Ali Vahedifar,Azim Akhtarshenas,Mariam Sabbaghian,Mohammad Rafatpanah</p>
<p><a href='http://arxiv.org/abs/2312.01991v1'>http://arxiv.org/abs/2312.01991v1</a></p>
<p><b>Compressor summary</b>: The paper introduces IMKNN, a novel method that improves the KNN algorithm by using Mutual Information and Shapley values to assign weights to neighbors, and shows its superior performance in various classification tasks.</p><hr><h3>Bootstrapping SparseFormers from Vision Foundation Models</h3>
<p>Ziteng Gao,Zhan Tong,Kevin Qinghong Lin,Joya Chen,Mike Zheng Shou</p>
<p><a href='http://arxiv.org/abs/2312.01987v1'>http://arxiv.org/abs/2312.01987v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to bootstrap SparseFormer architectures from ViT-based vision foundation models, reducing computational costs and enabling zero-shot performance with minimal training samples.</p><hr><h3>UniGS: Unified Representation for Image Generation and Segmentation</h3>
<p>Lu Qi,Lehan Yang,Weidong Guo,Yu Xu,Bo Du,Varun Jampani,Ming-Hsuan Yang</p>
<p><a href='http://arxiv.org/abs/2312.01985v1'>http://arxiv.org/abs/2312.01985v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new representation for diffusion models that enables image generation, segmentation, and adaptation to various tasks with efficient modules and inpainting pipeline.</p><hr><h3>Semantics-aware Motion Retargeting with Vision-Language Models</h3>
<p>Haodong Zhang,ZhiKe Chen,Haocheng Xu,Lei Hao,Xiaofei Wu,Songcen Xu,Zhensong Zhang,Yue Wang,Rong Xiong</p>
<p><a href='http://arxiv.org/abs/2312.01964v1'>http://arxiv.org/abs/2312.01964v1</a></p>
<p><b>Compressor summary</b>: The SMT method uses vision-language models to extract and maintain meaningful motion semantics for motion retargeting between animation characters, with a two-stage pipeline that ensures preservation of both fine-grained details and high-level semantics.</p><hr><h3>Learning-Based Approaches to Predictive Monitoring with Conformal  Statistical Guarantees</h3>
<p>Francesca Cairoli,Luca Bortolussi,Nicola Paoletti</p>
<p><a href='http://arxiv.org/abs/2312.01959v1'>http://arxiv.org/abs/2312.01959v1</a></p>
<p><b>Compressor summary</b>: This tutorial explains how to use machine learning and conformal prediction to efficiently predict and monitor future violations of requirements in complex systems.</p><hr><h3>Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian  Perspective</h3>
<p>Victor Gallego</p>
<p><a href='http://arxiv.org/abs/2312.01957v1'>http://arxiv.org/abs/2312.01957v1</a></p>
<p><b>Compressor summary</b>: The paper presents dSC, a method for improving LLM outputs using synthetic data and Bayesian inference, and shows its potential in various tasks.</p><hr><h3>Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large  Language Models</h3>
<p>Andrea Papaluca,Daniel Krefl,Sergio Mendez Rodriguez,Artem Lensky,Hanna Suominen</p>
<p><a href='http://arxiv.org/abs/2312.01954v1'>http://arxiv.org/abs/2312.01954v1</a></p>
<p><b>Compressor summary</b>: The authors evaluate how Large Language Models (LLMs) use contextual information from a Knowledge Base (KB) to perform Triplet Extraction (TE) in Zero- and Few-Shots settings, finding that the quality of the KB context strongly affects TE performance.</p><hr><h3>Instance-guided Cartoon Editing with a Large-scale Dataset</h3>
<p>Jian Lin,Chengze Li,Xueting Liu,Zhongping Ge</p>
<p><a href='http://arxiv.org/abs/2312.01943v1'>http://arxiv.org/abs/2312.01943v1</a></p>
<p><b>Compressor summary</b>: The authors introduce a new dataset and model for accurately segmenting characters in cartoons, which enables various creative applications in cartoon editing.</p><hr><h3>Foundations for Transfer in Reinforcement Learning: A Taxonomy of  Knowledge Modalities</h3>
<p>Markus Wulfmeier,Arunkumar Byravan,Sarah Bechtle,Karol Hausman,Nicolas Heess</p>
<p><a href='http://arxiv.org/abs/2312.01939v1'>http://arxiv.org/abs/2312.01939v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses how artificial intelligence systems are becoming more general and the challenges and opportunities in improving their knowledge representation and transfer across different domains using reinforcement learning modalities.</p><hr><h3>A Reliable Representation with Bidirectional Transition Model for Visual  Reinforcement Learning Generalization</h3>
<p>Xiaobo Hu,Youfang Lin,Yue Liu,Jinwen Wang,Shuo Wang,Hehe Fan,Kai Lv</p>
<p><a href='http://arxiv.org/abs/2312.01915v1'>http://arxiv.org/abs/2312.01915v1</a></p>
<p><b>Compressor summary</b>: The BiT model leverages bidirectional prediction of environmental transitions to extract reliable representations for vision-based control tasks.</p><hr><h3>Unsupervised Anomaly Detection using Aggregated Normative Diffusion</h3>
<p>Alexander Frotscher,Jaivardhan Kapoor,Thomas Wolfers,Christian F. Baumgartner</p>
<p><a href='http://arxiv.org/abs/2312.01904v1'>http://arxiv.org/abs/2312.01904v1</a></p>
<p><b>Compressor summary</b>: ANDi is a new unsupervised anomaly detection method for brain MRI that outperforms existing approaches and can identify diverse types of anomalies better.</p><hr><h3>Adapting Short-Term Transformers for Action Detection in Untrimmed  Videos</h3>
<p>Min Yang,Huan Gao,Ping Guo,Limin Wang</p>
<p><a href='http://arxiv.org/abs/2312.01897v1'>http://arxiv.org/abs/2312.01897v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new mechanism for adapting pre-trained ViT models as a unified long-form video transformer to capture inter-snippet relations for temporal action detection in untrimmed videos, while maintaining low computation overhead and memory consumption.</p><hr><h3>Non-Intrusive Load Monitoring for Feeder-Level EV Charging Detection:  Sliding Window-based Approaches to Offline and Online Detection</h3>
<p>Cameron Martin,Fucai Ke,Hao Wang</p>
<p><a href='http://arxiv.org/abs/2312.01887v1'>http://arxiv.org/abs/2312.01887v1</a></p>
<p><b>Compressor summary</b>: This paper presents a novel method for detecting electric vehicle (EV) charging at the feeder level using sliding-window feature extraction and machine learning techniques, achieving high accuracy in both offline and online detection.</p><hr><h3>InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language  Models</h3>
<p>Xunguang Wang,Zhenlan Ji,Pingchuan Ma,Zongjie Li,Shuai Wang</p>
<p><a href='http://arxiv.org/abs/2312.01886v1'>http://arxiv.org/abs/2312.01886v1</a></p>
<p><b>Compressor summary</b>: The paper proposes InstructTA, a targeted adversarial attack on large vision-language models that uses a text-to-image model, GPT-4, and a local surrogate model to generate instruction-aware features and optimize the adversarial example.</p><hr><h3>Correlation and Unintended Biases on Univariate and Multivariate  Decision Trees</h3>
<p>Mattia Setzu,Salvatore Ruggieri</p>
<p><a href='http://arxiv.org/abs/2312.01884v1'>http://arxiv.org/abs/2312.01884v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses how univariate and multivariate decision trees, which partition data differently, have similar performance despite the latter being more powerful, possibly due to dataset pre-processing bias.</p><hr><h3>Unleashing the Potential of Large Language Model: Zero-shot VQA for  Flood Disaster Scenario</h3>
<p>Yimin Sun,Chao Wang,Yan Peng</p>
<p><a href='http://arxiv.org/abs/2312.01882v1'>http://arxiv.org/abs/2312.01882v1</a></p>
<p><b>Compressor summary</b>: The paper introduces ZFDDA, a zero-shot VQA model for flood damage assessment, and FFD-IQA, a new dataset with diverse question types and more data to evaluate the model's performance.</p><hr><h3>HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot  Prompt Learning</h3>
<p>Xingtong Yu,Zemin Liu,Yuan Fang,Xinming Zhang</p>
<p><a href='http://arxiv.org/abs/2312.01878v1'>http://arxiv.org/abs/2312.01878v1</a></p>
<p><b>Compressor summary</b>: HGPROMPT is a novel framework that unifies pre-training and downstream tasks for homogeneous and heterogeneous graphs using dual-template design and dual-prompt to bridge the gap between them.</p><hr><h3>FeaInfNet: Diagnosis in Medical Image with Feature-Driven Inference and  Visual Explanations</h3>
<p>Yitao Peng,Lianghua He,Die Hu,Yihang Liu,Longzhen Yang,Shaohua Shang</p>
<p><a href='http://arxiv.org/abs/2312.01871v1'>http://arxiv.org/abs/2312.01871v1</a></p>
<p><b>Compressor summary</b>: The paper proposes FeaInfNet, a model for interpretable medical image diagnosis that simulates doctors' reasoning process, uses local feature masks and adaptive dynamic masks to enhance expressivity and interpretability, and achieves state-of-the-art performance on multiple datasets.</p><hr><h3>Evaluating Dependencies in Fact Editing for Language Models: Specificity  and Implication Awareness</h3>
<p>Zichao Li,Ines Arous,Siva Reddy,Jackie C. K. Cheung</p>
<p><a href='http://arxiv.org/abs/2312.01858v1'>http://arxiv.org/abs/2312.01858v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses a proposed evaluation protocol, DepEdit, for assessing the editing process of large language models (LLMs) with respect to logical constraints and implications of edited facts.</p><hr><h3>Generalization by Adaptation: Diffusion-Based Domain Extension for  Domain-Generalized Semantic Segmentation</h3>
<p>Joshua Niemeijer,Manuel Schwonberg,Jan-Aike Termöhlen,Nico M. Schmidt,Tim Fingscheidt</p>
<p><a href='http://arxiv.org/abs/2312.01850v1'>http://arxiv.org/abs/2312.01850v1</a></p>
<p><b>Compressor summary</b>: The authors propose DIDEX, a diffusion-based domain extension method that generates diverse pseudo-target images with text prompts and trains a model to adapt towards them, achieving improved domain generalization results on various datasets without using target data.</p><hr><h3>VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D  Hybrid Prior</h3>
<p>Xusen Sun,Longhao Zhang,Hao Zhu,Peng Zhang,Bang Zhang,Xinya Ji,Kangneng Zhou,Daiheng Gao,Liefeng Bo,Xun Cao</p>
<p><a href='http://arxiv.org/abs/2312.01841v1'>http://arxiv.org/abs/2312.01841v1</a></p>
<p><b>Compressor summary</b>: VividTalk is a framework that generates high-quality talking head videos with lip-sync, expressive facial expressions, natural head pose, and high video quality by learning two motions in two stages.</p><hr><h3>Prompting Disentangled Embeddings for Knowledge Graph Completion with  Pre-trained Language Model</h3>
<p>Yuxia Geng,Jiaoyan Chen,Yuhang Zeng,Zhuo Chen,Wen Zhang,Jeff Z. Pan,Yuxiang Wang,Xiaoliang Xu</p>
<p><a href='http://arxiv.org/abs/2312.01837v1'>http://arxiv.org/abs/2312.01837v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new Knowledge Graph Completion method (PDKGC) that uses prompts to train a frozen pre-trained language model, improving entity prediction by combining textual and structural information.</p><hr><h3>Few Clicks Suffice: Active Test-Time Adaptation for Semantic  Segmentation</h3>
<p>Longhui Yuan,Shuang Li,Zhuo He,Binhui Xie</p>
<p><a href='http://arxiv.org/abs/2312.01835v1'>http://arxiv.org/abs/2312.01835v1</a></p>
<p><b>Compressor summary</b>: The paper proposes active test-time adaptation (ATASeg) for semantic segmentation, which uses a human-in-the-loop pattern to query few labels online and reduce the performance gap between unsupervised and supervised methods.</p><hr><h3>Exchange-of-Thought: Enhancing Large Language Model Capabilities through  Cross-Model Communication</h3>
<p>Zhangyue Yin,Qiushi Sun,Cheng Chang,Qipeng Guo,Junqi Dai,Xuanjing Huang,Xipeng Qiu</p>
<p><a href='http://arxiv.org/abs/2312.01823v1'>http://arxiv.org/abs/2312.01823v1</a></p>
<p><b>Compressor summary</b>: The Exchange-of-Thought framework allows large language models to communicate with each other during problem-solving, improving their performance on complex reasoning tasks by incorporating external insights.</p><hr><h3>Learning Machine Morality through Experience and Interaction</h3>
<p>Elizaveta Tennant,Stephen Hailes,Mirco Musolesi</p>
<p><a href='http://arxiv.org/abs/2312.01818v1'>http://arxiv.org/abs/2312.01818v1</a></p>
<p><b>Compressor summary</b>: The paper explores different approaches to embedding morality in AI systems, argues that hybrid solutions combining hard-coded rules and learned preferences are needed, and presents three case studies using reinforcement learning to provide moral principles to agents.</p><hr><h3>Class Symbolic Regression: Gotta Fit 'Em All</h3>
<p>Wassim Tenachi,Rodrigo Ibata,Thibaut L. François,Foivos I. Diakogiannis</p>
<p><a href='http://arxiv.org/abs/2312.01816v1'>http://arxiv.org/abs/2312.01816v1</a></p>
<p><b>Compressor summary</b>: Class Symbolic Regression is a framework that finds a single function to fit multiple data sets with different parameters, using the idea that similar phenomena follow common laws. It improves on previous symbolic regression methods by integrating dimensional analysis and deep reinforcement learning, and shows its usefulness in astrophysics by finding an analytic galaxy potential from simulated orbits.</p><hr><h3>Energy-based Potential Games for Joint Motion Forecasting and Control</h3>
<p>Christopher Diehl,Tobias Klosek,Martin Krüger,Nils Murzyn,Timo Osterburg,Torsten Bertram</p>
<p><a href='http://arxiv.org/abs/2312.01811v1'>http://arxiv.org/abs/2312.01811v1</a></p>
<p><b>Compressor summary</b>: The authors propose a game-theoretic approach to model multi-agent interactions in robotics, combining energy-based models with neural networks for inference and optimization, which improves interpretability and predictive performance.</p><hr><h3>Collaborative Neural Painting</h3>
<p>Nicola Dall'Asen,Willi Menapace,Elia Peruzzo,Enver Sangineto,Yiming Wang,Elisa Ricci</p>
<p><a href='http://arxiv.org/abs/2312.01800v1'>http://arxiv.org/abs/2312.01800v1</a></p>
<p><b>Compressor summary</b>: The text describes a novel AI-based collaborative painting task that aims to produce coherent paintings with humans and machines, using parametrized strokes, attention mechanisms, and a new dataset.</p><hr><h3>Distributed Continual Learning with CoCoA in High-dimensional Linear  Regression</h3>
<p>Martin Hellkvist,Ayça Özçelikkale,Anders Ahlén</p>
<p><a href='http://arxiv.org/abs/2312.01795v1'>http://arxiv.org/abs/2312.01795v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper studies estimation under time-varying signals in continual learning setting
- It focuses on distributed learning using COCOA algorithm
- It provides analytical characterization for generalization error of COCOA
- It shows how network size, task similarity and number of tasks affect generalization error
- It demonstrates the results with a digit classification task

Summary: The paper analyzes how COCOA, a distributed learning algorithm, performs in continual learning with time-varying signals, and how to choose the network size to minimize the generalization error.</p><hr><h3>Wild-Tab: A Benchmark For Out-Of-Distribution Generalization In Tabular  Regression</h3>
<p>Sergey Kolesnikov</p>
<p><a href='http://arxiv.org/abs/2312.01792v1'>http://arxiv.org/abs/2312.01792v1</a></p>
<p><b>Compressor summary</b>: Wild-Tab is a benchmark for testing out-of-distribution generalization in tabular regression tasks, using real-world datasets from weather prediction and power consumption estimation.</p><hr><h3>Exploring Multi-Modal Fusion for Image Manipulation Detection and  Localization</h3>
<p>Konstantinos Triaridis,Vasileios Mezaris</p>
<p><a href='http://arxiv.org/abs/2312.01790v1'>http://arxiv.org/abs/2312.01790v1</a></p>
<p><b>Compressor summary</b>: The paper presents two methods for merging the outputs of different filters to improve image manipulation localization and detection (IMLD), achieving competitive results compared to existing approaches.</p><hr><h3>Two-stage optimized unified adversarial patch for attacking  visible-infrared cross-modal detectors in the physical world</h3>
<p>Chengyin Hu,Weiwen Shi</p>
<p><a href='http://arxiv.org/abs/2312.01789v1'>http://arxiv.org/abs/2312.01789v1</a></p>
<p><b>Compressor summary</b>: This paper introduces a novel attack method, TOUAP, for compromising cross-modal visible-infrared detectors in real-world scenarios using a two-stage optimization process involving an irregular polygonal infrared patch and a color QR code.</p><hr><h3>Developing Linguistic Patterns to Mitigate Inherent Human Bias in  Offensive Language Detection</h3>
<p>Toygar Tanyel,Besher Alkurdi,Serkan Ayvaz</p>
<p><a href='http://arxiv.org/abs/2312.01787v1'>http://arxiv.org/abs/2312.01787v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a data augmentation method to reduce human bias in offensive language detection on social media, aiming to improve accuracy and fairness in classifying offensive content across multiple languages.</p><hr><h3>IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks</h3>
<p>Jiarui Xu,Yossi Gandelsman,Amir Bar,Jianwei Yang,Jianfeng Gao,Trevor Darrell,Xiaolong Wang</p>
<p><a href='http://arxiv.org/abs/2312.01771v1'>http://arxiv.org/abs/2312.01771v1</a></p>
<p><b>Compressor summary</b>: The paper introduces IMProv, a generative model that learns visual tasks from textual and image prompts, achieving improvements in various computer vision tasks.</p><hr><h3>Localizing and Assessing Node Significance in Default Mode Network using  Sub-Community Detection in Mild Cognitive Impairment</h3>
<p>Ameiy Acharya,Chakka Sai Pradeep,Neelam Sinha</p>
<p><a href='http://arxiv.org/abs/2312.01768v1'>http://arxiv.org/abs/2312.01768v1</a></p>
<p><b>Compressor summary</b>: The study uses fMRI and NSS to identify brain regions in the DMN that are most impacted by MCI, finding significant differences for PCC and Fusiform nodes.</p><hr><h3>Dynamic Erasing Network Based on Multi-Scale Temporal Features for  Weakly Supervised Video Anomaly Detection</h3>
<p>Chen Zhang,Guorong Li,Yuankai Qi,Hanhua Ye,Laiyun Qing,Ming-Hsuan Yang,Qingming Huang</p>
<p><a href='http://arxiv.org/abs/2312.01764v1'>http://arxiv.org/abs/2312.01764v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a Dynamic Erasing Network (DE-Net) that learns multi-scale temporal features for weakly supervised video anomaly detection, handling duration variations and encouraging the discovery of gentle abnormal segments.</p><hr><h3>CZL-CIAE: CLIP-driven Zero-shot Learning for Correcting Inverse Age  Estimation</h3>
<p>Yuntao Shou,Wei Ai,Tao Meng,Keqin Li</p>
<p><a href='http://arxiv.org/abs/2312.01758v1'>http://arxiv.org/abs/2312.01758v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method called CZL-CIAE that leverages CLIP and FourierFormer to improve zero-shot age estimation from images and text, leading to better prediction results.</p><hr><h3>A Comprehensive Literature Review on Sweet Orange Leaf Diseases</h3>
<p>Yousuf Rayhan Emon,Md Golam Rabbani,Dr. Md. Taimur Ahad,Faruk Ahmed</p>
<p><a href='http://arxiv.org/abs/2312.01756v1'>http://arxiv.org/abs/2312.01756v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses a literature review of machine learning methods for detecting sweet orange leaf diseases using image classification techniques.</p><hr><h3>Long-Tail Learning with Rebalanced Contrastive Loss</h3>
<p>Charika De Alvis,Dishanika Denipitiyage,Suranga Seneviratne</p>
<p><a href='http://arxiv.org/abs/2312.01753v1'>http://arxiv.org/abs/2312.01753v1</a></p>
<p><b>Compressor summary</b>: Rebalanced Contrastive Learning (RCL) improves long tail classification by balancing feature space, reducing intra-class distance, and regularizing margins for imbalanced classes.</p><hr><h3>Open-DDVM: A Reproduction and Extension of Diffusion Model for Optical  Flow Estimation</h3>
<p>Qiaole Dong,Bo Zhao,Yanwei Fu</p>
<p><a href='http://arxiv.org/abs/2312.01746v1'>http://arxiv.org/abs/2312.01746v1</a></p>
<p><b>Compressor summary</b>: The authors reproduce the closed-source DDVM model for image-to-image translation, making it open-source, and achieve comparable performance to the original with public data and GPUs.</p><hr><h3>Cross-Modal Adaptive Dual Association for Text-to-Image Person Retrieval</h3>
<p>Dixuan Lin,Yixing Peng,Jingke Meng,Wei-Shi Zheng</p>
<p><a href='http://arxiv.org/abs/2312.01745v1'>http://arxiv.org/abs/2312.01745v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for person re-identification that builds fine bidirectional cross-modal associations between visual and textual modalities using adaptive dual association modules, ATP and ARA.</p><hr><h3>Fully Spiking Denoising Diffusion Implicit Models</h3>
<p>Ryo Watanabe,Yusuke Mukuta,Tatsuya Harada</p>
<p><a href='http://arxiv.org/abs/2312.01742v1'>http://arxiv.org/abs/2312.01742v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel approach, FSDDIM, to create a diffusion model within spiking neural networks (SNNs) using synaptic current learning (SCL), which enables high-speed and low-energy image generation while maintaining the advantages of SNNs.</p><hr><h3>SRSNetwork: Siamese Reconstruction-Segmentation Networks based on  Dynamic-Parameter Convolution</h3>
<p>Bingkun Nian,Fenghe Tang,Jianrui Ding,Pingping Zhang,Jie Yang,S. Kevin Zhou,Wei Liu</p>
<p><a href='http://arxiv.org/abs/2312.01741v1'>http://arxiv.org/abs/2312.01741v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new deep neural network for weak target image segmentation that leverages reconstruction tasks and outperforms existing methods on seven datasets.</p><hr><h3>Divide-and-Conquer Strategy for Large-Scale Dynamic Bayesian Network  Structure Learning</h3>
<p>Hui Ouyang,Cheng Chen,Ke Tang</p>
<p><a href='http://arxiv.org/abs/2312.01739v1'>http://arxiv.org/abs/2312.01739v1</a></p>
<p><b>Compressor summary</b>: This paper introduces a novel divide-and-conquer strategy for large-scale Dynamic Bayesian Network structure learning, specifically focusing on Time-sliced Bayesian Networks, and shows substantial improvements in scalability, accuracy, and computational efficiency.</p><hr><h3>Effective Adapter for Face Recognition in the Wild</h3>
<p>Yunhao Liu,Lu Qi,Yu-Ju Tsai,Xiangtai Li,Kelvin C. K. Chan,Ming-Hsuan Yang</p>
<p><a href='http://arxiv.org/abs/2312.01734v1'>http://arxiv.org/abs/2312.01734v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an adapter for face recognition models that processes both low-quality and enhanced images using dual-input structures to overcome the limitations of traditional approaches and achieve better performance in the wild.</p><hr><h3>Likelihood-Aware Semantic Alignment for Full-Spectrum  Out-of-Distribution Detection</h3>
<p>Fan Lu,Kai Zhu,Kecheng Zheng,Wei Zhai,Yang Cao</p>
<p><a href='http://arxiv.org/abs/2312.01732v1'>http://arxiv.org/abs/2312.01732v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for detecting out-of-distribution samples in images and text that uses semantic alignment and likelihood-aware sampling to adapt to complex domain transformations.</p><hr><h3>EdgeConvFormer: Dynamic Graph CNN and Transformer based Anomaly  Detection in Multivariate Time Series</h3>
<p>Jie Liu,Qilin Li,Senjian An,Bradley Ezard,Ling Li</p>
<p><a href='http://arxiv.org/abs/2312.01729v1'>http://arxiv.org/abs/2312.01729v1</a></p>
<p><b>Compressor summary</b>: EdgeConvFormer is a novel anomaly detection method for multivariate time series that combines Time2vec embedding, dynamic graph CNN, and Transformer to extract global and local spatial-time information and outperforms existing approaches on various real-world datasets.</p><hr><h3>ImputeFormer: Graph Transformers for Generalizable Spatiotemporal  Imputation</h3>
<p>Tong Nie,Guoyang Qin,Yuewen Mei,Jian Sun</p>
<p><a href='http://arxiv.org/abs/2312.01728v1'>http://arxiv.org/abs/2312.01728v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an effective and versatile deep neural model for multivariate time series imputation, which incorporates low-rank properties and achieves superior performance on various datasets.</p><hr><h3>StableVITON: Learning Semantic Correspondence with Latent Diffusion  Model for Virtual Try-On</h3>
<p>Jeongho Kim,Gyojung Gu,Minho Park,Sunghyun Park,Jaegul Choo</p>
<p><a href='http://arxiv.org/abs/2312.01725v1'>http://arxiv.org/abs/2312.01725v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes StableVITON, a method to do image-based virtual try-on using a pre-trained diffusion model.
- The method learns the semantic correspondence between clothing and body in the latent space of the model.
- The method uses zero cross-attention blocks, attention total variation loss, and augmentation to preserve clothing details and generate high-fidelity images.

Summary:
StableVITON is a novel virtual try-on method that leverages a pre-trained diffusion model to learn the semantic correspondence between clothing and body in the latent space and produce realistic images with sharp attention maps.</p><hr><h3>The Self-Loop Paradox: Investigating the Impact of Self-Loops on Graph  Neural Networks</h3>
<p>Moritz Lampert,Ingo Scholtes</p>
<p><a href='http://arxiv.org/abs/2312.01721v1'>http://arxiv.org/abs/2312.01721v1</a></p>
<p><b>Compressor summary</b>: The self-loop paradox is a phenomenon where the information a node gains from itself can be smaller in graphs with self-loops compared to graphs without, depending on the GNN architecture, number of layers, and whether the layer number is even or odd.</p><hr><h3>Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large  Language Models</h3>
<p>Bingshuai Liu,Chenyang Lyu,Zijun Min,Zhanyu Wang,Jinsong Su,Longyue Wang</p>
<p><a href='http://arxiv.org/abs/2312.01714v1'>http://arxiv.org/abs/2312.01714v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve multi-modal reasoning in LLMs by using retrieval mechanisms to select relevant examples, achieving state-of-the-art results on ScienceQA dataset.</p><hr><h3>Disentangled Interaction Representation for One-Stage Human-Object  Interaction Detection</h3>
<p>Xubin Zhong,Changxing Ding,Yupeng Hu,Dacheng Tao</p>
<p><a href='http://arxiv.org/abs/2312.01713v1'>http://arxiv.org/abs/2312.01713v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Shunted Cross-Attention (SCA) and Interaction-aware Pose Estimation (IPE) to improve one-stage HOI detection by extracting disentangled interaction representations using different attention heads and a novel attention module for human pose features.</p><hr><h3>Regressor-Segmenter Mutual Prompt Learning for Crowd Counting</h3>
<p>Mingyue Guo,Li Yuan,Zhaoyi Yan,Binghui Chen,Yaowei Wang,Qixiang Ye</p>
<p><a href='http://arxiv.org/abs/2312.01711v1'>http://arxiv.org/abs/2312.01711v1</a></p>
<p><b>Compressor summary</b>: mPrompt is a method that uses both point and segmentation annotations to guide each other, reducing bias and improving accuracy in crowd counting tasks.</p><hr><h3>Mitigating Fine-Grained Hallucination by Fine-Tuning Large  Vision-Language Models with Caption Rewrites</h3>
<p>Lei Wang,Jiabang He,Shenshen Li,Ning Liu,Ee-Peng Lim</p>
<p><a href='http://arxiv.org/abs/2312.01701v1'>http://arxiv.org/abs/2312.01701v1</a></p>
<p><b>Compressor summary</b>: The paper introduces ReCaption, a framework to reduce fine-grained object hallucinations in instruction-tuned large vision-language models using ChatGPT and a new probing-based evaluation method called Fine-Grained Object Hallucination Evaluation.</p><hr><h3>Data Management For Large Language Models: A Survey</h3>
<p>Zige Wang,Wanjun Zhong,Yufei Wang,Qi Zhu,Fei Mi,Baojun Wang,Lifeng Shang,Xin Jiang,Qun Liu</p>
<p><a href='http://arxiv.org/abs/2312.01700v1'>http://arxiv.org/abs/2312.01700v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses the importance of data management in training Large Language Models, and provides a survey of current research and challenges in this field.</p><hr><h3>Rethinking Urban Mobility Prediction: A Super-Multivariate Time Series  Forecasting Approach</h3>
<p>Jinguo Cheng,Ke Li,Yuxuan Liang,Lijun Sun,Junchi Yan,Yuankai Wu</p>
<p><a href='http://arxiv.org/abs/2312.01699v1'>http://arxiv.org/abs/2312.01699v1</a></p>
<p><b>Compressor summary</b>: SUMformer is a novel approach to urban mobility prediction that treats city data as complex multivariate time series and uses a special attention mechanism to capture temporal and cross-variable correlations, achieving better results than existing methods.</p><hr><h3>Hulk: A Universal Knowledge Translator for Human-Centric Tasks</h3>
<p>Yizhou Wang,Yixuan Wu,Shixiang Tang,Weizhen He,Xun Guo,Feng Zhu,Lei Bai,Rui Zhao,Jian Wu,Tong He,Wanli Ouyang</p>
<p><a href='http://arxiv.org/abs/2312.01697v1'>http://arxiv.org/abs/2312.01697v1</a></p>
<p><b>Compressor summary</b>: Hulk is a multimodal human-centric generalist model that can handle various perception tasks without fine-tuning, by using discrete and continuous representations for different modalities.</p><hr><h3>BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection</h3>
<p>Zhenxin Li,Shiyi Lan,Jose M. Alvarez,Zuxuan Wu</p>
<p><a href='http://arxiv.org/abs/2312.01696v1'>http://arxiv.org/abs/2312.01696v1</a></p>
<p><b>Compressor summary</b>: The paper proposes BEVNeXt, a dense Bird's Eye View framework for 3D object detection that combines depth estimation, temporal aggregation, and perspective techniques with CRF-modulated depth embedding, achieving state-of-the-art results on the nuScenes benchmark.</p><hr><h3>Risk-Controlling Model Selection via Guided Bayesian Optimization</h3>
<p>Bracha Laufer-Goldshtein,Adam Fisch,Regina Barzilay,Tommi Jaakkola</p>
<p><a href='http://arxiv.org/abs/2312.01692v1'>http://arxiv.org/abs/2312.01692v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to find machine learning model configurations that balance various risks and metrics using Bayesian Optimization and risk-controlling procedures.</p><hr><h3>Optimizing Bus Travel: A Novel Approach to Feature Mining with P-KMEANS  and P-LDA Algorithms</h3>
<p>Hongjie Liu,Haotian Shi,Sicheng Fu,Tengfei Yuan,Xinhuan Zhang,Hongzhe Xu,Bin Ran</p>
<p><a href='http://arxiv.org/abs/2312.01687v1'>http://arxiv.org/abs/2312.01687v1</a></p>
<p><b>Compressor summary</b>: The study presents a method for extracting features from bus travel data using Point of Interest (POI) data, enhanced P-KMENAS and P-LDA algorithms, which can improve bus travel attractiveness, usage, congestion, and emissions by understanding travel behavior.</p><hr><h3>ResEnsemble-DDPM: Residual Denoising Diffusion Probabilistic Models for  Ensemble Learning</h3>
<p>Shi Zhenning,Dong Changsheng,Xie Xueshuo,Pan Bin,He Along,Li Tao</p>
<p><a href='http://arxiv.org/abs/2312.01682v1'>http://arxiv.org/abs/2312.01682v1</a></p>
<p><b>Compressor summary</b>: ResEnsemble-DDPM is a method that combines denoising diffusion probabilistic models and end-to-end models for better image segmentation by introducing a residual term and using ensemble learning.</p><hr><h3>Jellyfish: A Large Language Model for Data Preprocessing</h3>
<p>Haochen Zhang,Yuyang Dong,Chuan Xiao,Masafumi Oyamada</p>
<p><a href='http://arxiv.org/abs/2312.01678v1'>http://arxiv.org/abs/2312.01678v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Jellyfish, an open-source LLM for DP tasks that can operate on a low-priced GPU, learn domain knowledge during tuning, and explain its output decisions with an interpreter.</p><hr><h3>Multi-task Image Restoration Guided By Robust DINO Features</h3>
<p>Xin Lin,Chao Ren,Kelvin C. K. Chan,Lu Qi,Jinshan Pan,Ming-Hsuan Yang</p>
<p><a href='http://arxiv.org/abs/2312.01677v1'>http://arxiv.org/abs/2312.01677v1</a></p>
<p><b>Compressor summary</b>: DINO-IR is a novel multi-task image restoration approach that uses robust features from DINOv2 to achieve better performance than existing methods in various tasks.</p><hr><h3>EDALearn: A Comprehensive RTL-to-Signoff EDA Benchmark for Democratized  and Reproducible ML for EDA Research</h3>
<p>Jingyu Pan,Chen-Chia Chang,Zhiyao Xie,Yiran Chen</p>
<p><a href='http://arxiv.org/abs/2312.01674v1'>http://arxiv.org/abs/2312.01674v1</a></p>
<p><b>Compressor summary</b>: The paper introduces EDALearn, a benchmark suite for Machine Learning in Electronic Design Automation, which provides a comprehensive and open-source dataset with end-to-end data collection, analysis, and reproducibility to promote research and efficiency in VLSI design.</p><hr><h3>STADEE: STAtistics-based DEEp Detection of Machine Generated Text</h3>
<p>Zheng Chen,Huming Liu</p>
<p><a href='http://arxiv.org/abs/2312.01672v1'>http://arxiv.org/abs/2312.01672v1</a></p>
<p><b>Compressor summary</b>: STADEE is a novel deep detection method that combines statistics with deep learning to identify machine-generated text, outperforming existing methods in various scenarios.</p><hr><h3>Multimodality-guided Image Style Transfer using Cross-modal GAN  Inversion</h3>
<p>Hanyu Wang,Pengxiang Wu,Kevin Dela Rosa,Chen Wang,Abhinav Shrivastava</p>
<p><a href='http://arxiv.org/abs/2312.01671v1'>http://arxiv.org/abs/2312.01671v1</a></p>
<p><b>Compressor summary</b>: The text introduces a novel method for MultiModality-guided Image Style Transfer (MMIST) that improves style transfer based on text guidance and allows inputs from various sources, achieving state-of-the-art performance on TIST task and effectiveness on MMIST task.</p><hr><h3>Customize your NeRF: Adaptive Source Driven 3D Scene Editing via  Local-Global Iterative Training</h3>
<p>Runze He,Shaofei Huang,Xuecheng Nie,Tianrui Hui,Luoqi Liu,Jiao Dai,Jizhong Han,Guanbin Li,Si Liu</p>
<p><a href='http://arxiv.org/abs/2312.01663v1'>http://arxiv.org/abs/2312.01663v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a CustomNeRF model that can edit 3D scenes based on texts or images, addressing challenges like foreground editing and multi-view consistency.</p><hr><h3>ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating  Pre-university Math Questions</h3>
<p>Phuoc Pham Van Long,Duc Anh Vu,Nhat M. Hoang,Xuan Long Do,Anh Tuan Luu</p>
<p><a href='http://arxiv.org/abs/2312.01661v1'>http://arxiv.org/abs/2312.01661v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses using ChatGPT, a large language model, to generate mathematical questions for different levels of education and evaluates its performance in both context-aware and context-unaware settings.</p><hr><h3>RiskBench: A Scenario-based Benchmark for Risk Identification</h3>
<p>Chi-Hsi Kung,Chieh-Chi Yang,Pang-Yuan Pao,Shu-Wei Lu,Pin-Lun Chen,Hsin-Cheng Lu,Yi-Ting Chen</p>
<p><a href='http://arxiv.org/abs/2312.01659v1'>http://arxiv.org/abs/2312.01659v1</a></p>
<p><b>Compressor summary</b>: The paper introduces RiskBench, a benchmark for evaluating risk identification algorithms in intelligent driving systems that aim to achieve zero collisions.</p><hr><h3>AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for  Preconditioning Matrix</h3>
<p>Yun Yue,Zhiling Ye,Jiadi Jiang,Yongchao Liu,Ke Zhang</p>
<p><a href='http://arxiv.org/abs/2312.01658v1'>http://arxiv.org/abs/2312.01658v1</a></p>
<p><b>Compressor summary</b>: The paper proposes AGD, a new optimizer for deep learning that uses a novel preconditioning matrix and an auto-switching function to improve generalization performance on various datasets.</p><hr><h3>On Tuning Neural ODE for Stability, Consistency and Faster Convergence</h3>
<p>Sheikh Waqas Akhtar</p>
<p><a href='http://arxiv.org/abs/2312.01657v1'>http://arxiv.org/abs/2312.01657v1</a></p>
<p><b>Compressor summary</b>: Neural-ODE use continuous neural networks to solve differential equations with advantages in memory efficiency, adaptability, and flexibility, but they have stability issues that can be addressed by using Nesterov's accelerated gradient (NAG) based ODE-solver.</p><hr><h3>An End-to-End Network Pruning Pipeline with Sparsity Enforcement</h3>
<p>Evan Dogariu</p>
<p><a href='http://arxiv.org/abs/2312.01653v1'>http://arxiv.org/abs/2312.01653v1</a></p>
<p><b>Compressor summary</b>: The text describes a new end-to-end training pipeline for neural network sparsification that reduces model size, complexity, and memory footprint while maintaining competitive performance.</p><hr><h3>Adaptive Confidence Threshold for ByteTrack in Multi-Object Tracking</h3>
<p>Linh Van Ma,Muhammad Ishfaq Hussain,JongHyun Park,Jeongbae Kim,Moongu Jeon</p>
<p><a href='http://arxiv.org/abs/2312.01650v1'>http://arxiv.org/abs/2312.01650v1</a></p>
<p><b>Compressor summary</b>: The paper presents an improved version of ByteTrack, a multiple object tracking algorithm, that adapts its confidence threshold based on detection performance.</p><hr><h3>Characterizing Large Language Model Geometry Solves Toxicity Detection  and Generation</h3>
<p>Randall Balestriero,Romain Cosentino,Sarath Shekkizhar</p>
<p><a href='http://arxiv.org/abs/2312.01648v1'>http://arxiv.org/abs/2312.01648v1</a></p>
<p><b>Compressor summary</b>: The authors propose a geometric approach to understand and manipulate large language models, revealing new features that help solve various tasks without relying on approximations or fine-tuning.</p><hr><h3>Voice-Based Smart Assistant System for Vehicles using RASA</h3>
<p>Aditya Paranjape,Yash Patwardhan,Vedant Deshpande,Aniket Darp,Jayashree Jagdale</p>
<p><a href='http://arxiv.org/abs/2312.01642v1'>http://arxiv.org/abs/2312.01642v1</a></p>
<p><b>Compressor summary</b>: The paper presents a voice-based chatbot for cars to improve road safety by automating tasks such as navigation, calls, weather forecasts, and music using voice commands instead of manual actions.</p><hr><h3>SequencePAR: Understanding Pedestrian Attributes via A Sequence  Generation Paradigm</h3>
<p>Jiandong Jin,Xiao Wang,Chenglong Li,Lili Huang,Jin Tang</p>
<p><a href='http://arxiv.org/abs/2312.01640v1'>http://arxiv.org/abs/2312.01640v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new generative model called SequencePAR for pedestrian attribute recognition that uses visual features, text prompts, and attention mechanisms to improve performance on complex and imbalanced data.</p><hr><h3>Robust Streaming, Sampling, and a Perspective on Online Learning</h3>
<p>Evan Dogariu,Jiatong Yu</p>
<p><a href='http://arxiv.org/abs/2312.01634v1'>http://arxiv.org/abs/2312.01634v1</a></p>
<p><b>Compressor summary</b>: The paper introduces statistical learning, robust streaming techniques, and their connections, aiming to enlighten and inspire further research in both fields.</p><hr><h3>GaussianHead: Impressive 3D Gaussian-based Head Avatars with Dynamic  Hybrid Neural Field</h3>
<p>Jie Wang,Xianyan Li,Jiucheng Xie,Feng Xu,Hao Gao</p>
<p><a href='http://arxiv.org/abs/2312.01632v1'>http://arxiv.org/abs/2312.01632v1</a></p>
<p><b>Compressor summary</b>: GaussianHead is a head avatar algorithm that uses 3D gaussian primitives to represent dynamic scenes efficiently and accurately, achieving optimal visual results in various tasks.</p><hr><h3>CLAMP: Contrastive LAnguage Model Prompt-tuning</h3>
<p>Piotr Teterwak,Ximeng Sun,Bryan A. Plummer,Kate Saenko,Ser-Nam Lim</p>
<p><a href='http://arxiv.org/abs/2312.01629v1'>http://arxiv.org/abs/2312.01629v1</a></p>
<p><b>Compressor summary</b>: The paper explores adapting large language models (LLMs) for image classification using contrastive learning, improving their performance and retaining their generative abilities.</p><hr><h3>GVFs in the Real World: Making Predictions Online for Water Treatment</h3>
<p>Muhammad Kamran Janjua,Haseeb Shah,Martha White,Erfan Miahi,Marlos C. Machado,Adam White</p>
<p><a href='http://arxiv.org/abs/2312.01624v1'>http://arxiv.org/abs/2312.01624v1</a></p>
<p><b>Compressor summary</b>: The paper explores using reinforcement learning for predicting water treatment plant operations and shows that online learning improves prediction accuracy.</p><hr><h3>Universal Segmentation at Arbitrary Granularity with Language  Instruction</h3>
<p>Yong Liu,Cairong Zhang,Yitong Wang,Jiahao Wang,Yujiu Yang,Yansong Tang</p>
<p><a href='http://arxiv.org/abs/2312.01623v1'>http://arxiv.org/abs/2312.01623v1</a></p>
<p><b>Compressor summary</b>: The paper introduces UniLSeg, a universal segmentation model that can perform segmentation at any semantic level using language instructions and a unified data format.</p><hr><h3>How Many Validation Labels Do You Need? Exploring the Design Space of  Label-Efficient Model Ranking</h3>
<p>Zhengyu Hu,Jieyu Zhang,Yue Yu,Yuchen Zhuang,Hui Xiong</p>
<p><a href='http://arxiv.org/abs/2312.01619v1'>http://arxiv.org/abs/2312.01619v1</a></p>
<p><b>Compressor summary</b>: LEMR is a framework that reduces annotation costs for model selection tasks by using ensemble methods, uncertainty sampling, and Z-score refinement, achieving comparable results to fully labeled datasets with less labeling effort.</p><hr><h3>SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation  System</h3>
<p>Yunfei Fan,Tianyu Zhao,Guidong Wang</p>
<p><a href='http://arxiv.org/abs/2312.01616v1'>http://arxiv.org/abs/2312.01616v1</a></p>
<p><b>Compressor summary</b>: The SchurVINS framework combines high accuracy and low computational complexity for visual inertial navigation systems by using a complete residual model and Schur complement.</p><hr><h3>xNeuSM: Explainable Neural Subgraph Matching with Graph Learnable  Multi-hop Attention Networks</h3>
<p>Duc Q. Nguyen,Thanh Toan Nguyen,Tho quan</p>
<p><a href='http://arxiv.org/abs/2312.01612v1'>http://arxiv.org/abs/2312.01612v1</a></p>
<p><b>Compressor summary</b>: The article introduces xNeuSM, an explainable neural subgraph matching method that adapts attention factors for each node and improves prediction accuracy and query time compared to existing methods.</p><hr><h3>Deep Learning-Driven Enhancement of Welding Quality Control: Predicting  Welding Depth and Pore Volume in Hairpin Welding</h3>
<p>Amena Darwish,Stefan Ericson,Rohollah Ghasemi,Tobias Andersson,Dan Lönn,Andreas Andersson Lassila,Kent Salomonsson</p>
<p><a href='http://arxiv.org/abs/2312.01606v1'>http://arxiv.org/abs/2312.01606v1</a></p>
<p><b>Compressor summary</b>: The study proposes a deep learning model that predicts two critical weld characteristics using various laser welding input factors and shows promising results for improving welding quality assurance.</p><hr><h3>TextAug: Test time Text Augmentation for Multimodal Person  Re-identification</h3>
<p>Mulham Fawakherji,Eduard Vazquez,Pasquale Giampa,Binod Bhattarai</p>
<p><a href='http://arxiv.org/abs/2312.01605v1'>http://arxiv.org/abs/2312.01605v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses a new text augmentation technique called CutMixOut, which combines cutout and cutmix methods to improve multimodal person re-identification performance.</p><hr><h3>Local-Global History-aware Contrastive Learning for Temporal Knowledge  Graph Reasoning</h3>
<p>Wei Chen,Huaiyu Wan,Yuting Wu,Shuyuan Zhao,Jiayaqi Cheng,Yuxin Li,Youfang Lin</p>
<p><a href='http://arxiv.org/abs/2312.01601v1'>http://arxiv.org/abs/2312.01601v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method, LogCL, for predicting future facts in temporal knowledge graphs by using contrastive learning to fuse local and global historical information and improving robustness against noise.</p><hr><h3>Good Questions Help Zero-Shot Image Reasoning</h3>
<p>Kaiwen Yang,Tao Shen,Xinmei Tian,Xiubo Geng,Chongyang Tao,Dacheng Tao,Tianyi Zhou</p>
<p><a href='http://arxiv.org/abs/2312.01598v1'>http://arxiv.org/abs/2312.01598v1</a></p>
<p><b>Compressor summary</b>: The paragraph introduces QVix, a new prompting strategy for large vision-language models to improve their zero-shot image reasoning capabilities by asking more detailed questions about the input images.</p><hr><h3>SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference</h3>
<p>Feng Wang,Jieru Mei,Alan Yuille</p>
<p><a href='http://arxiv.org/abs/2312.01597v1'>http://arxiv.org/abs/2312.01597v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel self-attention mechanism called Correlative Self-Attention (CSA) that adapts CLIP for zero-shot semantic segmentation, achieving significant improvements over existing methods.</p><hr><h3>Expand BERT Representation with Visual Information via Grounded Language  Learning with Multimodal Partial Alignment</h3>
<p>Cong-Duy Nguyen,The-Anh Vu-Le,Thong Nguyen,Tho Quan,Luu Anh Tuan</p>
<p><a href='http://arxiv.org/abs/2312.01592v1'>http://arxiv.org/abs/2312.01592v1</a></p>
<p><b>Compressor summary</b>: The paper introduces GroundedBERT, a method that combines BERT with visual information using Optimal Transport to improve language learning for grounded tasks.</p><hr><h3>OCGEC: One-class Graph Embedding Classification for DNN Backdoor  Detection</h3>
<p>Haoyu Jiang,Haiyang Yu,Nan Li,Ping Yi</p>
<p><a href='http://arxiv.org/abs/2312.01585v1'>http://arxiv.org/abs/2312.01585v1</a></p>
<p><b>Compressor summary</b>: The study proposes OCGEC, a novel one-class classification framework using graph neural networks for model-level backdoor detection in DNNs with minimal clean data and achieves high AUC scores.</p><hr><h3>Explaining with Contrastive Phrasal Highlighting: A Case Study in  Assisting Humans to Detect Translation Differences</h3>
<p>Eleftheria Briakou,Navita Goyal,Marine Carpuat</p>
<p><a href='http://arxiv.org/abs/2312.01582v1'>http://arxiv.org/abs/2312.01582v1</a></p>
<p><b>Compressor summary</b>: The authors propose a technique to generate contrastive highlights for explaining predictions of semantic divergence models, which improves on existing saliency methods in capturing fine-grained meaning differences.</p><hr><h3>Signed Binarization: Unlocking Efficiency Through Repetition-Sparsity  Trade-Off</h3>
<p>Sachit Kuhar,Yash Jain,Alexey Tumanov</p>
<p><a href='http://arxiv.org/abs/2312.01581v1'>http://arxiv.org/abs/2312.01581v1</a></p>
<p><b>Compressor summary</b>: Signed Binarization is a framework that improves accuracy and efficiency of DNNs on edge devices by combining hardware-software systems, quantization functions, and representation learning techniques to balance repetition and sparsity during inference.</p><hr><h3>RJHMC-Tree for Exploration of the Bayesian Decision Tree Posterior</h3>
<p>Jodie A. Cochrane,Adrian G. Wills,Sarah J. Johnson</p>
<p><a href='http://arxiv.org/abs/2312.01577v1'>http://arxiv.org/abs/2312.01577v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm for learning Bayesian decision trees using Hamiltonian Monte Carlo (HMC) to improve efficiency and exploration of the posterior.</p><hr><h3>Learning Efficient Unsupervised Satellite Image-based Building Damage  Detection</h3>
<p>Yiyun Zhang,Zijian Wang,Yadan Luo,Xin Yu,Zi Huang</p>
<p><a href='http://arxiv.org/abs/2312.01576v1'>http://arxiv.org/abs/2312.01576v1</a></p>
<p><b>Compressor summary</b>: The paper proposes U-BDD++, a self-supervised framework for detecting building damage from unlabelled satellite images, using vision-language models to handle domain-specific issues and improve training quality.</p><hr><h3>A Challenging Multimodal Video Summary: Simultaneously Extracting and  Generating Keyframe-Caption Pairs from Video</h3>
<p>Keito Kudo,Haruki Nagasawa,Jun Suzuki,Nobuyuki Shimizu</p>
<p><a href='http://arxiv.org/abs/2312.01575v1'>http://arxiv.org/abs/2312.01575v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new video summarization task that combines keyframe selection and caption generation, creates a dataset to evaluate it, and proposes a practical application for the task.</p><hr><h3>How to Configure Good In-Context Sequence for Visual Question Answering</h3>
<p>Li Li,Jiawei Peng,Huiyi Chen,Chongyang Gao,Xu Yang</p>
<p><a href='http://arxiv.org/abs/2312.01571v1'>http://arxiv.org/abs/2312.01571v1</a></p>
<p><b>Compressor summary</b>: The study explores diverse in-context configurations for Large Vision-Language Models using Visual Question Answering and improves their performance, while gaining insights into the inner properties of these models.</p><hr><h3>Toward Automated Quantum Variational Machine Learning</h3>
<p>Omer Subasi</p>
<p><a href='http://arxiv.org/abs/2312.01567v1'>http://arxiv.org/abs/2312.01567v1</a></p>
<p><b>Compressor summary</b>: The paper presents MUSE, a search algorithm for quantum variational machine learning that improves accuracy in classification and regression tasks compared to previous methods.</p><hr><h3>APoLLo: Unified Adapter and Prompt Learning for Vision Language Models</h3>
<p>Sanjoy Chowdhury,Sayan Nag,Dinesh Manocha</p>
<p><a href='http://arxiv.org/abs/2312.01564v1'>http://arxiv.org/abs/2312.01564v1</a></p>
<p><b>Compressor summary</b>: APoLLo is a method that combines adapter and prompt learning for vision-language models, improving their generalization in few-shot settings by using trainable cross-attention layers and enforcing encoder consistency.</p><hr><h3>Multi-View Person Matching and 3D Pose Estimation with Arbitrary  Uncalibrated Camera Networks</h3>
<p>Yan Xu,Kris Kitani</p>
<p><a href='http://arxiv.org/abs/2312.01561v1'>http://arxiv.org/abs/2312.01561v1</a></p>
<p><b>Compressor summary</b>: PME is a method for cross-view person matching and 3D human pose estimation in multi-camera networks without requiring 3D data or camera poses, using clustering and geometric constraints to solve the problem.</p><hr><h3>Hyperspectral Image Compression Using Sampling and Implicit Neural  Representations</h3>
<p>Shima Rezasoltani,Faisal Z. Qureshi</p>
<p><a href='http://arxiv.org/abs/2312.01558v1'>http://arxiv.org/abs/2312.01558v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a hyperspectral image compression method using neural networks that outperforms existing methods at low bitrates and is faster with sampling.</p><hr><h3>Explainable AI is Responsible AI: How Explainability Creates Trustworthy  and Socially Responsible Artificial Intelligence</h3>
<p>Stephanie Baker,Wei Xiang</p>
<p><a href='http://arxiv.org/abs/2312.01555v1'>http://arxiv.org/abs/2312.01555v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses how explainable AI (XAI) is not only important for transparency but also crucial for ensuring fairness, robustness, privacy, security, and transparency in various applications of responsible AI (RAI).</p><hr><h3>The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context  Learning</h3>
<p>Bill Yuchen Lin,Abhilasha Ravichander,Ximing Lu,Nouha Dziri,Melanie Sclar,Khyathi Chandu,Chandra Bhagavatula,Yejin Choi</p>
<p><a href='http://arxiv.org/abs/2312.01552v1'>http://arxiv.org/abs/2312.01552v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses a study (LIMA) that shows alignment tuning in large language models may be superficial, as base models and aligned versions perform similarly on most tokens, and proposes a new method (URIAL) for tuning-free alignment using in-context learning.</p><hr><h3>KEEC: Embed to Control on An Equivariant Geometry</h3>
<p>Xiaoyuan Cheng,Yiming Yang,Wei Jiang,Yukun Hu</p>
<p><a href='http://arxiv.org/abs/2312.01544v1'>http://arxiv.org/abs/2312.01544v1</a></p>
<p><b>Compressor summary</b>: The paper presents KEEC, a method for learning and controlling dynamical systems with complex dynamics using equivariant geometry, which achieves quadratic convergence and outperforms other loss functions.</p>