
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-07-03</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-07-03 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via  Dynamic Sparse Attention</h3>
<p><a href='http://arxiv.org/abs/2407.02490v1'>http://arxiv.org/abs/2407.02490v1</a></p>
<p><b>Compressor summary</b>: Minference is a sparse calculation method for efficient inference of long-context LLMs that reduces latency by up to 10x while maintaining accuracy.</p><hr><h3>Magic Insert: Style-Aware Drag-and-Drop</h3>
<p><a href='http://arxiv.org/abs/2407.02489v1'>http://arxiv.org/abs/2407.02489v1</a></p>
<p><b>Compressor summary</b>: Magic Insert is a technique that lets users insert realistic objects from one image into another with different style by fine-tuning a text-to-image model, infusing it with the target style, and adapting an object insertion model to diverse artistic styles.</p><hr><h3>Neurocache: Efficient Vector Retrieval for Long-range Language Modeling</h3>
<p><a href='http://arxiv.org/abs/2407.02486v1'>http://arxiv.org/abs/2407.02486v1</a></p>
<p><b>Compressor summary</b>: Neurocache extends the context of large language models using a cache to store past states, improving inference speed and accuracy in various tasks.</p><hr><h3>RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in  LLMs</h3>
<p><a href='http://arxiv.org/abs/2407.02485v1'>http://arxiv.org/abs/2407.02485v1</a></p>
<p><b>Compressor summary</b>: The paper proposes RankRAG, a framework that fine-tunes large language models for ranking contexts and generating answers in retrieval-augmented generation tasks, achieving better performance than existing models with less data.</p><hr><h3>MMedAgent: Learning to Use Medical Tools with Multi-modal Agent</h3>
<p><a href='http://arxiv.org/abs/2407.02483v1'>http://arxiv.org/abs/2407.02483v1</a></p>
<p><b>Compressor summary</b>: The paper presents MMedAgent, an AI agent for the medical domain that selects appropriate specialized models as tools based on user inputs and outperforms existing methods.</p><hr><h3>Boosting Consistency in Story Visualization with Rich-Contextual  Conditional Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2407.02482v1'>http://arxiv.org/abs/2407.02482v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method, Rich-contextual Conditional Diffusion Models (RCDMs), that improves story generation by using semantic and temporal context from known clips and reference images.</p><hr><h3>Understanding Alignment in Multimodal LLMs: A Comprehensive Study</h3>
<p><a href='http://arxiv.org/abs/2407.02477v1'>http://arxiv.org/abs/2407.02477v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes preference alignment methods for Multimodal Large Language Models (MLLMs), compares offline and online approaches, introduces a new dataset creation method called Bias-Driven Hallucination Sampling (BDHS), and shows its competitive performance.</p><hr><h3>Scalable Multi-Output Gaussian Processes with Stochastic Variational  Inference</h3>
<p><a href='http://arxiv.org/abs/2407.02476v1'>http://arxiv.org/abs/2407.02476v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to speed up computations in a model that uses latent variables to capture covariance between multiple outputs.</p><hr><h3>Free Energy in a Circumplex Model of Emotion</h3>
<p><a href='http://arxiv.org/abs/2407.02474v1'>http://arxiv.org/abs/2407.02474v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a two-dimensional model of emotion based on free energy, valence, and arousal, and demonstrates its application in simulating agents' emotions during a search task.</p><hr><h3>ValueScope: Unveiling Implicit Norms and Values via Return Potential  Model of Social Interactions</h3>
<p><a href='http://arxiv.org/abs/2407.02472v1'>http://arxiv.org/abs/2407.02472v1</a></p>
<p><b>Compressor summary</b>: ValueScope is a framework that uses language models to analyze and compare social norms across different online communities, revealing their diversity and evolution.</p><hr><h3>PWM: Policy Learning with Large World Models</h3>
<p><a href='http://arxiv.org/abs/2407.02466v1'>http://arxiv.org/abs/2407.02466v1</a></p>
<p><b>Compressor summary</b>: Policy Learning with large World Models (PWM) is a new model-based RL algorithm that learns continuous control policies from large multi-task world models, enabling efficient solution of complex tasks with many actions and tasks without the need for online planning.</p><hr><h3>Belief sharing: a blessing or a curse</h3>
<p><a href='http://arxiv.org/abs/2407.02465v1'>http://arxiv.org/abs/2407.02465v1</a></p>
<p><b>Compressor summary</b>: The paper explores how agents can communicate their beliefs more effectively to avoid echo chambers and self-doubt in collaborative tasks.</p><hr><h3>SUPER: Seated Upper Body Pose Estimation using mmWave Radars</h3>
<p><a href='http://arxiv.org/abs/2407.02455v1'>http://arxiv.org/abs/2407.02455v1</a></p>
<p><b>Compressor summary</b>: SUPER is a framework that uses dual-mmWave radars to estimate seated upper body human poses and outperforms existing methods by a large margin.</p><hr><h3>Ensemble of pre-trained language models and data augmentation for hate  speech detection from Arabic tweets</h3>
<p><a href='http://arxiv.org/abs/2407.02448v1'>http://arxiv.org/abs/2407.02448v1</a></p>
<p><b>Compressor summary</b>: The study proposes a new method for detecting hate speech in Arabic tweets using ensemble learning and semi-supervised learning, which improves accuracy over existing approaches.</p><hr><h3>PLeaS -- Merging Models with Permutations and Least Squares</h3>
<p><a href='http://arxiv.org/abs/2407.02447v1'>http://arxiv.org/abs/2407.02447v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm, PLeaS, that can merge different machine learning models without sharing data or base architecture, improving performance by 8 to 15 percentage points for certain tasks.</p><hr><h3>Predicting vs. Acting: A Trade-off Between World Modeling & Agent  Modeling</h3>
<p><a href='http://arxiv.org/abs/2407.02446v1'>http://arxiv.org/abs/2407.02446v1</a></p>
<p><b>Compressor summary</b>: RLHF models excel at text generation but struggle with world modeling due to their reliance on implicit blueprints for coherence.</p><hr><h3>Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry,  Texture, and PBR Materials</h3>
<p><a href='http://arxiv.org/abs/2407.02445v1'>http://arxiv.org/abs/2407.02445v1</a></p>
<p><b>Compressor summary</b>: AssetGen is a text-to-3D generation system that produces high-quality meshes with realistic textures and materials using few views and human preferable results.</p><hr><h3>Predicting Visual Attention in Graphic Design Documents</h3>
<p><a href='http://arxiv.org/abs/2407.02439v1'>http://arxiv.org/abs/2407.02439v1</a></p>
<p><b>Compressor summary</b>: The paper presents a model that predicts how people pay attention to graphic design documents, considering both the spatial and temporal aspects of visual fixation using deep learning techniques.</p><hr><h3>Parameter Matching Attack: Enhancing Practical Applicability of  Availability Attacks</h3>
<p><a href='http://arxiv.org/abs/2407.02437v1'>http://arxiv.org/abs/2407.02437v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Parameter Matching Attack (PMA), a new availability attack for machine learning models that can degrade their performance even when only partially perturbed data is used.</p><hr><h3>Evaluating the Robustness of Adverse Drug Event Classification Models  Using Templates</h3>
<p><a href='http://arxiv.org/abs/2407.02432v1'>http://arxiv.org/abs/2407.02432v1</a></p>
<p><b>Compressor summary</b>: The text discusses the challenges of evaluating ADE detection models in social media using hand-crafted templates for four capabilities.</p><hr><h3>On the Robustness of Graph Reduction Against GNN Backdoor</h3>
<p><a href='http://arxiv.org/abs/2407.02431v1'>http://arxiv.org/abs/2407.02431v1</a></p>
<p><b>Compressor summary</b>: Graph reduction methods' effectiveness in mitigating backdoor attacks on GNNs varies significantly and some even worsen the situation, raising concerns about security trade-offs in scalable GNN training.</p><hr><h3>Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D  Objects</h3>
<p><a href='http://arxiv.org/abs/2407.02430v1'>http://arxiv.org/abs/2407.02430v1</a></p>
<p><b>Compressor summary</b>: Meta 3D TextureGen is a fast and high-quality method for generating consistent textures on complex 3D objects using text-to-image networks and 3D semantics.</p><hr><h3>Reinforcement Learning and Machine ethics:a systematic review</h3>
<p><a href='http://arxiv.org/abs/2407.02425v1'>http://arxiv.org/abs/2407.02425v1</a></p>
<p><b>Compressor summary</b>: The text is a systematic review of how reinforcement learning can help achieve ethical behavior in autonomous systems.</p><hr><h3>A Pattern Language for Machine Learning Tasks</h3>
<p><a href='http://arxiv.org/abs/2407.02424v1'>http://arxiv.org/abs/2407.02424v1</a></p>
<p><b>Compressor summary</b>: The authors propose a graphical language for designing and unifying machine learning tasks, and introduce "manipulators", a novel task that converts classifiers into generative models without custom architectures or adversarial training.</p><hr><h3>On the Anatomy of Attention</h3>
<p><a href='http://arxiv.org/abs/2407.02423v1'>http://arxiv.org/abs/2407.02423v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new way to visualize and compare machine learning models using diagrams, and applies it to study different types of attention mechanisms in depth.</p><hr><h3>Close, But Not There: Boosting Geographic Distance Sensitivity in Visual  Place Recognition</h3>
<p><a href='http://arxiv.org/abs/2407.02422v1'>http://arxiv.org/abs/2407.02422v1</a></p>
<p><b>Compressor summary</b>: The paper proposes CliqueMining, a novel mining strategy that improves Visual Place Recognition by selecting examples from visually similar image cliques, boosting recall@1 on two benchmarks.</p><hr><h3>Video Watermarking: Safeguarding Your Video from (Unauthorized)  Annotations by Video-based LLMs</h3>
<p><a href='http://arxiv.org/abs/2407.02411v1'>http://arxiv.org/abs/2407.02411v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Video Watermarking, a technique to protect videos from unauthorized annotations by video-based LLMs, by embedding imperceptible watermarks into key frames and preserving the viewing experience.</p><hr><h3>CEB: Compositional Evaluation Benchmark for Fairness in Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2407.02408v1'>http://arxiv.org/abs/2407.02408v1</a></p>
<p><b>Compressor summary</b>: The authors propose CEB, a benchmark for evaluating various types of biases in large language models across different social groups and tasks, using a compositional taxonomy.</p><hr><h3>Face Reconstruction Transfer Attack as Out-of-Distribution  Generalization</h3>
<p><a href='http://arxiv.org/abs/2407.02403v1'>http://arxiv.org/abs/2407.02403v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to reconstruct face images that can fool face recognition systems on unseen encoders by using Averaged Latent Search and Unsupervised Validation with pseudo target (ALSUV).</p><hr><h3>Consistency Flow Matching: Defining Straight Flows with Velocity  Consistency</h3>
<p><a href='http://arxiv.org/abs/2407.02398v1'>http://arxiv.org/abs/2407.02398v1</a></p>
<p><b>Compressor summary</b>: Consistency-FM is a novel method that improves flow matching by enforcing self-consistency in the velocity field and using multi-segment training, resulting in faster convergence and better sample quality.</p><hr><h3>Learning to Refine with Fine-Grained Natural Language Feedback</h3>
<p><a href='http://arxiv.org/abs/2407.02397v1'>http://arxiv.org/abs/2407.02397v1</a></p>
<p><b>Compressor summary</b>: The authors propose a refinement approach with feedback that separates identification of bad generations, feedback generation, and refining with feedback in large language models to improve factual consistency in document grounded summaries.</p><hr><h3>Similarity Distance-Based Label Assignment for Tiny Object Detection</h3>
<p><a href='http://arxiv.org/abs/2407.02394v1'>http://arxiv.org/abs/2407.02394v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a label assignment strategy called SimD for tiny object detection that considers location and shape similarity between bounding boxes and adapts to different datasets and object sizes.</p><hr><h3>TokenPacker: Efficient Visual Projector for Multimodal LLM</h3>
<p><a href='http://arxiv.org/abs/2407.02392v1'>http://arxiv.org/abs/2407.02392v1</a></p>
<p><b>Compressor summary</b>: The proposed visual projector uses a coarse-to-fine scheme to generate condensed visual tokens for MLLMs, improving efficiency and reasoning capabilities.</p><hr><h3>SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring  Expression Segmentation</h3>
<p><a href='http://arxiv.org/abs/2407.02389v1'>http://arxiv.org/abs/2407.02389v1</a></p>
<p><b>Compressor summary</b>: SafaRi is a weakly-supervised bootstrapping architecture for Referring Expression Segmentation that uses less annotations, improves image-text alignment, and performs well in unseen scenarios.</p><hr><h3>Real HSI-MSI-PAN image dataset for the  hyperspectral/multi-spectral/panchromatic image fusion and super-resolution  fields</h3>
<p><a href='http://arxiv.org/abs/2407.02387v1'>http://arxiv.org/abs/2407.02387v1</a></p>
<p><b>Compressor summary</b>: The authors release a real hyperspectral image dataset to improve fusion algorithm development and comparison, as existing simulated datasets have inaccuracies.</p><hr><h3>OpenSlot: Mixed Open-set Recognition with Object-centric Learning</h3>
<p><a href='http://arxiv.org/abs/2407.02386v1'>http://arxiv.org/abs/2407.02386v1</a></p>
<p><b>Compressor summary</b>: The paper introduces OpenSlot, a framework for open-set recognition that handles multiple class semantics and reduces noise, achieving state-of-the-art performance on conventional and mixed tasks.</p><hr><h3>OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video  Generation</h3>
<p><a href='http://arxiv.org/abs/2407.02371v1'>http://arxiv.org/abs/2407.02371v1</a></p>
<p><b>Compressor summary</b>: The authors introduce a new high-quality dataset (OpenVid-1M) for text-to-video generation, along with a novel transformer model (MVDiT) that leverages both visual and textual information.</p><hr><h3>Investigating Event-Based Cameras for Video Frame Interpolation in  Sports</h3>
<p><a href='http://arxiv.org/abs/2407.02370v1'>http://arxiv.org/abs/2407.02370v1</a></p>
<p><b>Compressor summary</b>: This paper explores using event-based cameras to create affordable slow-motion sports videos with deep learning techniques.</p><hr><h3>Two-Step Q-Learning</h3>
<p><a href='http://arxiv.org/abs/2407.02369v1'>http://arxiv.org/abs/2407.02369v1</a></p>
<p><b>Compressor summary</b>: Two-step Q-learning is a novel off-policy algorithm that converges almost surely to optimal Q-values, outperforming existing methods on benchmark problems.</p><hr><h3>GCF: Graph Convolutional Networks for Facial Expression Recognition</h3>
<p><a href='http://arxiv.org/abs/2407.02361v1'>http://arxiv.org/abs/2407.02361v1</a></p>
<p><b>Compressor summary</b>: GCF is a novel approach that uses Graph Convolutional Networks to improve Facial Expression Recognition by enhancing local CNN features with global features, achieving significant performance improvements over state-of-the-art methods on benchmark datasets.</p><hr><h3>Talking to Machines: do you read me?</h3>
<p><a href='http://arxiv.org/abs/2407.02354v1'>http://arxiv.org/abs/2407.02354v1</a></p>
<p><b>Compressor summary</b>: The dissertation covers the author's research on dialogue systems, from modular architectures to end-to-end deep neural networks, and presents contributions to task-oriented dialogues, conversational QA, and large language models for multimodal dialogue.</p><hr><h3>Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition  and Program of Thought Verification</h3>
<p><a href='http://arxiv.org/abs/2407.02352v1'>http://arxiv.org/abs/2407.02352v1</a></p>
<p><b>Compressor summary</b>: Pelican is a framework that detects and mitigates hallucinations in large visual language models by decomposing claims into sub-claims, generating Python code for answering questions, and verifying the correctness of the claim using reasoning abilities.</p><hr><h3>Generative Large Language Models in Automated Fact-Checking: A Survey</h3>
<p><a href='http://arxiv.org/abs/2407.02351v1'>http://arxiv.org/abs/2407.02351v1</a></p>
<p><b>Compressor summary</b>: The paper explores how large language models can help fact-checkers identify false information online by using their knowledge and reasoning skills.</p><hr><h3>Conceptual Codebook Learning for Vision-Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.02350v1'>http://arxiv.org/abs/2407.02350v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CoCoLe, a method to improve vision-language models' generalization by learning a codebook of visual concepts linked to text encoder inputs for few-shot classification tasks.</p><hr><h3>Revisiting Cascaded Ensembles for Efficient Inference</h3>
<p><a href='http://arxiv.org/abs/2407.02348v1'>http://arxiv.org/abs/2407.02348v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a simple adaptive inference scheme called cascade of ensembles (CoE) that uses ensemble agreement to route examples through different models, achieving efficiency gains and reducing costs.</p><hr><h3>MORPHEUS: Modeling Role from Personalized Dialogue History by Exploring  and Utilizing Latent Space</h3>
<p><a href='http://arxiv.org/abs/2407.02345v1'>http://arxiv.org/abs/2407.02345v1</a></p>
<p><b>Compressor summary</b>: MORPHEUS is a novel framework for generating personalized dialogues that uses a persona codebook to represent roles in latent space and improve response generation without external role data.</p><hr><h3>RVISA: Reasoning and Verification for Implicit Sentiment Analysis</h3>
<p><a href='http://arxiv.org/abs/2407.02340v1'>http://arxiv.org/abs/2407.02340v1</a></p>
<p><b>Compressor summary</b>: The study proposes RVISA, a two-stage reasoning framework that combines generation and reasoning abilities of LLMs to identify implicit sentiment using three-hop reasoning prompting and a verification mechanism.</p><hr><h3>Open foundation models for Azerbaijani language</h3>
<p><a href='http://arxiv.org/abs/2407.02337v1'>http://arxiv.org/abs/2407.02337v1</a></p>
<p><b>Compressor summary</b>: This paper presents new resources and benchmarks to advance open-source foundation models for Azerbaijani language understanding and generation.</p><hr><h3>CALICO: Confident Active Learning with Integrated Calibration</h3>
<p><a href='http://arxiv.org/abs/2407.02335v1'>http://arxiv.org/abs/2407.02335v1</a></p>
<p><b>Compressor summary</b>: CALICO is an active learning framework that self-calibrates confidence for sample selection in deep neural networks, improving classification performance with limited labeled data.</p><hr><h3>Why do LLaVA Vision-Language Models Reply to Images in English?</h3>
<p><a href='http://arxiv.org/abs/2407.02333v1'>http://arxiv.org/abs/2407.02333v1</a></p>
<p><b>Compressor summary</b>: The paper investigates a multilingual bias in vision-language models and suggests that switching the language backbone and intervening on attention layers can reduce it.</p><hr><h3>MIGC++: Advanced Multi-Instance Generation Controller for Image  Synthesis</h3>
<p><a href='http://arxiv.org/abs/2407.02329v1'>http://arxiv.org/abs/2407.02329v1</a></p>
<p><b>Compressor summary</b>: The Multi-Instance Generation (MIG) task involves generating multiple instances in an image with specific attributes, and the proposed methods MIGC, MIGC++, and Consistent-MIG improve control, diversity, and consistency in this task.</p><hr><h3>Efficient Sparse Attention needs Adaptive Token Release</h3>
<p><a href='http://arxiv.org/abs/2407.02328v1'>http://arxiv.org/abs/2407.02328v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve the efficiency and scalability of large language models by adaptively sparsifying attention and rebuilding discarded tokens as needed, achieving significant throughput improvement in natural language generation tasks.</p><hr><h3>QSync: Quantization-Minimized Synchronous Distributed Training Across  Hybrid Devices</h3>
<p><a href='http://arxiv.org/abs/2407.02327v1'>http://arxiv.org/abs/2407.02327v1</a></p>
<p><b>Compressor summary</b>: QSync is a system that enables efficient DNN training on hybrid devices by selecting optimal quantized operators based on device resource capacities and synchronizing workers with minimized accuracy degradation.</p><hr><h3>Stochastic Differential Equations models for Least-Squares Stochastic  Gradient Descent</h3>
<p><a href='http://arxiv.org/abs/2407.02322v1'>http://arxiv.org/abs/2407.02322v1</a></p>
<p><b>Compressor summary</b>: The text studies SGD dynamics for the least-square problem using SDEs in different settings and provides convergence rates, stationary distribution properties, and numerical simulations.</p><hr><h3>Exploring the Role of Transliteration in In-Context Learning for  Low-resource Languages Written in Non-Latin Scripts</h3>
<p><a href='http://arxiv.org/abs/2407.02320v1'>http://arxiv.org/abs/2407.02320v1</a></p>
<p><b>Compressor summary</b>: Transliteration can improve low-resource language performance in LLMs across various tasks, but its effectiveness depends on the task and model size.</p><hr><h3>Soft Language Prompts for Language Transfer</h3>
<p><a href='http://arxiv.org/abs/2407.02317v1'>http://arxiv.org/abs/2407.02317v1</a></p>
<p><b>Compressor summary</b>: The study explores how to improve cross-lingual NLP applications by using fine-tuning methods along with language-specific and task-specific adapters and soft prompts, finding that combining a soft language prompt with a task adapter often works best.</p><hr><h3>VFIMamba: Video Frame Interpolation with State Space Models</h3>
<p><a href='http://arxiv.org/abs/2407.02315v1'>http://arxiv.org/abs/2407.02315v1</a></p>
<p><b>Compressor summary</b>: VFIMamba is a novel frame interpolation method that uses Selective State Space Models (S6) to efficiently model intermediate frames in videos, achieving state-of-the-art performance in high-resolution scenarios.</p><hr><h3>Evaluating the Ability of LLMs to Solve Semantics-Aware Process Mining  Tasks</h3>
<p><a href='http://arxiv.org/abs/2407.02310v1'>http://arxiv.org/abs/2407.02310v1</a></p>
<p><b>Compressor summary</b>: Large language models can perform well on semantics-aware process mining tasks after fine-tuning, while struggling without it.</p><hr><h3>Semantically Guided Representation Learning For Action Anticipation</h3>
<p><a href='http://arxiv.org/abs/2407.02309v1'>http://arxiv.org/abs/2407.02309v1</a></p>
<p><b>Compressor summary</b>: S-GEAR is a novel framework that learns action representations considering their semantic interconnectedness and improves action anticipation performance on several benchmarks.</p><hr><h3>Towards Human Understanding of Paraphrase Types in ChatGPT</h3>
<p><a href='http://arxiv.org/abs/2407.02302v1'>http://arxiv.org/abs/2407.02302v1</a></p>
<p><b>Compressor summary</b>: The study evaluates ChatGPT's ability to generate English paraphrases using different linguistic changes and introduces APTY, a dataset for improving language models.</p><hr><h3>CFinBench: A Comprehensive Chinese Financial Benchmark for Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.02301v1'>http://arxiv.org/abs/2407.02301v1</a></p>
<p><b>Compressor summary</b>: CFinBench is a benchmark to test Chinese LLMs' financial knowledge on various topics, tasks, and certifications with 99,100 questions.</p><hr><h3>Rethinking Data Augmentation for Robust LiDAR Semantic Segmentation in  Adverse Weather</h3>
<p><a href='http://arxiv.org/abs/2407.02286v1'>http://arxiv.org/abs/2407.02286v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Selective Jittering and Learnable Point Drop data augmentation techniques to improve LiDAR semantic segmentation performance in adverse weather conditions by addressing geometric perturbation and point drop issues.</p><hr><h3>Renard: A Modular Pipeline for Extracting Character Networks from  Narrative Texts</h3>
<p><a href='http://arxiv.org/abs/2407.02284v1'>http://arxiv.org/abs/2407.02284v1</a></p>
<p><b>Compressor summary</b>: Renard is a Python library for creating custom NLP pipelines to analyze dynamic and static networks of characters in narrative texts.</p><hr><h3>A Refreshed Similarity-based Upsampler for Direct High-Ratio Feature  Upsampling</h3>
<p><a href='http://arxiv.org/abs/2407.02283v1'>http://arxiv.org/abs/2407.02283v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Feature upsampling is essential for image segmentation tasks
- Existing similarity-based feature upsampling pipeline has limitations in alignment, similarity calculation, and neighbor selection
- ReSFU addresses these limitations with improved feature alignment, flexible similarity calculation, and fine-grained neighbor selection
- ReSFU works well on different architectures and segmentation applications

Summary:
ReSFU is a novel feature upsampling framework for image segmentation that overcomes the limitations of existing methods by aligning features better, calculating similarity flexibly, and selecting neighbors finely. It works well on various network structures and tasks.</p><hr><h3>How to Boost Any Loss Function</h3>
<p><a href='http://arxiv.org/abs/2407.02279v1'>http://arxiv.org/abs/2407.02279v1</a></p>
<p><b>Compressor summary</b>: Boosting can optimize any loss function without needing first-order information or smoothness conditions, using tools from quantum calculus.</p><hr><h3>Learning Paradigms and Modelling Methodologies for Digital Twins in  Process Industry</h3>
<p><a href='http://arxiv.org/abs/2407.02275v1'>http://arxiv.org/abs/2407.02275v1</a></p>
<p><b>Compressor summary</b>: This paper reviews various learning paradigms and methodologies used for creating digital twins in the process industry, and identifies challenges and future directions.</p><hr><h3>Multilingual Trolley Problems for Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.02273v1'>http://arxiv.org/abs/2407.02273v1</a></p>
<p><b>Compressor summary</b>: The study explores how large language models make moral decisions across various languages and cultures, finding that their alignment with human preferences varies depending on the language.</p><hr><h3>Aligning Human Motion Generation with Human Perceptions</h3>
<p><a href='http://arxiv.org/abs/2407.02272v1'>http://arxiv.org/abs/2407.02272v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new method to generate more realistic human motions by introducing a large dataset and a model that captures human preferences, which can be integrated into the generation pipeline.</p><hr><h3>Improving Explainability of Softmax Classifiers Using a Prototype-Based  Joint Embedding Method</h3>
<p><a href='http://arxiv.org/abs/2407.02271v1'>http://arxiv.org/abs/2407.02271v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve explainability and uncertainty estimation in softmax classifiers using prototype-based predictions and similarity learning.</p><hr><h3>DrugCLIP: Contrastive Drug-Disease Interaction For Drug Repurposing</h3>
<p><a href='http://arxiv.org/abs/2407.02265v1'>http://arxiv.org/abs/2407.02265v1</a></p>
<p><b>Compressor summary</b>: DrugCLIP is a contrastive learning method that uses machine learning to automatically discover new uses for existing drugs by analyzing drug and disease interactions in large datasets.</p><hr><h3>SOAF: Scene Occlusion-aware Neural Acoustic Field</h3>
<p><a href='http://arxiv.org/abs/2407.02264v1'>http://arxiv.org/abs/2407.02264v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Paper proposes SOAF, a new approach for novel view audio-visual synthesis in indoor scenes with sound-propagation modelling and scene transmittance learning
- SOAF generates binaural audio with directional attention using Fibonacci Sphere feature extraction
- SOAF outperforms previous techniques on real and synthetic datasets

Summary:
The paper presents SOAF, a novel method for generating binaural audio in indoor scenes with accurate sound propagation and directional attention, achieving superior results on real and synthetic data.</p><hr><h3>FreeCG: Free the Design Space of Clebsch-Gordan Transform for machine  learning force field</h3>
<p><a href='http://arxiv.org/abs/2407.02263v1'>http://arxiv.org/abs/2407.02263v1</a></p>
<p><b>Compressor summary</b>: The FreeCG method improves the Clebsch-Gordan Transform layer by using permutation-invariant abstract edges, group CG transform, sparse paths, and attention enhancement to achieve better force and property predictions for molecular datasets.</p><hr><h3>SiamTST: A Novel Representation Learning Framework for Enhanced  Multivariate Time Series Forecasting applied to Telco Networks</h3>
<p><a href='http://arxiv.org/abs/2407.02258v1'>http://arxiv.org/abs/2407.02258v1</a></p>
<p><b>Compressor summary</b>: SiamTST is a new way to learn from multivariate time series data using a special type of neural network and some tricks to improve accuracy.</p><hr><h3>Parameter-Selective Continual Test-Time Adaptation</h3>
<p><a href='http://arxiv.org/abs/2407.02253v1'>http://arxiv.org/abs/2407.02253v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method, Parameter-Selective Mean Teacher (PSMT), that updates only critical parameters in the Mean Teacher model to avoid error accumulation and catastrophic forgetting when adapting to changing environments.</p><hr><h3>GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion  Models and Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.02252v1'>http://arxiv.org/abs/2407.02252v1</a></p>
<p><b>Compressor summary</b>: The authors propose an end-to-end text rendering framework for poster generation using a triple cross-attention mechanism and a high-resolution dataset, aiming to create precise and contextually rich poster images.</p><hr><h3>EvolBA: Evolutionary Boundary Attack under Hard-label Black Box  condition</h3>
<p><a href='http://arxiv.org/abs/2407.02248v1'>http://arxiv.org/abs/2407.02248v1</a></p>
<p><b>Compressor summary</b>: The study proposes EvolBA, an adversarial attack method using CMA-ES under HL-BB condition, which can find AEs with smaller perturbations in images.</p><hr><h3>Robust Zero-Shot Text-to-Speech Synthesis with Reverse Inference  Optimization</h3>
<p><a href='http://arxiv.org/abs/2407.02243v1'>http://arxiv.org/abs/2407.02243v1</a></p>
<p><b>Compressor summary</b>: RIO uses reinforcement learning from human feedback to select exemplars that improve the robustness and quality of zero-shot text-to-speech systems by leveraging reverse inference based on the Bayesian principle.</p><hr><h3>Sign Language Recognition Based On Facial Expression and Hand Skeleton</h3>
<p><a href='http://arxiv.org/abs/2407.02241v1'>http://arxiv.org/abs/2407.02241v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a sign language recognition network that uses hand skeleton features and facial expressions to improve accuracy and robustness.</p><hr><h3>Towards a Holistic Framework for Multimodal Large Language Models in  Three-dimensional Brain CT Report Generation</h3>
<p><a href='http://arxiv.org/abs/2407.02235v1'>http://arxiv.org/abs/2407.02235v1</a></p>
<p><b>Compressor summary</b>: The text describes a study that developed a large language model called BrainGPT to generate accurate and informative 3D brain CT reports by addressing data complexity, model capacity, and evaluation metric issues, and demonstrated its clinical readiness through physician evaluations and a proposed FORTE metric.</p><hr><h3>Synthetic Multimodal Question Generation</h3>
<p><a href='http://arxiv.org/abs/2407.02233v1'>http://arxiv.org/abs/2407.02233v1</a></p>
<p><b>Compressor summary</b>: SMMQG is a framework for generating synthetic question-answer pairs from multimodal documents to evaluate MMRAG models, achieving high quality comparable to existing benchmarks.</p><hr><h3>LaMoD: Latent Motion Diffusion Model For Myocardial Strain Generation</h3>
<p><a href='http://arxiv.org/abs/2407.02229v1'>http://arxiv.org/abs/2407.02229v1</a></p>
<p><b>Compressor summary</b>: LaMoD is a novel deep learning model that predicts accurate DENSE motions from standard CMR videos for improved myocardial strain analysis in cardiac patients.</p><hr><h3>MTMamba: Enhancing Multi-Task Dense Scene Understanding by Mamba-Based  Decoders</h3>
<p><a href='http://arxiv.org/abs/2407.02228v1'>http://arxiv.org/abs/2407.02228v1</a></p>
<p><b>Compressor summary</b>: MTMamba is a new architecture for multi-task scene understanding that leverages Mamba to handle long-range dependency and model cross-task interactions, outperforming previous methods on NYUDv2 and PASCAL-Context datasets.</p><hr><h3>Detecting Driver Fatigue With Eye Blink Behavior</h3>
<p><a href='http://arxiv.org/abs/2407.02222v1'>http://arxiv.org/abs/2407.02222v1</a></p>
<p><b>Compressor summary</b>: The study evaluates an eye blink feature set to detect driver fatigue using camera-based solutions, which are non-intrusive and adapt to different drivers.</p><hr><h3>Multi-Modal Video Dialog State Tracking in the Wild</h3>
<p><a href='http://arxiv.org/abs/2407.02218v1'>http://arxiv.org/abs/2407.02218v1</a></p>
<p><b>Compressor summary</b>: MST-MIXER is a novel video dialog model that tracks multiple modalities and learns local latent graphs to improve performance on real-world scenarios.</p><hr><h3>Physics-Informed Model and Hybrid Planning for Efficient Dyna-Style  Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2407.02217v1'>http://arxiv.org/abs/2407.02217v1</a></p>
<p><b>Compressor summary</b>: This paper shows how using partial physical knowledge can improve reinforcement learning by enhancing sample efficiency, inference speed, and planning in real-world applications.</p><hr><h3>PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt  during Large Language Model Fine-tuning</h3>
<p><a href='http://arxiv.org/abs/2407.02211v1'>http://arxiv.org/abs/2407.02211v1</a></p>
<p><b>Compressor summary</b>: PromptIntern is a novel method that helps large language models learn prompt knowledge internally, reducing inference costs and increasing speed for complex natural language processing tasks.</p><hr><h3>Generative Monoculture in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.02209v1'>http://arxiv.org/abs/2407.02209v1</a></p>
<p><b>Compressor summary</b>: Generative monoculture is a phenomenon in large language models where they produce less diverse outputs than expected for certain tasks, which can have positive and negative consequences depending on the use case, and requires better alignment methods to avoid.</p><hr><h3>How to Learn in a Noisy World? Self-Correcting the Real-World Data Noise  on Machine Translation</h3>
<p><a href='http://arxiv.org/abs/2407.02208v1'>http://arxiv.org/abs/2407.02208v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-correction method to improve machine translation performance by using the model's prediction distribution to revise training supervision in the presence of semantic misalignment noise.</p><hr><h3>Automatic Adaptation Rule Optimization via Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.02203v1'>http://arxiv.org/abs/2407.02203v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using large language models to optimize rule-based self-adaptation systems by leveraging their common sense and reasoning abilities.</p><hr><h3>Research on Reliable and Safe Occupancy Grid Prediction in Underground  Parking Lots</h3>
<p><a href='http://arxiv.org/abs/2407.02197v1'>http://arxiv.org/abs/2407.02197v1</a></p>
<p><b>Compressor summary</b>: The study proposes a method to improve autonomous vehicle performance in complex indoor environments like underground parking lots using CARLA's simulation platform and an occupancy grid network.</p><hr><h3>Attack-Aware Noise Calibration for Differential Privacy</h3>
<p><a href='http://arxiv.org/abs/2407.02191v1'>http://arxiv.org/abs/2407.02191v1</a></p>
<p><b>Compressor summary</b>: The paper proposes methods to directly calibrate noise scale in differential privacy (DP) models based on attack risk, improving utility without sacrificing privacy.</p><hr><h3>Structure-Aware Consensus Network on Graphs with Few Labeled Nodes</h3>
<p><a href='http://arxiv.org/abs/2407.02188v1'>http://arxiv.org/abs/2407.02188v1</a></p>
<p><b>Compressor summary</b>: SACN is a novel graph node classification method that leverages structure-aware consensus learning between two augmented views, integrates structural information, and achieves strong performance especially at low label rates.</p><hr><h3>Virtually Objective Quantification of in vitro Wound Healing Scratch  Assays with the Segment Anything Model</h3>
<p><a href='http://arxiv.org/abs/2407.02187v1'>http://arxiv.org/abs/2407.02187v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a deep learning method using point-prompts for class-agnostic cell segmentation in the in vitro scratch assay, reducing subjectivity and increasing accuracy.</p><hr><h3>Occlusion-Aware Seamless Segmentation</h3>
<p><a href='http://arxiv.org/abs/2407.02182v1'>http://arxiv.org/abs/2407.02182v1</a></p>
<p><b>Compressor summary</b>: The authors introduce OASS, a new task that addresses challenges in panoramic image segmentation, present the BlendPASS dataset, and propose UnmaskFormer, a solution that uses Unmasking Attention and Amodal-oriented Mix to achieve state-of-the-art results.</p><hr><h3>BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event  Stream</h3>
<p><a href='http://arxiv.org/abs/2407.02174v1'>http://arxiv.org/abs/2407.02174v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to recover neural radiance fields (NeRF) from a single blurry image and its camera motion, enabling view-consistent sharp images and high-quality rendering.</p><hr><h3>WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation</h3>
<p><a href='http://arxiv.org/abs/2407.02165v1'>http://arxiv.org/abs/2407.02165v1</a></p>
<p><b>Compressor summary</b>: WildAvatar is a large dataset for creating 3D human avatars from YouTube videos, addressing the limitations of existing datasets and enabling real-world applications.</p><hr><h3>UltraPixel: Advancing Ultra-High-Resolution Image Synthesis to New Peaks</h3>
<p><a href='http://arxiv.org/abs/2407.02158v1'>http://arxiv.org/abs/2407.02158v1</a></p>
<p><b>Compressor summary</b>: UltraPixel is a novel architecture that efficiently generates high-quality images at multiple resolutions using cascade diffusion models, implicit neural representations, and scale-aware normalization layers.</p><hr><h3>FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression  Recognition with AdaptERs</h3>
<p><a href='http://arxiv.org/abs/2407.02157v1'>http://arxiv.org/abs/2407.02157v1</a></p>
<p><b>Compressor summary</b>: FineCLIPER is a novel framework that uses Multi-modal Fine-grained CLIP to recognize dynamic facial expressions with improved accuracy and adaptability by extending class labels, using hierarchical cues, and adopting Parameter-Efficient Fine-Tuning.</p><hr><h3>Equidistribution-based training of Free Knot Splines and ReLU Neural  Networks</h3>
<p><a href='http://arxiv.org/abs/2407.02153v1'>http://arxiv.org/abs/2407.02153v1</a></p>
<p><b>Compressor summary</b>: The paper compares one-dimensional function approximation using shallow neural networks with ReLU activation and traditional methods like Free Knot Splines, and proposes a two-level training method for better performance.</p><hr><h3>VRBiom: A New Periocular Dataset for Biometric Applications of HMD</h3>
<p><a href='http://arxiv.org/abs/2407.02150v1'>http://arxiv.org/abs/2407.02150v1</a></p>
<p><b>Compressor summary</b>: The VRBiom dataset contains periocular videos acquired using a VR headset for biometric applications, including iris and periocular recognition, with real and spoofed data.</p><hr><h3>LlamAr & GemmAr: Enhancing LLMs Through Arabic Instruction-Tuning</h3>
<p><a href='http://arxiv.org/abs/2407.02147v1'>http://arxiv.org/abs/2407.02147v1</a></p>
<p><b>Compressor summary</b>: The authors introduce InstAr-500k, a new Arabic instruction dataset, which improves the performance of language models on various Arabic NLP tasks by fine-tuning existing models.</p><hr><h3>Counterfactual Data Augmentation with Denoising Diffusion for Graph  Anomaly Detection</h3>
<p><a href='http://arxiv.org/abs/2407.02143v1'>http://arxiv.org/abs/2407.02143v1</a></p>
<p><b>Compressor summary</b>: CAGAD enhances anomaly detection in graphs by creating counterfactual node representations using a graph pointer neural network and a diffusion model.</p><hr><h3>Efficient Nearest Neighbor based Uncertainty Estimation for Natural  Language Processing Tasks</h3>
<p><a href='http://arxiv.org/abs/2407.02138v1'>http://arxiv.org/abs/2407.02138v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new uncertainty estimation method for DNNs using k-Nearest Neighbor (kNN) that performs well in calibration, selective prediction, and out-of-distribution detection with low inference cost.</p><hr><h3>Black Big Boxes: Do Language Models Hide a Theory of Adjective Order?</h3>
<p><a href='http://arxiv.org/abs/2407.02136v1'>http://arxiv.org/abs/2407.02136v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The text investigates how language models (LMs) learn adjective order preferences (AOPs) in English and other languages
- AOPs involve complex ordering patterns of multiple adjectives in noun phrases that cross syntax, semantics, and pragmatics
- The authors present a reusable corpus of adjective pairs and define AOP measures for LMs
- They find that LMs perform better than theoretical linguistic factors but still show strong data frequency effects and limited generalization
- They suggest future studies and discuss key questions about LM knowledge and generalization

Summary:
The text explores how LMs acquire AOPs, which are intricate rules of adjective ordering in languages, using a new corpus and measures. It reveals that LMs outperform linguistic theory but struggle with data frequency and generalization.</p><hr><h3>Hybrid Feature Collaborative Reconstruction Network for Few-Shot  Fine-Grained Image Classification</h3>
<p><a href='http://arxiv.org/abs/2407.02123v1'>http://arxiv.org/abs/2407.02123v1</a></p>
<p><b>Compressor summary</b>: HFCR-Net combines channel features and spatial features to improve few-shot fine-grained image classification by enhancing inter-class differences and reducing intra-class differences through a hybrid feature reconstruction process.</p><hr><h3>Fake News Detection: It's All in the Data!</h3>
<p><a href='http://arxiv.org/abs/2407.02122v1'>http://arxiv.org/abs/2407.02122v1</a></p>
<p><b>Compressor summary</b>: The text is a survey about fake news detection datasets, emphasizing their importance for model performance, and providing a GitHub repository with publicly accessible datasets.</p><hr><h3>Cost-Effective Proxy Reward Model Construction with On-Policy and Active  Learning</h3>
<p><a href='http://arxiv.org/abs/2407.02119v1'>http://arxiv.org/abs/2407.02119v1</a></p>
<p><b>Compressor summary</b>: RLHF relies on human feedback, but it's limited by the size of preference data; our approach uses online methods and proxy reward oracles to efficiently label preferences with minimal expert input.</p><hr><h3>Breaking Language Barriers: Cross-Lingual Continual Pre-Training at  Scale</h3>
<p><a href='http://arxiv.org/abs/2407.02118v1'>http://arxiv.org/abs/2407.02118v1</a></p>
<p><b>Compressor summary</b>: The paper explores constructing large language models for new languages by continually pretraining from existing models, showing faster convergence, resource savings, and different data-parameter allocation compared to training from scratch.</p><hr><h3>A Data-Centric Perspective on Evaluating Machine Learning Models for  Tabular Data</h3>
<p><a href='http://arxiv.org/abs/2407.02112v1'>http://arxiv.org/abs/2407.02112v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a data-centric evaluation framework for tabular data models, showing that dataset-specific preprocessing and feature engineering are crucial factors affecting performance, and highlights the importance of test-time adaptation for dynamic data.</p><hr><h3>HRSAM: Efficiently Segment Anything in High-Resolution Images</h3>
<p><a href='http://arxiv.org/abs/2407.02109v1'>http://arxiv.org/abs/2407.02109v1</a></p>
<p><b>Compressor summary</b>: HRSAM is a new interactive segmentation model that combines Flash Attention and PSCWin attention to handle high-resolution images and achieve low latency, outperforming previous models.</p><hr><h3>Automated Knowledge Graph Learning in Industrial Processes</h3>
<p><a href='http://arxiv.org/abs/2407.02106v1'>http://arxiv.org/abs/2407.02106v1</a></p>
<p><b>Compressor summary</b>: The paper presents a framework for creating knowledge graphs from time series data to help with industrial decision-making and optimization, using Granger causality to find key attributes for predictive models.</p><hr><h3>Joint-Dataset Learning and Cross-Consistent Regularization for  Text-to-Motion Retrieval</h3>
<p><a href='http://arxiv.org/abs/2407.02104v1'>http://arxiv.org/abs/2407.02104v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Pose-estimation methods extract human motion from videos as 3D skeleton sequences
- Text-motion retrieval tasks aim to search for motions or descriptions that match a given text
- Joint-dataset learning and CCCL help improve text-motion models by using more data and imposing uni-modal constraints
- MoT++ is a transformer-based encoder that uses spatio-temporal attention to process skeleton data
- The paper evaluates the proposed methods on KIT Motion-Language and HumanML3D datasets

Summary:
The paper proposes joint-dataset learning, CCCL, and MoT++ to improve text-motion retrieval tasks that search for video motions or descriptions based on natural language.</p><hr><h3>Helpful assistant or fruitful facilitator? Investigating how personas  affect language model behavior</h3>
<p><a href='http://arxiv.org/abs/2407.02099v1'>http://arxiv.org/abs/2407.02099v1</a></p>
<p><b>Compressor summary</b>: The paper explores how assigning different personas to large language models affects their behavior and compares it to a control setting with a generic "helpful assistant" and no persona.</p><hr><h3>DM3D: Distortion-Minimized Weight Pruning for Lossless 3D Object  Detection</h3>
<p><a href='http://arxiv.org/abs/2407.02098v1'>http://arxiv.org/abs/2407.02098v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a post-training weight pruning scheme for 3D object detection that reduces computational cost and memory footprint while maintaining or enhancing detection precision.</p><hr><h3>Efficient Bit Labeling in Factorization Machines with Annealing for  Traveling Salesman Problem</h3>
<p><a href='http://arxiv.org/abs/2407.02091v1'>http://arxiv.org/abs/2407.02091v1</a></p>
<p><b>Compressor summary</b>: This paper studies how different binary labeling methods affect the performance of solving large-scale optimization problems using factorization machines with annealing, and proposes a new method called Gray labeling that improves convergence speed and accuracy for the traveling salesman problem.</p><hr><h3>GPTCast: a weather language model for precipitation nowcasting</h3>
<p><a href='http://arxiv.org/abs/2407.02089v1'>http://arxiv.org/abs/2407.02089v1</a></p>
<p><b>Compressor summary</b>: GPTCast is a generative deep-learning method that uses a large language model to learn spatiotemporal precipitation dynamics from radar images and produce realistic ensemble forecasts with accurate uncertainty estimation.</p><hr><h3>Hierarchical Temporal Context Learning for Camera-based Semantic Scene  Completion</h3>
<p><a href='http://arxiv.org/abs/2407.02077v1'>http://arxiv.org/abs/2407.02077v1</a></p>
<p><b>Compressor summary</b>: HTCL is a novel method for improving camera-based semantic scene completion by learning hierarchical temporal context and adaptively refining feature sampling locations, achieving state-of-the-art results on benchmarks.</p><hr><h3>Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual  Prompts</h3>
<p><a href='http://arxiv.org/abs/2407.02075v1'>http://arxiv.org/abs/2407.02075v1</a></p>
<p><b>Compressor summary</b>: Label Anything is a neural network architecture for few-shot semantic segmentation that uses different visual prompts and trains end-to-end across multi-class scenarios, improving adaptability and generalization.</p><hr><h3>Latent Diffusion Model for Generating Ensembles of Climate Simulations</h3>
<p><a href='http://arxiv.org/abs/2407.02070v1'>http://arxiv.org/abs/2407.02070v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new deep learning method to quickly and efficiently generate multiple climate scenarios for uncertainty analysis.</p><hr><h3>LPViT: Low-Power Semi-structured Pruning for Vision Transformers</h3>
<p><a href='http://arxiv.org/abs/2407.02068v1'>http://arxiv.org/abs/2407.02068v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Vision transformers (ViTs) are powerful but resource-intensive for image analysis tasks
- Block-structured pruning leverages the block-wise structure of linear layers to reduce resource requirements
- A hardware-aware learning objective is proposed to optimize the pruning scheme and balance accuracy and power consumption
- Experiments on ImageNet show competitive performance and significant speedup and power reduction for ViTs

Summary:
The paper introduces a new pruning method for vision transformers that reduces resource usage by exploiting block-wise layer structure and optimizes it with a hardware-aware objective, achieving fast and energy-efficient image analysis.</p><hr><h3>Crossroads of Continents: Automated Artifact Extraction for Cultural  Adaptation with Large Multimodal Models</h3>
<p><a href='http://arxiv.org/abs/2407.02067v1'>http://arxiv.org/abs/2407.02067v1</a></p>
<p><b>Compressor summary</b>: The study examines large multimodal models' ability to recognize and adapt across different cultural contexts using a new dataset, Dalle Street.</p><hr><h3>BiasDora: Exploring Hidden Biased Associations in Vision-Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.02066v1'>http://arxiv.org/abs/2407.02066v1</a></p>
<p><b>Compressor summary</b>: The paper studies how Vision Language Models can have implicit social biases across nine dimensions and creates a dataset to identify and mitigate them.</p><hr><h3>Are Data Augmentation Methods in Named Entity Recognition Applicable for  Uncertainty Estimation?</h3>
<p><a href='http://arxiv.org/abs/2407.02062v1'>http://arxiv.org/abs/2407.02062v1</a></p>
<p><b>Compressor summary</b>: Data augmentation enhances confidence calibration and uncertainty estimation in Named Entity Recognition tasks using Deep Neural Networks.</p><hr><h3>Terminating Differentiable Tree Experts</h3>
<p><a href='http://arxiv.org/abs/2407.02060v1'>http://arxiv.org/abs/2407.02060v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new neuro-symbolic model that combines transformers and Tensor Product Representations, improves it by reducing parameters and introducing a mixture of experts, and proposes an automatic termination algorithm for controlling the number of steps in the computation.</p><hr><h3>HC-GLAD: Dual Hyperbolic Contrastive Learning for Unsupervised  Graph-Level Anomaly Detection</h3>
<p><a href='http://arxiv.org/abs/2407.02057v1'>http://arxiv.org/abs/2407.02057v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for detecting anomalies in graphs using hypergraphs, node group connections, and hyperbolic geometry.</p><hr><h3>Integrate the Essence and Eliminate the Dross: Fine-Grained  Self-Consistency for Free-Form Language Generation</h3>
<p><a href='http://arxiv.org/abs/2407.02056v1'>http://arxiv.org/abs/2407.02056v1</a></p>
<p><b>Compressor summary</b>: Fine-Grained Self-Consistency (FSC) enhances LLMs' performance in both open-ended and reasoning tasks by integrating segment-level commonalities from candidate samples, and introduces two strategies to further improve output quality.</p><hr><h3>Abstract Dialectical Frameworks are Boolean Networks (full version)</h3>
<p><a href='http://arxiv.org/abs/2407.02055v1'>http://arxiv.org/abs/2407.02055v1</a></p>
<p><b>Compressor summary</b>: The paper explores how dialectical frameworks and Boolean regulatory networks, two different models from argumentation and biology respectively, have similarities in appearance and can be related to produce new insights.</p><hr><h3>CountFormer: Multi-View Crowd Counting Transformer</h3>
<p><a href='http://arxiv.org/abs/2407.02047v1'>http://arxiv.org/abs/2407.02047v1</a></p>
<p><b>Compressor summary</b>: CountFormer is a 3D multi-view counting framework that handles different camera layouts and achieves superior performance using scene-level volume representation and attention mechanism.</p><hr><h3>Concise and Precise Context Compression for Tool-Using Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.02043v1'>http://arxiv.org/abs/2407.02043v1</a></p>
<p><b>Compressor summary</b>: The authors propose two methods to compress tool documentation for language models, reducing input length and decoding time while preserving key information.</p><hr><h3>Fake News Detection and Manipulation Reasoning via Large Vision-Language  Models</h3>
<p><a href='http://arxiv.org/abs/2407.02042v1'>http://arxiv.org/abs/2407.02042v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Paper proposes manipulation reasoning, a new research topic for detecting fake news based on news content
- Introduces HFFN, a benchmark with human-centric and fact-related domains and detailed annotations
- Presents M-DRUM, a multi-modal model that extracts fusion features and raises analytical reasoning about manipulations
- Outperforms SOTA models and LVLMs like GPT-4 and LLaVA

Summary:
The paper introduces manipulation reasoning, a new way to detect fake news by reasoning about content manipulations. It also presents HFFN, a benchmark with realistic domains and annotations, and M-DRUM, a multi-modal model that beats existing methods.</p><hr><h3>ScaleDreamer: Scalable Text-to-3D Synthesis with Asynchronous Score  Distillation</h3>
<p><a href='http://arxiv.org/abs/2407.02040v1'>http://arxiv.org/abs/2407.02040v1</a></p>
<p><b>Compressor summary</b>: ASD is a text-to-3D method that uses diffusion models to synthesize 3D contents faster and more accurately by adjusting the model's timestep, enabling it to handle large amounts of text prompts.</p><hr><h3>Prompt Stability Scoring for Text Annotation with Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.02039v1'>http://arxiv.org/abs/2407.02039v1</a></p>
<p><b>Compressor summary</b>: The authors propose a Prompt Stability Score (PSS) metric and a Python package to measure and improve the reproducibility of language models for text annotation tasks.</p><hr><h3>Camera-LiDAR Cross-modality Gait Recognition</h3>
<p><a href='http://arxiv.org/abs/2407.02038v1'>http://arxiv.org/abs/2407.02038v1</a></p>
<p><b>Compressor summary</b>: The paper proposes CL-Gait, a cross-modality gait recognition framework between cameras and LiDARs, using a two-stream network and contrastive pre-training with virtual data generation.</p><hr><h3>TrAME: Trajectory-Anchored Multi-View Editing for Text-Guided 3D  Gaussian Splatting Manipulation</h3>
<p><a href='http://arxiv.org/abs/2407.02034v1'>http://arxiv.org/abs/2407.02034v1</a></p>
<p><b>Compressor summary</b>: Our paper proposes a progressive 3D editing strategy with Trajectory-Anchored Scheme and dual-branch editing mechanism to ensure multi-view consistency and improve editing quality in text-guided 3D scene editing.</p><hr><h3>Breaking Bias, Building Bridges: Evaluation and Mitigation of Social  Biases in LLMs via Contact Hypothesis</h3>
<p><a href='http://arxiv.org/abs/2407.02030v1'>http://arxiv.org/abs/2407.02030v1</a></p>
<p><b>Compressor summary</b>: The text explores a debiasing technique for large language models using the Contact Hypothesis, which involves simulating social contact through prompts and instruction-tuning with unbiased responses to reduce prejudices in LLMs.</p><hr><h3>Why does in-context learning fail sometimes? Evaluating in-context  learning on open and closed questions</h3>
<p><a href='http://arxiv.org/abs/2407.02028v1'>http://arxiv.org/abs/2407.02028v1</a></p>
<p><b>Compressor summary</b>: The study evaluates in-context learning for open and closed questions of varying difficulty and novelty, revealing a counter-intuitive effect of context relevancy on hard and novel questions.</p><hr><h3>On the Expressive Power of Sparse Geometric MPNNs</h3>
<p><a href='http://arxiv.org/abs/2407.02025v1'>http://arxiv.org/abs/2407.02025v1</a></p>
<p><b>Compressor summary</b>: The paper presents a message-passing neural network architecture called EGENNET that can separate non-equivalent geometric graphs based on local features and rotation equivariance, improving upon previous methods for connected graphs and globally rigid graphs.</p><hr><h3>Multi-Grained Contrast for Data-Efficient Unsupervised Representation  Learning</h3>
<p><a href='http://arxiv.org/abs/2407.02014v1'>http://arxiv.org/abs/2407.02014v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new contrastive learning method that learns multi-grained representations for better generalization on various downstream tasks, outperforming existing methods without using large-scale pretraining data.</p><hr><h3>DiGRAF: Diffeomorphic Graph-Adaptive Activation Function</h3>
<p><a href='http://arxiv.org/abs/2407.02013v1'>http://arxiv.org/abs/2407.02013v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper introduces DiGRAF, a new activation function for graph neural networks (GNNs)
- DiGRAF is based on continuous piecewise-affine transformations and learns to adapt to different graphs
- DiGRAF has desirable properties such as differentiability, boundness and efficiency
- Experiments show that DiGRAF outperforms other activation functions for GNNs

Summary:
DiGRAF is a novel activation function for GNNs that learns to adapt to graph data using continuous piecewise-affine transformations and has desirable properties, achieving superior performance in experiments.</p><hr><h3>An End-to-End Speech Summarization Using Large Language Model</h3>
<p><a href='http://arxiv.org/abs/2407.02005v1'>http://arxiv.org/abs/2407.02005v1</a></p>
<p><b>Compressor summary</b>: Key points:
- SSum generates text summaries from spoken content
- Challenges include long speech input and cross-modal mapping
- Proposed model uses Q-Former and LLMs for audio-text fusion and summary generation
- Multi-stage training with ASR and TSum as auxiliary tasks
- Curriculum learning helps transition from TSum to SSum
- Competitive performance on How-2 dataset

Summary:
The paper presents a new SSum model that uses Q-Former and LLMs to fuse audio and text, trains with ASR and TSum tasks, and employs curriculum learning for transitioning between tasks. It performs well on the How-2 dataset.</p><hr><h3>SAVE: Segment Audio-Visual Easy way using Segment Anything Model</h3>
<p><a href='http://arxiv.org/abs/2407.02004v1'>http://arxiv.org/abs/2407.02004v1</a></p>
<p><b>Compressor summary</b>: The study presents SAVE, a lightweight approach that adapts the pre-trained SAM model for audio-visual segmentation by using image and audio encoder adapters to improve fusion and speed, achieving higher performance on real data than previous methods.</p><hr><h3>ViG-Bias: Visually Grounded Bias Discovery and Mitigation</h3>
<p><a href='http://arxiv.org/abs/2407.01996v1'>http://arxiv.org/abs/2407.01996v1</a></p>
<p><b>Compressor summary</b>: Visually Grounded Bias Discovery and Mitigation (ViG-Bias) improves bias detection and reduction in machine learning models by using visual explanations from large vision language models.</p><hr><h3>Simple Augmentations of Logical Rules for Neuro-Symbolic Knowledge Graph  Completion</h3>
<p><a href='http://arxiv.org/abs/2407.01994v1'>http://arxiv.org/abs/2407.01994v1</a></p>
<p><b>Compressor summary</b>: This work proposes three techniques to enhance rule sets for Neuro-Symbolic Knowledge Graph Completion models, achieving significant improvements in coverage and performance.</p><hr><h3>Is Your Large Language Model Knowledgeable or a Choices-Only Cheater?</h3>
<p><a href='http://arxiv.org/abs/2407.01992v1'>http://arxiv.org/abs/2407.01992v1</a></p>
<p><b>Compressor summary</b>: The authors use graph mining to create a contrast set from MCQA datasets and show that large language models do not rely on choices-only shortcuts for high performance in multiple-choice question answering.</p><hr><h3>Generation of Geodesics with Actor-Critic Reinforcement Learning to  Predict Midpoints</h3>
<p><a href='http://arxiv.org/abs/2407.01991v1'>http://arxiv.org/abs/2407.01991v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to find shortest paths on continuous surfaces using recursive midpoint prediction and an actor-critic learning approach, which is theoretically sound and performs better than previous methods in local and global path planning.</p><hr><h3>AHMsys: An Automated HVAC Modeling System for BIM Project</h3>
<p><a href='http://arxiv.org/abs/2407.01987v1'>http://arxiv.org/abs/2407.01987v1</a></p>
<p><b>Compressor summary</b>: AHMsys is a system that automates creating 3D HVAC models from 2D CAD drawings, reducing BIM process time by 20 percent.</p><hr><h3>SADL: An Effective In-Context Learning Method for Compositional Visual  QA</h3>
<p><a href='http://arxiv.org/abs/2407.01983v1'>http://arxiv.org/abs/2407.01983v1</a></p>
<p><b>Compressor summary</b>: The paper introduces SADL, a visual-linguistic prompting framework for in-context learning in Visual QA that samples, decomposes, and pseudo-labels image-question pairs to bridge the semantic gap between symbols and images.</p><hr><h3>Unveiling Global Interactive Patterns across Graphs: Towards  Interpretable Graph Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2407.01979v1'>http://arxiv.org/abs/2407.01979v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Graph Neural Networks (GNNs) are widely used for graph mining tasks
- Existing explanations focus on subgraph features and local structures, but graph classification requires global interactions
- The paper proposes Global Interactive Pattern (GIP) learning, which introduces learnable global patterns to interpret decisions
- GIP uses graph clustering and prototype matching to facilitate transparent graph-level reasoning
- Experiments show that GIP improves interpretability and performs well on synthetic and real-world data

Summary:
The paper presents a novel interpretation scheme for graph classification with GNNs, called GIP, which learns global interactive patterns using graph clustering and prototype matching.</p><hr><h3>A Bounding Box is Worth One Token: Interleaving Layout and Text in a  Large Language Model for Document Understanding</h3>
<p><a href='http://arxiv.org/abs/2407.01976v1'>http://arxiv.org/abs/2407.01976v1</a></p>
<p><b>Compressor summary</b>: LayTextLLM is a new method for document understanding that improves performance in Key Information Extraction and Visual Question Answering by interleaving text and spatial layout embeddings without long sequence issues.</p><hr><h3>Pseudo-Labeling by Multi-Policy Viewfinder Network for Image Cropping</h3>
<p><a href='http://arxiv.org/abs/2407.01971v1'>http://arxiv.org/abs/2407.01971v1</a></p>
<p><b>Compressor summary</b>: MPV-Net uses diverse refining policies to generate trusted pseudo labels for image cropping models using labeled and unlabeled data, achieving state-of-the-art results.</p><hr><h3>Unleash the Power of Local Representations for Few-Shot Classification</h3>
<p><a href='http://arxiv.org/abs/2407.01967v1'>http://arxiv.org/abs/2407.01967v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel pretraining paradigm with soft labels and a metric with adaptability to improve few-shot classification using local representations.</p><hr><h3>AdaCQR: Enhancing Query Reformulation for Conversational Search via  Sparse and Dense Retrieval Alignment</h3>
<p><a href='http://arxiv.org/abs/2407.01965v1'>http://arxiv.org/abs/2407.01965v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework called AdaCQR that improves conversational search by aligning reformulation models with different types of retrieval systems and using efficient techniques to acquire better labels and input candidates.</p><hr><h3>Enabling Discriminative Reasoning in Large Language Models for Legal  Judgment Prediction</h3>
<p><a href='http://arxiv.org/abs/2407.01964v1'>http://arxiv.org/abs/2407.01964v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new reasoning framework (ADAPT) to adapt large language models for better legal judgment prediction by understanding case facts, discriminating charges, and predicting judgments.</p><hr><h3>Zero-shot Video Restoration and Enhancement Using Pre-Trained Image  Diffusion Model</h3>
<p><a href='http://arxiv.org/abs/2407.01960v1'>http://arxiv.org/abs/2407.01960v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a framework for zero-shot video restoration and enhancement using a pre-trained image diffusion model with a cross-previous-frame attention layer and other techniques to reduce temporal flickering artifacts and improve quality.</p><hr><h3>FlowTrack: Point-level Flow Network for 3D Single Object Tracking</h3>
<p><a href='http://arxiv.org/abs/2407.01959v1'>http://arxiv.org/abs/2407.01959v1</a></p>
<p><b>Compressor summary</b>: The paper proposes FlowTrack, a point-level flow method for 3D single object tracking that captures local motion details, handles sparse points with a learnable target feature, and aggregates local motion information into global motion using an Instance Flow Head.</p><hr><h3>S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested  Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.01955v1'>http://arxiv.org/abs/2407.01955v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to speed up large language models' decoding process by using sorted speculative decoding with multiple draft models for different target models, reducing costs and improving performance.</p><hr><h3>Extracting and Encoding: Leveraging Large Language Models and Medical  Knowledge to Enhance Radiological Text Representation</h3>
<p><a href='http://arxiv.org/abs/2407.01948v1'>http://arxiv.org/abs/2407.01948v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The text presents a two-stage framework to extract factual statements from radiology reports and improve text encoders for various downstream tasks.
- The framework consists of a Fact Extractor using LLMs and a Fact Encoder based on a BERT model fine-tuned with objective functions.
- The text also introduces a new metric (CXRFEScore) for evaluating chest X-ray text generation systems.

Summary:
The text describes a framework that uses factual statements from radiology reports to enhance text encoders and evaluate chest X-ray text generation systems.</p><hr><h3>Indoor 3D Reconstruction with an Unknown Camera-Projector Pair</h3>
<p><a href='http://arxiv.org/abs/2407.01945v1'>http://arxiv.org/abs/2407.01945v1</a></p>
<p><b>Compressor summary</b>: The paper presents a simple and reliable method for calibrating a camera-projector pair (CPP) using an unknown cuboid corner, enabling direct 3D reconstruction in indoor scenes with weak textures.</p><hr><h3>Certainly Uncertain: A Benchmark and Metric for Multimodal Epistemic and  Aleatoric Awareness</h3>
<p><a href='http://arxiv.org/abs/2407.01942v1'>http://arxiv.org/abs/2407.01942v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a taxonomy of uncertainty in vision-language AI systems, creates a dataset with contrastive examples, and introduces a new metric for measuring calibration error.</p><hr><h3>Efficient-Empathy: Towards Efficient and Effective Selection of Empathy  Data</h3>
<p><a href='http://arxiv.org/abs/2407.01937v1'>http://arxiv.org/abs/2407.01937v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Efficient-Empathy, an algorithm that selects high-quality empathetic data to improve large language models' performance and efficiency in empathetic dialogues.</p><hr><h3>Probabilistic 3D Correspondence Prediction from Sparse Unsegmented  Images</h3>
<p><a href='http://arxiv.org/abs/2407.01931v1'>http://arxiv.org/abs/2407.01931v1</a></p>
<p><b>Compressor summary</b>: SPI-CorrNet is a deep learning model that predicts 3D correspondences from sparse imaging data, improving the accuracy and robustness of statistical shape modeling in clinical research.</p><hr><h3>Self-Cooperation Knowledge Distillation for Novel Class Discovery</h3>
<p><a href='http://arxiv.org/abs/2407.01930v1'>http://arxiv.org/abs/2407.01930v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a Self-Cooperation Knowledge Distillation (SCKD) method to balance reviewing known classes and discovering novel classes in unsupervised learning.</p><hr><h3>What We Talk About When We Talk About LMs: Implicit Paradigm Shifts and  the Ship of Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.01929v1'>http://arxiv.org/abs/2407.01929v1</a></p>
<p><b>Compressor summary</b>: The paper examines the evolution of the term "Language Models" in scientific discourse and calls for a new perspective on how systems and theories influence each other.</p><hr><h3>SymPoint Revolutionized: Boosting Panoptic Symbol Spotting with Layer  Feature Enhancement</h3>
<p><a href='http://arxiv.org/abs/2407.01928v1'>http://arxiv.org/abs/2407.01928v1</a></p>
<p><b>Compressor summary</b>: SymPoint-V2 is a new, improved method for recognizing symbols in CAD drawings that uses layer information and faster training to outperform its predecessor SymPoint.</p><hr><h3>Looking From the Future: Multi-order Iterations Can Enhance Adversarial  Attack Transferability</h3>
<p><a href='http://arxiv.org/abs/2407.01925v1'>http://arxiv.org/abs/2407.01925v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Paper proposes a new optimization concept, Looking From the Future (LFF), for adversarial attacks
- LFF improves generalization from different perspectives and transfers better to unseen tasks
- Paper also introduces multi-order attack method, $LLF^{\mathcal{N}}$, that extends LFF to more tasks
- Experiments on ImageNet1k dataset show significant enhancement of attack transferability

Summary:
The paper introduces a novel optimization concept, LFF, for adversarial attacks that improves generalization and transferability. It also presents a multi-order attack method, $LLF^{\mathcal{N}}$, and shows its effectiveness on ImageNet1k dataset.</p><hr><h3>GVDIFF: Grounded Text-to-Video Generation with Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2407.01921v1'>http://arxiv.org/abs/2407.01921v1</a></p>
<p><b>Compressor summary</b>: GVDIFF is a text-to-video framework that uses uncertainty-based representations, spatial-temporal grounding, and dynamic gates to generate videos guided by text with different applications.</p><hr><h3>To Forget or Not? Towards Practical Knowledge Unlearning for Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.01920v1'>http://arxiv.org/abs/2407.01920v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Large Language Models (LLMs) can have sensitive data
- Knowledge unlearning tries to erase specific knowledge from LLMs
- Current methods are not precise and may lose essential knowledge
- MemFlex is a new method that uses gradient information to target sensitive parameters
- MemFlex performs better than existing methods in unlearning and retaining knowledge

Summary:
MemFlex is a novel method that uses gradients to precisely erase sensitive data from LLMs without losing essential knowledge, unlike current imprecise unlearning approaches.</p><hr><h3>Sequential Manipulation Against Rank Aggregation: Theory and Algorithm</h3>
<p><a href='http://arxiv.org/abs/2407.01916v1'>http://arxiv.org/abs/2407.01916v1</a></p>
<p><b>Compressor summary</b>: The text discusses ranking manipulation using pairwise comparisons and proposes distributionally robust methods to attack and defend against such manipulations.</p><hr><h3>Investigating the Effects of Large-Scale Pseudo-Stereo Data and  Different Speech Foundation Model on Dialogue Generative Spoken Language  Model</h3>
<p><a href='http://arxiv.org/abs/2407.01911v1'>http://arxiv.org/abs/2407.01911v1</a></p>
<p><b>Compressor summary</b>: The authors developed a method to convert single-channel speech recordings into pseudo-stereo data, which increased the training dataset size and improved the performance of spoken dialogue language models.</p><hr><h3>MG-Verilog: Multi-grained Dataset Towards Enhanced LLM-assisted Verilog  Generation</h3>
<p><a href='http://arxiv.org/abs/2407.01910v1'>http://arxiv.org/abs/2407.01910v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Large Language Models (LLMs) can help streamline hardware design processes
- Existing hardware datasets are often limited and need to be improved
- Proposed a Multi-Grained-Verilog dataset with criteria for quality
- Introduced a balanced fine-tuning scheme to leverage the dataset's diversity

Summary:
The authors propose a high-quality Multi-Grained-Verilog dataset and a balanced fine-tuning scheme to enhance LLMs in hardware design tasks, addressing the limitations of existing hardware datasets.</p><hr><h3>Pinyin Regularization in Error Correction for Chinese Speech Recognition  with Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.01909v1'>http://arxiv.org/abs/2407.01909v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new Chinese ASR error correction dataset and shows that Pinyin regularization improves the performance of large language models in this task.</p><hr><h3>The Solution for the ICCV 2023 Perception Test Challenge 2023 -- Task 6  -- Grounded videoQA</h3>
<p><a href='http://arxiv.org/abs/2407.01907v1'>http://arxiv.org/abs/2407.01907v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new method for answering questions about videos that uses two steps: first, it uses VALOR to answer questions based on video information, and second, it uses TubeDETR to find bounding boxes of objects in the video.</p><hr><h3>Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for  Sparse Architectural Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.01906v1'>http://arxiv.org/abs/2407.01906v1</a></p>
<p><b>Compressor summary</b>: The paper explores parameter-efficient fine-tuning for sparse-architecture large language models with a Mixture-of-Experts architecture, proposing Expert-Specialized Fine-Tuning that improves tuning efficiency and performance.</p><hr><h3>Enhancing Multi-Class Anomaly Detection via Diffusion Refinement with  Dual Conditioning</h3>
<p><a href='http://arxiv.org/abs/2407.01905v1'>http://arxiv.org/abs/2407.01905v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a diffusion model and a transformer combined approach for multi-class anomaly detection in industry, which improves accuracy and avoids common problems such as blurry reconstruction and identical shortcuts.</p><hr><h3>Text-Aware Diffusion for Policy Learning</h3>
<p><a href='http://arxiv.org/abs/2407.01903v1'>http://arxiv.org/abs/2407.01903v1</a></p>
<p><b>Compressor summary</b>: TADPoLe is a method that uses pretrained generative models to learn policies from natural language without explicit rewards or demonstrations, achieving natural and diverse behaviors in various robotic domains.</p><hr><h3>Scope-enhanced Compositional Semantic Parsing for DRT</h3>
<p><a href='http://arxiv.org/abs/2407.01899v1'>http://arxiv.org/abs/2407.01899v1</a></p>
<p><b>Compressor summary</b>: The AMS parser is a neurosymbolic semantic parser for DRT that excels at handling complex sentences by predicting quantifier scope.</p><hr><h3>Proposal Report for the 2nd SciCAP Competition 2024</h3>
<p><a href='http://arxiv.org/abs/2407.01897v1'>http://arxiv.org/abs/2407.01897v1</a></p>
<p><b>Compressor summary</b>: The paper presents a document summarization method that uses auxiliary information to efficiently summarize content related to described objects in texts, achieving top scores in two tracks of the SciCAP competition.</p><hr><h3>LogEval: A Comprehensive Benchmark Suite for Large Language Models In  Log Analysis</h3>
<p><a href='http://arxiv.org/abs/2407.01896v1'>http://arxiv.org/abs/2407.01896v1</a></p>
<p><b>Compressor summary</b>: LogEval is a benchmark suite to evaluate how well large language models perform in various log analysis tasks for AIOps.</p><hr><h3>Adaptive Modality Balanced Online Knowledge Distillation for  Brain-Eye-Computer based Dim Object Detection</h3>
<p><a href='http://arxiv.org/abs/2407.01894v1'>http://arxiv.org/abs/2407.01894v1</a></p>
<p><b>Compressor summary</b>: The paper presents a brain-eye-computer system for detecting dim targets in aerial images using EEG and computer vision, and proposes an adaptive modality balanced online knowledge distillation (AMBOKD) method to fuse EEG and image features effectively.</p><hr><h3>GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial  Reasoning</h3>
<p><a href='http://arxiv.org/abs/2407.01892v1'>http://arxiv.org/abs/2407.01892v1</a></p>
<p><b>Compressor summary</b>: The paper introduces GRASP, a large-scale benchmark for evaluating commonsense spatial reasoning in language models, and shows that current advanced LLMs perform poorly on it.</p><hr><h3>Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents</h3>
<p><a href='http://arxiv.org/abs/2407.01887v1'>http://arxiv.org/abs/2407.01887v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates LLMs' decision-making abilities in Dueling Bandits, compares their performance to existing algorithms, and proposes an augmented algorithm that combines LLMs' strengths with classic DB guarantees.</p><hr><h3>Core Knowledge Learning Framework for Graph Adaptation and Scalability  Learning</h3>
<p><a href='http://arxiv.org/abs/2407.01886v1'>http://arxiv.org/abs/2407.01886v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel algorithm that learns the core subgraph of a graph to improve adaptability, scalability, and generalizability in graph classification tasks.</p><hr><h3>Survey on Knowledge Distillation for Large Language Models: Methods,  Evaluation, and Application</h3>
<p><a href='http://arxiv.org/abs/2407.01885v1'>http://arxiv.org/abs/2407.01885v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Large Language Models (LLMs) are powerful but computationally expensive
- Knowledge distillation is a technique to compress LLMs while preserving accuracy
- The paper surveys various knowledge distillation methods, evaluation tasks, and applications for LLMs

Summary:
The paper reviews different ways of compressing large language models using knowledge distillation techniques, as well as how to evaluate and apply them.</p><hr><h3>EIT-1M: One Million EEG-Image-Text Pairs for Human Visual-textual  Recognition and More</h3>
<p><a href='http://arxiv.org/abs/2407.01884v1'>http://arxiv.org/abs/2407.01884v1</a></p>
<p><b>Compressor summary</b>: The authors propose a large multi-modal EEG dataset, EIT-1M, with over 1 million pairs of images, texts, and brain activity recordings to study how the brain processes multiple modalities simultaneously.</p><hr><h3>Compare without Despair: Reliable Preference Evaluation with Generation  Separability</h3>
<p><a href='http://arxiv.org/abs/2407.01878v1'>http://arxiv.org/abs/2407.01878v1</a></p>
<p><b>Compressor summary</b>: Separability is a meta-evaluation measure that estimates how suitable a test instance is for pairwise preference evaluation by measuring the distinguishability of multiple generations from a model pair.</p><hr><h3>Spatio-Temporal Graphical Counterfactuals: An Overview</h3>
<p><a href='http://arxiv.org/abs/2407.01875v1'>http://arxiv.org/abs/2407.01875v1</a></p>
<p><b>Compressor summary</b>: This paper reviews various counterfactual models for AI and proposes a unified graphical approach to handle spatial and temporal interactions.</p><hr><h3>Automated Text Scoring in the Age of Generative AI for the GPU-poor</h3>
<p><a href='http://arxiv.org/abs/2407.01873v1'>http://arxiv.org/abs/2407.01873v1</a></p>
<p><b>Compressor summary</b>: The study explores using open-source, small-scale generative language models for automated text scoring and feedback generation on modest hardware.</p><hr><h3>Referring Atomic Video Action Recognition</h3>
<p><a href='http://arxiv.org/abs/2407.01872v1'>http://arxiv.org/abs/2407.01872v1</a></p>
<p><b>Compressor summary</b>: The paper introduces RAVAR, a new task to recognize atomic actions of a specific person based on text and video, and presents RefAtomNet, a novel method that uses cross-stream attention to address the challenges of this task.</p><hr><h3>Image-GS: Content-Adaptive Image Representation via 2D Gaussians</h3>
<p><a href='http://arxiv.org/abs/2407.01866v1'>http://arxiv.org/abs/2407.01866v1</a></p>
<p><b>Compressor summary</b>: Image-GS is a novel image representation using anisotropic 2D Gaussians that enables content-adaptive rendering with high memory efficiency, fast random access, and a natural level of detail stack for various applications.</p><hr><h3>Research on target detection method of distracted driving behavior based  on improved YOLOv8</h3>
<p><a href='http://arxiv.org/abs/2407.01864v1'>http://arxiv.org/abs/2407.01864v1</a></p>
<p><b>Compressor summary</b>: The study presents a more efficient and accurate YOLOv8-based method for detecting and classifying distracted driving behaviour by integrating BoTNet, GAM attention and EIoU loss.</p><hr><h3>VSP: Assessing the dual challenges of perception and reasoning in  spatial planning tasks for VLMs</h3>
<p><a href='http://arxiv.org/abs/2407.01863v1'>http://arxiv.org/abs/2407.01863v1</a></p>
<p><b>Compressor summary</b>: The study introduces VSP, a benchmark to evaluate visual spatial planning capabilities of vision language models, and reveals their deficiencies in perception, reasoning, and general planning tasks.</p>