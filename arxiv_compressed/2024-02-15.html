
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-02-15</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-02-15 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential  Reasoning Ability</h3>
<p><a href='http://arxiv.org/abs/2402.09404v1'>http://arxiv.org/abs/2402.09404v1</a></p>
<p><b>Compressor summary</b>: AQA-Bench is a benchmark to test large language models' ability to reason sequentially in algorithmic contexts, revealing insights on closed-source vs open-source models and other factors affecting performance.</p><hr><h3>Reinforcement Learning from Human Feedback with Active Queries</h3>
<p><a href='http://arxiv.org/abs/2402.09401v1'>http://arxiv.org/abs/2402.09401v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a query-efficient reinforcement learning method for aligning large language models with human preferences by formalizing the problem as a contextual dueling bandit problem and designing an active-query-based proximal policy optimization algorithm.</p><hr><h3>Get More with LESS: Synthesizing Recurrence with KV Cache Compression  for Efficient LLM Inference</h3>
<p><a href='http://arxiv.org/abs/2402.09398v1'>http://arxiv.org/abs/2402.09398v1</a></p>
<p><b>Compressor summary</b>: The paper proposes LESS, a cache method for large language models that balances memory efficiency and information retention by integrating a constant-sized cache with eviction-based methods.</p><hr><h3>Long-form evaluation of model editing</h3>
<p><a href='http://arxiv.org/abs/2402.09394v1'>http://arxiv.org/abs/2402.09394v1</a></p>
<p><b>Compressor summary</b>: The paper introduces LEME, a protocol to evaluate model editing techniques for long-form text generation, which reveals different performance aspects than short-form metrics and identifies common failure modes in long-form settings.</p><hr><h3>LlaSMol: Advancing Large Language Models for Chemistry with a  Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset</h3>
<p><a href='http://arxiv.org/abs/2402.09391v1'>http://arxiv.org/abs/2402.09391v1</a></p>
<p><b>Compressor summary</b>: The paper introduces SMolInstruct, a large dataset for instruction tuning, and shows that LLMs can achieve strong results on various chemistry tasks, outperforming GPT-4.</p><hr><h3>HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context  Learning in Factuality Evaluation</h3>
<p><a href='http://arxiv.org/abs/2402.09390v1'>http://arxiv.org/abs/2402.09390v1</a></p>
<p><b>Compressor summary</b>: The hierarchical graph of thoughts (HGOT) is a multi-layered graph approach that improves the factuality and quality of language models by enhancing the retrieval of relevant passages during in-context learning.</p><hr><h3>Entropy-regularized Point-based Value Iteration</h3>
<p><a href='http://arxiv.org/abs/2402.09388v1'>http://arxiv.org/abs/2402.09388v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an entropy-regularized model-based planner for partially observable problems that improves robustness and objective inference by reducing policy commitment to a single action.</p><hr><h3>GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in  Metagenomic Assembly</h3>
<p><a href='http://arxiv.org/abs/2402.09381v1'>http://arxiv.org/abs/2402.09381v1</a></p>
<p><b>Compressor summary</b>: GraSSRep is a novel method that uses graph neural networks to classify DNA sequences as repetitive or non-repetitive, improving repeat detection accuracy in metagenomic data.</p><hr><h3>Loss Shaping Constraints for Long-Term Time Series Forecasting</h3>
<p><a href='http://arxiv.org/abs/2402.09373v1'>http://arxiv.org/abs/2402.09373v1</a></p>
<p><b>Compressor summary</b>: The text proposes a Constrained Learning method for long-term time series forecasting that minimizes both average performance and maximum error, using a Primal-Dual algorithm.</p><hr><h3>Transformers Can Achieve Length Generalization But Not Robustly</h3>
<p><a href='http://arxiv.org/abs/2402.09371v1'>http://arxiv.org/abs/2402.09371v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how data format and position encoding affect length generalization in Transformers for adding two integers and achieves a 2.5x extrapolation, but finds the process sensitive to random seeds and other factors.</p><hr><h3>Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking</h3>
<p><a href='http://arxiv.org/abs/2402.09369v1'>http://arxiv.org/abs/2402.09369v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CultureAtlas, a dataset for acquiring multicultural knowledge from Wikipedia links to improve language models' cross-cultural communication and understanding.</p><hr><h3>Magic-Me: Identity-Specific Video Customized Diffusion</h3>
<p><a href='http://arxiv.org/abs/2402.09368v1'>http://arxiv.org/abs/2402.09368v1</a></p>
<p><b>Compressor summary</b>: VCD is a simple framework for generating videos with controlled subject identities using various modules to disentangle and enhance identity information.</p><hr><h3>Prediction of Activated Sludge Settling Characteristics from Microscopy  Images with Deep Convolutional Neural Networks and Transfer Learning</h3>
<p><a href='http://arxiv.org/abs/2402.09367v1'>http://arxiv.org/abs/2402.09367v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Microbial communities affect wastewater treatment processes and settling characteristics
- Computer vision-based approach using deep CNN models to assess sludge settling based on microscopy images
- Transfer learning, data augmentation, and various CNN architectures tested for performance
- Approach provides less labour-intensive, objective, and consistent assessments

Summary:
The study presents a computer vision-based approach using deep CNN models to predict sludge settling characteristics in wastewater treatment plants based on microscopy images. The approach uses transfer learning, data augmentation, and different CNN architectures to overcome limitations of existing techniques and provide objective and consistent assessments.</p><hr><h3>Copyright Traps for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.09363v1'>http://arxiv.org/abs/2402.09363v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using copyright traps made of fictitious entries in original content to detect the use of copyrighted materials in Large Language Models that do not naturally memorize.</p><hr><h3>HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM  Inference</h3>
<p><a href='http://arxiv.org/abs/2402.09360v1'>http://arxiv.org/abs/2402.09360v1</a></p>
<p><b>Compressor summary</b>: HiRE is a method that combines compression and efficient multi-device operations to speed up autoregressive decoding with sparse large language models on accelerators.</p><hr><h3>Integrating ChatGPT into Secure Hospital Networks: A Case Study on  Improving Radiology Report Analysis</h3>
<p><a href='http://arxiv.org/abs/2402.09358v1'>http://arxiv.org/abs/2402.09358v1</a></p>
<p><b>Compressor summary</b>: The study shows how to use a cloud-based AI similar to ChatGPT to analyze radiology reports in hospitals, keeping data private and improving accuracy, reliability, and interpretability.</p><hr><h3>DoRA: Weight-Decomposed Low-Rank Adaptation</h3>
<p><a href='http://arxiv.org/abs/2402.09353v1'>http://arxiv.org/abs/2402.09353v1</a></p>
<p><b>Compressor summary</b>: DoRA is a novel method that combines weight decomposition and LoRA to improve fine-tuning performance while avoiding extra inference costs.</p><hr><h3>Developing a Framework for Auditing Large Language Models Using  Human-in-the-Loop</h3>
<p><a href='http://arxiv.org/abs/2402.09346v1'>http://arxiv.org/abs/2402.09346v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an automatic method to create probes to audit large language models using different versions of the same question and human verification, increasing transparency and scientific rigor.</p><hr><h3>Mitigating Reward Hacking via Information-Theoretic Reward Modeling</h3>
<p><a href='http://arxiv.org/abs/2402.09345v1'>http://arxiv.org/abs/2402.09345v1</a></p>
<p><b>Compressor summary</b>: The paper proposes InfoRM, an information theoretic-based framework for reward modeling in reinforcement learning from human feedback, which can detect and mitigate reward overoptimization by using a variational information bottleneck objective and a cluster deviation score.</p><hr><h3>Generating Diverse Translation with Perturbed kNN-MT</h3>
<p><a href='http://arxiv.org/abs/2402.09344v1'>http://arxiv.org/abs/2402.09344v1</a></p>
<p><b>Compressor summary</b>: The paper introduces methods to generate more diverse translations using perturbed k-nearest neighbor machine translation, addressing the overcorrection problem in previous methods.</p><hr><h3>AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe  Approach</h3>
<p><a href='http://arxiv.org/abs/2402.09334v1'>http://arxiv.org/abs/2402.09334v1</a></p>
<p><b>Compressor summary</b>: AuditLLM is a tool that evaluates the safety, consistency, and reliability of Large Language Models by probing them with multiple variations of a single question, helping to identify potential issues such as bias or hallucinations.</p><hr><h3>YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture  Detection</h3>
<p><a href='http://arxiv.org/abs/2402.09329v1'>http://arxiv.org/abs/2402.09329v1</a></p>
<p><b>Compressor summary</b>: The YOLOv8-AM model incorporating attention mechanisms improves fracture detection performance and achieves state-of-the-art results in computer-assisted diagnosis of wrist trauma.</p><hr><h3>Information Complexity of Stochastic Convex Optimization: Applications  to Generalization and Memorization</h3>
<p><a href='http://arxiv.org/abs/2402.09327v1'>http://arxiv.org/abs/2402.09327v1</a></p>
<p><b>Compressor summary</b>: This paper studies how memorizing training data affects learning performance in stochastic convex optimization and shows a tradeoff between accuracy and memorization measured by conditional mutual information.</p><hr><h3>Stability and Multigroup Fairness in Ranking with Uncertain Predictions</h3>
<p><a href='http://arxiv.org/abs/2402.09326v1'>http://arxiv.org/abs/2402.09326v1</a></p>
<p><b>Compressor summary</b>: The text discusses ranking functions that incorporate uncertainty in classification tasks, focusing on their stability and multigroup fairness properties.</p><hr><h3>PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames  in Autonomous Driving Environments</h3>
<p><a href='http://arxiv.org/abs/2402.09325v1'>http://arxiv.org/abs/2402.09325v1</a></p>
<p><b>Compressor summary</b>: PC-NeRF is a novel framework for 3D scene reconstruction and view synthesis using sparse LiDAR frames, which divides the scene into different levels of representation and leverages hierarchical spatial partitioning.</p><hr><h3>ICDPO: Effectively Borrowing Alignment Capability of Others via  In-context Direct Preference Optimization</h3>
<p><a href='http://arxiv.org/abs/2402.09320v1'>http://arxiv.org/abs/2402.09320v1</a></p>
<p><b>Compressor summary</b>: ICDPO is a novel approach that improves LLMs' ability to generate safe content by borrowing HPA capabilities from superior LLMs with In-context Learning, without the need for fine-tuning.</p><hr><h3>Only My Model On My Data: A Privacy Preserving Approach Protecting one  Model and Deceiving Unauthorized Black-Box Models</h3>
<p><a href='http://arxiv.org/abs/2402.09316v1'>http://arxiv.org/abs/2402.09316v1</a></p>
<p><b>Compressor summary</b>: The study presents a method to protect image data privacy by generating human-perceivable images that can be accurately classified by authorized models while confusing unauthorized ones.</p><hr><h3>Few-Shot Object Detection with Sparse Context Transformers</h3>
<p><a href='http://arxiv.org/abs/2402.09315v1'>http://arxiv.org/abs/2402.09315v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a sparse context transformer (SCT) for few-shot object detection, which leverages source domain knowledge and learns sparse context from few target domain images to reduce class confusion and enhance detector performance.</p><hr><h3>Embracing the black box: Heading towards foundation models for causal  discovery from time series data</h3>
<p><a href='http://arxiv.org/abs/2402.09305v1'>http://arxiv.org/abs/2402.09305v1</a></p>
<p><b>Compressor summary</b>: Causal Pretraining is a method to learn causal graphs from time series data in a supervised way, which can improve performance with more data and larger models.</p><hr><h3>Immediate generalisation in humans but a generalisation lag in deep  neural networks$\unicode{x2014}$evidence for representational divergence?</h3>
<p><a href='http://arxiv.org/abs/2402.09303v1'>http://arxiv.org/abs/2402.09303v1</a></p>
<p><b>Compressor summary</b>: This study compares how humans and deep neural networks learn image classification and finds significant differences in their representational changes during the learning process.</p><hr><h3>Learning Interpretable Policies in Hindsight-Observable POMDPs through  Partially Supervised Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2402.09290v1'>http://arxiv.org/abs/2402.09290v1</a></p>
<p><b>Compressor summary</b>: PSRL is a reinforcement learning framework that fuses supervised and unsupervised learning to improve policy interpretability and performance.</p><hr><h3>EcoVal: An Efficient Data Valuation Framework for Machine Learning</h3>
<p><a href='http://arxiv.org/abs/2402.09288v1'>http://arxiv.org/abs/2402.09288v1</a></p>
<p><b>Compressor summary</b>: The paper introduces EcoVal, a fast and practical framework to estimate the value of clusters of similar data points for machine learning models, using intrinsic and extrinsic values and a production function concept.</p><hr><h3>Nutrition Facts, Drug Facts, and Model Facts: Putting AI Ethics into  Practice in Gun Violence Research</h3>
<p><a href='http://arxiv.org/abs/2402.09286v1'>http://arxiv.org/abs/2402.09286v1</a></p>
<p><b>Compressor summary</b>: The study proposes a Model Facts template to increase AI trust and transparency in firearm injury research, allowing users to assess the validity and biases of models without technical knowledge.</p><hr><h3>Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey</h3>
<p><a href='http://arxiv.org/abs/2402.09283v1'>http://arxiv.org/abs/2402.09283v1</a></p>
<p><b>Compressor summary</b>: This text is a survey of research on how to make large language models safe for conversational applications and prevent harmful responses.</p><hr><h3>Leveraging Large Language Models for Enhanced NLP Task Performance  through Knowledge Distillation and Optimized Training Strategies</h3>
<p><a href='http://arxiv.org/abs/2402.09282v1'>http://arxiv.org/abs/2402.09282v1</a></p>
<p><b>Compressor summary</b>: This paper shows how using GPT-4's Chain of Thought technique can improve NER performance in BERT by combining distilled knowledge from GPT-4 with human annotations, leading to better results and cost savings.</p><hr><h3>Synergistic eigenanalysis of covariance and Hessian matrices for  enhanced binary classification</h3>
<p><a href='http://arxiv.org/abs/2402.09281v1'>http://arxiv.org/abs/2402.09281v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method that combines covariance and Hessian matrices to improve binary classification by maximizing between-class distance and minimizing within-class variance in a two-dimensional space.</p><hr><h3>Hybrid Machine Learning techniques in the management of harmful algal  blooms impact</h3>
<p><a href='http://arxiv.org/abs/2402.09271v1'>http://arxiv.org/abs/2402.09271v1</a></p>
<p><b>Compressor summary</b>: BAGNET is a hybrid machine learning model that accurately predicts the toxicity levels of mollusc meat due to harmful algal blooms, helping to control shellfish production areas effectively.</p><hr><h3>Fast Window-Based Event Denoising with Spatiotemporal Correlation  Enhancement</h3>
<p><a href='http://arxiv.org/abs/2402.09270v1'>http://arxiv.org/abs/2402.09270v1</a></p>
<p><b>Compressor summary</b>: Window-based event denoising is a method that improves interpretability and real-time processing for deep learning-based noise removal in complex scenes, using temporal and spatial analysis to filter out irrelevant events.</p><hr><h3>Transformers, parallel computation, and logarithmic depth</h3>
<p><a href='http://arxiv.org/abs/2402.09268v1'>http://arxiv.org/abs/2402.09268v1</a></p>
<p><b>Compressor summary</b>: Transformers can efficiently simulate and be simulated by communication rounds, enabling them to solve basic computational tasks faster than other neural sequence models.</p><hr><h3>Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via  Self-Evaluation</h3>
<p><a href='http://arxiv.org/abs/2402.09267v1'>http://arxiv.org/abs/2402.09267v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to reduce factual errors in large language models by using their own self-evaluation abilities for training and fine-tuning.</p><hr><h3>Machine Learning in management of precautionary closures caused by  lipophilic biotoxins</h3>
<p><a href='http://arxiv.org/abs/2402.09266v1'>http://arxiv.org/abs/2402.09266v1</a></p>
<p><b>Compressor summary</b>: The text proposes a predictive model using the kNN algorithm to support precautionary closures in mussel farming based on harmful algal blooms, achieving high accuracy and sensitivity values.</p><hr><h3>UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers</h3>
<p><a href='http://arxiv.org/abs/2402.09264v1'>http://arxiv.org/abs/2402.09264v1</a></p>
<p><b>Compressor summary</b>: UR2M is a framework for accurate event detection and reliable uncertainty estimation on wearable devices with low resource constraints.</p><hr><h3>MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical  Vision-Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.09262v1'>http://arxiv.org/abs/2402.09262v1</a></p>
<p><b>Compressor summary</b>: MultiMedEval is an open-source toolkit for evaluating medical vision-language models on various tasks and datasets.</p><hr><h3>SyntaxShap: Syntax-aware Explainability Method for Text Generation</h3>
<p><a href='http://arxiv.org/abs/2402.09259v1'>http://arxiv.org/abs/2402.09259v1</a></p>
<p><b>Compressor summary</b>: SyntaxShap is a new method to explain text generation by considering syntactic dependencies in the data.</p><hr><h3>TDViT: Temporal Dilated Video Transformer for Dense Video Tasks</h3>
<p><a href='http://arxiv.org/abs/2402.09257v1'>http://arxiv.org/abs/2402.09257v1</a></p>
<p><b>Compressor summary</b>: The Temporal Dilated Video Transformer (TDViT) is a model that efficiently extracts spatiotemporal representations for dense video tasks, such as object detection and instance segmentation, by using temporal dilated transformer blocks and hierarchical structures.</p><hr><h3>Exploring the Relationship: Transformative Adaptive Activation Functions  in Comparison to Other Activation Functions</h3>
<p><a href='http://arxiv.org/abs/2402.09249v1'>http://arxiv.org/abs/2402.09249v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new adaptive activation function for neural networks that can transform and generalize existing activation functions, improving their performance and versatility.</p><hr><h3>Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food  Detection</h3>
<p><a href='http://arxiv.org/abs/2402.09242v1'>http://arxiv.org/abs/2402.09242v1</a></p>
<p><b>Compressor summary</b>: Zero-Shot Food Detection (ZSFD) is tackled by a new framework, ZSFDet, which uses multi-source graphs to model the correlation between food categories and attributes and improves performance on FOWA and UECFOOD-256 datasets.</p><hr><h3>Efficient One-stage Video Object Detection by Exploiting Temporal  Consistency</h3>
<p><a href='http://arxiv.org/abs/2402.09241v1'>http://arxiv.org/abs/2402.09241v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework to improve one-stage video object detection by exploiting temporal consistency, reducing computational costs, and increasing efficiency.</p><hr><h3>Switch EMA: A Free Lunch for Better Flatness and Sharpness</h3>
<p><a href='http://arxiv.org/abs/2402.09240v1'>http://arxiv.org/abs/2402.09240v1</a></p>
<p><b>Compressor summary</b>: Switch EMA (SEMA) is a simple modification to the Exponential Moving Average regularization method in deep neural networks, which improves generalization and convergence without extra costs.</p><hr><h3>Robust Training of Temporal GNNs using Nearest Neighbours based Hard  Negatives</h3>
<p><a href='http://arxiv.org/abs/2402.09239v1'>http://arxiv.org/abs/2402.09239v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to train temporal graph neural networks (TGNNs) by using importance-based negative sampling instead of uniform random sampling, which improves their performance in future-link prediction tasks.</p><hr><h3>Weatherproofing Retrieval for Localization with Generative AI and  Geometric Consistency</h3>
<p><a href='http://arxiv.org/abs/2402.09237v1'>http://arxiv.org/abs/2402.09237v1</a></p>
<p><b>Compressor summary</b>: The paper improves image retrieval for visual localization by generating synthetic variants of training images and using a tailored training approach.</p><hr><h3>Learning Interpretable Concepts: Unifying Causal Representation Learning  and Foundation Models</h3>
<p><a href='http://arxiv.org/abs/2402.09236v1'>http://arxiv.org/abs/2402.09236v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to learn interpretable concepts from data using ideas from causal representation learning and foundation models, and demonstrates its effectiveness on synthetic and natural language data.</p><hr><h3>Multi-Hierarchical Surrogate Learning for Structural Dynamics of  Automotive Crashworthiness Using Graph Convolutional Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.09234v1'>http://arxiv.org/abs/2402.09234v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Crash simulations are important but computationally expensive
- A multi-hierarchical framework for creating surrogate models at different resolutions is proposed
- Surrogates learn low-dimensional latent dynamics and transfer learning is used to pass information between levels

Summary:
The authors propose a method to create surrogate models of crash simulations at different resolutions, using graph convolutional networks and transfer learning to capture macroscale and microscale features efficiently.</p><hr><h3>Directional Convergence Near Small Initializations and Saddles in  Two-Homogeneous Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.09226v1'>http://arxiv.org/abs/2402.09226v1</a></p>
<p><b>Compressor summary</b>: The paper studies how two-homogeneous neural networks with small initializations converge in direction to KKT points of a correlation function using gradient flow dynamics and saddle-to-saddle dynamics.</p><hr><h3>Is my Data in your AI Model? Membership Inference Test with Application  to Face Images</h3>
<p><a href='http://arxiv.org/abs/2402.09225v1'>http://arxiv.org/abs/2402.09225v1</a></p>
<p><b>Compressor summary</b>: The paper presents MINT, a method to determine if an AI model was trained with specific data, using two novel architectures based on MLP and CNNs, and evaluates them on face recognition tasks with six databases.</p><hr><h3>Spectral Filters, Dark Signals, and Attention Sinks</h3>
<p><a href='http://arxiv.org/abs/2402.09221v1'>http://arxiv.org/abs/2402.09221v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a method to analyze and control attention mechanisms in transformer-based LLMs using spectral filters on intermediate representations.</p><hr><h3>Scaling the Authoring of AutoTutors with Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.09216v1'>http://arxiv.org/abs/2402.09216v1</a></p>
<p><b>Compressor summary</b>: The paper explores using Large Language Models (LLMs) to create Intelligent Tutoring Systems with handcrafted pedagogical designs, and presents MWPTutor, a sample system that outperforms GPT-4 in human evaluation on math word problems.</p><hr><h3>DivaTrack: Diverse Bodies and Motions from Acceleration-Enhanced  Three-Point Trackers</h3>
<p><a href='http://arxiv.org/abs/2402.09211v1'>http://arxiv.org/abs/2402.09211v1</a></p>
<p><b>Compressor summary</b>: DivaTrack is a deep learning framework that improves full-body pose estimation in digital reality by using linear accelerations from IMUs and blending predictions in two reference frames, outperforming existing methods on diverse body sizes and activities.</p><hr><h3>Tell Me More! Towards Implicit User Intention Understanding of Language  Model Driven Agents</h3>
<p><a href='http://arxiv.org/abs/2402.09205v1'>http://arxiv.org/abs/2402.09205v1</a></p>
<p><b>Compressor summary</b>: IN3 is a benchmark for inspecting users' implicit intentions in language model-driven agents, while Mistral-Interact is a powerful model that uses IN3 to improve user-agent interaction by refining vague tasks into actionable goals.</p><hr><h3>Domain-adaptive and Subgroup-specific Cascaded Temperature Regression  for Out-of-distribution Calibration</h3>
<p><a href='http://arxiv.org/abs/2402.09204v1'>http://arxiv.org/abs/2402.09204v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new post-hoc calibration method for deep neural networks that adapts to different test sets by using data augmentation and simulating domain shifts.</p><hr><h3>Better-than-KL PAC-Bayes Bounds</h3>
<p><a href='http://arxiv.org/abs/2402.09201v1'>http://arxiv.org/abs/2402.09201v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel better-than-KL divergence for PAC-Bayes concentration inequalities and shows it can achieve strictly tighter bounds for estimating the mean of random sequences.</p><hr><h3>Ten Words Only Still Help: Improving Black-Box AI-Generated Text  Detection via Proxy-Guided Efficient Re-Sampling</h3>
<p><a href='http://arxiv.org/abs/2402.09199v1'>http://arxiv.org/abs/2402.09199v1</a></p>
<p><b>Compressor summary</b>: The paper proposes POGER, a method to improve AIGT detection by estimating word generation probabilities using multiple re-sampling in black-box settings.</p><hr><h3>Implementing local-explainability in Gradient Boosting Trees: Feature  Contribution</h3>
<p><a href='http://arxiv.org/abs/2402.09197v1'>http://arxiv.org/abs/2402.09197v1</a></p>
<p><b>Compressor summary</b>: Key points:
- GBDT is a black-box model based on tree ensembles
- A feature contribution method for GBDT is developed using node residues
- The method allows to calculate node decisions and explain GBDT's behavior
- The method is useful for ethical analysis of AI and compliance with GDPR

Summary:
The paper proposes a feature contribution method for GBDT that uses node residues to calculate node decisions and explain the model's behavior, which can help with ethical and legal issues in AI.</p><hr><h3>(Ir)rationality and Cognitive Biases in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.09193v1'>http://arxiv.org/abs/2402.09193v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates language models' rational reasoning abilities using cognitive tasks and finds that they display irrationality similar to humans but with different biases and inconsistent responses.</p><hr><h3>Generalized Portrait Quality Assessment</h3>
<p><a href='http://arxiv.org/abs/2402.09178v1'>http://arxiv.org/abs/2402.09178v1</a></p>
<p><b>Compressor summary</b>: FHIQA is a learning-based method for assessing portrait quality in images that uses image semantics to improve precision and generalize to various scenes, as shown by experiments on the PIQ23 benchmark.</p><hr><h3>Leveraging the Context through Multi-Round Interactions for Jailbreaking  Attacks</h3>
<p><a href='http://arxiv.org/abs/2402.09177v1'>http://arxiv.org/abs/2402.09177v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new Jailbreaking attack on large language models that uses preliminary question-answer pairs to guide the model's response towards revealing harmful information.</p><hr><h3>Nearly Optimal Regret for Decentralized Online Convex Optimization</h3>
<p><a href='http://arxiv.org/abs/2402.09173v1'>http://arxiv.org/abs/2402.09173v1</a></p>
<p><b>Compressor summary</b>: The paper proposes new decentralized online convex optimization algorithms with improved regret bounds by using an online accelerated gossip strategy and exploiting network topology spectral properties.</p><hr><h3>Evolving Restricted Boltzmann Machine-Kohonen Network for Online  Clustering</h3>
<p><a href='http://arxiv.org/abs/2402.09167v1'>http://arxiv.org/abs/2402.09167v1</a></p>
<p><b>Compressor summary</b>: The paper introduces ERBM-KNet, a novel online clustering algorithm that combines an evolving restricted Boltzmann machine with a Kohonen network, achieving improved performance and handling streaming data efficiently.</p><hr><h3>Deinterleaving of Discrete Renewal Process Mixtures with Application to  Electronic Support Measures</h3>
<p><a href='http://arxiv.org/abs/2402.09166v1'>http://arxiv.org/abs/2402.09166v1</a></p>
<p><b>Compressor summary</b>: Key points:
- New deinterleaving method for mixtures of discrete renewal Markov chains
- Method maximizes a penalized likelihood score
- Theoretical analysis proves the method's accuracy under mild conditions
- Experiments on synthetic data validate the theory
- Applied to pulse trains in RESM context and performs well

Summary:
The paper presents a new deinterleaving method that uses a penalized likelihood score to separate symbols from discrete renewal Markov chains, and shows its theoretical and practical advantages.</p><hr><h3>Unifying Invariance and Spuriousity for Graph Out-of-Distribution via  Probability of Necessity and Sufficiency</h3>
<p><a href='http://arxiv.org/abs/2402.09165v1'>http://arxiv.org/abs/2402.09165v1</a></p>
<p><b>Compressor summary</b>: The PNSIS framework uses probability theory to extract invariant subgraphs and improve generalization for graph Out-of-Distribution tasks, outperforming existing methods.</p><hr><h3>Less is More: Fewer Interpretable Region via Submodular Subset Selection</h3>
<p><a href='http://arxiv.org/abs/2402.09164v1'>http://arxiv.org/abs/2402.09164v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new image attribution method using submodular subset selection to improve model interpretability and accuracy for various samples, outperforming existing methods on three datasets.</p><hr><h3>Role-Playing Simulation Games using ChatGPT</h3>
<p><a href='http://arxiv.org/abs/2402.09161v1'>http://arxiv.org/abs/2402.09161v1</a></p>
<p><b>Compressor summary</b>: The article shows how Large Language Models and ChatGPT can improve teaching quality and student engagement through role-playing simulations during the digital transformation of education.</p><hr><h3>Attacking Large Language Models with Projected Gradient Descent</h3>
<p><a href='http://arxiv.org/abs/2402.09154v1'>http://arxiv.org/abs/2402.09154v1</a></p>
<p><b>Compressor summary</b>: Our method improves the speed and efficiency of creating adversarial prompts against large language models while maintaining their effectiveness.</p><hr><h3>Improved Regret for Bandit Convex Optimization with Delayed Feedback</h3>
<p><a href='http://arxiv.org/abs/2402.09152v1'>http://arxiv.org/abs/2402.09152v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm for bandit convex optimization with delayed feedback that improves the regret bound for various scenarios, including strong convexity and unconstrained action sets.</p><hr><h3>Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for  Chinese Mental Health Text Analysis</h3>
<p><a href='http://arxiv.org/abs/2402.09151v1'>http://arxiv.org/abs/2402.09151v1</a></p>
<p><b>Compressor summary</b>: The authors created a specialized pre-trained language model for psychological text analysis using a large dataset from Chinese social media platforms and integrated psychological lexicons into the training process.</p><hr><h3>Into the Unknown: Self-Learning Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.09147v1'>http://arxiv.org/abs/2402.09147v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework for large language models (LLMs) to learn unknown knowledge from their own hallucinations, using a score called Points in The Unknown (PiUs), and shows that finetuned or aligned 7B-Mistral models can self-learn effectively.</p><hr><h3>ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural  Networks</h3>
<p><a href='http://arxiv.org/abs/2402.09146v1'>http://arxiv.org/abs/2402.09146v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Residual Quanvolutional Neural Networks (ResQuNNs) that enable training of quanvolutional layers, improving the performance of quantum deep learning by addressing gradient-based optimization challenges.</p><hr><h3>When Representations Align: Universality in Representation Learning  Dynamics</h3>
<p><a href='http://arxiv.org/abs/2402.09142v1'>http://arxiv.org/abs/2402.09142v1</a></p>
<p><b>Compressor summary</b>: The study proposes an effective theory of representation learning in deep neural networks that shows how different architectures learn similar representations when they are flexible enough and highlights behaviors that are conserved across various models.</p><hr><h3>Advancing NLP Models with Strategic Text Augmentation: A Comprehensive  Study of Augmentation Methods and Curriculum Strategies</h3>
<p><a href='http://arxiv.org/abs/2402.09141v1'>http://arxiv.org/abs/2402.09141v1</a></p>
<p><b>Compressor summary</b>: The study evaluates text augmentation techniques and their effects on NLP tasks, proposing Modified Cyclical Curriculum Learning (MCCL) as a novel approach to improve performance.</p><hr><h3>DolphCoder: Echo-Locating Code Large Language Models with Diverse and  Multi-Objective Instruction Tuning</h3>
<p><a href='http://arxiv.org/abs/2402.09136v1'>http://arxiv.org/abs/2402.09136v1</a></p>
<p><b>Compressor summary</b>: DolphCoder is a diverse instruction model for code generation that learns from various instruction targets and self-evaluates its code quality, achieving superior performance on benchmarks.</p><hr><h3>Exploring the Adversarial Capabilities of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.09132v1'>http://arxiv.org/abs/2402.09132v1</a></p>
<p><b>Compressor summary</b>: The text discusses the security risks of large language models, which can generate adversarial examples that fool hate speech detection systems and other safety measures.</p><hr><h3>Measuring Exploration in Reinforcement Learning via Optimal Transport in  Policy Space</h3>
<p><a href='http://arxiv.org/abs/2402.09113v1'>http://arxiv.org/abs/2402.09113v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new measure called Exploration Index, which quantifies how much an RL algorithm explores compared to supervised learning, by measuring the distance travelled in the data distribution space using optimal transport metrics.</p><hr><h3>Headset: Human emotion awareness under partial occlusions multimodal  dataset</h3>
<p><a href='http://arxiv.org/abs/2402.09107v1'>http://arxiv.org/abs/2402.09107v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new multimodal database with diverse and ethically compliant volumetric data of people speaking and wearing HMDs, recorded using a VoCap studio and a Lytro Illum camera, to aid XR algorithm development and testing.</p><hr><h3>Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs</h3>
<p><a href='http://arxiv.org/abs/2402.09100v1'>http://arxiv.org/abs/2402.09100v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a network for removing occlusions from facial videos using GANs and preserving emotional expression, which can be useful for various applications like video conferencing and virtual makeup.</p><hr><h3>Exploring Neuron Interactions and Emergence in LLMs: From the  Multifractal Analysis Perspective</h3>
<p><a href='http://arxiv.org/abs/2402.09099v1'>http://arxiv.org/abs/2402.09099v1</a></p>
<p><b>Compressor summary</b>: The study explores how neuron interactions evolve during training in large language models, using self-organization and multifractal analysis to reveal emergent behavior and propose Neuron-based Multifractal Analysis (NeuroMFA) as a tool for quantitative analysis.</p><hr><h3>Three Decades of Activations: A Comprehensive Survey of 400 Activation  Functions for Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.09092v1'>http://arxiv.org/abs/2402.09092v1</a></p>
<p><b>Compressor summary</b>: The paper presents a large survey of 400 activation functions for neural networks, providing a comprehensive overview and systematization with links to original sources.</p><hr><h3>Polynomial Semantics of Tractable Probabilistic Circuits</h3>
<p><a href='http://arxiv.org/abs/2402.09085v1'>http://arxiv.org/abs/2402.09085v1</a></p>
<p><b>Compressor summary</b>: The paper proves that different probabilistic circuit models for binary distributions are equivalent and explores the challenges of extending them to categorical random variables.</p><hr><h3>Sobolev Training for Operator Learning</h3>
<p><a href='http://arxiv.org/abs/2402.09084v1'>http://arxiv.org/abs/2402.09084v1</a></p>
<p><b>Compressor summary</b>: Sobolev Training improves model performance by using derivative information on irregular meshes for operator learning.</p><hr><h3>Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic  Methods</h3>
<p><a href='http://arxiv.org/abs/2402.09078v1'>http://arxiv.org/abs/2402.09078v1</a></p>
<p><b>Compressor summary</b>: The paper presents two new RL algorithms (ExpD3 and BE-TD3) that address and use estimation biases for better continuous control tasks, outperforming existing methods like TD3 in some scenarios.</p><hr><h3>Affine transformation estimation improves visual self-supervised  learning</h3>
<p><a href='http://arxiv.org/abs/2402.09071v1'>http://arxiv.org/abs/2402.09071v1</a></p>
<p><b>Compressor summary</b>: The paper proposes adding a predictive module to self-supervised learning models that improves their performance and efficiency by constraining representations with an additional loss term based on an affine transformation.</p><hr><h3>Solid Waste Detection in Remote Sensing Images: A Survey</h3>
<p><a href='http://arxiv.org/abs/2402.09066v1'>http://arxiv.org/abs/2402.09066v1</a></p>
<p><b>Compressor summary</b>: Remote sensing using EO satellites can help detect and monitor illegal landfills, mitigating pollution and health hazards, by providing high-resolution data at low cost.</p><hr><h3>Soft Prompt Threats: Attacking Safety Alignment and Unlearning in  Open-Source LLMs through the Embedding Space</h3>
<p><a href='http://arxiv.org/abs/2402.09063v1'>http://arxiv.org/abs/2402.09063v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new attack on open-source language models that targets their continuous input representations, showing how it can lead to harmful behaviors and extract deleted information.</p><hr><h3>I can't see it but I can Fine-tune it: On Encrypted Fine-tuning of  Transformers using Fully Homomorphic Encryption</h3>
<p><a href='http://arxiv.org/abs/2402.09059v1'>http://arxiv.org/abs/2402.09059v1</a></p>
<p><b>Compressor summary</b>: BlindTuner is a system that enables privacy-preserving fine-tuning of transformer models on encrypted image data for image classification, achieving comparable accuracy and significant speed improvements over existing methods.</p><hr><h3>Is Epistemic Uncertainty Faithfully Represented by Evidential Deep  Learning Methods?</h3>
<p><a href='http://arxiv.org/abs/2402.09056v1'>http://arxiv.org/abs/2402.09056v1</a></p>
<p><b>Compressor summary</b>: The paper discusses challenges and novel insights of evidential deep learning for quantifying uncertainty in ML systems, focusing on optimizing second-order loss functions and interpreting epistemic uncertainty measures.</p><hr><h3>Comment-aided Video-Language Alignment via Contrastive Pre-training for  Short-form Video Humor Detection</h3>
<p><a href='http://arxiv.org/abs/2402.09055v1'>http://arxiv.org/abs/2402.09055v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new multi-modal humor detection model for short videos that aligns video and text in a shared meaning space and beats existing methods on two datasets.</p><hr><h3>L3GO: Language Agents with Chain-of-3D-Thoughts for Generating  Unconventional Objects</h3>
<p><a href='http://arxiv.org/abs/2402.09052v1'>http://arxiv.org/abs/2402.09052v1</a></p>
<p><b>Compressor summary</b>: L3GO is a language agent that uses large language models to reason about 3D mesh generation of unconventional objects in simulation environments, outperforming standard GPT-4 and other models.</p><hr><h3>FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep  Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2402.09051v1'>http://arxiv.org/abs/2402.09051v1</a></p>
<p><b>Compressor summary</b>: The paper presents FGeoDRL, a neural-symbolic system that uses reinforcement learning to perform human-like geometric deductive reasoning without human supervision.</p><hr><h3>End-to-End Training Induces Information Bottleneck through Layer-Role  Differentiation: A Comparative Analysis with Layer-wise Training</h3>
<p><a href='http://arxiv.org/abs/2402.09050v1'>http://arxiv.org/abs/2402.09050v1</a></p>
<p><b>Compressor summary</b>: E2E training outperforms non-E2E methods due to efficient input information propagation and layer-role differentiation in intermediate representations.</p><hr><h3>FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems</h3>
<p><a href='http://arxiv.org/abs/2402.09047v1'>http://arxiv.org/abs/2402.09047v1</a></p>
<p><b>Compressor summary</b>: This paper introduces FGeo-TP, a theorem predictor that uses Transformers to help solve geometry problems faster and with fewer timeouts.</p><hr><h3>Inference of Abstraction for a Unified Account of Reasoning and Learning</h3>
<p><a href='http://arxiv.org/abs/2402.09046v1'>http://arxiv.org/abs/2402.09046v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a probabilistic inference theory that unifies reasoning and learning by modeling how data leads to symbolic knowledge through abstraction and selective ignorance.</p><hr><h3>Under manipulations, are some AI models harder to audit?</h3>
<p><a href='http://arxiv.org/abs/2402.09043v1'>http://arxiv.org/abs/2402.09043v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how difficult it is to audit web platforms with large-capacity models that can fit any data, and shows that such platforms are hard to audit robustly.</p><hr><h3>Can Text-to-image Model Assist Multi-modal Learning for Visual  Recognition with Visual Modality Missing?</h3>
<p><a href='http://arxiv.org/abs/2402.09036v1'>http://arxiv.org/abs/2402.09036v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a framework called GTI-MM that uses text-to-image models to help multi-modal learning for vision recognition tasks, improving data efficiency and robustness when some visual modalities are missing.</p><hr><h3>Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST)  Activation Under Data Constraints</h3>
<p><a href='http://arxiv.org/abs/2402.09034v1'>http://arxiv.org/abs/2402.09034v1</a></p>
<p><b>Compressor summary</b>: Squared Sigmoid TanH (SST) activation improves the performance of sequential neural networks on small and sparse datasets by amplifying differences between strong and weak activations over time.</p><hr><h3>SLEB: Streamlining LLMs through Redundancy Verification and Elimination  of Transformer Blocks</h3>
<p><a href='http://arxiv.org/abs/2402.09025v1'>http://arxiv.org/abs/2402.09025v1</a></p>
<p><b>Compressor summary</b>: SLEB prunes redundant transformer blocks from large language models to speed up inference without sacrificing linguistic capabilities.</p><hr><h3>Pyramid Attention Network for Medical Image Registration</h3>
<p><a href='http://arxiv.org/abs/2402.09016v1'>http://arxiv.org/abs/2402.09016v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a pyramid attention network (PAN) for deformable medical image registration, which improves feature representation and motion pattern analysis using a dual-stream encoder and a local attention Transformer decoder.</p><hr><h3>Towards better Human-Agent Alignment: Assessing Task Utility in  LLM-Powered Applications</h3>
<p><a href='http://arxiv.org/abs/2402.09015v1'>http://arxiv.org/abs/2402.09015v1</a></p>
<p><b>Compressor summary</b>: AgentEval is a framework that simplifies verifying the utility of LLM-powered applications by proposing criteria tailored to their unique purposes and assessing their performance against them.</p><hr><h3>Multi-Query Focused Disaster Summarization via Instruction-Based  Prompting</h3>
<p><a href='http://arxiv.org/abs/2402.09008v1'>http://arxiv.org/abs/2402.09008v1</a></p>
<p><b>Compressor summary</b>: CrisisFACTS is a competition that challenges participants to develop systems for disaster summarization using web sources, focusing on fact extraction and QA-based prompting with LLaMA-13b model.</p><hr><h3>Gradient Alignment with Prototype Feature for Fully Test-time Adaptation</h3>
<p><a href='http://arxiv.org/abs/2402.09004v1'>http://arxiv.org/abs/2402.09004v1</a></p>
<p><b>Compressor summary</b>: GAP is a regularizer for Test-time Adaptation that uses gradient alignment and prototype features to prevent negative impacts from misclassified pseudo labels and improve performance on various datasets.</p><hr><h3>Nearly Minimax Optimal Regret for Learning Linear Mixture Stochastic  Shortest Path</h3>
<p><a href='http://arxiv.org/abs/2402.08998v1'>http://arxiv.org/abs/2402.08998v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm for the Stochastic Shortest Path problem with a linear mixture transition kernel, which eliminates restrictive assumptions and achieves near-optimality in regret bound.</p><hr><h3>CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic  Decoding</h3>
<p><a href='http://arxiv.org/abs/2402.08994v1'>http://arxiv.org/abs/2402.08994v1</a></p>
<p><b>Compressor summary</b>: The CLIP-MUSED method uses a Transformer-based feature extractor, learnable subject-specific tokens, and representational similarity analysis to decode visual neural information from multiple subjects, overcoming limitations of prior methods.</p><hr><h3>MEL: Efficient Multi-Task Evolutionary Learning for High-Dimensional  Feature Selection</h3>
<p><a href='http://arxiv.org/abs/2402.08982v1'>http://arxiv.org/abs/2402.08982v1</a></p>
<p><b>Compressor summary</b>: MEL is a novel evolutionary computational approach that uses multi-task learning to improve feature selection in high-dimensional data and achieve better performance and efficiency than existing methods.</p><hr><h3>Research and application of Transformer based anomaly detection model: A  literature review</h3>
<p><a href='http://arxiv.org/abs/2402.08975v1'>http://arxiv.org/abs/2402.08975v1</a></p>
<p><b>Compressor summary</b>: This is a review paper that provides an overview of how Transformers and their variants are used for anomaly detection, discusses the challenges, datasets, evaluation metrics, and future trends in this domain, and compiles over 100 references related to Transformer-based anomaly detection.</p><hr><h3>GrounDial: Human-norm Grounded Safe Dialog Response Generation</h3>
<p><a href='http://arxiv.org/abs/2402.08968v1'>http://arxiv.org/abs/2402.08968v1</a></p>
<p><b>Compressor summary</b>: GrounDial is a safe conversational AI system that uses commonsense social rules instead of fine-tuning, improving safety and reducing costs.</p><hr><h3>Pretraining Vision-Language Model for Difference Visual Question  Answering in Longitudinal Chest X-rays</h3>
<p><a href='http://arxiv.org/abs/2402.08966v1'>http://arxiv.org/abs/2402.08966v1</a></p>
<p><b>Compressor summary</b>: The paper introduces PLURAL, a pretrained vision-language model that excels at difference visual question answering for chest X-ray images, by using longitudinal data to train on both natural and longitudinal image-text pairs.</p><hr><h3>Predicting User Experience on Laptops from Hardware Specifications</h3>
<p><a href='http://arxiv.org/abs/2402.08964v1'>http://arxiv.org/abs/2402.08964v1</a></p>
<p><b>Compressor summary</b>: The authors present a method to predict user experience on laptops from hardware specifications, using web browsing, video playback, and audio/video calls as indicators.</p><hr><h3>DUEL: Duplicate Elimination on Active Memory for Self-Supervised  Class-Imbalanced Learning</h3>
<p><a href='http://arxiv.org/abs/2402.08963v1'>http://arxiv.org/abs/2402.08963v1</a></p>
<p><b>Compressor summary</b>: DUEL is a novel framework that uses active data filtering during self-supervised pre-training to address class imbalances cost-efficiently by enhancing distinctiveness information in an active memory.</p><hr><h3>HyCubE: Efficient Knowledge Hypergraph 3D Circular Convolutional  Embedding</h3>
<p><a href='http://arxiv.org/abs/2402.08961v1'>http://arxiv.org/abs/2402.08961v1</a></p>
<p><b>Compressor summary</b>: HyCubE is a novel 3D circular convolutional neural network that improves knowledge hypergraph embedding performance and efficiency, achieving significant improvements over existing methods.</p><hr><h3>Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision</h3>
<p><a href='http://arxiv.org/abs/2402.08960v1'>http://arxiv.org/abs/2402.08960v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a weakly-supervised open-vocabulary segmentation framework that uses independent image-mask and image-text pairs to predict masks and associate them with entities in CLIP embedding space, improving performance on challenging datasets.</p><hr><h3>Towards Next-Level Post-Training Quantization of Hyper-Scale  Transformers</h3>
<p><a href='http://arxiv.org/abs/2402.08958v1'>http://arxiv.org/abs/2402.08958v1</a></p>
<p><b>Compressor summary</b>: The paper proposes aespa, a novel PTQ algorithm for Transformers that balances accuracy and efficiency by quantizing layer-wise and considering cross-layer dependency.</p><hr><h3>MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data</h3>
<p><a href='http://arxiv.org/abs/2402.08957v1'>http://arxiv.org/abs/2402.08957v1</a></p>
<p><b>Compressor summary</b>: MUSTARD is a data generation framework that creates high-quality theorem and proof datasets for training large language models in mathematical reasoning tasks.</p><hr><h3>Using Counterfactual Tasks to Evaluate the Generality of Analogical  Reasoning in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.08955v1'>http://arxiv.org/abs/2402.08955v1</a></p>
<p><b>Compressor summary</b>: The study shows that large language models struggle with analogies that are dissimilar to their training data, unlike humans who maintain high performance across different analogy problems.</p><hr><h3>Mean-Field Analysis for Learning Subspace-Sparse Polynomials with  Gaussian Input</h3>
<p><a href='http://arxiv.org/abs/2402.08948v1'>http://arxiv.org/abs/2402.08948v1</a></p>
<p><b>Compressor summary</b>: The paper investigates learning sparse polynomials with neural networks and gradient descent, and provides a basis-free generalization and a nearly sufficient condition for learnability.</p><hr><h3>Measuring Sharpness in Grokking</h3>
<p><a href='http://arxiv.org/abs/2402.08946v1'>http://arxiv.org/abs/2402.08946v1</a></p>
<p><b>Compressor summary</b>: This paper introduces a method to measure grokking in neural networks, a phenomenon where performance improves significantly after reaching a plateau on a validation set, and investigates its sharpness under two settings.</p><hr><h3>Evaluating DTW Measures via a Synthesis Framework for Time-Series Data</h3>
<p><a href='http://arxiv.org/abs/2402.08943v1'>http://arxiv.org/abs/2402.08943v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a synthesis framework to model variations between time-series data sequences and evaluate different DTW measures for alignment and classification tasks.</p><hr><h3>Premise Order Matters in Reasoning with Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.08939v1'>http://arxiv.org/abs/2402.08939v1</a></p>
<p><b>Compressor summary</b>: LLMs struggle with reasoning tasks when the order of premises is changed, even though the underlying task remains unchanged.</p><hr><h3>Predictive Temporal Attention on Event-based Video Stream for  Energy-efficient Situation Awareness</h3>
<p><a href='http://arxiv.org/abs/2402.08936v1'>http://arxiv.org/abs/2402.08936v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a temporal attention mechanism for DVS cameras that reduces power consumption by only focusing on unpredictable visual events, filtering out noise and decreasing computational workload.</p><hr><h3>Depth-aware Volume Attention for Texture-less Stereo Matching</h3>
<p><a href='http://arxiv.org/abs/2402.08931v1'>http://arxiv.org/abs/2402.08931v1</a></p>
<p><b>Compressor summary</b>: Our method improves stereo matching and depth estimation for textured and texture-less images using a volume refinement scheme that incorporates depth, attention, and a new evaluation metric.</p><hr><h3>Second Order Methods for Bandit Optimization and Control</h3>
<p><a href='http://arxiv.org/abs/2402.08929v1'>http://arxiv.org/abs/2402.08929v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a simple and practical online decision making algorithm for high dimensional data that achieves optimal regret bounds for a large class of convex functions, including linear, quadratic, and generalized linear models.</p><hr><h3>MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with  Diverse Human Preferences</h3>
<p><a href='http://arxiv.org/abs/2402.08925v1'>http://arxiv.org/abs/2402.08925v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to align language models with diverse human preferences using a mixture of preference distributions and a MaxMin alignment objective, achieving better performance and fairness than conventional methods.</p><hr><h3>IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human  Pose Estimation with Transformer Architecture</h3>
<p><a href='http://arxiv.org/abs/2402.08923v1'>http://arxiv.org/abs/2402.08923v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for predicting human poses using IMU data, which improves upon previous models by optimizing IMU placement and using a transformer-based model.</p><hr><h3>The Mirrored Influence Hypothesis: Efficient Data Influence Estimation  by Harnessing Forward Passes</h3>
<p><a href='http://arxiv.org/abs/2402.08922v1'>http://arxiv.org/abs/2402.08922v1</a></p>
<p><b>Compressor summary</b>: The paper introduces the Mirrored Influence Hypothesis, which suggests that evaluating the influence of training data on test predictions can be reformulated as an inverse problem, and proposes a new method for estimating this influence more efficiently.</p><hr><h3>Interpretable Measures of Conceptual Similarity by  Complexity-Constrained Descriptive Auto-Encoding</h3>
<p><a href='http://arxiv.org/abs/2402.08919v1'>http://arxiv.org/abs/2402.08919v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to measure conceptual similarity between images using captions and shows it correlates with human judgement and outperforms existing methods.</p><hr><h3>Graph Inference Acceleration by Learning MLPs on Graphs without  Supervision</h3>
<p><a href='http://arxiv.org/abs/2402.08918v1'>http://arxiv.org/abs/2402.08918v1</a></p>
<p><b>Compressor summary</b>: The paper introduces SimMLP, a framework to learn MLPs on graphs without supervision, achieving better generalization and acceleration than existing methods.</p><hr><h3>Learning-based Bone Quality Classification Method for Spinal Metastasis</h3>
<p><a href='http://arxiv.org/abs/2402.08910v1'>http://arxiv.org/abs/2402.08910v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a learning-based method for automatic bone quality classification in spinal metastasis using CT images, which improves performance with multi-task learning and self-paced learning.</p><hr><h3>Tackling Negative Transfer on Graphs</h3>
<p><a href='http://arxiv.org/abs/2402.08907v1'>http://arxiv.org/abs/2402.08907v1</a></p>
<p><b>Compressor summary</b>: The paper studies negative transfer in graph learning, finding that structural differences cause node embedding discrepancies, and proposes subgraph pooling methods to address this issue.</p><hr><h3>Weakly Supervised Segmentation of Vertebral Bodies with Iterative  Slice-propagation</h3>
<p><a href='http://arxiv.org/abs/2402.08892v1'>http://arxiv.org/abs/2402.08892v1</a></p>
<p><b>Compressor summary</b>: WISS is a weakly supervised method that uses corner landmarks to segment vertebral bodies from CT images with high accuracy and reduced annotation costs.</p><hr><h3>Moving Object Proposals with Deep Learned Optical Flow for Video Object  Segmentation</h3>
<p><a href='http://arxiv.org/abs/2402.08882v1'>http://arxiv.org/abs/2402.08882v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a new neural network architecture for moving object proposals (MOP) using optical flow estimation and semantic segmentation.
- The paper uses DAVIS Dataset and Encoder-Decoder architecture as main contributions.
- The paper provides the codes with TensorFlow and runs them on AWS EC2 instance.

Summary:
The paper presents a new neural network method for MOP using optical flow and semantic segmentation, leveraging DAVIS Dataset and Encoder-Decoder model, and providing TensorFlow codes and AWS execution.</p><hr><h3>DUDF: Differentiable Unsigned Distance Fields with Hyperbolic Scaling</h3>
<p><a href='http://arxiv.org/abs/2402.08876v1'>http://arxiv.org/abs/2402.08876v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a hyperbolic scaling method for learning unsigned distance fields, which improves 3D reconstruction quality, training performance, and enables accurate computation of topological properties.</p><hr><h3>TikTokActions: A TikTok-Derived Video Dataset for Human Action  Recognition</h3>
<p><a href='http://arxiv.org/abs/2402.08875v1'>http://arxiv.org/abs/2402.08875v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new TikTok dataset, TikTokActions, for human action recognition and shows its effectiveness in improving computer vision models.</p><hr><h3>Tree-Based Hard Attention with Self-Motivation for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.08874v1'>http://arxiv.org/abs/2402.08874v1</a></p>
<p><b>Compressor summary</b>: TEAROOM is a novel framework that helps large language models understand hierarchical text structures and improve their performance in estimating task-specific properties.</p><hr><h3>Position Paper: Challenges and Opportunities in Topological Deep  Learning</h3>
<p><a href='http://arxiv.org/abs/2402.08871v1'>http://arxiv.org/abs/2402.08871v1</a></p>
<p><b>Compressor summary</b>: Topological deep learning uses topological features to enhance deep learning models and has promising applications and theoretical foundations that require further investigation.</p><hr><h3>ScamSpot: Fighting Financial Fraud in Instagram Comments</h3>
<p><a href='http://arxiv.org/abs/2402.08869v1'>http://arxiv.org/abs/2402.08869v1</a></p>
<p><b>Compressor summary</b>: ScamSpot is a system that combats spam and fraud in Instagram comments using a browser extension, a BERT model, and a REST API, with data annotation, user feedback, and open-source availability.</p><hr><h3>Large Language Model with Graph Convolution for Recommendation</h3>
<p><a href='http://arxiv.org/abs/2402.08859v1'>http://arxiv.org/abs/2402.08859v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to use large language models for improving item descriptions in recommendations by leveraging graph information and avoiding hallucination problems.</p>