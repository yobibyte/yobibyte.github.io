
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-02-08</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-02-08 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Edu-ConvoKit: An Open-Source Library for Education Conversation Data</h3>
<p><a href='http://arxiv.org/abs/2402.05111v1'>http://arxiv.org/abs/2402.05111v1</a></p>
<p><b>Compressor summary</b>: Edu-ConvoKit is an open-source library for analyzing education conversation data with pre-processing, annotation, and analysis features.</p><hr><h3>Opening the AI black box: program synthesis via mechanistic  interpretability</h3>
<p><a href='http://arxiv.org/abs/2402.05110v1'>http://arxiv.org/abs/2402.05110v1</a></p>
<p><b>Compressor summary</b>: MIPS is a new method that uses neural networks to synthesize Python code from algorithmic tasks and makes them more interpretable and trustworthy.</p><hr><h3>Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding</h3>
<p><a href='http://arxiv.org/abs/2402.05109v1'>http://arxiv.org/abs/2402.05109v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Hydra heads, a sequentially dependent replacement for standard draft heads in speculative decoding, which improves accuracy and throughput compared to existing methods.</p><hr><h3>Image captioning for Brazilian Portuguese using GRIT model</h3>
<p><a href='http://arxiv.org/abs/2402.05106v1'>http://arxiv.org/abs/2402.05106v1</a></p>
<p><b>Compressor summary</b>: The authors developed a Brazilian Portuguese version of the GRIT model, which generates better image captions using two visual features.</p><hr><h3>Hydragen: High-Throughput LLM Inference with Shared Prefixes</h3>
<p><a href='http://arxiv.org/abs/2402.05099v1'>http://arxiv.org/abs/2402.05099v1</a></p>
<p><b>Compressor summary</b>: Hydragen is a hardware-aware attention implementation that significantly improves efficiency for large language models with shared prefixes.</p><hr><h3>On diffusion models for amortized inference: Benchmarking and improving  stochastic control and sampling</h3>
<p><a href='http://arxiv.org/abs/2402.05098v1'>http://arxiv.org/abs/2402.05098v1</a></p>
<p><b>Compressor summary</b>: The paper compares different diffusion models, proposes a new exploration strategy for off-policy methods, and provides open-source code for future research.</p><hr><h3>NITO: Neural Implicit Fields for Resolution-free Topology Optimization</h3>
<p><a href='http://arxiv.org/abs/2402.05073v1'>http://arxiv.org/abs/2402.05073v1</a></p>
<p><b>Compressor summary</b>: NITO is a novel deep learning approach for topology optimization that offers faster, more efficient, and domain-agnostic solutions compared to existing methods.</p><hr><h3>A Roadmap to Pluralistic Alignment</h3>
<p><a href='http://arxiv.org/abs/2402.05070v1'>http://arxiv.org/abs/2402.05070v1</a></p>
<p><b>Compressor summary</b>: The text discusses the challenge of creating AI systems that serve diverse human values and proposes a roadmap with different types of pluralistic models and benchmarks to address this issue.</p><hr><h3>LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content  Creation</h3>
<p><a href='http://arxiv.org/abs/2402.05054v1'>http://arxiv.org/abs/2402.05054v1</a></p>
<p><b>Compressor summary</b>: The paper introduces LGM, a novel framework that generates high-resolution 3D models from text or single-view images using multi-view Gaussian features and an asymmetric U-Net backbone.</p><hr><h3>Causal Representation Learning from Multiple Distributions: A General  Setting</h3>
<p><a href='http://arxiv.org/abs/2402.05052v1'>http://arxiv.org/abs/2402.05052v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a general nonparametric method for learning causal representations from multiple distributions and shows that under certain conditions, it can recover the underlying causal graph and latent variables.</p><hr><h3>How VADER is your AI? Towards a definition of artificial intelligence  systems appropriate for regulation</h3>
<p><a href='http://arxiv.org/abs/2402.05048v1'>http://arxiv.org/abs/2402.05048v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework (VADER) to assess how well AI definitions are suited for regulation, highlighting issues with current AI regulation proposals affecting non-AI works.</p><hr><h3>Efficient Multi-Resolution Fusion for Remote Sensing Data with Label  Uncertainty</h3>
<p><a href='http://arxiv.org/abs/2402.05045v1'>http://arxiv.org/abs/2402.05045v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method for fusing multi-modal and multi-resolution remote sensor data without pixel-level labels, which improves efficiency by using binary fuzzy measures instead of the previous fuzzy measures.</p><hr><h3>SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.05044v1'>http://arxiv.org/abs/2402.05044v1</a></p>
<p><b>Compressor summary</b>: SALAD-Bench is a safety benchmark for evaluating the robustness of large language models against various attacks and defense methods.</p><hr><h3>PAC Learnability under Explanation-Preserving Graph Perturbations</h3>
<p><a href='http://arxiv.org/abs/2402.05039v1'>http://arxiv.org/abs/2402.05039v1</a></p>
<p><b>Compressor summary</b>: This paper studies how to use graph explanations to improve GNNs' performance, sample complexity, and robustness.</p><hr><h3>A Survey on Domain Generalization for Medical Image Analysis</h3>
<p><a href='http://arxiv.org/abs/2402.05035v1'>http://arxiv.org/abs/2402.05035v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper reviews developments in Domain Generalization (DG) for Medical Image Analysis (MedIA), a tool for computer-aided diagnosis systems using deep learning.
- It defines domain shift and DG, discusses settings, summarizes methods from three viewpoints, introduces datasets, and suggests future research topics.
- It also provides a GitHub project with supporting resources.

Summary:
The paper gives an overview of Domain Generalization for Medical Image Analysis, which deals with the performance drop of deep learning models across different medical data distributions. It covers definitions, settings, methods, datasets, and future directions, and provides a GitHub project as a resource.</p><hr><h3>How BERT Speaks Shakespearean English? Evaluating Historical Bias in  Contextual Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.05034v1'>http://arxiv.org/abs/2402.05034v1</a></p>
<p><b>Compressor summary</b>: The paper examines how well BERT and other models capture historical changes in English by testing them on fill-in-the-blank questions with sentences from different time periods.</p><hr><h3>Simulated Overparameterization</h3>
<p><a href='http://arxiv.org/abs/2402.05033v1'>http://arxiv.org/abs/2402.05033v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Simulated Overparametrization (SOP): trains larger model with fewer parameters for inference
- Majority kernels: novel algorithm that integrates with different architectures and boosts performance
- Low overhead and strong results on various datasets and models

Summary:
The paper proposes SOP, a method to train overparameterized models with fewer parameters for inference, and majority kernels, an algorithm that improves performance across architectures with minimal cost.</p><hr><h3>Strong convexity-guided hyper-parameter optimization for flatter losses</h3>
<p><a href='http://arxiv.org/abs/2402.05025v1'>http://arxiv.org/abs/2402.05025v1</a></p>
<p><b>Compressor summary</b>: The proposed white-box method optimizes hyper-parameters for neural networks by minimizing the strong convexity of the loss to improve flatness and generalization.</p><hr><h3>A Sober Look at LLMs for Material Discovery: Are They Actually Good for  Bayesian Optimization Over Molecules?</h3>
<p><a href='http://arxiv.org/abs/2402.05015v1'>http://arxiv.org/abs/2402.05015v1</a></p>
<p><b>Compressor summary</b>: The text discusses using large language models to improve Bayesian optimization in molecular space discovery, but only if they are pretrained or finetuned with relevant data.</p><hr><h3>Compression of Structured Data with Autoencoders: Provable Benefit of  Nonlinearities and Depth</h3>
<p><a href='http://arxiv.org/abs/2402.05013v1'>http://arxiv.org/abs/2402.05013v1</a></p>
<p><b>Compressor summary</b>: Shallow autoencoders lose the sparse structure of input data during gradient descent, but adding denoising and multi-layer decoding improves compression for sparse data.</p><hr><h3>Navigating Complexity: Toward Lossless Graph Condensation via Expanding  Window Matching</h3>
<p><a href='http://arxiv.org/abs/2402.05011v1'>http://arxiv.org/abs/2402.05011v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new lossless graph condensation method that uses curriculum learning and expanding window matching to better transfer knowledge from the original graph to the condensed one, reducing computational cost for training GNNs.</p><hr><h3>EfficientViT-SAM: Accelerated Segment Anything Model Without Performance  Loss</h3>
<p><a href='http://arxiv.org/abs/2402.05008v1'>http://arxiv.org/abs/2402.05008v1</a></p>
<p><b>Compressor summary</b>: EfficientViT-SAM is a faster and accurate segment anything model that combines EfficientViT and SAM, achieving 48.9x speedup on A100 GPU without compromising performance.</p><hr><h3>Example-based Explanations for Random Forests using Machine Unlearning</h3>
<p><a href='http://arxiv.org/abs/2402.05007v1'>http://arxiv.org/abs/2402.05007v1</a></p>
<p><b>Compressor summary</b>: FairDebugger is a system that uses machine unlearning to find and explain unfair outcomes in tree-based classifiers.</p><hr><h3>Randomized Confidence Bounds for Stochastic Partial Monitoring</h3>
<p><a href='http://arxiv.org/abs/2402.05002v1'>http://arxiv.org/abs/2402.05002v1</a></p>
<p><b>Compressor summary</b>: The paper introduces new randomized strategies for sequential learning problems with incomplete feedback and shows their effectiveness in contextual and non-contextual settings with stochastic outcomes, using a real-world example of classifier monitoring.</p><hr><h3>Pedagogical Alignment of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.05000v1'>http://arxiv.org/abs/2402.05000v1</a></p>
<p><b>Compressor summary</b>: The paper introduces pedagogically aligned large language models that use reinforcement learning and human feedback to guide students towards solving complex problems in education, outperforming supervised fine-tuning.</p><hr><h3>PriorBoost: An Adaptive Algorithm for Learning from Aggregate Responses</h3>
<p><a href='http://arxiv.org/abs/2402.04987v1'>http://arxiv.org/abs/2402.04987v1</a></p>
<p><b>Compressor summary</b>: The paper explores how to construct aggregation sets for better event-level prediction using one-dimensional clustering and the PriorBoost algorithm, which improves homogeneity of samples and considers label differential privacy.</p><hr><h3>Beyond explaining: XAI-based Adaptive Learning with SHAP Clustering for  Energy Consumption Prediction</h3>
<p><a href='http://arxiv.org/abs/2402.04982v1'>http://arxiv.org/abs/2402.04982v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method that combines explainable AI with adaptive learning for energy consumption prediction models, using SHAP clustering to provide insights and balance complexity and performance.</p><hr><h3>Detection and Pose Estimation of flat, Texture-less Industry Objects on  HoloLens using synthetic Training</h3>
<p><a href='http://arxiv.org/abs/2402.04979v1'>http://arxiv.org/abs/2402.04979v1</a></p>
<p><b>Compressor summary</b>: The authors propose a client-server-based augmented reality app that uses synthetic data to enable object pose estimation on edge devices like HoloLens 2 and iPad, without relying on real photographs.</p><hr><h3>An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge  Graph-Integrated Collaboration</h3>
<p><a href='http://arxiv.org/abs/2402.04978v1'>http://arxiv.org/abs/2402.04978v1</a></p>
<p><b>Compressor summary</b>: The study proposes a cooperative reasoning scheme between Knowledge Graph and Large Language Models to improve their performance in knowledge-based reasoning tasks and enhance transparency.</p><hr><h3>Multi-Sender Persuasion -- A Computational Perspective</h3>
<p><a href='http://arxiv.org/abs/2402.04971v1'>http://arxiv.org/abs/2402.04971v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method using differentiable neural networks to find local Nash equilibria in complex signaling games with multiple senders and self-interested receivers, showing improvements over existing approaches.</p><hr><h3>Text or Image? What is More Important in Cross-Domain Generalization  Capabilities of Hate Meme Detection Models?</h3>
<p><a href='http://arxiv.org/abs/2402.04967v1'>http://arxiv.org/abs/2402.04967v1</a></p>
<p><b>Compressor summary</b>: The paper shows that text helps multimodal hate meme detection across domains, while images hinder it.</p><hr><h3>ConvLoRA and AdaBN based Domain Adaptation via Self-Training</h3>
<p><a href='http://arxiv.org/abs/2402.04964v1'>http://arxiv.org/abs/2402.04964v1</a></p>
<p><b>Compressor summary</b>: Key points:
- ConvLoRA is a method for multi-target domain adaptation that reduces parameters by adding low-rank decomposition matrices to convolutional layers and using adaptive batch normalization.
- It outperforms or matches independent fine-tuned networks with much fewer trainable parameters.
- It can be applied to any architecture with convolutional and batch normalization layers.

Summary: ConvLoRA is a simple and effective method for multi-target domain adaptation that uses low-rank decomposition and adaptive batch normalization to reduce parameters and improve performance.</p><hr><h3>Channel-Selective Normalization for Label-Shift Robust Test-Time  Adaptation</h3>
<p><a href='http://arxiv.org/abs/2402.04958v1'>http://arxiv.org/abs/2402.04958v1</a></p>
<p><b>Compressor summary</b>: Test-time adaptation with selective channel adaptation improves robustness to label distribution shifts and reduces failure risks in deep neural networks.</p><hr><h3>Reconfidencing LLMs from the Grouping Loss Perspective</h3>
<p><a href='http://arxiv.org/abs/2402.04957v1'>http://arxiv.org/abs/2402.04957v1</a></p>
<p><b>Compressor summary</b>: Large language models like ChatGPT and LLaMA can generate wrong answers confidently; researchers propose a method to correct their overconfidence using a knowledge base.</p><hr><h3>4-Dimensional deformation part model for pose estimation using Kalman  filter constraints</h3>
<p><a href='http://arxiv.org/abs/2402.04953v1'>http://arxiv.org/abs/2402.04953v1</a></p>
<p><b>Compressor summary</b>: The article explores how a Kalman filter enhances pose estimation accuracy in 4D deformation models using two data sets.</p><hr><h3>An approach to automated videogame beta testing</h3>
<p><a href='http://arxiv.org/abs/2402.04938v1'>http://arxiv.org/abs/2402.04938v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a way to automate quality assurance in AAA game development, which is currently mostly manual and done by human beta testers.</p><hr><h3>A Bayesian Approach to Online Learning for Contextual Restless Bandits  with Applications to Public Health</h3>
<p><a href='http://arxiv.org/abs/2402.04933v1'>http://arxiv.org/abs/2402.04933v1</a></p>
<p><b>Compressor summary</b>: BCoR is an online RL approach for RMABs that combines Bayesian modeling and Thompson sampling to handle contextual and non-stationary settings, improving performance in public health interventions.</p><hr><h3>Blue noise for diffusion models</h3>
<p><a href='http://arxiv.org/abs/2402.04930v1'>http://arxiv.org/abs/2402.04930v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new class of diffusion models that use correlated noise to improve the training process and generation quality for computer graphics tasks.</p><hr><h3>Source-Free Domain Adaptation with Diffusion-Guided Source Data  Generation</h3>
<p><a href='http://arxiv.org/abs/2402.04929v1'>http://arxiv.org/abs/2402.04929v1</a></p>
<p><b>Compressor summary</b>: The paper proposes DM-SFDA, a method that uses diffusion models to generate source domain images from target features, improving domain adaptation performance.</p><hr><h3>Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient  Matching</h3>
<p><a href='http://arxiv.org/abs/2402.04924v1'>http://arxiv.org/abs/2402.04924v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new graph condensation method, CTRL, that reduces errors in training trajectories and improves performance on various graph datasets and tasks.</p><hr><h3>Prompting Implicit Discourse Relation Annotation</h3>
<p><a href='http://arxiv.org/abs/2402.04918v1'>http://arxiv.org/abs/2402.04918v1</a></p>
<p><b>Compressor summary</b>: The text explores how to improve ChatGPT's performance in identifying discourse relations using different prompting techniques, but finds that it still struggles with the task even with advanced methods.</p><hr><h3>Moco: A Learnable Meta Optimizer for Combinatorial Optimization</h3>
<p><a href='http://arxiv.org/abs/2402.04915v1'>http://arxiv.org/abs/2402.04915v1</a></p>
<p><b>Compressor summary</b>: Moco is a meta optimizer that learns to adapt its solution construction procedure based on features extracted from the current search state, improving performance on combinatorial optimization problems like TSP and MIS.</p><hr><h3>Personalized Text Generation with Fine-Grained Linguistic Control</h3>
<p><a href='http://arxiv.org/abs/2402.04914v1'>http://arxiv.org/abs/2402.04914v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new benchmark for evaluating generative models' ability to control fine-grained linguistic attributes in text generation and analyzes various language models' performance on it.</p><hr><h3>Conformal Monte Carlo Meta-learners for Predictive Inference of  Individual Treatment Effects</h3>
<p><a href='http://arxiv.org/abs/2402.04906v1'>http://arxiv.org/abs/2402.04906v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new method called Conformal Monte Carlo (CMC) meta-learners that can estimate the uncertainty in the treatment effect and help make personalized decisions based on this information.</p><hr><h3>L4Q: Parameter Efficient Quantization-Aware Training on Large Language  Models via LoRA-wise LSQ</h3>
<p><a href='http://arxiv.org/abs/2402.04902v1'>http://arxiv.org/abs/2402.04902v1</a></p>
<p><b>Compressor summary</b>: L4Q is an algorithm that combines parameter-efficient fine-tuning and quantization-aware training for large language models, improving accuracy with sub-4-bit precision.</p><hr><h3>The Strain of Success: A Predictive Model for Injury Risk Mitigation and  Team Success in Soccer</h3>
<p><a href='http://arxiv.org/abs/2402.04898v1'>http://arxiv.org/abs/2402.04898v1</a></p>
<p><b>Compressor summary</b>: Key points:
- novel sequential team selection model in soccer
- models player injury and unavailability using real-world data
- Monte-Carlo Tree Search for optimal long-term performance
- validated on 2018/19 English Premier League season
- reduced injuries and costs compared to benchmark

Summary:
The paper proposes a soccer team selection model that uses real-world data and Monte-Carlo Tree Search to reduce injuries and costs while maintaining performance.</p><hr><h3>A Unified Framework for Probabilistic Verification of AI Systems via  Weighted Model Integration</h3>
<p><a href='http://arxiv.org/abs/2402.04892v1'>http://arxiv.org/abs/2402.04892v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a general framework for verifying various properties of AI systems using Weighted Model Integration, which can handle different models and properties without strong distributional assumptions.</p><hr><h3>Toward Accurate Camera-based 3D Object Detection via Cascade Depth  Estimation and Calibration</h3>
<p><a href='http://arxiv.org/abs/2402.04883v1'>http://arxiv.org/abs/2402.04883v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a cascade framework that uses depth information for effective feature lifting and 3D object localization in camera-based 3D detection, improving performance on NuScenes benchmark and other detectors.</p><hr><h3>STAR: Shape-focused Texture Agnostic Representations for Improved Object  Detection and 6D Pose Estimation</h3>
<p><a href='http://arxiv.org/abs/2402.04878v1'>http://arxiv.org/abs/2402.04878v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a texture-agnostic object detection method that focuses on shape features by randomizing textures during training with CAD models, addressing the challenge of textureless and metallic objects in robotics.</p><hr><h3>On Provable Length and Compositional Generalization</h3>
<p><a href='http://arxiv.org/abs/2402.04875v1'>http://arxiv.org/abs/2402.04875v1</a></p>
<p><b>Compressor summary</b>: The text explores proving how various sequence-to-sequence models achieve length and compositional generalization, which are essential forms of out-of-distribution generalization.</p><hr><h3>Learning by Doing: An Online Causal Reinforcement Learning Framework  with Causal-Aware Policy</h3>
<p><a href='http://arxiv.org/abs/2402.04869v1'>http://arxiv.org/abs/2402.04869v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework for incorporating causality into reinforcement learning, using interventions for causal structure learning during exploration and policy guidance during exploitation, and evaluates it on a simulated fault alarm environment.</p><hr><h3>CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay</h3>
<p><a href='http://arxiv.org/abs/2402.04858v1'>http://arxiv.org/abs/2402.04858v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CodeIt, a self-improvement method for large language models that uses program sampling, hindsight relabeling, and prioritized experience replay to solve tasks requiring human-level reasoning, achieving state-of-the-art performance on the Abstraction and Reasoning Corpus.</p><hr><h3>Explaining Learned Reward Functions with Counterfactual Trajectories</h3>
<p><a href='http://arxiv.org/abs/2402.04856v1'>http://arxiv.org/abs/2402.04856v1</a></p>
<p><b>Compressor summary</b>: CTEs are explanations for reinforcement learning reward functions that show how different actions affect outcomes and can help users understand and evaluate learned rewards better.</p><hr><h3>Dual-Path Coupled Image Deraining Network via Spatial-Frequency  Interaction</h3>
<p><a href='http://arxiv.org/abs/2402.04855v1'>http://arxiv.org/abs/2402.04855v1</a></p>
<p><b>Compressor summary</b>: DPCNet is a novel image deraining method that uses spatial and frequency information from two separate feature extraction blocks and an adaptive fusion module to outperform existing methods and provide visually pleasing results.</p><hr><h3>Multi-Patch Prediction: Adapting LLMs for Time Series Representation  Learning</h3>
<p><a href='http://arxiv.org/abs/2402.04852v1'>http://arxiv.org/abs/2402.04852v1</a></p>
<p><b>Compressor summary</b>: aLLM4TS adapts Large Language Models for time-series representation learning by using multi-patch prediction, which captures temporal dynamics better than traditional methods, and achieves superior performance in various downstream tasks.</p><hr><h3>Data-efficient Large Vision Models through Sequential Autoregression</h3>
<p><a href='http://arxiv.org/abs/2402.04841v1'>http://arxiv.org/abs/2402.04841v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Paper proposes an efficient vision model that works on minimal visual data without linguistic inputs
- Model uses autoregression and reduces parameter size and training data requirements
- Model shows proficiency in various high-level and low-level visual tasks

Summary: The paper introduces a new vision model that can understand visual data with little data and no language, using an autoregressive architecture to improve efficiency and adaptability.</p><hr><h3>PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity  Recognition</h3>
<p><a href='http://arxiv.org/abs/2402.04838v1'>http://arxiv.org/abs/2402.04838v1</a></p>
<p><b>Compressor summary</b>: The study proposes PaDeLLM-NER, a method to reduce NER generation latency using parallel decoding in large language models without additional modules or architecture changes.</p><hr><h3>On the Completeness of Invariant Geometric Deep Learning Models</h3>
<p><a href='http://arxiv.org/abs/2402.04836v1'>http://arxiv.org/abs/2402.04836v1</a></p>
<p><b>Compressor summary</b>: The paper studies the expressiveness of invariant geometric deep learning models, introduces a new model called GeoNGNN that can handle some corner cases, and proves its E(3)-completeness for three existing models.</p><hr><h3>SARI: Simplistic Average and Robust Identification based Noisy Partial  Label Learning</h3>
<p><a href='http://arxiv.org/abs/2402.04835v1'>http://arxiv.org/abs/2402.04835v1</a></p>
<p><b>Compressor summary</b>: SARI is a novel framework for noisy partial label learning that uses pseudo-labeling and neural network classification to achieve state-of-the-art results in various settings.</p><hr><h3>Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for  Instruction Fine-Tuning</h3>
<p><a href='http://arxiv.org/abs/2402.04833v1'>http://arxiv.org/abs/2402.04833v1</a></p>
<p><b>Compressor summary</b>: The paper suggests that using the 1,000 longest instructions from standard datasets as a simple and effective way to fine-tune LLMs, and that refining these long instructions further improves their performance.</p><hr><h3>Structured d-DNNF Is Not Closed Under Negation</h3>
<p><a href='http://arxiv.org/abs/2402.04832v1'>http://arxiv.org/abs/2402.04832v1</a></p>
<p><b>Compressor summary</b>: Structured d-DNNF is more succinct than SDD and less tractable in terms of transformations, while OBDD supports more tractable transformations but is less succinct.</p><hr><h3>Closing the Gap Between SGP4 and High-Precision Propagation via  Differentiable Programming</h3>
<p><a href='http://arxiv.org/abs/2402.04830v1'>http://arxiv.org/abs/2402.04830v1</a></p>
<p><b>Compressor summary</b>: dSGP4 is a differentiable version of SGP4 that enables fast and precise orbital propagation for space applications, integrating with machine learning techniques to further improve precision.</p><hr><h3>NeRF as Non-Distant Environment Emitter in Physics-based Inverse  Rendering</h3>
<p><a href='http://arxiv.org/abs/2402.04829v1'>http://arxiv.org/abs/2402.04829v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using NeRF as a non-distant environment emitter for inverse rendering, improving accuracy over the common distant environment map approach.</p><hr><h3>Learning Communication Policies for Different Follower Behaviors in a  Collaborative Reference Game</h3>
<p><a href='http://arxiv.org/abs/2402.04824v1'>http://arxiv.org/abs/2402.04824v1</a></p>
<p><b>Compressor summary</b>: The authors evaluate how well neural agents can adapt their language grounding and coordination strategies to different Follower behaviors in a collaborative reference game, using PPO reinforcement learning with an additional communicative effort signal.</p><hr><h3>How Realistic Is Your Synthetic Data? Constraining Deep Generative  Models for Tabular Data</h3>
<p><a href='http://arxiv.org/abs/2402.04823v1'>http://arxiv.org/abs/2402.04823v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Constrained Deep Generative Models (C-DGMs) that ensure synthetic data complies with given constraints by integrating a Constraint Layer with the model, improving utility and detection.</p><hr><h3>E(3)-Equivariant Mesh Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.04821v1'>http://arxiv.org/abs/2402.04821v1</a></p>
<p><b>Compressor summary</b>: The paper proposes EMNN, an improved equivariant method for 3D mesh tasks that simplifies geometric deep learning by incorporating face information and hierarchy, achieving better results with less complexity and pre-processing.</p><hr><h3>BOWLL: A Deceptively Simple Open World Lifelong Learner</h3>
<p><a href='http://arxiv.org/abs/2402.04814v1'>http://arxiv.org/abs/2402.04814v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Deep learning often optimizes for scalar performance on benchmarks, not real-world applications
- Open world lifelong learning is a new trend that requires recognition of novel concepts, avoidance of uninformative data, and retention of previous knowledge
- The paper introduces a simple baseline using batch normalization to repurpose standard models for open world lifelong learning
- The approach shows promising results and should be a future standard for this field

Summary:
The paper proposes a batch normalization-based baseline for open world lifelong learning, a challenging real-world task that requires adaptability and knowledge retention.</p><hr><h3>Aspect-Based Sentiment Analysis for Open-Ended HR Survey Responses</h3>
<p><a href='http://arxiv.org/abs/2402.04812v1'>http://arxiv.org/abs/2402.04812v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a machine learning method for analyzing employee satisfaction surveys in Dutch, identifying key aspects and using pre-trained language models.</p><hr><h3>Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with  Parallel Spike-driven Transformer</h3>
<p><a href='http://arxiv.org/abs/2402.04798v1'>http://arxiv.org/abs/2402.04798v1</a></p>
<p><b>Compressor summary</b>: The Spiking-PhysFormer model combines artificial neural networks and spiking neural networks to measure cardiac activity and physiological signals from facial videos with less power consumption than existing methods.</p><hr><h3>Scalable Multi-view Clustering via Explicit Kernel Features Maps</h3>
<p><a href='http://arxiv.org/abs/2402.04794v1'>http://arxiv.org/abs/2402.04794v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new scalable framework for multi-view subspace clustering, using kernel feature maps to reduce computation time, and shows its effectiveness on large networks.</p><hr><h3>Direct Language Model Alignment from Online AI Feedback</h3>
<p><a href='http://arxiv.org/abs/2402.04792v1'>http://arxiv.org/abs/2402.04792v1</a></p>
<p><b>Compressor summary</b>: OAIF improves direct alignment from preferences methods by providing online feedback from a large language model annotator.</p><hr><h3>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with  Vision-Language Benchmark</h3>
<p><a href='http://arxiv.org/abs/2402.04788v1'>http://arxiv.org/abs/2402.04788v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new benchmark, MLLM-as-a-Judge, to evaluate multimodal large language models in assisting judges, but finds they still have limitations and biases.</p><hr><h3>A Hypothesis-Driven Framework for the Analysis of Self-Rationalising  Models</h3>
<p><a href='http://arxiv.org/abs/2402.04787v1'>http://arxiv.org/abs/2402.04787v1</a></p>
<p><b>Compressor summary</b>: The authors propose a framework that uses Bayesian networks to generate and compare explanations for natural language inference tasks with LLM-generated explanations, aiming to understand how LLMs solve problems.</p><hr><h3>Analyzing the Neural Tangent Kernel of Periodically Activated Coordinate  Networks</h3>
<p><a href='http://arxiv.org/abs/2402.04783v1'>http://arxiv.org/abs/2402.04783v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes how periodic activation functions improve performance in vision tasks and provides theoretical evidence for their better behavior compared to ReLU-activated networks.</p><hr><h3>StableMask: Refining Causal Masking in Decoder-only Transformer</h3>
<p><a href='http://arxiv.org/abs/2402.04779v1'>http://arxiv.org/abs/2402.04779v1</a></p>
<p><b>Compressor summary</b>: StableMask improves the decoder-only Transformer by refining the causal mask to balance attention distributions, encode absolute positional information, and support efficient extrapolation and integration with other techniques.</p><hr><h3>Code as Reward: Empowering Reinforcement Learning with VLMs</h3>
<p><a href='http://arxiv.org/abs/2402.04764v1'>http://arxiv.org/abs/2402.04764v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Code as Reward (VLM-CaR), a framework that generates dense reward functions from VLMs through code generation, enabling faster and more accurate training of RL agents.</p><hr><h3>Color Recognition in Challenging Lighting Environments: CNN Approach</h3>
<p><a href='http://arxiv.org/abs/2402.04762v1'>http://arxiv.org/abs/2402.04762v1</a></p>
<p><b>Compressor summary</b>: The text proposes a CNN-based color detection method for computer vision that improves robustness in various lighting conditions and outperforms existing methods.</p><hr><h3>Boundary-aware Contrastive Learning for Semi-supervised Nuclei Instance  Segmentation</h3>
<p><a href='http://arxiv.org/abs/2402.04756v1'>http://arxiv.org/abs/2402.04756v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a network that uses contrastive learning to improve nuclei boundary denoising in semi-supervised segmentation, addressing challenges in pathological images due to color and morphological variations.</p><hr><h3>Towards Aligned Layout Generation via Diffusion Model with Aesthetic  Constraints</h3>
<p><a href='http://arxiv.org/abs/2402.04754v1'>http://arxiv.org/abs/2402.04754v1</a></p>
<p><b>Compressor summary</b>: LACE is a continuous diffusion model for controllable layout generation that incorporates aesthetic constraints and outperforms existing methods.</p><hr><h3>Progressive Gradient Flow for Robust N:M Sparsity Training in  Transformers</h3>
<p><a href='http://arxiv.org/abs/2402.04744v1'>http://arxiv.org/abs/2402.04744v1</a></p>
<p><b>Compressor summary</b>: This paper studies the problem of high-sparsity regions in N:M structured sparsity models and proposes a method to reduce induced noise and improve performance by using decay mechanisms on gradient flows.</p><hr><h3>Graph Cuts with Arbitrary Size Constraints Through Optimal Transport</h3>
<p><a href='http://arxiv.org/abs/2402.04732v1'>http://arxiv.org/abs/2402.04732v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a new graph cut algorithm for partitioning graphs under arbitrary size constraints
- The algorithm is based on a regularized Gromov-Wasserstein problem and uses an accelerated proximal gradient descent method
- The algorithm has several advantages over classical methods, such as global convergence, sparsity and efficiency

Summary:
The paper presents a new graph partitioning algorithm that can handle arbitrary size constraints by formulating the problem as a regularized Gromov-Wasserstein optimization and solving it with an efficient gradient-based method.</p><hr><h3>InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with  Semantic Graph Prior</h3>
<p><a href='http://arxiv.org/abs/2402.04717v1'>http://arxiv.org/abs/2402.04717v1</a></p>
<p><b>Compressor summary</b>: InstructScene is a new method for generating 3D indoor scenes from natural language instructions, improving controllability and fidelity with a semantic graph prior and a layout decoder.</p><hr><h3>Incorporating Retrieval-based Causal Learning with Information  Bottlenecks for Interpretable Graph Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.04710v1'>http://arxiv.org/abs/2402.04710v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel interpretable causal Graph Neural Network framework that combines retrieval-based causal learning and Graph Information Bottleneck theory to improve both explanation and prediction.</p><hr><h3>EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World  Illusions</h3>
<p><a href='http://arxiv.org/abs/2402.04699v1'>http://arxiv.org/abs/2402.04699v1</a></p>
<p><b>Compressor summary</b>: Key points:
- EvoSeed is a new algorithm to generate natural adversarial samples for deep neural networks
- It uses evolutionary strategy, diffusion model, and classifier model in a black-box setting
- The generated samples are of high quality and transferable to different classifiers

Summary:
EvoSeed is an evolutionary algorithm that creates natural adversarial samples for deep neural networks by using a diffusion model and a classifier model in a black-box way, resulting in high-quality and transferable samples.</p><hr><h3>The Influence of Autofocus Lenses in the Camera Calibration Process</h3>
<p><a href='http://arxiv.org/abs/2402.04686v1'>http://arxiv.org/abs/2402.04686v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes camera calibration in robotics and computer vision, proposing a modified method that considers distance-dependent focal length to improve accuracy.</p><hr><h3>Large Language Models As Faithful Explainers</h3>
<p><a href='http://arxiv.org/abs/2402.04678v1'>http://arxiv.org/abs/2402.04678v1</a></p>
<p><b>Compressor summary</b>: xLLM is a framework that improves the faithfulness and accuracy of natural language explanations for large language models' decisions by optimizing an evaluator that quantifies faithfulness.</p><hr><h3>Source Identification in Abstractive Summarization</h3>
<p><a href='http://arxiv.org/abs/2402.04677v1'>http://arxiv.org/abs/2402.04677v1</a></p>
<p><b>Compressor summary</b>: The paper studies how neural summarization models convert source information into summaries by analyzing the source sentences of reference and system summaries on two datasets.</p><hr><h3>G-NAS: Generalizable Neural Architecture Search for Single Domain  Generalization Object Detection</h3>
<p><a href='http://arxiv.org/abs/2402.04672v1'>http://arxiv.org/abs/2402.04672v1</a></p>
<p><b>Compressor summary</b>: The paper proposes G-NAS, a method that uses Differentiable NAS and Generalizable loss to train object detectors on one source domain and generalize to multiple target domains with complex feature imbalance issues.</p><hr><h3>V2VSSC: A 3D Semantic Scene Completion Benchmark for Perception with  Vehicle to Vehicle Communication</h3>
<p><a href='http://arxiv.org/abs/2402.04671v1'>http://arxiv.org/abs/2402.04671v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a collaborative semantic scene completion framework for autonomous vehicles using vehicle-to-vehicle communication to overcome occlusion and short-range perception challenges, improving performance on geometric and visual metrics.</p><hr><h3>A Perspective on Individualized Treatment Effects Estimation from  Time-series Health Data</h3>
<p><a href='http://arxiv.org/abs/2402.04668v1'>http://arxiv.org/abs/2402.04668v1</a></p>
<p><b>Compressor summary</b>: This paper provides an overview of individualized treatment effects methods for electronic health records data, discussing challenges and future research directions in this emerging field.</p><hr><h3>Open-Vocabulary Calibration for Vision-Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.04655v1'>http://arxiv.org/abs/2402.04655v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method called Distance-Aware Calibration (DAC) to improve confidence calibration in vision-language models fine-tuned with prompt learning, especially for open-vocabulary tasks.</p><hr><h3>An Over Complete Deep Learning Method for Inverse Problems</h3>
<p><a href='http://arxiv.org/abs/2402.04653v1'>http://arxiv.org/abs/2402.04653v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new method to improve machine learning techniques for solving inverse problems by embedding the solution into higher dimensions and jointly designing and learning the regularizer.</p><hr><h3>OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language  Foundation Models for 3D Semantic Understanding</h3>
<p><a href='http://arxiv.org/abs/2402.04648v1'>http://arxiv.org/abs/2402.04648v1</a></p>
<p><b>Compressor summary</b>: OV-NeRF improves semantic field learning for 3D scenes using pre-trained vision and language models, addressing noisy and view-inconsistent semantics issues with single-view and cross-view strategies.</p><hr><h3>Latent Plan Transformer: Planning as Latent Variable Inference</h3>
<p><a href='http://arxiv.org/abs/2402.04647v1'>http://arxiv.org/abs/2402.04647v1</a></p>
<p><b>Compressor summary</b>: The Latent Plan Transformer (LPT) is a model that uses latent space to connect a Trajectory Generator and the final return, enabling improved decisions and planning with suboptimal trajectories in tasks without step-wise rewards.</p><hr><h3>Learning with Diversification from Block Sparse Signal</h3>
<p><a href='http://arxiv.org/abs/2402.04646v1'>http://arxiv.org/abs/2402.04646v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new prior for block sparse learning that adapts to data and reduces sensitivity to pre-defined block information, leading to better performance than existing methods.</p><hr><h3>LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different  Views</h3>
<p><a href='http://arxiv.org/abs/2402.04644v1'>http://arxiv.org/abs/2402.04644v1</a></p>
<p><b>Compressor summary</b>: LEVI is a novel method to improve fine-tuning generalization by adaptively ensembling pre-trained and task-specific models, addressing limitations in both data sources.</p><hr><h3>Domain Bridge: Generative model-based domain forensic for black-box  models</h3>
<p><a href='http://arxiv.org/abs/2402.04640v1'>http://arxiv.org/abs/2402.04640v1</a></p>
<p><b>Compressor summary</b>: The paper presents an enhanced approach to determine not only the general data domain but also its specific attributes using image embeddings and generative models, leveraging the large LAION-5B dataset.</p><hr><h3>TransLLaMa: LLM-based Simultaneous Translation System</h3>
<p><a href='http://arxiv.org/abs/2402.04636v1'>http://arxiv.org/abs/2402.04636v1</a></p>
<p><b>Compressor summary</b>: The study shows that large language models can perform simultaneous machine translation by generating a "wait" token and achieving comparable results to state-of-the-art baselines.</p><hr><h3>GSN: Generalisable Segmentation in Neural Radiance Field</h3>
<p><a href='http://arxiv.org/abs/2402.04632v1'>http://arxiv.org/abs/2402.04632v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new representation called GSN that combines generalized radiance fields with distilled semantic features, enabling multi-view segmentation of unseen scenes.</p><hr><h3>LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained  Descriptors</h3>
<p><a href='http://arxiv.org/abs/2402.04630v1'>http://arxiv.org/abs/2402.04630v1</a></p>
<p><b>Compressor summary</b>: DVDet is a detector that uses conditional context prompts and hierarchical textual descriptors to align visual embeddings with fine-grained text descriptions of object parts, improving open-vocabulary detection performance.</p><hr><h3>SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question  Answering over a Life Science Knowledge Graph</h3>
<p><a href='http://arxiv.org/abs/2402.04627v1'>http://arxiv.org/abs/2402.04627v1</a></p>
<p><b>Compressor summary</b>: The study evaluates strategies for fine-tuning an LLM for question answering over life science KGs, using data augmentation and semantic clues in queries to overcome the scarcity of training data.</p><hr><h3>Noise Map Guidance: Inversion with Spatial Context for Real Image  Editing</h3>
<p><a href='http://arxiv.org/abs/2402.04625v1'>http://arxiv.org/abs/2402.04625v1</a></p>
<p><b>Compressor summary</b>: Noise Map Guidance (NMG) is a new text-guided diffusion model that improves real-image editing by preserving quality and context without requiring optimization.</p><hr><h3>MEMORYLLM: Towards Self-Updatable Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.04624v1'>http://arxiv.org/abs/2402.04624v1</a></p>
<p><b>Compressor summary</b>: MEMORYLLM is a self-updating language model that can memorize new knowledge and maintain its performance over time.</p><hr><h3>Feature Distribution on Graph Topology Mediates the Effect of Graph  Convolution: Homophily Perspective</h3>
<p><a href='http://arxiv.org/abs/2402.04621v1'>http://arxiv.org/abs/2402.04621v1</a></p>
<p><b>Compressor summary</b>: Randomly shuffling features among nodes of the same class improves graph neural network performance by reducing the dependence between graph topology and features.</p><hr><h3>Multi-Scale Semantic Segmentation with Modified MBConv Blocks</h3>
<p><a href='http://arxiv.org/abs/2402.04618v1'>http://arxiv.org/abs/2402.04618v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new adaptation of MBConv blocks for U-Net architectures to improve semantic segmentation by extracting more detailed spatial information.</p><hr><h3>InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding  Extremely Long Sequences with Training-Free Memory</h3>
<p><a href='http://arxiv.org/abs/2402.04617v1'>http://arxiv.org/abs/2402.04617v1</a></p>
<p><b>Compressor summary</b>: InfLLM is a memory-based method that allows large language models to process and understand long sequences without training, improving their performance on long-distance dependency tasks.</p><hr><h3>TinyLLM: Learning a Small Student from Multiple Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.04616v1'>http://arxiv.org/abs/2402.04616v1</a></p>
<p><b>Compressor summary</b>: TinyLLM is a novel knowledge distillation approach that leverages multiple large teacher models to train a small student model with diverse reasoning skills and contextual understanding.</p><hr><h3>ScreenAI: A Vision-Language Model for UI and Infographics Understanding</h3>
<p><a href='http://arxiv.org/abs/2402.04615v1'>http://arxiv.org/abs/2402.04615v1</a></p>
<p><b>Compressor summary</b>: ScreenAI is a vision-language model that understands UIs and infographics, using a unique mixture of datasets and text annotations to improve performance on various tasks.</p><hr><h3>Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations  from Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.04614v1'>http://arxiv.org/abs/2402.04614v1</a></p>
<p><b>Compressor summary</b>: The text discusses the trade-off between faithfulness and plausibility in self-explanations generated by large language models, emphasizing the importance of faithfulness for high-stakes decision-making and suggesting ways to improve it without losing plausibility.</p><hr><h3>Improving Cross-Domain Low-Resource Text Generation through LLM  Post-Editing: A Programmer-Interpreter Approach</h3>
<p><a href='http://arxiv.org/abs/2402.04609v1'>http://arxiv.org/abs/2402.04609v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Post-editing improves text quality of LLMs but has limitations
- Neural programmer-interpreter approach preserves domain generalization and adapts editing actions for text generation
- Approach outperforms other post-editing methods in cross-domain settings

Summary:
The paper proposes a neural programmer-interpreter that enhances LLM text quality by adapting editing actions to text generation tasks, while maintaining domain generalization.</p><hr><h3>Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector</h3>
<p><a href='http://arxiv.org/abs/2402.04601v1'>http://arxiv.org/abs/2402.04601v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes an alignment-enhanced corrector for Chinese grammatical error correction (CGEC) to address overcorrection problems in Seq2Seq models and decoder-only LLMs.
- The method involves training a correction model, using two alignment models, and transferring knowledge from them to the correction model.
- The approach improves CGEC performance on three datasets.

Summary:
The paper presents an alignment-enhanced corrector for CGEC that uses two alignment models and knowledge transfer to reduce overcorrection in both Seq2Seq and decoder-only LLMs, leading to better CGEC results.</p><hr><h3>Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via  Temporal-Viewpoint Alignment</h3>
<p><a href='http://arxiv.org/abs/2402.04599v1'>http://arxiv.org/abs/2402.04599v1</a></p>
<p><b>Compressor summary</b>: JEANIE is a method for aligning 3D skeleton sequences by adjusting temporal and camera views to improve few-shot action recognition and clustering.</p><hr><h3>CMSA algorithm for solving the prioritized pairwise test data generation  problem in software product lines</h3>
<p><a href='http://arxiv.org/abs/2402.04597v1'>http://arxiv.org/abs/2402.04597v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new hybrid metaheuristic algorithm to generate test data for software product families, which outperforms existing methods in quality but takes longer to run.</p><hr><h3>Towards Improved Imbalance Robustness in Continual Multi-Label Learning  with Dual Output Spiking Architecture (DOSA)</h3>
<p><a href='http://arxiv.org/abs/2402.04596v1'>http://arxiv.org/abs/2402.04596v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new spiking neural network architecture for continual multi-label learning that is computationally efficient and robust to data imbalance.</p><hr><h3>UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised  Fine-tuning Dataset</h3>
<p><a href='http://arxiv.org/abs/2402.04588v1'>http://arxiv.org/abs/2402.04588v1</a></p>
<p><b>Compressor summary</b>: This paper introduces UltraLink, an open-source multilingual supervised fine-tuning dataset that considers both language-specific and language-agnostic abilities of large language models to improve their cross-lingual transfer capabilities.</p><hr><h3>Sparse Anatomical Prompt Semi-Supervised Learning with Masked Image  Modeling for CBCT Tooth Segmentation</h3>
<p><a href='http://arxiv.org/abs/2402.04587v1'>http://arxiv.org/abs/2402.04587v1</a></p>
<p><b>Compressor summary</b>: The study proposes a new method using a self-supervised masked autoencoder and a sparse masked boundary prompt to accurately segment teeth in CBCT dental images with limited labeled data.</p><hr><h3>A Psychological Study: Importance of Contrast and Luminance in Color to  Grayscale Mapping</h3>
<p><a href='http://arxiv.org/abs/2402.04583v1'>http://arxiv.org/abs/2402.04583v1</a></p>
<p><b>Compressor summary</b>: The paper compares different algorithms for converting color images to grayscale using a psychological experiment with participants imagining a "colorless world" and evaluates their effectiveness based on visual quality, information preservation, and selection times.</p><hr><h3>Collective Counterfactual Explanations via Optimal Transport</h3>
<p><a href='http://arxiv.org/abs/2402.04579v1'>http://arxiv.org/abs/2402.04579v1</a></p>
<p><b>Compressor summary</b>: The text proposes a collective method for generating counterfactual explanations that considers the current density of individuals, which improves upon classical approaches by using optimal transport.</p><hr><h3>S-Agents: self-organizing agents in open-ended environment</h3>
<p><a href='http://arxiv.org/abs/2402.04578v1'>http://arxiv.org/abs/2402.04578v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-organizing agent system (S-Agents) for flexible collaboration in open-ended settings, inspired by human organizational behavior.</p><hr><h3>Progressive Conservative Adaptation for Evolving Target Domains</h3>
<p><a href='http://arxiv.org/abs/2402.04573v1'>http://arxiv.org/abs/2402.04573v1</a></p>
<p><b>Compressor summary</b>: PCAda is a meta-learning approach for evolving domain adaptation that fine-tunes classifier heads with progressive class prototypes and uses conservative sparse attention to prevent interference with historical knowledge.</p><hr><h3>OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences</h3>
<p><a href='http://arxiv.org/abs/2402.04567v1'>http://arxiv.org/abs/2402.04567v1</a></p>
<p><b>Compressor summary</b>: OIL-AD is an unsupervised method for detecting anomalies in decision-making sequences using offline imitation learning and two features derived from Q function and state value function.</p><hr><h3>Attention Guided CAM: Visual Explanations of Vision Transformer Guided  by Self-Attention</h3>
<p><a href='http://arxiv.org/abs/2402.04563v1'>http://arxiv.org/abs/2402.04563v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an attention-guided visualization method for ViT models that explains their decisions and localizes objects with high performance using only class labels.</p><hr><h3>Can Large Language Model Agents Simulate Human Trust Behaviors?</h3>
<p><a href='http://arxiv.org/abs/2402.04559v1'>http://arxiv.org/abs/2402.04559v1</a></p>
<p><b>Compressor summary</b>: The paper examines whether large language model agents can simulate human trust behaviors in Trust Games and finds that they can, with potential implications for scenarios where trust is important.</p><hr><h3>DMAT: A Dynamic Mask-Aware Transformer for Human De-occlusion</h3>
<p><a href='http://arxiv.org/abs/2402.04558v1'>http://arxiv.org/abs/2402.04558v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a dynamic mask-aware transformer (DMAT) for human de-occlusion, using an expanded convolution head, a multi-head attention mechanism, and an amodal loss to improve the model's performance on AHP dataset.</p><hr><h3>FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language  Foundation Models</h3>
<p><a href='http://arxiv.org/abs/2402.04555v1'>http://arxiv.org/abs/2402.04555v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a probabilistic label fusion and instance refinement method to improve instance-aware semantic mapping from object detection generated by foundation models, achieving better zero-shot performance on real-world datasets.</p><hr><h3>BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial  Imagery</h3>
<p><a href='http://arxiv.org/abs/2402.04554v1'>http://arxiv.org/abs/2402.04554v1</a></p>
<p><b>Compressor summary</b>: BirdNeRF is a novel method that uses aerial imagery to reconstruct large-scale scenes faster and with better visual fidelity than traditional approaches, by decomposing the images into smaller sub-scenes and using a projection-guided re-rendering strategy.</p><hr><h3>Curvature-Informed SGD via General Purpose Lie-Group Preconditioners</h3>
<p><a href='http://arxiv.org/abs/2402.04553v1'>http://arxiv.org/abs/2402.04553v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method to accelerate stochastic gradient descent using curvature information and preconditioners based on connected Lie groups, which improves convergence and performance across various tasks and architectures.</p><hr><h3>Share What You Already Know: Cross-Language-Script Transfer and  Alignment for Sentiment Detection in Code-Mixed Data</h3>
<p><a href='http://arxiv.org/abs/2402.04542v1'>http://arxiv.org/abs/2402.04542v1</a></p>
<p><b>Compressor summary</b>: The study proposes a method to improve multilingual models by using native scripts for each language and aligning their representations in code-switched texts, achieving better results on Nepali-English and Hindi-English datasets.</p><hr><h3>BRI3L: A Brightness Illusion Image Dataset for Identification and  Localization of Regions of Illusory Perception</h3>
<p><a href='http://arxiv.org/abs/2402.04541v1'>http://arxiv.org/abs/2402.04541v1</a></p>
<p><b>Compressor summary</b>: The authors created a large dataset of five types of brightness illusions and tested data-driven neural network approaches for classifying and locating them, achieving high accuracy and pixel accuracy.</p><hr><h3>Learning Diverse Policies with Soft Self-Generated Guidance</h3>
<p><a href='http://arxiv.org/abs/2402.04539v1'>http://arxiv.org/abs/2402.04539v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a reinforcement learning method that uses diverse past trajectories as guidance to learn faster and more efficiently, even with sparse and deceptive rewards.</p><hr><h3>Triplet Interaction Improves Graph Transformers: Accurate Molecular  Graph Learning with Triplet Graph Transformers</h3>
<p><a href='http://arxiv.org/abs/2402.04538v1'>http://arxiv.org/abs/2402.04538v1</a></p>
<p><b>Compressor summary</b>: The Triplet Graph Transformer (TGT) is a new model that enables direct communication between neighboring pairs in graphs and achieves state-of-the-art results on various molecular property prediction and optimization tasks.</p><hr><h3>SumRec: A Framework for Recommendation using Open-Domain Dialogue</h3>
<p><a href='http://arxiv.org/abs/2402.04523v1'>http://arxiv.org/abs/2402.04523v1</a></p>
<p><b>Compressor summary</b>: The SumRec framework uses chat summaries to personalize information recommendations based on speakers' interests, preferences, and experiences.</p><hr><h3>On Computational Limits of Modern Hopfield Models: A Fine-Grained  Complexity Analysis</h3>
<p><a href='http://arxiv.org/abs/2402.04520v1'>http://arxiv.org/abs/2402.04520v1</a></p>
<p><b>Compressor summary</b>: The paper studies the efficiency of modern Hopfield models for memory retrieval and shows a phase transition behavior based on pattern norms, with efficient variants possible under SETH assumptions.</p><hr><h3>BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for  Robust Vision</h3>
<p><a href='http://arxiv.org/abs/2402.04519v1'>http://arxiv.org/abs/2402.04519v1</a></p>
<p><b>Compressor summary</b>: BioDrone is a bionic drone-based visual benchmark for single object tracking (SOT) that evaluates the robustness of SOT methods in challenging conditions such as tiny target and fast motion, camera shake, and drastic changes between frames.</p><hr><h3>Online Cascade Learning for Efficient Inference over Streams</h3>
<p><a href='http://arxiv.org/abs/2402.04513v1'>http://arxiv.org/abs/2402.04513v1</a></p>
<p><b>Compressor summary</b>: The paper proposes online cascade learning, a method to use lower-capacity models and a deferral policy to answer queries about data streams with the help of large language models, achieving high accuracy while reducing inference costs by up to 90%.</p><hr><h3>Developments in Sheaf-Theoretic Models of Natural Language Ambiguities</h3>
<p><a href='http://arxiv.org/abs/2402.04505v1'>http://arxiv.org/abs/2402.04505v1</a></p>
<p><b>Compressor summary</b>: Sheaves are mathematical tools used to model discourse ambiguities in natural language processing and improve contextual models.</p><hr><h3>Text2Street: Controllable Text-to-image Generation for Street Views</h3>
<p><a href='http://arxiv.org/abs/2402.04504v1'>http://arxiv.org/abs/2402.04504v1</a></p>
<p><b>Compressor summary</b>: The Text2Street framework generates controllable street-view images from text by using a lane-aware road topology generator, an object layout generator, and a multiple control image generator.</p><hr><h3>The Fine-Grained Complexity of Gradient Computation for Training Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.04497v1'>http://arxiv.org/abs/2402.04497v1</a></p>
<p><b>Compressor summary</b>: This paper shows that the time complexity of training large language models (LLMs) is almost-linear for some parameter regimes, and provides a complete characterization of their fine-grained complexity.</p><hr><h3>Grandmaster-Level Chess Without Search</h3>
<p><a href='http://arxiv.org/abs/2402.04494v1'>http://arxiv.org/abs/2402.04494v1</a></p>
<p><b>Compressor summary</b>: This paper trains a large transformer model on a huge chess dataset to achieve strong chess performance without complex heuristics or explicit search algorithms.</p><hr><h3>ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation</h3>
<p><a href='http://arxiv.org/abs/2402.04492v1'>http://arxiv.org/abs/2402.04492v1</a></p>
<p><b>Compressor summary</b>: The ColorSwap dataset helps evaluate and improve multimodal models' ability to match objects with their colors by providing image-caption pairs with swapped color words.</p><hr><h3>De-amplifying Bias from Differential Privacy in Language Model  Fine-tuning</h3>
<p><a href='http://arxiv.org/abs/2402.04489v1'>http://arxiv.org/abs/2402.04489v1</a></p>
<p><b>Compressor summary</b>: The text discusses the trade-offs between privacy and fairness in machine learning, showing that differential privacy amplifies bias but can be mitigated by counterfactual data augmentation.</p><hr><h3>BEBLID: Boosted efficient binary local image descriptor</h3>
<p><a href='http://arxiv.org/abs/2402.04482v1'>http://arxiv.org/abs/2402.04482v1</a></p>
<p><b>Compressor summary</b>: BEBLID is a learned binary image descriptor that improves matching accuracy and efficiency for computer vision applications on devices with limited hardware and energy resources.</p>