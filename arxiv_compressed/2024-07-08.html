
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-07-08</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-07-08 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>LaRa: Efficient Large-Baseline Radiance Fields</h3>
<p><a href='http://arxiv.org/abs/2407.04699v1'>http://arxiv.org/abs/2407.04699v1</a></p>
<p><b>Compressor summary</b>: The paper presents a novel method using transformers with local and global attention for efficient and accurate 3D reconstruction from novel views.</p><hr><h3>VCoME: Verbal Video Composition with Multimodal Editing Effects</h3>
<p><a href='http://arxiv.org/abs/2407.04697v1'>http://arxiv.org/abs/2407.04697v1</a></p>
<p><b>Compressor summary</b>: The paper introduces VCoME, a framework for generating coherent and visually appealing verbal videos with multimodal editing effects, which is 85 times more efficient than professional editors.</p><hr><h3>Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs</h3>
<p><a href='http://arxiv.org/abs/2407.04694v1'>http://arxiv.org/abs/2407.04694v1</a></p>
<p><b>Compressor summary</b>: The Situational Awareness Dataset (SAD) is a benchmark that tests the self-knowledge and situational awareness of large language models (LLMs), which are important for their capacity to plan and act autonomously, but also raise novel risks.</p><hr><h3>ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2407.04693v1'>http://arxiv.org/abs/2407.04693v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an iterative self-training framework that scales up hallucination annotation and improves the accuracy of the annotator, outperforming GPT-4 and achieving state-of-the-art results in hallucination detection.</p><hr><h3>Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for  Interpreting Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2407.04690v1'>http://arxiv.org/abs/2407.04690v1</a></p>
<p><b>Compressor summary</b>: Interpretability research using counterfactual theories faces issues with capturing multiple causes and transitive dependencies in neural networks, which affects the accuracy of causal graphs extraction and interpretation.</p><hr><h3>Enhancing Vehicle Re-identification and Matching for Weaving Analysis</h3>
<p><a href='http://arxiv.org/abs/2407.04688v1'>http://arxiv.org/abs/2407.04688v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new method to collect video data on lane-specific weaving patterns, which can help improve traffic management systems.</p><hr><h3>Rethinking Visual Prompting for Multimodal Large Language Models with  External Knowledge</h3>
<p><a href='http://arxiv.org/abs/2407.04681v1'>http://arxiv.org/abs/2407.04681v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new visual prompt approach to integrate fine-grained external knowledge into multimodal large language models, improving their ability to understand detailed or localized visual elements.</p><hr><h3>XQSV: A Structurally Variable Network to Imitate Human Play in Xiangqi</h3>
<p><a href='http://arxiv.org/abs/2407.04678v1'>http://arxiv.org/abs/2407.04678v1</a></p>
<p><b>Compressor summary</b>: The paper introduces XQSV, a deep learning architecture for Xiangqi that can change its structure and mimic human players with high accuracy and indistinguishability.</p><hr><h3>Is plantar thermography a valid digital biomarker for characterising  diabetic foot ulceration risk?</h3>
<p><a href='http://arxiv.org/abs/2407.04676v1'>http://arxiv.org/abs/2407.04676v1</a></p>
<p><b>Compressor summary</b>: The study found strong associations between plantar thermography clusters and diabetic foot ulceration risk factors, but these associations were not predictive of the risk factors.</p><hr><h3>Unsupervised 4D Cardiac Motion Tracking with Spatiotemporal Optical Flow  Networks</h3>
<p><a href='http://arxiv.org/abs/2407.04663v1'>http://arxiv.org/abs/2407.04663v1</a></p>
<p><b>Compressor summary</b>: The paper introduces an unsupervised deep learning method for tracking myocardial motion from low-resolution echocardiography images, which improves accuracy and speed over existing methods.</p><hr><h3>SAM Fewshot Finetuning for Anatomical Segmentation in Medical Images</h3>
<p><a href='http://arxiv.org/abs/2407.04651v1'>http://arxiv.org/abs/2407.04651v1</a></p>
<p><b>Compressor summary</b>: The authors propose a few-shot fine-tuning strategy for adapting Segment Anything (SAM) to medical image segmentation tasks, which reduces user interaction and improves efficiency by using embeddings from annotated slices as prompts.</p><hr><h3>Semi-Supervised Segmentation via Embedding Matching</h3>
<p><a href='http://arxiv.org/abs/2407.04638v1'>http://arxiv.org/abs/2407.04638v1</a></p>
<p><b>Compressor summary</b>: The authors propose a semi-supervised segmentation method that uses unlabeled images and a few labeled ones to train a model, improving hip bone segmentation in CT scans with less data.</p><hr><h3>Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity  Recognition Framework</h3>
<p><a href='http://arxiv.org/abs/2407.04629v1'>http://arxiv.org/abs/2407.04629v1</a></p>
<p><b>Compressor summary</b>: This paper proposes EDF, a novel framework that improves open NER LLMs' performance in clinical named entity recognition by decomposing the task into sub-entity retrievals and filtering incorrect entities.</p><hr><h3>On scalable oversight with weak LLMs judging strong LLMs</h3>
<p><a href='http://arxiv.org/abs/2407.04622v1'>http://arxiv.org/abs/2407.04622v1</a></p>
<p><b>Compressor summary</b>: The paper explores how different oversight protocols involving AI and human judges perform across various tasks with information asymmetry and finds that debate outperforms consultancy and is comparable to direct question-answering depending on the task type.</p><hr><h3>OneRestore: A Universal Restoration Framework for Composite Degradation</h3>
<p><a href='http://arxiv.org/abs/2407.04621v1'>http://arxiv.org/abs/2407.04621v1</a></p>
<p><b>Compressor summary</b>: OneRestore is a transformer-based framework that adapts to different image degradation scenarios by merging scene descriptors with image features using cross-attention, achieving superior restoration results.</p><hr><h3>Learning to (Learn at Test Time): RNNs with Expressive Hidden States</h3>
<p><a href='http://arxiv.org/abs/2407.04620v1'>http://arxiv.org/abs/2407.04620v1</a></p>
<p><b>Compressor summary</b>: The authors propose Test-Time Training layers that use a hidden state as a machine learning model and update it during self-supervised training, achieving linear complexity with expressive power comparable to Transformer and RNNs in sequence modeling.</p><hr><h3>CountGD: Multi-Modal Open-World Counting</h3>
<p><a href='http://arxiv.org/abs/2407.04619v1'>http://arxiv.org/abs/2407.04619v1</a></p>
<p><b>Compressor summary</b>: The paper proposes CountGD, a model that can count objects in images using text or visual exemplars or both, and shows its superior performance on multiple counting benchmarks.</p><hr><h3>Randomized Physics-Informed Neural Networks for Bayesian Data  Assimilation</h3>
<p><a href='http://arxiv.org/abs/2407.04617v1'>http://arxiv.org/abs/2407.04617v1</a></p>
<p><b>Compressor summary</b>: The rPINN method is a faster and more effective alternative to HMC for uncertainty quantification in inverse PDE problems with noisy data.</p><hr><h3>Isomorphic Pruning for Vision Models</h3>
<p><a href='http://arxiv.org/abs/2407.04616v1'>http://arxiv.org/abs/2407.04616v1</a></p>
<p><b>Compressor summary</b>: Isomorphic Pruning is a simple approach that effectively prunes heterogeneous sub-structures in advanced vision models, improving accuracy and reducing computation.</p><hr><h3>ARM: Efficient Guided Decoding with Autoregressive Reward Models</h3>
<p><a href='http://arxiv.org/abs/2407.04615v1'>http://arxiv.org/abs/2407.04615v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an efficient way to improve language models with task-specific rewards for real world applications like detoxification and sentiment control.</p><hr><h3>PartCraft: Crafting Creative Objects by Parts</h3>
<p><a href='http://arxiv.org/abs/2407.04604v1'>http://arxiv.org/abs/2407.04604v1</a></p>
<p><b>Compressor summary</b>: PartCraft is a generative visual AI system that allows users to select and combine parts of objects for fine-grained, faithful, and plausible results.</p><hr><h3>AWT: Transferring Vision-Language Models via Augmentation, Weighting,  and Transportation</h3>
<p><a href='http://arxiv.org/abs/2407.04603v1'>http://arxiv.org/abs/2407.04603v1</a></p>
<p><b>Compressor summary</b>: AWT is a novel adaptation framework that enhances vision-language models for various visual tasks by augmenting inputs, dynamically weighting them, and mining semantic correlations in the vision-language space.</p><hr><h3>Understanding the Gains from Repeated Self-Distillation</h3>
<p><a href='http://arxiv.org/abs/2407.04600v1'>http://arxiv.org/abs/2407.04600v1</a></p>
<p><b>Compressor summary</b>: Self-distillation improves performance, especially with multiple steps, and can significantly reduce excess risk in linear regression.</p><hr><h3>Feature Attenuation of Defective Representation Can Resolve Incomplete  Masking on Anomaly Detection</h3>
<p><a href='http://arxiv.org/abs/2407.04597v1'>http://arxiv.org/abs/2407.04597v1</a></p>
<p><b>Compressor summary</b>: FADeR improves unsupervised anomaly detection using minimal changes and fewer layers in a neural network, reducing false alarms and increasing efficiency for edge computing.</p><hr><h3>Testing learning hypotheses using neural networks by manipulating  learning data</h3>
<p><a href='http://arxiv.org/abs/2407.04593v1'>http://arxiv.org/abs/2407.04593v1</a></p>
<p><b>Compressor summary</b>: The study explores how English speakers learn exceptions to the passive voice rule using neural network language models, and finds that verb frequency in the passive affects its passivizability more than semantics.</p><hr><h3>Smell and Emotion: Recognising emotions in smell-related artworks</h3>
<p><a href='http://arxiv.org/abs/2407.04592v1'>http://arxiv.org/abs/2407.04592v1</a></p>
<p><b>Compressor summary</b>: This paper shows how to recognize emotions from smell-related artworks using style transfer and hyperparameter tuning, and suggests ways to improve it further.</p><hr><h3>Proximal Point Method for Online Saddle Point Problem</h3>
<p><a href='http://arxiv.org/abs/2407.04591v1'>http://arxiv.org/abs/2407.04591v1</a></p>
<p><b>Compressor summary</b>: The paper proposes and analyzes three variants of the online proximal point method for solving time-varying convex-concave games, achieving near-optimal duality gap and dynamic Nash equilibrium regret bounds in benign environments.</p><hr><h3>SH17: A Dataset for Human Safety and Personal Protective Equipment  Detection in Manufacturing Industry</h3>
<p><a href='http://arxiv.org/abs/2407.04590v1'>http://arxiv.org/abs/2407.04590v1</a></p>
<p><b>Compressor summary</b>: The researchers developed a system using object detection and neural networks to detect and verify the proper use of personal protective equipment in various industries, creating a dataset and achieving promising results.</p><hr><h3>Multimodal Classification via Modal-Aware Interactive Enhancement</h3>
<p><a href='http://arxiv.org/abs/2407.04587v1'>http://arxiv.org/abs/2407.04587v1</a></p>
<p><b>Compressor summary</b>: The paper introduces modal-aware interactive enhancement (MIE), a novel multimodal learning method that uses sharpness aware minimization and gradient modification to balance learning speeds, improve generalization, and reduce modality forgetting.</p><hr><h3>Leveraging Large Language Models for Integrated  Satellite-Aerial-Terrestrial Networks: Recent Advances and Future Directions</h3>
<p><a href='http://arxiv.org/abs/2407.04581v1'>http://arxiv.org/abs/2407.04581v1</a></p>
<p><b>Compressor summary</b>: The paper discusses how integrating Large Language Models into Integrated Satellite, Aerial, and Terrestrial Networks can improve connectivity and performance by enhancing data processing, network management, and advanced algorithms.</p><hr><h3>GOALPlace: Begin with the End in Mind</h3>
<p><a href='http://arxiv.org/abs/2407.04579v1'>http://arxiv.org/abs/2407.04579v1</a></p>
<p><b>Compressor summary</b>: GOALPlace is a new learning-based method that improves placement congestion by controlling cell density, achieving superior or comparable results to commercial tools.</p><hr><h3>Real Time Emotion Analysis Using Deep Learning for Education,  Entertainment, and Beyond</h3>
<p><a href='http://arxiv.org/abs/2407.04560v1'>http://arxiv.org/abs/2407.04560v1</a></p>
<p><b>Compressor summary</b>: The authors are developing a system that uses deep learning to detect facial expressions and display matching emojis in real-time for various applications such as education and entertainment.</p><hr><h3>Not (yet) the whole story: Evaluating Visual Storytelling Requires More  than Measuring Coherence, Grounding, and Repetition</h3>
<p><a href='http://arxiv.org/abs/2407.04559v1'>http://arxiv.org/abs/2407.04559v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to measure visual storytelling quality based on human likeness and evaluates several models, finding LLaVA as the best performer but with room for improvement.</p><hr><h3>Spontaneous Reward Hacking in Iterative Self-Refinement</h3>
<p><a href='http://arxiv.org/abs/2407.04549v1'>http://arxiv.org/abs/2407.04549v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how iterative self-refinement in language models can lead to reward hacking, where the generated output optimizes the evaluator's ratings instead of actual user preference.</p><hr><h3>Gaussian Eigen Models for Human Heads</h3>
<p><a href='http://arxiv.org/abs/2407.04545v1'>http://arxiv.org/abs/2407.04545v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for generating facial expressions using low-dimensional linear spaces based on dynamic 3D Gaussians, enabling real-time rendering and control.</p><hr><h3>Strengthening Structural Inductive Biases by Pre-training to Perform  Syntactic Transformations</h3>
<p><a href='http://arxiv.org/abs/2407.04543v1'>http://arxiv.org/abs/2407.04543v1</a></p>
<p><b>Compressor summary</b>: The paper proposes intermediate pre-training for Transformers to improve their inductive biases for syntactic transformations in seq2seq tasks, leading to better few-shot learning and structural generalization.</p><hr><h3>PoPreRo: A New Dataset for Popularity Prediction of Romanian Reddit  Posts</h3>
<p><a href='http://arxiv.org/abs/2407.04541v1'>http://arxiv.org/abs/2407.04541v1</a></p>
<p><b>Compressor summary</b>: The authors present PoPreRo, a dataset for predicting the popularity of Romanian Reddit posts, and show that it is a challenging task even for state-of-the-art models.</p><hr><h3>PDiscoFormer: Relaxing Part Discovery Constraints with Vision  Transformers</h3>
<p><a href='http://arxiv.org/abs/2407.04538v1'>http://arxiv.org/abs/2407.04538v1</a></p>
<p><b>Compressor summary</b>: The paper shows that pre-trained transformer-based vision models enable more flexible object part detection and improve fine-grained classification tasks, challenging restrictive assumptions on part properties.</p><hr><h3>Introducing 'Inside' Out of Distribution</h3>
<p><a href='http://arxiv.org/abs/2407.04534v1'>http://arxiv.org/abs/2407.04534v1</a></p>
<p><b>Compressor summary</b>: This study proposes a new way to categorize out-of-distribution samples (inside and outside) and analyzes their impact on machine learning models' performance.</p><hr><h3>Performance Analysis of Speech Encoders for Low-Resource SLU and ASR in  Tunisian Dialect</h3>
<p><a href='http://arxiv.org/abs/2407.04533v1'>http://arxiv.org/abs/2407.04533v1</a></p>
<p><b>Compressor summary</b>: SSL models pretrained on speech improve SLU and ASR for low-resource Tunisian Arabic dialect, especially when fine-tuned with limited data.</p><hr><h3>GPT vs RETRO: Exploring the Intersection of Retrieval and  Parameter-Efficient Fine-Tuning</h3>
<p><a href='http://arxiv.org/abs/2407.04528v1'>http://arxiv.org/abs/2407.04528v1</a></p>
<p><b>Compressor summary</b>: The paper compares different fine-tuning techniques for large language models, showing that RETRO models excel in zero-shot settings while GPT models have higher potential with parameter efficiency, and 8B models strike an optimal balance between cost and performance.</p><hr><h3>Graph Reinforcement Learning in Power Grids: A Survey</h3>
<p><a href='http://arxiv.org/abs/2407.04522v1'>http://arxiv.org/abs/2407.04522v1</a></p>
<p><b>Compressor summary</b>: This review discusses the potential of graph neural networks (GNNs) and reinforcement learning (RL) for improving power grid control, addressing different use cases and challenges in transmission and distribution grids.</p><hr><h3>Success or Failure? Analyzing Segmentation Refinement with Few-Shot  Segmentation</h3>
<p><a href='http://arxiv.org/abs/2407.04519v1'>http://arxiv.org/abs/2407.04519v1</a></p>
<p><b>Compressor summary</b>: The paper proposes JFS, a method to assess the success of segmentation refinement using few-shot segmentation, which evaluates the quality of refined masks compared to coarse masks.</p><hr><h3>G-Adaptive mesh refinement -- leveraging graph neural networks and  differentiable finite element solvers</h3>
<p><a href='http://arxiv.org/abs/2407.04516v1'>http://arxiv.org/abs/2407.04516v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel graph neural network approach that optimizes mesh adaptivity in finite element methods by minimizing the solution error, improving accuracy and efficiency compared to classical and previous machine learning methods.</p><hr><h3>LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing  Layer Execution Order</h3>
<p><a href='http://arxiv.org/abs/2407.04513v1'>http://arxiv.org/abs/2407.04513v1</a></p>
<p><b>Compressor summary</b>: The paper proposes training methods for vision transformers that allow them to adapt to different layer execution orders, random merging, and layer pruning at test time, but with some reduction in accuracy.</p><hr><h3>Hyperspectral Dataset and Deep Learning methods for Waste from Electric  and Electronic Equipment Identification (WEEE)</h3>
<p><a href='http://arxiv.org/abs/2407.04505v1'>http://arxiv.org/abs/2407.04505v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates deep learning architectures for hyperspectral image segmentation and shows that combining spectral and spatial information improves results, while also releasing a new dataset for the task.</p><hr><h3>Segment Any 4D Gaussians</h3>
<p><a href='http://arxiv.org/abs/2407.04504v1'>http://arxiv.org/abs/2407.04504v1</a></p>
<p><b>Compressor summary</b>: The paper introduces SA4D, a framework for segmenting objects in 4D digital scenes using 4D Gaussians and temporal identity features, with applications like removal, recoloring, composition, and rendering of masks.</p><hr><h3>PROUD: PaRetO-gUided Diffusion Model for Multi-objective Generation</h3>
<p><a href='http://arxiv.org/abs/2407.04493v1'>http://arxiv.org/abs/2407.04493v1</a></p>
<p><b>Compressor summary</b>: PROUD is a deep generative model that optimizes multiple properties simultaneously, preserving sample quality and achieving Pareto optimality in image and protein generation tasks.</p><hr><h3>Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular  Data</h3>
<p><a href='http://arxiv.org/abs/2407.04491v1'>http://arxiv.org/abs/2407.04491v1</a></p>
<p><b>Compressor summary</b>: The authors introduce RealMLP, an improved MLP, and better default parameters for GBDTs and RealMLP, which offer a good time-accuracy tradeoff and can achieve excellent results on tabular data without hyperparameter tuning.</p><hr><h3>Micro-gesture Online Recognition using Learnable Query Points</h3>
<p><a href='http://arxiv.org/abs/2407.04490v1'>http://arxiv.org/abs/2407.04490v1</a></p>
<p><b>Compressor summary</b>: The paper presents HFUT-VUT, a system for recognizing micro-gestures in videos that ranks 2nd in a challenge.</p><hr><h3>Dude: Dual Distribution-Aware Context Prompt Learning For Large  Vision-Language Model</h3>
<p><a href='http://arxiv.org/abs/2407.04489v1'>http://arxiv.org/abs/2407.04489v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new framework for few-shot learning that uses dual contexts and unbalanced optimal transport theory to improve feature representation, alignment, and image augmentation, achieving better results than existing methods.</p><hr><h3>Leveraging Graph Structures to Detect Hallucinations in Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2407.04485v1'>http://arxiv.org/abs/2407.04485v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method using graph attention networks and contrastive learning to detect hallucinations in large language models and improve their trustworthiness.</p><hr><h3>Optimizing the image correction pipeline for pedestrian detection in the  thermal-infrared domain</h3>
<p><a href='http://arxiv.org/abs/2407.04484v1'>http://arxiv.org/abs/2407.04484v1</a></p>
<p><b>Compressor summary</b>: This paper investigates how different infrared processing methods affect pedestrian detection in low-visibility urban scenarios and recommends using a specific shutterless pipeline with tonemapping for autonomous driving.</p><hr><h3>Using Petri Nets as an Integrated Constraint Mechanism for Reinforcement  Learning Tasks</h3>
<p><a href='http://arxiv.org/abs/2407.04481v1'>http://arxiv.org/abs/2407.04481v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using Petri nets to integrate RL models in real-world domains, improving trustworthiness by enabling verification of model properties and enforcing constraints.</p><hr><h3>LoCo: Low-Bit Communication Adaptor for Large-scale Model Training</h3>
<p><a href='http://arxiv.org/abs/2407.04480v1'>http://arxiv.org/abs/2407.04480v1</a></p>
<p><b>Compressor summary</b>: LoCo is a method that compensates gradients before compression, ensuring efficient synchronization and maintaining training quality for large-scale models.</p><hr><h3>Rethinking Data Input for Point Cloud Upsampling</h3>
<p><a href='http://arxiv.org/abs/2407.04476v1'>http://arxiv.org/abs/2407.04476v1</a></p>
<p><b>Compressor summary</b>: The text discusses a new method for point cloud upsampling that compares patch-based and full-model inputs, but finds that patch-based methods perform better.</p><hr><h3>Are Large Language Models Strategic Decision Makers? A Study of  Performance and Bias in Two-Player Non-Zero-Sum Games</h3>
<p><a href='http://arxiv.org/abs/2407.04467v1'>http://arxiv.org/abs/2407.04467v1</a></p>
<p><b>Compressor summary</b>: This study examines how large language models perform in strategic games and finds they have systematic biases that affect their decision-making, especially when game settings or prompts change.</p><hr><h3>Using LLMs to label medical papers according to the CIViC evidence model</h3>
<p><a href='http://arxiv.org/abs/2407.04466v1'>http://arxiv.org/abs/2407.04466v1</a></p>
<p><b>Compressor summary</b>: The authors present CIViC Evidence, a sequence classification problem in medical NLP, and compare different language models and GPT-4's few-shot performance on it.</p><hr><h3>VCD-Texture: Variance Alignment based 3D-2D Co-Denoising for Text-Guided  Texturing</h3>
<p><a href='http://arxiv.org/abs/2407.04461v1'>http://arxiv.org/abs/2407.04461v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel texture synthesis method for 3D shapes using a 2D-3D collaborative denoising framework, which addresses the modal gap and improves the consistency of textures by aligning variances and refining inpainting.</p><hr><h3>Generalists vs. Specialists: Evaluating Large Language Models for Urdu</h3>
<p><a href='http://arxiv.org/abs/2407.04459v1'>http://arxiv.org/abs/2407.04459v1</a></p>
<p><b>Compressor summary</b>: The paper compares general and special-purpose language models on Urdu tasks, finding that special-purpose models perform better and GPT-4-Turbo is more aligned with human evaluation than Llama-3-8b-Instruct.</p><hr><h3>Robust Multimodal Learning via Representation Decoupling</h3>
<p><a href='http://arxiv.org/abs/2407.04458v1'>http://arxiv.org/abs/2407.04458v1</a></p>
<p><b>Compressor summary</b>: DMRNet is a novel method for robust multimodal learning that samples embeddings from probabilistic distributions instead of fixed points, allowing it to capture modality-specific information and perform better than existing methods.</p><hr><h3>Hindsight Preference Learning for Offline Preference-based Reinforcement  Learning</h3>
<p><a href='http://arxiv.org/abs/2407.04451v1'>http://arxiv.org/abs/2407.04451v1</a></p>
<p><b>Compressor summary</b>: HPL uses hindsight information to model human preferences for offline RL, resulting in more robust and advantageous rewards.</p><hr><h3>Multi-modal Masked Siamese Network Improves Chest X-Ray Representation  Learning</h3>
<p><a href='http://arxiv.org/abs/2407.04449v1'>http://arxiv.org/abs/2407.04449v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using Electronic Health Record (EHR) data to improve self-supervised learning for chest X-ray images by incorporating it into a Masked Siamese Network.</p><hr><h3>TokenVerse: Unifying Speech and NLP Tasks via Transducer-based ASR</h3>
<p><a href='http://arxiv.org/abs/2407.04444v1'>http://arxiv.org/abs/2407.04444v1</a></p>
<p><b>Compressor summary</b>: TokenVerse is a single Transducer-based model for conversational intelligence that handles multiple tasks by integrating task-specific tokens during training and improving ASR and task performance over the cascaded pipeline approach.</p><hr><h3>Wavelet-based Temporal Attention Improves Traffic Forecasting</h3>
<p><a href='http://arxiv.org/abs/2407.04440v1'>http://arxiv.org/abs/2407.04440v1</a></p>
<p><b>Compressor summary</b>: The paper presents a wavelet-based neural network model that efficiently captures spatio-temporal correlations in traffic flow data and outperforms existing methods on three real-world datasets.</p><hr><h3>From 'Showgirls' to 'Performers': Fine-tuning with Gender-inclusive  Language for Bias Reduction in LLMs</h3>
<p><a href='http://arxiv.org/abs/2407.04434v1'>http://arxiv.org/abs/2407.04434v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a catalogue of gender-exclusive terms and their neutral alternatives, which is used to fine-tune LLMs and reduce gender stereotyping.</p><hr><h3>The Complexity of Symmetry Breaking Beyond Lex-Leader</h3>
<p><a href='http://arxiv.org/abs/2407.04419v1'>http://arxiv.org/abs/2407.04419v1</a></p>
<p><b>Compressor summary</b>: The paper studies the complexity of finding symmetry breaking predicates (SBPs) for solving constraint programming problems like SAT, and shows that certifying graph non-isomorphism is a natural barrier for efficient SBP computation.</p><hr><h3>Trustworthy Classification through Rank-Based Conformal Prediction Sets</h3>
<p><a href='http://arxiv.org/abs/2407.04407v1'>http://arxiv.org/abs/2407.04407v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new conformal prediction method for classification tasks that uses rank-based scores to capture uncertainty and provides better coverage than existing methods.</p><hr><h3>On Quantum Channel Learning</h3>
<p><a href='http://arxiv.org/abs/2407.04406v1'>http://arxiv.org/abs/2407.04406v1</a></p>
<p><b>Compressor summary</b>: The text proposes an optimization problem and algorithm for finding the best mapping between two Hilbert spaces using density matrix measurements and quantum channels, which generalizes unitary learning and allows studying probabilistic mixtures and superpositions.</p>