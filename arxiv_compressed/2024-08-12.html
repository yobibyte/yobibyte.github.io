
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-08-12</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-08-12 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>VITA: Towards Open-Source Interactive Omni Multimodal LLM</h3>
<p><a href='http://arxiv.org/abs/2408.05211v1'>http://arxiv.org/abs/2408.05211v1</a></p>
<p><b>Compressor summary</b>: VITA is an open-source multimodal language model that excels at processing and analyzing video, image, text, and audio modalities while offering advanced interactive experiences.</p><hr><h3>Multi-Garment Customized Model Generation</h3>
<p><a href='http://arxiv.org/abs/2408.05206v1'>http://arxiv.org/abs/2408.05206v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method to generate realistic images of dressed models with various clothing combinations using a garment encoder and multi-garment feature fusion.</p><hr><h3>Kalman-Inspired Feature Propagation for Video Face Super-Resolution</h3>
<p><a href='http://arxiv.org/abs/2408.05205v1'>http://arxiv.org/abs/2408.05205v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method, KEEP, for improving the quality of low-resolution face videos by using Kalman filtering principles to maintain temporal consistency and facial details.</p><hr><h3>TaSL: Task Skill Localization and Consolidation for Language Model  Continual Learning</h3>
<p><a href='http://arxiv.org/abs/2408.05200v1'>http://arxiv.org/abs/2408.05200v1</a></p>
<p><b>Compressor summary</b>: TaSL is a novel CL framework for language models that enhances knowledge transfer without relying on memory replay by dividing the model into skill units, localizing their importance for new tasks, and consolidating them to prevent forgetting and enable bi-directional knowledge transfer.</p><hr><h3>Cell Morphology-Guided Small Molecule Generation with GFlowNets</h3>
<p><a href='http://arxiv.org/abs/2408.05196v1'>http://arxiv.org/abs/2408.05196v1</a></p>
<p><b>Compressor summary</b>: The text describes a novel approach for molecule design using unsupervised multimodal joint embedding that generates new molecules with similar phenotypic effects to a given image target without pre-annotated labels.</p><hr><h3>HistoKernel: Whole Slide Image Level Maximum Mean Discrepancy Kernels  for Pan-Cancer Predictive Modelling</h3>
<p><a href='http://arxiv.org/abs/2408.05195v1'>http://arxiv.org/abs/2408.05195v1</a></p>
<p><b>Compressor summary</b>: HistoKernel is a novel method that uses Maximum Mean Discrepancy to measure distributional differences between Whole Slide Images and improve prediction performance in computational pathology.</p><hr><h3>Separating Style from Substance: Enhancing Cross-Genre Authorship  Attribution through Data Selection and Presentation</h3>
<p><a href='http://arxiv.org/abs/2408.05192v1'>http://arxiv.org/abs/2408.05192v1</a></p>
<p><b>Compressor summary</b>: The authors propose methods to improve machine authorship attribution by reducing reliance on topic information and focusing more on style across different genres and topics.</p><hr><h3>Deep-change at AXOLOTL-24: Orchestrating WSD and WSI Models for Semantic  Change Modeling</h3>
<p><a href='http://arxiv.org/abs/2408.05184v1'>http://arxiv.org/abs/2408.05184v1</a></p>
<p><b>Compressor summary</b>: The paper presents new methods for modeling semantic change and a word-definition mismatch detection model that can improve understanding of polysemous words across different time periods.</p><hr><h3>ECG-FM: An Open Electrocardiogram Foundation Model</h3>
<p><a href='http://arxiv.org/abs/2408.05178v1'>http://arxiv.org/abs/2408.05178v1</a></p>
<p><b>Compressor summary</b>: ECG-FM is an open foundation model for ECG analysis that uses transformer architecture, pretraining on large data with ECG-specific augmentations and contrastive learning, achieving strong performance, rich embeddings, and interpretability in various downstream tasks.</p><hr><h3>Beyond Closure Models: Learning Chaotic-Systems via Physics-Informed  Neural Operators</h3>
<p><a href='http://arxiv.org/abs/2408.05177v1'>http://arxiv.org/abs/2408.05177v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a physics-informed neural operator (PINO) that can accurately predict the long-term behavior of chaotic systems without requiring expensive full-resolution simulations, achieving significant speedup and reduced error compared to closure models.</p><hr><h3>EasyInv: Toward Fast and Better DDIM Inversion</h3>
<p><a href='http://arxiv.org/abs/2408.05159v1'>http://arxiv.org/abs/2408.05159v1</a></p>
<p><b>Compressor summary</b>: EasyInv improves DDIM Inversion by better approximating inversion noise and prioritizing the initial latent state for faster and more accurate results.</p><hr><h3>Meta-Learning Guided Label Noise Distillation for Robust Signal  Modulation Classification</h3>
<p><a href='http://arxiv.org/abs/2408.05151v1'>http://arxiv.org/abs/2408.05151v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a meta-learning guided label noise distillation method for robust automatic modulation classification (AMC)
- The method uses a teacher-student heterogeneous network (TSHN) framework to distill and reuse label noise
- The method also uses a multi-view signal (MVS) method to improve performance on hard-to-classify categories with few-shot trusted labels

Summary:
The paper presents a robust AMC method that uses a TSHN framework and an MVS method to deal with label noise and improve performance on IoT security.</p><hr><h3>Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2</h3>
<p><a href='http://arxiv.org/abs/2408.05147v1'>http://arxiv.org/abs/2408.05147v1</a></p>
<p><b>Compressor summary</b>: Gemma Scope is a suite of sparse autoencoders trained on various layers of Gemma 2 models to facilitate safety and interpretability research.</p><hr><h3>Performative Prediction on Games and Mechanism Design</h3>
<p><a href='http://arxiv.org/abs/2408.05146v1'>http://arxiv.org/abs/2408.05146v1</a></p>
<p><b>Compressor summary</b>: The paper examines the consequences of performative prediction in multiagent scenarios, where accuracy maximization can harm social welfare, and proposes a method based on Bayesian agent behavior modeling to improve outcomes.</p><hr><h3>A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning</h3>
<p><a href='http://arxiv.org/abs/2408.05141v1'>http://arxiv.org/abs/2408.05141v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a hybrid retrieval-augmented generation system that improves accuracy, reasoning, and numerical computation by integrating external knowledge bases and optimizing various components, achieving significant results on the CRAG dataset.</p><hr><h3>Cycle-Configuration: A Novel Graph-theoretic Descriptor Set for  Molecular Inference</h3>
<p><a href='http://arxiv.org/abs/2408.05136v1'>http://arxiv.org/abs/2408.05136v1</a></p>
<p><b>Compressor summary</b>: The paper introduces cycle-configuration descriptors for molecular inference, which enable better prediction of chemical properties using mixed integer linear programming and machine learning.</p><hr><h3>Range Membership Inference Attacks</h3>
<p><a href='http://arxiv.org/abs/2408.05131v1'>http://arxiv.org/abs/2408.05131v1</a></p>
<p><b>Compressor summary</b>: Range membership inference attacks (RaMIAs) are a new method to measure privacy risks in machine learning models by testing if the model was trained on data in a specified range, rather than just checking for exact matches.</p><hr><h3>Cautious Calibration in Binary Classification</h3>
<p><a href='http://arxiv.org/abs/2408.05120v1'>http://arxiv.org/abs/2408.05120v1</a></p>
<p><b>Compressor summary</b>: The text introduces cautious calibration, a method to make machine learning probabilities intentionally underconfident for better decision-making in high-risk scenarios.</p><hr><h3>How Well Do LLMs Identify Cultural Unity in Diversity?</h3>
<p><a href='http://arxiv.org/abs/2408.05102v1'>http://arxiv.org/abs/2408.05102v1</a></p>
<p><b>Compressor summary</b>: CUNIT is a dataset for evaluating large language models' understanding of cultural unity across clothing and food concepts in 10 countries, revealing their limitations compared to humans.</p><hr><h3>MooER: LLM-based Speech Recognition and Translation Models from Moore  Threads</h3>
<p><a href='http://arxiv.org/abs/2408.05101v1'>http://arxiv.org/abs/2408.05101v1</a></p>
<p><b>Compressor summary</b>: MooER is an LLM-based ASR/AST model that uses pseudo labeled data for training and achieves competitive performance with open source models.</p><hr><h3>Unlocking Decoding-time Controllability: Gradient-Free Multi-Objective  Alignment with Contrastive Prompts</h3>
<p><a href='http://arxiv.org/abs/2408.05094v1'>http://arxiv.org/abs/2408.05094v1</a></p>
<p><b>Compressor summary</b>: MCA is a new method that uses expert and adversarial prompts to balance multiple alignment objectives of large language models without training separate models for each preference.</p><hr><h3>Order Matters in Hallucination: Reasoning Order as Benchmark and  Reflexive Prompting for Large-Language-Models</h3>
<p><a href='http://arxiv.org/abs/2408.05093v1'>http://arxiv.org/abs/2408.05093v1</a></p>
<p><b>Compressor summary</b>: The study proposes a new method to assess and improve the consistency of large language models by comparing answers generated through different approaches, addressing the issue of fabricated responses and justifications.</p><hr><h3>PriPHiT: Privacy-Preserving Hierarchical Training of Deep Neural  Networks</h3>
<p><a href='http://arxiv.org/abs/2408.05092v1'>http://arxiv.org/abs/2408.05092v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to train deep neural networks on edge devices and cloud servers while protecting sensitive data from leaks.</p><hr><h3>Loc4Plan: Locating Before Planning for Outdoor Vision and Language  Navigation</h3>
<p><a href='http://arxiv.org/abs/2408.05090v1'>http://arxiv.org/abs/2408.05090v1</a></p>
<p><b>Compressor summary</b>: Loc4Plan is a novel framework for outdoor navigation tasks that uses spatial perception to improve action planning by locating the agent's position before following instructions.</p><hr><h3>UNIC: Universal Classification Models via Multi-teacher Distillation</h3>
<p><a href='http://arxiv.org/abs/2408.05088v1'>http://arxiv.org/abs/2408.05088v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a unique encoder that combines several pretrained models using multi-teacher distillation
- The paper analyzes the benefits of different improvements to the basic distillation setup, such as expendable projectors and teacher dropping
- The paper achieves strong or better results than the best teacher for each classification task

Summary:
The paper presents a method to learn a unique encoder from multiple pretrained models using multi-teacher distillation with various improvements, and shows its effectiveness on different classification tasks.</p><hr><h3>Bootstrap Latents of Nodes and Neighbors for Graph Self-Supervised  Learning</h3>
<p><a href='http://arxiv.org/abs/2408.05087v1'>http://arxiv.org/abs/2408.05087v1</a></p>
<p><b>Compressor summary</b>: Our proposed method improves graph self-supervised learning by introducing noisy positive pairs from neighboring nodes and using cross-attention to score their supportiveness, reducing computation and enhancing downstream tasks.</p><hr><h3>Generating novel experimental hypotheses from language models: A case  study on cross-dative generalization</h3>
<p><a href='http://arxiv.org/abs/2408.05086v1'>http://arxiv.org/abs/2408.05086v1</a></p>
<p><b>Compressor summary</b>: The authors use neural network language models to investigate cross-dative generalization in language acquisition and propose a novel hypothesis regarding the role of exposure context features and harmonic alignment.</p><hr><h3>Generalizing Few Data to Unseen Domains Flexibly Based on Label  Smoothing Integrated with Distributionally Robust Optimization</h3>
<p><a href='http://arxiv.org/abs/2408.05082v1'>http://arxiv.org/abs/2408.05082v1</a></p>
<p><b>Compressor summary</b>: The paper introduces distributionally robust optimization to label smoothing for deep neural networks, improving their generalization on small-scale datasets by flexibly shifting data to unseen domains.</p><hr><h3>RT-Surv: Improving Mortality Prediction After Radiotherapy with Large  Language Model Structuring of Large-Scale Unstructured Electronic Health  Records</h3>
<p><a href='http://arxiv.org/abs/2408.05074v1'>http://arxiv.org/abs/2408.05074v1</a></p>
<p><b>Compressor summary</b>: Large language models can improve radiotherapy survival prediction by structuring unstructured electronic health record data, achieving high accuracy and interpretability.</p><hr><h3>Masked adversarial neural network for cell type deconvolution in spatial  transcriptomics</h3>
<p><a href='http://arxiv.org/abs/2408.05065v1'>http://arxiv.org/abs/2408.05065v1</a></p>
<p><b>Compressor summary</b>: The text introduces MACD, a method that uses adversarial learning to align spatial transcriptomics and single-cell RNA sequencing data for cell type deconvolution in disease tissues.</p><hr><h3>GLEAMS: Bridging the Gap Between Local and Global Explanations</h3>
<p><a href='http://arxiv.org/abs/2408.05060v1'>http://arxiv.org/abs/2408.05060v1</a></p>
<p><b>Compressor summary</b>: GLEAMS is a method that combines local and global approaches to explain machine learning algorithms by partitioning the input space and learning interpretable models in each subregion.</p><hr><h3>Graph Neural Networks as Ordering Heuristics for Parallel Graph Coloring</h3>
<p><a href='http://arxiv.org/abs/2408.05054v1'>http://arxiv.org/abs/2408.05054v1</a></p>
<p><b>Compressor summary</b>: The paper presents a graph neural network (GNN) based ordering heuristic for the graph coloring problem that balances quality, performance, and scalability, outperforming existing greedy heuristics.</p><hr><h3>BoFire: Bayesian Optimization Framework Intended for Real Experiments</h3>
<p><a href='http://arxiv.org/abs/2408.05040v1'>http://arxiv.org/abs/2408.05040v1</a></p>
<p><b>Compressor summary</b>: BoFire is an open-source Python package that uses Bayesian Optimization and other experimental designs to develop and optimize new chemistry, with features that make it adaptable to real-world settings like self-driving laboratories.</p><hr><h3>A conformalized learning of a prediction set with applications to  medical imaging classification</h3>
<p><a href='http://arxiv.org/abs/2408.05037v1'>http://arxiv.org/abs/2408.05037v1</a></p>
<p><b>Compressor summary</b>: The text introduces an algorithm that improves uncertainty quantification for medical imaging classifiers by generating prediction sets with specified probabilities, achieving better performance than existing methods.</p><hr><h3>Examining the Behavior of LLM Architectures Within the Framework of  Standardized National Exams in Brazil</h3>
<p><a href='http://arxiv.org/abs/2408.05035v1'>http://arxiv.org/abs/2408.05035v1</a></p>
<p><b>Compressor summary</b>: The study compares GPT-3.5 and 4 with MariTalk and human performance on the ENEM test, a standardized exam for university admission in Brazil, to assess AI biases and differences in answering styles.</p><hr><h3>Livestock Fish Larvae Counting using DETR and YOLO based Deep Networks</h3>
<p><a href='http://arxiv.org/abs/2408.05032v1'>http://arxiv.org/abs/2408.05032v1</a></p>
<p><b>Compressor summary</b>: The authors evaluate neural network architectures for fish larvae counting using a new annotated image dataset.</p><hr><h3>Collaborative Static-Dynamic Teaching: A Semi-Supervised Framework for  Stripe-Like Space Target Detection</h3>
<p><a href='http://arxiv.org/abs/2408.05029v1'>http://arxiv.org/abs/2408.05029v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel semi-supervised learning framework for detecting stripe-like space targets, which improves generalization and uses a feedback loop to enhance pseudo-label quality.</p><hr><h3>Investigating a Benchmark for Training-set free Evaluation of Linguistic  Capabilities in Machine Reading Comprehension</h3>
<p><a href='http://arxiv.org/abs/2408.05023v1'>http://arxiv.org/abs/2408.05023v1</a></p>
<p><b>Compressor summary</b>: The text proposes an alternative method for evaluating NLP systems using synthetic data sets that are natural and diverse, while not relying on crowd-sourced datasets.</p><hr><h3>RadarPillars: Efficient Object Detection from 4D Radar Point Clouds</h3>
<p><a href='http://arxiv.org/abs/2408.05020v1'>http://arxiv.org/abs/2408.05020v1</a></p>
<p><b>Compressor summary</b>: Key points:
- 4D radar data has sparsity and velocity information
- Existing deep learning methods for 3D object detection are not optimized for this data
- RadarPillars is a pillar-based network that decomposes radial velocity, uses PillarAttention, and studies layer scaling
- RadarPillars achieves better detection results, efficiency, and real-time performance on the View-of-Delft dataset

Summary:
RadarPillars is a novel network for 3D object detection from 4D radar data that leverages radial velocity decomposition, PillarAttention, and layer scaling to outperform existing methods in accuracy, efficiency, and real-time performance.</p><hr><h3>Instruction Tuning-free Visual Token Complement for Multimodal LLMs</h3>
<p><a href='http://arxiv.org/abs/2408.05019v1'>http://arxiv.org/abs/2408.05019v1</a></p>
<p><b>Compressor summary</b>: The Visual Token Complement framework helps multimodal language models improve their responses by generating complementary visual tokens from text-to-image generation, without requiring additional image-text pairs.</p><hr><h3>DreamCouple: Exploring High Quality Text-to-3D Generation Via Rectified  Flow</h3>
<p><a href='http://arxiv.org/abs/2408.05008v1'>http://arxiv.org/abs/2408.05008v1</a></p>
<p><b>Compressor summary</b>: The paper adapts Score Distillation Sampling to rectified flow models for 3D generation, addressing the over-smoothing issue with a novel coupled noise method called DreamCouple and achieving state-of-the-art results on NeRF and Gaussian splatting.</p><hr><h3>ProFuser: Progressive Fusion of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.04998v1'>http://arxiv.org/abs/2408.04998v1</a></p>
<p><b>Compressor summary</b>: The paper proposes ProFuser, a method that fuses large language models by considering both training and inference modes, improving performance in knowledge, reasoning, and safety.</p><hr><h3>On the use of neurosymbolic AI for defending against cyber attacks</h3>
<p><a href='http://arxiv.org/abs/2408.04996v1'>http://arxiv.org/abs/2408.04996v1</a></p>
<p><b>Compressor summary</b>: The paper argues for combining connectionist and symbolic AI to improve detection and response to cyber attacks, proposes use cases and challenges, and shows two experiments demonstrating feasibility of neurosymbolic AI.</p><hr><h3>Get Confused Cautiously: Textual Sequence Memorization Erasure with  Selective Entropy Maximization</h3>
<p><a href='http://arxiv.org/abs/2408.04983v1'>http://arxiv.org/abs/2408.04983v1</a></p>
<p><b>Compressor summary</b>: The paper introduces EMSO, a framework for textual sequence memorization erasure in large language models, which balances the trade-off between effectiveness and model utility using entropy maximization and contrastive gradient metric.</p><hr><h3>\textit{re}CSE: Portable Reshaping Features for Sentence Embedding in  Self-supervised Contrastive Learning</h3>
<p><a href='http://arxiv.org/abs/2408.04975v1'>http://arxiv.org/abs/2408.04975v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new sentence representation framework called reCSE that uses feature reshaping to improve semantic similarity and reduce memory consumption.</p><hr><h3>Towards aerodynamic surrogate modeling based on $Î²$-variational  autoencoders</h3>
<p><a href='http://arxiv.org/abs/2408.04969v1'>http://arxiv.org/abs/2408.04969v1</a></p>
<p><b>Compressor summary</b>: The proposed surrogate model combines a $\beta$-VAE with PCA and Gaussian Process Regression to predict pressure distributions on a transonic wing using flight conditions, providing a fast, cost-effective, and accurate alternative to high-fidelity CFD data.</p><hr><h3>Generalisation First, Memorisation Second? Memorisation Localisation for  Natural Language Classification Tasks</h3>
<p><a href='http://arxiv.org/abs/2408.04965v1'>http://arxiv.org/abs/2408.04965v1</a></p>
<p><b>Compressor summary</b>: The study investigates memorisation in neural language models across 12 tasks, finding it to be gradual, task-dependent, and challenging the generalisation-first hypothesis.</p><hr><h3>DAFT-GAN: Dual Affine Transformation Generative Adversarial Network for  Text-Guided Image Inpainting</h3>
<p><a href='http://arxiv.org/abs/2408.04962v1'>http://arxiv.org/abs/2408.04962v1</a></p>
<p><b>Compressor summary</b>: The paper introduces DAFT-GAN, a model that uses two affine transformation networks to combine text and image features for text-guided image inpainting, improving quality and consistency.</p><hr><h3>Surgical-VQLA++: Adversarial Contrastive Learning for Calibrated Robust  Visual Question-Localized Answering in Robotic Surgery</h3>
<p><a href='http://arxiv.org/abs/2408.04958v1'>http://arxiv.org/abs/2408.04958v1</a></p>
<p><b>Compressor summary</b>: VQLA is a new method that improves surgical VQA by providing precise and context-aware answers to questions about surgical images, while C$^2$G-ViL embeddings ensure safety and robustness in surgical scenarios.</p><hr><h3>LLaVA-VSD: Large Language-and-Vision Assistant for Visual Spatial  Description</h3>
<p><a href='http://arxiv.org/abs/2408.04957v1'>http://arxiv.org/abs/2408.04957v1</a></p>
<p><b>Compressor summary</b>: The paper introduces LLaVA-VSD, a large language-and-vision model that can classify, describe, and generate diverse sentences for visual spatial relationships in images using figure-caption pairs and a refinement step.</p><hr><h3>Model Debiasing by Learnable Data Augmentation</h3>
<p><a href='http://arxiv.org/abs/2408.04955v1'>http://arxiv.org/abs/2408.04955v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a 2-stage learning pipeline that uses data augmentation and over-biased models to improve Deep Neural Networks' generalization capabilities on biased and unbiased datasets.</p><hr><h3>HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented  Generation for Efficient Information Extraction</h3>
<p><a href='http://arxiv.org/abs/2408.04948v1'>http://arxiv.org/abs/2408.04948v1</a></p>
<p><b>Compressor summary</b>: The HybridRAG technique combines VectorRAG and GraphRAG to improve question-answer systems for extracting information from complex financial documents, outperforming traditional methods.</p><hr><h3>Quantitative Information Extraction from Humanitarian Documents</h3>
<p><a href='http://arxiv.org/abs/2408.04941v1'>http://arxiv.org/abs/2408.04941v1</a></p>
<p><b>Compressor summary</b>: The authors provide an annotated dataset and a custom NLP pipeline for extracting quantitative information from humanitarian texts, improving performance especially in documents about the Dominican Republic and some African countries.</p><hr><h3>Capsule Vision 2024 Challenge: Multi-Class Abnormality Classification  for Video Capsule Endoscopy</h3>
<p><a href='http://arxiv.org/abs/2408.04940v1'>http://arxiv.org/abs/2408.04940v1</a></p>
<p><b>Compressor summary</b>: The Capsule Vision 2024 Challenge is a virtual competition on detecting abnormalities in video capsule endoscopy images using multi-class classification.</p><hr><h3>UAV-Enhanced Combination to Application: Comprehensive Analysis and  Benchmarking of a Human Detection Dataset for Disaster Scenarios</h3>
<p><a href='http://arxiv.org/abs/2408.04922v1'>http://arxiv.org/abs/2408.04922v1</a></p>
<p><b>Compressor summary</b>: The C2A dataset improves machine learning models' performance in detecting humans in disaster scenes, enhancing search and rescue operations.</p><hr><h3>Avoid Wasted Annotation Costs in Open-set Active Learning with  Pre-trained Vision-Language Model</h3>
<p><a href='http://arxiv.org/abs/2408.04917v1'>http://arxiv.org/abs/2408.04917v1</a></p>
<p><b>Compressor summary</b>: CLIPNAL is a novel active learning strategy that minimizes annotation costs by selectively collecting informative data without requiring out-of-distribution samples.</p><hr><h3>PTrajM: Efficient and Semantic-rich Trajectory Learning with Pretrained  Trajectory-Mamba</h3>
<p><a href='http://arxiv.org/abs/2408.04916v1'>http://arxiv.org/abs/2408.04916v1</a></p>
<p><b>Compressor summary</b>: The paper introduces PTrajM, a novel method for efficient and semantic-rich vehicle trajectory learning that can extract continuous movement behavior and travel purposes from irregular and discrete trajectory points.</p><hr><h3>Knowledge Base Embeddings: Semantics and Theoretical Properties</h3>
<p><a href='http://arxiv.org/abs/2408.04913v1'>http://arxiv.org/abs/2408.04913v1</a></p>
<p><b>Compressor summary</b>: This paper studies how to embed knowledge bases in description logic into vector spaces while considering conceptual knowledge and geometric-based semantics.</p><hr><h3>A Geometric Nash Approach in Tuning the Learning Rate in Q-Learning  Algorithm</h3>
<p><a href='http://arxiv.org/abs/2408.04911v1'>http://arxiv.org/abs/2408.04911v1</a></p>
<p><b>Compressor summary</b>: The paper presents a geometric method to optimize the learning rate in Q learning by linking it to the angle between time steps and reward vector, using the angular bisector and Nash Equilibrium concepts.</p><hr><h3>Unleashing Artificial Cognition: Integrating Multiple AI Systems</h3>
<p><a href='http://arxiv.org/abs/2408.04910v1'>http://arxiv.org/abs/2408.04910v1</a></p>
<p><b>Compressor summary</b>: The study introduces a novel AI system that combines language models and query analysis to explain its decisions in Chess and potentially other domains.</p><hr><h3>Surveying the Landscape of Image Captioning Evaluation: A Comprehensive  Taxonomy and Novel Ensemble Method</h3>
<p><a href='http://arxiv.org/abs/2408.04909v1'>http://arxiv.org/abs/2408.04909v1</a></p>
<p><b>Compressor summary</b>: The authors provide a comprehensive survey and taxonomy of image captioning metrics, propose EnsembEval, an ensemble of methods with the highest correlation to human judgements, and suggest that more diverse metrics can improve image captioning evaluation.</p><hr><h3>Towards a Generative Approach for Emotion Detection and Reasoning</h3>
<p><a href='http://arxiv.org/abs/2408.04906v1'>http://arxiv.org/abs/2408.04906v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel generative question-answering method to detect and reason about emotions using large language models, instead of relying on textual entailment models with fixed labels.</p><hr><h3>GlitchProber: Advancing Effective Detection and Mitigation of Glitch  Tokens in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.04905v1'>http://arxiv.org/abs/2408.04905v1</a></p>
<p><b>Compressor summary</b>: The text discusses the issue of glitch tokens in large language models, which can cause incorrect or harmful outputs, and presents GlitchProber, a tool for detecting and mitigating these tokens.</p><hr><h3>Axiomatic Characterisations of Sample-based Explainers</h3>
<p><a href='http://arxiv.org/abs/2408.04903v1'>http://arxiv.org/abs/2408.04903v1</a></p>
<p><b>Compressor summary</b>: The paper examines feature-based explainers for black-box classifiers, identifies desirable properties, characterizes their subfamilies, and introduces new instances with guaranteed existence and consistency of explanations.</p><hr><h3>Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural  Communication in Codenames</h3>
<p><a href='http://arxiv.org/abs/2408.04900v1'>http://arxiv.org/abs/2408.04900v1</a></p>
<p><b>Compressor summary</b>: RSA+C3, a method to improve cross-cultural communication, is tested and shown to ease collaboration in the game Codenames Duet by inferring sociocultural context from interaction.</p><hr><h3>Better Not to Propagate: Understanding Edge Uncertainty and  Over-smoothing in Signed Graph Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2408.04895v1'>http://arxiv.org/abs/2408.04895v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to estimate graph properties and dynamically select between blocked and signed propagation schemes for GNNs, improving their performance on both homophilic and heterophilic graphs.</p><hr><h3>Clustering-friendly Representation Learning for Enhancing Salient  Features</h3>
<p><a href='http://arxiv.org/abs/2408.04891v1'>http://arxiv.org/abs/2408.04891v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Representation learning with contrastive learning algorithms is effective for unlabeled datasets but struggles to distinguish important features from unimportant ones.
- The paper proposes a method that enhances features critical for unsupervised image clustering by using a reference dataset and a contrastive analysis approach.
- The method outperforms conventional contrastive analysis and deep clustering methods on three datasets with different backgrounds.

Summary:
The paper presents a novel representation learning method for unsupervised image clustering that uses a reference dataset and a contrastive analysis approach to distinguish important features from unimportant ones, achieving higher clustering scores than existing methods.</p><hr><h3>On the Element-Wise Representation and Reasoning in Zero-Shot Image  Recognition: A Systematic Survey</h3>
<p><a href='http://arxiv.org/abs/2408.04879v1'>http://arxiv.org/abs/2408.04879v1</a></p>
<p><b>Compressor summary</b>: This paper reviews element-wise zero-shot image recognition techniques, which learn generalized knowledge from limited data to recognize and reason about unseen domains.</p><hr><h3>Unsupervised Episode Detection for Large-Scale News Events</h3>
<p><a href='http://arxiv.org/abs/2408.04873v1'>http://arxiv.org/abs/2408.04873v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel task called episode detection to identify cohesive clusters of entities and actions in news articles related to key events, and introduces EpiMine, an unsupervised framework that significantly improves performance on this task.</p><hr><h3>SCOI: Syntax-augmented Coverage-based In-context Example Selection for  Machine Translation</h3>
<p><a href='http://arxiv.org/abs/2408.04872v1'>http://arxiv.org/abs/2408.04872v1</a></p>
<p><b>Compressor summary</b>: The paper introduces SCOI, a method that uses both syntactic and lexical information to select better in-context examples for machine translation.</p><hr><h3>UCB Exploration for Fixed-Budget Bayesian Best Arm Identification</h3>
<p><a href='http://arxiv.org/abs/2408.04869v1'>http://arxiv.org/abs/2408.04869v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a Bayesian UCB algorithm for fixed-budget best-arm identification
- The algorithm learns prior information to enhance performance
- The paper provides theoretical and empirical bounds on the regret and failure probability

Summary:
The paper introduces a Bayesian UCB exploration algorithm that learns prior information to improve fixed-budget best-arm identification, and shows its theoretical and empirical advantages.</p><hr><h3>ChatGPT Meets Iris Biometrics</h3>
<p><a href='http://arxiv.org/abs/2408.04868v1'>http://arxiv.org/abs/2408.04868v1</a></p>
<p><b>Compressor summary</b>: The study shows that GPT-4 is effective at iris recognition, even under challenging conditions, outperforming Google's Gemini Advanced model.</p><hr><h3>An Evaluation of Standard Statistical Models and LLMs on Time Series  Forecasting</h3>
<p><a href='http://arxiv.org/abs/2408.04867v1'>http://arxiv.org/abs/2408.04867v1</a></p>
<p><b>Compressor summary</b>: The paper studies how Large Language Models (LLMs) perform in predicting time series, finding that their accuracy drops when faced with complex or diverse data.</p><hr><h3>High dimensional Bayesian Optimization via Condensing-Expansion  Projection</h3>
<p><a href='http://arxiv.org/abs/2408.04860v1'>http://arxiv.org/abs/2408.04860v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CEPBO, a new random projection-based Bayesian optimization method for high-dimensional problems that does not rely on the effective subspace assumption and performs better than existing methods in most cases.</p><hr><h3>MSG-Chart: Multimodal Scene Graph for ChartQA</h3>
<p><a href='http://arxiv.org/abs/2408.04852v1'>http://arxiv.org/abs/2408.04852v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a multimodal scene graph for charts to better understand their structure and semantics, improving performance on chart question answering tasks.</p><hr><h3>Your Classifier Can Be Secretly a Likelihood-Based OOD Detector</h3>
<p><a href='http://arxiv.org/abs/2408.04851v1'>http://arxiv.org/abs/2408.04851v1</a></p>
<p><b>Compressor summary</b>: INK is a novel OOD detection method that leverages likelihood interpretation of discriminative classifiers on hyperspherical embeddings, achieving state-of-the-art performance in various scenarios.</p><hr><h3>Ensemble BERT: A student social network text sentiment classification  model based on ensemble learning and BERT architecture</h3>
<p><a href='http://arxiv.org/abs/2408.04849v1'>http://arxiv.org/abs/2408.04849v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new BERT-based ensemble learning network to assess emotional tendencies in middle school students' social network texts, finding that deeper networks are more efficient but ensembles offer better interpretability.</p><hr><h3>MDS-GNN: A Mutual Dual-Stream Graph Neural Network on Graphs with  Incomplete Features and Structure</h3>
<p><a href='http://arxiv.org/abs/2408.04845v1'>http://arxiv.org/abs/2408.04845v1</a></p>
<p><b>Compressor summary</b>: MDS-GNN is a novel method that leverages both node features and graph structure for learning on incomplete graphs using contrastive learning.</p><hr><h3>Counterfactual Explanations with Probabilistic Guarantees on their  Robustness to Model Change</h3>
<p><a href='http://arxiv.org/abs/2408.04842v1'>http://arxiv.org/abs/2408.04842v1</a></p>
<p><b>Compressor summary</b>: BetaRCE is a method for generating counterfactual explanations for machine learning models that can handle data or model changes, providing probabilistic guarantees on robustness and interpretability.</p><hr><h3>Kolmogorov-Arnold Network for Online Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2408.04841v1'>http://arxiv.org/abs/2408.04841v1</a></p>
<p><b>Compressor summary</b>: The paper explores using Kolmogorov-Arnold Networks in Proximal Policy Optimization and shows they can achieve comparable performance to MLPs with fewer parameters, suggesting efficiency gains.</p><hr><h3>mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal  Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.04840v1'>http://arxiv.org/abs/2408.04840v1</a></p>
<p><b>Compressor summary</b>: mPLUG-Owl3 is a versatile multi-modal large language model that excels at understanding long image sequences by efficiently integrating vision and language with novel hyper attention blocks, achieving state-of-the-art performance on various benchmarks.</p><hr><h3>Dual-Channel Latent Factor Analysis Enhanced Graph Contrastive Learning  for Recommendation</h3>
<p><a href='http://arxiv.org/abs/2408.04838v1'>http://arxiv.org/abs/2408.04838v1</a></p>
<p><b>Compressor summary</b>: LFA-GCL is a new graph contrastive learning technique that improves recommender systems by refining global collaborative graphs without noise and outperforming existing methods.</p><hr><h3>Self-augmented Gaussian Splatting with Structure-aware Masks for  Sparse-view 3D Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2408.04831v1'>http://arxiv.org/abs/2408.04831v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for 3D reconstruction from few images, using a coarse-to-fine Gaussian model with structure-aware mask and augmentation, achieving state-of-the-art results on two datasets.</p><hr><h3>Interventional Causal Structure Discovery over Graphical Models with  Convergence and Optimality Guarantees</h3>
<p><a href='http://arxiv.org/abs/2408.04819v1'>http://arxiv.org/abs/2408.04819v1</a></p>
<p><b>Compressor summary</b>: Bloom is a novel framework for discovering causal structures from observational and interventional data, outperforming existing methods in efficiency and privacy.</p><hr><h3>Performance Metric for Multiple Anomaly Score Distributions with  Discrete Severity Levels</h3>
<p><a href='http://arxiv.org/abs/2408.04817v1'>http://arxiv.org/abs/2408.04817v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new performance metric (WS-AUROC) for anomaly detection in smart factories, which considers severity levels and physical quantities causing anomalies.</p><hr><h3>FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt  Optimization Across Tokenizers</h3>
<p><a href='http://arxiv.org/abs/2408.04816v1'>http://arxiv.org/abs/2408.04816v1</a></p>
<p><b>Compressor summary</b>: FUSE is a method to unify different language models' semantic embeddings using a tensor representation, enabling knowledge transfer across tokenizers for tasks like image captioning.</p><hr><h3>Towards improving Alzheimer's intervention: a machine learning approach  for biomarker detection through combining MEG and MRI pipelines</h3>
<p><a href='http://arxiv.org/abs/2408.04815v1'>http://arxiv.org/abs/2408.04815v1</a></p>
<p><b>Compressor summary</b>: MEG can detect brain changes in Alzheimer's disease before symptoms appear, and combining MRI and MEG data improves the accuracy of diagnosis.</p><hr><h3>Rethinking Multiple Instance Learning: Developing an Instance-Level  Classifier via Weakly-Supervised Self-Training</h3>
<p><a href='http://arxiv.org/abs/2408.04813v1'>http://arxiv.org/abs/2408.04813v1</a></p>
<p><b>Compressor summary</b>: Key points:
- MIL problem is formulated as a semi-supervised instance classification problem to utilize labeled and unlabeled instances fully
- Traditional self-training techniques degenerate in generating pseudo labels for unlabeled instances in MIL
- A weakly-supervised self-training method is proposed that uses bag labels to prevent pseudo label degradation and learn hard positive instances
- Experiments on various datasets show the superiority of the proposed method over existing methods

Summary:
The paper proposes a weakly-supervised self-training method for MIL that leverages bag labels to generate non-degenerate pseudo labels and learn hard positive instances, achieving new SOTA performance on multiple datasets.</p><hr><h3>UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond  Scaling</h3>
<p><a href='http://arxiv.org/abs/2408.04810v1'>http://arxiv.org/abs/2408.04810v1</a></p>
<p><b>Compressor summary</b>: UniBench is a unified framework for evaluating vision-language models on over 50 benchmarks, revealing the limitations and strengths of current models.</p><hr><h3>On the Geometry of Deep Learning</h3>
<p><a href='http://arxiv.org/abs/2408.04809v1'>http://arxiv.org/abs/2408.04809v1</a></p>
<p><b>Compressor summary</b>: The paper explores how deep learning's connection to affine splines, continuous piecewise linear functions in multiple dimensions, can help understand and improve deep networks by analyzing their geometrical properties and input space tessellation.</p><hr><h3>Hyper-YOLO: When Visual Object Detection Meets Hypergraph Computation</h3>
<p><a href='http://arxiv.org/abs/2408.04804v1'>http://arxiv.org/abs/2408.04804v1</a></p>
<p><b>Compressor summary</b>: Hyper-YOLO is a new object detection method that uses hypergraph computations to capture complex high-order correlations among visual features, improving performance on various scale models and datasets.</p><hr><h3>FewShotNeRF: Meta-Learning-based Novel View Synthesis for Rapid  Scene-Specific Adaptation</h3>
<p><a href='http://arxiv.org/abs/2408.04803v1'>http://arxiv.org/abs/2408.04803v1</a></p>
<p><b>Compressor summary</b>: FewShotNeRF is a method that uses meta-learning to quickly adapt NeRF models to new scenes with limited multi-view images, producing realistic views of various objects.</p><hr><h3>AI and Machine Learning Driven Indoor Localization and Navigation with  Mobile Embedded Systems</h3>
<p><a href='http://arxiv.org/abs/2408.04797v1'>http://arxiv.org/abs/2408.04797v1</a></p>
<p><b>Compressor summary</b>: This article discusses the challenges of indoor navigation and how AI algorithms can help solve them using WiFi and sensors in mobile devices.</p>