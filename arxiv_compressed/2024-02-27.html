
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-02-27</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-02-27 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Pre-training Cross-lingual Open Domain Question Answering with  Large-scale Synthetic Supervision</h3>
<p><a href='http://arxiv.org/abs/2402.16508v1'>http://arxiv.org/abs/2402.16508v1</a></p>
<p><b>Compressor summary</b>: The paper presents a single encoder-decoder model for cross-lingual question answering (CLQA) that uses self-supervision from Wikipedia's link structure to perform retrieval and answer generation without auxiliary resources like machine translation.</p><hr><h3>Stochastic Conditional Diffusion Models for Semantic Image Synthesis</h3>
<p><a href='http://arxiv.org/abs/2402.16506v1'>http://arxiv.org/abs/2402.16506v1</a></p>
<p><b>Compressor summary</b>: The paper proposes SCDM, a robust conditional diffusion model for semantic image synthesis with noisy labels, which enhances robustness by stochastically perturbing the semantic label maps through Label Diffusion and using a class-wise noise schedule.</p><hr><h3>Memory GAPS: Would LLM pass the Tulving Test?</h3>
<p><a href='http://arxiv.org/abs/2402.16505v1'>http://arxiv.org/abs/2402.16505v1</a></p>
<p><b>Compressor summary</b>: The Tulving Test measures memory tasks to evaluate a model of human recall, and explores if this model applies to LLMs' remembering abilities.</p><hr><h3>LLMArena: Assessing Capabilities of Large Language Models in Dynamic  Multi-Agent Environments</h3>
<p><a href='http://arxiv.org/abs/2402.16499v1'>http://arxiv.org/abs/2402.16499v1</a></p>
<p><b>Compressor summary</b>: LLMArena is a framework for testing large language models' abilities in multi-agent dynamics using seven gaming environments and Trueskill scoring.</p><hr><h3>Intelligent Known and Novel Aircraft Recognition -- A Shift from  Classification to Similarity Learning for Combat Identification</h3>
<p><a href='http://arxiv.org/abs/2402.16486v1'>http://arxiv.org/abs/2402.16486v1</a></p>
<p><b>Compressor summary</b>: The text describes a novel AI method that can accurately identify both known and unknown military and civilian aircraft from low-resolution images, overcoming the limitations of traditional methods.</p><hr><h3>On Languaging a Simulation Engine</h3>
<p><a href='http://arxiv.org/abs/2402.16482v1'>http://arxiv.org/abs/2402.16482v1</a></p>
<p><b>Compressor summary</b>: The Lang2Sim framework uses functionalized language models to transform textual descriptions of material simulations into executable code, enabling interactive navigation and efficient programming.</p><hr><h3>Edge Detectors Can Make Deep Convolutional Neural Networks More Robust</h3>
<p><a href='http://arxiv.org/abs/2402.16479v1'>http://arxiv.org/abs/2402.16479v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a binary edge feature branch for deep convolutional neural networks to improve their robustness against adversarial examples by incorporating shape-like features with texture features.</p><hr><h3>DCVSMNet: Double Cost Volume Stereo Matching Network</h3>
<p><a href='http://arxiv.org/abs/2402.16473v1'>http://arxiv.org/abs/2402.16473v1</a></p>
<p><b>Compressor summary</b>: DCVSMNet is a fast stereo matching network that uses two small cost volumes to produce accurate results with some trade-offs in inference time.</p><hr><h3>mEdIT: Multilingual Text Editing via Instruction Tuning</h3>
<p><a href='http://arxiv.org/abs/2402.16472v1'>http://arxiv.org/abs/2402.16472v1</a></p>
<p><b>Compressor summary</b>: mEdIT is a multi-lingual text editing model that takes user instructions and uses pre-trained language models to perform tasks like error correction, simplification, and paraphrasing across diverse languages.</p><hr><h3>Unveiling Vulnerability of Self-Attention</h3>
<p><a href='http://arxiv.org/abs/2402.16470v1'>http://arxiv.org/abs/2402.16470v1</a></p>
<p><b>Compressor summary</b>: The paper proposes HackAttend, a perturbation technique that attacks PLMs by altering attention scores in self-attention mechanisms, and introduces S-Attend, a smoothing technique that makes SA robust to such attacks.</p><hr><h3>Learning to Schedule Online Tasks with Bandit Feedback</h3>
<p><a href='http://arxiv.org/abs/2402.16463v1'>http://arxiv.org/abs/2402.16463v1</a></p>
<p><b>Compressor summary</b>: DOL-RM is an online task scheduling algorithm that optimizes performance by estimating rewards, costs, and arrival distributions under uncertainty.</p><hr><h3>Defending LLMs against Jailbreaking Attacks via Backtranslation</h3>
<p><a href='http://arxiv.org/abs/2402.16459v1'>http://arxiv.org/abs/2402.16459v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a backtranslation method to reveal the hidden intent of original prompts and defend language models against jailbreaking attacks.</p><hr><h3>D-XCB: Data-independent Debiasing for Fair and Accurate  Transformer-based Cyberbullying Detection</h3>
<p><a href='http://arxiv.org/abs/2402.16458v1'>http://arxiv.org/abs/2402.16458v1</a></p>
<p><b>Compressor summary</b>: ID-XCB is a new technique that reduces biases in cyberbullying detection by ignoring swear words without harming performance, and it works well on different datasets.</p><hr><h3>RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for  Short-form Open-Domain Question Answering</h3>
<p><a href='http://arxiv.org/abs/2402.16457v1'>http://arxiv.org/abs/2402.16457v1</a></p>
<p><b>Compressor summary</b>: This paper introduces RetrievalQA, a benchmark to evaluate adaptive retrieval-augmented generation methods, and proposes Time-Aware Adaptive Retrieval (TA-ARE), which improves the efficiency and relevance of sourced information.</p><hr><h3>ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable  Safety Detectors</h3>
<p><a href='http://arxiv.org/abs/2402.16444v1'>http://arxiv.org/abs/2402.16444v1</a></p>
<p><b>Compressor summary</b>: ShieldLM is a safety detector for Large Language Models that aligns with human safety standards, supports customization, and provides explanations.</p><hr><h3>On Distributed Larger-Than-Memory Subset Selection With Pairwise  Submodular Functions</h3>
<p><a href='http://arxiv.org/abs/2402.16442v1'>http://arxiv.org/abs/2402.16442v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a distributed bounding algorithm for subset selection problems with provable approximation guarantees and shows its effectiveness on large-scale datasets.</p><hr><h3>Language-Specific Neurons: The Key to Multilingual Capabilities in Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.16438v1'>http://arxiv.org/abs/2402.16438v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to identify language-specific regions in Transformer architectures of large language models (LLMs) and demonstrates how to control their output language by activating or deactivating these regions.</p><hr><h3>Training Implicit Generative Models via an Invariant Statistical Loss</h3>
<p><a href='http://arxiv.org/abs/2402.16435v1'>http://arxiv.org/abs/2402.16435v1</a></p>
<p><b>Compressor summary</b>: The authors propose a discriminator-free method for training one-dimensional and multivariate generative implicit models that learns complex data distributions and avoids mode-dropping issues.</p><hr><h3>RoCoIns: Enhancing Robustness of Large Language Models through  Code-Style Instructions</h3>
<p><a href='http://arxiv.org/abs/2402.16431v1'>http://arxiv.org/abs/2402.16431v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using code-style instructions to improve LLMs' robustness to adversarial samples and introduces a novel method for few-shot learning with both clean and adversarial contexts.</p><hr><h3>COMAE: COMprehensive Attribute Exploration for Zero-shot Hashing</h3>
<p><a href='http://arxiv.org/abs/2402.16424v1'>http://arxiv.org/abs/2402.16424v1</a></p>
<p><b>Compressor summary</b>: COMAE is a method for improving zero-shot hashing by exploring locality relationships and utilizing continuous-value attributes, achieving better performance on large-scale retrieval scenarios.</p><hr><h3>Outline-Guided Object Inpainting with Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2402.16421v1'>http://arxiv.org/abs/2402.16421v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a method to generate new instance segmentation images by filling out masked areas with desired object classes using diffusion-based inpainting and object outline guidance.
- The method preserves mask annotations, shape characteristics, and introduces diversity within the augmented area.
- The method can be combined with text guidance and other image augmentation techniques.

Summary:
The paper presents a technique to create diverse instance segmentation datasets by inpainting object classes into masked regions using object outlines as guidance, while keeping mask annotations and shape features intact.</p><hr><h3>Predicting Sustainable Development Goals Using Course Descriptions --  from LLMs to Conventional Foundation Models</h3>
<p><a href='http://arxiv.org/abs/2402.16420v1'>http://arxiv.org/abs/2402.16420v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to generate training data using PaLM 2 and train smaller models to predict SDGs for university courses, achieving an F1-score of 0.786.</p><hr><h3>TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis</h3>
<p><a href='http://arxiv.org/abs/2402.16412v1'>http://arxiv.org/abs/2402.16412v1</a></p>
<p><b>Compressor summary</b>: The paper introduces TOTEM, a method to represent time series data with discrete vectors, enabling generalist training across tasks and domains without tuning.</p><hr><h3>CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency</h3>
<p><a href='http://arxiv.org/abs/2402.16407v1'>http://arxiv.org/abs/2402.16407v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a simple depth-aware consistency method for NeRF that uses layered representations and constrained rendering to improve novel view synthesis when few input views are available.</p><hr><h3>From RAGs to riches: Using large language models to write documents for  clinical trials</h3>
<p><a href='http://arxiv.org/abs/2402.16406v1'>http://arxiv.org/abs/2402.16406v1</a></p>
<p><b>Compressor summary</b>: The text evaluates the use of large language models (LLMs) to generate clinical trial protocols and shows that combining them with retrieval-augmented generation can improve their quality significantly.</p><hr><h3>Graph Learning with Distributional Edge Layouts</h3>
<p><a href='http://arxiv.org/abs/2402.16402v1'>http://arxiv.org/abs/2402.16402v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Distributional Edge Layouts (DELs), a pre-processing method for Graph Neural Networks (GNNs) that samples edge layouts using Langevin dynamics and Boltzmann distribution, improving GNN performance on various tasks.</p><hr><h3>Analysis of Embeddings Learned by End-to-End Machine Learning Eye  Movement-driven Biometrics Pipeline</h3>
<p><a href='http://arxiv.org/abs/2402.16399v1'>http://arxiv.org/abs/2402.16399v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how eye movement biometrics using machine learning are affected by input data variations in terms of temporal persistence, reliability, and efficacy.</p><hr><h3>Placing Objects in Context via Inpainting for Out-of-distribution  Segmentation</h3>
<p><a href='http://arxiv.org/abs/2402.16392v1'>http://arxiv.org/abs/2402.16392v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Placing Objects in Context (POC), a pipeline that can realistically add any object into any image, and shows how it can improve anomaly segmentation and learning new classes for semantic segmentation models.</p><hr><h3>MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in  Intellectual Property</h3>
<p><a href='http://arxiv.org/abs/2402.16389v1'>http://arxiv.org/abs/2402.16389v1</a></p>
<p><b>Compressor summary</b>: The paper introduces MoZIP, a new benchmark for evaluating large language models in the intellectual property domain, and presents MoZi, a multilingual IP-oriented model that outperforms other LLMs on the benchmark.</p><hr><h3>On the Generalization Capability of Temporal Graph Learning Algorithms:  Theoretical Insights and a Simpler Method</h3>
<p><a href='http://arxiv.org/abs/2402.16387v1'>http://arxiv.org/abs/2402.16387v1</a></p>
<p><b>Compressor summary</b>: This paper explores the generalization ability of different Temporal Graph Learning (TGL) algorithms and proposes Simplified-Temporal-Graph-Network with improved performance and lower complexity.</p><hr><h3>Self Supervised Correlation-based Permutations for Multi-View Clustering</h3>
<p><a href='http://arxiv.org/abs/2402.16383v1'>http://arxiv.org/abs/2402.16383v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel end-to-end deep learning framework for multi-view clustering that learns fused data representations and cluster assignments simultaneously.</p><hr><h3>Immunization against harmful fine-tuning attacks</h3>
<p><a href='http://arxiv.org/abs/2402.16382v1'>http://arxiv.org/abs/2402.16382v1</a></p>
<p><b>Compressor summary</b>: The paper proposes "Immunization conditions" as a framework for defending against harmful fine-tuning attacks on large language models (LLMs) by bad actors.</p><hr><h3>Improving LLM-based Machine Translation with Systematic Self-Correction</h3>
<p><a href='http://arxiv.org/abs/2402.16379v1'>http://arxiv.org/abs/2402.16379v1</a></p>
<p><b>Compressor summary</b>: The TER system helps improve machine translations by using human feedback to correct errors in large language models across different languages.</p><hr><h3>Graph Learning under Distribution Shifts: A Comprehensive Survey on  Domain Adaptation, Out-of-distribution, and Continual Learning</h3>
<p><a href='http://arxiv.org/abs/2402.16374v1'>http://arxiv.org/abs/2402.16374v1</a></p>
<p><b>Compressor summary</b>: The text discusses graph learning methods that handle distribution shifts in real-world data, categorizes them into different scenarios, and provides a survey of existing approaches and future directions.</p><hr><h3>DEYO: DETR with YOLO for End-to-End Object Detection</h3>
<p><a href='http://arxiv.org/abs/2402.16370v1'>http://arxiv.org/abs/2402.16370v1</a></p>
<p><b>Compressor summary</b>: Step-by-step training improves object detection by initializing the backbone with a classic detector, then training the decoder from scratch, achieving real-time performance and accuracy without extra data.</p><hr><h3>Generative AI in Vision: A Survey on Models, Metrics and Applications</h3>
<p><a href='http://arxiv.org/abs/2402.16369v1'>http://arxiv.org/abs/2402.16369v1</a></p>
<p><b>Compressor summary</b>: This paper surveys generative AI diffusion models, their techniques, applications, and challenges across various domains, highlighting their potential for creative tasks and data augmentation.</p><hr><h3>Unraveling Babel: Exploring Multilingual Activation Patterns within  Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.16367v1'>http://arxiv.org/abs/2402.16367v1</a></p>
<p><b>Compressor summary</b>: The study analyzes how large language models process multiple languages using a Mixture of Experts architecture, discovering both non-language-specific and language-specific neurons that can be used to improve performance and guide model training.</p><hr><h3>SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field</h3>
<p><a href='http://arxiv.org/abs/2402.16366v1'>http://arxiv.org/abs/2402.16366v1</a></p>
<p><b>Compressor summary</b>: The paper introduces SPC-NeRF, a new compression technique for Neural Radiance Fields using spatial predictive coding, which achieves better efficiency and quality than existing methods.</p><hr><h3>Where Do We Go from Here? Multi-scale Allocentric Relational Inference  from Natural Spatial Descriptions</h3>
<p><a href='http://arxiv.org/abs/2402.16364v1'>http://arxiv.org/abs/2402.16364v1</a></p>
<p><b>Compressor summary</b>: The paper introduces the Rendezvous (RVS) task and dataset for studying geospatial instructions with map knowledge that involve more complex spatial relations than previous navigation benchmarks.</p><hr><h3>LLM Inference Unveiled: Survey and Roofline Model Insights</h3>
<p><a href='http://arxiv.org/abs/2402.16363v1'>http://arxiv.org/abs/2402.16363v1</a></p>
<p><b>Compressor summary</b>: This paper surveys the current state of research on efficient Large Language Model inference, introducing a roofline model-based framework to analyze and compare various techniques, and provides valuable insights for practical implementation.</p><hr><h3>Layer-wise Regularized Dropout for Neural Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.16361v1'>http://arxiv.org/abs/2402.16361v1</a></p>
<p><b>Compressor summary</b>: LR-Drop is a new technique for Transformer-based language models that uses consistency training to regularize dropout at the output layer, leading to improved performance on various natural language understanding and generation tasks.</p><hr><h3>Feedback Efficient Online Fine-Tuning of Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2402.16359v1'>http://arxiv.org/abs/2402.16359v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new RL method to efficiently explore and optimize complex data distributions using diffusion models.</p><hr><h3>An Integrated Data Processing Framework for Pretraining Foundation  Models</h3>
<p><a href='http://arxiv.org/abs/2402.16358v1'>http://arxiv.org/abs/2402.16358v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a data processing framework to improve foundation models' performance by refining their pretraining data quality using operators at different levels, probing, and evaluation tools.</p><hr><h3>What Text Design Characterizes Book Genres?</h3>
<p><a href='http://arxiv.org/abs/2402.16356v1'>http://arxiv.org/abs/2402.16356v1</a></p>
<p><b>Compressor summary</b>: The study examines how text design on book covers affects our understanding of book genres using semantic information and visual design.</p><hr><h3>Language-guided Skill Learning with Temporal Variational Inference</h3>
<p><a href='http://arxiv.org/abs/2402.16354v1'>http://arxiv.org/abs/2402.16354v1</a></p>
<p><b>Compressor summary</b>: The algorithm uses LLMs to segment trajectories and then merges them to discover reusable skills for agents, improving their performance on new tasks.</p><hr><h3>MathGenie: Generating Synthetic Data with Question Back-translation for  Enhancing Mathematical Reasoning of LLMs</h3>
<p><a href='http://arxiv.org/abs/2402.16352v1'>http://arxiv.org/abs/2402.16352v1</a></p>
<p><b>Compressor summary</b>: MathGenie is a new method to create diverse and reliable math problems from seed data, using augmentation, back-translation, and rationale-based verification, achieving state-of-the-art performance in mathematical reasoning.</p><hr><h3>Impression-CLIP: Contrastive Shape-Impression Embedding for Fonts</h3>
<p><a href='http://arxiv.org/abs/2402.16350v1'>http://arxiv.org/abs/2402.16350v1</a></p>
<p><b>Compressor summary</b>: Impression-CLIP is a machine-learning model that uses CLIP to co-embed font images and their impressions for cross-modal retrieval, achieving better accuracy than existing methods and being robust to noise and missing tags.</p><hr><h3>C-GAIL: Stabilizing Generative Adversarial Imitation Learning with  Control Theory</h3>
<p><a href='http://arxiv.org/abs/2402.16349v1'>http://arxiv.org/abs/2402.16349v1</a></p>
<p><b>Compressor summary</b>: Controlled-GAIL (C-GAIL) is a new algorithm that uses control theory to improve the stability and efficiency of imitation learning by GANs.</p><hr><h3>CodeS: Towards Building Open-source Language Models for Text-to-SQL</h3>
<p><a href='http://arxiv.org/abs/2402.16347v1'>http://arxiv.org/abs/2402.16347v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CodeS, an open-source series of pre-trained language models designed for the text-to-SQL task, which outperforms existing closed-source models in accuracy and robustness while having smaller parameter sizes.</p><hr><h3>Boosting Graph Pooling with Persistent Homology</h3>
<p><a href='http://arxiv.org/abs/2402.16346v1'>http://arxiv.org/abs/2402.16346v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method to improve graph neural networks by injecting global topological invariance into pooling layers using persistent homology.</p><hr><h3>Contingency Planning Using Bi-level Markov Decision Processes for Space  Missions</h3>
<p><a href='http://arxiv.org/abs/2402.16342v1'>http://arxiv.org/abs/2402.16342v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a bi-level MDP framework for efficient and flexible rover mission contingency planning, addressing computational challenges in stochastic scenarios.</p><hr><h3>BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning  of SAM</h3>
<p><a href='http://arxiv.org/abs/2402.16338v1'>http://arxiv.org/abs/2402.16338v1</a></p>
<p><b>Compressor summary</b>: BLO-SAM is a model that improves semantic segmentation by automatically identifying objects without manual prompts and reducing overfitting risk through bi-level optimization.</p><hr><h3>Achieving $\tilde{O}(1/Îµ)$ Sample Complexity for Constrained  Markov Decision Process</h3>
<p><a href='http://arxiv.org/abs/2402.16324v1'>http://arxiv.org/abs/2402.16324v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new framework for analyzing constrained Markov decision processes (CMDPs), deriving optimal regret bounds and improving sample complexity by operating in the primal space and resolving the primal LP online with adaptive resource capacities.</p><hr><h3>Data-freeWeight Compress and Denoise for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.16319v1'>http://arxiv.org/abs/2402.16319v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel data-free method for compressing large language models' parameters by using rank-k approximation, achieving significant parameter reduction while preserving performance.</p><hr><h3>Gradient-Guided Modality Decoupling for Missing-Modality Robustness</h3>
<p><a href='http://arxiv.org/abs/2402.16318v1'>http://arxiv.org/abs/2402.16318v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve multimodal learning with incomplete data by reducing modality dominance and decoupling modalities, achieving better performance on three benchmarks.</p><hr><h3>Finer: Investigating and Enhancing Fine-Grained Visual Concept  Recognition in Large Vision Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.16315v1'>http://arxiv.org/abs/2402.16315v1</a></p>
<p><b>Compressor summary</b>: Instruction-tuned Large Vision-Language Models struggle with fine-grained visual categorization and explanation due to modality gap, while Finer benchmark aims to improve evaluation of their abilities.</p><hr><h3>Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based  Question Answering</h3>
<p><a href='http://arxiv.org/abs/2402.16313v1'>http://arxiv.org/abs/2402.16313v1</a></p>
<p><b>Compressor summary</b>: The Chain-of-Discussion framework uses multiple open-source LLMs to improve the correctness and comprehensiveness of answers for open-ended question answering tasks.</p><hr><h3>Cross-domain Chinese Sentence Pattern Parsing</h3>
<p><a href='http://arxiv.org/abs/2402.16311v1'>http://arxiv.org/abs/2402.16311v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new SPS parsing method that uses large language models and self-training to adapt to different domains, improving performance over rule-based approaches.</p><hr><h3>REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility  for Location Prediction over Sparse Trajectories</h3>
<p><a href='http://arxiv.org/abs/2402.16310v1'>http://arxiv.org/abs/2402.16310v1</a></p>
<p><b>Compressor summary</b>: REPLAY is a new RNN architecture for location prediction that incorporates timestamp embeddings with adaptive bandwidths to capture time-varying temporal regularities in human mobility data.</p><hr><h3>Referee Can Play: An Alternative Approach to Conditional Generation via  Model Inversion</h3>
<p><a href='http://arxiv.org/abs/2402.16305v1'>http://arxiv.org/abs/2402.16305v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a training-free method to improve text-image alignment in Diffusion Probabilistic Models by using discriminative Vision-Language Models and Score Distillation Sampling, achieving near state-of-the-art performance on T2I-Compbench.</p><hr><h3>Graph Diffusion Policy Optimization</h3>
<p><a href='http://arxiv.org/abs/2402.16302v1'>http://arxiv.org/abs/2402.16302v1</a></p>
<p><b>Compressor summary</b>: Graph Diffusion Policy Optimization (GDPO) is a new method that uses reinforcement learning to optimize graph diffusion models for arbitrary objectives, achieving state-of-the-art performance in various graph generation tasks.</p><hr><h3>Conformalized Selective Regression</h3>
<p><a href='http://arxiv.org/abs/2402.16300v1'>http://arxiv.org/abs/2402.16300v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new selective regression method called conformalized selective regression that uses model-specific biases to measure uncertainty and evaluate its performance.</p><hr><h3>MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer</h3>
<p><a href='http://arxiv.org/abs/2402.16298v1'>http://arxiv.org/abs/2402.16298v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new transformer-based multi-view network for breast cancer classification that leverages inter-view correlations using a novel dynamic attention block.</p><hr><h3>Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics</h3>
<p><a href='http://arxiv.org/abs/2402.16297v1'>http://arxiv.org/abs/2402.16297v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new Bayesian model for count time series that can adapt to changing dynamics over time, and shows that it performs better than existing models in predicting future values.</p><hr><h3>mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation  in RLD detection</h3>
<p><a href='http://arxiv.org/abs/2402.16291v1'>http://arxiv.org/abs/2402.16291v1</a></p>
<p><b>Compressor summary</b>: The proposed multi-scale Attention Pyramid module (mAPm) enhances object detection by integrating dilated convolutions, global self-attention, and refined up-sampling into the Feature Pyramid Network (FPN), achieving significant improvements in scale-variant tasks like Rice Leaf Disease detection.</p><hr><h3>PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification,  Retrieval, and Synthesis in Question Answering</h3>
<p><a href='http://arxiv.org/abs/2402.16288v1'>http://arxiv.org/abs/2402.16288v1</a></p>
<p><b>Compressor summary</b>: PerLTQA is a dataset for question answering that incorporates personalized memories, focusing on social interactions and events, to enhance dialogues using a novel framework for memory classification, retrieval, and synthesis.</p><hr><h3>Few-Shot Learning for Annotation-Efficient Nucleus Instance Segmentation</h3>
<p><a href='http://arxiv.org/abs/2402.16280v1'>http://arxiv.org/abs/2402.16280v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a meta-learning based framework for nucleus instance segmentation using few-shot learning and structural guidance, which achieves high performance with minimal annotations.</p><hr><h3>A Self-matching Training Method with Annotation Embedding Models for  Ontology Subsumption Prediction</h3>
<p><a href='http://arxiv.org/abs/2402.16278v1'>http://arxiv.org/abs/2402.16278v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-matching training method to improve concept subsumption prediction in ontologies using InME and CoME embeddings, which capture global and local information in annotation axioms.</p><hr><h3>From Large Language Models and Optimization to Decision Optimization  CoPilot: A Research Manifesto</h3>
<p><a href='http://arxiv.org/abs/2402.16269v1'>http://arxiv.org/abs/2402.16269v1</a></p>
<p><b>Compressor summary</b>: The paper proposes DOCP, an AI tool that uses LLMs to help users create and solve optimization models for business problems in natural language.</p><hr><h3>Foundation Model Transparency Reports</h3>
<p><a href='http://arxiv.org/abs/2402.16268v1'>http://arxiv.org/abs/2402.16268v1</a></p>
<p><b>Compressor summary</b>: The authors propose Foundation Model Transparency Reports based on 6 design principles and 100 indicators to ensure responsible development and deployment of AI models.</p><hr><h3>Infrared and visible Image Fusion with Language-driven Loss in CLIP  Embedding Space</h3>
<p><a href='http://arxiv.org/abs/2402.16267v1'>http://arxiv.org/abs/2402.16267v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using natural language to express the objective of infrared-visible image fusion, improving performance by encoding texts into a multi-modal embedding space and constructing a language-driven fusion model with a supervised loss function.</p><hr><h3>UniRetriever: Multi-task Candidates Selection for Various  Context-Adaptive Conversational Retrieval</h3>
<p><a href='http://arxiv.org/abs/2402.16261v1'>http://arxiv.org/abs/2402.16261v1</a></p>
<p><b>Compressor summary</b>: The text proposes a multi-task framework with a dual-encoder architecture to act as a universal retriever for persona, knowledge, and response selection in conversational retrieval systems, improving efficiency and performance.</p><hr><h3>SeqTrack3D: Exploring Sequence Information for Robust 3D Point Cloud  Tracking</h3>
<p><a href='http://arxiv.org/abs/2402.16249v1'>http://arxiv.org/abs/2402.16249v1</a></p>
<p><b>Compressor summary</b>: SeqTrack3D is a novel Sequence-to-Sequence tracker that combines point clouds and bounding boxes to improve 3D single object tracking performance, especially in scenes with sparse points.</p><hr><h3>Topic-to-essay generation with knowledge-based content selection</h3>
<p><a href='http://arxiv.org/abs/2402.16248v1'>http://arxiv.org/abs/2402.16248v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel copy mechanism model for generating diverse and coherent paragraphs from given topics, and introduces an improved prefix tuning method and a new Chinese dataset for this task.</p><hr><h3>Learning Translations: Emergent Communication Pretraining for  Cooperative Language Acquisition</h3>
<p><a href='http://arxiv.org/abs/2402.16247v1'>http://arxiv.org/abs/2402.16247v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new AI challenge called CLAP, where a 'joiner' agent learns communication strategies by imitating or translating existing interactions in a target community.</p><hr><h3>Real-Time Vehicle Detection and Urban Traffic Behavior Analysis Based on  UAV Traffic Videos on Mobile Devices</h3>
<p><a href='http://arxiv.org/abs/2402.16246v1'>http://arxiv.org/abs/2402.16246v1</a></p>
<p><b>Compressor summary</b>: The paper presents a system that uses drones and deep learning to collect and analyze urban traffic data in real-time on mobile devices.</p><hr><h3>HSONet:A Siamese foreground association-driven hard case sample  optimization network for high-resolution remote sensing image change  detection</h3>
<p><a href='http://arxiv.org/abs/2402.16242v1'>http://arxiv.org/abs/2402.16242v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new method (HSONet) to improve change detection models by addressing imbalance and missingness issues in learning hard cases using equilibrium optimization and scene context.</p><hr><h3>Active Level Set Estimation for Continuous Search Space with Theoretical  Guarantee</h3>
<p><a href='http://arxiv.org/abs/2402.16237v1'>http://arxiv.org/abs/2402.16237v1</a></p>
<p><b>Compressor summary</b>: Our novel algorithm finds level sets in continuous search spaces without discretization, using a confidence-based acquisition function and achieving theoretical convergence and superior performance on various datasets.</p><hr><h3>GARNN: An Interpretable Graph Attentive Recurrent Neural Network for  Predicting Blood Glucose Levels via Multivariate Time Series</h3>
<p><a href='http://arxiv.org/abs/2402.16230v1'>http://arxiv.org/abs/2402.16230v1</a></p>
<p><b>Compressor summary</b>: GARNNs are interpretable graph attentive recurrent neural networks that predict future blood glucose levels in diabetics using sensor and self-reported data, while explaining variable importance and generating feature maps, outperforming existing methods in accuracy and interpretability.</p>