
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-03-07</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-03-07 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Stop Regressing: Training Value Functions via Classification for  Scalable Deep RL</h3>
<p><a href='http://arxiv.org/abs/2403.03950v1'>http://arxiv.org/abs/2403.03950v1</a></p>
<p><b>Compressor summary</b>: The paper explores using categorical cross-entropy for training value functions in deep reinforcement learning, improving performance and scalability in various domains.</p><hr><h3>The Heuristic Core: Understanding Subnetwork Generalization in  Pretrained Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.03942v1'>http://arxiv.org/abs/2403.03942v1</a></p>
<p><b>Compressor summary</b>: The study reveals that different subnetworks within a single language model can achieve similar performance in-domain but generalize differently, and this is related to the use of attention heads that compute shallow and non-generalizing features versus higher-level ones.</p><hr><h3>GUIDE: Guidance-based Incremental Learning with Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2403.03938v1'>http://arxiv.org/abs/2403.03938v1</a></p>
<p><b>Compressor summary</b>: GUIDE is a new method for continuous learning that uses diffusion models and classifier guidance to generate rehearsal examples for forgotten information, reducing catastrophic forgetting and outperforming previous approaches.</p><hr><h3>Extreme Precipitation Nowcasting using Transformer-based Generative  Models</h3>
<p><a href='http://arxiv.org/abs/2403.03929v1'>http://arxiv.org/abs/2403.03929v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for short-term precipitation prediction using Transformer models and shows its effectiveness in capturing extreme weather events.</p><hr><h3>Did Translation Models Get More Robust Without Anyone Even Noticing?</h3>
<p><a href='http://arxiv.org/abs/2403.03923v1'>http://arxiv.org/abs/2403.03923v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how neural machine translation and large language models handle noisy inputs and shows they are more robust than previous models.</p><hr><h3>Enhancing Instructional Quality: Leveraging Computer-Assisted Textual  Analysis to Generate In-Depth Insights from Educational Artifacts</h3>
<p><a href='http://arxiv.org/abs/2403.03920v1'>http://arxiv.org/abs/2403.03920v1</a></p>
<p><b>Compressor summary</b>: The paper examines how AI and ML methods can enhance instructional quality by analyzing educational content, teacher discourse, and student responses using Elmore's Instructional Core Framework.</p><hr><h3>A Measure for Transparent Comparison of Linguistic Diversity in  Multilingual NLP Data Sets</h3>
<p><a href='http://arxiv.org/abs/2403.03909v1'>http://arxiv.org/abs/2403.03909v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to measure the linguistic diversity of multilingual NLP datasets by comparing them to a reference language sample using features extracted from typological databases and automatic text-based measures.</p><hr><h3>DART: Implicit Doppler Tomography for Radar Novel View Synthesis</h3>
<p><a href='http://arxiv.org/abs/2403.03896v1'>http://arxiv.org/abs/2403.03896v1</a></p>
<p><b>Compressor summary</b>: DART is a neural method that uses radar physics to create realistic radar images for simulation and design.</p><hr><h3>IRCoder: Intermediate Representations Make Language Models Robust  Multilingual Code Generators</h3>
<p><a href='http://arxiv.org/abs/2403.03894v1'>http://arxiv.org/abs/2403.03894v1</a></p>
<p><b>Compressor summary</b>: The authors introduce SLTrans, a large dataset with source code files and compiler intermediate representations, to improve Code-LMs' multilingual capabilities for code generation tasks.</p><hr><h3>From One to Many: Expanding the Scope of Toxicity Mitigation in Language  Models</h3>
<p><a href='http://arxiv.org/abs/2403.03893v1'>http://arxiv.org/abs/2403.03893v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to reduce toxicity across multiple languages using various techniques, and evaluates its effectiveness on nine languages with different resources.</p><hr><h3>FaaF: Facts as a Function for the evaluation of RAG systems</h3>
<p><a href='http://arxiv.org/abs/2403.03888v1'>http://arxiv.org/abs/2403.03888v1</a></p>
<p><b>Compressor summary</b>: Facts as a Function (FaaF) is a new method for evaluating RAG systems that uses function calling abilities of LMs to improve fact verification efficiency and reliability.</p><hr><h3>SaulLM-7B: A pioneering Large Language Model for Law</h3>
<p><a href='http://arxiv.org/abs/2403.03883v1'>http://arxiv.org/abs/2403.03883v1</a></p>
<p><b>Compressor summary</b>: SaulLM-7B, a 7 billion parameter language model, excels at legal text comprehension and generation after being trained on a large English legal corpus.</p><hr><h3>Self and Mixed Supervision to Improve Training Labels for Multi-Class  Medical Image Segmentation</h3>
<p><a href='http://arxiv.org/abs/2403.03882v1'>http://arxiv.org/abs/2403.03882v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a dual-branch network and transfer learning method to automatically improve weak training labels for multi-class medical image segmentation, achieving significant improvements in accuracy on abdominal CT scans.</p><hr><h3>Latent Dataset Distillation with Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2403.03881v1'>http://arxiv.org/abs/2403.03881v1</a></p>
<p><b>Compressor summary</b>: LD3M is a new method that uses diffusion in latent space to generate high-quality synthetic images from small datasets for machine learning models.</p><hr><h3>Graph neural network outputs are almost surely asymptotically constant</h3>
<p><a href='http://arxiv.org/abs/2403.03880v1'>http://arxiv.org/abs/2403.03880v1</a></p>
<p><b>Compressor summary</b>: The predictions of GNNs for probabilistic classification tasks become constant as the graph size increases, limiting their expressive power.</p><hr><h3>Redefining cystoscopy with ai: bladder cancer diagnosis using an  efficient hybrid cnn-transformer model</h3>
<p><a href='http://arxiv.org/abs/2403.03879v1'>http://arxiv.org/abs/2403.03879v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a deep learning approach using CNNs and a transformer to detect and segment bladder cancer in cystoscopic images, improving accuracy and efficiency for diagnosis.</p><hr><h3>Impoverished Language Technology: The Lack of (Social) Class in NLP</h3>
<p><a href='http://arxiv.org/abs/2403.03874v1'>http://arxiv.org/abs/2403.03874v1</a></p>
<p><b>Compressor summary</b>: The authors survey NLP literature and find little attention to socio-economic class, which they propose to include in future language technologies.</p><hr><h3>Learning to Decode Collaboratively with Multiple Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.03870v1'>http://arxiv.org/abs/2403.03870v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to teach large language models to collaborate by interleaving their generations and optimizing the marginal likelihood, leading to better performance on various tasks and interesting collaboration patterns.</p><hr><h3>On the Origins of Linear Representations in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.03867v1'>http://arxiv.org/abs/2403.03867v1</a></p>
<p><b>Compressor summary</b>: This paper investigates how large language models learn linear representations of semantic concepts and shows that a simple latent variable model can explain this phenomenon.</p><hr><h3>KIWI: A Dataset of Knowledge-Intensive Writing Instructions for  Answering Research Questions</h3>
<p><a href='http://arxiv.org/abs/2403.03866v1'>http://arxiv.org/abs/2403.03866v1</a></p>
<p><b>Compressor summary</b>: The paper introduces KIWI, a dataset for evaluating large language models' ability to follow instructions and provide writing assistance in the scientific domain, finding that current models struggle with incorporating new information and judging their own performance.</p><hr><h3>Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious  Challenges in Multimodal Reasoning</h3>
<p><a href='http://arxiv.org/abs/2403.03864v1'>http://arxiv.org/abs/2403.03864v1</a></p>
<p><b>Compressor summary</b>: The paper presents AlgoPuzzleVQA, a new dataset for multimodal puzzle-solving tasks that require visual understanding, language comprehension, and complex algorithmic reasoning, and shows that large language models struggle with these tasks.</p><hr><h3>X-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot  Learning Simultaneously in Classification</h3>
<p><a href='http://arxiv.org/abs/2403.03863v1'>http://arxiv.org/abs/2403.03863v1</a></p>
<p><b>Compressor summary</b>: X-shot learning is a new challenge that involves adapting to different levels of label occurrences in real-world settings, and BinBin is a versatile system that outperforms previous methods on this task.</p><hr><h3>Designing Informative Metrics for Few-Shot Example Selection</h3>
<p><a href='http://arxiv.org/abs/2403.03861v1'>http://arxiv.org/abs/2403.03861v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to select complex examples for few-shot sequence tagging tasks, improving the performance of pretrained language models significantly.</p><hr><h3>Emojinize : Enriching Any Text with Emoji Translations</h3>
<p><a href='http://arxiv.org/abs/2403.03857v1'>http://arxiv.org/abs/2403.03857v1</a></p>
<p><b>Compressor summary</b>: Emojinize is a method that uses large language models to translate text phrases into sequences of one or more emoji, increasing guessability and enabling various applications.</p><hr><h3>Public-data Assisted Private Stochastic Optimization: Power and  Limitations</h3>
<p><a href='http://arxiv.org/abs/2403.03856v1'>http://arxiv.org/abs/2403.03856v1</a></p>
<p><b>Compressor summary</b>: The paper studies how well private algorithms can work using public data, focusing on optimization problems and showing the limits and optimal strategies for different settings.</p><hr><h3>ECAP: Extensive Cut-and-Paste Augmentation for Unsupervised Domain  Adaptive Semantic Segmentation</h3>
<p><a href='http://arxiv.org/abs/2403.03854v1'>http://arxiv.org/abs/2403.03854v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve unsupervised domain adaptation for semantic segmentation by using confident pseudo-labels from target data as data augmentation.</p><hr><h3>ShortGPT: Layers in Large Language Models are More Redundant Than You  Expect</h3>
<p><a href='http://arxiv.org/abs/2403.03853v1'>http://arxiv.org/abs/2403.03853v1</a></p>
<p><b>Compressor summary</b>: The study introduces Block Influence, a metric to measure layer significance in large language models, and proposes ShortGPT, a method that removes redundant layers based on their scores, achieving better results than previous methods.</p><hr><h3>Accelerating Convergence of Score-Based Diffusion Models, Provably</h3>
<p><a href='http://arxiv.org/abs/2403.03852v1'>http://arxiv.org/abs/2403.03852v1</a></p>
<p><b>Compressor summary</b>: The paper proposes novel training-free algorithms to accelerate diffusion generative models' samplers, achieving faster convergence rates than existing methods.</p><hr><h3>On the Effectiveness of Distillation in Mitigating Backdoors in  Pre-trained Encoder</h3>
<p><a href='http://arxiv.org/abs/2403.03846v1'>http://arxiv.org/abs/2403.03846v1</a></p>
<p><b>Compressor summary</b>: The paper explores using distillation to defend against poisoned encoders in SSL by transferring benign knowledge from a teacher net to a student net, achieving significant reduction in attack success rate with minimal accuracy loss.</p><hr><h3>Feature Selection as Deep Sequential Generative Learning</h3>
<p><a href='http://arxiv.org/abs/2403.03838v1'>http://arxiv.org/abs/2403.03838v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new method for feature selection that uses a deep variational transformer model to generate feature selection decision sequences based on learned utility scores.</p><hr><h3>Cobweb: An Incremental and Hierarchical Model of Human-Like Category  Learning</h3>
<p><a href='http://arxiv.org/abs/2403.03835v1'>http://arxiv.org/abs/2403.03835v1</a></p>
<p><b>Compressor summary</b>: Cobweb is a human-like categorization system that builds hierarchical structures using utility, captures psychological effects, and can exhibit both exemplar and prototype learning.</p><hr><h3>Your device may know you better than you know yourself -- continuous  authentication on novel dataset using machine learning</h3>
<p><a href='http://arxiv.org/abs/2403.03832v1'>http://arxiv.org/abs/2403.03832v1</a></p>
<p><b>Compressor summary</b>: The text is about a study on continuous authentication using gesture data from Minecraft players and machine learning models, with the most accurate model achieving 90% accuracy.</p><hr><h3>From Clicks to Security: Investigating Continuous Authentication via  Mouse Dynamics</h3>
<p><a href='http://arxiv.org/abs/2403.03828v1'>http://arxiv.org/abs/2403.03828v1</a></p>
<p><b>Compressor summary</b>: The paper explores how mouse movement patterns can be used as a reliable and efficient method for continuous user authentication in gaming scenarios, using various machine learning models.</p><hr><h3>Temporal Enhanced Floating Car Observers</h3>
<p><a href='http://arxiv.org/abs/2403.03825v1'>http://arxiv.org/abs/2403.03825v1</a></p>
<p><b>Compressor summary</b>: The text describes how sensor-equipped vehicles can collect traffic data efficiently by emulating detection in a microscopic traffic simulation and using deep learning to recover hidden vehicles, improving traffic management.</p><hr><h3>A Modular Approach for Multimodal Summarization of TV Shows</h3>
<p><a href='http://arxiv.org/abs/2403.03823v1'>http://arxiv.org/abs/2403.03823v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a modular approach for summarizing TV shows using scene detection, reordering, visual-to-text conversion, dialogue summarization, and fusion, and introduces PREFS, a new metric that evaluates summary quality based on fact recall and precision.</p><hr><h3>Evaluating the Elementary Multilingual Capabilities of Large Language  Models with MultiQ</h3>
<p><a href='http://arxiv.org/abs/2403.03814v1'>http://arxiv.org/abs/2403.03814v1</a></p>
<p><b>Compressor summary</b>: MultiQ is a benchmark to evaluate multilingual question answering by open LLMs beyond their intended languages.</p><hr><h3>ProbSAINT: Probabilistic Tabular Regression for Used Car Pricing</h3>
<p><a href='http://arxiv.org/abs/2403.03812v1'>http://arxiv.org/abs/2403.03812v1</a></p>
<p><b>Compressor summary</b>: ProbSAINT is a machine learning model for used car pricing that can quantify uncertainties and adapt to different expected offer durations, providing accurate and fair transactions.</p><hr><h3>KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing  Patient Data with Knowledge Graphs</h3>
<p><a href='http://arxiv.org/abs/2403.03791v1'>http://arxiv.org/abs/2403.03791v1</a></p>
<p><b>Compressor summary</b>: KG-TREAT is a novel framework that uses biomedical knowledge graphs to enhance the estimation of treatment effects on patient outcomes, showing improved performance over existing methods.</p><hr><h3>Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection  from Remote Sensing Imagery</h3>
<p><a href='http://arxiv.org/abs/2403.03790v1'>http://arxiv.org/abs/2403.03790v1</a></p>
<p><b>Compressor summary</b>: The article proposes a novel unified visual-language model called Popeye for multi-source ship detection from remote sensing imagery using various methods, knowledge adaption, and pixel-level segmentation.</p><hr><h3>PPTC-R benchmark: Towards Evaluating the Robustness of Large Language  Models for PowerPoint Task Completion</h3>
<p><a href='http://arxiv.org/abs/2403.03788v1'>http://arxiv.org/abs/2403.03788v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a benchmark (PPTC-R) to evaluate how well Large Language Models (LLMs) can complete PowerPoint tasks in different real-world situations, and finds that GPT-4 performs best but all LLMs struggle with multiple challenges.</p><hr><h3>ENOT: Expectile Regularization for Fast and Accurate Training of Neural  Optimal Transport</h3>
<p><a href='http://arxiv.org/abs/2403.03777v1'>http://arxiv.org/abs/2403.03777v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Expectile-Regularised Neural Optimal Transport (ENOT), a new method for estimating optimal transportation plans that improves both accuracy and efficiency compared to previous approaches.</p><hr><h3>Verified Training for Counterfactual Explanation Robustness under Data  Shift</h3>
<p><a href='http://arxiv.org/abs/2403.03773v1'>http://arxiv.org/abs/2403.03773v1</a></p>
<p><b>Compressor summary</b>: VeriTraCER generates counterfactual explanations that are robust to small model updates, providing users with reliable guidance on how to change their inputs to achieve desired predictions.</p><hr><h3>AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs</h3>
<p><a href='http://arxiv.org/abs/2403.03772v1'>http://arxiv.org/abs/2403.03772v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method to efficiently parallelize existing causal discovery methods, enabling their application on large-scale datasets by significantly speeding up the process.</p><hr><h3>DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models</h3>
<p><a href='http://arxiv.org/abs/2403.03768v1'>http://arxiv.org/abs/2403.03768v1</a></p>
<p><b>Compressor summary</b>: DeepCRE is a novel computational model that significantly improves cross-drug response evaluation and can help discover new therapeutics.</p><hr><h3>German also Hallucinates! Inconsistency Detection in News Summaries with  the Absinth Dataset</h3>
<p><a href='http://arxiv.org/abs/2403.03750v1'>http://arxiv.org/abs/2403.03750v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new dataset for detecting hallucinations in German summarization tasks using large language models, which can help improve their faithfulness to the source text.</p><hr><h3>Towards Safe and Aligned Large Language Models for Medicine</h3>
<p><a href='http://arxiv.org/abs/2403.03744v1'>http://arxiv.org/abs/2403.03744v1</a></p>
<p><b>Compressor summary</b>: This paper evaluates the safety and alignment of medical large language models (LLMs) using a dataset of harmful questions and suggests fine-tuning as a mitigation strategy to reduce potential harms.</p><hr><h3>SUPClust: Active Learning at the Boundaries</h3>
<p><a href='http://arxiv.org/abs/2403.03741v1'>http://arxiv.org/abs/2403.03741v1</a></p>
<p><b>Compressor summary</b>: SUPClust is an active learning method that focuses on finding points at the decision boundary between classes to improve model performance, especially in imbalanced datasets.</p><hr><h3>Self-supervised Photographic Image Layout Representation Learning</h3>
<p><a href='http://arxiv.org/abs/2403.03740v1'>http://arxiv.org/abs/2403.03740v1</a></p>
<p><b>Compressor summary</b>: Our method learns to represent photographic image layouts using heterogeneous graphs and autoencoders, outperforming existing approaches with a new dataset, LODB.</p><hr><h3>A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network</h3>
<p><a href='http://arxiv.org/abs/2403.03739v1'>http://arxiv.org/abs/2403.03739v1</a></p>
<p><b>Compressor summary</b>: The paper proposes A&B BNN, which reduces binary neural network's multiplication operations by replacing them with bit operations and achieves competitive results on image classification tasks.</p><hr><h3>Probabilistic Topic Modelling with Transformer Representations</h3>
<p><a href='http://arxiv.org/abs/2403.03737v1'>http://arxiv.org/abs/2403.03737v1</a></p>
<p><b>Compressor summary</b>: The TNTM model combines transformer embeddings and probabilistic modelling to achieve high-quality topic representation with fast inference and good diversity.</p><hr><h3>Unifying Generation and Compression: Ultra-low bitrate Image Coding Via  Multi-stage Transformer</h3>
<p><a href='http://arxiv.org/abs/2403.03736v1'>http://arxiv.org/abs/2403.03736v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel image generation-compression paradigm that uses vector-quantized image models and a multi-stage transformer to improve perceptual quality, especially in ultra-low bitrate scenarios.</p><hr><h3>Learning 3D object-centric representation through prediction</h3>
<p><a href='http://arxiv.org/abs/2403.03730v1'>http://arxiv.org/abs/2403.03730v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new network architecture that learns to segment, localize, and perceive depth of objects from images and self-motion, mimicking human infants' abilities.</p><hr><h3>Bridging Diversity and Uncertainty in Active learning with  Self-Supervised Pre-Training</h3>
<p><a href='http://arxiv.org/abs/2403.03728v1'>http://arxiv.org/abs/2403.03728v1</a></p>
<p><b>Compressor summary</b>: The study proposes a heuristic called TCM that combines diversity-based and uncertainty-based sampling strategies for active learning, improving performance across different data levels.</p><hr><h3>Diffusion on language model embeddings for protein sequence generation</h3>
<p><a href='http://arxiv.org/abs/2403.03726v1'>http://arxiv.org/abs/2403.03726v1</a></p>
<p><b>Compressor summary</b>: DiMA is a model that generates diverse and accurate protein sequences using continuous diffusion on embeddings derived from the ESM-2 protein language model, outperforming existing methods in quality and diversity metrics.</p><hr><h3>CMDA: Cross-Modal and Domain Adversarial Adaptation for LiDAR-Based 3D  Object Detection</h3>
<p><a href='http://arxiv.org/abs/2403.03721v1'>http://arxiv.org/abs/2403.03721v1</a></p>
<p><b>Compressor summary</b>: CMDA is a novel unsupervised domain adaptation method that uses visual semantic cues from camera images to improve generalization of 3D object detection models across different domains.</p><hr><h3>Multimodal Transformer for Comics Text-Cloze</h3>
<p><a href='http://arxiv.org/abs/2403.03719v1'>http://arxiv.org/abs/2403.03719v1</a></p>
<p><b>Compressor summary</b>: The paper presents a Multimodal-LLM architecture for Text-cloze, a challenging task in comics, which improves performance by using a Domain-Adapted ResNet-50 visual encoder and new OCR annotations.</p><hr><h3>MeaCap: Memory-Augmented Zero-shot Image Captioning</h3>
<p><a href='http://arxiv.org/abs/2403.03715v1'>http://arxiv.org/abs/2403.03715v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel memory-augmented method for zero-shot image captioning that generates concept-centered captions with fewer hallucinations and more world-knowledge, outperforming existing methods.</p><hr><h3>Multi-Grained Cross-modal Alignment for Learning Open-vocabulary  Semantic Segmentation from Text Supervision</h3>
<p><a href='http://arxiv.org/abs/2403.03707v1'>http://arxiv.org/abs/2403.03707v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework that learns pixel-level alignment between images and text to improve semantic segmentation without dense annotations.</p><hr><h3>Causal Prototype-inspired Contrast Adaptation for Unsupervised Domain  Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery</h3>
<p><a href='http://arxiv.org/abs/2403.03704v1'>http://arxiv.org/abs/2403.03704v1</a></p>
<p><b>Compressor summary</b>: CPCA is a novel method for semantic segmentation of remote sensing imagery across domains, which disentangles causal features from bias features and adapts to invariant causal factors using intervention techniques.</p><hr><h3>Towards Controllable Time Series Generation</h3>
<p><a href='http://arxiv.org/abs/2403.03698v1'>http://arxiv.org/abs/2403.03698v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CTS, a framework that generates controllable time series by decoupling the mapping process from VAE training and evaluates its effectiveness on three real-world datasets.</p><hr><h3>MolNexTR: A Generalized Deep Learning Model for Molecular Image  Recognition</h3>
<p><a href='http://arxiv.org/abs/2403.03691v1'>http://arxiv.org/abs/2403.03691v1</a></p>
<p><b>Compressor summary</b>: MolNexTR is a deep learning model that converts molecular images into graph structures and SMILES strings by fusing ConvNext and Vision-Transformer, using advanced algorithms for data augmentation and post-processing to handle diverse image styles.</p><hr><h3>Rapidly Developing High-quality Instruction Data and Evaluation  Benchmark for Large Language Models with Minimal Human Effort: A Case Study  on Japanese</h3>
<p><a href='http://arxiv.org/abs/2403.03690v1'>http://arxiv.org/abs/2403.03690v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Human annotation is costly for creating instruction data and evaluation benchmarks for large language models in non-English languages like Japanese.
- The authors propose a self-instruct method based on GPT-4 that uses English instructions translated and edited into Japanese as demonstrations to generate Japanese instruction data.
- They also construct an evaluation benchmark using GPT-4 to assess the response quality of LLMs without human references, which shows their models outperformed existing ones.

Summary:
The authors use GPT-4 to create instruction data and evaluation benchmarks for large language models in Japanese with minimal human annotation, achieving better results than previous methods.</p><hr><h3>General2Specialized LLMs Translation for E-commerce</h3>
<p><a href='http://arxiv.org/abs/2403.03689v1'>http://arxiv.org/abs/2403.03689v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a two-step fine-tuning method to improve Neural Machine Translation for domains with special writing formulas like e-commerce, using domain-specific resources and self-contrastive semantic enhancement.</p><hr><h3>Adversarial Infrared Geometry: Using Geometry to Perform Adversarial  Attack against Infrared Pedestrian Detectors</h3>
<p><a href='http://arxiv.org/abs/2403.03674v1'>http://arxiv.org/abs/2403.03674v1</a></p>
<p><b>Compressor summary</b>: The study proposes AdvIG, a novel infrared physical attack that uses geometric shapes and optimizes their parameters to execute efficient black-box query attacks with high success rates.</p><hr><h3>Learning Adversarial MDPs with Stochastic Hard Constraints</h3>
<p><a href='http://arxiv.org/abs/2403.03672v1'>http://arxiv.org/abs/2403.03672v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Study online learning problems in CMDPs with adversarial losses and hard constraints
- Design two algorithms: one for general CMDPs with sublinear regret and constraint violation, and one for CMDPs with known policy that satisfies constraints with high probability and has sublinear regret
- First work to consider both adversarial losses and hard constraints in CMDPs
- Algorithms can handle non-stationary environments and stricter requirements than existing ones
- Applicable to real-world scenarios like autonomous driving, online advertising, and recommender systems

Summary: The paper proposes two novel algorithms for online learning in CMDPs with adversarial losses and hard constraints, achieving sublinear regret and satisfying constraints in non-stationary environments, and applies them to various real-world problems.</p><hr><h3>Portraying the Need for Temporal Data in Flood Detection via Sentinel-1</h3>
<p><a href='http://arxiv.org/abs/2403.03671v1'>http://arxiv.org/abs/2403.03671v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new method to detect floods in remote sensing data using temporal anomaly detection and show promising results.</p><hr><h3>CDC: A Simple Framework for Complex Data Clustering</h3>
<p><a href='http://arxiv.org/abs/2403.03670v1'>http://arxiv.org/abs/2403.03670v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a simple framework for clustering complex data with linear complexity by using graph filtering and similarity-preserving regularization.</p><hr><h3>Provable Filter for Real-world Graph Clustering</h3>
<p><a href='http://arxiv.org/abs/2403.03666v1'>http://arxiv.org/abs/2403.03666v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for graph clustering that handles both homophilic and heterophilic graphs, outperforming existing methods in experiments.</p><hr><h3>Harnessing Meta-Learning for Improving Full-Frame Video Stabilization</h3>
<p><a href='http://arxiv.org/abs/2403.03662v1'>http://arxiv.org/abs/2403.03662v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a test-time adaptation method that improves pixel-level synthesis solutions for video stabilization by adapting models to individual input videos, leading to significant stability and quality gains with only one adaptation step.</p><hr><h3>Robust Graph Structure Learning under Heterophily</h3>
<p><a href='http://arxiv.org/abs/2403.03659v1'>http://arxiv.org/abs/2403.03659v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Graph is important for learning tasks but often noisy and sparse
- Most methods assume homophilic graphs, ignoring heterophily
- Proposed a novel robust graph structure learning method for heterophilic data
- Method uses high-pass filter, adaptive norm, and regularizer to refine graph structure

Summary: The paper proposes a new method to learn robust graphs from noisy and sparse heterophilic data by using a high-pass filter, an adaptive norm, and a regularizer.</p><hr><h3>K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation  Learning in Multivariate Time-Series Data</h3>
<p><a href='http://arxiv.org/abs/2403.03645v1'>http://arxiv.org/abs/2403.03645v1</a></p>
<p><b>Compressor summary</b>: K-Link uses large language models to create a knowledge-link graph that improves graph construction from multivariate time-series data, enhancing graph neural network performance on various tasks.</p><hr><h3>A Survey on Applications of Reinforcement Learning in Spatial Resource  Allocation</h3>
<p><a href='http://arxiv.org/abs/2403.03643v1'>http://arxiv.org/abs/2403.03643v1</a></p>
<p><b>Compressor summary</b>: This paper reviews recent reinforcement learning methods for spatial resource allocation problems, discussing their advantages, challenges, and open questions.</p><hr><h3>Apollo: Lightweight Multilingual Medical LLMs towards Democratizing  Medical AI to 6B People</h3>
<p><a href='http://arxiv.org/abs/2403.03640v1'>http://arxiv.org/abs/2403.03640v1</a></p>
<p><b>Compressor summary</b>: The authors develop multilingual medical AI models that can provide tailored healthcare services in various languages, using the ApolloCorpora dataset and achieving state-of-the-art performance.</p><hr><h3>SheetAgent: A Generalist Agent for Spreadsheet Reasoning and  Manipulation via Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.03636v1'>http://arxiv.org/abs/2403.03636v1</a></p>
<p><b>Compressor summary</b>: SheetAgent is a novel autonomous agent that uses a large language model to perform long-horizon and multi-category spreadsheet manipulation tasks with reasoning, achieving improved precision and table reasoning abilities.</p><hr><h3>Tackling Missing Values in Probabilistic Wind Power Forecasting: A  Generative Approach</h3>
<p><a href='http://arxiv.org/abs/2403.03631v1'>http://arxiv.org/abs/2403.03631v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an efficient probabilistic forecasting method for wind power data with missing values by using a generative model that estimates joint distributions without preprocessing.</p><hr><h3>GPTopic: Dynamic and Interactive Topic Representations</h3>
<p><a href='http://arxiv.org/abs/2403.03628v1'>http://arxiv.org/abs/2403.03628v1</a></p>
<p><b>Compressor summary</b>: GPTopic is a software package that uses large language models to create interactive, dynamic topic representations for text corpora, making topic modeling more accessible and comprehensive.</p><hr><h3>Multimodal Large Language Models to Support Real-World Fact-Checking</h3>
<p><a href='http://arxiv.org/abs/2403.03627v1'>http://arxiv.org/abs/2403.03627v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to assess how well multimodal language models can support fact-checking and find that GPT-4V performs better than existing models, while also identifying their limitations and biases.</p><hr><h3>GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D  Scene Understanding</h3>
<p><a href='http://arxiv.org/abs/2403.03608v1'>http://arxiv.org/abs/2403.03608v1</a></p>
<p><b>Compressor summary</b>: GSNeRF is a method to generate images and semantic maps from unseen scenes by combining multi-view inputs, semantic features, and geometry information.</p><hr><h3>The Geometric Structure of Topic Models</h3>
<p><a href='http://arxiv.org/abs/2403.03607v1'>http://arxiv.org/abs/2403.03607v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to analyze and visualize topic models, which allows for higher-dimensional conceptual relationships between topics.</p><hr><h3>A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain  Recommendation</h3>
<p><a href='http://arxiv.org/abs/2403.03600v1'>http://arxiv.org/abs/2403.03600v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a privacy-preserving framework using multi-modal data to improve cross-domain recommendation accuracy by disentangling domain-common and domain-specific features.</p><hr><h3>Learning Invariant Representations of Graph Neural Networks via Cluster  Generalization</h3>
<p><a href='http://arxiv.org/abs/2403.03599v1'>http://arxiv.org/abs/2403.03599v1</a></p>
<p><b>Compressor summary</b>: The paper proposes CIT, a mechanism that improves GNNs' generalization by transferring cluster information and preserving node diversity when the test graph structure differs from the training one.</p><hr><h3>Assessing the Aesthetic Evaluation Capabilities of GPT-4 with Vision:  Insights from Group and Individual Assessments</h3>
<p><a href='http://arxiv.org/abs/2403.03594v1'>http://arxiv.org/abs/2403.03594v1</a></p>
<p><b>Compressor summary</b>: This study shows how GPT-4 with Vision can predict human aesthetic evaluations of images better than other models and suggests creating an AI system based on scientific knowledge of beauty perception.</p><hr><h3>RouteExplainer: An Explanation Framework for Vehicle Routing Problem</h3>
<p><a href='http://arxiv.org/abs/2403.03585v1'>http://arxiv.org/abs/2403.03585v1</a></p>
<p><b>Compressor summary</b>: RouteExplainer is a framework that provides post-hoc explanations for vehicle routing problems by classifying edges based on their intentions and using large language models to generate explanation texts.</p><hr><h3>Design of an Open-Source Architecture for Neural Machine Translation</h3>
<p><a href='http://arxiv.org/abs/2403.03582v1'>http://arxiv.org/abs/2403.03582v1</a></p>
<p><b>Compressor summary</b>: adaptNMT is an open-source tool for easy development and deployment of neural machine translation models with features like subword segmentation, intuitive UI, and eco-friendly evaluation.</p><hr><h3>Enhancing ASD detection accuracy: a combined approach of machine  learning and deep learning models with natural language processing</h3>
<p><a href='http://arxiv.org/abs/2403.03581v1'>http://arxiv.org/abs/2403.03581v1</a></p>
<p><b>Compressor summary</b>: The study used various AI models to analyze tweets and showed their potential in improving ASD diagnosis with high accuracy.</p><hr><h3>gaHealth: An English-Irish Bilingual Corpus of Health Data</h3>
<p><a href='http://arxiv.org/abs/2403.03575v1'>http://arxiv.org/abs/2403.03575v1</a></p>
<p><b>Compressor summary</b>: The gaHealth corpus, a bilingual dataset for English to Irish health translation, improved BLEU scores by 40% compared to the best performing models from the LoResMT2021 Shared Task, and provides linguistic guidelines for creating similar low-resource data sets.</p><hr><h3>On Transfer in Classification: How Well do Subsets of Classes  Generalize?</h3>
<p><a href='http://arxiv.org/abs/2403.03569v1'>http://arxiv.org/abs/2403.03569v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a theoretical framework for analyzing class transferability in machine learning using a partially ordered set of subsets of classes and explores its practical applications in few-shot learning.</p><hr><h3>Efficient Algorithms for Empirical Group Distributional Robust  Optimization and Beyond</h3>
<p><a href='http://arxiv.org/abs/2403.03562v1'>http://arxiv.org/abs/2403.03562v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm for group distributionally robust optimization with better performance and convergence guarantees than existing methods.</p><hr><h3>HMD-Poser: On-Device Real-time Human Motion Tracking from Scalable  Sparse Observations</h3>
<p><a href='http://arxiv.org/abs/2403.03561v1'>http://arxiv.org/abs/2403.03561v1</a></p>
<p><b>Compressor summary</b>: HMD-Poser is a novel approach for real-time human motion tracking using scalable sparse observations from a VR headset and body-worn inertial measurement units (IMUs), achieving state-of-the-art accuracy and performance.</p><hr><h3>Benchmarking Hallucination in Large Language Models based on  Unanswerable Math Word Problem</h3>
<p><a href='http://arxiv.org/abs/2403.03558v1'>http://arxiv.org/abs/2403.03558v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method for evaluating language models' reliability in answering math word problems using an unanswerable question dataset and shows that training with human feedback improves their performance.</p><hr><h3>Emotional Manipulation Through Prompt Engineering Amplifies  Disinformation Generation in AI Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.03550v1'>http://arxiv.org/abs/2403.03550v1</a></p>
<p><b>Compressor summary</b>: The study shows how OpenAI's LLMs can create fake news and respond to emotions, and suggests that they should be used responsibly to prevent misinformation.</p><hr><h3>Prompt Mining for Language-based Human Mobility Forecasting</h3>
<p><a href='http://arxiv.org/abs/2403.03544v1'>http://arxiv.org/abs/2403.03544v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new framework for designing diverse and effective prompts to improve language-based forecasting of human mobility patterns using large language models.</p><hr><h3>DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE  Pre-Training</h3>
<p><a href='http://arxiv.org/abs/2403.03542v1'>http://arxiv.org/abs/2403.03542v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new pre-training method and a scalable model architecture for neural operators in partial differential equations, achieving state-of-the-art results on various benchmarks and tasks.</p><hr><h3>Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and  Applications</h3>
<p><a href='http://arxiv.org/abs/2403.03535v1'>http://arxiv.org/abs/2403.03535v1</a></p>
<p><b>Compressor summary</b>: This paper introduces Task Attribute Distance (TAD), a model-agnostic metric to quantify the relationship between training and novel tasks in few-shot learning, and shows its effectiveness in applications like data augmentation and test-time intervention.</p><hr><h3>Extend Your Own Correspondences: Unsupervised Distant Point Cloud  Registration by Progressive Distance Extension</h3>
<p><a href='http://arxiv.org/abs/2403.03532v1'>http://arxiv.org/abs/2403.03532v1</a></p>
<p><b>Compressor summary</b>: EYOC is an unsupervised method for registering distant point clouds in driving scenarios without global pose labels, achieving comparable performance to supervised methods and better generalization.</p><hr><h3>BiVert: Bidirectional Vocabulary Evaluation using Relations for Machine  Translation</h3>
<p><a href='http://arxiv.org/abs/2403.03521v1'>http://arxiv.org/abs/2403.03521v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a bidirectional semantic-based evaluation method for neural machine translation that uses BabelNet to measure the sense distance between source and output sentences.</p><hr><h3>IB-Net: Initial Branch Network for Variable Decision in Boolean  Satisfiability</h3>
<p><a href='http://arxiv.org/abs/2403.03517v1'>http://arxiv.org/abs/2403.03517v1</a></p>
<p><b>Compressor summary</b>: IB-Net is a framework that uses graph neural networks to help solve Boolean Satisfiability problems, making Electronic Design Automation faster and more efficient.</p><hr><h3>Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling</h3>
<p><a href='http://arxiv.org/abs/2403.03516v1'>http://arxiv.org/abs/2403.03516v1</a></p>
<p><b>Compressor summary</b>: UMR is an unsupervised method to train multilingual dense retrievers using sequence likelihood estimation of multilingual language models, achieving better performance than supervised baselines.</p><hr><h3>CLongEval: A Chinese Benchmark for Evaluating Long-Context Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.03514v1'>http://arxiv.org/abs/2403.03514v1</a></p>
<p><b>Compressor summary</b>: CLongEval is a comprehensive Chinese benchmark for evaluating long-context language models with sufficient data volume, broad applicability, and high quality.</p><hr><h3>Dcl-Net: Dual Contrastive Learning Network for Semi-Supervised  Multi-Organ Segmentation</h3>
<p><a href='http://arxiv.org/abs/2403.03512v1'>http://arxiv.org/abs/2403.03512v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a two-stage network that uses global and local contrastive learning to improve multi-organ segmentation with semi-supervised learning, considering relations among images and categories.</p><hr><h3>Probing the Robustness of Time-series Forecasting Models with  CounterfacTS</h3>
<p><a href='http://arxiv.org/abs/2403.03508v1'>http://arxiv.org/abs/2403.03508v1</a></p>
<p><b>Compressor summary</b>: CounterfacTS is a tool that helps visualize and create counterfactuals for time-series forecasting models, enabling users to explore how changes in the data affect their performance and robustness.</p><hr><h3>GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</h3>
<p><a href='http://arxiv.org/abs/2403.03507v1'>http://arxiv.org/abs/2403.03507v1</a></p>
<p><b>Compressor summary</b>: GaLore is a training strategy for large language models that reduces memory usage while maintaining efficiency and performance in pre-training and fine-tuning stages.</p><hr><h3>Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid  Texts</h3>
<p><a href='http://arxiv.org/abs/2403.03506v1'>http://arxiv.org/abs/2403.03506v1</a></p>
<p><b>Compressor summary</b>: The study examines detecting AI-generated sentences in realistic human-AI collaboration texts and suggests using the CoAuthor dataset and considering segment length for better detection.</p><hr><h3>A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation</h3>
<p><a href='http://arxiv.org/abs/2403.03496v1'>http://arxiv.org/abs/2403.03496v1</a></p>
<p><b>Compressor summary</b>: The paper introduces multi-source Wizard of Wikipedia (Ms.WoW), a benchmark for evaluating dialogue systems that can select and use knowledge from multiple sources, and a challenge to test their ability to adapt to new sources.</p><hr><h3>VastTrack: Vast Category Visual Object Tracking</h3>
<p><a href='http://arxiv.org/abs/2403.03493v1'>http://arxiv.org/abs/2403.03493v1</a></p>
<p><b>Compressor summary</b>: The paper introduces VastTrack, a large-scale benchmark for visual tracking with diverse object categories, more videos, and rich linguistic annotations.</p><hr><h3>NoiseCollage: A Layout-Aware Text-to-Image Diffusion Model Based on  Noise Cropping and Merging</h3>
<p><a href='http://arxiv.org/abs/2403.03485v1'>http://arxiv.org/abs/2403.03485v1</a></p>
<p><b>Compressor summary</b>: NoiseCollage is a new text-to-image diffusion model that improves layout conditions by independently estimating and cropping noises for each object, outperforming several existing models and integrating with ControlNet to enhance edge, sketch, and pose skeleton control.</p><hr><h3>A Teacher-Free Graph Knowledge Distillation Framework with Dual  Self-Distillation</h3>
<p><a href='http://arxiv.org/abs/2403.03483v1'>http://arxiv.org/abs/2403.03483v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Teacher-Free Graph Self-Distillation (TGS), a method that improves the performance of MLPs on graph-related tasks without using GNNs or teachers, achieving fast inference and outperforming existing methods.</p><hr><h3>Magic Markup: Maintaining Document-External Markup with an LLM</h3>
<p><a href='http://arxiv.org/abs/2403.03481v1'>http://arxiv.org/abs/2403.03481v1</a></p>
<p><b>Compressor summary</b>: The paper presents a system that uses language models to automatically update annotations in changing text documents, enabling new applications in program writing and debugging.</p><hr><h3>Continual Segmentation with Disentangled Objectness Learning and Class  Recognition</h3>
<p><a href='http://arxiv.org/abs/2403.03477v1'>http://arxiv.org/abs/2403.03477v1</a></p>
<p><b>Compressor summary</b>: CoMasTRe is a two-stage segmentation method that combines objectness learning and classification, using distillation to improve performance and prevent forgetting on PASCAL VOC and ADE20K.</p><hr><h3>Inverse-Free Fast Natural Gradient Descent Method for Deep Learning</h3>
<p><a href='http://arxiv.org/abs/2403.03473v1'>http://arxiv.org/abs/2403.03473v1</a></p>
<p><b>Compressor summary</b>: The paper introduces FNGD, a fast natural gradient descent method that reduces computational complexity and achieves speedup in image classification and machine translation tasks.</p><hr><h3>Boosting Meta-Training with Base Class Information for Few-Shot Learning</h3>
<p><a href='http://arxiv.org/abs/2403.03472v1'>http://arxiv.org/abs/2403.03472v1</a></p>
<p><b>Compressor summary</b>: Our proposed end-to-end training method for few-shot learning combines cross entropy loss with meta-learning and improves performance significantly.</p><hr><h3>Multi-task Learning for Real-time Autonomous Driving Leveraging  Task-adaptive Attention Generator</h3>
<p><a href='http://arxiv.org/abs/2403.03468v1'>http://arxiv.org/abs/2403.03468v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new real-time multi-task network for autonomous driving that handles 3D object detection, semantic segmentation, and dense depth estimation using a task-adaptive attention generator to prevent negative transfer.</p><hr><h3>Self-Attention Empowered Graph Convolutional Network for Structure  Learning and Node Embedding</h3>
<p><a href='http://arxiv.org/abs/2403.03465v1'>http://arxiv.org/abs/2403.03465v1</a></p>
<p><b>Compressor summary</b>: The paper introduces GCN-SA, a graph neural network that uses self-attention to capture long-range dependencies and perform better representation learning on heterophilous graphs.</p><hr><h3>FLAME Diffuser: Grounded Wildfire Image Synthesis using Mask Guided  Diffusion</h3>
<p><a href='http://arxiv.org/abs/2403.03463v1'>http://arxiv.org/abs/2403.03463v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Machine learning benefits research fields but challenges remain for small/rare object detection
- The authors present a dataset automata using diffusion models to generate paired wildfire images with controlled flame position and size
- They vary the background of synthesized images by controlling text prompt and input image
- They use CLIP model to filter out low-quality images and preserve domain shift

Summary:
The authors propose a dataset automata that uses diffusion models and CLIP filtering to generate high-quality, paired wildfire images with controlled flame features and varying backgrounds for small/rare object detection tasks.</p><hr><h3>A Density-Guided Temporal Attention Transformer for Indiscernible Object  Counting in Underwater Video</h3>
<p><a href='http://arxiv.org/abs/2403.03461v1'>http://arxiv.org/abs/2403.03461v1</a></p>
<p><b>Compressor summary</b>: The paper introduces YoutubeFish-35, a new large-scale dataset for indiscernible object counting, and proposes TransVidCount, a method that combines density and regression branches to perform well on it.</p><hr><h3>Slot Abstractors: Toward Scalable Abstract Visual Reasoning</h3>
<p><a href='http://arxiv.org/abs/2403.03458v1'>http://arxiv.org/abs/2403.03458v1</a></p>
<p><b>Compressor summary</b>: Slot Abstractors is a scalable method for abstract visual reasoning with multi-object inputs and multiple relations, achieving state-of-the-art performance in four tasks.</p><hr><h3>DLP-GAN: Learning to Draw Modern Chinese Landscape Photos with  Generative Adversarial Network</h3>
<p><a href='http://arxiv.org/abs/2403.03456v1'>http://arxiv.org/abs/2403.03456v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes DLP-GAN, a novel framework for translating ancient Chinese landscape paintings into modern photos and sketches.
- The framework uses asymmetric cycle mapping, dense-fusion module, and dual-consistency loss to balance realism and abstraction.
- Experiments show that the model outperforms existing methods.

Summary:
The paper presents DLP-GAN, a new method for converting ancient Chinese landscape paintings into modern photos and sketches using a novel framework with various components to ensure quality.</p><hr><h3>Learning Constrained Optimization with Deep Augmented Lagrangian Methods</h3>
<p><a href='http://arxiv.org/abs/2403.03454v1'>http://arxiv.org/abs/2403.03454v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for learning to optimize constrained problems using dual solution estimates and improves convergence by incorporating augmented Lagrangian techniques.</p><hr><h3>D4C glove-train: solving the RPM and Bongard-logo problem by  distributing and Circumscribing concepts</h3>
<p><a href='http://arxiv.org/abs/2403.03452v1'>http://arxiv.org/abs/2403.03452v1</a></p>
<p><b>Compressor summary</b>: The paper proposes novel methods for abstract reasoning tasks like Raven's Progressive Matrices and Bongard-Logo problems by redefining concept boundaries and improving distribution estimation, leading to state-of-the-art performance.</p><hr><h3>Kernel Correlation-Dissimilarity for Multiple Kernel k-Means Clustering</h3>
<p><a href='http://arxiv.org/abs/2403.03448v1'>http://arxiv.org/abs/2403.03448v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method to improve kernel k-means clustering by integrating both kernel correlation and dissimilarity, leading to better performance and more objective information extraction.</p><hr><h3>HDRFlow: Real-Time HDR Video Reconstruction with Large Motions</h3>
<p><a href='http://arxiv.org/abs/2403.03447v1'>http://arxiv.org/abs/2403.03447v1</a></p>
<p><b>Compressor summary</b>: HDRFlow is a robust and efficient flow estimator that uses an HDR-domain alignment loss, an efficient flow network with a multi-size large kernel, and synthetic data to reconstruct high dynamic range video from alternating exposure image sequences in real-time.</p><hr><h3>Uncertainty quantification for deeponets with ensemble kalman inversion</h3>
<p><a href='http://arxiv.org/abs/2403.03444v1'>http://arxiv.org/abs/2403.03444v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel inference approach using Ensemble Kalman Inversion (EKI) to efficiently and informatively estimate uncertainty in DeepONet predictions, especially for limited and noisy data.</p><hr><h3>VLSP 2023 -- LTER: A Summary of the Challenge on Legal Textual  Entailment Recognition</h3>
<p><a href='http://arxiv.org/abs/2403.03435v1'>http://arxiv.org/abs/2403.03435v1</a></p>
<p><b>Compressor summary</b>: The paper presents the first research on AI for the Vietnamese language in the legal domain, highlighting key linguistic challenges.</p><hr><h3>Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2403.03432v1'>http://arxiv.org/abs/2403.03432v1</a></p>
<p><b>Compressor summary</b>: The Mixture-of-LoRAs (MoA) architecture is a novel tuning method that enhances multi-task learning with large language models by combining multiple LoRA modules using an explicit routing strategy and domain labels, enabling quick adaptation to new domains.</p><hr><h3>Towards Understanding Cross and Self-Attention in Stable Diffusion for  Text-Guided Image Editing</h3>
<p><a href='http://arxiv.org/abs/2403.03431v1'>http://arxiv.org/abs/2403.03431v1</a></p>
<p><b>Compressor summary</b>: This paper analyzes how cross and self-attention maps in Stable Diffusion affect image editing and proposes a simpler, more efficient tuning-free method based on the findings.</p><hr><h3>Sculpting Molecules in 3D: A Flexible Substructure Aware Framework for  Text-Oriented Molecular Optimization</h3>
<p><a href='http://arxiv.org/abs/2403.03425v1'>http://arxiv.org/abs/2403.03425v1</a></p>
<p><b>Compressor summary</b>: 3DToMolo is an innovative deep learning method that generates novel molecules with specified symmetries and properties by harmonizing diverse modalities and aligning them seamlessly.</p><hr><h3>LEAD: Learning Decomposition for Source-free Universal Domain Adaptation</h3>
<p><a href='http://arxiv.org/abs/2403.03421v1'>http://arxiv.org/abs/2403.03421v1</a></p>
<p><b>Compressor summary</b>: LEAD is a novel method for Universal Domain Adaptation without source data that uses feature decomposition and instance-level decision boundaries to identify target-private data.</p><hr><h3>Negating Negatives: Alignment without Human Positive Samples via  Distributional Dispreference Optimization</h3>
<p><a href='http://arxiv.org/abs/2403.03419v1'>http://arxiv.org/abs/2403.03419v1</a></p>
<p><b>Compressor summary</b>: D$^2$O is a novel alignment method for LLMs that uses only negative samples to reduce harmfulness while preserving helpfulness, achieving superior results in generating safe and informative responses.</p><hr><h3>Leveraging The Finite States of Emotion Processing to Study Late-Life  Mental Health</h3>
<p><a href='http://arxiv.org/abs/2403.03414v1'>http://arxiv.org/abs/2403.03414v1</a></p>
<p><b>Compressor summary</b>: The text describes a method called vcHMM that uses Hidden Markov Models and Finite State Automata to study the system-level dynamics of mental health, questionnaire data, and fMRI data, providing insights into how behavior and neural activity relate to depression.</p><hr><h3>Advancing Out-of-Distribution Detection through Data Purification and  Dynamic Activation Function Design</h3>
<p><a href='http://arxiv.org/abs/2403.03412v1'>http://arxiv.org/abs/2403.03412v1</a></p>
<p><b>Compressor summary</b>: OOD-R is a curated dataset with enhanced noise reduction and ActFun is a method to fine-tune model response for better OOD detection and uncertainty estimation in neural networks.</p><hr><h3>Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial  Regression</h3>
<p><a href='http://arxiv.org/abs/2403.03410v1'>http://arxiv.org/abs/2403.03410v1</a></p>
<p><b>Compressor summary</b>: The text compares three algorithms for predicting crypto currency prices and finds that the Support Vector Machine with a linear kernel has the smallest error.</p><hr><h3>Scene Depth Estimation from Traditional Oriental Landscape Paintings</h3>
<p><a href='http://arxiv.org/abs/2403.03408v1'>http://arxiv.org/abs/2403.03408v1</a></p>
<p><b>Compressor summary</b>: The authors propose a novel framework for estimating scene depth from oriental landscape painting images, enabling 3D sculpture creation and improving accessibility for visually impaired people.</p><hr><h3>An EnKF-LSTM Assimilation Algorithm for Crop Growth Model</h3>
<p><a href='http://arxiv.org/abs/2403.03406v1'>http://arxiv.org/abs/2403.03406v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an EnKF-LSTM data assimilation method for crop growth prediction that combines ensemble Kalman filter and LSTM neural network, improving accuracy by incorporating real-time data.</p><hr><h3>Causality-based Cross-Modal Representation Learning for  Vision-and-Language Navigation</h3>
<p><a href='http://arxiv.org/abs/2403.03405v1'>http://arxiv.org/abs/2403.03405v1</a></p>
<p><b>Compressor summary</b>: CausalVLN is a framework that uses causal learning to train robust navigators with unbiased feature representations, improving generalization across different environments.</p><hr><h3>BAIT: Benchmarking (Embedding) Architectures for Interactive  Theorem-Proving</h3>
<p><a href='http://arxiv.org/abs/2403.03401v1'>http://arxiv.org/abs/2403.03401v1</a></p>
<p><b>Compressor summary</b>: BAIT is a framework to compare learning methods in Interactive Theorem Proving, showing that Structure Aware Transformers perform well and leading to a novel end-to-end system.</p><hr><h3>Contrastive Learning of Person-independent Representations for Facial  Action Unit Detection</h3>
<p><a href='http://arxiv.org/abs/2403.03400v1'>http://arxiv.org/abs/2403.03400v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to learn facial action unit (AU) representations from unlabelled videos using contrastive learning, which improves AU detection performance and reduces data scarcity.</p><hr><h3>Japanese-English Sentence Translation Exercises Dataset for Automatic  Grading</h3>
<p><a href='http://arxiv.org/abs/2403.03396v1'>http://arxiv.org/abs/2403.03396v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new task of grading sentence translation exercises in L2 language learning and creates a dataset for it, showing that existing models struggle to classify responses accurately.</p><hr><h3>Performance Evaluation of Semi-supervised Learning Frameworks for  Multi-Class Weed Detection</h3>
<p><a href='http://arxiv.org/abs/2403.03390v1'>http://arxiv.org/abs/2403.03390v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a semi-supervised learning framework for weed detection using object detection frameworks, which achieves high accuracy with less labeled data and promotes sustainable agriculture.</p><hr><h3>Adaptive Discovering and Merging for Incremental Novel Class Discovery</h3>
<p><a href='http://arxiv.org/abs/2403.03382v1'>http://arxiv.org/abs/2403.03382v1</a></p>
<p><b>Compressor summary</b>: ADM is a new paradigm for lifelong learning that adaptively discovers and merges novel classes without losing established knowledge, using Triple Comparison and Probability Regularization for category assignment and Adaptive Model Merging for knowledge integration.</p>