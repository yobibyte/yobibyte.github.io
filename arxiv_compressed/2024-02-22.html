
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-02-22</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-02-22 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>D-Flow: Differentiating through Flows for Controlled Generation</h3>
<p><a href='http://arxiv.org/abs/2402.14017v1'>http://arxiv.org/abs/2402.14017v1</a></p>
<p><b>Compressor summary</b>: D-Flow is a method for controlling generation in Diffusion and Flow-Matching models by differentiating through the flow, achieving state-of-the-art results on various tasks.</p><hr><h3>Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on  Zero-shot LLM Assessment</h3>
<p><a href='http://arxiv.org/abs/2402.14016v1'>http://arxiv.org/abs/2402.14016v1</a></p>
<p><b>Compressor summary</b>: The study finds that large language models used for assessment are vulnerable to simple attacks that can manipulate their outputs, raising concerns about their reliability in real-world situations.</p><hr><h3>Corrective Machine Unlearning</h3>
<p><a href='http://arxiv.org/abs/2402.14015v1'>http://arxiv.org/abs/2402.14015v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Machine Learning models face data integrity issues due to internet data sources
- Corrective Machine Unlearning is the problem of mitigating the impact of unknown manipulations on a model, possibly knowing only a subset of affected samples
- Existing unlearning methods require most of the manipulated data to be identified, but SSD achieves limited success with just a small portion

Summary:
The paper studies Corrective Machine Unlearning, a problem of removing the effect of unknown internet data manipulations on a ML model, and shows that SSD can partially achieve this goal with few samples.</p><hr><h3>Misalignment, Learning, and Ranking: Harnessing Users Limited Attention</h3>
<p><a href='http://arxiv.org/abs/2402.14013v1'>http://arxiv.org/abs/2402.14013v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an algorithm that leverages users' limited attention to improve recommendation systems in digital health and EdTech, addressing the challenge of impulsive user choices.</p><hr><h3>Geometry-Informed Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.14009v1'>http://arxiv.org/abs/2402.14009v1</a></p>
<p><b>Compressor summary</b>: GINNs are neural networks that learn under geometric constraints, represent solutions as neural fields, and generate diverse solutions without training data using differentiable losses based on Morse theory.</p><hr><h3>OlympiadBench: A Challenging Benchmark for Promoting AGI with  Olympiad-Level Bilingual Multimodal Scientific Problems</h3>
<p><a href='http://arxiv.org/abs/2402.14008v1'>http://arxiv.org/abs/2402.14008v1</a></p>
<p><b>Compressor summary</b>: OlympiadBench is a new bilingual multimodal scientific benchmark that tests the advanced abilities of large language and multimodal models, revealing their limitations in physics reasoning.</p><hr><h3>Can Watermarks Survive Translation? On the Cross-lingual Consistency of  Text Watermark for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.14007v1'>http://arxiv.org/abs/2402.14007v1</a></p>
<p><b>Compressor summary</b>: The study introduces cross-lingual consistency in text watermarking, shows its lack in current methods, proposes a removal attack, and suggests a defense method.</p><hr><h3>Hallucinations or Attention Misdirection? The Path to Strategic Value  Extraction in Business Using Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.14002v1'>http://arxiv.org/abs/2402.14002v1</a></p>
<p><b>Compressor summary</b>: The paper discusses the difference between hallucinations and attention misdirection in large language models, proposing a method to minimize errors and unlock innovation potential in business settings.</p><hr><h3>Real-time 3D-aware Portrait Editing from a Single Image</h3>
<p><a href='http://arxiv.org/abs/2402.14000v1'>http://arxiv.org/abs/2402.14000v1</a></p>
<p><b>Compressor summary</b>: 3DPE is a fast and flexible tool for real-time face image editing using 3D geometry and text descriptions.</p><hr><h3>Analysing The Impact of Sequence Composition on Language Model  Pre-Training</h3>
<p><a href='http://arxiv.org/abs/2402.13991v1'>http://arxiv.org/abs/2402.13991v1</a></p>
<p><b>Compressor summary</b>: The authors explore how pre-training sequence composition affects generalization and propose intra-document causal masking, BM25Chunk for better in-context learning and downstream tasks.</p><hr><h3>A Simple and Yet Fairly Effective Defense for Graph Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.13987v1'>http://arxiv.org/abs/2402.13987v1</a></p>
<p><b>Compressor summary</b>: NoisyGNNs defend GNNs against small adversarial perturbations by injecting noise into the model's architecture, improving robustness with minimal performance impact and high compatibility with existing techniques.</p><hr><h3>Stability-Aware Training of Neural Network Interatomic Potentials with  Differentiable Boltzmann Estimators</h3>
<p><a href='http://arxiv.org/abs/2402.13984v1'>http://arxiv.org/abs/2402.13984v1</a></p>
<p><b>Compressor summary</b>: StABlE Training improves the stability and accuracy of neural network interatomic potentials by combining supervised learning from quantum-mechanical energies and forces with reference system observables, and iteratively correcting instabilities using a Boltzmann Estimator.</p><hr><h3>The Importance of Architecture Choice in Deep Learning for Climate  Applications</h3>
<p><a href='http://arxiv.org/abs/2402.13979v1'>http://arxiv.org/abs/2402.13979v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a neural network model for predicting the Atlantic Meridional Overturning Circulation (AMOC), an important climate factor, under various scenarios and uncertainty, challenging previous AMOC collapse predictions.</p><hr><h3>Towards Building Multilingual Language Model for Medicine</h3>
<p><a href='http://arxiv.org/abs/2402.13963v1'>http://arxiv.org/abs/2402.13963v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper develops an open-source multilingual language model for medicine (MMedLM 2)
- It creates a new medical corpus (MMedC) and a benchmark (MMedBench) for multilingual LLMs in medicine
- MMedLM 2 outperforms other open-source models and rivals GPT-4 on the benchmark

Summary:
The paper presents an open-source, multilingual language model for medicine, based on a new medical corpus and benchmark, that surpasses existing models and competes with GPT-4.</p><hr><h3>Can You Learn Semantics Through Next-Word Prediction? The Case of  Entailment</h3>
<p><a href='http://arxiv.org/abs/2402.13956v1'>http://arxiv.org/abs/2402.13956v1</a></p>
<p><b>Compressor summary</b>: This study investigates how neural language models learn entailment relations from text data and proposes a revised theory accounting for redundancy in natural language.</p><hr><h3>BEE-NET: A deep neural network to identify in-the-wild Bodily Expression  of Emotions</h3>
<p><a href='http://arxiv.org/abs/2402.13955v1'>http://arxiv.org/abs/2402.13955v1</a></p>
<p><b>Compressor summary</b>: The study presents BEE-NET, a deep neural network that uses environmental factors to recognize emotions from body language, outperforming the current state-of-the-art by 2.07% on the BoLD dataset.</p><hr><h3>Measuring Social Biases in Masked Language Models by Proxy of Prediction  Quality</h3>
<p><a href='http://arxiv.org/abs/2402.13954v1'>http://arxiv.org/abs/2402.13954v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper evaluates social biases encoded by transformer models trained with masked language modeling (MLM) objective
- It uses proxy functions and iterative masking experiments to measure bias quality and preference for disadvantaged/advantaged groups
- It compares bias estimations with other methods using two benchmark datasets and finds high religious and disability biases, low gender bias
- It extends previous work by evaluating biases after re-training MLM under masked language modeling objective

Summary:
The paper assesses social biases in transformer models trained with MLM using novel measures and methods, finding varying bias levels across datasets and tasks.</p><hr><h3>Making Reasoning Matter: Measuring and Improving Faithfulness of  Chain-of-Thought Reasoning</h3>
<p><a href='http://arxiv.org/abs/2402.13950v1'>http://arxiv.org/abs/2402.13950v1</a></p>
<p><b>Compressor summary</b>: The paper examines how large language models reason step-by-step and introduces FRODO, a framework to improve their reasoning by generating correct intermediate steps and faithfully reasoning over them.</p><hr><h3>AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement  Learning</h3>
<p><a href='http://arxiv.org/abs/2402.13946v1'>http://arxiv.org/abs/2402.13946v1</a></p>
<p><b>Compressor summary</b>: The authors propose AttackGNN, a novel reinforcement learning agent that generates adversarial examples to fool graph neural network-based techniques for hardware security applications.</p><hr><h3>Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP  Guided Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2402.13936v1'>http://arxiv.org/abs/2402.13936v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new image captioning model training strategy using ground truth captions to improve distinctiveness, fluency, and contrastive reward in reinforcement learning.</p><hr><h3>Do Efficient Transformers Really Save Computation?</h3>
<p><a href='http://arxiv.org/abs/2402.13934v1'>http://arxiv.org/abs/2402.13934v1</a></p>
<p><b>Compressor summary</b>: This paper compares the reasoning abilities of two efficient Transformer variants (Sparse and Linear) using Chain-of-Thought prompts and finds their model size scales with problem size, but they can be more efficient for a specific class of Dynamic Programming problems.</p><hr><h3>Tumor segmentation on whole slide images: training or prompting?</h3>
<p><a href='http://arxiv.org/abs/2402.13932v1'>http://arxiv.org/abs/2402.13932v1</a></p>
<p><b>Compressor summary</b>: Visual prompting is a new method that improves tumor segmentation in cancer diagnosis using subtle input modifications, outperforming traditional methods with less fine-tuning.</p><hr><h3>Enhancing Reinforcement Learning Agents with Local Guides</h3>
<p><a href='http://arxiv.org/abs/2402.13930v1'>http://arxiv.org/abs/2402.13930v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for combining local guide policies with Reinforcement Learning algorithms, and shows it improves performance on various classical problems and safety-critical systems.</p><hr><h3>SDXL-Lightning: Progressive Adversarial Diffusion Distillation</h3>
<p><a href='http://arxiv.org/abs/2402.13929v1'>http://arxiv.org/abs/2402.13929v1</a></p>
<p><b>Compressor summary</b>: The text introduces a diffusion distillation method for improving text-to-image generation using SDXL, with a focus on quality and mode coverage.</p><hr><h3>The Delusional Hedge Algorithm as a Model of Human Learning from Diverse  Opinions</h3>
<p><a href='http://arxiv.org/abs/2402.13927v1'>http://arxiv.org/abs/2402.13927v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm called "delusional hedge" that extends the classic hedge algorithm to learn from both labeled and unlabeled information sources, and shows that humans behave similarly when learning from diverse opinions.</p><hr><h3>Large Language Models are Vulnerable to Bait-and-Switch Attacks for  Generating Harmful Content</h3>
<p><a href='http://arxiv.org/abs/2402.13926v1'>http://arxiv.org/abs/2402.13926v1</a></p>
<p><b>Compressor summary</b>: Bait-and-Switch attacks can easily manipulate safe text generated by large language models into harmful narratives, posing a major challenge for developing reliable safety measures.</p><hr><h3>SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in  Clinical Summarization</h3>
<p><a href='http://arxiv.org/abs/2402.13919v1'>http://arxiv.org/abs/2402.13919v1</a></p>
<p><b>Compressor summary</b>: The study uses GPT-3.5 and GPT-4 to generate feedback for improving the factual accuracy of clinical note summarization by AI systems, leveraging their expertise in clinical NLP tasks.</p><hr><h3>BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for  Cloud Detection and Segmentation in Remote Sensing Imagery</h3>
<p><a href='http://arxiv.org/abs/2402.13918v1'>http://arxiv.org/abs/2402.13918v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The text is about cloud segmentation from remote sensing imagery and its challenges and applications.
- The paper evaluates seven semantic segmentation and detection algorithms for cloud identification and compares their performance.
- The paper also investigates the impact of image type and spectral bands on model adaptability.
- The paper aims to produce machine learning algorithms that can perform cloud segmentation with few spectral bands.
- The paper uses Sentinel-2 and Landsat-8 imagery as datasets and provides a github link for reproduction.

Summary:
The text summarizes a paper that benchmarks seven cloud segmentation methods using remote sensing imagery, assesses their adaptability with different image types and spectral bands, and proposes algorithms that can segment clouds with only RGB and RGBN-IR bands. The paper provides a github link for reproduction and uses Sentinel-2 and Landsat-8 datasets.</p><hr><h3>What Linguistic Features and Languages are Important in LLM Translation?</h3>
<p><a href='http://arxiv.org/abs/2402.13917v1'>http://arxiv.org/abs/2402.13917v1</a></p>
<p><b>Compressor summary</b>: The study evaluates Llama2's machine translation performance across various languages and explores factors influencing its translation quality, suggesting that multilingual LLMs could be more effective than English-centric ones.</p><hr><h3>Bias correction of wind power forecasts with SCADA data and continuous  learning</h3>
<p><a href='http://arxiv.org/abs/2402.13916v1'>http://arxiv.org/abs/2402.13916v1</a></p>
<p><b>Compressor summary</b>: The text introduces four machine learning models for wind power forecasting, evaluates them on a dataset from a wind park, and shows that a convolutional neural network with continuous learning performs best.</p><hr><h3>Explain to Question not to Justify</h3>
<p><a href='http://arxiv.org/abs/2402.13914v1'>http://arxiv.org/abs/2402.13914v1</a></p>
<p><b>Compressor summary</b>: The text introduces two cultures of XAI research, focusing on human-oriented explanations (BLUE) and model-oriented explanions (RED), with the latter being under-explored but crucial for AI safety.</p><hr><h3>Replication Study: Enhancing Hydrological Modeling with Physics-Guided  Machine Learning</h3>
<p><a href='http://arxiv.org/abs/2402.13911v1'>http://arxiv.org/abs/2402.13911v1</a></p>
<p><b>Compressor summary</b>: The study proposes a Physics Informed Machine Learning model that combines the strengths of both physics-based models and ML algorithms for more reliable hydrological predictions.</p><hr><h3>Leveraging Collection-Wide Similarities for Unsupervised Document  Structure Extraction</h3>
<p><a href='http://arxiv.org/abs/2402.13906v1'>http://arxiv.org/abs/2402.13906v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a graph-based method to identify the typical structure of documents within a collection by capturing recurring topics across domains and languages.</p><hr><h3>Calibrating Large Language Models with Sample Consistency</h3>
<p><a href='http://arxiv.org/abs/2402.13904v1'>http://arxiv.org/abs/2402.13904v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper explores how to measure confidence of LLMs using multiple random generations and consistency measures
- Consistency-based methods outperform existing post-hoc approaches
- Confidence scores can enhance model performance

Summary:
The paper proposes a new way to calibrate the confidence of large language models using multiple random outputs and consistency metrics, which improves both accuracy and performance.</p><hr><h3>Dealing with unbounded gradients in stochastic saddle-point optimization</h3>
<p><a href='http://arxiv.org/abs/2402.13903v1'>http://arxiv.org/abs/2402.13903v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a regularization technique for stochastic first-order methods that stabilizes optimization and provides performance guarantees for finding saddle points of convex-concave functions, especially in reinforcement learning.</p><hr><h3>Non-asymptotic Convergence of Discrete-time Diffusion Models: New  Approach and Improved Rate</h3>
<p><a href='http://arxiv.org/abs/2402.13901v1'>http://arxiv.org/abs/2402.13901v1</a></p>
<p><b>Compressor summary</b>: The paper studies convergence guarantees and improved acceleration for discrete-time denoising diffusion models, covering various distributions with different properties.</p><hr><h3>Overcoming Saturation in Density Ratio Estimation by Iterated  Regularization</h3>
<p><a href='http://arxiv.org/abs/2402.13891v1'>http://arxiv.org/abs/2402.13891v1</a></p>
<p><b>Compressor summary</b>: The paper proposes iterated regularization to improve kernel methods for estimating the ratio of two probability densities and achieve fast error convergence rates in machine learning and statistics tasks.</p><hr><h3>Beyond Probabilities: Unveiling the Misalignment in Evaluating Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.13887v1'>http://arxiv.org/abs/2402.13887v1</a></p>
<p><b>Compressor summary</b>: This paper questions the effectiveness of using output probabilities to evaluate large language models, particularly for multiple choice questions, as it finds that these methods do not accurately reflect how LLMs are used in real-world applications.</p><hr><h3>Scene Prior Filtering for Depth Map Super-Resolution</h3>
<p><a href='http://arxiv.org/abs/2402.13876v1'>http://arxiv.org/abs/2402.13876v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new method called SPFNet that uses scene priors to improve super-resolution of depth images by reducing texture interference and enhancing edges.</p><hr><h3>$\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for  In-Context Learning</h3>
<p><a href='http://arxiv.org/abs/2402.13874v1'>http://arxiv.org/abs/2402.13874v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a sequential-aware method ($	exttt{Se}^2$) for selecting and organizing example sequences that leverage LLM feedback to improve in-context learning (ICL) performance, quality, and diversity across 23 NLP tasks.</p><hr><h3>Generative Probabilistic Time Series Forecasting and Applications in  Grid Operations</h3>
<p><a href='http://arxiv.org/abs/2402.13870v1'>http://arxiv.org/abs/2402.13870v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method called weak innovation autoencoder for generating probable future time series data, which outperforms existing methods in predicting volatile electricity prices.</p><hr><h3>An Explainable Transformer-based Model for Phishing Email Detection: A  Large Language Model Approach</h3>
<p><a href='http://arxiv.org/abs/2402.13871v1'>http://arxiv.org/abs/2402.13871v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a fine-tuned DistilBERT model with Explainable-AI techniques for effective phishing email detection using preprocessing methods on a balanced dataset.</p><hr><h3>Kuaiji: the First Chinese Accounting Large Language Model</h3>
<p><a href='http://arxiv.org/abs/2402.13866v1'>http://arxiv.org/abs/2402.13866v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Large Language Models are good at natural language but not specialized domains like accounting
- Kuaiji is an Accounting Large Language Model fine-tuned using Baichuan framework
- Kuaiji uses CAtAcctQA, a large Chinese accounting dataset, and shows high accuracy and speed
- Kuaiji is the first Chinese accounting LLM and validated in real-world scenarios

Summary:
Kuaiji is a tailored Accounting Large Language Model that uses a continuous pre-training and supervised fine-tuning process to adapt to specialized domains. It leverages CAtAcctQA, the first Chinese accounting dataset, and performs well in real-world accounting tasks.</p><hr><h3>Replicable Learning of Large-Margin Halfspaces</h3>
<p><a href='http://arxiv.org/abs/2402.13857v1'>http://arxiv.org/abs/2402.13857v1</a></p>
<p><b>Compressor summary</b>: The paper presents new efficient and dimension-independent replicable algorithms for learning large-margin halfspaces that improve upon existing ones in terms of sample and runtime complexity.</p><hr><h3>Neural Control System for Continuous Glucose Monitoring and Maintenance</h3>
<p><a href='http://arxiv.org/abs/2402.13852v1'>http://arxiv.org/abs/2402.13852v1</a></p>
<p><b>Compressor summary</b>: The novel neural control system monitors and adjusts insulin delivery in real-time to optimize glucose levels for diabetic individuals.</p><hr><h3>VL-Trojan: Multimodal Instruction Backdoor Attacks against  Autoregressive Visual Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.13851v1'>http://arxiv.org/abs/2402.13851v1</a></p>
<p><b>Compressor summary</b>: VL-Trojan is a multimodal instruction backdoor attack that can effectively manipulate autoregressive VLMs by injecting poisoned samples with triggers in instructions or images, overcoming challenges such as frozen visual encoder and black-box attacks.</p><hr><h3>Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps</h3>
<p><a href='http://arxiv.org/abs/2402.13848v1'>http://arxiv.org/abs/2402.13848v1</a></p>
<p><b>Compressor summary</b>: Key points:
- BEV maps are important for robotics, but existing algorithms have limitations
- The new model can project any modality to BEV without depth information or supervision
- The model is general and outperforms competing methods

Summary:
The paper presents a novel model that can map any first-person modality to BEV maps for robotics, overcoming the limitations of existing algorithms.</p><hr><h3>MLXP: A framework for conducting replicable Machine Learning eXperiments  in Python</h3>
<p><a href='http://arxiv.org/abs/2402.13831v1'>http://arxiv.org/abs/2402.13831v1</a></p>
<p><b>Compressor summary</b>: MLXP is an open-source, simple, and lightweight experiment management tool for machine learning research that helps ensure reproducible results.</p><hr><h3>Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering  of 3D Gaussian Splatting</h3>
<p><a href='http://arxiv.org/abs/2402.13827v1'>http://arxiv.org/abs/2402.13827v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a technique to reduce unnecessary 3D Gaussians in real-time for faster and high-quality rendering with 3D Gaussian splatting, and proposes an efficient hardware architecture to support it.</p><hr><h3>MSTAR: Multi-Scale Backbone Architecture Search for Timeseries  Classification</h3>
<p><a href='http://arxiv.org/abs/2402.13822v1'>http://arxiv.org/abs/2402.13822v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework for NAS that addresses frequency and time resolution issues in TSC, achieving state-of-the-art results on four datasets.</p><hr><h3>Performance Improvement Bounds for Lipschitz Configurable Markov  Decision Processes</h3>
<p><a href='http://arxiv.org/abs/2402.13821v1'>http://arxiv.org/abs/2402.13821v1</a></p>
<p><b>Compressor summary</b>: The paper studies Lipschitz continuous Configurable Markov Decision Processes (Conf-MDPs) and provides bounds on Wasserstein distance between stationary distributions and a new performance improvement lower bound.</p><hr><h3>FLD: Fourier Latent Dynamics for Structured Motion Representation and  Learning</h3>
<p><a href='http://arxiv.org/abs/2402.13820v1'>http://arxiv.org/abs/2402.13820v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to learn and track motions using self-supervised representations that capture spatial-temporal patterns and enable safe action execution.</p><hr><h3>Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering  Dehumanizing Language</h3>
<p><a href='http://arxiv.org/abs/2402.13818v1'>http://arxiv.org/abs/2402.13818v1</a></p>
<p><b>Compressor summary</b>: The paper tests advanced NLP models' ability to detect dehumanizing language and finds they are promising but biased, lacking accuracy in identifying dehumanization for some groups.</p><hr><h3>A unified framework of non-local parametric methods for image denoising</h3>
<p><a href='http://arxiv.org/abs/2402.13816v1'>http://arxiv.org/abs/2402.13816v1</a></p>
<p><b>Compressor summary</b>: The paper presents a unified view of non-local methods for image denoising and proposes a new method called NL-Ridge that simplifies the process and achieves better performance.</p><hr><h3>Voice-Driven Mortality Prediction in Hospitalized Heart Failure  Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers</h3>
<p><a href='http://arxiv.org/abs/2402.13812v1'>http://arxiv.org/abs/2402.13812v1</a></p>
<p><b>Compressor summary</b>: The study shows that using voice biomarkers and machine learning can accurately predict heart failure patients' 5-year mortality rates and improve their care.</p><hr><h3>The Expected Loss of Preconditioned Langevin Dynamics Reveals the  Hessian Rank</h3>
<p><a href='http://arxiv.org/abs/2402.13810v1'>http://arxiv.org/abs/2402.13810v1</a></p>
<p><b>Compressor summary</b>: The paper derives a formula for how well Langevin dynamics (a sampling method) works near optimal solutions, depending on the properties of its preconditioning matrix, and shows its application in neural networks and comparison with other methods.</p><hr><h3>The Geography of Information Diffusion in Online Discourse on Europe and  Migration</h3>
<p><a href='http://arxiv.org/abs/2402.13800v1'>http://arxiv.org/abs/2402.13800v1</a></p>
<p><b>Compressor summary</b>: The study analyses online information about Europe and migration, focusing on its geography, language, and dynamics, revealing a transnational imbalance in information flow and the role of football as a popular topic.</p><hr><h3>Scalable Methods for Brick Kiln Detection and Compliance Monitoring from  Satellite Imagery: A Deployment Case Study in India</h3>
<p><a href='http://arxiv.org/abs/2402.13796v1'>http://arxiv.org/abs/2402.13796v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a system to detect and monitor brick kiln pollution using satellite imagery and machine learning, which could help governments regulate them more effectively.</p><hr><h3>Opening the Black-Box: A Systematic Review on Explainable AI in Remote  Sensing</h3>
<p><a href='http://arxiv.org/abs/2402.13791v1'>http://arxiv.org/abs/2402.13791v1</a></p>
<p><b>Compressor summary</b>: This paper reviews explainable AI methods in Remote Sensing, identifying trends, challenges, insights, and future directions.</p><hr><h3>Synthesis of Hierarchical Controllers Based on Deep Reinforcement  Learning Policies</h3>
<p><a href='http://arxiv.org/abs/2402.13785v1'>http://arxiv.org/abs/2402.13785v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel hierarchical method for controller design in MDPs using deep reinforcement learning and reactive synthesis, with PAC guarantees and sparse reward handling.</p><hr><h3>Semirings for Probabilistic and Neuro-Symbolic Logic Programming</h3>
<p><a href='http://arxiv.org/abs/2402.13782v1'>http://arxiv.org/abs/2402.13782v1</a></p>
<p><b>Compressor summary</b>: Probabilistic logic programming integrates probabilistic models into logical languages, using algebraic methods for modeling, inference, and learning.</p><hr><h3>Preserving Near-Optimal Gradient Sparsification Cost for Scalable  Distributed Deep Learning</h3>
<p><a href='http://arxiv.org/abs/2402.13781v1'>http://arxiv.org/abs/2402.13781v1</a></p>
<p><b>Compressor summary</b>: ExDyna is a novel gradient sparsification scheme that reduces communication overhead in distributed training systems by efficiently managing workload balance, partition topology, and threshold scaling.</p><hr><h3>Contextual Molecule Representation Learning from Chemical Reaction  Knowledge</h3>
<p><a href='http://arxiv.org/abs/2402.13779v1'>http://arxiv.org/abs/2402.13779v1</a></p>
<p><b>Compressor summary</b>: REMO is a self-supervised learning framework for molecular representation learning that leverages atom-combination rules and chemical reactions to pre-train encoders, achieving better performance on various downstream tasks than existing methods.</p><hr><h3>Weakly supervised localisation of prostate cancer using reinforcement  learning for bi-parametric MR images</h3>
<p><a href='http://arxiv.org/abs/2402.13778v1'>http://arxiv.org/abs/2402.13778v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Reinforcement learning based weakly supervised system for localisation
- Novel reward definition using non-binarised classification probability from pre-trained binary classifier
- Minimises human bias and labelling costs
- Outperforms multi-instance learning and competes with fully-supervised learning on prostate cancer lesion localisation task

Summary:
The paper proposes a weakly supervised system that uses reinforcement learning and novel reward definition to localise regions of interest in images, such as prostate cancer lesions, with minimal human bias and labelling costs, achieving comparable performance to fully-supervised learning.</p><hr><h3>Deep Generative Models for Offline Policy Learning: Tutorial, Survey,  and Perspectives on Future Directions</h3>
<p><a href='http://arxiv.org/abs/2402.13777v1'>http://arxiv.org/abs/2402.13777v1</a></p>
<p><b>Compressor summary</b>: The text is a systematic review of deep generative models for offline policy learning in reinforcement learning and imitation learning domains.</p><hr><h3>Accuracy-Preserving Calibration via Statistical Modeling on Probability  Simplex</h3>
<p><a href='http://arxiv.org/abs/2402.13765v1'>http://arxiv.org/abs/2402.13765v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new accuracy-preserving calibration method for deep neural networks using the Concrete distribution and shows its effectiveness on benchmarks.</p><hr><h3>CriticBench: Evaluating Large Language Models as Critic</h3>
<p><a href='http://arxiv.org/abs/2402.13764v1'>http://arxiv.org/abs/2402.13764v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new benchmark, \shortname, to measure the critique abilities of Large Language Models (LLMs) in four dimensions using nine diverse tasks.</p><hr><h3>Factual Consistency Evaluation of Summarisation in the Era of Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.13758v1'>http://arxiv.org/abs/2402.13758v1</a></p>
<p><b>Compressor summary</b>: This paper introduces TreatFact, a dataset of LLM-generated summaries for clinical texts, and benchmarks 11 LLMs for factual consistency evaluation across news and clinical domains, revealing open-source LLMs' limitations and challenges in clinical summary assessment.</p><hr><h3>High-throughput Visual Nano-drone to Nano-drone Relative Localization  using Onboard Fully Convolutional Networks</h3>
<p><a href='http://arxiv.org/abs/2402.13756v1'>http://arxiv.org/abs/2402.13756v1</a></p>
<p><b>Compressor summary</b>: The authors propose a novel vision-based neural network for relative pose estimation between nano-drones using a low-resolution camera and a low-power chip, achieving significant improvements in accuracy and endurance compared to previous methods.</p><hr><h3>LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens</h3>
<p><a href='http://arxiv.org/abs/2402.13753v1'>http://arxiv.org/abs/2402.13753v1</a></p>
<p><b>Compressor summary</b>: LongRoPE extends the context window of LLMs to 2048k tokens using an efficient search for non-uniformities, a progressive extension strategy, and readjustments on shorter lengths.</p><hr><h3>AI-Powered Predictions for Electricity Load in Prosumer Communities</h3>
<p><a href='http://arxiv.org/abs/2402.13752v1'>http://arxiv.org/abs/2402.13752v1</a></p>
<p><b>Compressor summary</b>: The paper reviews and tests various electricity load forecasting techniques for optimizing consumption in communities with renewable energy sources and storage, using AI models and weather forecasts.</p><hr><h3>Reasoning Algorithmically in Graph Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.13744v1'>http://arxiv.org/abs/2402.13744v1</a></p>
<p><b>Compressor summary</b>: The text describes the research on Neural Algorithmic Reasoning (NAR), which combines neural networks and algorithms to enable machines to reason logically and learn from data.</p><hr><h3>Unlocking Instructive In-Context Learning with Tabular Prompting for  Relational Triple Extraction</h3>
<p><a href='http://arxiv.org/abs/2402.13741v1'>http://arxiv.org/abs/2402.13741v1</a></p>
<p><b>Compressor summary</b>: The text proposes a table generation prompt for relational triple extraction (RTE) task to improve in-context learning (ICL) performance by incorporating structured information and selecting semantically relevant samples.</p><hr><h3>From Text to CQL: Bridging Natural Language and Corpus Search Engine</h3>
<p><a href='http://arxiv.org/abs/2402.13740v1'>http://arxiv.org/abs/2402.13740v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework for automating natural language to Corpus Query Language translation, using large language models and advanced evaluation metrics.</p><hr><h3>SRNDiff: Short-term Rainfall Nowcasting with Condition Diffusion Model</h3>
<p><a href='http://arxiv.org/abs/2402.13737v1'>http://arxiv.org/abs/2402.13737v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Diffusion models are good for image generation and introduced to precipitation nowcasting
- SRNDiff is a diffusion model based on historical data that uses conditional UNet networks for accurate predictions
- SRNDiff outperforms GANs in accuracy, stability, and efficiency, and generates high-quality samples

Summary:
SRNDiff is a diffusion model that uses conditional UNet networks to predict short-term precipitation nowcasting from historical data, achieving better accuracy, stability, and efficiency than GANs.</p><hr><h3>The Da Vinci Code of Large Pre-trained Language Models: Deciphering  Degenerate Knowledge Neurons</h3>
<p><a href='http://arxiv.org/abs/2402.13731v1'>http://arxiv.org/abs/2402.13731v1</a></p>
<p><b>Compressor summary</b>: The study defines degenerate knowledge neurons in pre-trained language models and proposes methods to analyze and form them for better factual knowledge storage.</p><hr><h3>Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet  Representation</h3>
<p><a href='http://arxiv.org/abs/2402.13729v1'>http://arxiv.org/abs/2402.13729v1</a></p>
<p><b>Compressor summary</b>: The HVDM model uses a hybrid autoencoder to capture spatio-temporal dependencies in videos and generate high-quality synthetic videos with fine structures and details.</p><hr><h3>Average gradient outer product as a mechanism for deep neural collapse</h3>
<p><a href='http://arxiv.org/abs/2402.13728v1'>http://arxiv.org/abs/2402.13728v1</a></p>
<p><b>Compressor summary</b>: Deep Neural Collapse occurs mainly due to deep feature learning with the average gradient outer product, which is related to the singular structure of neural network weights.</p><hr><h3>Sparse and Structured Hopfield Networks</h3>
<p><a href='http://arxiv.org/abs/2402.13725v1'>http://arxiv.org/abs/2402.13725v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new way to train sparse Hopfield networks that links sparsity, memory retrieval, and loss margins, and applies it to structured Hopfield networks for pattern association tasks.</p><hr><h3>Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment  Analysis</h3>
<p><a href='http://arxiv.org/abs/2402.13722v1'>http://arxiv.org/abs/2402.13722v1</a></p>
<p><b>Compressor summary</b>: The authors propose adaptive masking methods to improve Aspect-Based Sentiment Analysis by removing irrelevant tokens based on context, achieving better results on online review datasets.</p><hr><h3>Ouroboros: Speculative Decoding with Large Model Enhanced Drafting</h3>
<p><a href='http://arxiv.org/abs/2402.13720v1'>http://arxiv.org/abs/2402.13720v1</a></p>
<p><b>Compressor summary</b>: Ouroboros improves the efficiency and effectiveness of speculative decoding by using a phrase candidate pool from the LLM's verification process to generate better drafts.</p><hr><h3>$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens</h3>
<p><a href='http://arxiv.org/abs/2402.13718v1'>http://arxiv.org/abs/2402.13718v1</a></p>
<p><b>Compressor summary</b>: $\infty$Bench is a new benchmark for large language models that tests their ability to process and reason over long contexts, with average data lengths exceeding 100K tokens in both English and Chinese.</p><hr><h3>Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character  Role-Playing Agent</h3>
<p><a href='http://arxiv.org/abs/2402.13717v1'>http://arxiv.org/abs/2402.13717v1</a></p>
<p><b>Compressor summary</b>: Neeko is a framework that uses a dynamic low-rank adapter strategy to enable efficient imitation of multiple characters in open-domain dialogue, improving user interaction experiences.</p><hr><h3>DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based  Graph Continual Learning</h3>
<p><a href='http://arxiv.org/abs/2402.13711v1'>http://arxiv.org/abs/2402.13711v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new graph continual learning method, DSLR, that improves rehearsal-based approaches by considering class representativeness and diversity, as well as ensuring informative neighbors for replayed nodes to reduce catastrophic forgetting.</p><hr><h3>SaGE: Evaluating Moral Consistency in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.13709v1'>http://arxiv.org/abs/2402.13709v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new measure called SaGE to evaluate the moral consistency of LLMs in conversational systems by using "Rules of Thumb" extracted from their responses to moral questions.</p><hr><h3>Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand  for Multilingual Instructions?</h3>
<p><a href='http://arxiv.org/abs/2402.13703v1'>http://arxiv.org/abs/2402.13703v1</a></p>
<p><b>Compressor summary</b>: The study shows that instruction-tuning multilingual LLMs on parallel data improves cross-lingual performance and challenges the Superficial Alignment Hypothesis.</p><hr><h3>On the Conflict of Robustness and Learning in Collaborative Machine  Learning</h3>
<p><a href='http://arxiv.org/abs/2402.13700v1'>http://arxiv.org/abs/2402.13700v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes existing robust aggregators for collaborative machine learning and shows that they are either ineffective or too restrictive to ensure privacy, safety, and learning.</p><hr><h3>Explainable Classification Techniques for Quantum Dot Device  Measurements</h3>
<p><a href='http://arxiv.org/abs/2402.13699v1'>http://arxiv.org/abs/2402.13699v1</a></p>
<p><b>Compressor summary</b>: The text introduces a synthetic data-based technique that uses Explainable Boosting Machines (EBMs) to create explainable features for image data in quantum information science without sacrificing accuracy.</p><hr><h3>Generalizable Semantic Vision Query Generation for Zero-shot Panoptic  and Semantic Segmentation</h3>
<p><a href='http://arxiv.org/abs/2402.13697v1'>http://arxiv.org/abs/2402.13697v1</a></p>
<p><b>Compressor summary</b>: CONCAT is a method to improve zero-shot panoptic segmentation by aligning semantic queries with visual CLS tokens and training a generator to synthesize fine-grained vision queries for unseen categories.</p><hr><h3>CMNER: A Chinese Multimodal NER Dataset based on Social Media</h3>
<p><a href='http://arxiv.org/abs/2402.13693v1'>http://arxiv.org/abs/2402.13693v1</a></p>
<p><b>Compressor summary</b>: The study creates a large Chinese Multimodal Named Entity Recognition dataset using Weibo posts and images, showing improved performance with image integration and cross-lingual learning.</p><hr><h3>KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual  Machine-Generated Text Detection</h3>
<p><a href='http://arxiv.org/abs/2402.13671v1'>http://arxiv.org/abs/2402.13671v1</a></p>
<p><b>Compressor summary</b>: The paper describes a method for detecting machine-generated text across multiple languages and domains, using fine-tuned language models and statistical metrics, which ranked fourth in a competition.</p><hr><h3>Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning</h3>
<p><a href='http://arxiv.org/abs/2402.13669v1'>http://arxiv.org/abs/2402.13669v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Self-Distillation Fine-Tuning (SDFT), a method that helps large language models maintain their general abilities while adapting to specific tasks by using a distilled dataset generated by the model itself.</p><hr><h3>GCOF: Self-iterative Text Generation for Copywriting Using Large  Language Model</h3>
<p><a href='http://arxiv.org/abs/2402.13667v1'>http://arxiv.org/abs/2402.13667v1</a></p>
<p><b>Compressor summary</b>: The Genetic Copy Optimization Framework enhances the efficiency and engagement of marketing copy creation using large language models and a modified crossover operator.</p><hr><h3>Stable Update of Regression Trees</h3>
<p><a href='http://arxiv.org/abs/2402.13655v1'>http://arxiv.org/abs/2402.13655v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a regularization method for updating regression trees that balances predictability and empirical stability by weighting data points based on their uncertainty in the initial model.</p><hr><h3>PQA: Zero-shot Protein Question Answering for Free-form Scientific  Enquiry with Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.13653v1'>http://arxiv.org/abs/2402.13653v1</a></p>
<p><b>Compressor summary</b>: The authors introduce a new task called zero-shot Protein Question Answering (PQA) for scientific enquiry, provide a large dataset and benchmarks, and present a multi-modal framework named Pika that achieves state-of-the-art performance.</p><hr><h3>Robustness of Deep Neural Networks for Micro-Doppler Radar  Classification</h3>
<p><a href='http://arxiv.org/abs/2402.13651v1'>http://arxiv.org/abs/2402.13651v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates the robustness of deep classifiers for radar data processing and proposes training methods to reduce overfitting and improve generalization.</p><hr><h3>Unsupervised Text Style Transfer via LLMs and Attention Masking with  Multi-way Interactions</h3>
<p><a href='http://arxiv.org/abs/2402.13647v1'>http://arxiv.org/abs/2402.13647v1</a></p>
<p><b>Compressor summary</b>: This paper proposes four ways to combine attention masking and LLMs for unsupervised text style transfer, improving style strength, content preservation, and text fluency.</p><hr><h3>Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition</h3>
<p><a href='http://arxiv.org/abs/2402.13643v1'>http://arxiv.org/abs/2402.13643v1</a></p>
<p><b>Compressor summary</b>: The paper proposes CAM, a novel approach for scene text recognition that uses class-aware glyph masks to suppress background noise and align features for improved performance.</p><hr><h3>FlexHB: a More Efficient and Flexible Framework for Hyperparameter  Optimization</h3>
<p><a href='http://arxiv.org/abs/2402.13641v1'>http://arxiv.org/abs/2402.13641v1</a></p>
<p><b>Compressor summary</b>: The paper proposes FlexHB, a new multi-fidelity Bayesian Optimization method that improves efficiency and performance on Hyperparameter Optimization tasks by integrating fine-grained fidelity and FlexBand frameworks.</p><hr><h3>A Unified Framework and Dataset for Assessing Gender Bias in  Vision-Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.13636v1'>http://arxiv.org/abs/2402.13636v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates gender-profession bias in various vision-language models by using a synthetic dataset of blurred gender distinctions across professional actions.</p><hr><h3>The METRIC-framework for assessing data quality for trustworthy AI in  medicine: a systematic review</h3>
<p><a href='http://arxiv.org/abs/2402.13635v1'>http://arxiv.org/abs/2402.13635v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The text discusses the importance of data quality for trustworthy AI in medicine and proposes a framework for evaluating it.
- Data quality affects aspects such as fairness, robustness, interpretability and ethical implications.
- The framework is based on a systematic review of 62 studies and covers 15 dimensions.

Summary:
The authors propose a data quality framework for medical AI applications that helps to ensure trustworthiness by addressing issues such as fairness, robustness, interpretability and ethics.</p><hr><h3>Delving into Dark Regions for Robust Shadow Detection</h3>
<p><a href='http://arxiv.org/abs/2402.13631v1'>http://arxiv.org/abs/2402.13631v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel shadow detection approach that focuses on low-intensity regions to learn better discriminative features and outperforms existing methods.</p><hr><h3>UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural  Language</h3>
<p><a href='http://arxiv.org/abs/2402.13630v1'>http://arxiv.org/abs/2402.13630v1</a></p>
<p><b>Compressor summary</b>: The UniGraph framework trains a graph foundation model to generalize across diverse domains using Text-Attributed Graphs, a cascaded LM and GNN architecture, and large language models for tuning and prediction.</p><hr><h3>Improving Building Temperature Forecasting: A Data-driven Approach with  System Scenario Clustering</h3>
<p><a href='http://arxiv.org/abs/2402.13628v1'>http://arxiv.org/abs/2402.13628v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new data-driven method for predicting room temperature in HVAC systems using k-means clustering, which simplifies the system model and reduces modeling time while maintaining prediction accuracy.</p><hr><h3>MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning</h3>
<p><a href='http://arxiv.org/abs/2402.13625v1'>http://arxiv.org/abs/2402.13625v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework that uses both text and images to improve language models' commonsense abilities.</p><hr><h3>FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.13623v1'>http://arxiv.org/abs/2402.13623v1</a></p>
<p><b>Compressor summary</b>: FLAME is a novel approach that uses large language models and few-shot prompting to automatically expand taxonomies in low-resource environments, improving accuracy and overcoming challenges faced by traditional supervised methods.</p><hr><h3>YOLOv9: Learning What You Want to Learn Using Programmable Gradient  Information</h3>
<p><a href='http://arxiv.org/abs/2402.13616v1'>http://arxiv.org/abs/2402.13616v1</a></p>
<p><b>Compressor summary</b>: This paper proposes PGI and GELAN to address data loss in deep learning, improving performance on object detection tasks.</p><hr><h3>Analyizing the Conjunction Fallacy as a Fact</h3>
<p><a href='http://arxiv.org/abs/2402.13615v1'>http://arxiv.org/abs/2402.13615v1</a></p>
<p><b>Compressor summary</b>: This article examines the range of factual possibilities for the conjunction fallacy, finding that most research has focused on a narrow part of them, potentially biasing explanations of this cognitive phenomenon.</p><hr><h3>Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for  Comparative Opinion Mining from Vietnamese Product Reviews</h3>
<p><a href='http://arxiv.org/abs/2402.13613v1'>http://arxiv.org/abs/2402.13613v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a shared task on extracting comparative opinions from Vietnamese product reviews, with a human-annotated dataset and an evaluation metric.</p><hr><h3>Data-driven Discovery with Large Generative Models</h3>
<p><a href='http://arxiv.org/abs/2402.13610v1'>http://arxiv.org/abs/2402.13610v1</a></p>
<p><b>Compressor summary</b>: The authors propose using large generative models to develop automated systems for end-to-end data-driven discovery, but argue that current LGMs are not yet sufficient and suggest integrating them with feedback mechanisms for better results.</p><hr><h3>CODIS: Benchmarking Context-Dependent Visual Comprehension for  Multimodal Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.13607v1'>http://arxiv.org/abs/2402.13607v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CODIS, a new benchmark to evaluate multimodal language models' ability to use textual context to improve their image understanding, which they find MLLMs perform poorly on compared to humans.</p><hr><h3>A Comprehensive Study of Multilingual Confidence Estimation on Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.13606v1'>http://arxiv.org/abs/2402.13606v1</a></p>
<p><b>Compressor summary</b>: The paper investigates multi-lingual confidence estimation for large language models, introducing a new dataset, examining performance, and proposing a cross-lingual method that improves reliability.</p><hr><h3>KorNAT: LLM Alignment Benchmark for Korean Social Values and Common  Knowledge</h3>
<p><a href='http://arxiv.org/abs/2402.13605v1'>http://arxiv.org/abs/2402.13605v1</a></p>
<p><b>Compressor summary</b>: National Alignment is a measure for large language models to understand a country's culture and basic knowledge, with KorNAT being the first benchmark for South Korea.</p><hr><h3>Breaking the HISCO Barrier: Automatic Occupational Standardization with  OccCANINE</h3>
<p><a href='http://arxiv.org/abs/2402.13604v1'>http://arxiv.org/abs/2402.13604v1</a></p>
<p><b>Compressor summary</b>: OccCANINE is a tool that automates the process of classifying occupations into HISCO codes, improving speed and accuracy and enabling new research possibilities.</p><hr><h3>Hybrid Reasoning Based on Large Language Models for Autonomous Car  Driving</h3>
<p><a href='http://arxiv.org/abs/2402.13602v1'>http://arxiv.org/abs/2402.13602v1</a></p>
<p><b>Compressor summary</b>: The study explores how large language models can improve autonomous driving by combining natural language text, arithmetic reasoning, and common sense in dynamic situations like low visibility.</p><hr><h3>User-LLM: Efficient LLM Contextualization with User Embeddings</h3>
<p><a href='http://arxiv.org/abs/2402.13598v1'>http://arxiv.org/abs/2402.13598v1</a></p>
<p><b>Compressor summary</b>: User-LLM is a novel framework that uses user embeddings to contextualize large language models for various natural language processing tasks, achieving significant performance gains and efficient computation.</p><hr><h3>Knowledge Graph Enhanced Large Language Model Editing</h3>
<p><a href='http://arxiv.org/abs/2402.13593v1'>http://arxiv.org/abs/2402.13593v1</a></p>
<p><b>Compressor summary</b>: GLAME is a novel method that uses knowledge graphs to enhance large language models by tracking and incorporating changes in knowledge during editing, improving their generalization ability.</p><hr><h3>A Multimodal In-Context Tuning Approach for E-Commerce Product  Description Generation</h3>
<p><a href='http://arxiv.org/abs/2402.13587v1'>http://arxiv.org/abs/2402.13587v1</a></p>
<p><b>Compressor summary</b>: The paper proposes ModICT, a method that uses in-context learning with a similar product sample and dynamic prompts to improve the accuracy and diversity of generating product descriptions from images and keywords.</p><hr><h3>WinoViz: Probing Visual Properties of Objects Under Different States</h3>
<p><a href='http://arxiv.org/abs/2402.13584v1'>http://arxiv.org/abs/2402.13584v1</a></p>
<p><b>Compressor summary</b>: WinoViz evaluates language models' reasoning abilities on variant visual properties of objects across different contexts, finding that large vision-language models outperform language models and image-generating models.</p><hr><h3>LongWanjuan: Towards Systematic Measurement for Long Text Quality</h3>
<p><a href='http://arxiv.org/abs/2402.13583v1'>http://arxiv.org/abs/2402.13583v1</a></p>
<p><b>Compressor summary</b>: Our work introduces a systematic approach and metrics to evaluate the quality of long texts and presents LongWanjuan, a bilingual dataset for training language models on long-text tasks.</p><hr><h3>Mastering the Game of Guandan with Deep Reinforcement Learning and  Behavior Regulating</h3>
<p><a href='http://arxiv.org/abs/2402.13582v1'>http://arxiv.org/abs/2402.13582v1</a></p>
<p><b>Compressor summary</b>: The paper proposes GuanZero, a framework for AI agents to learn and master the challenging game of Guandan using Monte-Carlo methods and deep neural networks with a novel encoding scheme.</p><hr><h3>Learning Pixel-wise Continuous Depth Representation via Clustering for  Depth Completion</h3>
<p><a href='http://arxiv.org/abs/2402.13579v1'>http://arxiv.org/abs/2402.13579v1</a></p>
<p><b>Compressor summary</b>: CluDe is a novel clustering-based framework for depth completion that learns pixel-wise and continuous depth representation, reducing depth smearing around object boundaries.</p><hr><h3>TransGOP: Transformer-Based Gaze Object Prediction</h3>
<p><a href='http://arxiv.org/abs/2402.13578v1'>http://arxiv.org/abs/2402.13578v1</a></p>
<p><b>Compressor summary</b>: This paper introduces Transformer to improve gaze object prediction by using a Transformer-based object detector and a Transformer-based gaze autoencoder with an object-to-gaze cross-attention mechanism, achieving state-of-the-art results.</p><hr><h3>BBA: Bi-Modal Behavioral Alignment for Reasoning with Large  Vision-Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.13577v1'>http://arxiv.org/abs/2402.13577v1</a></p>
<p><b>Compressor summary</b>: BBA is a prompting method that improves LVLMs' multi-modal reasoning by aligning separate reasoning chains for visual and DSL representations, leading to better performance on geometry, chess, and molecular problems.</p><hr><h3>Improving Video Corpus Moment Retrieval with Partial Relevance  Enhancement</h3>
<p><a href='http://arxiv.org/abs/2402.13576v1'>http://arxiv.org/abs/2402.13576v1</a></p>
<p><b>Compressor summary</b>: The text introduces PREM, a model for video corpus moment retrieval, which captures partial relevance between query and video using multi-modal collaborative retrievers and focus-then-fuse moment localizers.</p><hr><h3>Flexible Physical Camouflage Generation Based on a Differential Approach</h3>
<p><a href='http://arxiv.org/abs/2402.13575v1'>http://arxiv.org/abs/2402.13575v1</a></p>
<p><b>Compressor summary</b>: The study presents FPA, a novel neural rendering method for realistic adversarial camouflage that simulates lighting and material variations using a generative approach and achieves high attack success rate and transferability.</p><hr><h3>ToDo: Token Downsampling for Efficient Generation of High-Resolution  Images</h3>
<p><a href='http://arxiv.org/abs/2402.13573v1'>http://arxiv.org/abs/2402.13573v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a method to speed up image diffusion models by using less attention, achieving better trade-off between efficiency and quality.</p><hr><h3>On the Expressive Power of a Variant of the Looped Transformer</h3>
<p><a href='http://arxiv.org/abs/2402.13572v1'>http://arxiv.org/abs/2402.13572v1</a></p>
<p><b>Compressor summary</b>: The Algorithm Transformer (AlgoFormer) is a novel transformer block that achieves higher expressiveness in algorithm representation, enabling it to potentially outperform human-designed algorithms in challenging tasks.</p><hr><h3>Multilingual Coreference Resolution in Low-resource South Asian  Languages</h3>
<p><a href='http://arxiv.org/abs/2402.13571v1'>http://arxiv.org/abs/2402.13571v1</a></p>
<p><b>Compressor summary</b>: The paper introduces TransMuCoRes, a multilingual coreference resolution dataset in 31 South Asian languages created using translated texts and off-the-shelf tools, and evaluates two models on it.</p><hr><h3>Spot Check Equivalence: an Interpretable Metric for Information  Elicitation Mechanisms</h3>
<p><a href='http://arxiv.org/abs/2402.13567v1'>http://arxiv.org/abs/2402.13567v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a unified metric, spot check equivalence, to compare and evaluate the performance of different mechanisms for eliciting high-quality data from human labelers for AI systems.</p><hr><h3>Event-aware Video Corpus Moment Retrieval</h3>
<p><a href='http://arxiv.org/abs/2402.13566v1'>http://arxiv.org/abs/2402.13566v1</a></p>
<p><b>Compressor summary</b>: EventFormer is a model that uses events within videos to retrieve specific moments using natural language queries and outperforms existing methods in video corpus moment retrieval tasks.</p><hr><h3>Analysis of Multi-Source Language Training in Cross-Lingual Transfer</h3>
<p><a href='http://arxiv.org/abs/2402.13562v1'>http://arxiv.org/abs/2402.13562v1</a></p>
<p><b>Compressor summary</b>: The paper explores how multiple source languages in cross-lingual transfer affect multilingual language models' feature emphasis and suggests heuristics to identify effective language combinations.</p><hr><h3>Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension  with Enhanced Visual Knowledge Alignment</h3>
<p><a href='http://arxiv.org/abs/2402.13561v1'>http://arxiv.org/abs/2402.13561v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a Cognitive Visual-Language Mapper to improve large multimodal models for answering knowledge-based visual questions by aligning visuals with relevant knowledge.</p><hr><h3>Inductive Graph Alignment Prompt: Bridging the Gap between Graph  Pre-training and Inductive Fine-tuning From Spectral Perspective</h3>
<p><a href='http://arxiv.org/abs/2402.13556v1'>http://arxiv.org/abs/2402.13556v1</a></p>
<p><b>Compressor summary</b>: IGAP is a novel method that uses learnable prompts in the spectral space to align graphs with different structures and signals for inductive graph pre-training and fine-tuning.</p><hr><h3>Graph Representation of Narrative Context: Coherence Dependency via  Retrospective Questions</h3>
<p><a href='http://arxiv.org/abs/2402.13551v1'>http://arxiv.org/abs/2402.13551v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a graph called NARCO that captures coherence dependencies in narratives using retrospective questions and shows its usefulness for various tasks without human annotations.</p><hr><h3>Are LLMs Effective Negotiators? Systematic Evaluation of the  Multifaceted Capabilities of LLMs in Negotiation Dialogues</h3>
<p><a href='http://arxiv.org/abs/2402.13550v1'>http://arxiv.org/abs/2402.13550v1</a></p>
<p><b>Compressor summary</b>: The text discusses using large language models (LLMs) like GPT-4 to advance negotiation research, analyze their multifaceted capabilities in different scenarios, and identify areas where they still struggle, such as subjective assessments and contextually appropriate responses.</p><hr><h3>DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of  EV Charging Load</h3>
<p><a href='http://arxiv.org/abs/2402.13548v1'>http://arxiv.org/abs/2402.13548v1</a></p>
<p><b>Compressor summary</b>: The text proposes a novel diffusion model called DiffPLF for probabilistic forecasting of electric vehicle charging load, which can handle volatile patterns and use covariates for better prediction.</p><hr><h3>ActiveRAG: Revealing the Treasures of Knowledge via Active Learning</h3>
<p><a href='http://arxiv.org/abs/2402.13547v1'>http://arxiv.org/abs/2402.13547v1</a></p>
<p><b>Compressor summary</b>: ActiveRAG is an improved Retrieval Augmented Generation framework that uses active learning to enhance Large Language Models' understanding of external knowledge, leading to better question-answering performance.</p><hr><h3>LLMs Meet Long Video: Advancing Long Video Comprehension with An  Interactive Visual Adapter in LLMs</h3>
<p><a href='http://arxiv.org/abs/2402.13546v1'>http://arxiv.org/abs/2402.13546v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an Interactive Visual Adapter (IVA) to improve the understanding of long videos by enhancing interaction with fine-grained visual elements within large language models.</p><hr><h3>A Two-Stage Dual-Path Framework for Text Tampering Detection and  Recognition</h3>
<p><a href='http://arxiv.org/abs/2402.13545v1'>http://arxiv.org/abs/2402.13545v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a deep learning method for text tamper detection with three steps: feature assistance, audit point positioning, and tamper recognition.
- The method simulates and augments data samples with artificial tamper data features in various scenarios.
- The method achieves high accuracy, recall, and precision using a dual-path dual-stream recognition network and vlad.

Summary:
The paper presents a deep learning text tamper detection method that uses feature assistance, audit point positioning, and tamper recognition, and simulates data with artificial tamper features, achieving high performance.</p><hr><h3>ARL2: Aligning Retrievers for Black-box Large Language Models via  Self-guided Adaptive Relevance Labeling</h3>
<p><a href='http://arxiv.org/abs/2402.13542v1'>http://arxiv.org/abs/2402.13542v1</a></p>
<p><b>Compressor summary</b>: ARL2 is a technique that improves large language models by using them to label and score relevant evidence, leading to better retrieval-augmented generation.</p><hr><h3>EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera  Relocalization</h3>
<p><a href='http://arxiv.org/abs/2402.13537v1'>http://arxiv.org/abs/2402.13537v1</a></p>
<p><b>Compressor summary</b>: EffLoc is an efficient Vision Transformer for single-image camera relocalization using self-attention, sequential group attention, and feed-forward layers.</p><hr><h3>Exploring the Limits of Semantic Image Compression at Micro-bits per  Pixel</h3>
<p><a href='http://arxiv.org/abs/2402.13536v1'>http://arxiv.org/abs/2402.13536v1</a></p>
<p><b>Compressor summary</b>: The text explores using AI models like GPT-4V and DALL-E3 for semantic image compression that uses natural language to store concepts, achieving extremely low bitrates by ignoring structural information, and introduces an iterative reflection process to improve decoded images.</p><hr><h3>An Effective Incorporating Heterogeneous Knowledge Curriculum Learning  for Sequence Labeling</h3>
<p><a href='http://arxiv.org/abs/2402.13534v1'>http://arxiv.org/abs/2402.13534v1</a></p>
<p><b>Compressor summary</b>: The text proposes a two-stage curriculum learning framework for sequence labeling tasks that improves performance, training speed, and handles data heterogeneity.</p><hr><h3>FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models  for Financial Applications with High-Performance Computing</h3>
<p><a href='http://arxiv.org/abs/2402.13533v1'>http://arxiv.org/abs/2402.13533v1</a></p>
<p><b>Compressor summary</b>: The paper presents efficient GPU-based methods to pretrain and finetune large language models for financial applications by using low-rank structures, replacing linear layers, and quantizing parameters, achieving significant speedup, compression, and accuracy improvements.</p><hr><h3>Backdoor Attacks on Dense Passage Retrievers for Disseminating  Misinformation</h3>
<p><a href='http://arxiv.org/abs/2402.13532v1'>http://arxiv.org/abs/2402.13532v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a backdoor attack on dense passage retrieval systems using grammar errors to disseminate targeted misinformation.</p><hr><h3>Private Gradient Descent for Linear Regression: Tighter Error Bounds and  Instance-Specific Uncertainty Estimation</h3>
<p><a href='http://arxiv.org/abs/2402.13531v1'>http://arxiv.org/abs/2402.13531v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes differentially private gradient descent for linear regression, showing its accuracy and sample complexity match those of non-private methods and allowing confidence intervals for the empirical optimizer.</p><hr><h3>MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via  Automating Deep Neural Network Porting for Mobile Deployment</h3>
<p><a href='http://arxiv.org/abs/2402.13525v1'>http://arxiv.org/abs/2402.13525v1</a></p>
<p><b>Compressor summary</b>: The paper proposes MatchNAS, a method to adapt deep neural networks for mobile devices using both labelled and unlabelled data, without manual re-specialization or retraining.</p><hr><h3>OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.13524v1'>http://arxiv.org/abs/2402.13524v1</a></p>
<p><b>Compressor summary</b>: OMGEval is an open-source test set for assessing multilingual large language models' capabilities in various languages, covering general knowledge and logical reasoning.</p><hr><h3>RecMind: Japanese Movie Recommendation Dialogue with Seeker's Internal  State</h3>
<p><a href='http://arxiv.org/abs/2402.13522v1'>http://arxiv.org/abs/2402.13522v1</a></p>
<p><b>Compressor summary</b>: The paper introduces RecMind, a dataset for analyzing seeker's internal state in Japanese movie recommendation dialogues, and proposes a response generation framework based on it.</p><hr><h3>Round Trip Translation Defence against Large Language Model Jailbreaking  Attacks</h3>
<p><a href='http://arxiv.org/abs/2402.13517v1'>http://arxiv.org/abs/2402.13517v1</a></p>
<p><b>Compressor summary</b>: The Round Trip Translation method defends large language models against social-engineered attacks by paraphrasing adversarial prompts and making them easier for LLMs to detect.</p><hr><h3>ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity  within Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.13516v1'>http://arxiv.org/abs/2402.13516v1</a></p>
<p><b>Compressor summary</b>: ProSparse is a method that introduces ReLU activation function in large language models and uses progressive sparsity regularization to achieve high activation sparsity without compromising model performance or inference speed.</p><hr><h3>Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer  for Compositional Unknown Questions</h3>
<p><a href='http://arxiv.org/abs/2402.13514v1'>http://arxiv.org/abs/2402.13514v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CuQA, a dataset for compositional unknown questions in open-domain question-answering, and proposes a Self Divide-and-Conquer framework to improve efficiency and performance by adaptively calling different methods.</p><hr><h3>From Self-Attention to Markov Models: Unveiling the Dynamics of  Generative Transformers</h3>
<p><a href='http://arxiv.org/abs/2402.13512v1'>http://arxiv.org/abs/2402.13512v1</a></p>
<p><b>Compressor summary</b>: This paper shows how self-attention in language models is equivalent to context-conditioned Markov chains and explores their properties and limitations in text generation.</p><hr><h3>SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural  Radiance Fields</h3>
<p><a href='http://arxiv.org/abs/2402.13510v1'>http://arxiv.org/abs/2402.13510v1</a></p>
<p><b>Compressor summary</b>: SealD-NeRF is a method that enables pixel-level editing in dynamic NeRF scenes by mapping edits to a specific timeframe and using a teacher-student approach.</p><hr><h3>SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed  Semi-Supervised Learning</h3>
<p><a href='http://arxiv.org/abs/2402.13505v1'>http://arxiv.org/abs/2402.13505v1</a></p>
<p><b>Compressor summary</b>: The SimPro framework is a novel, highly adaptable semi-supervised learning approach that does not rely on predefined assumptions about unlabeled data class distribution and improves pseudo-label quality using a refined EM algorithm and a Bayes classifier.</p><hr><h3>The Lay Person's Guide to Biomedicine: Orchestrating Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2402.13498v1'>http://arxiv.org/abs/2402.13498v1</a></p>
<p><b>Compressor summary</b>: The text explores using large language models (LLMs) for automated lay summarization (LS) of biomedical articles, proposing a novel framework and evaluation metrics that leverage LLMs' abilities to generate background knowledge and assess layness.</p><hr><h3>Push Quantization-Aware Training Toward Full Precision Performances via  Consistency Regularization</h3>
<p><a href='http://arxiv.org/abs/2402.13497v1'>http://arxiv.org/abs/2402.13497v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new Consistency Regularization method for Quantization-Aware Training, which uses vicinal data distribution information to improve generalization and achieve better results than existing QAT methods and Full Precision counterparts.</p><hr><h3>HetTree: Heterogeneous Tree Graph Neural Network</h3>
<p><a href='http://arxiv.org/abs/2402.13496v1'>http://arxiv.org/abs/2402.13496v1</a></p>
<p><b>Compressor summary</b>: HetTree is a novel heterogeneous tree graph neural network that models the hierarchy among metapaths, captures parent-children relationships using subtree attention, and matches node features and labels based on corresponding metapaths for accurate and scalable graph analysis.</p><hr><h3>GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient  Analysis</h3>
<p><a href='http://arxiv.org/abs/2402.13494v1'>http://arxiv.org/abs/2402.13494v1</a></p>
<p><b>Compressor summary</b>: GradSafe detects unsafe prompts in LLMs by analyzing the gradients of safety-critical parameters, outperforming existing methods like Llama Guard.</p>