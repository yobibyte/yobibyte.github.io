
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<link rel="stylesheet" href="../style.css"/>
		<title>Welcome to yobihome</title>
		<a href="https://yobibyte.github.io/"><img src=".././pics/socrat.png" class="center"></a>
		<h1>arxiv compressed, 2023-11-24</h1>
		<p>This page contains one-sentence summaries of cs.AI/ML/CV papers announced on 2023-11-24 generated by the compressor, my personal LLM-based project.</p>
	<hr><h3>SEGIC: Unleashing the Emergent Correspondence for In-Context  Segmentation</h3>
<p>Lingchen Meng,Shiyi Lan,Hengduo Li,Jose M. Alvarez,Zuxuan Wu,Yu-Gang Jiang</p>
<p><a href='http://arxiv.org/abs/2311.14671v1'>http://arxiv.org/abs/2311.14671v1</a></p>
<p><b>Compressor summary</b>: SEGIC is an end-to-end framework that uses a single vision foundation model to learn segmentation rules from few labeled examples and achieve state-of-the-art performance on one-shot segmentation tasks.</p><hr><h3>Understanding Self-Supervised Features for Learning Unsupervised  Instance Segmentation</h3>
<p>Paul Engstler,Luke Melas-Kyriazi,Christian Rupprecht,Iro Laina</p>
<p><a href='http://arxiv.org/abs/2311.14665v1'>http://arxiv.org/abs/2311.14665v1</a></p>
<p><b>Compressor summary</b>: The paper explores how self-supervised learning methods can be used for instance segmentation without manual annotations and compares different methods based on their ability to separate instances.</p><hr><h3>Convergence Analysis for Learning Orthonormal Deep Linear Neural  Networks</h3>
<p>Zhen Qin,Xuwei Tan,Zhihui Zhu</p>
<p><a href='http://arxiv.org/abs/2311.14658v1'>http://arxiv.org/abs/2311.14658v1</a></p>
<p><b>Compressor summary</b>: The text discusses the benefits and challenges of using orthonormal weight matrices in deep neural networks, and presents a new approach to analyze their convergence with linear gradient descent and a class of loss functions.</p><hr><h3>Charting New Territories: Exploring the Geographic and Geospatial  Capabilities of Multimodal LLMs</h3>
<p>Jonathan Roberts,Timo Lüddecke,Rehan Sheikh,Kai Han,Samuel Albanie</p>
<p><a href='http://arxiv.org/abs/2311.14656v1'>http://arxiv.org/abs/2311.14656v1</a></p>
<p><b>Compressor summary</b>: The paper explores how well large language models can perform geographic tasks and evaluates GPT-4V's performance on a new visual benchmark.</p><hr><h3>Data-driven Prior Learning for Bayesian Optimisation</h3>
<p>Sigrid Passano Hellan,Christopher G. Lucas,Nigel H. Goddard</p>
<p><a href='http://arxiv.org/abs/2311.14653v1'>http://arxiv.org/abs/2311.14653v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a weaker assumption for transfer learning in Bayesian optimization, analyses the method Prior Learning for Bayesian Optimization (PLeBO), and shows its effectiveness using synthetic and real-world data.</p><hr><h3>One Pass Streaming Algorithm for Super Long Token Attention  Approximation in Sublinear Space</h3>
<p>Raghav Addanki,Chenyang Li,Zhao Song,Chiwun Yang</p>
<p><a href='http://arxiv.org/abs/2311.14652v1'>http://arxiv.org/abs/2311.14652v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new algorithm that uses sublinear space to store Key and Value matrices for large language models in streaming applications, improving memory efficiency.</p><hr><h3>Learning in Deep Factor Graphs with Gaussian Belief Propagation</h3>
<p>Seth Nabarro,Mark van der Wilk,Andrew J Davison</p>
<p><a href='http://arxiv.org/abs/2311.14649v1'>http://arxiv.org/abs/2311.14649v1</a></p>
<p><b>Compressor summary</b>: The authors propose an efficient method to train and predict using Gaussian factor graphs, which can handle large-scale problems and enable continual learning.</p><hr><h3>More is Better in Modern Machine Learning: when Infinite  Overparameterization is Optimal and Overfitting is Obligatory</h3>
<p>James B. Simon,Dhruva Karkada,Nikhil Ghosh,Mikhail Belkin</p>
<p><a href='http://arxiv.org/abs/2311.14646v1'>http://arxiv.org/abs/2311.14646v1</a></p>
<p><b>Compressor summary</b>: The paper provides theoretical support for the idea that larger models, more data, and more computation improve performance in random feature regression models, which are equivalent to shallow neural networks with only the last layer trained.</p><hr><h3>A General Framework for User-Guided Bayesian Optimization</h3>
<p>Carl Hvarfner,Frank Hutter,Luigi Nardi</p>
<p><a href='http://arxiv.org/abs/2311.14645v1'>http://arxiv.org/abs/2311.14645v1</a></p>
<p><b>Compressor summary</b>: ColaBO is a Bayesian optimization framework that allows domain experts to customize the optimization routine by incorporating their prior beliefs about the function being optimized.</p><hr><h3>Continuous football player tracking from discrete broadcast data</h3>
<p>Matthew J. Penn,Christl A. Donnelly,Samir Bhatt</p>
<p><a href='http://arxiv.org/abs/2311.14642v1'>http://arxiv.org/abs/2311.14642v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to estimate continuous full-pitch player tracking data from broadcast footage, which could be affordable for many football teams.</p><hr><h3>Automated Detection and Counting of Windows using UAV Imagery based  Remote Sensing</h3>
<p>Dhruv Patel,Shivani Chepuri,Sarvesh Thakur,K. Harikumar,Ravi Kiran S.,K. Madhava Krishna</p>
<p><a href='http://arxiv.org/abs/2311.14635v1'>http://arxiv.org/abs/2311.14635v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to use UAVs and computer vision to automatically count windows in buildings for earthquake analysis.</p><hr><h3>One Strike, You're Out: Detecting Markush Structures in Low  Signal-to-Noise Ratio Images</h3>
<p>Thomas Jurriaans,Kinga Szarkowska,Eric Nalisnick,Markus Schwoerer,Camilo Thorne,Saber Akhondi</p>
<p><a href='http://arxiv.org/abs/2311.14633v1'>http://arxiv.org/abs/2311.14633v1</a></p>
<p><b>Compressor summary</b>: This paragraph discusses a novel method for classifying Markush chemical structures using end-to-end learning (CNN), which significantly outperforms fixed-feature extraction and has the potential to improve OCSR pipelines.</p><hr><h3>Differentially Private SGD Without Clipping Bias: An Error-Feedback  Approach</h3>
<p>Xinwei Zhang,Zhiqi Bu,Zhiwei Steven Wu,Mingyi Hong</p>
<p><a href='http://arxiv.org/abs/2311.14632v1'>http://arxiv.org/abs/2311.14632v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an error-feedback differential privacy algorithm for training deep learning models that reduces the constant bias from gradient clipping and provides better performance and privacy guarantees.</p><hr><h3>CatVersion: Concatenating Embeddings for Diffusion-Based Text-to-Image  Personalization</h3>
<p>Ruoyu Zhao,Mingrui Zhu,Shiyin Dong,Nannan Wang,Xinbo Gao</p>
<p><a href='http://arxiv.org/abs/2311.14631v1'>http://arxiv.org/abs/2311.14631v1</a></p>
<p><b>Compressor summary</b>: CatVersion is a text-to-image method that learns a personalized concept from few examples, preserves prior knowledge in diffusion models, and improves image alignment scores for better editing.</p><hr><h3>Neural Style Transfer for Computer Games</h3>
<p>Eleftherios Ioannou,Steve Maddock</p>
<p><a href='http://arxiv.org/abs/2311.14617v1'>http://arxiv.org/abs/2311.14617v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method for applying depth-aware Neural Style Transfer to 3D computer games in real-time, resulting in high-quality stylized scenes that surpass existing image and video NST techniques.</p><hr><h3>Animate124: Animating One Image to 4D Dynamic Scene</h3>
<p>Yuyang Zhao,Zhiwen Yan,Enze Xie,Lanqing Hong,Zhenguo Li,Gim Hee Lee</p>
<p><a href='http://arxiv.org/abs/2311.14603v1'>http://arxiv.org/abs/2311.14603v1</a></p>
<p><b>Compressor summary</b>: Animate124 is a new method that can animate a single image into 3D video using textual descriptions and a neural model with multiple diffusion priors to address semantic drift.</p><hr><h3>A Metalearned Neural Circuit for Nonparametric Bayesian Inference</h3>
<p>Jake C. Snell,Gianluca Bencomo,Thomas L. Griffiths</p>
<p><a href='http://arxiv.org/abs/2311.14601v1'>http://arxiv.org/abs/2311.14601v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method to transfer the inductive bias of nonparametical Bayesian models to neural networks, allowing them to handle long-tailed class distributions and perform sequential inference over an open set of classes efficiently.</p><hr><h3>Example-Based Explanations of Random Forest Predictions</h3>
<p>Henrik Boström</p>
<p><a href='http://arxiv.org/abs/2311.14581v1'>http://arxiv.org/abs/2311.14581v1</a></p>
<p><b>Compressor summary</b>: The text describes a method for explaining random forest predictions by using a subset of training examples, which can reduce the number of examples and improve the explanations' usefulness.</p><hr><h3>Large Language Models as Automated Aligners for benchmarking  Vision-Language Models</h3>
<p>Yuanfeng Ji,Chongjian Ge,Weikai Kong,Enze Xie,Zhengying Liu,Zhengguo Li,Ping Luo</p>
<p><a href='http://arxiv.org/abs/2311.14580v1'>http://arxiv.org/abs/2311.14580v1</a></p>
<p><b>Compressor summary</b>: Auto-Bench is a new benchmark that uses large language models to create question-answer-reasoning tasks to evaluate vision-language models' alignment with human intelligence.</p><hr><h3>RAISE -- Radiology AI Safety, an End-to-end lifecycle approach</h3>
<p>M. Jorge Cardoso,Julia Moosbauer,Tessa S. Cook,B. Selnur Erdal,Brad Genereaux,Vikash Gupta,Bennett A. Landman,Tiarna Lee,Parashkev Nachev,Elanchezhian Somasundaram,Ronald M. Summers,Khaled Younis,Sebastien Ourselin,Franz MJ Pfister</p>
<p><a href='http://arxiv.org/abs/2311.14570v1'>http://arxiv.org/abs/2311.14570v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses the importance of rigorous evaluation, safety, effectiveness, and collaboration for integrating AI into radiology to achieve its potential benefits while addressing risks and challenges.</p><hr><h3>Electric Vehicles coordination for grid balancing using multi-objective  Harris Hawks Optimization</h3>
<p>Cristina Bianca Pop,Tudor Cioara,Viorica Chifu,Ionut Anghel,Francesco Bellesini</p>
<p><a href='http://arxiv.org/abs/2311.14563v1'>http://arxiv.org/abs/2311.14563v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a model for coordinating electric vehicles (EVs) charging and discharging to balance the local grid, using Harris Hawks Optimization (HHO) to optimize schedules based on energy, time, and location criteria.</p><hr><h3>Griffon: Spelling out All Object Locations at Any Granularity with Large  Language Models</h3>
<p>Yufei Zhan,Yousong Zhu,Zhiyang Chen,Fan Yang,Ming Tang,Jinqiao Wang</p>
<p><a href='http://arxiv.org/abs/2311.14552v1'>http://arxiv.org/abs/2311.14552v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel dataset and a baseline model, $\textbf{Griffon}$, that shows LVLMs can perform fine-grained object perception and location awareness without additional modules or expert models.</p><hr><h3>Inferring Latent Class Statistics from Text for Robust Visual Few-Shot  Learning</h3>
<p>Yassir Bendou,Vincent Gripon,Bastien Pasdeloup,Giulia Lioi,Lukas Mauch,Fabien Cardinaux,Ghouthi Boukli Hacene</p>
<p><a href='http://arxiv.org/abs/2311.14544v1'>http://arxiv.org/abs/2311.14544v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel approach using text to predict mean and covariance statistics of visual features for each class, improving few-shot learning robustness and generalizability.</p><hr><h3>ToddlerDiffusion: Flash Interpretable Controllable Diffusion Model</h3>
<p>Eslam Mohamed Bakr,Liangbing Zhao,Vincent Tao Hu,Matthieu Cord,Patrick Perez,Mohamed Elhoseiny</p>
<p><a href='http://arxiv.org/abs/2311.14542v1'>http://arxiv.org/abs/2311.14542v1</a></p>
<p><b>Compressor summary</b>: ToddlerDiffusion is an interpretable image synthesis framework that generates contours, palettes, and detailed colored images, outperforming existing methods while being faster and more efficient.</p><hr><h3>Finding Foundation Models for Time Series Classification with a PreText  Task</h3>
<p>Ali Ismail-Fawaz,Maxime Devanne,Stefano Berretti,Jonathan Weber,Germain Forestier</p>
<p><a href='http://arxiv.org/abs/2311.14534v1'>http://arxiv.org/abs/2311.14534v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to reduce overfitting in Time Series Classification using pre-trained domain foundation models that can identify the originating dataset of each sample and apply flexible convolution filters across different datasets.</p><hr><h3>Comparing Feature Engineering and End-to-End Deep Learning for Autism  Spectrum Disorder Assessment based on Fullbody-Tracking</h3>
<p>Alberto Altozano,Maria Eleonora Minissi,Mariano Alcañiz,Javier Marín-Morales</p>
<p><a href='http://arxiv.org/abs/2311.14533v1'>http://arxiv.org/abs/2311.14533v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses a study comparing end-to-end models and hand-crafted features for autism spectrum disorder assessment using virtual reality tasks, finding that both methods have strengths and weaknesses.</p><hr><h3>GaussianEditor: Swift and Controllable 3D Editing with Gaussian  Splatting</h3>
<p>Yiwen Chen,Zilong Chen,Chi Zhang,Feng Wang,Xiaofeng Yang,Yikai Wang,Zhongang Cai,Lei Yang,Huaping Liu,Guosheng Lin</p>
<p><a href='http://arxiv.org/abs/2311.14521v1'>http://arxiv.org/abs/2311.14521v1</a></p>
<p><b>Compressor summary</b>: GaussianEditor is an efficient 3D editing algorithm based on Gaussian Splatting that improves precision, control, and performance in complex scenes using novel techniques like semantic tracing and hierarchical splatting.</p><hr><h3>Multi-Class Anomaly Detection based on Regularized Discriminative  Coupled hypersphere-based Feature Adaptation</h3>
<p>Mehdi Rafiei,Alexandros Iosifidis</p>
<p><a href='http://arxiv.org/abs/2311.14506v1'>http://arxiv.org/abs/2311.14506v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new model for multi-class anomaly detection that combines a modified Regularized Discriminative Variational Auto-Encoder (RD-VAE) with Coupled-hypersphere-based Feature Adaptation (CFA), achieving better results than eight existing methods.</p><hr><h3>StableSSM: Alleviating the Curse of Memory in State-space Models through  Stable Reparameterization</h3>
<p>Shida Wang,Qianxiao Li</p>
<p><a href='http://arxiv.org/abs/2311.14495v1'>http://arxiv.org/abs/2311.14495v1</a></p>
<p><b>Compressor summary</b>: The paper explores how different parameterizations affect the long-term memory learning abilities of state-space models and introduces new techniques to improve their performance.</p><hr><h3>MVControl: Adding Conditional Control to Multi-view Diffusion for  Controllable Text-to-3D Generation</h3>
<p>Zhiqi Li,Yiming Chen,Lingzhe Zhao,Peidong Liu</p>
<p><a href='http://arxiv.org/abs/2311.14494v1'>http://arxiv.org/abs/2311.14494v1</a></p>
<p><b>Compressor summary</b>: MVControl is a new neural network architecture that improves multi-view image generation by incorporating extra input conditions, enabling controllable image creation and view-consistent 3D content generation using a hybrid diffusion prior.</p><hr><h3>Towards Interpretable Classification of Leukocytes based on Deep  Learning</h3>
<p>Stefan Röhrl,Johannes Groll,Manuel Lengl,Simon Schumann,Christian Klenk,Dominik Heim,Martin Knopp,Oliver Hayden,Klaus Diepold</p>
<p><a href='http://arxiv.org/abs/2311.14485v1'>http://arxiv.org/abs/2311.14485v1</a></p>
<p><b>Compressor summary</b>: This work explores label-free cytological imaging using machine learning, confidence calibration, visual explanations, and detection patterns in neural networks for automated leukocyte classification and analysis.</p><hr><h3>MRxaI: Black-Box Explainability for Image Classifiers in a Medical  Setting</h3>
<p>Nathan Blake,Hana Chockler,David A. Kelly,Santiago Calderon Pena,Akchunya Chanchal</p>
<p><a href='http://arxiv.org/abs/2311.14471v1'>http://arxiv.org/abs/2311.14471v1</a></p>
<p><b>Compressor summary</b>: The paper compares black-box methods to white-box method gradcam in explaining medical image classifications and finds that most black-box tools are not suitable but one, called causal explainability-based rex, performs as well as gradcam.</p><hr><h3>Efficient Gradient Estimation via Adaptive Sampling and Importance  Sampling</h3>
<p>Corentin Salaün,Xingchang Huang,Iliyan Georgiev,Niloy J. Mitra,Gurprit Singh</p>
<p><a href='http://arxiv.org/abs/2311.14468v1'>http://arxiv.org/abs/2311.14468v1</a></p>
<p><b>Compressor summary</b>: The paper introduces an algorithm that incorporates existing importance functions into a framework for adaptive or importance sampling in SGD, improving convergence in classification and regression tasks with minimal computational overhead.</p><hr><h3>Finite Volume Features, Global Geometry Representations, and Residual  Training for Deep Learning-based CFD Simulation</h3>
<p>Loh Sher En Jessica,Naheed Anjum Arafat,Wei Xian Lim,Wai Lee Chan,Adams Wai Kin Kong</p>
<p><a href='http://arxiv.org/abs/2311.14464v1'>http://arxiv.org/abs/2311.14464v1</a></p>
<p><b>Compressor summary</b>: The paper proposes new geometric representations and features for graph neural network-based computational fluid dynamics simulations to improve accuracy and reduce computation cost.</p><hr><h3>IDD-AW: A Benchmark for Safe and Robust Segmentation of Drive Scenes in  Unstructured Traffic and Adverse Weather</h3>
<p>Furqan Ahmed Shaik,Abhishek Malreddy,Nikhil Reddy Billa,Kunal Chaudhary,Sunny Manchanda,Girish Varma</p>
<p><a href='http://arxiv.org/abs/2311.14459v1'>http://arxiv.org/abs/2311.14459v1</a></p>
<p><b>Compressor summary</b>: The IDD-AW dataset contains 5000 pairs of annotated images in various adverse weather and traffic conditions, designed to evaluate the safety and robustness of autonomous vehicles.</p><hr><h3>How to ensure a safe control strategy? Towards a SRL for urban transit  autonomous operation</h3>
<p>Zicong Zhao</p>
<p><a href='http://arxiv.org/abs/2311.14457v1'>http://arxiv.org/abs/2311.14457v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a SSA-DRL framework that combines linear temporal logic, reinforcement learning, Monte Carlo tree search, and an additional actor to ensure safe and efficient autonomous operation of urban rail transit trains.</p><hr><h3>Universal Jailbreak Backdoors from Poisoned Human Feedback</h3>
<p>Javier Rando,Florian Tramèr</p>
<p><a href='http://arxiv.org/abs/2311.14455v1'>http://arxiv.org/abs/2311.14455v1</a></p>
<p><b>Compressor summary</b>: The paper explores how adversaries can create powerful backdoors in large language models trained with Reinforcement Learning from Human Feedback (RLHF) by poisoning the training data, enabling harmful responses with a single trigger word.</p><hr><h3>GCPV: Guided Concept Projection Vectors for the Explainable Inspection  of CNN Feature Spaces</h3>
<p>Georgii Mikriukov,Gesina Schwalbe,Christian Hellert,Korinna Bade</p>
<p><a href='http://arxiv.org/abs/2311.14435v1'>http://arxiv.org/abs/2311.14435v1</a></p>
<p><b>Compressor summary</b>: The paragraph introduces a new approach called Guided Concept Projection Vectors (GCPV) that improves the interpretation and debugging of computer vision neural networks by generating precise and multi-layer concept vectors from latent representations.</p><hr><h3>Human-Machine Cooperative Multimodal Learning Method for Cross-subject  Olfactory Preference Recognition</h3>
<p>Xiuxin Xia,Yuchen Guo,Yanwei Wang,Yuchao Yang,Yan Shi,Hong Men</p>
<p><a href='http://arxiv.org/abs/2311.14426v1'>http://arxiv.org/abs/2311.14426v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a multimodal learning method combining E-nose and olfactory EEG to improve cross-subject odor preference recognition, overcoming their individual limitations and achieving better results than existing methods.</p><hr><h3>A Comparison of PDF Projection with Normalizing Flows and SurVAE</h3>
<p>Paul M. Baggenstoss,Felix Govaers</p>
<p><a href='http://arxiv.org/abs/2311.14412v1'>http://arxiv.org/abs/2311.14412v1</a></p>
<p><b>Compressor summary</b>: SurVAE extends normalizing flows to handle dimension-altering transformations, but it is essentially a re-invention of older technique called PDF projection.</p><hr><h3>Unveiling The Factors of Aesthetic Preferences with Explainable AI</h3>
<p>Derya Soydaner,Johan Wagemans</p>
<p><a href='http://arxiv.org/abs/2311.14410v1'>http://arxiv.org/abs/2311.14410v1</a></p>
<p><b>Compressor summary</b>: The authors use machine learning models and explainable AI techniques to understand how different aesthetic attributes affect people's preferences for images.</p><hr><h3>LLamol: A Dynamic Multi-Conditional Generative Transformer for De Novo  Molecular Design</h3>
<p>Niklas Dobberstein,Astrid Maass,Jan Hamaekers</p>
<p><a href='http://arxiv.org/abs/2311.14407v1'>http://arxiv.org/abs/2311.14407v1</a></p>
<p><b>Compressor summary</b>: LLamol is a novel generative transformer model that can create organic compounds with various conditions by using stochastic context learning and token sequences.</p><hr><h3>OneFormer3D: One Transformer for Unified Point Cloud Segmentation</h3>
<p>Maxim Kolodiazhnyi,Anna Vorontsova,Anton Konushin,Danila Rukhovich</p>
<p><a href='http://arxiv.org/abs/2311.14405v1'>http://arxiv.org/abs/2311.14405v1</a></p>
<p><b>Compressor summary</b>: OneFormer3D is a unified model that performs instance, semantic, and panoptic segmentation of 3D point clouds using learnable kernels trained with a transformer-based decoder, achieving state-of-the-art results on several benchmarks.</p><hr><h3>BHGNN-RT: Network embedding for directed heterogeneous graphs</h3>
<p>Xiyang Sun,Fumiyasu Komaki</p>
<p><a href='http://arxiv.org/abs/2311.14404v1'>http://arxiv.org/abs/2311.14404v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a bidirectional heterogeneous graph neural network (BHGNN-RT) for directed heterogeneous graphs that uses message-passing and teleportation to overcome over-smoothing, achieving state-of-the-art performance in node classification and clustering tasks.</p><hr><h3>TEA: Test-time Energy Adaptation</h3>
<p>Yige Yuan,Bingbing Xu,Liang Hou,Fei Sun,Huawei Shen,Xueqi Cheng</p>
<p><a href='http://arxiv.org/abs/2311.14402v1'>http://arxiv.org/abs/2311.14402v1</a></p>
<p><b>Compressor summary</b>: TEA is a novel energy-based method to improve model generalizability by enhancing its perception of test data distributions without needing training data or processes.</p><hr><h3>Multi-scale Semantic Correlation Mining for Visible-Infrared Person  Re-Identification</h3>
<p>Ke Cheng,Xuecheng Hua,Hu Lu,Juanjuan Tu,Yuanquan Wang,Shitong Wang</p>
<p><a href='http://arxiv.org/abs/2311.14395v1'>http://arxiv.org/abs/2311.14395v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a network called MSCMNet to effectively use semantic features and modality information for person re-identification tasks by using multiple scales, novel components, and a specific loss function.</p><hr><h3>Directly Attention Loss Adjusted Prioritized Experience Replay</h3>
<p>Zhuoying Chen,Huiping Li,Zhaoxu Wang</p>
<p><a href='http://arxiv.org/abs/2311.14390v1'>http://arxiv.org/abs/2311.14390v1</a></p>
<p><b>Compressor summary</b>: DALAP is a new off policy RL framework that uses self-attention to correct the distribution shift caused by PER, and also optimizes sample screening for faster and more stable training.</p><hr><h3>A Parameterized Generative Adversarial Network Using Cyclic Projection  for Explainable Medical Image Classification</h3>
<p>Xiangyu Xiong,Yue Sun,Xiaohong Liu,ChanTong Lam,Tong Tong,Hao Chen,Qinquan Gao,Wei Ke,Tao Tan</p>
<p><a href='http://arxiv.org/abs/2311.14388v1'>http://arxiv.org/abs/2311.14388v1</a></p>
<p><b>Compressor summary</b>: ParaGAN is a novel method that uses projection distance parameters and class-difference maps to generate domain-specific synthetic samples for improved image classification on small-scale datasets.</p><hr><h3>Achieving Margin Maximization Exponentially Fast via Progressive Norm  Rescaling</h3>
<p>Mingze Wang,Zeping Min,Lei Wu</p>
<p><a href='http://arxiv.org/abs/2311.14387v1'>http://arxiv.org/abs/2311.14387v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm called Progressive Rescaling Gradient Descent (PRGD) that can efficiently maximize the margin for linearly separable data, unlike existing algorithms like gradient descent and normalized gradient descent.</p><hr><h3>Ethical implications of ChatGPT in higher education: A scoping review</h3>
<p>Ming Li,Ariunaa Enkhtur,Fei Cheng,Beverley Anne Yamamoto</p>
<p><a href='http://arxiv.org/abs/2311.14378v1'>http://arxiv.org/abs/2311.14378v1</a></p>
<p><b>Compressor summary</b>: The scoping review examines the ethical issues of using ChatGPT in higher education by reviewing academic articles and identifying six main areas of concern.</p><hr><h3>Deciphering and integrating invariants for neural operator learning with  various physical mechanisms</h3>
<p>Rui Zhang,Qi Meng,Zhi-Ming Ma</p>
<p><a href='http://arxiv.org/abs/2311.14361v1'>http://arxiv.org/abs/2311.14361v1</a></p>
<p><b>Compressor summary</b>: PIANO is a novel neural operator method that learns from physical invariants in PDEs and achieves better performance than existing techniques on various forecasting tasks.</p><hr><h3>Highly Detailed and Temporal Consistent Video Stylization via  Synchronized Multi-Frame Diffusion</h3>
<p>Minshan Xie,Hanyuan Liu,Chengze Li,Tien-Tsin Wong</p>
<p><a href='http://arxiv.org/abs/2311.14343v1'>http://arxiv.org/abs/2311.14343v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a synchronized multi-frame diffusion framework for text-guided video stylization that maintains visual details and temporal consistency by sharing information among frames using optical flow.</p><hr><h3>Towards Concept-based Interpretability of Skin Lesion Diagnosis using  Vision-Language Models</h3>
<p>Cristiano Patrício,Luís F. Teixeira,João C. Neves</p>
<p><a href='http://arxiv.org/abs/2311.14339v1'>http://arxiv.org/abs/2311.14339v1</a></p>
<p><b>Compressor summary</b>: The authors propose a vision-language model that uses CLIP with textual embeddings based on concepts to classify skin lesions, reducing the need for concept-annotated data and outperforming other methods.</p><hr><h3>TVT: Training-Free Vision Transformer Search on Tiny Datasets</h3>
<p>Zimian Wei,Hengyue Pan,Lujun Li,Peijie Dong,Zhiliang Tian,Xin Niu,Dongsheng Li</p>
<p><a href='http://arxiv.org/abs/2311.14337v1'>http://arxiv.org/abs/2311.14337v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a training-free method to search for the best ViT model for distilling with ConvNet teachers, using teacher-aware and student-capability metrics, and shows improved efficiency and effectiveness compared to previous methods.</p><hr><h3>Comparative Analysis of Transformers for Modeling Tabular Data: A  Casestudy using Industry Scale Dataset</h3>
<p>Usneek Singh,Piyush Arora,Shamika Ganesan,Mohit Kumar,Siddhant Kulkarni,Salil R. Joshi</p>
<p><a href='http://arxiv.org/abs/2311.14335v1'>http://arxiv.org/abs/2311.14335v1</a></p>
<p><b>Compressor summary</b>: The paper compares transformer-based models for tabular data on a large industry dataset, addressing challenges like high-dimensional data and efficient pre-processing, and discusses trade-offs between resources and performance.</p><hr><h3>Maximizing Discrimination Capability of Knowledge Distillation with  Energy-based Score</h3>
<p>Seonghak Kim,Gyeongdo Ham,Suin Lee,Donggon Jang,Daeshik Kim</p>
<p><a href='http://arxiv.org/abs/2311.14334v1'>http://arxiv.org/abs/2311.14334v1</a></p>
<p><b>Compressor summary</b>: The authors propose an energy-based knowledge distillation method that uses temperature scaling to adjust non-target class predictions, improving performance on various datasets and enabling data augmentation on resource-limited devices.</p><hr><h3>Cycle Invariant Positional Encoding for Graph Representation Learning</h3>
<p>Zuoyu Yan,Tengfei Ma,Liangcai Gao,Zhi Tang,Chao Chen,Yusu Wang</p>
<p><a href='http://arxiv.org/abs/2311.14333v1'>http://arxiv.org/abs/2311.14333v1</a></p>
<p><b>Compressor summary</b>: CycleNet is a structure encoding module for graph neural networks that uses edge structure encoding to incorporate cycle information in a permutation invariant way, improving network performance on various benchmarks.</p><hr><h3>GATGPT: A Pre-trained Large Language Model with Graph Attention Network  for Spatiotemporal Imputation</h3>
<p>Yakun Chen,Xianzhi Wang,Guandong Xu</p>
<p><a href='http://arxiv.org/abs/2311.14332v1'>http://arxiv.org/abs/2311.14332v1</a></p>
<p><b>Compressor summary</b>: The GATGPT framework combines a graph attention mechanism with pre-trained large language models to impute missing values in spatiotemporal data, improving on traditional methods.</p><hr><h3>Large Language Models as Topological Structure Enhancers for  Text-Attributed Graphs</h3>
<p>Shengyin Sun,Yuxiang Ren,Chen Ma,Xuecang Zhang</p>
<p><a href='http://arxiv.org/abs/2311.14324v1'>http://arxiv.org/abs/2311.14324v1</a></p>
<p><b>Compressor summary</b>: The authors explore using large language models to improve the structure of text-attributed graphs for node classification tasks by removing unreliable edges, adding reliable ones, and refining edge weights with pseudo-labels.</p><hr><h3>Binarized 3D Whole-body Human Mesh Recovery</h3>
<p>Zhiteng Li,Yulun Zhang,Jing Lin,Haotong Qin,Jinjin Gu,Xin Yuan,Linghe Kong,Xiaokang Yang</p>
<p><a href='http://arxiv.org/abs/2311.14323v1'>http://arxiv.org/abs/2311.14323v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper introduces BiDRN, a binarization method for 3D human reconstruction from a single image
- BiDRN consists of BiDRB units with Local Convolution Residual and Block Residual modules
- BiDRN achieves comparable performance with Hand4Whole while using much fewer parameters and operations

Summary:
The paper presents BiDRN, a novel binarization method for 3D human reconstruction that uses less memory and computation than existing methods.</p><hr><h3>Robust Domain Misinformation Detection via Multi-modal Feature Alignment</h3>
<p>Hui Liu,Wenya Wang,Hao Sun,Anderson Rocha,Haoliang Li</p>
<p><a href='http://arxiv.org/abs/2311.14315v1'>http://arxiv.org/abs/2311.14315v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses a new approach called RDCM for detecting multi-modal misinformation on social media by aligning textual and visual modalities and handling domain shift issues.</p><hr><h3>Stable Cluster Discrimination for Deep Clustering</h3>
<p>Qi Qian</p>
<p><a href='http://arxiv.org/abs/2311.14310v1'>http://arxiv.org/abs/2311.14310v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses a novel method called SeCu for one-stage deep clustering that overcomes challenges in representation learning and clustering by introducing a stable cluster discrimination task and a hardness-aware criterion.</p><hr><h3>Cosine Similarity Knowledge Distillation for Individual Class  Information Transfer</h3>
<p>Gyeongdo Ham,Seonghak Kim,Suin Lee,Jae-Hyeok Lee,Daeshik Kim</p>
<p><a href='http://arxiv.org/abs/2311.14307v1'>http://arxiv.org/abs/2311.14307v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel Knowledge Distillation method using cosine similarity and a weighted temperature technique to improve student performance, achieving results comparable or better than teacher models.</p><hr><h3>New Epochs in AI Supervision: Design and Implementation of an Autonomous  Radiology AI Monitoring System</h3>
<p>Vasantha Kumar Venugopal,Abhishek Gupta,Rohit Takhar,Vidur Mahajan</p>
<p><a href='http://arxiv.org/abs/2311.14305v1'>http://arxiv.org/abs/2311.14305v1</a></p>
<p><b>Compressor summary</b>: The authors propose two metrics to monitor AI radiology models' accuracy and stability, ensuring reliable AI use in healthcare.</p><hr><h3>AdaMedGraph: Adaboosting Graph Neural Networks for Personalized Medicine</h3>
<p>Jie Lian,Xufang Luo,Caihua Shan,Dongqi Han,Varut Vardhanabhuti,Dongsheng Li</p>
<p><a href='http://arxiv.org/abs/2311.14304v1'>http://arxiv.org/abs/2311.14304v1</a></p>
<p><b>Compressor summary</b>: The paper presents a novel algorithm that automatically selects important features to build patient similarity graphs and use graph neural networks for precision medicine, improving performance in two real-medical scenarios.</p><hr><h3>GeoViT: A Versatile Vision Transformer Architecture for Geospatial Image  Analysis</h3>
<p>Madhav Khirwar,Ankur Narang</p>
<p><a href='http://arxiv.org/abs/2311.14301v1'>http://arxiv.org/abs/2311.14301v1</a></p>
<p><b>Compressor summary</b>: The paper introduces GeoViT, a compact vision transformer model that processes satellite imagery to estimate CO2 and NO2 emissions and power generation, outperforming previous models and helping monitor and regulate greenhouse gas emissions.</p><hr><h3>Decouple Content and Motion for Conditional Image-to-Video Generation</h3>
<p>Cuifeng Shen,Yulu Gan,Chen Chen,Xiongwei Zhu,Lele Cheng,Jinzhi Wang</p>
<p><a href='http://arxiv.org/abs/2311.14294v1'>http://arxiv.org/abs/2311.14294v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for conditional image-to-video generation that disentangles spatial content and temporal motions, improving motion consistency and visual continuity while being more efficient than previous approaches.</p><hr><h3>Paragraph-to-Image Generation with Information-Enriched Diffusion Model</h3>
<p>Weijia Wu,Zhuang Li,Yefei He,Mike Zheng Shou,Chunhua Shen,Lele Cheng,Yan Li,Tingting Gao,Di Zhang,Zhongyuan Wang</p>
<p><a href='http://arxiv.org/abs/2311.14284v1'>http://arxiv.org/abs/2311.14284v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new model called ParaDiffusion that uses a language model to encode long paragraphs and generate images with better alignment and fidelity than existing models.</p><hr><h3>Image Super-Resolution with Text Prompt Diffusion</h3>
<p>Zheng Chen,Yulun Zhang,Jinjin Gu,Xin Yuan,Linghe Kong,Guihai Chen,Xiaokang Yang</p>
<p><a href='http://arxiv.org/abs/2311.14282v1'>http://arxiv.org/abs/2311.14282v1</a></p>
<p><b>Compressor summary</b>: Text prompts are used to improve image super-resolution by providing degradation information in a flexible and abstract manner, resulting in excellent performance on synthetic and real-world images.</p><hr><h3>Multi-modal Instance Refinement for Cross-domain Action Recognition</h3>
<p>Yuan Qing,Naixing Wu,Shaohua Wan,Lixin Duan</p>
<p><a href='http://arxiv.org/abs/2311.14281v1'>http://arxiv.org/abs/2311.14281v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a reinforcement learning-based method to reduce negative transfer in unsupervised cross-domain action recognition by refining training data with a multi-modal instance refinement technique.</p><hr><h3>Cooperative Dual Attention for Audio-Visual Speech Enhancement with  Facial Cues</h3>
<p>Feixiang Wang,Shuang Yang,Shiguang Shan,Xilin Chen</p>
<p><a href='http://arxiv.org/abs/2311.14275v1'>http://arxiv.org/abs/2311.14275v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a DualAVSE method that leverages facial cues beyond the lip region for robust Audio-Visual Speech Enhancement, ignoring speech-unrelated information and dynamically integrating audio and visual features.</p><hr><h3>CRISP: Hybrid Structured Sparsity for Class-aware Model Pruning</h3>
<p>Shivam Aggarwal,Kuluhan Binici,Tulika Mitra</p>
<p><a href='http://arxiv.org/abs/2311.14272v1'>http://arxiv.org/abs/2311.14272v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CRISP, a new pruning method for machine learning models that combines structured sparsity patterns and class-aware saliency scores to reduce memory consumption and improve efficiency while maintaining accuracy.</p><hr><h3>Segmentation-Based Parametric Painting</h3>
<p>Manuel Ladron de Guevara,Matthew Fisher,Aaron Hertzmann</p>
<p><a href='http://arxiv.org/abs/2311.14271v1'>http://arxiv.org/abs/2311.14271v1</a></p>
<p><b>Compressor summary</b>: The method creates high-quality paintings from large images using segmentation and dynamic attention maps, allowing for control over details and style.</p><hr><h3>Efficient Open-world Reinforcement Learning via Knowledge Distillation  and Autonomous Rule Discovery</h3>
<p>Ekaterina Nikonova,Cheng Xue,Jochen Renz</p>
<p><a href='http://arxiv.org/abs/2311.14270v1'>http://arxiv.org/abs/2311.14270v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework for deep reinforcement learning agents that enables them to discover task-specific rules in new environments and self-supervise their learning, improving their ability to adapt to novelties.</p><hr><h3>Bursting Spikes: Efficient and High-performance SNNs for Event-based  Vision</h3>
<p>Ziqing Wang,Yuetong Fang,Jiahang Cao,Renjing Xu</p>
<p><a href='http://arxiv.org/abs/2311.14265v1'>http://arxiv.org/abs/2311.14265v1</a></p>
<p><b>Compressor summary</b>: The authors propose a burst-spike mechanism for spiking neural networks (SNNs) that reduces conversion errors, lowers latency, and saves energy compared to state-of-the-art methods.</p><hr><h3>ZeroPS: High-quality Cross-modal Knowledge Transfer for Zero-Shot 3D  Part Segmentation</h3>
<p>Yuheng Xue,Nenglun Chen,Jun Liu,Wenyun Sun</p>
<p><a href='http://arxiv.org/abs/2311.14262v1'>http://arxiv.org/abs/2311.14262v1</a></p>
<p><b>Compressor summary</b>: ZeroPS is a novel pipeline for zero-shot 3D part segmentation that leverages multi-view correspondences and prompt mechanisms of 2D pretrained foundational models, achieving state-of-the-art results without training or fine-tuning.</p><hr><h3>Out-of-Distribution Generalized Dynamic Graph Neural Network with  Disentangled Intervention and Invariance Promotion</h3>
<p>Zeyang Zhang,Xin Wang,Ziwei Zhang,Haoyang Li,Wenwu Zhu</p>
<p><a href='http://arxiv.org/abs/2311.14255v1'>http://arxiv.org/abs/2311.14255v1</a></p>
<p><b>Compressor summary</b>: I-DIDA is a novel model that handles distribution shifts in dynamic graphs by discovering invariant patterns and making predictions based on them.</p><hr><h3>RSB-Pose: Robust Short-Baseline Binocular 3D Human Pose Estimation with  Occlusion Handling</h3>
<p>Xiaoyue Wan,Zhuo Chen,Yiming Bao,Xu Zhao</p>
<p><a href='http://arxiv.org/abs/2311.14242v1'>http://arxiv.org/abs/2311.14242v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method for 3D human pose estimation using binocular cameras that handles view inconsistency and occlusions by utilizing disparity and joint correlations.</p><hr><h3>Pseudo-label Correction for Instance-dependent Noise Using  Teacher-student Framework</h3>
<p>Eugene Kim</p>
<p><a href='http://arxiv.org/abs/2311.14237v1'>http://arxiv.org/abs/2311.14237v1</a></p>
<p><b>Compressor summary</b>: The paper proposes P-LC, a teacher-student framework that uses a triple encoder and pseudo-label correction to handle label noise and improve generalization for deep learning models.</p>
