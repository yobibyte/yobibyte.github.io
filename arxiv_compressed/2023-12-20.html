
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2023-12-20</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2023-12-20 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Weakly Supervised Open-Vocabulary Object Detection</h3>
<p>Jianghang Lin,Yunhang Shen,Bingquan Wang,Shaohui Lin,Ke Li,Liujuan Cao</p>
<p><a href='http://arxiv.org/abs/2312.12437v1'>http://arxiv.org/abs/2312.12437v1</a></p>
<p><b>Compressor summary</b>: The paper proposes WSOVOD, a framework for weakly supervised open-vocabulary object detection that can detect novel concepts and use diverse datasets with only image-level annotations.</p><hr><h3>A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise</h3>
<p>Chaoyou Fu,Renrui Zhang,Haojia Lin,Zihan Wang,Timin Gao,Yongdong Luo,Yubo Huang,Zhengye Zhang,Longtian Qiu,Gaoxiang Ye,Yunhang Shen,Mengdan Zhang,Peixian Chen,Sirui Zhao,Xiawu Zheng,Shaohui Lin,Deqiang Jiang,Di Yin,Peng Gao,Ke Li,Xing Sun,Rongrong Ji</p>
<p><a href='http://arxiv.org/abs/2312.12436v1'>http://arxiv.org/abs/2312.12436v1</a></p>
<p><b>Compressor summary</b>: The paper explores Gemini Pro's visual understanding abilities and compares it with GPT-4V and Sphinx, finding that Gemini can be a strong challenger to GPT-4V in multi-modal tasks.</p><hr><h3>Tracking Any Object Amodally</h3>
<p>Cheng-Yen Hsieh,Tarasha Khurana,Achal Dave,Deva Ramanan</p>
<p><a href='http://arxiv.org/abs/2312.12433v1'>http://arxiv.org/abs/2312.12433v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new benchmark dataset and a module that improves amodal perception for detection and tracking of occluded objects in videos.</p><hr><h3>On Inference Stability for Diffusion Models</h3>
<p>Viet Nguyen,Giang Vu,Tung Nguyen Thanh,Khoat Than,Toan Tran</p>
<p><a href='http://arxiv.org/abs/2312.12431v1'>http://arxiv.org/abs/2312.12431v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new loss function for Denoising Probabilistic Models that considers the correlation between timesteps, improving image quality and generalization.</p><hr><h3>The Endoscapes Dataset for Surgical Scene Segmentation, Object  Detection, and Critical View of Safety Assessment: Official Splits and  Benchmark</h3>
<p>Aditya Murali,Deepak Alapatt,Pietro Mascagni,Armine Vardazaryan,Alain Garcia,Nariaki Okamoto,Guido Costamagna,Didier Mutter,Jacques Marescaux,Bernard Dallemagne,Nicolas Padoy</p>
<p><a href='http://arxiv.org/abs/2312.12429v1'>http://arxiv.org/abs/2312.12429v1</a></p>
<p><b>Compressor summary</b>: The Endoscapes dataset contains LC videos with detailed annotations for assessing CVS and other aspects of the surgery, along with benchmarks and public access to the data and models.</p><hr><h3>SegRefiner: Towards Model-Agnostic Segmentation Refinement with Discrete  Diffusion Process</h3>
<p>Mengyu Wang,Henghui Ding,Jun Hao Liew,Jiajun Liu,Yao Zhao,Yunchao Wei</p>
<p><a href='http://arxiv.org/abs/2312.12425v1'>http://arxiv.org/abs/2312.12425v1</a></p>
<p><b>Compressor summary</b>: Key points:
- SegRefiner is a model-agnostic solution for enhancing object masks from different segmentation models
- It uses a discrete diffusion process and predicts label and transition probabilities for each pixel
- It performs well on various segmentation tasks, improving both metrics and details

Summary:
SegRefiner refines object masks using a discrete diffusion process that predicts label and transition probabilities for each pixel, achieving superior results on different segmentation tasks.</p><hr><h3>Jack of All Tasks, Master of Many: Designing General-purpose  Coarse-to-Fine Vision-Language Model</h3>
<p>Shraman Pramanick,Guangxing Han,Rui Hou,Sayan Nag,Ser-Nam Lim,Nicolas Ballas,Qifan Wang,Rama Chellappa,Amjad Almahairi</p>
<p><a href='http://arxiv.org/abs/2312.12423v1'>http://arxiv.org/abs/2312.12423v1</a></p>
<p><b>Compressor summary</b>: VistaLLM is a visual system that uses instruction-guided image tokenization and adaptive sampling to perform various vision-language tasks with single and multiple input images, while leveraging the CoinIt dataset and introducing the AttCoSeg task.</p><hr><h3>Scene-Conditional 3D Object Stylization and Composition</h3>
<p>Jinghao Zhou,Tomas Jakab,Philip Torr,Christian Rupprecht</p>
<p><a href='http://arxiv.org/abs/2312.12419v1'>http://arxiv.org/abs/2312.12419v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a framework to stylize 3D assets to fit into 2D scenes
- The framework uses differentiable ray tracing and text-to-image diffusion models
- The method can handle different environments and objects

Summary:
The paper presents a method to adapt 3D assets to match 2D scenes using ray tracing and image priors, enabling object stylization and realistic composition.</p><hr><h3>Prompting Hard or Hardly Prompting: Prompt Inversion for Text-to-Image  Diffusion Models</h3>
<p>Shweta Mahajan,Tanzila Rahman,Kwang Moo Yi,Leonid Sigal</p>
<p><a href='http://arxiv.org/abs/2312.12416v1'>http://arxiv.org/abs/2312.12416v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to obtain interpretable language prompts from text-to-image diffusion models by using a delayed projection scheme and focusing on later timesteps of the diffusion process.</p><hr><h3>On Alternating-time Temporal Logic, Hyperproperties, and Strategy  Sharing</h3>
<p>Raven Beutner,Bernd Finkbeiner</p>
<p><a href='http://arxiv.org/abs/2312.12403v1'>http://arxiv.org/abs/2312.12403v1</a></p>
<p><b>Compressor summary</b>: HyperATLS$^*_S$ extends ATL$^*$ to compare outcomes of multiple strategic interactions and enforce shared strategies among agents, capturing important AI properties and enabling decidable model checking.</p><hr><h3>New classes of the greedy-applicable arm feature distributions in the  sparse linear bandit problem</h3>
<p>Koji Ichikawa,Shinji Ito,Daisuke Hatano,Hanna Sumita,Takuro Fukunaga,Naonori Kakimura,Ken-ichi Kawarabayashi</p>
<p><a href='http://arxiv.org/abs/2312.12400v1'>http://arxiv.org/abs/2312.12400v1</a></p>
<p><b>Compressor summary</b>: The paper proposes new distribution classes that allow the greedy algorithm to be applied to more contextual bandit problems without strong assumptions on arm feature diversity.</p><hr><h3>Mixture of Cluster-conditional LoRA Experts for Vision-language  Instruction Tuning</h3>
<p>Yunhao Gou,Zhili Liu,Kai Chen,Lanqing Hong,Hang Xu,Aoxue Li,Dit-Yan Yeung,James T. Kwok,Yu Zhang</p>
<p><a href='http://arxiv.org/abs/2312.12379v1'>http://arxiv.org/abs/2312.12379v1</a></p>
<p><b>Compressor summary</b>: MoCLE is a new MoE architecture that improves LVLMs' instruction-following abilities and generalization across various zero-shot vision-language tasks by activating task-specific model parameters based on instruction clusters and adding a universal expert.</p><hr><h3>Chasing Fairness in Graphs: A GNN Architecture Perspective</h3>
<p>Zhimeng Jiang,Xiaotian Han,Chao Fan,Zirui Liu,Na Zou,Ali Mostafavi,Xia Hu</p>
<p><a href='http://arxiv.org/abs/2312.12369v1'>http://arxiv.org/abs/2312.12369v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new graph neural network architecture called Fair Message Passing that improves fairness and prediction performance by explicitly using sensitive attributes and mitigating biases in node classification tasks.</p><hr><h3>SpokesBiz -- an Open Corpus of Conversational Polish</h3>
<p>Piotr Pęzik,Sylwia Karasińska,Anna Cichosz,Łukasz Jałowiecki,Konrad Kaczyński,Małgorzata Krawentek,Karolina Walkusz,Paweł Wilk,Mariusz Kleć,Krzysztof Szklanny,Szymon Marszałkowski</p>
<p><a href='http://arxiv.org/abs/2312.12364v1'>http://arxiv.org/abs/2312.12364v1</a></p>
<p><b>Compressor summary</b>: SpokesBiz is a large conversational Polish corpus with many uses in linguistics and ASR development.</p><hr><h3>CLIP-DINOiser: Teaching CLIP a few DINO tricks</h3>
<p>Monika Wysoczańska,Oriane Siméoni,Michaël Ramamonjisoa,Andrei Bursuc,Tomasz Trzciński,Patrick Pérez</p>
<p><a href='http://arxiv.org/abs/2312.12359v1'>http://arxiv.org/abs/2312.12359v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a zero-shot open-vocabulary semantic segmentation method that improves MaskCLIP features with self-supervised localization priors and achieves state-of-the-art results on various benchmarks using only one pass through CLIP.</p><hr><h3>SMC-NCA: Semantic-guided Multi-level Contrast for Semi-supervised Action  Segmentation</h3>
<p>Feixiang Zhou,Zheheng Jiang,Huiyu Zhou,Xuelong Li</p>
<p><a href='http://arxiv.org/abs/2312.12347v1'>http://arxiv.org/abs/2312.12347v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel approach for semi-supervised action segmentation in long untrimmed videos using contrastive learning with intra- and inter-information variations exploration and neighbour consistency enforcement.</p><hr><h3>Avoiding Data Contamination in Language Model Evaluation: Dynamic Test  Construction with Latest Materials</h3>
<p>Yucheng Li,Frank Geurin,Chenghua Lin</p>
<p><a href='http://arxiv.org/abs/2312.12343v1'>http://arxiv.org/abs/2312.12343v1</a></p>
<p><b>Compressor summary</b>: LatestEval is an automatic method that creates uncontaminated reading comprehension evaluations by using texts published within a recent time window, avoiding overlap with pre-trained language models' training corpora.</p><hr><h3>Engineering an Exact Pseudo-Boolean Model Counter</h3>
<p>Suwei Yang,Kuldeep S. Meel</p>
<p><a href='http://arxiv.org/abs/2312.12341v1'>http://arxiv.org/abs/2312.12341v1</a></p>
<p><b>Compressor summary</b>: The paper introduces PBCount, an exact Pseudo-Boolean model counter using knowledge compilation via algebraic decision diagrams, which can handle more instances than existing methods.</p><hr><h3>Scalable Geometric Fracture Assembly via Co-creation Space among  Assemblers</h3>
<p>Ruiyuan Zhang,Jiaxiang Liu,Zexi Li,Hao Dong,Jie Fu,Chao Wu</p>
<p><a href='http://arxiv.org/abs/2312.12340v1'>http://arxiv.org/abs/2312.12340v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a co-creation space with multiple assemblers for geometric fracture assembly without semantic information
- It introduces a novel loss function to address collision issues during the process
- It outperforms existing frameworks on two datasets and has linear computational complexity, enhanced abstraction, and improved generalization

Summary:
The paper presents a new framework for assembling fragmented 3D objects without semantic information, using multiple assemblers and a novel loss function that improves performance and collision handling.</p><hr><h3>Value Explicit Pretraining for Goal-Based Transfer Learning</h3>
<p>Kiran Lekkala,Henghui Bao,Sumedh Sontakke,Laurent Itti</p>
<p><a href='http://arxiv.org/abs/2312.12339v1'>http://arxiv.org/abs/2312.12339v1</a></p>
<p><b>Compressor summary</b>: The proposed method enables learning task-independent representations from value function estimates that can transfer skills across different tasks regardless of their appearance and dynamics.</p><hr><h3>pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable  Generalizable 3D Reconstruction</h3>
<p>David Charatan,Sizhe Li,Andrea Tagliasacchi,Vincent Sitzmann</p>
<p><a href='http://arxiv.org/abs/2312.12337v1'>http://arxiv.org/abs/2312.12337v1</a></p>
<p><b>Compressor summary</b>: PixelSplat is a model that learns to reconstruct 3D images from two images using Gaussian primitives, enabling fast and memory-efficient rendering and 3D reconstruction.</p><hr><h3>PowMix: A Versatile Regularizer for Multimodal Sentiment Analysis</h3>
<p>Efthymios Georgiou,Yannis Avrithis,Alexandros Potamianos</p>
<p><a href='http://arxiv.org/abs/2312.12334v1'>http://arxiv.org/abs/2312.12334v1</a></p>
<p><b>Compressor summary</b>: PowMix is a novel embedding space regularizer for multimodal sentiment analysis that improves performance without sacrificing robustness or text dominance.</p><hr><h3>An Alternate View on Optimal Filtering in an RKHS</h3>
<p>Benjamin Colburn,Jose C. Principe,Luis G. Sanchez Giraldo</p>
<p><a href='http://arxiv.org/abs/2312.12318v1'>http://arxiv.org/abs/2312.12318v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Kernel Adaptive Filtering (KAF) are methods that search for functions in a Reproducing Kernel Hilbert Space (RKHS) for tasks like time series prediction and system identification.
- KAF have a linear relationship between number of training samples and model size, which limits their use on large data sets.
- The paper proposes a novel view of optimal filtering that preserves the time structure of a stochastic process in RKHS, using correntropy as a nonlinear functional.

Summary:
The paper presents a new approach to Kernel Adaptive Filtering that avoids the linear growth of model size with training samples by using correntropy to capture nonlinear mappings in a Reproducing Kernel Hilbert Space that respect the time structure of stochastic processes.</p><hr><h3>First qualitative observations on deep learning vision model YOLO and  DETR for automated driving in Austria</h3>
<p>Stefan Schoder</p>
<p><a href='http://arxiv.org/abs/2312.12314v1'>http://arxiv.org/abs/2312.12314v1</a></p>
<p><b>Compressor summary</b>: The study explores how 2D-object detection algorithms like YOLO can improve road safety for autonomous driving in Austria by detecting and tracking objects in various conditions.</p><hr><h3>Instruct-SCTG: Guiding Sequential Controlled Text Generation through  Instructions</h3>
<p>Yinhong Liu,Yixuan Su,Ehsan Shareghi,Nigel Collier</p>
<p><a href='http://arxiv.org/abs/2312.12299v1'>http://arxiv.org/abs/2312.12299v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Instruct-SCTG, a framework that uses instruction-tuned language models to generate structurally coherent articles in various domains with section-by-section alignment and measures discourse divergence using a new metric.</p><hr><h3>Toward enriched Cognitive Learning with XAI</h3>
<p>Muhammad Suffian,Ulrike Kuhl,Jose M. Alonso-Moral,Alessandro Bogliolo</p>
<p><a href='http://arxiv.org/abs/2312.12290v1'>http://arxiv.org/abs/2312.12290v1</a></p>
<p><b>Compressor summary</b>: The paper introduces an AI system (CL-XAI) that helps learners understand how AI models work and evaluates its effectiveness through human feedback.</p><hr><h3>Prompt-based Domain Discrimination for Multi-source Time Series Domain  Adaptation</h3>
<p>Junxiang Wang,Guangji Bai,Wei Cheng,Zhengzhang Chen,Liang Zhao,Haifeng Chen</p>
<p><a href='http://arxiv.org/abs/2312.12276v1'>http://arxiv.org/abs/2312.12276v1</a></p>
<p><b>Compressor summary</b>: POND is a novel prompt-based deep learning model for multi-source time series domain adaptation, focusing on extracting and leveraging domain-specific meta-data information.</p><hr><h3>Emergence of In-Context Reinforcement Learning from Noise Distillation</h3>
<p>Ilya Zisman,Vladislav Kurenkov,Alexander Nikulin,Viacheslav Sinii,Sergey Kolesnikov</p>
<p><a href='http://arxiv.org/abs/2312.12275v1'>http://arxiv.org/abs/2312.12275v1</a></p>
<p><b>Compressor summary</b>: $AD^{\epsilon}$ is a method that improves in-context learning with suboptimal human demonstrations by gradually introducing noise into them.</p><hr><h3>Intrinsic Image Diffusion for Single-view Material Estimation</h3>
<p>Peter Kocsis,Vincent Sitzmann,Matthias Nießner</p>
<p><a href='http://arxiv.org/abs/2312.12274v1'>http://arxiv.org/abs/2312.12274v1</a></p>
<p><b>Compressor summary</b>: Intrinsic Image Diffusion is a model that generates multiple realistic material explanations for indoor scenes, using probabilistic methods and learned priors from real images to overcome challenges in appearance decomposition.</p><hr><h3>VQA4CIR: Boosting Composed Image Retrieval with Visual Question  Answering</h3>
<p>Chun-Mei Feng,Yang Bai,Tao Luo,Zhen Li,Salman Khan,Wangmeng Zuo,Xinxing Xu,Rick Siow Mong Goh,Yong Liu</p>
<p><a href='http://arxiv.org/abs/2312.12273v1'>http://arxiv.org/abs/2312.12273v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a VQA-based post-processing approach (VQA4CIR) to improve Composed Image Retrieval by detecting and correcting inconsistent results with captions using LLM and LVLM fine-tuning.</p><hr><h3>Automated speech audiometry: Can it work using open-source pre-trained  Kaldi-NL automatic speech recognition?</h3>
<p>Gloria Araiza-Illan,Luke Meyer,Khiet P. Truong,Deniz Baskent</p>
<p><a href='http://arxiv.org/abs/2312.12269v1'>http://arxiv.org/abs/2312.12269v1</a></p>
<p><b>Compressor summary</b>: The text proposes an automated DIN test using Kaldi-NL toolkit that can evaluate spoken responses without a human supervisor and evaluates its performance in two studies.</p><hr><h3>Inferring the relationship between soil temperature and the normalized  difference vegetation index with machine learning</h3>
<p>Steven Mortier,Amir Hamedpour,Bart Bussmann,Ruth Phoebe Tchana Wandji,Steven Latré,Bjarni D. Sigurdsson,Tom De Schepper,Tim Verdonck</p>
<p><a href='http://arxiv.org/abs/2312.12258v1'>http://arxiv.org/abs/2312.12258v1</a></p>
<p><b>Compressor summary</b>: This study shows that soil temperature affects the start and peak of the growing season in subarctic grasslands, while other factors like air temperature, precipitation, and irradiance also play a role in vegetation phenology.</p><hr><h3>TaskFlex Solver for Multi-Agent Pursuit via Automatic Curriculum  Learning</h3>
<p>Jiayu Chen,Guosheng Li,Chao Yu,Xinyi Yang,Botian Xu,Huazhong Yang,Yu Wang</p>
<p><a href='http://arxiv.org/abs/2312.12255v1'>http://arxiv.org/abs/2312.12255v1</a></p>
<p><b>Compressor summary</b>: The paper presents TaskFlex Solver, a reinforcement learning and curriculum learning method for multi-agent pursuit problems in complex environments, which achieves high capture rates and adapts to various task conditions.</p><hr><h3>Geo-located Aspect Based Sentiment Analysis (ABSA) for Crowdsourced  Evaluation of Urban Environments</h3>
<p>Demircan Tas,Rohit Priyadarshi Sanatani</p>
<p><a href='http://arxiv.org/abs/2312.12253v1'>http://arxiv.org/abs/2312.12253v1</a></p>
<p><b>Compressor summary</b>: The authors develop a sentiment analysis model for urban environments that can extract specific aspects and their sentiments from crowdsourced reviews of public parks, and show its improvement in prediction accuracy using BERT with LCF.</p><hr><h3>ST(OR)2: Spatio-Temporal Object Level Reasoning for Activity Recognition  in the Operating Room</h3>
<p>Idris Hamoud,Muhammad Abdullah Jamal,Vinkle Srivastav,Didier Mutter,Nicolas Padoy,Omid Mohareri</p>
<p><a href='http://arxiv.org/abs/2312.12250v1'>http://arxiv.org/abs/2312.12250v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new object-based approach to recognize surgical activities in the OR using geometric arrangements between clinicians and devices, improving efficiency and performance with less data.</p><hr><h3>MDD-UNet: Domain Adaptation for Medical Image Segmentation with  Theoretical Guarantees, a Proof of Concept</h3>
<p>Asbjørn Munk,Ao Ma,Mads Nielsen</p>
<p><a href='http://arxiv.org/abs/2312.12246v1'>http://arxiv.org/abs/2312.12246v1</a></p>
<p><b>Compressor summary</b>: The MDD-UNet is an unsupervised domain adaptation framework for U-Nets that improves image segmentation performance across different data characteristics using a theory-based approach with theoretical guarantees.</p><hr><h3>GeomVerse: A Systematic Evaluation of Large Models for Geometric  Reasoning</h3>
<p>Mehran Kazemi,Hamidreza Alvari,Ankit Anand,Jialin Wu,Xi Chen,Radu Soricut</p>
<p><a href='http://arxiv.org/abs/2312.12241v1'>http://arxiv.org/abs/2312.12241v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates vision language models' mathematical reasoning abilities on geometry problems and finds they struggle with higher-depth problems requiring long chains of reasoning.</p><hr><h3>Roll With the Punches: Expansion and Shrinkage of Soft Label Selection  for Semi-supervised Fine-Grained Learning</h3>
<p>Yue Duan,Zhen Zhao,Lei Qi,Luping Zhou,Lei Wang,Yinghuan Shi</p>
<p><a href='http://arxiv.org/abs/2312.12237v1'>http://arxiv.org/abs/2312.12237v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve semi-supervised learning for fine-grained visual classification by selecting soft labels based on class transition tracking and confidence.</p><hr><h3>Generalization Analysis of Machine Learning Algorithms via the  Worst-Case Data-Generating Probability Measure</h3>
<p>Xinying Zou,Samir M. Perlaza,Iñaki Esnaola,Eitan Altman</p>
<p><a href='http://arxiv.org/abs/2312.12236v1'>http://arxiv.org/abs/2312.12236v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new tool to measure generalization in ML algorithms using the worst-case probability measure, which is a Gibbs probability measure.</p><hr><h3>Brush Your Text: Synthesize Any Scene Text on Images via Diffusion Model</h3>
<p>Lingjun Zhang,Xinyuan Chen,Yaohui Wang,Yue Lu,Yu Qiao</p>
<p><a href='http://arxiv.org/abs/2312.12232v1'>http://arxiv.org/abs/2312.12232v1</a></p>
<p><b>Compressor summary</b>: Diff-Text is a training-free framework that uses Stable Diffusion to generate realistic images with text in any language, improving text recognition and background blending.</p><hr><h3>HuTuMotion: Human-Tuned Navigation of Latent Motion Diffusion Models  with Minimal Feedback</h3>
<p>Gaoge Han,Shaoli Huang,Mingming Gong,Jinglei Tang</p>
<p><a href='http://arxiv.org/abs/2312.12227v1'>http://arxiv.org/abs/2312.12227v1</a></p>
<p><b>Compressor summary</b>: HuTuMotion is a new method that uses limited human feedback to improve the quality of generating natural human motions by adapting the prior distribution in latent diffusion models.</p><hr><h3>On the Parameterization of Second-Order Optimization Effective Towards  the Infinite Width</h3>
<p>Satoki Ishikawa,Ryo Karakida</p>
<p><a href='http://arxiv.org/abs/2312.12226v1'>http://arxiv.org/abs/2312.12226v1</a></p>
<p><b>Compressor summary</b>: The study proposes a specific parameterization for second-order optimization that enhances feature learning and allows transferring hyperparameters across different network widths.</p><hr><h3>Self-Supervised Detection of Perfect and Partial Input-Dependent  Symmetries</h3>
<p>Alonso Urbano,David W. Romero</p>
<p><a href='http://arxiv.org/abs/2312.12223v1'>http://arxiv.org/abs/2312.12223v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to detect different levels of symmetry in data without labels, improving generalization and robustness of models.</p><hr><h3>EarthVQA: Towards Queryable Earth via Relational Reasoning-Based Remote  Sensing Visual Question Answering</h3>
<p>Junjue Wang,Zhuo Zheng,Zihang Chen,Ailong Ma,Yanfei Zhong</p>
<p><a href='http://arxiv.org/abs/2312.12222v1'>http://arxiv.org/abs/2312.12222v1</a></p>
<p><b>Compressor summary</b>: The text introduces EarthVQA, a multi-modal multi-task dataset for advancing relational reasoning in Earth vision, and SOBA, a framework that leverages object semantics and relations for VQA.</p><hr><h3>Sharing is CAIRing: Characterizing Principles and Assessing Properties  of Universal Privacy Evaluation for Synthetic Tabular Data</h3>
<p>Tobias Hyrup,Anton Danholt Lautrup,Arthur Zimek,Peter Schneider-Kamp</p>
<p><a href='http://arxiv.org/abs/2312.12216v1'>http://arxiv.org/abs/2312.12216v1</a></p>
<p><b>Compressor summary</b>: The text discusses the need for a common framework to evaluate the privacy of synthetic data in healthcare, proposing four principles (CAIR) and a rubric to assess existing metrics.</p><hr><h3>Identification of Causal Structure in the Presence of Missing Data with  Additive Noise Model</h3>
<p>Jie Qiao,Zhengming Chen,Jianhua Yu,Ruichu Cai,Zhifeng Hao</p>
<p><a href='http://arxiv.org/abs/2312.12206v1'>http://arxiv.org/abs/2312.12206v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how to learn causal structure from data with self-masking missingness, which makes existing methods fail, using an additive noise model, and proposes a practical algorithm based on theoretical results.</p><hr><h3>Mask Grounding for Referring Image Segmentation</h3>
<p>Yong Xien Chng,Henry Zheng,Yizeng Han,Xuchong Qiu,Gao Huang</p>
<p><a href='http://arxiv.org/abs/2312.12198v1'>http://arxiv.org/abs/2312.12198v1</a></p>
<p><b>Compressor summary</b>: The text introduces MagNet, a novel method for referring image segmentation that uses mask grounding and cross-modal alignment to improve visual grounding and correspondence between language and images.</p><hr><h3>Gaussian process learning of nonlinear dynamics</h3>
<p>Dongwei Ye,Mengwu Guo</p>
<p><a href='http://arxiv.org/abs/2312.12193v1'>http://arxiv.org/abs/2312.12193v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new Bayesian method to learn nonlinear dynamics from time series data without explicitly calculating state derivatives, which can improve the accuracy of learned models when data is scarce or noisy.</p><hr><h3>CUDC: A Curiosity-Driven Unsupervised Data Collection Method with  Adaptive Temporal Distances for Offline Reinforcement Learning</h3>
<p>Chenyu Sun,Hangwei Qian,Chunyan Miao</p>
<p><a href='http://arxiv.org/abs/2312.12191v1'>http://arxiv.org/abs/2312.12191v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a Curiosity-driven Unsupervised Data Collection method to improve multi-task offline reinforcement learning by expanding feature space using adaptive temporal distances and reaching higher-quality data.</p><hr><h3>Poincaré Differential Privacy for Hierarchy-aware Graph Embedding</h3>
<p>Yuecen Wei,Haonan Yuan,Xingcheng Fu,Qingyun Sun,Hao Peng,Xianxian Li,Chunming Hu</p>
<p><a href='http://arxiv.org/abs/2312.12183v1'>http://arxiv.org/abs/2312.12183v1</a></p>
<p><b>Compressor summary</b>: PoinDP is a framework that uses hyperbolic geometry to protect hierarchical graph data from inference attacks and privacy leaks, while preserving performance in node classification tasks.</p><hr><h3>All for One, and One for All: UrbanSyn Dataset, the third Musketeer of  Synthetic Driving Scenes</h3>
<p>Jose L. Gómez,Manuel Silva,Antonio Seoane,Agnès Borrás,Mario Noriega,Germán Ros,Jose A. Iglesias-Guitian,Antonio M. López</p>
<p><a href='http://arxiv.org/abs/2312.12176v1'>http://arxiv.org/abs/2312.12176v1</a></p>
<p><b>Compressor summary</b>: UrbanSyn is a high-quality synthetic urban driving dataset that enhances existing ones for unsupervised domain adaptation in image semantic segmentation.</p><hr><h3>Towards Balanced Alignment: Modal-Enhanced Semantic Modeling for Video  Moment Retrieval</h3>
<p>Zhihang Liu,Jun Li,Hongtao Xie,Pandeng Li,Jiannan Ge,Sun-Ao Liu,Guoqing Jin</p>
<p><a href='http://arxiv.org/abs/2312.12155v1'>http://arxiv.org/abs/2312.12155v1</a></p>
<p><b>Compressor summary</b>: MESM is a novel framework for improving video moment retrieval by enhancing video and text features to balance the modality gap between videos and queries.</p><hr><h3>Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models:  A Critical Review and Assessment</h3>
<p>Lingling Xu,Haoran Xie,Si-Zhao Joe Qin,Xiaohui Tao,Fu Lee Wang</p>
<p><a href='http://arxiv.org/abs/2312.12148v1'>http://arxiv.org/abs/2312.12148v1</a></p>
<p><b>Compressor summary</b>: This paper reviews parameter efficient fine-tuning (PEFT) methods for pretrained language models, discussing their applications and future directions, and conducting experiments to understand their effectiveness.</p><hr><h3>OVD-Explorer:Optimism Should Not Be the Sole Pursuit of Exploration in  Noisy Environments</h3>
<p>Jinyi Liu,Zhi Wang,Yan Zheng,Jianye Hao,Chenjia Bai,Junjie Ye,Zhen Wang,Haiyin Piao,Yang Sun</p>
<p><a href='http://arxiv.org/abs/2312.12145v1'>http://arxiv.org/abs/2312.12145v1</a></p>
<p><b>Compressor summary</b>: OVD-Explorer is a new method for noisy environment exploration in continuous control RL that balances optimism with over-exploration mitigation.</p><hr><h3>M-BEV: Masked BEV Perception for Robust Autonomous Driving</h3>
<p>Siran Chen,Yue Ma,Yu Qiao,Yali Wang</p>
<p><a href='http://arxiv.org/abs/2312.12144v1'>http://arxiv.org/abs/2312.12144v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a Masked Bird-Eye-View (M-BEV) perception framework that improves robustness to failed camera views by randomly masking and reconstructing them in end-to-end training, achieving significant performance gains on the NuScenes benchmark.</p><hr><h3>Integrating Human Vision Perception in Vision Transformers for  Classifying Waste Items</h3>
<p>Akshat Kishore Shrivastava,Tapan Kumar Gandhi</p>
<p><a href='http://arxiv.org/abs/2312.12143v1'>http://arxiv.org/abs/2312.12143v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for waste classification that simulates nystagmus, a biological phenomenon affecting human vision, improving the accuracy of the Vision Transformer model by 2%.</p><hr><h3>FontDiffuser: One-Shot Font Generation via Denoising Diffusion with  Multi-Scale Content Aggregation and Style Contrastive Learning</h3>
<p>Zhenhua Yang,Dezhi Peng,Yuxin Kong,Yuyi Zhang,Cong Yao,Lianwen Jin</p>
<p><a href='http://arxiv.org/abs/2312.12142v1'>http://arxiv.org/abs/2312.12142v1</a></p>
<p><b>Compressor summary</b>: FontDiffuser is a diffusion-based method that improves font generation by combining global and local content cues, handling large style variations, and learning style representation from images.</p><hr><h3>Exploring the Residual Stream of Transformers</h3>
<p>Zeping Yu,Kailai Yang,Zhiwei Liu,Sophia Ananiadou</p>
<p><a href='http://arxiv.org/abs/2312.12141v1'>http://arxiv.org/abs/2312.12141v1</a></p>
<p><b>Compressor summary</b>: This paper explores residual connections in transformers to better understand how they store and merge knowledge for language modeling, and proposes a method to analyze the influence of previous layers.</p><hr><h3>Best Arm Identification with Fixed Budget: A Large Deviation Perspective</h3>
<p>Po-An Wang,Ruo-Chun Tzeng,Alexandre Proutiere</p>
<p><a href='http://arxiv.org/abs/2312.12137v1'>http://arxiv.org/abs/2312.12137v1</a></p>
<p><b>Compressor summary</b>: The paper studies how to identify the best arm in stochastic Multi-Armed Bandits with a fixed sampling budget, and proposes a new adaptive algorithm that outperforms existing ones using Large Deviation techniques.</p><hr><h3>Object-Aware Domain Generalization for Object Detection</h3>
<p>Wooju Lee,Dasol Hong,Hyungtae Lim,Hyun Myung</p>
<p><a href='http://arxiv.org/abs/2312.12133v1'>http://arxiv.org/abs/2312.12133v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an object-aware domain generalization method for single-domain generalization in object detection, using data augmentation and training strategy to improve object localization and classification.</p><hr><h3>Probabilistic Prediction of Longitudinal Trajectory Considering Driving  Heterogeneity with Interpretability</h3>
<p>Shuli Wang,Kun Gao,Lanfang Zhang,Yang Liu,Lei Chen</p>
<p><a href='http://arxiv.org/abs/2312.12123v1'>http://arxiv.org/abs/2312.12123v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework that combines Mixture Density Networks and LSTM-based encoder-decoder networks to predict vehicle trajectories considering driver heterogeneity, achieving better predictions than existing models.</p><hr><h3>ZS-SRT: An Efficient Zero-Shot Super-Resolution Training Method for  Neural Radiance Fields</h3>
<p>Xiang Feng,Yongbo He,Yubo Wang,Chengkai Wang,Zhenzhong Kuang,Jiajun Ding,Feiwei Qin,Jun Yu,Jianping Fan</p>
<p><a href='http://arxiv.org/abs/2312.12122v1'>http://arxiv.org/abs/2312.12122v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a zero-shot super-resolution framework for NeRF that uses internal learning to synthesize high-quality high-resolution novel views from low-resolution training data, without requiring external high-resolution data or additional scene information.</p><hr><h3>Mindful Explanations: Prevalence and Impact of Mind Attribution in XAI  Research</h3>
<p>Susanne Hindennach,Lei Shi,Filip Miletić,Andreas Bulling</p>
<p><a href='http://arxiv.org/abs/2312.12119v1'>http://arxiv.org/abs/2312.12119v1</a></p>
<p><b>Compressor summary</b>: The study examines how mind-attributing explanations in AI research affect perceptions of AI awareness and responsibility, finding that they can conceal AI responsibility from users.</p><hr><h3>Shaping Up SHAP: Enhancing Stability through Layer-Wise Neighbor  Selection</h3>
<p>Gwladys Kelodjou,Laurence Rozé,Véronique Masson,Luis Galárraga,Romaric Gaudel,Maurice Tchuente,Alexandre Termier</p>
<p><a href='http://arxiv.org/abs/2312.12115v1'>http://arxiv.org/abs/2312.12115v1</a></p>
<p><b>Compressor summary</b>: The paper proposes two improvements for Kernel SHAP, a post-hoc explainability method for black-box machine learning models: (1) making it fully stable by changing its neighbor selection procedure, and (2) using the coalitions of Layer 1 for faster and more meaningful feature attribution.</p><hr><h3>Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation  in ultra low-data regimes</h3>
<p>Nabeel Seedat,Nicolas Huynh,Boris van Breugel,Mihaela van der Schaar</p>
<p><a href='http://arxiv.org/abs/2312.12112v1'>http://arxiv.org/abs/2312.12112v1</a></p>
<p><b>Compressor summary</b>: CLLM is a method that uses large language models to generate and curate high-quality augmented datasets for machine learning tasks in low-data settings, improving performance compared to conventional generators.</p><hr><h3>Knowledge Graph Error Detection with Contrastive Confidence Adaption</h3>
<p>Xiangyu Liu,Yang Liu,Wei Hu</p>
<p><a href='http://arxiv.org/abs/2312.12108v1'>http://arxiv.org/abs/2312.12108v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new model, CCA, that uses both textual and graph information to better detect errors in knowledge graphs, especially noisy triplets with similar correct ones.</p><hr><h3>I-CEE: Tailoring Explanations of Image Classifications Models to User  Expertise</h3>
<p>Yao Rong,Peizhu Qian,Vaibhav Unhelkar,Enkelejda Kasneci</p>
<p><a href='http://arxiv.org/abs/2312.12102v1'>http://arxiv.org/abs/2312.12102v1</a></p>
<p><b>Compressor summary</b>: I-CEE is a framework for explaining image classification models to users based on their expertise level, using example images and local explanations tailored to each user.</p><hr><h3>Domain Generalization in LiDAR Semantic Segmentation Leveraged by  Density Discriminative Feature Embedding</h3>
<p>Jaeyeul Kim,Jungwan Woo,Jeonghoon Kim,Sunghoon Im</p>
<p><a href='http://arxiv.org/abs/2312.12098v1'>http://arxiv.org/abs/2312.12098v1</a></p>
<p><b>Compressor summary</b>: The paper introduces DDFE, a module that extracts features related to LiDAR point cloud density and improves domain generalization for 3D perception tasks.</p><hr><h3>DLCA-Recon: Dynamic Loose Clothing Avatar Reconstruction from Monocular  Videos</h3>
<p>Chunjie Luo,Fei Luo,Yusen Wang,Enxu Zhao,Chunxia Xiao</p>
<p><a href='http://arxiv.org/abs/2312.12096v1'>http://arxiv.org/abs/2312.12096v1</a></p>
<p><b>Compressor summary</b>: DLCA-Recon is a method to create human avatars from monocular videos, using physical connection information and dynamic deformation fields to accurately model loose clothing movement.</p><hr><h3>GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion  Prediction</h3>
<p>Haodong Yan,Zhiming Hu,Syn Schmitt,Andreas Bulling</p>
<p><a href='http://arxiv.org/abs/2312.12090v1'>http://arxiv.org/abs/2312.12090v1</a></p>
<p><b>Compressor summary</b>: GazeMoDiff is a new model that uses eye gaze to guide the generation of realistic human body motions for virtual reality applications, outperforming existing methods.</p><hr><h3>Learning Subject-Aware Cropping by Outpainting Professional Photos</h3>
<p>James Hong,Lu Yuan,Michaël Gharbi,Matthew Fisher,Kayvon Fatahalian</p>
<p><a href='http://arxiv.org/abs/2312.12080v1'>http://arxiv.org/abs/2312.12080v1</a></p>
<p><b>Compressor summary</b>: GenCrop is a weakly-supervised method that learns subject-aware cropping from stock images and text-to-image diffusion models, achieving competitive results with supervised methods.</p><hr><h3>PICNN: A Pathway towards Interpretable Convolutional Neural Networks</h3>
<p>Wengang Guo,Jiayi Yang,Huilin Yin,Qijun Chen,Wei Ye</p>
<p><a href='http://arxiv.org/abs/2312.12068v1'>http://arxiv.org/abs/2312.12068v1</a></p>
<p><b>Compressor summary</b>: PICNN is a novel method that improves the interpretability and performance of CNNs by clustering filters into class-specific groups using Bernoulli sampling and a reparameterization trick.</p><hr><h3>PPO-Clip Attains Global Optimality: Towards Deeper Understandings of  Clipping</h3>
<p>Nai-Chieh Huang,Ping-Chun Hsieh,Kuo-Hao Ho,I-Chen Wu</p>
<p><a href='http://arxiv.org/abs/2312.12065v1'>http://arxiv.org/abs/2312.12065v1</a></p>
<p><b>Compressor summary</b>: The paper provides the first global convergence results for a variant of the Proximal Policy Optimization algorithm with clipping, using new analysis techniques and showing that the clipping range affects only the convergence constant.</p><hr><h3>MPI Planar Correction of Pulse Based ToF Cameras</h3>
<p>Marian-Leontin Pop,Levente Tamas</p>
<p><a href='http://arxiv.org/abs/2312.12064v1'>http://arxiv.org/abs/2312.12064v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using Feature Pyramid Networks to reduce multipath inference artifacts in pulse-based ToF cameras, improving surface detection on planar surfaces.</p><hr><h3>Extension of the Dip-test Repertoire -- Efficient and Differentiable  p-value Calculation for Clustering</h3>
<p>Lena G. M. Bauer,Collin Leiber,Christian Böhm,Claudia Plant</p>
<p><a href='http://arxiv.org/abs/2312.12050v1'>http://arxiv.org/abs/2312.12050v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a sigmoid function to replace look-up tables for transforming Dip-values to Dip-p-values, improving computation speed and integration with learning algorithms.</p><hr><h3>XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX</h3>
<p>Alexander Nikulin,Vladislav Kurenkov,Ilya Zisman,Artem Agarkov,Viacheslav Sinii,Sergey Kolesnikov</p>
<p><a href='http://arxiv.org/abs/2312.12044v1'>http://arxiv.org/abs/2312.12044v1</a></p>
<p><b>Compressor summary</b>: XLand-MiniGrid is a JAX-based library for meta-reinforcement learning research with scalable grid-worlds and diverse tasks.</p><hr><h3>Pose2Gaze: Generating Realistic Human Gaze Behaviour from Full-body  Poses using an Eye-body Coordination Model</h3>
<p>Zhiming Hu,Jiahui Xu,Syn Schmitt,Andreas Bulling</p>
<p><a href='http://arxiv.org/abs/2312.12042v1'>http://arxiv.org/abs/2312.12042v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes human eye and body movement coordination during everyday activities, and proposes Pose2Gaze, a model that generates realistic eye movements from full-body poses, outperforming existing methods.</p><hr><h3>Founder-GPT: Self-play to evaluate the Founder-Idea fit</h3>
<p>Sichao Xiong,Yigit Ihlamur</p>
<p><a href='http://arxiv.org/abs/2312.12037v1'>http://arxiv.org/abs/2312.12037v1</a></p>
<p><b>Compressor summary</b>: The research develops a new way to evaluate how well startup founders match their ideas using advanced language models, suggesting that each idea's success depends on the founder's background.</p><hr><h3>Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint  Method</h3>
<p>Jiachun Pan,Hanshu Yan,Jun Hao Liew,Jiashi Feng,Vincent Y. F. Tan</p>
<p><a href='http://arxiv.org/abs/2312.12030v1'>http://arxiv.org/abs/2312.12030v1</a></p>
<p><b>Compressor summary</b>: SAG is a training-free guidance technique for diffusion models that uses symplectic adjoint method to accurately estimate clean images and generate high-quality images and videos.</p><hr><h3>EyePreserve: Identity-Preserving Iris Synthesis</h3>
<p>Siamul Karim Khan,Patrick Tinsley,Mahsa Mitcheff,Patrick Flynn,Kevin W. Bowyer,Adam Czajka</p>
<p><a href='http://arxiv.org/abs/2312.12028v1'>http://arxiv.org/abs/2312.12028v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method to create realistic iris images that change size and preserve identity, improving iris recognition and forensic analysis.</p><hr><h3>Synergistic Anchored Contrastive Pre-training for Few-Shot Relation  Extraction</h3>
<p>DaLuo,Yanglei Gan,Rui Hou,Run Lin,Qiao Liu,Yuxiang Cai,Wannian Gao</p>
<p><a href='http://arxiv.org/abs/2312.12021v1'>http://arxiv.org/abs/2312.12021v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework for few-shot relation extraction that uses sentence-anchored and label-anchored contrastive losses to learn robust and uniform representations from incomplete instance-label pairs.</p><hr><h3>Flexible categorization using formal concept analysis and  Dempster-Shafer theory</h3>
<p>Marcel Boersma,Krishna Manoorkar,Alessandra Palmigiano,Mattia Panettiere,Apostolos Tzimoulis,Nachoem Wijnberg</p>
<p><a href='http://arxiv.org/abs/2312.12010v1'>http://arxiv.org/abs/2312.12010v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method to categorize business processes using bipartite graphs, formal concept analysis, and Dempster-Shafer theory to obtain explainable results for auditing purposes.</p><hr><h3>Active Preference Inference using Language Models and Probabilistic  Reasoning</h3>
<p>Top Piriyakulkij,Volodymyr Kuleshov,Kevin Ellis</p>
<p><a href='http://arxiv.org/abs/2312.12009v1'>http://arxiv.org/abs/2312.12009v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an algorithm to help large language models ask better questions and infer user preferences more efficiently in interactive systems, improving task performance and reducing user interactions.</p><hr><h3>Can ChatGPT be Your Personal Medical Assistant?</h3>
<p>Md. Rafiul Biswas,Ashhadul Islam,Zubair Shah,Wajdi Zaghouani,Samir Brahim Belhaouari</p>
<p><a href='http://arxiv.org/abs/2312.12006v1'>http://arxiv.org/abs/2312.12006v1</a></p>
<p><b>Compressor summary</b>: This study evaluates a fine-tuned ChatGPT model as a personal medical assistant in Arabic, using online datasets and human evaluation metrics.</p><hr><h3>Diffusing More Objects for Semi-Supervised Domain Adaptation with Less  Labeling</h3>
<p>Leander van den Heuvel,Gertjan Burghouts,David W. Zhang,Gwenn Englebienne,Sabina B. van Rooij</p>
<p><a href='http://arxiv.org/abs/2312.12000v1'>http://arxiv.org/abs/2312.12000v1</a></p>
<p><b>Compressor summary</b>: The text proposes a diffusion model that refines bounding boxes for object detection, improves performance, and uses the results for semi-supervised learning without human involvement.</p><hr><h3>Coreference Graph Guidance for Mind-Map Generation</h3>
<p>Zhuowei Zhang,Mengting Hu,Yinhao Bai,Zhen Zhang</p>
<p><a href='http://arxiv.org/abs/2312.11997v1'>http://arxiv.org/abs/2312.11997v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a coreference-guided mind-map generation network (CMGN) that uses a coreference graph to capture structural information and improve the understanding of document logic and semantics, achieving better performance than existing methods.</p><hr><h3>Optimizing Diffusion Noise Can Serve As Universal Motion Priors</h3>
<p>Korrawe Karunratanakul,Konpat Preechakul,Emre Aksan,Thabo Beeler,Supasorn Suwajanakorn,Siyu Tang</p>
<p><a href='http://arxiv.org/abs/2312.11994v1'>http://arxiv.org/abs/2312.11994v1</a></p>
<p><b>Compressor summary</b>: DNO is a new method that uses existing motion diffusion models to optimize motion-related tasks without training new models or relying on complex algorithms.</p><hr><h3>Climate Change from Large Language Models</h3>
<p>Hongyin Zhu,Prayag Tiwari</p>
<p><a href='http://arxiv.org/abs/2312.11985v1'>http://arxiv.org/abs/2312.11985v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a framework to assess climate crisis knowledge in large language models using diverse questions and comprehensive metrics, revealing gaps in their up-to-date information.</p><hr><h3>Fluctuation-based Adaptive Structured Pruning for Large Language Models</h3>
<p>Yongqi An,Xu Zhao,Tao Yu,Ming Tang,Jinqiao Wang</p>
<p><a href='http://arxiv.org/abs/2312.11983v1'>http://arxiv.org/abs/2312.11983v1</a></p>
<p><b>Compressor summary</b>: FLAP is a novel retraining-free structured pruning framework for large language models that reduces storage, enhances inference speed, and outperforms existing methods without retraining.</p><hr><h3>When Model Meets New Normals: Test-time Adaptation for Unsupervised  Time-series Anomaly Detection</h3>
<p>Dongmin Kim,Sunghyun Park,Jaegul Choo</p>
<p><a href='http://arxiv.org/abs/2312.11976v1'>http://arxiv.org/abs/2312.11976v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for detecting anomalies in time-series data that adapts to changing normalities over time and improves performance on real-world benchmarks.</p><hr><h3>Continual Learning: Forget-free Winning Subnetworks for Video  Representations</h3>
<p>Haeyong Kang,Jaehong Yoon,Sung Ju Hwang,Chang D. Yoo</p>
<p><a href='http://arxiv.org/abs/2312.11973v1'>http://arxiv.org/abs/2312.11973v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Winning Subnetworks (WSN), a method that uses reused weights in dense networks to improve continual learning, and proposes the Fourier Subneural Operator (FSO) to address limitations in video incremental learning.</p><hr><h3>Expressive Forecasting of 3D Whole-body Human Motions</h3>
<p>Pengxiang Ding,Qiongjie Cui,Min Zhang,Mengyuan Liu,Haofan Wang,Donglin Wang</p>
<p><a href='http://arxiv.org/abs/2312.11972v1'>http://arxiv.org/abs/2312.11972v1</a></p>
<p><b>Compressor summary</b>: The text proposes a novel framework that predicts both coarse and fine-grained human activities collaboratively, achieving state-of-the-art performance on a large-scale benchmark.</p><hr><h3>Large Language Models Empowered Agent-based Modeling and Simulation: A  Survey and Perspectives</h3>
<p>Chen Gao,Xiaochong Lan,Nian Li,Yuan Yuan,Jingtao Ding,Zhilun Zhou,Fengli Xu,Yong Li</p>
<p><a href='http://arxiv.org/abs/2312.11970v1'>http://arxiv.org/abs/2312.11970v1</a></p>
<p><b>Compressor summary</b>: This paper reviews how large language models can improve agent-based modeling and simulation, exploring their challenges and applications in various domains.</p><hr><h3>GroupMixNorm Layer for Learning Fair Models</h3>
<p>Anubha Pandey,Aditi Rai,Maneet Singh,Deepak Bhatt,Tanmoy Bhowmik</p>
<p><a href='http://arxiv.org/abs/2312.11969v1'>http://arxiv.org/abs/2312.11969v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to reduce bias in automated prediction algorithms by mixing feature statistics across different groups based on protected attributes, improving fairness metrics with minimal accuracy loss.</p><hr><h3>Context Disentangling and Prototype Inheriting for Robust Visual  Grounding</h3>
<p>Wei Tang,Liang Li,Xuejing Liu,Lu Jin,Jinhui Tang,Zechao Li</p>
<p><a href='http://arxiv.org/abs/2312.11967v1'>http://arxiv.org/abs/2312.11967v1</a></p>
<p><b>Compressor summary</b>: Our proposed framework for visual grounding uses context disentangling and prototype inheriting to improve discrimination and generalization, achieving state-of-the-art results on standard and open-vocabulary scenes.</p><hr><h3>Vertical Symbolic Regression</h3>
<p>Nan Jiang,Md Nasim,Yexiang Xue</p>
<p><a href='http://arxiv.org/abs/2312.11955v1'>http://arxiv.org/abs/2312.11955v1</a></p>
<p><b>Compressor summary</b>: Vertical Symbolic Regression is a method to speed up AI-driven scientific discovery by fitting simple expressions involving a few independent variables at a time and gradually adding more variables, resulting in a significantly smaller search space than horizontal approaches.</p><hr><h3>Adversarial AutoMixup</h3>
<p>Huafeng Qin,Xin Jin,Yun Jiang,Mounim A. El-Yacoubi,Xinbo Gao</p>
<p><a href='http://arxiv.org/abs/2312.11954v1'>http://arxiv.org/abs/2312.11954v1</a></p>
<p><b>Compressor summary</b>: AdAutomixup is an adversarial automatic mixup augmentation method that generates diverse and challenging mixed samples to improve image classification accuracy by alternatively optimizing a mixed example generator and a target classifier.</p><hr><h3>Automatic Parameter Selection for Non-Redundant Clustering</h3>
<p>Collin Leiber,Dominik Mautz,Claudia Plant,Christian Böhm</p>
<p><a href='http://arxiv.org/abs/2312.11952v1'>http://arxiv.org/abs/2312.11952v1</a></p>
<p><b>Compressor summary</b>: The paper presents a framework using the Minimum Description Length Principle (MDL) to automatically find the number of subspaces and clusters in high-dimensional datasets, with efficient procedures for splitting, merging, and outlier detection.</p><hr><h3>Emotion Rendering for Conversational Speech Synthesis with Heterogeneous  Graph-Based Context Modeling</h3>
<p>Rui Liu,Yifan Hu,Yi Ren,Xiang Yin,Haizhou Li</p>
<p><a href='http://arxiv.org/abs/2312.11947v1'>http://arxiv.org/abs/2312.11947v1</a></p>
<p><b>Compressor summary</b>: The paper proposes ECSS, a novel model that enhances emotion understanding using a heterogeneous graph-based mechanism and achieves emotion rendering with contrastive learning, improving emotional CSS performance.</p><hr><h3>Multi-Granularity Information Interaction Framework for Incomplete  Utterance Rewriting</h3>
<p>Haowei Du,Dingyu Zhang,Chen Li,Yang Li,Dongyan Zhao</p>
<p><a href='http://arxiv.org/abs/2312.11945v1'>http://arxiv.org/abs/2312.11945v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new framework for incomplete utterance rewriting that captures multi-granularity semantic information, selects relevant context, and constructs an edit matrix, achieving state-of-the-art results on two benchmark datasets.</p><hr><h3>Time-Series Contrastive Learning against False Negatives and Class  Imbalance</h3>
<p>Xiyuan Jin,Jing Wang,Lei Liu,Youfang Lin</p>
<p><a href='http://arxiv.org/abs/2312.11939v1'>http://arxiv.org/abs/2312.11939v1</a></p>
<p><b>Compressor summary</b>: This study introduces a modification to time-series contrastive learning that addresses false negatives and class imbalance issues, improving representation learning for minority classes using instance graphs and semi-supervised classification.</p><hr><h3>DMT: Comprehensive Distillation with Multiple Self-supervised Teachers</h3>
<p>Yuang Liu,Jing Wang,Qiang Zhou,Fan Wang,Jun Wang,Wei Zhang</p>
<p><a href='http://arxiv.org/abs/2312.11938v1'>http://arxiv.org/abs/2312.11938v1</a></p>
<p><b>Compressor summary</b>: DMT is a pretraining method that uses multiple self-supervised models to improve general visual representations and achieve state-of-the-art results on classification and dense tasks.</p><hr><h3>Parameterized Decision-making with Multi-modal Perception for Autonomous  Driving</h3>
<p>Yuyang Xia,Shuncheng Liu,Quanlin Yu,Liwei Deng,You Zhang,Han Su,Kai Zheng</p>
<p><a href='http://arxiv.org/abs/2312.11935v1'>http://arxiv.org/abs/2312.11935v1</a></p>
<p><b>Compressor summary</b>: AUTO is a deep reinforcement learning framework for autonomous vehicles that handles complex environments and considers the impact on surrounding vehicles, leading to improved safety, efficiency, and comfort.</p><hr><h3>Identification of Causal Structure with Latent Variables Based on Higher  Order Cumulants</h3>
<p>Wei Chen,Zhiyi Huang,Ruichu Cai,Zhifeng Hao,Kun Zhang</p>
<p><a href='http://arxiv.org/abs/2312.11934v1'>http://arxiv.org/abs/2312.11934v1</a></p>
<p><b>Compressor summary</b>: The text describes a novel method to identify causal edges between observed variables using higher-order cumulants and latent variable influence, with an asymmetry criterion to determine the causal direction.</p><hr><h3>Dynamic Frequency Domain Graph Convolutional Network for Traffic  Forecasting</h3>
<p>Yujie Li,Zezhi Shao,Yongjun Xu,Qiang Qiu,Zhaogang Cao,Fei Wang</p>
<p><a href='http://arxiv.org/abs/2312.11933v1'>http://arxiv.org/abs/2312.11933v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel dynamic frequency domain graph convolution network (DFDGCN) for traffic prediction that captures spatial dependencies, mitigates time-shift effects, and handles noise in data.</p><hr><h3>Empowering Dual-Level Graph Self-Supervised Pretraining with Motif  Discovery</h3>
<p>Pengwei Yan,Kaisong Song,Zhuoren Jiang,Yangyang Kang,Tianqianjin Lin,Changlong Sun,Xiaozhong Liu</p>
<p><a href='http://arxiv.org/abs/2312.11927v1'>http://arxiv.org/abs/2312.11927v1</a></p>
<p><b>Compressor summary</b>: DGPM is a new method that improves graph pretraining by discovering and using significant graph motifs for better representation learning and transferability.</p><hr><h3>Big Learning Expectation Maximization</h3>
<p>Yulai Cong,Sijia Li</p>
<p><a href='http://arxiv.org/abs/2312.11926v1'>http://arxiv.org/abs/2312.11926v1</a></p>
<p><b>Compressor summary</b>: The Big Learning EM algorithm improves mixture model training by using a foundation model approach to avoid bad local optima and achieve optimal results.</p><hr><h3>IPAD: Iterative, Parallel, and Diffusion-based Network for Scene Text  Recognition</h3>
<p>Xiaomeng Yang,Zhi Qiao,Yu Zhou,Weiping Wang</p>
<p><a href='http://arxiv.org/abs/2312.11923v1'>http://arxiv.org/abs/2312.11923v1</a></p>
<p><b>Compressor summary</b>: This paper presents a fast and accurate scene text recognition method using a parallel and iterative decoder with an easy-first strategy and discrete diffusion for bidirectional context exploration.</p><hr><h3>Relation-Aware Question Answering for Heterogeneous Knowledge Graphs</h3>
<p>Haowei Du,Quzhe Huang,Chen Li,Chen Zhang,Yang Li,Dongyan Zhao</p>
<p><a href='http://arxiv.org/abs/2312.11922v1'>http://arxiv.org/abs/2312.11922v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for answering questions in knowledge graphs using dual relation graphs that improve the representations of entities and relations and achieve better performance.</p><hr><h3>External Knowledge Augmented Polyphone Disambiguation Using Large  Language Model</h3>
<p>Chen Li</p>
<p><a href='http://arxiv.org/abs/2312.11920v1'>http://arxiv.org/abs/2312.11920v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel TTS method using large language models and prompt learning to disambiguate polyphonic characters in Mandarin Chinese.</p><hr><h3>A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on  NVIDIA Hopper Architecture using the CUTLASS Library</h3>
<p>Ganesh Bikshandi,Jay Shah</p>
<p><a href='http://arxiv.org/abs/2312.11918v1'>http://arxiv.org/abs/2312.11918v1</a></p>
<p><b>Compressor summary</b>: The paper presents an optimized implementation of the FlashAttention-2 attention algorithm on NVIDIA Hopper GPUs using the CUTLASS library, achieving 20-50% higher FLOPs/s than the previous Ampere version.</p><hr><h3>EVI-SAM: Robust, Real-time, Tightly-coupled Event-Visual-Inertial State  Estimation and 3D Dense Mapping</h3>
<p>Weipeng Guan,Peiyu Chen,Huibin Zhao,Yu Wang,Peng Lu</p>
<p><a href='http://arxiv.org/abs/2312.11911v1'>http://arxiv.org/abs/2312.11911v1</a></p>
<p><b>Compressor summary</b>: Event cameras use motion-activated sensors to track 6 DoF pose and reconstruct 3D scenes with high accuracy, robustness, and efficiency using a novel event-based hybrid tracking framework called EVI-SAM.</p><hr><h3>Short-Term Multi-Horizon Line Loss Rate Forecasting of a Distribution  Network Using Attention-GCN-LSTM</h3>
<p>Jie Liu,Yijia Cao,Yong Li,Yixiu Guo,Wei Deng</p>
<p><a href='http://arxiv.org/abs/2312.11898v1'>http://arxiv.org/abs/2312.11898v1</a></p>
<p><b>Compressor summary</b>: The study introduces a new method (Attention-GCN-LSTM) that uses graph convolutional networks, long short-term memory, and attention to predict line loss rates accurately across multiple horizons in distribution networks.</p><hr><h3>Text-Conditioned Resampler For Long Form Video Understanding</h3>
<p>Bruno Korbar,Yongqin Xian,Alessio Tonioni,Andrew Zisserman,Federico Tombari</p>
<p><a href='http://arxiv.org/abs/2312.11897v1'>http://arxiv.org/abs/2312.11897v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a text-conditioned video resampler module that uses a frozen visual encoder and a large language model to process long videos for various tasks, achieving state-of-the-art results on several benchmarks.</p><hr><h3>3D-LFM: Lifting Foundation Model</h3>
<p>Mosam Dabhi,Laszlo A. Jeni,Simon Lucey</p>
<p><a href='http://arxiv.org/abs/2312.11894v1'>http://arxiv.org/abs/2312.11894v1</a></p>
<p><b>Compressor summary</b>: The 3D Lifting Foundation Model (3D-LFM) is a transformer-based approach that can reconstruct various object classes from 2D landmarks, overcoming limitations of traditional methods and handling occlusions and perspectives.</p><hr><h3>Difficulty-Focused Contrastive Learning for Knowledge Tracing with a  Large Language Model-Based Difficulty Prediction</h3>
<p>Unggi Lee,Sungjun Yoon,Joon Seo Yun,Kyoungsoo Park,YoungHoon Jung,Damji Stratton,Hyeoncheol Kim</p>
<p><a href='http://arxiv.org/abs/2312.11890v1'>http://arxiv.org/abs/2312.11890v1</a></p>
<p><b>Compressor summary</b>: The paper introduces new methods to improve knowledge tracing models by considering question and concept difficulty levels using contrastive learning and a large language model for prediction.</p><hr><h3>ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for  Accelerating Language Models Inference</h3>
<p>Ziqian Zeng,Yihuai Hong,Hongliang Dai,Huiping Zhuang,Cen Chen</p>
<p><a href='http://arxiv.org/abs/2312.11882v1'>http://arxiv.org/abs/2312.11882v1</a></p>
<p><b>Compressor summary</b>: ConsistentEE is a reinforcement learning-based early exiting method for efficient inference that ensures correct prediction by one internal classifier and adapts to instance difficulty.</p><hr><h3>Punctuation restoration Model and Spacing Model for Korean Ancient  Document</h3>
<p>Taehong Jang,Joonmo Ahn,Sojung Lucia Kim</p>
<p><a href='http://arxiv.org/abs/2312.11881v1'>http://arxiv.org/abs/2312.11881v1</a></p>
<p><b>Compressor summary</b>: The authors developed models to predict punctuation and spacing for Korean historical texts, achieving good results and enabling fast inference on low-performance GPUs.</p><hr><h3>Point Cloud Segmentation Using Transfer Learning with RandLA-Net: A Case  Study on Urban Areas</h3>
<p>Alperen Enes Bayar,Ufuk Uyan,Elif Toprak,Cao Yuheng,Tang Juncheng,Ahmet Alp Kindiroglu</p>
<p><a href='http://arxiv.org/abs/2312.11880v1'>http://arxiv.org/abs/2312.11880v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper studies how RandLA-Net, a neural network, can segment 3D point cloud data in urban environments
- It uses transfer learning and class remapping to overcome data scarcity and adapt to specific cities in China
- It achieves over 80% F1 score for each city, showing its effectiveness

Summary:
The paper applies RandLA-Net with transfer learning and class remapping to segment 3D point cloud data in urban areas of three Chinese cities, achieving high accuracy.</p><hr><h3>Sparse is Enough in Fine-tuning Pre-trained Large Language Model</h3>
<p>Weixi Song,Zuchao Li,Lefei Zhang,Hai Zhao,Bo Du</p>
<p><a href='http://arxiv.org/abs/2312.11875v1'>http://arxiv.org/abs/2312.11875v1</a></p>
<p><b>Compressor summary</b>: The paper investigates the loss landscape transition in fine-tuning pre-trained models, proposes Sparse Increment Fine-Tuning (SIFT) algorithm to exploit sparsity in gradients for efficient adaptation, and shows its effectiveness on various tasks.</p><hr><h3>Beyond Prototypes: Semantic Anchor Regularization for Better  Representation Learning</h3>
<p>Yanqi Ge,Qiang Nie,Ye Huang,Yong Liu,Chengjie Wang,Feng Zheng,Wen Li,Lixin Duan</p>
<p><a href='http://arxiv.org/abs/2312.11872v1'>http://arxiv.org/abs/2312.11872v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Semantic Anchor Regularization, a method that uses pre-defined class anchors to guide feature learning and achieve compactness within classes and separability between classes, while avoiding biases from long-tailed data.</p><hr><h3>A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT</h3>
<p>Zizhong Li,Haopeng Zhang,Jiawei Zhang</p>
<p><a href='http://arxiv.org/abs/2312.11870v1'>http://arxiv.org/abs/2312.11870v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an augmented fake news dataset using ChatGPT for fact-checking, which can help reduce bias and improve detection compared to relying on human journalists alone.</p><hr><h3>Point Cloud Part Editing: Segmentation, Generation, Assembly, and  Selection</h3>
<p>Kaiyi Zhang,Yang Chen,Ximing Yang,Weizhong Zhang,Cheng Jin</p>
<p><a href='http://arxiv.org/abs/2312.11867v1'>http://arxiv.org/abs/2312.11867v1</a></p>
<p><b>Compressor summary</b>: The text proposes a four-stage process for point cloud part editing and introduces SGAS, a model that uses feature disentanglement and constraint strategies to improve diversity, fidelity, and quality.</p><hr><h3>Large Language Models Play StarCraft II: Benchmarks and A Chain of  Summarization Approach</h3>
<p>Weiyu Ma,Qirui Mi,Xue Yan,Yuqiao Wu,Runji Lin,Haifeng Zhang,Jun Wang</p>
<p><a href='http://arxiv.org/abs/2312.11865v1'>http://arxiv.org/abs/2312.11865v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper explores using large language model agents for StarCraft II, a complex RTS game.
- It develops a textual environment, TextStarCraft II, and a Chain of Summarization method to leverage LLMs' reasoning abilities.
- It evaluates the performance and knowledge of LLM agents in the game and compares them with human experts and built-in AI.
- The results show that LLM agents can achieve similar or better performance than average players and defeat the built-in AI.

Summary:
The paper presents a textual StarCraft II environment and a summarization method to enable large language model agents to play the game effectively. It shows that LLM agents can master StarCraft II knowledge and outperform human experts and built-in AI in the game.</p><hr><h3>Neural Network Approximation for Pessimistic Offline Reinforcement  Learning</h3>
<p>Di Wu,Yuling Jiao,Li Shen,Haizhao Yang,Xiliang Lu</p>
<p><a href='http://arxiv.org/abs/2312.11863v1'>http://arxiv.org/abs/2312.11863v1</a></p>
<p><b>Compressor summary</b>: This paper analyzes the theoretical guarantees of deep reinforcement learning in offline decision-making scenarios, considering general neural network approximation and various data properties.</p><hr><h3>Topo-MLP : A Simplicial Network Without Message Passing</h3>
<p>Karthikeyan Natesan Ramamurthy,Aldo Guzmán-Sáenz,Mustafa Hajij</p>
<p><a href='http://arxiv.org/abs/2312.11862v1'>http://arxiv.org/abs/2312.11862v1</a></p>
<p><b>Compressor summary</b>: Topo-MLP is a fast and robust MLP-based algorithm to learn representations in higher order network models using a novel HONC loss that implicitly incorporates the simplicial structure.</p><hr><h3>SimCalib: Graph Neural Network Calibration based on Similarity between  Nodes</h3>
<p>Boshi Tang,Zhiyong Wu,Xixin Wu,Qiaochu Huang,Jun Chen,Shun Lei,Helen Meng</p>
<p><a href='http://arxiv.org/abs/2312.11858v1'>http://arxiv.org/abs/2312.11858v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel GNN calibration framework, SimCalib, that considers nodewise similarity at global and local levels to improve performance in cost-sensitive scenarios.</p><hr><h3>Self-supervised Learning for Enhancing Geometrical Modeling in 3D-Aware  Generative Adversarial Network</h3>
<p>Jiarong Guo,Xiaogang Xu,Hengshuang Zhao</p>
<p><a href='http://arxiv.org/abs/2312.11856v1'>http://arxiv.org/abs/2312.11856v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-supervised learning technique for 3D-GANs that uses an encoder and a cyclic constraint to improve the quality of 3D geometrical modeling.</p><hr><h3>Predicting Human Translation Difficulty with Neural Machine Translation</h3>
<p>Zheng Wei Lim,Ekaterina Vylomova,Charles Kemp,Trevor Cohn</p>
<p><a href='http://arxiv.org/abs/2312.11852v1'>http://arxiv.org/abs/2312.11852v1</a></p>
<p><b>Compressor summary</b>: The study uses data from human translators and a machine translation model to explore how surprisal and attention affect translation difficulty and time spent on translation tasks.</p><hr><h3>GCNext: Towards the Unity of Graph Convolutions for Human Motion  Prediction</h3>
<p>Xinshun Wang,Qiongjie Cui,Chen Chen,Mengyuan Liu</p>
<p><a href='http://arxiv.org/abs/2312.11850v1'>http://arxiv.org/abs/2312.11850v1</a></p>
<p><b>Compressor summary</b>: Universal Graph Convolution (UniGC) introduces a new graph convolution concept that adapts different graph convolutions as special cases and GCNext, a novel GCN-building paradigm that dynamically determines the best-fitting graph convolutions for human motion prediction, achieving state-of-the-art performance with up to 9x lower computational cost.</p><hr><h3>Active contours driven by local and global intensity fitting energy with  application to SAR image segmentation and its fast solvers</h3>
<p>Guangming Liu,Qi Liu,Jing Liang,Quanying Sun</p>
<p><a href='http://arxiv.org/abs/2312.11849v1'>http://arxiv.org/abs/2312.11849v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new variational active contour model based on Aubert-Aujol denoising that can segment images with multiplicative gamma noise, and proposes two fast fixed point algorithms to solve the problem efficiently.</p><hr><h3>Initializing Services in Interactive ML Systems for Diverse Users</h3>
<p>Avinandan Bose,Mihaela Curmei,Daniel L. Jiang,Jamie Morgenstern,Sarah Dean,Lillian J. Ratliff,Maryam Fazel</p>
<p><a href='http://arxiv.org/abs/2312.11846v1'>http://arxiv.org/abs/2312.11846v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a randomized algorithm to learn user preferences for multiple subpopulations with heterogeneous data distributions, and shows that it achieves near-optimal results in terms of total loss.</p><hr><h3>MixRT: Mixed Neural Representations For Real-Time NeRF Rendering</h3>
<p>Chaojian Li,Bichen Wu,Peter Vajda,Yingyan,Lin</p>
<p><a href='http://arxiv.org/abs/2312.11841v1'>http://arxiv.org/abs/2312.11841v1</a></p>
<p><b>Compressor summary</b>: MixRT is a novel NeRF representation that enables real-time rendering on edge devices using a low-quality mesh, a displacement map, and a compressed model.</p><hr><h3>The Validity of a Machine Learning-Based Video Game in the Objective  Screening of Attention Deficit Hyperactivity Disorder in Children Aged 5 to  12 Years</h3>
<p>Zeinab Zakani,Hadi Moradi,Sogand Ghasemzadeh,Maryam Riazi,Fatemeh Mortazavi</p>
<p><a href='http://arxiv.org/abs/2312.11832v1'>http://arxiv.org/abs/2312.11832v1</a></p>
<p><b>Compressor summary</b>: The FishFinder is a video game that can accurately identify ADHD in children by measuring their attention, impulsivity, and hyperactivity during gameplay.</p><hr><h3>Locally-Minimal Probabilistic Explanations</h3>
<p>Yacine Izza,Kuldeep S. Meel,Joao Marques-Silva</p>
<p><a href='http://arxiv.org/abs/2312.11831v1'>http://arxiv.org/abs/2312.11831v1</a></p>
<p><b>Compressor summary</b>: The paper presents efficient algorithms to compute approximate probabilistic abductive explanations for machine learning, addressing their theoretical and practical complexity.</p><hr><h3>TESS: A Multi-intent Parser for Conversational Multi-Agent Systems with  Decentralized Natural Language Understanding Models</h3>
<p>Burak Aksar,Yara Rizk,Tathagata Chakraborti</p>
<p><a href='http://arxiv.org/abs/2312.11828v1'>http://arxiv.org/abs/2312.11828v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an efficient algorithm for parsing and orchestrating multi-intent inputs in decentralized NLU for chatbots in multi-agent systems, achieving high accuracy and speed.</p><hr><h3>Decoupled Textual Embeddings for Customized Image Generation</h3>
<p>Yufei Cai,Yuxiang Wei,Zhilong Ji,Jinfeng Bai,Hu Han,Wangmeng Zuo</p>
<p><a href='http://arxiv.org/abs/2312.11826v1'>http://arxiv.org/abs/2312.11826v1</a></p>
<p><b>Compressor summary</b>: DETEX is a novel text-to-image generation approach that learns disentangled concept embeddings with multiple word embeddings and attribute mappers, achieving better representation and editability of the target concept.</p><hr><h3>An Adaptive Placement and Parallelism Framework for Accelerating RLHF  Training</h3>
<p>Youshao Xiao,Weichang Wu,Zhenglei Zhou,Fagui Mao,Shangchun Zhao,Lin Ju,Lei Liang,Xiaolu Zhang,Jun Zhou</p>
<p><a href='http://arxiv.org/abs/2312.11819v1'>http://arxiv.org/abs/2312.11819v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Large language models like ChatGPT are versatile but face challenges in RLHF training efficiency
- Current Flattening strategy treats all models as one entity and causes bottlenecks
- Proposed adaptive model placement framework with two strategies: Interleaving and Separation
- The framework improves the throughput, reduces memory redundancy, and communication costs
- Experiments show significant improvements over SOTA approaches in various scenarios

Summary:
The paper presents an adaptive model placement framework for distributed RLHF training that uses two strategies to improve efficiency, reduce bottlenecks, and outperform existing methods.</p><hr><h3>Root Cause Explanation of Outliers under Noisy Mechanisms</h3>
<p>Phuoc Nguyen,Truyen Tran,Sunil Gupta,Thin Nguyen,Svetha Venkatesh</p>
<p><a href='http://arxiv.org/abs/2312.11818v1'>http://arxiv.org/abs/2312.11818v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a noisy functional causal model for identifying root causes of anomalies in causal processes by considering both node and edge contributions, using Bayesian learning and inference methods and an efficient gradient-based attribution method.</p><hr><h3>A Dual-way Enhanced Framework from Text Matching Point of View for  Multimodal Entity Linking</h3>
<p>Shezheng Song,Shan Zhao,Chengyu Wang,Tianwei Yan,Shasha Li,Xiaoguang Mao,Meng Wang</p>
<p><a href='http://arxiv.org/abs/2312.11816v1'>http://arxiv.org/abs/2312.11816v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Multimodal Entity Linking (MEL) links ambiguous mentions with multimodal information to entities in Knowledge Graph (KG).
- Existing methods face challenges such as modality impurity and ambiguity.
- The paper proposes a Dual-way Enhanced (DWE) framework that refines queries with multimodal data, leverages fine-grained image attributes, and enriches entity semantics using Wikipedia descriptions.
- The method achieves state-of-the-art performance on three benchmarks.

Summary:
The paper presents DWE, a novel framework for MEL that improves query refinement, visual enhancement, and entity semantics using multimodal data and Wikipedia descriptions, and outperforms existing methods.</p><hr><h3>Urban Generative Intelligence (UGI): A Foundational Platform for Agents  in Embodied City Environment</h3>
<p>Fengli Xu,Jun Zhang,Chen Gao,Jie Feng,Yong Li</p>
<p><a href='http://arxiv.org/abs/2312.11813v1'>http://arxiv.org/abs/2312.11813v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Urban Generative Intelligence (UGI), a platform that uses large language models to create embodied agents for various urban tasks, simulating and addressing complex urban systems.</p><hr><h3>Advancements and Challenges in Arabic Optical Character Recognition: A  Comprehensive Survey</h3>
<p>Mahmoud SalahEldin Kasem,Mohamed Mahmoud,Hyun-Soo Kang</p>
<p><a href='http://arxiv.org/abs/2312.11812v1'>http://arxiv.org/abs/2312.11812v1</a></p>
<p><b>Compressor summary</b>: This paper reviews applications, methodologies, and challenges of Arabic Optical Character Recognition (OCR) and identifies research gaps to guide future development.</p><hr><h3>Gemini: A Family of Highly Capable Multimodal Models</h3>
<p>Gemini Team,Rohan Anil,Sebastian Borgeaud,Yonghui Wu,Jean-Baptiste Alayrac,Jiahui Yu,Radu Soricut,Johan Schalkwyk,Andrew M. Dai,Anja Hauth,Katie Millican,David Silver,Slav Petrov,Melvin Johnson,Ioannis Antonoglou,Julian Schrittwieser,Amelia Glaese,Jilin Chen,Emily Pitler,Timothy Lillicrap,Angeliki Lazaridou,Orhan Firat,James Molloy,Michael Isard,Paul R. Barham,Tom Hennigan,Benjamin Lee,Fabio Viola,Malcolm Reynolds,Yuanzhong Xu,Ryan Doherty,Eli Collins,Clemens Meyer,Eliza Rutherford,Erica Moreira,Kareem Ayoub,Megha Goel,George Tucker,Enrique Piqueras,Maxim Krikun,Iain Barr,Nikolay Savinov,Ivo Danihelka,Becca Roelofs,Anaïs White,Anders Andreassen,Tamara von Glehn,Lakshman Yagati,Mehran Kazemi,Lucas Gonzalez,Misha Khalman,Jakub Sygnowski,Alexandre Frechette,Charlotte Smith,Laura Culp,Lev Proleev,Yi Luan,Xi Chen,James Lottes,Nathan Schucher,Federico Lebron,Alban Rrustemi,Natalie Clay,Phil Crone,Tomas Kocisky,Jeffrey Zhao,Bartek Perz,Dian Yu,Heidi Howard,Adam Bloniarz,Jack W. Rae,Han Lu,Laurent Sifre,Marcello Maggioni,Fred Alcober,Dan Garrette,Megan Barnes,Shantanu Thakoor,Jacob Austin,Gabriel Barth-Maron,William Wong,Rishabh Joshi,Rahma Chaabouni,Deeni Fatiha,Arun Ahuja,Ruibo Liu,Yunxuan Li,Sarah Cogan,Jeremy Chen,Chao Jia,Chenjie Gu,Qiao Zhang,Jordan Grimstad,Ale Jakse Hartman,Martin Chadwick,Gaurav Singh Tomar,Xavier Garcia,Evan Senter,Emanuel Taropa,Thanumalayan Sankaranarayana Pillai,Jacob Devlin,Michael Laskin,Diego de Las Casas,Dasha Valter,Connie Tao,Lorenzo Blanco,Adrià Puigdomènech Badia,David Reitter,Mianna Chen,Jenny Brennan,Clara Rivera,Sergey Brin,Shariq Iqbal,Gabriela Surita,Jane Labanowski,Abhi Rao,Stephanie Winkler,Emilio Parisotto,Yiming Gu,Kate Olszewska,Yujing Zhang,Ravi Addanki,Antoine Miech,Annie Louis,Laurent El Shafey,Denis Teplyashin,Geoff Brown,Elliot Catt,Nithya Attaluri,Jan Balaguer,Jackie Xiang,Pidong Wang,Zoe Ashwood,Anton Briukhov,Albert Webson,Sanjay Ganapathy,Smit Sanghavi,Ajay Kannan,Ming-Wei Chang,Axel Stjerngren,Josip Djolonga,Yuting Sun,Ankur Bapna,Matthew Aitchison,Pedram Pejman,Henryk Michalewski,Tianhe Yu,Cindy Wang,Juliette Love,Junwhan Ahn,Dawn Bloxwich,Kehang Han,Peter Humphreys,Thibault Sellam,James Bradbury,Varun Godbole,Sina Samangooei,Bogdan Damoc,Alex Kaskasoli,Sébastien M. R. Arnold,Vijay Vasudevan,Shubham Agrawal,Jason Riesa,Dmitry Lepikhin,Richard Tanburn,Srivatsan Srinivasan,Hyeontaek Lim,Sarah Hodkinson,Pranav Shyam,Johan Ferret,Steven Hand,Ankush Garg,Tom Le Paine,Jian Li,Yujia Li,Minh Giang,Alexander Neitz,Zaheer Abbas,Sarah York,Machel Reid,Elizabeth Cole,Aakanksha Chowdhery,Dipanjan Das,Dominika Rogozińska,Vitaly Nikolaev,Pablo Sprechmann,Zachary Nado,Lukas Zilka,Flavien Prost,Luheng He,Marianne Monteiro,Gaurav Mishra,Chris Welty,Josh Newlan,Dawei Jia,Miltiadis Allamanis,Clara Huiyi Hu,Raoul de Liedekerke,Justin Gilmer,Carl Saroufim,Shruti Rijhwani,Shaobo Hou,Disha Shrivastava,Anirudh Baddepudi,Alex Goldin,Adnan Ozturel,Albin Cassirer,Yunhan Xu,Daniel Sohn,Devendra Sachan,Reinald Kim Amplayo,Craig Swanson,Dessie Petrova,Shashi Narayan,Arthur Guez,Siddhartha Brahma,Jessica Landon,Miteyan Patel,Ruizhe Zhao,Kevin Villela,Luyu Wang,Wenhao Jia,Matthew Rahtz,Mai Giménez,Legg Yeung,Hanzhao Lin,James Keeling,Petko Georgiev,Diana Mincu,Boxi Wu,Salem Haykal,Rachel Saputro,Kiran Vodrahalli,James Qin,Zeynep Cankara,Abhanshu Sharma,Nick Fernando,Will Hawkins,Behnam Neyshabur,Solomon Kim,Adrian Hutter,Priyanka Agrawal,Alex Castro-Ros,George van den Driessche,Tao Wang,Fan Yang,Shuo-yiin Chang,Paul Komarek,Ross McIlroy,Mario Lučić,Guodong Zhang,Wael Farhan,Michael Sharman,Paul Natsev,Paul Michel,Yong Cheng,Yamini Bansal,Siyuan Qiao,Kris Cao,Siamak Shakeri,Christina Butterfield,Justin Chung,Paul Kishan Rubenstein,Shivani Agrawal,Arthur Mensch,Kedar Soparkar,Karel Lenc,Timothy Chung,Aedan Pope,Loren Maggiore,Jackie Kay,Priya Jhakra,Shibo Wang,Joshua Maynez,Mary Phuong,Taylor Tobin,Andrea Tacchetti,Maja Trebacz,Kevin Robinson,Yash Katariya,Sebastian Riedel,Paige Bailey,Kefan Xiao,Nimesh Ghelani,Lora Aroyo,Ambrose Slone,Neil Houlsby,Xuehan Xiong,Zhen Yang,Elena Gribovskaya,Jonas Adler,Mateo Wirth,Lisa Lee,Music Li,Thais Kagohara,Jay Pavagadhi,Sophie Bridgers,Anna Bortsova,Sanjay Ghemawat,Zafarali Ahmed,Tianqi Liu,Richard Powell,Vijay Bolina,Mariko Iinuma,Polina Zablotskaia,James Besley,Da-Woon Chung,Timothy Dozat,Ramona Comanescu,Xiance Si,Jeremy Greer,Guolong Su,Martin Polacek,Raphaël Lopez Kaufman,Simon Tokumine,Hexiang Hu,Elena Buchatskaya,Yingjie Miao,Mohamed Elhawaty,Aditya Siddhant,Nenad Tomasev,Jinwei Xing,Christina Greer,Helen Miller,Shereen Ashraf,Aurko Roy,Zizhao Zhang,Ada Ma,Angelos Filos,Milos Besta,Rory Blevins,Ted Klimenko,Chih-Kuan Yeh,Soravit Changpinyo,Jiaqi Mu,Oscar Chang,Mantas Pajarskas,Carrie Muir,Vered Cohen,Charline Le Lan,Krishna Haridasan,Amit Marathe,Steven Hansen,Sholto Douglas,Rajkumar Samuel,Mingqiu Wang,Sophia Austin,Chang Lan,Jiepu Jiang,Justin Chiu,Jaime Alonso Lorenzo,Lars Lowe Sjösund,Sébastien Cevey,Zach Gleicher,Thi Avrahami,Anudhyan Boral,Hansa Srinivasan,Vittorio Selo,Rhys May,Konstantinos Aisopos,Léonard Hussenot,Livio Baldini Soares,Kate Baumli,Michael B. Chang,Adrià Recasens,Ben Caine,Alexander Pritzel,Filip Pavetic,Fabio Pardo,Anita Gergely,Justin Frye,Vinay Ramasesh,Dan Horgan,Kartikeya Badola,Nora Kassner,Subhrajit Roy,Ethan Dyer,Víctor Campos,Alex Tomala,Yunhao Tang,Dalia El Badawy,Elspeth White,Basil Mustafa,Oran Lang,Abhishek Jindal,Sharad Vikram,Zhitao Gong,Sergi Caelles,Ross Hemsley,Gregory Thornton,Fangxiaoyu Feng,Wojciech Stokowiec,Ce Zheng,Phoebe Thacker,Çağlar Ünlü,Zhishuai Zhang,Mohammad Saleh,James Svensson,Max Bileschi,Piyush Patil,Ankesh Anand,Roman Ring,Katerina Tsihlas,Arpi Vezer,Marco Selvi,Toby Shevlane,Mikel Rodriguez,Tom Kwiatkowski,Samira Daruki,Keran Rong,Allan Dafoe,Nicholas FitzGerald,Keren Gu-Lemberg,Mina Khan,Lisa Anne Hendricks,Marie Pellat,Vladimir Feinberg,James Cobon-Kerr,Tara Sainath,Maribeth Rauh,Sayed Hadi Hashemi,Richard Ives,Yana Hasson,YaGuang Li,Eric Noland,Yuan Cao,Nathan Byrd,Le Hou,Qingze Wang,Thibault Sottiaux,Michela Paganini,Jean-Baptiste Lespiau,Alexandre Moufarek,Samer Hassan,Kaushik Shivakumar,Joost van Amersfoort,Amol Mandhane,Pratik Joshi,Anirudh Goyal,Matthew Tung,Andrew Brock,Hannah Sheahan,Vedant Misra,Cheng Li,Nemanja Rakićević,Mostafa Dehghani,Fangyu Liu,Sid Mittal,Junhyuk Oh,Seb Noury,Eren Sezener,Fantine Huot,Matthew Lamm,Nicola De Cao,Charlie Chen,Gamaleldin Elsayed,Ed Chi,Mahdis Mahdieh,Ian Tenney,Nan Hua,Ivan Petrychenko,Patrick Kane,Dylan Scandinaro,Rishub Jain,Jonathan Uesato,Romina Datta,Adam Sadovsky,Oskar Bunyan,Dominik Rabiej,Shimu Wu,John Zhang,Gautam Vasudevan,Edouard Leurent,Mahmoud Alnahlawi,Ionut Georgescu,Nan Wei,Ivy Zheng,Betty Chan,Pam G Rabinovitch,Piotr Stanczyk,Ye Zhang,David Steiner,Subhajit Naskar,Michael Azzam,Matthew Johnson,Adam Paszke,Chung-Cheng Chiu,Jaume Sanchez Elias,Afroz Mohiuddin,Faizan Muhammad,Jin Miao,Andrew Lee,Nino Vieillard,Sahitya Potluri,Jane Park,Elnaz Davoodi,Jiageng Zhang,Jeff Stanway,Drew Garmon,Abhijit Karmarkar,Zhe Dong,Jong Lee,Aviral Kumar,Luowei Zhou,Jonathan Evens,William Isaac,Zhe Chen,Johnson Jia,Anselm Levskaya,Zhenkai Zhu,Chris Gorgolewski,Peter Grabowski,Yu Mao,Alberto Magni,Kaisheng Yao,Javier Snaider,Norman Casagrande,Paul Suganthan,Evan Palmer,Geoffrey Irving,Edward Loper,Manaal Faruqui,Isha Arkatkar,Nanxin Chen,Izhak Shafran,Michael Fink,Alfonso Castaño,Irene Giannoumis,Wooyeol Kim,Mikołaj Rybiński,Ashwin Sreevatsa,Jennifer Prendki,David Soergel,Adrian Goedeckemeyer,Willi Gierke,Mohsen Jafari,Meenu Gaba,Jeremy Wiesner,Diana Gage Wright,Yawen Wei,Harsha Vashisht,Yana Kulizhskaya,Jay Hoover,Maigo Le,Lu Li,Chimezie Iwuanyanwu,Lu Liu,Kevin Ramirez,Andrey Khorlin,Albert Cui,Tian LIN,Marin Georgiev,Marcus Wu,Ricardo Aguilar,Keith Pallo,Abhishek Chakladar,Alena Repina,Xihui Wu,Tom van der Weide,Priya Ponnapalli,Caroline Kaplan,Jiri Simsa,Shuangfeng Li,Olivier Dousse,Fan Yang,Jeff Piper,Nathan Ie,Minnie Lui,Rama Pasumarthi,Nathan Lintz,Anitha Vijayakumar,Lam Nguyen Thiet,Daniel Andor,Pedro Valenzuela,Cosmin Paduraru,Daiyi Peng,Katherine Lee,Shuyuan Zhang,Somer Greene,Duc Dung Nguyen,Paula Kurylowicz,Sarmishta Velury,Sebastian Krause,Cassidy Hardin,Lucas Dixon,Lili Janzer,Kiam Choo,Ziqiang Feng,Biao Zhang,Achintya Singhal,Tejasi Latkar,Mingyang Zhang,Quoc Le,Elena Allica Abellan,Dayou Du,Dan McKinnon,Natasha Antropova,Tolga Bolukbasi,Orgad Keller,David Reid,Daniel Finchelstein,Maria Abi Raad,Remi Crocker,Peter Hawkins,Robert Dadashi,Colin Gaffney,Sid Lall,Ken Franko,Egor Filonov,Anna Bulanova,Rémi Leblond,Vikas Yadav,Shirley Chung,Harry Askham,Luis C. Cobo,Kelvin Xu,Felix Fischer,Jun Xu,Christina Sorokin,Chris Alberti,Chu-Cheng Lin,Colin Evans,Hao Zhou,Alek Dimitriev,Hannah Forbes,Dylan Banarse,Zora Tung,Jeremiah Liu,Mark Omernick,Colton Bishop,Chintu Kumar,Rachel Sterneck,Ryan Foley,Rohan Jain,Swaroop Mishra,Jiawei Xia,Taylor Bos,Geoffrey Cideron,Ehsan Amid,Francesco Piccinno,Xingyu Wang,Praseem Banzal,Petru Gurita,Hila Noga,Premal Shah,Daniel J. Mankowitz,Alex Polozov,Nate Kushman,Victoria Krakovna,Sasha Brown,MohammadHossein Bateni,Dennis Duan,Vlad Firoiu,Meghana Thotakuri,Tom Natan,Anhad Mohananey,Matthieu Geist,Sidharth Mudgal,Sertan Girgin,Hui Li,Jiayu Ye,Ofir Roval,Reiko Tojo,Michael Kwong,James Lee-Thorp,Christopher Yew,Quan Yuan,Sumit Bagri,Danila Sinopalnikov,Sabela Ramos,John Mellor,Abhishek Sharma,Aliaksei Severyn,Jonathan Lai,Kathy Wu,Heng-Tze Cheng,David Miller,Nicolas Sonnerat,Denis Vnukov,Rory Greig,Jennifer Beattie,Emily Caveness,Libin Bai,Julian Eisenschlos,Alex Korchemniy,Tomy Tsai,Mimi Jasarevic,Weize Kong,Phuong Dao,Zeyu Zheng,Frederick Liu,Fan Yang,Rui Zhu,Mark Geller,Tian Huey Teh,Jason Sanmiya,Evgeny Gladchenko,Nejc Trdin,Andrei Sozanschi,Daniel Toyama,Evan Rosen,Sasan Tavakkol,Linting Xue,Chen Elkind,Oliver Woodman,John Carpenter,George Papamakarios,Rupert Kemp,Sushant Kafle,Tanya Grunina,Rishika Sinha,Alice Talbert,Abhimanyu Goyal,Diane Wu,Denese Owusu-Afriyie,Cosmo Du,Chloe Thornton,Jordi Pont-Tuset,Pradyumna Narayana,Jing Li,Sabaer Fatehi,John Wieting,Omar Ajmeri,Benigno Uria,Tao Zhu,Yeongil Ko,Laura Knight,Amélie Héliou,Ning Niu,Shane Gu,Chenxi Pang,Dustin Tran,Yeqing Li,Nir Levine,Ariel Stolovich,Norbert Kalb,Rebeca Santamaria-Fernandez,Sonam Goenka,Wenny Yustalim,Robin Strudel,Ali Elqursh,Balaji Lakshminarayanan,Charlie Deck,Shyam Upadhyay,Hyo Lee,Mike Dusenberry,Zonglin Li,Xuezhi Wang,Kyle Levin,Raphael Hoffmann,Dan Holtmann-Rice,Olivier Bachem,Summer Yue,Sho Arora,Eric Malmi,Daniil Mirylenka,Qijun Tan,Christy Koh,Soheil Hassas Yeganeh,Siim Põder,Steven Zheng,Francesco Pongetti,Mukarram Tariq,Yanhua Sun,Lucian Ionita,Mojtaba Seyedhosseini,Pouya Tafti,Ragha Kotikalapudi,Zhiyu Liu,Anmol Gulati,Jasmine Liu,Xinyu Ye,Bart Chrzaszcz,Lily Wang,Nikhil Sethi,Tianrun Li,Ben Brown,Shreya Singh,Wei Fan,Aaron Parisi,Joe Stanton,Chenkai Kuang,Vinod Koverkathu,Christopher A. Choquette-Choo,Yunjie Li,TJ Lu,Abe Ittycheriah,Prakash Shroff,Pei Sun,Mani Varadarajan,Sanaz Bahargam,Rob Willoughby,David Gaddy,Ishita Dasgupta,Guillaume Desjardins,Marco Cornero,Brona Robenek,Bhavishya Mittal,Ben Albrecht,Ashish Shenoy,Fedor Moiseev,Henrik Jacobsson,Alireza Ghaffarkhah,Morgane Rivière,Alanna Walton,Clément Crepy,Alicia Parrish,Yuan Liu,Zongwei Zhou,Clement Farabet,Carey Radebaugh,Praveen Srinivasan,Claudia van der Salm,Andreas Fidjeland,Salvatore Scellato,Eri Latorre-Chimoto,Hanna Klimczak-Plucińska,David Bridson,Dario de Cesare,Tom Hudson,Piermaria Mendolicchio,Lexi Walker,Alex Morris,Ivo Penchev,Matthew Mauger,Alexey Guseynov,Alison Reid,Seth Odoom,Lucia Loher,Victor Cotruta,Madhavi Yenugula,Dominik Grewe,Anastasia Petrushkina,Tom Duerig,Antonio Sanchez,Steve Yadlowsky,Amy Shen,Amir Globerson,Adam Kurzrok,Lynette Webb,Sahil Dua,Dong Li,Preethi Lahoti,Surya Bhupatiraju,Dan Hurt,Haroon Qureshi,Ananth Agarwal,Tomer Shani,Matan Eyal,Anuj Khare,Shreyas Rammohan Belle,Lei Wang,Chetan Tekur,Mihir Sanjay Kale,Jinliang Wei,Ruoxin Sang,Brennan Saeta,Tyler Liechty,Yi Sun,Yao Zhao,Stephan Lee,Pandu Nayak,Doug Fritz,Manish Reddy Vuyyuru,John Aslanides,Nidhi Vyas,Martin Wicke,Xiao Ma,Taylan Bilal,Evgenii Eltyshev,Daniel Balle,Nina Martin,Hardie Cate,James Manyika,Keyvan Amiri,Yelin Kim,Xi Xiong,Kai Kang,Florian Luisier,Nilesh Tripuraneni,David Madras,Mandy Guo,Austin Waters,Oliver Wang,Joshua Ainslie,Jason Baldridge,Han Zhang,Garima Pruthi,Jakob Bauer,Feng Yang,Riham Mansour,Jason Gelman,Yang Xu,George Polovets,Ji Liu,Honglong Cai,Warren Chen,XiangHai Sheng,Emily Xue,Sherjil Ozair,Adams Yu,Christof Angermueller,Xiaowei Li,Weiren Wang,Julia Wiesinger,Emmanouil Koukoumidis,Yuan Tian,Anand Iyer,Madhu Gurumurthy,Mark Goldenson,Parashar Shah,MK Blake,Hongkun Yu,Anthony Urbanowicz,Jennimaria Palomaki,Chrisantha Fernando,Kevin Brooks,Ken Durden,Harsh Mehta,Nikola Momchev,Elahe Rahimtoroghi,Maria Georgaki,Amit Raul,Sebastian Ruder,Morgan Redshaw,Jinhyuk Lee,Komal Jalan,Dinghua Li,Ginger Perng,Blake Hechtman,Parker Schuh,Milad Nasr,Mia Chen,Kieran Milan,Vladimir Mikulik,Trevor Strohman,Juliana Franco,Tim Green,Demis Hassabis,Koray Kavukcuoglu,Jeffrey Dean,Oriol Vinyals</p>
<p><a href='http://arxiv.org/abs/2312.11805v1'>http://arxiv.org/abs/2312.11805v1</a></p>
<p><b>Compressor summary</b>: The report presents Gemini, a new family of multimodal models with various sizes for different applications, that show impressive performance across image, audio, video, and text understanding tasks.</p><hr><h3>Designing Guiding Principles for NLP for Healthcare: A Case Study of  Maternal Health</h3>
<p>Maria Antoniak,Aakanksha Naik,Carla S. Alvarado,Lucy Lu Wang,Irene Y. Chen</p>
<p><a href='http://arxiv.org/abs/2312.11803v1'>http://arxiv.org/abs/2312.11803v1</a></p>
<p><b>Compressor summary</b>: The authors propose nine ethical principles for using large language models in healthcare applications, based on input from healthcare workers and birthing people.</p><hr><h3>MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA</h3>
<p>Lang Yu,Qin Chen,Jie Zhou,Liang He</p>
<p><a href='http://arxiv.org/abs/2312.11795v1'>http://arxiv.org/abs/2312.11795v1</a></p>
<p><b>Compressor summary</b>: MELO is a plug-in Model Editing method that dynamically adjusts LLM behavior using inner vector database indices for efficient and effective editing of various NLP tasks.</p><hr><h3>An effective image copy-move forgery detection using entropy image</h3>
<p>Zhaowei Lu,Li Jiang</p>
<p><a href='http://arxiv.org/abs/2312.11793v1'>http://arxiv.org/abs/2312.11793v1</a></p>
<p><b>Compressor summary</b>: The text describes an improved keypoint-based algorithm for detecting copy-move forgery in images, using entropy images to increase the number of keypoints and a clustering algorithm to handle non-ideal distribution of grayscale values.</p><hr><h3>COOPER: Coordinating Specialized Agents towards a Complex Dialogue Goal</h3>
<p>Yi Cheng,Wenge Liu,Jian Wang,Chak Tou Leong,Yi Ouyang,Wenjie Li,Xian Wu,Yefeng Zheng</p>
<p><a href='http://arxiv.org/abs/2312.11792v1'>http://arxiv.org/abs/2312.11792v1</a></p>
<p><b>Compressor summary</b>: The Cooper framework is a novel dialogue system that coordinates multiple specialized agents to achieve complex goals, such as negotiation and emotional support, by focusing on different aspects of these goals.</p><hr><h3>Faster Convergence with Multiway Preferences</h3>
<p>Aadirupa Saha,Vitaly Feldman,Tomer Koren,Yishay Mansour</p>
<p><a href='http://arxiv.org/abs/2312.11788v1'>http://arxiv.org/abs/2312.11788v1</a></p>
<p><b>Compressor summary</b>: The paper studies convex optimization with preference feedback, where it designs efficient algorithms with improved convergence rates for batched and multiway comparisons, and provides lower bounds to show their optimality.</p><hr><h3>Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs</h3>
<p>Zhangdie Yuan,Andreas Vlachos</p>
<p><a href='http://arxiv.org/abs/2312.11785v1'>http://arxiv.org/abs/2312.11785v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a zero-shot method for fact-checking that uses semantic triples and external knowledge graphs to generalize to unseen data and domains.</p><hr><h3>Learning Object State Changes in Videos: An Open-World Perspective</h3>
<p>Zihui Xue,Kumar Ashutosh,Kristen Grauman</p>
<p><a href='http://arxiv.org/abs/2312.11782v1'>http://arxiv.org/abs/2312.11782v1</a></p>
<p><b>Compressor summary</b>: The authors propose VidOSC, a method that uses text and vision-language models to learn how objects change over time without manual annotations, and introduce an open-world benchmark for video object state changes called HowToChange.</p><hr><h3>Are you talking to ['xem'] or ['x', 'em']? On Tokenization and  Addressing Misgendering in LLMs with Pronoun Tokenization Parity</h3>
<p>Anaelia Ovalle,Ninareh Mehrabi,Palash Goyal,Jwala Dhamala,Kai-Wei Chang,Richard Zemel,Aram Galstyan,Yuval Pinter,Rahul Gupta</p>
<p><a href='http://arxiv.org/abs/2312.11779v1'>http://arxiv.org/abs/2312.11779v1</a></p>
<p><b>Compressor summary</b>: This paper investigates how data scarcity affects large language models' misgendering of non-binary people and proposes a new tokenization method, PTP, to improve neopronoun consistency.</p><hr><h3>Text-Image Conditioned Diffusion for Consistent Text-to-3D Generation</h3>
<p>Yuze He,Yushi Bai,Matthieu Lin,Jenny Sheng,Yubin Hu,Qi Wang,Yu-Hui Wen,Yong-Jin Liu</p>
<p><a href='http://arxiv.org/abs/2312.11774v1'>http://arxiv.org/abs/2312.11774v1</a></p>
<p><b>Compressor summary</b>: The authors propose a text-to-3D method that improves fine-grained view consistency and reduces floaters and empty spaces by incorporating multi-view image conditions into NeRF optimization.</p><hr><h3>CAManim: Animating end-to-end network activation maps</h3>
<p>Emily Kaczmarek,Olivier X. Miguel,Alexa C. Bowie,Robin Ducharme,Alysha L. J. Dingwall-Harvey,Steven Hawken,Christine M. Armour,Mark C. Walker,Kevin Dick</p>
<p><a href='http://arxiv.org/abs/2312.11772v1'>http://arxiv.org/abs/2312.11772v1</a></p>
<p><b>Compressor summary</b>: CAManim is a novel XAI visualization method that animates CAM-based network activation maps through all layers to improve understanding of CNN predictions.</p><hr><h3>Bridging the Gap: Generalising State-of-the-Art U-Net Models to  Sub-Saharan African Populations</h3>
<p>Alyssa R. Amod,Alexandra Smith,Pearly Joubert,Confidence Raymond,Dong Zhang,Udunna C. Anazodo,Dodzi Motchon,Tinashe E. M. Mutsvangwa,Sébastien Quetin</p>
<p><a href='http://arxiv.org/abs/2312.11770v1'>http://arxiv.org/abs/2312.11770v1</a></p>
<p><b>Compressor summary</b>: The authors investigated how different training approaches affect tumor segmentation models' performance on poor-quality neuroimaging data from Sub-Saharan Africa and found that fine-tuning a model pretrained on high-quality data improved its results.</p><hr><h3>Clustering Mixtures of Bounded Covariance Distributions Under Optimal  Separation</h3>
<p>Ilias Diakonikolas,Daniel M. Kane,Jasper C. H. Lee,Thanasis Pittas</p>
<p><a href='http://arxiv.org/abs/2312.11769v1'>http://arxiv.org/abs/2312.11769v1</a></p>
<p><b>Compressor summary</b>: The paper proposes efficient algorithms for clustering mixtures of bounded covariance distributions with a fine-grained separation assumption and shows their applicability to various settings, including high-dimensional log-concave distributions and robust clustering.</p><hr><h3>Curriculum Learning for Cooperation in Multi-Agent Reinforcement  Learning</h3>
<p>Rupali Bhati,Sai Krishna Gottipati,Clodéric Mars,Matthew E. Taylor</p>
<p><a href='http://arxiv.org/abs/2312.11768v1'>http://arxiv.org/abs/2312.11768v1</a></p>
<p><b>Compressor summary</b>: The paper investigates the best type and curriculum of cooperative teammates to train a learning agent in multi-agent reinforcement learning for achieving task performance and overall team reward, finding that less skilled but pre-trained teammates and skill-decreasing curricula perform better.</p><hr><h3>ADMM-MM Algorithm for General Tensor Decomposition</h3>
<p>Manabu Mukai,Hidekata Hontani,Tatsuya Yokota</p>
<p><a href='http://arxiv.org/abs/2312.11763v1'>http://arxiv.org/abs/2312.11763v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new optimization algorithm for tensor decomposition that supports different loss functions and models, and can be applied to various applications.</p><hr><h3>MineObserver 2.0: A Deep Learning & In-Game Framework for Assessing  Natural Language Descriptions of Minecraft Imagery</h3>
<p>Jay Mahajan,Samuel Hum,Jack Henhapl,Diya Yunus,Matthew Gadbury,Emi Brown,Jeff Ginger,H. Chad Lane</p>
<p><a href='http://arxiv.org/abs/2312.11761v1'>http://arxiv.org/abs/2312.11761v1</a></p>
<p><b>Compressor summary</b>: MineObserver 2.0 is an improved AI framework that helps assess the accuracy of learner-generated descriptions of Minecraft images related to science, using computer vision and natural language processing, and provides feedback to teachers.</p>