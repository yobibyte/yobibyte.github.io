
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-08-20</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-08-20 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>KAN 2.0: Kolmogorov-Arnold Networks Meet Science</h3>
<p><a href='http://arxiv.org/abs/2408.10205v1'>http://arxiv.org/abs/2408.10205v1</a></p>
<p><b>Compressor summary</b>: The text proposes a framework that combines connectionist AI (Kolmogorov-Arnold Networks) with science for discovering features, structures, and formulas in physical laws.</p><hr><h3>Criticality Leveraged Adversarial Training (CLAT) for Boosted  Performance via Parameter Efficiency</h3>
<p><a href='http://arxiv.org/abs/2408.10204v1'>http://arxiv.org/abs/2408.10204v1</a></p>
<p><b>Compressor summary</b>: CLAT is an approach that improves both clean accuracy and adversarial robustness of neural networks by fine-tuning only critical layers, reducing parameters, and adapting to changes in layer criticality.</p><hr><h3>SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP</h3>
<p><a href='http://arxiv.org/abs/2408.10202v1'>http://arxiv.org/abs/2408.10202v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method called SANER to reduce societal bias in CLIP without losing attribute information or using attribute annotations during debiasing.</p><hr><h3>MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction  Model</h3>
<p><a href='http://arxiv.org/abs/2408.10198v1'>http://arxiv.org/abs/2408.10198v1</a></p>
<p><b>Compressor summary</b>: MeshFormer is a sparse-view 3D reconstruction model that uses transformers, 3D convolutions, input normal maps, and SDF supervision to efficiently train and generate high-quality textured meshes with geometric details.</p><hr><h3>Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic  Models</h3>
<p><a href='http://arxiv.org/abs/2408.10189v1'>http://arxiv.org/abs/2408.10189v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes MOHAWK, a method to distill pretrained Transformers into alternative architectures like SSMs.
- MOHAWK matches different degrees of granularity in the mixing matrices and hidden units of Transformers and SSMs.
- Phi-Mamba, a Mamba-2 variant, achieves strong performance with less than 1% of the training data typically used for non-Transformer models.

Summary:
MOHAWK learns to convert pretrained Transformers into state space models by matching their mixing matrices and hidden units, enabling Phi-Mamba to outperform past non-Transformer models with much less data.</p><hr><h3>LongVILA: Scaling Long-Context Visual Language Models for Long Videos</h3>
<p><a href='http://arxiv.org/abs/2408.10188v1'>http://arxiv.org/abs/2408.10188v1</a></p>
<p><b>Compressor summary</b>: LongVILA is a system that enables efficient long-context training and inference for vision-language models, improving performance on tasks like long video captioning.</p><hr><h3>Assessment of Spectral based Solutions for the Detection of Floating  Marine Debris</h3>
<p><a href='http://arxiv.org/abs/2408.10187v1'>http://arxiv.org/abs/2408.10187v1</a></p>
<p><b>Compressor summary</b>: The text discusses using remote sensing data and Machine Learning algorithms to detect floating plastic debris in the ocean, and introduces the Marine Debris Archive as a standard dataset for evaluating these methods.</p><hr><h3>NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface  Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2408.10178v1'>http://arxiv.org/abs/2408.10178v1</a></p>
<p><b>Compressor summary</b>: NeuRodin is a new neural framework that improves surface reconstruction in volume rendering by addressing challenges in SDF-based methods and retaining flexibility of density-based methods.</p><hr><h3>Fairness Under Cover: Evaluating the Impact of Occlusions on Demographic  Bias in Facial Recognition</h3>
<p><a href='http://arxiv.org/abs/2408.10175v1'>http://arxiv.org/abs/2408.10175v1</a></p>
<p><b>Compressor summary</b>: The study shows that occlusions worsen face recognition system fairness by increasing bias against some demographic groups, especially Africans.</p><hr><h3>SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From  Pre-Trained Foundation Models</h3>
<p><a href='http://arxiv.org/abs/2408.10174v1'>http://arxiv.org/abs/2408.10174v1</a></p>
<p><b>Compressor summary</b>: The study proposes a novel zero-shot model fusion method called SMILE that addresses parameter interference issues and improves performance without extra training data or parameters.</p><hr><h3>NeuFlow v2: High-Efficiency Optical Flow Estimation on Edge Devices</h3>
<p><a href='http://arxiv.org/abs/2408.10161v1'>http://arxiv.org/abs/2408.10161v1</a></p>
<p><b>Compressor summary</b>: The paper presents NeuFlow v2, an efficient optical flow method that balances high accuracy with reduced computational costs, achieving 10x-70x speedup and running at over 20 FPS on a Jetson Orin Nano.</p><hr><h3>LoopSplat: Loop Closure by Registering 3D Gaussian Splats</h3>
<p><a href='http://arxiv.org/abs/2408.10154v1'>http://arxiv.org/abs/2408.10154v1</a></p>
<p><b>Compressor summary</b>: LoopSplat improves 3D scene mapping accuracy by triggering loop closure online and aligning submaps via 3D Gaussian Splats registration.</p><hr><h3>Structure-preserving Image Translation for Depth Estimation in  Colonoscopy Video</h3>
<p><a href='http://arxiv.org/abs/2408.10153v1'>http://arxiv.org/abs/2408.10153v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to translate unrealistic synthetic data to realistic clinical data for training monocular depth estimation in colonoscopy videos, improving its generalization to the clinical domain.</p><hr><h3>Multilingual Needle in a Haystack: Investigating Long-Context Behavior  of Multilingual Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.10151v1'>http://arxiv.org/abs/2408.10151v1</a></p>
<p><b>Compressor summary</b>: The MLNeedle test evaluates LLMs' ability to retrieve relevant information from multilingual contexts and reveals their limitations in handling long contexts across languages.</p><hr><h3>In-Context Learning with Representations: Contextual Generalization of  Trained Transformers</h3>
<p><a href='http://arxiv.org/abs/2408.10147v1'>http://arxiv.org/abs/2408.10147v1</a></p>
<p><b>Compressor summary</b>: This paper studies how transformer models can learn new tasks from few examples during inference, using non-linear regression to show they can acquire contextual knowledge for generalization.</p><hr><h3>Multi-Scale Representation Learning for Image Restoration with  State-Space Model</h3>
<p><a href='http://arxiv.org/abs/2408.10145v1'>http://arxiv.org/abs/2408.10145v1</a></p>
<p><b>Compressor summary</b>: MS-Mamba is a novel image restoration method that uses a multi-scale state-space model and adaptive gradient block to enhance detail and contrast, achieving state-of-the-art results with low computational complexity.</p><hr><h3>Instruction Finetuning for Leaderboard Generation from Empirical AI  Research</h3>
<p><a href='http://arxiv.org/abs/2408.10141v1'>http://arxiv.org/abs/2408.10141v1</a></p>
<p><b>Compressor summary</b>: The study shows how to use large language models to automatically generate leaderboard data from AI research articles, improving the speed and accuracy of information extraction.</p><hr><h3>$R^2$-Mesh: Reinforcement Learning Powered Mesh Reconstruction via  Geometry and Appearance Refinement</h3>
<p><a href='http://arxiv.org/abs/2408.10135v1'>http://arxiv.org/abs/2408.10135v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new algorithm that improves NeRF's ability to generate and optimize meshes from multi-view images by refining SDF and appearance representation, and adaptively incorporating additional images for training.</p><hr><h3>Perceptual Depth Quality Assessment of Stereoscopic Omnidirectional  Images</h3>
<p><a href='http://arxiv.org/abs/2408.10134v1'>http://arxiv.org/abs/2408.10134v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new model, DQI, to measure depth quality in stereoscopic omnidirectional images, which improves upon existing methods and considers human visual system characteristics.</p><hr><h3>Rhyme-aware Chinese lyric generator based on GPT</h3>
<p><a href='http://arxiv.org/abs/2408.10130v1'>http://arxiv.org/abs/2408.10130v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve lyric generation by integrating rhyme information into a pre-trained language model.</p><hr><h3>Learning Brave Assumption-Based Argumentation Frameworks via ASP</h3>
<p><a href='http://arxiv.org/abs/2408.10126v1'>http://arxiv.org/abs/2408.10126v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm for learning assumption-based argumentation from background knowledge and examples using transformation rules and answer set programming.</p><hr><h3>Molecular Graph Representation Learning Integrating Large Language  Models with Domain-specific Small Models</h3>
<p><a href='http://arxiv.org/abs/2408.10124v1'>http://arxiv.org/abs/2408.10124v1</a></p>
<p><b>Compressor summary</b>: MolGraph-LarDo integrates Large language models and Domain-specific small models for accurate molecular property prediction by using a two-stage prompt strategy and multi-modal alignment.</p><hr><h3>Geometry Informed Tokenization of Molecules for Language Model  Generation</h3>
<p><a href='http://arxiv.org/abs/2408.10120v1'>http://arxiv.org/abs/2408.10120v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Geo2Seq, a method to convert 3D molecular geometries into discrete sequences, enabling better generation of molecules using language models.</p><hr><h3>Factorized-Dreamer: Training A High-Quality Video Generator with Limited  and Low-Quality Data</h3>
<p><a href='http://arxiv.org/abs/2408.10119v1'>http://arxiv.org/abs/2408.10119v1</a></p>
<p><b>Compressor summary</b>: Key points:
- T2V generation is challenging due to real world complex motions
- The paper proposes Factorized-Dreamer, a framework that uses limited and low-quality data to generate HQ videos
- Factorized-Dreamer has several designs, such as adapter, cross attention, T5 encoder, and PredictNet
- Noise schedule is used to ensure quality and stability of video generation
- Experiments show the effectiveness of Factorized-Dreamer on various tasks

Summary:
The paper presents Factorized-Dreamer, a framework that can generate high-quality videos from limited and low-quality data using text and image embeddings, cross attention, T5 encoder, PredictNet, and noise schedule.</p><hr><h3>GLIMMER: Incorporating Graph and Lexical Features in Unsupervised  Multi-Document Summarization</h3>
<p><a href='http://arxiv.org/abs/2408.10115v1'>http://arxiv.org/abs/2408.10115v1</a></p>
<p><b>Compressor summary</b>: GLIMMER is an unsupervised multi-document summarization approach that uses sentence graphs and semantic clusters to generate fluent and informative summaries, outperforming existing methods and pre-trained models.</p><hr><h3>Enhancing Reinforcement Learning Through Guided Search</h3>
<p><a href='http://arxiv.org/abs/2408.10113v1'>http://arxiv.org/abs/2408.10113v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using Monte Carlo Tree Search (MCTS) as a guide for Reinforcement Learning (RL) agents to improve performance, especially in Off-Policy settings, and shows significant results on Atari 100k benchmark.</p><hr><h3>PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil  Financial Time Series Regularities</h3>
<p><a href='http://arxiv.org/abs/2408.10111v1'>http://arxiv.org/abs/2408.10111v1</a></p>
<p><b>Compressor summary</b>: PLUTUS is a large-scale, open-source, transformer-based model that uses contrastive learning and attention mechanisms to capture complex patterns in financial time series data.</p><hr><h3>Perturb-and-Compare Approach for Detecting Out-of-Distribution Samples  in Constrained Access Environments</h3>
<p><a href='http://arxiv.org/abs/2408.10107v1'>http://arxiv.org/abs/2408.10107v1</a></p>
<p><b>Compressor summary</b>: MixDiff is a framework for detecting out-of-distribution inputs for machine learning models without accessing their parameters or activations, by applying input-level perturbations and comparing the model outputs of two similar samples.</p><hr><h3>ARMADA: Attribute-Based Multimodal Data Augmentation</h3>
<p><a href='http://arxiv.org/abs/2408.10086v1'>http://arxiv.org/abs/2408.10086v1</a></p>
<p><b>Compressor summary</b>: ARMADA is a novel multimodal data augmentation method that generates semantically consistent image-text pairs by manipulating visual attributes of entities using knowledge bases and large language models.</p><hr><h3>MASALA: Model-Agnostic Surrogate Explanations by Locality Adaptation</h3>
<p><a href='http://arxiv.org/abs/2408.10085v1'>http://arxiv.org/abs/2408.10085v1</a></p>
<p><b>Compressor summary</b>: MASALA is a new XAI method that automatically determines the appropriate local region for explaining each instance, unlike existing methods that require a user-defined locality size.</p><hr><h3>TANGO: Clustering with Typicality-Aware Nonlocal Mode-Seeking and  Graph-Cut Optimization</h3>
<p><a href='http://arxiv.org/abs/2408.10084v1'>http://arxiv.org/abs/2408.10084v1</a></p>
<p><b>Compressor summary</b>: TANGO is a density-based clustering algorithm that uses global typicality to improve local dependencies, achieving better peak selection and sub-cluster characterization than mode-seeking methods.</p><hr><h3>Personalizing Reinforcement Learning from Human Feedback with  Variational Preference Learning</h3>
<p><a href='http://arxiv.org/abs/2408.10075v1'>http://arxiv.org/abs/2408.10075v1</a></p>
<p><b>Compressor summary</b>: Multimodal RLHF methods use a latent variable formulation to infer user-specific preferences and learn reward models and policies tailored to each individual, improving alignment with diverse populations.</p><hr><h3>Modelling the Distribution of Human Motion for Sign Language Assessment</h3>
<p><a href='http://arxiv.org/abs/2408.10073v1'>http://arxiv.org/abs/2408.10073v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new Sign Language Assessment tool that models natural human motion and provides useful feedback for language learners.</p><hr><h3>FFAA: Multimodal Large Language Model based Explainable Open-World Face  Forgery Analysis Assistant</h3>
<p><a href='http://arxiv.org/abs/2408.10072v1'>http://arxiv.org/abs/2408.10072v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new face forgery analysis task with descriptions and reasoning, and proposes an assistive system based on a multimodal language model and a decision system that provides user-friendly and explainable results.</p><hr><h3>Efficient Exploration in Deep Reinforcement Learning: A Novel Bayesian  Actor-Critic Algorithm</h3>
<p><a href='http://arxiv.org/abs/2408.10055v1'>http://arxiv.org/abs/2408.10055v1</a></p>
<p><b>Compressor summary</b>: The text discusses the theoretical foundations of deep reinforcement learning, focusing on exploration methods, and proposing a novel Bayesian actor-critic algorithm with empirical evaluation on benchmarks.</p><hr><h3>Privacy Checklist: Privacy Violation Detection Grounding on Contextual  Integrity Theory</h3>
<p><a href='http://arxiv.org/abs/2408.10053v1'>http://arxiv.org/abs/2408.10053v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a comprehensive checklist for contextual integrity-based privacy research that covers social identities, private attributes, and existing regulations using large language models.</p><hr><h3>Exploiting Fine-Grained Prototype Distribution for Boosting Unsupervised  Class Incremental Learning</h3>
<p><a href='http://arxiv.org/abs/2408.10046v1'>http://arxiv.org/abs/2408.10046v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for unsupervised class incremental learning that uses fine-grained prototypes, granularity alignment, and strategy to minimize overlap between classes to discover unknown novel classes and preserve historical knowledge.</p><hr><h3>Implicit Gaussian Splatting with Efficient Multi-Level Tri-Plane  Representation</h3>
<p><a href='http://arxiv.org/abs/2408.10041v1'>http://arxiv.org/abs/2408.10041v1</a></p>
<p><b>Compressor summary</b>: Implicit Gaussian Splatting (IGS) is an efficient and compact model for photo-realistic novel view synthesis that integrates explicit point clouds with implicit feature embeddings using a multi-level tri-plane architecture and progressive training scheme, achieving high rendering quality while consuming only a few MBs.</p><hr><h3>The Practimum-Optimum Algorithm for Manufacturing Scheduling: A Paradigm  Shift Leading to Breakthroughs in Scale and Performance</h3>
<p><a href='http://arxiv.org/abs/2408.10040v1'>http://arxiv.org/abs/2408.10040v1</a></p>
<p><b>Compressor summary</b>: The P-O algorithm creates virtual human expert agents to generate many valid schedules and uses reinforced machine learning to improve them, achieving breakthrough performance in automatic manufacturing scheduling.</p><hr><h3>MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis</h3>
<p><a href='http://arxiv.org/abs/2408.10039v1'>http://arxiv.org/abs/2408.10039v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a multi-step clinical diagnostic dataset (MSDiagnosis) and a framework that combines forward and backward inference with reflection and refinement to improve the performance of language models in complex medical diagnosis tasks.</p><hr><h3>Deterministic Policy Gradient Primal-Dual Methods for Continuous-Space  Constrained MDPs</h3>
<p><a href='http://arxiv.org/abs/2408.10015v1'>http://arxiv.org/abs/2408.10015v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new method for finding optimal deterministic policies for continuous-state and -action constrained MDPs using a primal-dual policy gradient approach, which is applied to robot navigation and fluid control problems.</p><hr><h3>CLIPCleaner: Cleaning Noisy Labels with CLIP</h3>
<p><a href='http://arxiv.org/abs/2408.10012v1'>http://arxiv.org/abs/2408.10012v1</a></p>
<p><b>Compressor summary</b>: The paper proposes CLIPCleaner, a method that uses a vision-language model to select clean samples from noisy labels, outperforming existing methods on benchmark datasets.</p><hr><h3>PinnDE: Physics-Informed Neural Networks for Solving Differential  Equations</h3>
<p><a href='http://arxiv.org/abs/2408.10011v1'>http://arxiv.org/abs/2408.10011v1</a></p>
<p><b>Compressor summary</b>: PinnDE is an open-source python library for solving differential equations using physics-informed neural networks (PINNs) and deep operator networks (DeepONets).</p><hr><h3>P3P: Pseudo-3D Pre-training for Scaling 3D Masked Autoencoders</h3>
<p><a href='http://arxiv.org/abs/2408.10007v1'>http://arxiv.org/abs/2408.10007v1</a></p>
<p><b>Compressor summary</b>: Key points:
- propose self-supervised pre-training framework using real 3D data and pseudo-3D data from images
- use efficient token embedding and 2D reconstruction target to overcome data scaling and efficiency challenges
- achieve state-of-the-art performance in 3D perception tasks

Summary:
The paper presents a self-supervised pre-training method for 3D perception that leverages real and pseudo-3D data, and improves efficiency with novel token embedding and reconstruction target, leading to superior results.</p><hr><h3>Unlocking the Power of LSTM for Long Term Time Series Forecasting</h3>
<p><a href='http://arxiv.org/abs/2408.10006v1'>http://arxiv.org/abs/2408.10006v1</a></p>
<p><b>Compressor summary</b>: P-sLSTM is a modified sLSTM algorithm that improves time series forecasting by incorporating patching and channel independence, achieving state-of-the-art results with theoretical justifications.</p><hr><h3>Towards a Knowledge Graph for Models and Algorithms in Applied  Mathematics</h3>
<p><a href='http://arxiv.org/abs/2408.10003v1'>http://arxiv.org/abs/2408.10003v1</a></p>
<p><b>Compressor summary</b>: The text describes how a living knowledge graph was created by merging and extending ontologies to represent mathematical models and algorithms semantically and enrich them with metadata, including subject-specific properties.</p><hr><h3>The Fairness-Quality Trade-off in Clustering</h3>
<p><a href='http://arxiv.org/abs/2408.10002v1'>http://arxiv.org/abs/2408.10002v1</a></p>
<p><b>Compressor summary</b>: The paper introduces algorithms to find trade-offs between quality and fairness in clustering problems by exploring the complete Pareto front.</p><hr><h3>Uniting contrastive and generative learning for event sequences models</h3>
<p><a href='http://arxiv.org/abs/2408.09995v1'>http://arxiv.org/abs/2408.09995v1</a></p>
<p><b>Compressor summary</b>: The study combines two self-supervised methods to create balanced representations of transactional data, improving performance in tasks like sequence classification and next-event prediction in banking applications.</p><hr><h3>Boosting Open-Domain Continual Learning via Leveraging Intra-domain  Category-aware Prototype</h3>
<p><a href='http://arxiv.org/abs/2408.09984v1'>http://arxiv.org/abs/2408.09984v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve open-domain continual learning in vision-language models by using category-aware prototypes as Task-ID discriminators and domain prior prompts.</p><hr><h3>Preference-Optimized Pareto Set Learning for Blackbox Optimization</h3>
<p><a href='http://arxiv.org/abs/2408.09976v1'>http://arxiv.org/abs/2408.09976v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an efficient method to approximate the whole Pareto set in multi-objective optimization problems using bilevel optimization and differentiable cross-entropy methods, which improves upon previous naive approaches.</p><hr><h3>The Exploration-Exploitation Dilemma Revisited: An Entropy Perspective</h3>
<p><a href='http://arxiv.org/abs/2408.09974v1'>http://arxiv.org/abs/2408.09974v1</a></p>
<p><b>Compressor summary</b>: AdaZero is an end-to-end adaptive framework for reinforcement learning that balances exploration and exploitation based on entropy, achieving significant improvements in various environments.</p><hr><h3>Unsupervised Machine Learning Hybrid Approach Integrating Linear  Programming in Loss Function: A Robust Optimization Technique</h3>
<p><a href='http://arxiv.org/abs/2408.09967v1'>http://arxiv.org/abs/2408.09967v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a new method that combines linear programming and an unsupervised machine learning model to solve complex optimization problems with constraints while preserving interpretability and adapting to different scenarios.</p><hr><h3>Mask in the Mirror: Implicit Sparsification</h3>
<p><a href='http://arxiv.org/abs/2408.09966v1'>http://arxiv.org/abs/2408.09966v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to control the sparsity of deep neural networks using continuous sparsification and shows improved performance in high-sparsity scenarios.</p><hr><h3>AdaResNet: Enhancing Residual Networks with Dynamic Weight Adjustment  for Improved Feature Integration</h3>
<p><a href='http://arxiv.org/abs/2408.09958v1'>http://arxiv.org/abs/2408.09958v1</a></p>
<p><b>Compressor summary</b>: AdaResNet automatically adjusts the ratio between input pixels and transformed data in skip connections, improving the performance of deep neural networks.</p><hr><h3>Contextual Importance and Utility in Python: New Functionality and  Insights with the py-ciu Package</h3>
<p><a href='http://arxiv.org/abs/2408.09957v1'>http://arxiv.org/abs/2408.09957v1</a></p>
<p><b>Compressor summary</b>: The paper presents py-ciu, a Python tool for generating explanations from machine learning models using the CIU method, which offers novel features compared to existing methods.</p><hr><h3>Principle Driven Parameterized Fiber Model based on GPT-PINN Neural  Network</h3>
<p><a href='http://arxiv.org/abs/2408.09951v1'>http://arxiv.org/abs/2408.09951v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new AI-based fiber model for Beyond 5G communications that reduces re-training time and increases efficiency by using linear combinations of pre-trained models.</p><hr><h3>C${^2}$RL: Content and Context Representation Learning for Gloss-free  Sign Language Translation and Retrieval</h3>
<p><a href='http://arxiv.org/abs/2408.09949v1'>http://arxiv.org/abs/2408.09949v1</a></p>
<p><b>Compressor summary</b>: C${^2}$RL is a novel pretraining paradigm for gloss-free Sign Language Representation Learning that emphasizes Implicit Content Learning and Explicit Context Learning to improve performance in tasks like Sign Language Translation and Sign Language Retrieval.</p><hr><h3>Caption-Driven Explorations: Aligning Image and Text Embeddings through  Human-Inspired Foveated Vision</h3>
<p><a href='http://arxiv.org/abs/2408.09948v1'>http://arxiv.org/abs/2408.09948v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a dataset and method to study and predict human attention during image captioning tasks using CLIP models and NeVA algorithms, improving existing models.</p><hr><h3>Fiber Transmission Model with Parameterized Inputs based on GPT-PINN  Neural Network</h3>
<p><a href='http://arxiv.org/abs/2408.09947v1'>http://arxiv.org/abs/2408.09947v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new fiber transmission model that can handle different bit rates without retraining and uses universal solutions based on the novelty principle.</p><hr><h3>Microscopic Analysis on LLM players via Social Deduction Game</h3>
<p><a href='http://arxiv.org/abs/2408.09946v1'>http://arxiv.org/abs/2408.09946v1</a></p>
<p><b>Compressor summary</b>: The text describes an approach to evaluate autonomous game players for social deduction games using a variant of SpyFall called SpyGame, introducing new metrics and qualitative analysis methods to assess their skills in intent identification and camouflage.</p><hr><h3>Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating  Adequacy, Fluency, and Elegance</h3>
<p><a href='http://arxiv.org/abs/2408.09945v1'>http://arxiv.org/abs/2408.09945v1</a></p>
<p><b>Compressor summary</b>: The authors introduce a benchmark for translating classical Chinese poetry into English and propose RAT, a retrieval-augmented machine translation method that improves translation quality using knowledge about classical poetry and an automatic evaluation metric based on GPT-4.</p><hr><h3>ML-CrAIST: Multi-scale Low-high Frequency Information-based Cross black  Attention with Image Super-resolving Transformer</h3>
<p><a href='http://arxiv.org/abs/2408.09940v1'>http://arxiv.org/abs/2408.09940v1</a></p>
<p><b>Compressor summary</b>: The ML-CrAIST model uses spatial and channel self-attention and a cross-attention block to effectively utilize multi-scale image details and improve single-image super-resolution performance, outperforming state-of-the-art methods.</p><hr><h3>"Image, Tell me your story!" Predicting the original meta-context of  visual misinformation</h3>
<p><a href='http://arxiv.org/abs/2408.09939v1'>http://arxiv.org/abs/2408.09939v1</a></p>
<p><b>Compressor summary</b>: The researchers propose a new automated method to provide context for images, which can help detect misinformation and support fact-checking efforts.</p><hr><h3>Data Augmentation of Contrastive Learning is Estimating  Positive-incentive Noise</h3>
<p><a href='http://arxiv.org/abs/2408.09929v1'>http://arxiv.org/abs/2408.09929v1</a></p>
<p><b>Compressor summary</b>: The paper explores how to learn beneficial noise for contrastive learning using Positive-incentive Noise (Pi-Noise) and proposes a framework to generate such noise as data augmentations.</p><hr><h3>Sliced Maximal Information Coefficient: A Training-Free Approach for  Image Quality Assessment Enhancement</h3>
<p><a href='http://arxiv.org/abs/2408.09920v1'>http://arxiv.org/abs/2408.09920v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a human visual attention estimation strategy to improve existing image quality assessment models by measuring the statistical dependency between degraded and reference images.</p><hr><h3>Expressive Power of Temporal Message Passing</h3>
<p><a href='http://arxiv.org/abs/2408.09918v1'>http://arxiv.org/abs/2408.09918v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes how two types of temporal message passing mechanisms in graph neural networks differ in their expressive power and performance on color-persistent temporal graphs.</p><hr><h3>Attribution Analysis Meets Model Editing: Advancing Knowledge Correction  in Vision Language Models with VisEdit</h3>
<p><a href='http://arxiv.org/abs/2408.09916v1'>http://arxiv.org/abs/2408.09916v1</a></p>
<p><b>Compressor summary</b>: VisEdit is a novel model editor for vision-language models that edits intermediate visual representations in relevant regions to correct knowledge, based on attribution analysis showing the importance of these representations for token predictions.</p><hr><h3>Active Learning for Identifying Disaster-Related Tweets: A Comparison  with Keyword Filtering and Generic Fine-Tuning</h3>
<p><a href='http://arxiv.org/abs/2408.09914v1'>http://arxiv.org/abs/2408.09914v1</a></p>
<p><b>Compressor summary</b>: The study investigates the potential of Active Learning for identifying disaster-related posts on social media, finding that it outperforms other methods with minimal labelling effort.</p><hr><h3>$p$SVM: Soft-margin SVMs with $p$-norm Hinge Loss</h3>
<p><a href='http://arxiv.org/abs/2408.09908v1'>http://arxiv.org/abs/2408.09908v1</a></p>
<p><b>Compressor summary</b>: The paper explores properties and performance of soft-margin SVMs with $p$-norm hinge loss, called $p$SVMs, and proposes a generalized version of the SMO algorithm to train them.</p><hr><h3>LCE: A Framework for Explainability of DNNs for Ultrasound Image Based  on Concept Discovery</h3>
<p><a href='http://arxiv.org/abs/2408.09899v1'>http://arxiv.org/abs/2408.09899v1</a></p>
<p><b>Compressor summary</b>: The Lesion Concept Explainer (LCE) framework combines attribution and concept-based methods to explain the decisions of Deep Neural Networks for medical images, especially ultrasound images, using a fine-tuned Segment Anything Model (SAM).</p><hr><h3>Instruction-Based Molecular Graph Generation with Unified Text-Graph  Diffusion Model</h3>
<p><a href='http://arxiv.org/abs/2408.09896v1'>http://arxiv.org/abs/2408.09896v1</a></p>
<p><b>Compressor summary</b>: The UTGDiff model generates molecules from textual instructions using a unified text-graph transformer derived from language models, achieving better performance than sequence-based methods with fewer parameters.</p><hr><h3>Performance Law of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.09895v1'>http://arxiv.org/abs/2408.09895v1</a></p>
<p><b>Compressor summary</b>: The authors propose a "Performance Law" equation that predicts the MMLU score of large language models based on their architecture and training data size, helping in selecting architectures and allocating computational resources efficiently.</p><hr><h3>Differential Private Stochastic Optimization with Heavy-tailed Data:  Towards Optimal Rates</h3>
<p><a href='http://arxiv.org/abs/2408.09891v1'>http://arxiv.org/abs/2408.09891v1</a></p>
<p><b>Compressor summary</b>: The paper explores optimal rates for differential privacy optimization with heavy-tailed gradients using a simple clipping method and an iterative updating method, improving on existing methods and matching the minimax lower bound.</p><hr><h3>GINO-Q: Learning an Asymptotically Optimal Index Policy for Restless  Multi-armed Bandits</h3>
<p><a href='http://arxiv.org/abs/2408.09882v1'>http://arxiv.org/abs/2408.09882v1</a></p>
<p><b>Compressor summary</b>: GINO-Q is a new algorithm that efficiently learns optimal policies for restless multi-armed bandits without requiring indexability, outperforming existing methods.</p><hr><h3>Uncertainty Quantification of Pre-Trained and Fine-Tuned Surrogate  Models using Conformal Prediction</h3>
<p><a href='http://arxiv.org/abs/2408.09881v1'>http://arxiv.org/abs/2408.09881v1</a></p>
<p><b>Compressor summary</b>: The text proposes a method to estimate reliable uncertainty for spatio-temporal surrogate models using conformal prediction with minimal computational cost and broad applicability.</p><hr><h3>New spectral imaging biomarkers for sepsis and mortality in intensive  care</h3>
<p><a href='http://arxiv.org/abs/2408.09873v1'>http://arxiv.org/abs/2408.09873v1</a></p>
<p><b>Compressor summary</b>: Hyperspectral imaging can predict sepsis and mortality rates by monitoring microcirculatory changes in the palm and fingers, improving diagnosis and treatment management.</p><hr><h3>Docling Technical Report</h3>
<p><a href='http://arxiv.org/abs/2408.09869v1'>http://arxiv.org/abs/2408.09869v1</a></p>
<p><b>Compressor summary</b>: Docling is an open-source package that converts PDF documents using AI models for layout analysis and table structure recognition, running efficiently on common hardware.</p><hr><h3>MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in  Explainable Recommendation</h3>
<p><a href='http://arxiv.org/abs/2408.09865v1'>http://arxiv.org/abs/2408.09865v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a model called MAPLE that generates fine-grained explanations for recommending items to users, using aspect categories as input and achieving better performance than existing review-generation models.</p><hr><h3>ShortCircuit: AlphaZero-Driven Circuit Design</h3>
<p><a href='http://arxiv.org/abs/2408.09858v1'>http://arxiv.org/abs/2408.09858v1</a></p>
<p><b>Compressor summary</b>: ShortCircuit is a transformer-based architecture that uses supervised and reinforcement learning to generate efficient Boolean circuits from truth tables, outperforming existing tools.</p><hr><h3>TaSL: Continual Dialog State Tracking via Task Skill Localization and  Consolidation</h3>
<p><a href='http://arxiv.org/abs/2408.09857v1'>http://arxiv.org/abs/2408.09857v1</a></p>
<p><b>Compressor summary</b>: TaSL is a framework that improves Continual Dialogue State Tracking by using group-wise techniques and skill consolidation to balance knowledge preservation and adaptation.</p><hr><h3>TeamLoRA: Boosting Low-Rank Adaptation with Expert Collaboration and  Competition</h3>
<p><a href='http://arxiv.org/abs/2408.09856v1'>http://arxiv.org/abs/2408.09856v1</a></p>
<p><b>Compressor summary</b>: TeamLoRA is a novel PEFT method that combines collaboration and competition among task-specific LoRA modules to enhance multi-task learning efficiency and performance.</p><hr><h3>Self-Directed Turing Test for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.09853v1'>http://arxiv.org/abs/2408.09853v1</a></p>
<p><b>Compressor summary</b>: The Self-Directed Turing Test is a new way to evaluate Large Language Models' human-like behaviour in natural language conversations by allowing more dynamic exchanges and reducing human involvement.</p><hr><h3>Importance Weighting Can Help Large Language Models Self-Improve</h3>
<p><a href='http://arxiv.org/abs/2408.09849v1'>http://arxiv.org/abs/2408.09849v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Large language models (LLMs) are useful but costly to fine-tune with external supervision
- LLM self-improvement involves training on self-generated data, which may have low quality
- The paper proposes a new metric called DS weight to filter out correct but highly shifted samples
- The approach improves reasoning ability and competes with methods using pre-trained reward models

Summary:
The paper introduces DS weight, a new metric to filter self-generated data for LLM self-improvement, which enhances reasoning and rivals external supervision.</p><hr><h3>Continual Dialogue State Tracking via Reason-of-Select Distillation</h3>
<p><a href='http://arxiv.org/abs/2408.09846v1'>http://arxiv.org/abs/2408.09846v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Reason-of-Select distillation, a method that enhances dialogue systems with meta-reasoning and domain bootstrapping to improve continual learning and mitigate forgetting.</p><hr><h3>Demystifying Reinforcement Learning in Production Scheduling via  Explainable AI</h3>
<p><a href='http://arxiv.org/abs/2408.09841v1'>http://arxiv.org/abs/2408.09841v1</a></p>
<p><b>Compressor summary</b>: The paper applies xAI frameworks to explain the reasoning behind scheduling decisions of a DRL agent, but finds that current methods lack falsifiability, consistent terminology, and causal interpretations; they propose a hypotheses-based workflow to address these issues.</p><hr><h3>Machine Learning with Physics Knowledge for Prediction: A Survey</h3>
<p><a href='http://arxiv.org/abs/2408.09840v1'>http://arxiv.org/abs/2408.09840v1</a></p>
<p><b>Compressor summary</b>: The survey explores various methods and models for combining machine learning with physics knowledge to improve prediction and forecast using partial differential equations, considering both architectural and data-driven approaches and their industrial applications.</p><hr><h3>Mitigating the Stability-Plasticity Dilemma in Adaptive Train Scheduling  with Curriculum-Driven Continual DQN Expansion</h3>
<p><a href='http://arxiv.org/abs/2408.09838v1'>http://arxiv.org/abs/2408.09838v1</a></p>
<p><b>Compressor summary</b>: CDE is a new algorithm that uses curriculum learning and Q-function subspaces to improve learning efficiency and adaptability in complex multi-agent domains like train scheduling.</p><hr><h3>Minor DPO reject penalty to increase training robustness</h3>
<p><a href='http://arxiv.org/abs/2408.09834v1'>http://arxiv.org/abs/2408.09834v1</a></p>
<p><b>Compressor summary</b>: The text describes a method called Direct Preference Optimization (DPO) for fine-tuning language models based on human preferences without reinforcement learning, and proposes MinorDPO as an improvement to address some limitations.</p><hr><h3>TDNetGen: Empowering Complex Network Resilience Prediction with  Generative Augmentation of Topology and Dynamics</h3>
<p><a href='http://arxiv.org/abs/2408.09825v1'>http://arxiv.org/abs/2408.09825v1</a></p>
<p><b>Compressor summary</b>: The paper proposes TDNetGen, a novel framework that uses generative data augmentation to predict network resilience without needing labeled data or prior knowledge of network dynamics.</p><hr><h3>SurgicaL-CD: Generating Surgical Images via Unpaired Image Translation  with Latent Consistency Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2408.09822v1'>http://arxiv.org/abs/2408.09822v1</a></p>
<p><b>Compressor summary</b>: SurgicaL-CD is a new method to create realistic surgical images for training machine learning models using diffusion and consistency distillation, improving quality and utility over previous methods.</p><hr><h3>Symplectic Neural Networks Based on Dynamical Systems</h3>
<p><a href='http://arxiv.org/abs/2408.09821v1'>http://arxiv.org/abs/2408.09821v1</a></p>
<p><b>Compressor summary</b>: SympNets are a new type of neural network that can approximate symplectic maps and solve Hamiltonian systems more accurately and efficiently than existing methods.</p><hr><h3>CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2408.09819v1'>http://arxiv.org/abs/2408.09819v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Paper introduces CMoralEval, a large and diverse moral evaluation dataset for Chinese LLMs
- Data sources are TV program discussing moral norms and newspaper articles on moral anomalies
- Morality taxonomy and principles based on traditional Chinese culture and contemporary norms
- Platform with AI-assisted instance generation and annotation to streamline construction and evaluation of CMoralEval
- Experiments show that CMoralEval is a challenging benchmark for Chinese LLMs

Summary:
The paper presents CMoralEval, a new dataset to test the morality of Chinese LLMs using TV programs and newspaper articles as data sources, with a morality taxonomy and principles derived from Chinese culture and norms.</p><hr><h3>Liquid Fourier Latent Dynamics Networks for fast GPU-based numerical  simulations in computational cardiology</h3>
<p><a href='http://arxiv.org/abs/2408.09818v1'>http://arxiv.org/abs/2408.09818v1</a></p>
<p><b>Compressor summary</b>: Liquid Fourier LDNets (LFLDNets) are an extension of Latent Dynamics Networks for creating surrogate models of complex differential equations, improving performance, accuracy, and efficiency in computational cardiology applications.</p><hr><h3>A Population-to-individual Tuning Framework for Adapting Pretrained LM  to On-device User Intent Prediction</h3>
<p><a href='http://arxiv.org/abs/2408.09815v1'>http://arxiv.org/abs/2408.09815v1</a></p>
<p><b>Compressor summary</b>: PITuning is a framework that uses pre-trained language models to improve user intent prediction on smartphones by adapting to diverse event sequences and addressing long-tailed preferences.</p><hr><h3>World Models Increase Autonomy in Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2408.09807v1'>http://arxiv.org/abs/2408.09807v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Reinforcement learning (RL) is a paradigm for training intelligent agents from experience
- Model-based (MB) RL methods are better suited for reset-free setting than previous methods
- MoReFree agent adapts exploration and policy learning to prioritize task-relevant states
- MoReFree outperforms privileged baselines with less supervision and data

Summary:
MoReFree is a model-based reset-free RL agent that learns from experience and prioritizes task-relevant states, achieving superior performance with minimal supervision.</p><hr><h3>Latent Diffusion for Guided Document Table Generation</h3>
<p><a href='http://arxiv.org/abs/2408.09800v1'>http://arxiv.org/abs/2408.09800v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a novel method to generate realistic and annotated images of complex table structures using latent diffusion models, which improves the performance of object detection models like YOLOv5.</p><hr><h3>Enhance Modality Robustness in Text-Centric Multimodal Alignment with  Adversarial Prompting</h3>
<p><a href='http://arxiv.org/abs/2408.09798v1'>http://arxiv.org/abs/2408.09798v1</a></p>
<p><b>Compressor summary</b>: Text-centric adversarial training improves robustness of multimodal models by converting diverse inputs into unified textual representation despite noise, order changes, and missing modalities.</p><hr><h3>AutoML-guided Fusion of Entity and LLM-based representations</h3>
<p><a href='http://arxiv.org/abs/2408.09794v1'>http://arxiv.org/abs/2408.09794v1</a></p>
<p><b>Compressor summary</b>: This paper shows how incorporating knowledge base information into language models improves text classification and enables faster, efficient classifiers using matrix factorization.</p><hr><h3>Unsupervised Composable Representations for Audio</h3>
<p><a href='http://arxiv.org/abs/2408.09792v1'>http://arxiv.org/abs/2408.09792v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a framework for compositional representation learning for music data using generative models
- The framework can perform unsupervised audio source separation, generation, and variation generation
- The framework achieves comparable or superior performance to other methods and has lower computational cost

Summary:
The paper presents a novel framework that leverages generative models and compositional representation learning for music data, enabling unsupervised source separation, generation, and variation generation with high quality and low computational cost.</p><hr><h3>Structure-enhanced Contrastive Learning for Graph Clustering</h3>
<p><a href='http://arxiv.org/abs/2408.09790v1'>http://arxiv.org/abs/2408.09790v1</a></p>
<p><b>Compressor summary</b>: SECL is a novel contrastive learning method for graph clustering that leverages network structures and outperforms existing methods.</p><hr><h3>Anim-Director: A Large Multimodal Model Powered Agent for Controllable  Animation Video Generation</h3>
<p><a href='http://arxiv.org/abs/2408.09787v1'>http://arxiv.org/abs/2408.09787v1</a></p>
<p><b>Compressor summary</b>: The text describes an autonomous animation-making agent called Anim-Director that uses large multimodal models and generative AI tools to create coherent and context-rich animations from concise narratives or simple instructions.</p><hr><h3>Cross-composition Feature Disentanglement for Compositional Zero-shot  Learning</h3>
<p><a href='http://arxiv.org/abs/2408.09786v1'>http://arxiv.org/abs/2408.09786v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to learn disentangled visual features across different compositions using a compositional graph and CLIP with adapters, improving Compositional Zero-shot Learning performance.</p><hr><h3>GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining  Automotive Software Release Decision-Making</h3>
<p><a href='http://arxiv.org/abs/2408.09785v1'>http://arxiv.org/abs/2408.09785v1</a></p>
<p><b>Compressor summary</b>: GoNoGo is a Large Language Model agent system that automates software deployment decisions in the automotive industry, reducing costs and delays while meeting functional and industrial constraints.</p><hr><h3>Summarizing long regulatory documents with a multi-step pipeline</h3>
<p><a href='http://arxiv.org/abs/2408.09777v1'>http://arxiv.org/abs/2408.09777v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a two-step approach to summarize long regulatory texts and shows that its effectiveness depends on the encoder-decoder model used, highlighting challenges in evaluating generated texts.</p><hr><h3>Faster Adaptive Decentralized Learning Algorithms</h3>
<p><a href='http://arxiv.org/abs/2408.09775v1'>http://arxiv.org/abs/2408.09775v1</a></p>
<p><b>Compressor summary</b>: The paper proposes new adaptive decentralized algorithms for distributed machine learning tasks and proves their near-optimal sample complexity.</p><hr><h3>Are Large Language Models More Honest in Their Probabilistic or  Verbalized Confidence?</h3>
<p><a href='http://arxiv.org/abs/2408.09773v1'>http://arxiv.org/abs/2408.09773v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes how large language models perceive their knowledge boundaries through probabilistic and verbalized confidence, finding that probabilistic perception is more accurate but both are affected by question frequency and natural language expression challenges.</p><hr><h3>MalLight: Influence-Aware Coordinated Traffic Signal Control for Traffic  Signal Malfunctions</h3>
<p><a href='http://arxiv.org/abs/2408.09768v1'>http://arxiv.org/abs/2408.09768v1</a></p>
<p><b>Compressor summary</b>: The paper proposes MalLight, a novel traffic signal control framework that uses reinforcement learning to optimize the functioning of surrounding signals and reduce congestion and collisions caused by malfunctioning signals.</p><hr><h3>Baby Bear: Seeking a Just Right Rating Scale for Scalar Annotations</h3>
<p><a href='http://arxiv.org/abs/2408.09765v1'>http://arxiv.org/abs/2408.09765v1</a></p>
<p><b>Compressor summary</b>: The paper proposes IBWS, an iterative method for robustly ranking elements using crowd-sourced data, and evaluates cheaper direct assessment methods that can scale to large datasets.</p><hr><h3>Strategic Demonstration Selection for Improved Fairness in LLM  In-Context Learning</h3>
<p><a href='http://arxiv.org/abs/2408.09757v1'>http://arxiv.org/abs/2408.09757v1</a></p>
<p><b>Compressor summary</b>: This study shows how changing demonstrations in in-context learning can improve the fairness of large language models without losing accuracy and proposes a new technique to curate diverse data samples for better performance and fairness.</p><hr><h3>A Unified Framework for Iris Anti-Spoofing: Introducing IrisGeneral  Dataset and Masked-MoE Method</h3>
<p><a href='http://arxiv.org/abs/2408.09752v1'>http://arxiv.org/abs/2408.09752v1</a></p>
<p><b>Compressor summary</b>: The paper introduces IrisGeneral, a comprehensive dataset for iris anti-spoofing evaluation, and proposes Masked-MoE, a novel method to improve generalization across devices and racial groups using multiple sub-neural networks.</p><hr><h3>Enhanced Cascade Prostate Cancer Classifier in mp-MRI Utilizing Recall  Feedback Adaptive Loss and Prior Knowledge-Based Feature Extraction</h3>
<p><a href='http://arxiv.org/abs/2408.09746v1'>http://arxiv.org/abs/2408.09746v1</a></p>
<p><b>Compressor summary</b>: The authors propose a solution for automated prostate cancer grading in mpMRI that incorporates prior knowledge, addresses data imbalance, and maintains interpretability using feature extraction, adaptive feedback loss, and an enhanced cascade classifier.</p><hr><h3>RealCustom++: Representing Images as Real-Word for Real-Time  Customization</h3>
<p><a href='http://arxiv.org/abs/2408.09744v1'>http://arxiv.org/abs/2408.09744v1</a></p>
<p><b>Compressor summary</b>: RealCustom++ is a new method for text-to-image customization that uses real words instead of pseudo-words to improve both subject similarity and text controllability in generated images.</p><hr><h3>R2GenCSR: Retrieving Context Samples for Large Language Model based  X-ray Medical Report Generation</h3>
<p><a href='http://arxiv.org/abs/2408.09743v1'>http://arxiv.org/abs/2408.09743v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel context-guided efficient X-ray medical report generation framework using Mamba as the vision backbone and context retrieval from the training set to enhance feature representation and generate high-quality reports.</p><hr><h3>Paired Completion: Flexible Quantification of Issue-framing at Scale  with LLMs</h3>
<p><a href='http://arxiv.org/abs/2408.09742v1'>http://arxiv.org/abs/2408.09742v1</a></p>
<p><b>Compressor summary</b>: The paper develops and evaluates paired completion, a novel method for detecting and quantifying issue framing in textual discourse using next-token log probabilities from generative large language models, which shows promising results in scalability, accuracy and low bias.</p><hr><h3>TraDiffusion: Trajectory-Based Training-Free Image Generation</h3>
<p><a href='http://arxiv.org/abs/2408.09739v1'>http://arxiv.org/abs/2408.09739v1</a></p>
<p><b>Compressor summary</b>: TraDiffusion is a training-free method for controlling image generation with mouse movements that can manipulate various aspects of the image while following a specified trajectory.</p><hr><h3>Mutually-Aware Feature Learning for Few-Shot Object Counting</h3>
<p><a href='http://arxiv.org/abs/2408.09734v1'>http://arxiv.org/abs/2408.09734v1</a></p>
<p><b>Compressor summary</b>: MAFEA is a novel framework for few-shot object counting that encodes query and exemplar features mutually aware of each other, reducing target confusion and achieving state-of-the-art performance on two benchmarks.</p><hr><h3>sTransformer: A Modular Approach for Extracting Inter-Sequential and  Temporal Information for Time-Series Forecasting</h3>
<p><a href='http://arxiv.org/abs/2408.09723v1'>http://arxiv.org/abs/2408.09723v1</a></p>
<p><b>Compressor summary</b>: The paper proposes sTransformer, a new Transformer-based model with STCN and Sequence-guided Mask Attention to improve long-term time-series forecasting by capturing sequential and temporal information.</p><hr><h3>Towards Few-Shot Learning in the Open World: A Review and Beyond</h3>
<p><a href='http://arxiv.org/abs/2408.09722v1'>http://arxiv.org/abs/2408.09722v1</a></p>
<p><b>Compressor summary</b>: This paper reviews recent progress in adapting few-shot learning methods for open-world settings, where data is uncertain, incomplete, and dynamic, and discusses the challenges, strengths, and weaknesses of three types of open-world few-shot learning approaches.</p><hr><h3>Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large  Language Model Augmented Framework</h3>
<p><a href='http://arxiv.org/abs/2408.09720v1'>http://arxiv.org/abs/2408.09720v1</a></p>
<p><b>Compressor summary</b>: The paper introduces MSP60K, a large-scale cross-domain pedestrian attribute recognition dataset, and proposes LLM-PAR, a framework that combines vision transformers with language models for better performance.</p><hr><h3>SEMDR: A Semantic-Aware Dual Encoder Model for Legal Judgment Prediction  with Legal Clue Tracing</h3>
<p><a href='http://arxiv.org/abs/2408.09717v1'>http://arxiv.org/abs/2408.09717v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel Semantic-Aware Dual Encoder Model (SEMDR) that uses a legal clue tracing mechanism to conduct fine-grained semantic reasoning between criminal facts and instruments for accurate Legal Judgment Prediction (LJP).</p><hr><h3>HYDEN: Hyperbolic Density Representations for Medical Images and Reports</h3>
<p><a href='http://arxiv.org/abs/2408.09715v1'>http://arxiv.org/abs/2408.09715v1</a></p>
<p><b>Compressor summary</b>: HYDEN uses hyperbolic space to learn image-text representations that handle semantic uncertainty in the medical domain, outperforming baselines on zero-shot tasks.</p><hr><h3>Dataset Distillation for Histopathology Image Classification</h3>
<p><a href='http://arxiv.org/abs/2408.09709v1'>http://arxiv.org/abs/2408.09709v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Histo-DD, a novel dataset distillation algorithm for histopathology image analysis that improves compatibility with high colour heterogeneity and generates more informative synthetic samples than previous methods.</p><hr><h3>MePT: Multi-Representation Guided Prompt Tuning for Vision-Language  Model</h3>
<p><a href='http://arxiv.org/abs/2408.09706v1'>http://arxiv.org/abs/2408.09706v1</a></p>
<p><b>Compressor summary</b>: MePT is a novel method that uses diverse visual prompts to improve VLMs' generalization ability for various downstream tasks.</p><hr><h3>Community-Centric Graph Unlearning</h3>
<p><a href='http://arxiv.org/abs/2408.09705v1'>http://arxiv.org/abs/2408.09705v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Graph unlearning technology is important for privacy and security of AI
- Existing methods are inefficient and lack structural information
- The paper proposes a novel framework called Community-centric Graph Eraser (CGE) that reduces data and parameters
Summary:
The paper introduces CGE, a new graph unlearning method that efficiently eliminates specific data from graph neural networks by mapping community subgraphs to nodes.</p><hr><h3>Partial-Multivariate Model for Forecasting</h3>
<p><a href='http://arxiv.org/abs/2408.09703v1'>http://arxiv.org/abs/2408.09703v1</a></p>
<p><b>Compressor summary</b>: PMformer is a new Transformer-based model that captures partial relationships among some time-series features and achieves better forecasting results than existing univariate or complete-multivariate models, while also being efficient and robust to missing data.</p><hr><h3>Photorealistic Object Insertion with Diffusion-Guided Inverse Rendering</h3>
<p><a href='http://arxiv.org/abs/2408.09702v1'>http://arxiv.org/abs/2408.09702v1</a></p>
<p><b>Compressor summary</b>: The text describes a method to realistically insert virtual objects into real-world images by using a diffusion model to guide an inverse rendering process that recovers scene lighting and other parameters, improving the appearance of the composited object.</p><hr><h3>Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code  Generation in LLMs via Zero-Shot Cross-Lingual Transfer</h3>
<p><a href='http://arxiv.org/abs/2408.09701v1'>http://arxiv.org/abs/2408.09701v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a zero-shot cross-lingual approach using a neural projection technique to improve code generation for non-English prompts, addressing biases and limitations of current Large Language Models.</p><hr><h3>LightWeather: Harnessing Absolute Positional Encoding to Efficient and  Scalable Global Weather Forecasting</h3>
<p><a href='http://arxiv.org/abs/2408.09695v1'>http://arxiv.org/abs/2408.09695v1</a></p>
<p><b>Compressor summary</b>: The paper proposes LightWeather, a lightweight and effective Transformer-based model for global weather forecasting, using absolute positional encoding to capture spatial-temporal correlations without attention mechanisms.</p><hr><h3>Recording for Eyes, Not Echoing to Ears: Contextualized  Spoken-to-Written Conversion of ASR Transcripts</h3>
<p><a href='http://arxiv.org/abs/2408.09688v1'>http://arxiv.org/abs/2408.09688v1</a></p>
<p><b>Compressor summary</b>: The authors propose a task to convert spoken language transcripts into written text with improved readability and evaluate the performance of large language models on this task using a new benchmark dataset.</p><hr><h3>Simulating Field Experiments with Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.09682v1'>http://arxiv.org/abs/2408.09682v1</a></p>
<p><b>Compressor summary</b>: The paper explores using large language models to simulate field experiments and proposes two prompting strategies, observer and participant modes, which show promising results in certain scenarios but also identify limitations.</p><hr><h3>MambaLoc: Efficient Camera Localisation via State Space Model</h3>
<p><a href='http://arxiv.org/abs/2408.09680v1'>http://arxiv.org/abs/2408.09680v1</a></p>
<p><b>Compressor summary</b>: MambaLoc is a new visual localization model that uses selective state space (SSM) to improve training efficiency, robustness in sparse data environments, and global feature extraction, while GIS enhances Non-local Neural Networks' performance with SSM's computational efficiency.</p><hr><h3>Image-based Freeform Handwriting Authentication with Energy-oriented  Self-Supervised Learning</h3>
<p><a href='http://arxiv.org/abs/2408.09676v1'>http://arxiv.org/abs/2408.09676v1</a></p>
<p><b>Compressor summary</b>: SherlockNet is a self-supervised learning framework for handwriting authentication that handles noisy data, high-dimensional features, and lack of supervision by using energy-oriented contrastive learning and personalized fine-tuning.</p><hr><h3>Implicit Grid Convolution for Multi-Scale Image Super-Resolution</h3>
<p><a href='http://arxiv.org/abs/2408.09674v1'>http://arxiv.org/abs/2408.09674v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new framework for super-resolution using neural networks that trains multiple scales with one model and introduces a novel upsampler called Implicit Grid Convolution (IGConv) that improves performance.</p><hr><h3>BLADE: Benchmarking Language Model Agents for Data-Driven Science</h3>
<p><a href='http://arxiv.org/abs/2408.09667v1'>http://arxiv.org/abs/2408.09667v1</a></p>
<p><b>Compressor summary</b>: BLADE is a benchmark to evaluate agents' abilities in data-driven scientific discovery by comparing their analyses with expert-verified ground truth.</p><hr><h3>SG-GS: Photo-realistic Animatable Human Avatars with Semantically-Guided  Gaussian Splatting</h3>
<p><a href='http://arxiv.org/abs/2408.09665v1'>http://arxiv.org/abs/2408.09665v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes SG-GS, a method to reconstruct human avatars from monocular videos using semantics-embedded 3D Gaussians, skeleton deformation, and cloth dynamics deformation.
- The paper also introduces SHA, a tool for efficient body part semantic labeling, and a 3D network that integrates geometric and semantic associations for human avatar deformation.
- The paper enhances the semantic accuracy of 3D Gaussians and rendering quality with three strategies: semantic projection, semantic-guided density regularization, and semantic-aware regularization with neighborhood consistency.

Summary:
The paper presents SG-GS, a method that uses semantics-embedded 3D Gaussians and deformation techniques to reconstruct realistic human avatars from monocular videos, achieving state-of-the-art performance with semantic guidance and regularization.</p><hr><h3>CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian  Splatting and Contrastive Learning</h3>
<p><a href='http://arxiv.org/abs/2408.09663v1'>http://arxiv.org/abs/2408.09663v1</a></p>
<p><b>Compressor summary</b>: CHASE is a method for creating realistic human avatars with supervision from intrinsic 3D consistency and 3D geometry contrastive learning, achieving better performance than current state-of-the-art methods in both full and sparse input settings.</p><hr><h3>A Comparison of Large Language Model and Human Performance on Random  Number Generation Tasks</h3>
<p><a href='http://arxiv.org/abs/2408.09656v1'>http://arxiv.org/abs/2408.09656v1</a></p>
<p><b>Compressor summary</b>: This study tests if ChatGPT-3.5, an AI language model, shows similar biases to humans when generating random numbers, finding that it avoids repetition better than humans.</p><hr><h3>Contextual Bandits for Unbounded Context Distributions</h3>
<p><a href='http://arxiv.org/abs/2408.09655v1'>http://arxiv.org/abs/2408.09655v1</a></p>
<p><b>Compressor summary</b>: The paper proposes two nearest neighbor methods for nonparametric contextual bandits with unbounded contexts and analyzes their regret bounds under different conditions.</p><hr><h3>ExpoMamba: Exploiting Frequency SSM Blocks for Efficient and Effective  Image Enhancement</h3>
<p><a href='http://arxiv.org/abs/2408.09650v1'>http://arxiv.org/abs/2408.09650v1</a></p>
<p><b>Compressor summary</b>: ExpoMamba is a fast and efficient model that enhances low-light images by combining frequency components with a modified U-Net, outperforming traditional models in speed and quality.</p><hr><h3>C2P-CLIP: Injecting Category Common Prompt in CLIP to Enhance  Generalization in Deepfake Detection</h3>
<p><a href='http://arxiv.org/abs/2408.09647v1'>http://arxiv.org/abs/2408.09647v1</a></p>
<p><b>Compressor summary</b>: The study analyzes how CLIP detects deepfakes by recognizing similar concepts and introduces C2P-CLIP, an improved method that enhances detection performance using category-related concepts.</p><hr><h3>Acquiring Bidirectionality via Large and Small Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.09640v1'>http://arxiv.org/abs/2408.09640v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new method to improve token representation for bidirectional language models by adding a small backward LM, which improves performance in named entity recognition and other tasks.</p><hr><h3>How to Make the Most of LLMs' Grammatical Knowledge for Acceptability  Judgments</h3>
<p><a href='http://arxiv.org/abs/2408.09639v1'>http://arxiv.org/abs/2408.09639v1</a></p>
<p><b>Compressor summary</b>: The study compares different methods to measure the grammatical knowledge of large language models and suggests using a variety of methods for comprehensive evaluation.</p>