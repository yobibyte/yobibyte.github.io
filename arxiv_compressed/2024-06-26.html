
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-06-26</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-06-26 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Text-Animator: Controllable Visual Text Video Generation</h3>
<p><a href='http://arxiv.org/abs/2406.17777v1'>http://arxiv.org/abs/2406.17777v1</a></p>
<p><b>Compressor summary</b>: Text-Animator is a novel method for generating videos with accurate and coherent visual texts by controlling camera movement and refining text motions.</p><hr><h3>Fast and Uncertainty-Aware SVBRDF Recovery from Multi-View Capture using  Frequency Domain Analysis</h3>
<p><a href='http://arxiv.org/abs/2406.17774v1'>http://arxiv.org/abs/2406.17774v1</a></p>
<p><b>Compressor summary</b>: The authors propose a fast and accurate method to estimate material properties of objects under uncontrolled lighting using signal-processing techniques, while also quantifying uncertainty for improved acquisition quality.</p><hr><h3>MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning</h3>
<p><a href='http://arxiv.org/abs/2406.17770v1'>http://arxiv.org/abs/2406.17770v1</a></p>
<p><b>Compressor summary</b>: MG-LLaVA is a multi-modal large language model that enhances visual processing by using multi-granularity features and outperforms existing models on perception tasks.</p><hr><h3>BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context  Learning</h3>
<p><a href='http://arxiv.org/abs/2406.17764v1'>http://arxiv.org/abs/2406.17764v1</a></p>
<p><b>Compressor summary</b>: The authors introduce BMIKE-53, a benchmark for evaluating cross-lingual knowledge editing methods on 53 languages, and propose MIKE, a gradient-free method that shows promising results.</p><hr><h3>DiffusionPDE: Generative PDE-Solving Under Partial Observation</h3>
<p><a href='http://arxiv.org/abs/2406.17763v1'>http://arxiv.org/abs/2406.17763v1</a></p>
<p><b>Compressor summary</b>: The paper presents DiffusionPDE, a method that uses generative diffusion models to solve partial differential equations (PDEs) with missing information, achieving better results than existing methods.</p><hr><h3>Solving Hard Mizar Problems with Instantiation and Strategy Invention</h3>
<p><a href='http://arxiv.org/abs/2406.17762v1'>http://arxiv.org/abs/2406.17762v1</a></p>
<p><b>Compressor summary</b>: The authors use various ATP and AI methods to solve over 3000 previously unsolved Mizar problems, increasing the percentage of ATP-solved Mizar problems from 75% to above 80%.</p><hr><h3>CaLMQA: Exploring culturally specific long-form question answering  across 23 languages</h3>
<p><a href='http://arxiv.org/abs/2406.17761v1'>http://arxiv.org/abs/2406.17761v1</a></p>
<p><b>Compressor summary</b>: CaLMQA is a diverse dataset of complex questions in 23 languages that reveals limitations of large language models in handling low-resource, culturally specific questions.</p><hr><h3>Interpreting Attention Layer Outputs with Sparse Autoencoders</h3>
<p><a href='http://arxiv.org/abs/2406.17759v1'>http://arxiv.org/abs/2406.17759v1</a></p>
<p><b>Compressor summary</b>: This paper trains sparse autoencoders on attention layer outputs in transformers to decompose them into interpretable features, discovering different feature families and roles, and using them to better understand and explain model behavior.</p><hr><h3>MotionBooth: Motion-Aware Customized Text-to-Video Generation</h3>
<p><a href='http://arxiv.org/abs/2406.17758v1'>http://arxiv.org/abs/2406.17758v1</a></p>
<p><b>Compressor summary</b>: MotionBooth is a framework that animates customized subjects with precise control over their shape, attributes, and motions using text-to-video models and training-free techniques.</p><hr><h3>Accelerating Clinical Evidence Synthesis with Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.17755v1'>http://arxiv.org/abs/2406.17755v1</a></p>
<p><b>Compressor summary</b>: TrialMind is a generative AI pipeline for conducting medical systematic reviews, using large language models and human expert oversight, that outperforms traditional methods in literature search, screening, and data extraction.</p><hr><h3>Measuring and Benchmarking Large Language Models' Capabilities to  Generate Persuasive Language</h3>
<p><a href='http://arxiv.org/abs/2406.17753v1'>http://arxiv.org/abs/2406.17753v1</a></p>
<p><b>Compressor summary</b>: The authors study how well Large Language Models (LLMs) can produce persuasive text and create a new dataset, Persuasive-Pairs, to measure and compare LLMs' abilities across various domains.</p><hr><h3>A New Perspective on Shampoo's Preconditioner</h3>
<p><a href='http://arxiv.org/abs/2406.17748v1'>http://arxiv.org/abs/2406.17748v1</a></p>
<p><b>Compressor summary</b>: Shampoo is an optimization algorithm that approximates the Gauss-Newton component or the covariance matrix using a Kronecker product, and its approximation is close to the optimal one.</p><hr><h3>Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted  Phenomenon</h3>
<p><a href='http://arxiv.org/abs/2406.17746v1'>http://arxiv.org/abs/2406.17746v1</a></p>
<p><b>Compressor summary</b>: The text proposes a taxonomy for memorization in language models, considering various factors that affect each type of memorization and using it to build a predictive model.</p><hr><h3>Following Length Constraints in Instructions</h3>
<p><a href='http://arxiv.org/abs/2406.17744v1'>http://arxiv.org/abs/2406.17744v1</a></p>
<p><b>Compressor summary</b>: Instruction-length controlling models perform better than standard models in evaluations that consider response length.</p><hr><h3>Point-SAM: Promptable 3D Segmentation Model for Point Clouds</h3>
<p><a href='http://arxiv.org/abs/2406.17741v1'>http://arxiv.org/abs/2406.17741v1</a></p>
<p><b>Compressor summary</b>: Point-SAM is a transformer-based 3D model that leverages 2D knowledge from SAM to segment point clouds with part-level and object-level annotations, achieving state-of-the-art performance on various benchmarks.</p><hr><h3>Structured Unrestricted-Rank Matrices for Parameter Efficient  Fine-tuning</h3>
<p><a href='http://arxiv.org/abs/2406.17740v1'>http://arxiv.org/abs/2406.17740v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new framework for fine-tuning large Transformer models using structured unrestricted-rank matrices, which offer more flexibility and parameter efficiency than existing methods like Adapters and LoRA.</p><hr><h3>Find Parent then Label Children: A Two-stage Taxonomy Completion Method  with Pre-trained Language Model</h3>
<p><a href='http://arxiv.org/abs/2406.17739v1'>http://arxiv.org/abs/2406.17739v1</a></p>
<p><b>Compressor summary</b>: ATTEMPT is a novel method for updating taxonomies by inserting new concepts at the appropriate position using pre-trained language models.</p><hr><h3>LLM Targeted Underperformance Disproportionately Impacts Vulnerable  Users</h3>
<p><a href='http://arxiv.org/abs/2406.17737v1'>http://arxiv.org/abs/2406.17737v1</a></p>
<p><b>Compressor summary</b>: The study examines how the quality of responses from large language models varies depending on a user's English proficiency, education level, and country of origin, finding that these models are less reliable for users with lower proficiency or education, and those outside the US.</p><hr><h3>Arboretum: A Large Multimodal Dataset Enabling AI for Biodiversity</h3>
<p><a href='http://arxiv.org/abs/2406.17720v1'>http://arxiv.org/abs/2406.17720v1</a></p>
<p><b>Compressor summary</b>: Arboretum is a large dataset of diverse species images from iNaturalist with rich annotations for AI applications in biodiversity assessment and agriculture research.</p><hr><h3>When does Self-Prediction help? Understanding Auxiliary Tasks in  Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2406.17718v1'>http://arxiv.org/abs/2406.17718v1</a></p>
<p><b>Compressor summary</b>: The paper examines how auxiliary tasks like observation reconstruction and latent self-prediction affect representation learning in reinforcement learning, and shows that latent self-prediction is more helpful as an auxiliary task than observation reconstruction when dealing with distractions and non-linear functions.</p><hr><h3>ViANLI: Adversarial Natural Language Inference for Vietnamese</h3>
<p><a href='http://arxiv.org/abs/2406.17716v1'>http://arxiv.org/abs/2406.17716v1</a></p>
<p><b>Compressor summary</b>: The ViANLI dataset is an adversarial NLP dataset for Vietnamese natural language inference that challenges current machine learning models and improves their performance.</p><hr><h3>Compositional Models for Estimating Causal Effects</h3>
<p><a href='http://arxiv.org/abs/2406.17714v1'>http://arxiv.org/abs/2406.17714v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a modular, compositional approach to estimate individual treatment effects in structured systems composed of multiple heterogeneous components, with benefits such as systematic generalization and improved overlap guarantees.</p><hr><h3>Data curation via joint example selection further accelerates multimodal  learning</h3>
<p><a href='http://arxiv.org/abs/2406.17711v1'>http://arxiv.org/abs/2406.17711v1</a></p>
<p><b>Compressor summary</b>: JEST is an algorithm that selects batches of data jointly and improves training speed and efficiency in multimodal contrastive learning.</p><hr><h3>SurgeMOD: Translating image-space tissue motions into vision-based  surgical forces</h3>
<p><a href='http://arxiv.org/abs/2406.17707v1'>http://arxiv.org/abs/2406.17707v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new method to estimate forces in robotic surgery using video data and frequency domain analysis of organ motion.</p><hr><h3>HGTDP-DTA: Hybrid Graph-Transformer with Dynamic Prompt for Drug-Target  Binding Affinity Prediction</h3>
<p><a href='http://arxiv.org/abs/2406.17697v1'>http://arxiv.org/abs/2406.17697v1</a></p>
<p><b>Compressor summary</b>: HGTDP-DTA is a novel method for predicting drug target binding affinity using dynamic prompts and a hybrid Graph-Transformer architecture that integrates structural, sequence, and contextual information.</p><hr><h3>From Distributional to Overton Pluralism: Investigating Large Language  Model Alignment</h3>
<p><a href='http://arxiv.org/abs/2406.17692v1'>http://arxiv.org/abs/2406.17692v1</a></p>
<p><b>Compressor summary</b>: Alignment changes large language models' output distribution, but the effects are mostly superficial and can be replicated without fine-tuning.</p><hr><h3>Unified Auto-Encoding with Masked Diffusion</h3>
<p><a href='http://arxiv.org/abs/2406.17688v1'>http://arxiv.org/abs/2406.17688v1</a></p>
<p><b>Compressor summary</b>: UMD is a new auto-encoder that combines patch-based and noise-based image corruption techniques, leading to improved generative and representation learning performance.</p><hr><h3>VarBench: Robust Language Model Benchmarking Through Dynamic Variable  Perturbation</h3>
<p><a href='http://arxiv.org/abs/2406.17681v1'>http://arxiv.org/abs/2406.17681v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to dynamically evaluate language models by variabilizing benchmarks and sampling new values from test cases, ensuring fresh evaluations and reducing data contamination.</p><hr><h3>End-to-End Autonomous Driving without Costly Modularization and 3D  Manual Annotation</h3>
<p><a href='http://arxiv.org/abs/2406.17680v1'>http://arxiv.org/abs/2406.17680v1</a></p>
<p><b>Compressor summary</b>: UAD is a vision-based autonomous driving method that uses unsupervised learning to reduce annotation requirements, computation overhead, and improve performance in nuScenes and CARLA.</p><hr><h3>Local-to-Global Cross-Modal Attention-Aware Fusion for HSI-X Semantic  Segmentation</h3>
<p><a href='http://arxiv.org/abs/2406.17679v1'>http://arxiv.org/abs/2406.17679v1</a></p>
<p><b>Compressor summary</b>: The LoGoCAF framework uses a two-branch semantic segmentation architecture with local-to-global encoder and MLP decoder to fuse hyperspectral and X-modality data for efficient, accurate, and generalizable classification.</p><hr><h3>Quantifying AI Psychology: A Psychometrics Benchmark for Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2406.17675v1'>http://arxiv.org/abs/2406.17675v1</a></p>
<p><b>Compressor summary</b>: The paper presents a framework to study psychological attributes in large language models, creating a benchmark with six dimensions, and finds discrepancies between self-reported traits and real-world behaviors.</p><hr><h3>LaTable: Towards Large Tabular Models</h3>
<p><a href='http://arxiv.org/abs/2406.17673v1'>http://arxiv.org/abs/2406.17673v1</a></p>
<p><b>Compressor summary</b>: The paper introduces LaTable, a novel diffusion model for generating tabular data that works across different datasets and improves out-of-distribution performance, while exploring its limitations in zero-shot settings.</p><hr><h3>LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic</h3>
<p><a href='http://arxiv.org/abs/2406.17663v1'>http://arxiv.org/abs/2406.17663v1</a></p>
<p><b>Compressor summary</b>: LLM-ARC combines a large language model with an automated reasoning critic to improve logical reasoning and achieve state-of-the-art accuracy on the FOLIO benchmark.</p><hr><h3>Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse  Gradients</h3>
<p><a href='http://arxiv.org/abs/2406.17660v1'>http://arxiv.org/abs/2406.17660v1</a></p>
<p><b>Compressor summary</b>: Grass is a novel sparse projection-based optimization method that reduces memory usage and computational costs for large language model training, enabling half-precision pretraining on a 13B parameter model with a $2	imes$ throughput improvement.</p><hr><h3>DKPROMPT: Domain Knowledge Prompting Vision-Language Models for  Open-World Planning</h3>
<p><a href='http://arxiv.org/abs/2406.17659v1'>http://arxiv.org/abs/2406.17659v1</a></p>
<p><b>Compressor summary</b>: DKPROMPT combines vision-language models with domain knowledge in PDDL to improve robot task planning in open worlds.</p><hr><h3>ELIZA Reinterpreted: The world's first chatbot was not intended as a  chatbot at all</h3>
<p><a href='http://arxiv.org/abs/2406.17650v1'>http://arxiv.org/abs/2406.17650v1</a></p>
<p><b>Compressor summary</b>: ELIZA, the first chatbot created by Joseph Weizenbaum in the early 1960s, was actually meant for research on human-machine conversation, but its accidental release and misunderstanding led to its fame as a chatbot and loss of the original source for over 50 years.</p><hr><h3>Privacy Preserving Reinforcement Learning for Population Processes</h3>
<p><a href='http://arxiv.org/abs/2406.17649v1'>http://arxiv.org/abs/2406.17649v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a meta algorithm to make any Reinforcement Learning (RL) algorithm privacy-preserving in the setting of population processes, such as controlling epidemics, and shows that it achieves reasonable trade-offs between privacy and utility.</p><hr><h3>Variationist: Exploring Multifaceted Variation and Bias in Written  Language Data</h3>
<p><a href='http://arxiv.org/abs/2406.17647v1'>http://arxiv.org/abs/2406.17647v1</a></p>
<p><b>Compressor summary</b>: Variationist is a new tool that helps researchers explore and visualize language variation and bias across multiple variables and metrics.</p><hr><h3>Banishing LLM Hallucinations Requires Rethinking Generalization</h3>
<p><a href='http://arxiv.org/abs/2406.17642v1'>http://arxiv.org/abs/2406.17642v1</a></p>
<p><b>Compressor summary</b>: Large Language Models (LLMs) often generate false information, and this study explores why traditional methods fail to prevent it and proposes a new model called Lamini-1 that uses multiple memory experts to store facts and reduce hallucinations.</p><hr><h3>BayTTA: Uncertainty-aware medical image classification with optimized  test-time augmentation using Bayesian model averaging</h3>
<p><a href='http://arxiv.org/abs/2406.17640v1'>http://arxiv.org/abs/2406.17640v1</a></p>
<p><b>Compressor summary</b>: BayTTA is a framework that optimizes test-time augmentation for computer vision tasks using Bayesian Model Averaging, improving accuracy and robustness on various medical image analysis and gene editing datasets.</p><hr><h3>Mitigate the Gap: Investigating Approaches for Improving Cross-Modal  Alignment in CLIP</h3>
<p><a href='http://arxiv.org/abs/2406.17639v1'>http://arxiv.org/abs/2406.17639v1</a></p>
<p><b>Compressor summary</b>: AlignCLIP is a method to improve cross-modal alignment in CLIP embeddings by sharing parameters and separating uni-modal embeddings, reducing the modality gap while maintaining performance on various tasks.</p><hr><h3>Aligning Diffusion Models with Noise-Conditioned Perception</h3>
<p><a href='http://arxiv.org/abs/2406.17636v1'>http://arxiv.org/abs/2406.17636v1</a></p>
<p><b>Compressor summary</b>: The proposed method improves text-to-image diffusion models by using a perceptual objective in the U-Net embedding space, leading to better human preference alignment and reduced computational cost.</p><hr><h3>Knowledge Distillation in Automated Annotation: Supervised Text  Classification with LLM-Generated Training Labels</h3>
<p><a href='http://arxiv.org/abs/2406.17633v1'>http://arxiv.org/abs/2406.17633v1</a></p>
<p><b>Compressor summary</b>: The study shows that using large language models to generate training labels can replace human annotations for text classification tasks in computational social science, leading to similar performance with faster and cheaper methods.</p><hr><h3>Video Inpainting Localization with Contrastive Learning</h3>
<p><a href='http://arxiv.org/abs/2406.17628v1'>http://arxiv.org/abs/2406.17628v1</a></p>
<p><b>Compressor summary</b>: The text proposes a method called ViLocal that uses contrastive learning to identify inpainted regions in videos and localize them using a 3D Uniformer encoder and a lightweight convolution decoder.</p><hr><h3>CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue  Coreference</h3>
<p><a href='http://arxiv.org/abs/2406.17626v1'>http://arxiv.org/abs/2406.17626v1</a></p>
<p><b>Compressor summary</b>: The text discusses a study on large language models' safety in multi-turn dialogue coreference and reveals their vulnerability to such attacks.</p><hr><h3>Self-assessment, Exhibition, and Recognition: a Review of Personality in  Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.17624v1'>http://arxiv.org/abs/2406.17624v1</a></p>
<p><b>Compressor summary</b>: This paper reviews current research on personality in large language models, categorizing studies into self-assessment, exhibition, and recognition, and providing a comprehensive overview of findings, challenges, resources, and future directions.</p><hr><h3>Embedded event based object detection with spiking neural network</h3>
<p><a href='http://arxiv.org/abs/2406.17617v1'>http://arxiv.org/abs/2406.17617v1</a></p>
<p><b>Compressor summary</b>: The research introduces an embedded neuromorphic testbench using SPLEAT accelerator to train and deploy efficient SNNs for event-based object detection on low-power hardware.</p><hr><h3>MSRS: Training Multimodal Speech Recognition Models from Scratch with  Sparse Mask Optimization</h3>
<p><a href='http://arxiv.org/abs/2406.17614v1'>http://arxiv.org/abs/2406.17614v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a regularization technique called MSRS for training speech recognition models from scratch, which reduces costs and improves performance.</p><hr><h3>Distributed Training of Large Graph Neural Networks with Variable  Communication Rates</h3>
<p><a href='http://arxiv.org/abs/2406.17611v1'>http://arxiv.org/abs/2406.17611v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a variable compression scheme for distributed GNN training that reduces communication volume without sacrificing accuracy, and shows its effectiveness in empirical results.</p><hr><h3>Test-Time Generative Augmentation for Medical Image Segmentation</h3>
<p><a href='http://arxiv.org/abs/2406.17608v1'>http://arxiv.org/abs/2406.17608v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using a generative model to create multiple views of test images for medical image segmentation, improving performance and error estimation.</p><hr><h3>Director3D: Real-world Camera Trajectory and 3D Scene Generation from  Text</h3>
<p><a href='http://arxiv.org/abs/2406.17601v1'>http://arxiv.org/abs/2406.17601v1</a></p>
<p><b>Compressor summary</b>: Director3D is a framework for generating realistic 3D scenes and camera trajectories from textual descriptions, using a combination of transformers, diffusion models, and refinement losses.</p><hr><h3>"Seeing the Big through the Small": Can LLMs Approximate Human Judgment  Distributions on NLI from a Few Explanations?</h3>
<p><a href='http://arxiv.org/abs/2406.17600v1'>http://arxiv.org/abs/2406.17600v1</a></p>
<p><b>Compressor summary</b>: The study suggests using expert labels and explanations with LLMs to approximate human label variation in NLI tasks, improving the scalability of annotations.</p><hr><h3>DocParseNet: Advanced Semantic Segmentation and OCR Embeddings for  Efficient Scanned Document Annotation</h3>
<p><a href='http://arxiv.org/abs/2406.17591v1'>http://arxiv.org/abs/2406.17591v1</a></p>
<p><b>Compressor summary</b>: DocParseNet combines deep learning and multi-modal learning to improve text and image recognition in scanned documents, achieving high accuracy and efficiency for real-world document processing applications.</p><hr><h3>LongIns: A Challenging Long-context Instruction-based Exam for LLMs</h3>
<p><a href='http://arxiv.org/abs/2406.17588v1'>http://arxiv.org/abs/2406.17588v1</a></p>
<p><b>Compressor summary</b>: LongIns is a new benchmark dataset that tests large language models' long-context and reasoning abilities in various settings, revealing their limitations in handling short context windows.</p><hr><h3>Learning Dynamic Bayesian Networks from Data: Foundations, First  Principles and Numerical Comparisons</h3>
<p><a href='http://arxiv.org/abs/2406.17585v1'>http://arxiv.org/abs/2406.17585v1</a></p>
<p><b>Compressor summary</b>: The paper provides a guide on learning Dynamic Bayesian Networks from multiple trajectory samples, covering formalism, structure-weight interdependence, learning methods, and optimization functions, with comparisons of various algorithms.</p><hr><h3>Towards Compositional Interpretability for XAI</h3>
<p><a href='http://arxiv.org/abs/2406.17583v1'>http://arxiv.org/abs/2406.17583v1</a></p>
<p><b>Compressor summary</b>: The authors propose a categorical approach to define and compare AI models' interpretability using string diagrams, revealing common themes in XAI and demonstrating explainability benefits of compositionally-interpretable models.</p><hr><h3>Toward Universal Medical Image Registration via Sharpness-Aware  Meta-Continual Learning</h3>
<p><a href='http://arxiv.org/abs/2406.17575v1'>http://arxiv.org/abs/2406.17575v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a continual learning method for universal 3D medical image registration using meta-learning and sharpness-aware meta-continual learning, achieving better or comparable results to sequential or centralized multi-task training strategies on four datasets.</p><hr><h3>Beyond Text-to-SQL for IoT Defense: A Comprehensive Framework for  Querying and Classifying IoT Threats</h3>
<p><a href='http://arxiv.org/abs/2406.17574v1'>http://arxiv.org/abs/2406.17574v1</a></p>
<p><b>Compressor summary</b>: The research introduces a new text-to-SQL dataset for IoT devices and shows that joint training on query generation and data inference improves performance.</p><hr><h3>FrenchToxicityPrompts: a Large Benchmark for Evaluating and Mitigating  Toxicity in French Texts</h3>
<p><a href='http://arxiv.org/abs/2406.17566v1'>http://arxiv.org/abs/2406.17566v1</a></p>
<p><b>Compressor summary</b>: The authors create a French dataset for evaluating and improving toxicity detection in language models, as current efforts mainly focus on English.</p><hr><h3>Multi-property Steering of Large Language Models with Dynamic Activation  Composition</h3>
<p><a href='http://arxiv.org/abs/2406.17563v1'>http://arxiv.org/abs/2406.17563v1</a></p>
<p><b>Compressor summary</b>: This paper evaluates activation steering methods for language models and proposes Dynamic Activation Composition, an approach to modulate steering intensity based on multiple properties during generation.</p><hr><h3>Minimal Interaction Edge Tuning: A New Paradigm for Visual Adaptation</h3>
<p><a href='http://arxiv.org/abs/2406.17559v1'>http://arxiv.org/abs/2406.17559v1</a></p>
<p><b>Compressor summary</b>: Edge tuning uses pretrained models as feature extractors on cloud servers and fine-tunes small networks on edge devices with minimal information transfer and high adaptation capability using MIET.</p><hr><h3>The FineWeb Datasets: Decanting the Web for the Finest Text Data at  Scale</h3>
<p><a href='http://arxiv.org/abs/2406.17557v1'>http://arxiv.org/abs/2406.17557v1</a></p>
<p><b>Compressor summary</b>: FineWeb is a large pretraining dataset for language models that outperforms other open datasets and reveals insights into data curation strategies.</p><hr><h3>Retrieval-Augmented Code Generation for Situated Action Generation: A  Case Study on Minecraft</h3>
<p><a href='http://arxiv.org/abs/2406.17553v1'>http://arxiv.org/abs/2406.17553v1</a></p>
<p><b>Compressor summary</b>: The paper explores using large language models to predict actions in Minecraft collaborative building with few-shot prompts and analyzes performance gaps.</p><hr><h3>CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained  Models using Greedy Coordinate Descent</h3>
<p><a href='http://arxiv.org/abs/2406.17542v1'>http://arxiv.org/abs/2406.17542v1</a></p>
<p><b>Compressor summary</b>: CDQuant is a simple and scalable alternative to GPTQ that outperforms it in compressing large language models with minimal impact on performance.</p><hr><h3>Principal Component Clustering for Semantic Segmentation in Synthetic  Data Generation</h3>
<p><a href='http://arxiv.org/abs/2406.17541v1'>http://arxiv.org/abs/2406.17541v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Method for generating synthetic dataset for semantic segmentation using latent diffusion model
- No need for additional segmentation models
- Part of submission to CVPR 2024 workshop challenge
- Self-attentions for semantic information condensation
- Non-prompt-influencing cross-attentions for mask classification
- Mask refinement step using only output image by Stable Diffusion

Summary:
The authors present a method to create synthetic images with segmented objects using a latent diffusion model, without extra segmentation models, and propose various attention mechanisms and a mask refinement step for the CVPR 2024 workshop challenge.</p><hr><h3>SKD-TSTSAN: Three-Stream Temporal-Shift Attention Network Based on  Self-Knowledge Distillation for Micro-Expression Recognition</h3>
<p><a href='http://arxiv.org/abs/2406.17538v1'>http://arxiv.org/abs/2406.17538v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel network that uses motion magnification, channel attention, temporal modeling, and self-knowledge distillation to enhance micro-expression recognition performance.</p><hr><h3>SincVAE: a New Approach to Improve Anomaly Detection on EEG Data Using  SincNet and Variational Autoencoder</h3>
<p><a href='http://arxiv.org/abs/2406.17537v1'>http://arxiv.org/abs/2406.17537v1</a></p>
<p><b>Compressor summary</b>: The text proposes a semi-supervised deep learning method called SincVAE for detecting epileptic seizures in EEG data, which can identify early and late stages of seizures.</p><hr><h3>Disce aut Deficere: Evaluating LLMs Proficiency on the INVALSI Italian  Benchmark</h3>
<p><a href='http://arxiv.org/abs/2406.17535v1'>http://arxiv.org/abs/2406.17535v1</a></p>
<p><b>Compressor summary</b>: The text introduces a structured benchmark using INVALSI tests to evaluate Large Language Models in Italian, providing a reference point for researchers and assessing their performance against human results.</p><hr><h3>Retrieval-style In-Context Learning for Few-shot Hierarchical Text  Classification</h3>
<p><a href='http://arxiv.org/abs/2406.17534v1'>http://arxiv.org/abs/2406.17534v1</a></p>
<p><b>Compressor summary</b>: The authors propose a framework that combines in-context learning with large language models to improve few-shot hierarchical text classification by using a retrieval database, label-aware representations, and a novel contrastive learning objective.</p><hr><h3>Can Large Language Models Understand DL-Lite Ontologies? An Empirical  Study</h3>
<p><a href='http://arxiv.org/abs/2406.17532v1'>http://arxiv.org/abs/2406.17532v1</a></p>
<p><b>Compressor summary</b>: Large language models can understand some aspects of Description Logic ontologies, but struggle with others like transitivity and handling large amounts of data.</p><hr><h3>Point Tree Transformer for Point Cloud Registration</h3>
<p><a href='http://arxiv.org/abs/2406.17530v1'>http://arxiv.org/abs/2406.17530v1</a></p>
<p><b>Compressor summary</b>: The Point Tree Transformer (PTT) is a novel transformer-based approach for point cloud registration that efficiently extracts local and global features while maintaining linear computational complexity by constructing hierarchical feature trees and using a new Point Tree Attention mechanism.</p><hr><h3>LumberChunker: Long-Form Narrative Document Segmentation</h3>
<p><a href='http://arxiv.org/abs/2406.17526v1'>http://arxiv.org/abs/2406.17526v1</a></p>
<p><b>Compressor summary</b>: LumberChunker is a method that uses an LLM to dynamically segment documents for dense retrieval, and it outperforms other methods on the GutenQA benchmark.</p><hr><h3>On the consistency of hyper-parameter selection in value-based deep  reinforcement learning</h3>
<p><a href='http://arxiv.org/abs/2406.17523v1'>http://arxiv.org/abs/2406.17523v1</a></p>
<p><b>Compressor summary</b>: The paper studies how reliable hyper-parameter selection affects value-based deep reinforcement learning agents and introduces a new score to measure consistency.</p><hr><h3>Tell Me Where You Are: Multimodal LLMs Meet Place Recognition</h3>
<p><a href='http://arxiv.org/abs/2406.17520v1'>http://arxiv.org/abs/2406.17520v1</a></p>
<p><b>Compressor summary</b>: The authors propose a multimodal approach using vision-based retrieval and language-based reasoning for visual place recognition in robotics, without requiring supervised training.</p><hr><h3>Entropy-Based Decoding for Retrieval-Augmented Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.17519v1'>http://arxiv.org/abs/2406.17519v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a decoding method to improve retrieval-augmented LLMs by prioritizing relevant and reliable external knowledge, reducing distractibility issues.</p><hr><h3>Enhancing Explainability of Knowledge Learning Paths: Causal Knowledge  Networks</h3>
<p><a href='http://arxiv.org/abs/2406.17518v1'>http://arxiv.org/abs/2406.17518v1</a></p>
<p><b>Compressor summary</b>: The text proposes a method to build causal knowledge networks for effective adaptive learning systems using Bayesian networks and recommendations based on human-centered explainable AI in education.</p><hr><h3>Preserving Node Distinctness in Graph Autoencoders via Similarity  Distillation</h3>
<p><a href='http://arxiv.org/abs/2406.17517v1'>http://arxiv.org/abs/2406.17517v1</a></p>
<p><b>Compressor summary</b>: The authors propose a technique to improve graph autoencoders by transferring node similarity knowledge from raw graphs to reconstructed graphs using a KL constraint, enhancing their distinctiveness and performance.</p><hr><h3>Benchmarking Mental State Representations in Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.17513v1'>http://arxiv.org/abs/2406.17513v1</a></p>
<p><b>Compressor summary</b>: The study examines how different language model characteristics affect their ability to represent mental states and reason about them, using probes and prompt variations.</p><hr><h3>WAVE: Weight Template for Adaptive Initialization of Variable-sized  Models</h3>
<p><a href='http://arxiv.org/abs/2406.17503v1'>http://arxiv.org/abs/2406.17503v1</a></p>
<p><b>Compressor summary</b>: WAVE is a multitasking method that initializes variable-sized models with weight templates learned from pre-trained models, improving efficiency and performance.</p><hr><h3>MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment  and Knowledge Aggregation</h3>
<p><a href='http://arxiv.org/abs/2406.17484v1'>http://arxiv.org/abs/2406.17484v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a two-stage fine-tuning pipeline for large language models to improve their performance on diverse medical tasks by encoding knowledge and filtering noise, as well as aligning the model with task-specific data.</p><hr><h3>TRIP: Trainable Region-of-Interest Prediction for Hardware-Efficient  Neuromorphic Processing on Event-based Vision</h3>
<p><a href='http://arxiv.org/abs/2406.17483v1'>http://arxiv.org/abs/2406.17483v1</a></p>
<p><b>Compressor summary</b>: TRIP is a hardware-efficient hard attention framework for event-based vision processing on neuromorphic processors that produces low-resolution ROIs for efficient and accurate classification, achieving state-of-the-art accuracies and significant improvements in computation, latency, and energy.</p><hr><h3>Transformer-based Named Entity Recognition with Combined Data  Representation</h3>
<p><a href='http://arxiv.org/abs/2406.17474v1'>http://arxiv.org/abs/2406.17474v1</a></p>
<p><b>Compressor summary</b>: The study explores transformer-based models for named entity recognition, finding that combining different data representation strategies improves performance across multiple languages and datasets.</p><hr><h3>UHD-IQA Benchmark Database: Pushing the Boundaries of Blind Photo  Quality Assessment</h3>
<p><a href='http://arxiv.org/abs/2406.17472v1'>http://arxiv.org/abs/2406.17472v1</a></p>
<p><b>Compressor summary</b>: The authors present a new Image Quality Assessment dataset with 6073 high-resolution, aesthetic images, annotated by experts and enriched with metadata, to improve perceptual image quality evaluation research.</p><hr><h3>Cross-Modal Spherical Aggregation for Weakly Supervised Remote Sensing  Shadow Removal</h3>
<p><a href='http://arxiv.org/abs/2406.17469v1'>http://arxiv.org/abs/2406.17469v1</a></p>
<p><b>Compressor summary</b>: The paper proposes S2-ShadowNet, a network that uses both visible and infrared images for shadow removal, by learning cross-domain mapping and exploiting a spherical feature space with similarity and orthogonality losses.</p><hr><h3>Early learning of the optimal constant solution in neural networks and  humans</h3>
<p><a href='http://arxiv.org/abs/2406.17467v1'>http://arxiv.org/abs/2406.17467v1</a></p>
<p><b>Compressor summary</b>: The text describes how deep neural networks initially learn the optimal constant solution (OCS), which is a pattern in the target labels, before adapting to more complex functions during training. This OCS phase is observed not only in linear networks but also in nonlinear ones and human learners, suggesting it as a universal learning principle.</p><hr><h3>Enhancing Tool Retrieval with Iterative Feedback from Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2406.17465v1'>http://arxiv.org/abs/2406.17465v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve tool learning for large language models using iterative feedback between the tool usage model and the tool retriever model, addressing challenges such as complex user instructions and misalignment between models.</p><hr><h3>The Tree of Diffusion Life: Evolutionary Embeddings to Understand the  Generation Process of Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2406.17462v1'>http://arxiv.org/abs/2406.17462v1</a></p>
<p><b>Compressor summary</b>: TDL is a method to visualize and understand the data evolution in diffusion models by embedding high-dimensional samples into a lower-dimensional space preserving their relations and evolutionary structure.</p><hr><h3>Investigating Self-Supervised Methods for Label-Efficient Learning</h3>
<p><a href='http://arxiv.org/abs/2406.17460v1'>http://arxiv.org/abs/2406.17460v1</a></p>
<p><b>Compressor summary</b>: The paper compares different self-supervised learning tasks for vision transformers and introduces a framework using masked image modelling and clustering that performs well on low-shot downstream tasks.</p><hr><h3>Continuous Urban Change Detection from Satellite Image Time Series with  Temporal Feature Refinement and Multi-Task Integration</h3>
<p><a href='http://arxiv.org/abs/2406.17458v1'>http://arxiv.org/abs/2406.17458v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a deep learning method using self-attention and Markov networks for continuous urban change detection from satellite image time series, achieving promising results.</p><hr><h3>Improving Grammatical Error Correction via Contextual Data Augmentation</h3>
<p><a href='http://arxiv.org/abs/2406.17456v1'>http://arxiv.org/abs/2406.17456v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a contextual augmentation method for creating synthetic data in Grammatical Error Correction, which improves error distribution consistency and uses relabeling to reduce noisy labels.</p><hr><h3>Learning to Ask Informative Questions: Enhancing LLMs with Preference  Optimization and Expected Information Gain</h3>
<p><a href='http://arxiv.org/abs/2406.17453v1'>http://arxiv.org/abs/2406.17453v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve LLM-generated questions' informativeness for 20-question game dialogues using Direct Preference Optimization on question pairs.</p><hr><h3>Pseudo Labelling for Enhanced Masked Autoencoders</h3>
<p><a href='http://arxiv.org/abs/2406.17450v1'>http://arxiv.org/abs/2406.17450v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an enhanced Masked Autoencoder model that uses token-level reconstruction and pseudo labeling with a decoupled teacher network to improve performance on various image tasks.</p><hr><h3>Using joint angles based on the international biomechanical standards  for human action recognition and related tasks</h3>
<p><a href='http://arxiv.org/abs/2406.17443v1'>http://arxiv.org/abs/2406.17443v1</a></p>
<p><b>Compressor summary</b>: The paper introduces biomechanical notions to convert keypoint data into joint angles that are suitable for machine learning and interpretation by humans in various applications like sports and medicine.</p><hr><h3>Mamba24/8D: Enhancing Global Interaction in Point Clouds via State Space  Model</h3>
<p><a href='http://arxiv.org/abs/2406.17442v1'>http://arxiv.org/abs/2406.17442v1</a></p>
<p><b>Compressor summary</b>: Mamba is a new architecture that uses state space models to improve 3D point cloud semantic segmentation with linear complexity and strong global modeling capability.</p><hr><h3>Implicit-Zoo: A Large-Scale Dataset of Neural Implicit Functions for 2D  Images and 3D Scenes</h3>
<p><a href='http://arxiv.org/abs/2406.17438v1'>http://arxiv.org/abs/2406.17438v1</a></p>
<p><b>Compressor summary</b>: Implicit-Zoo is a large dataset for neural implicit functions that improves performance in computer vision tasks like image classification, semantic segmentation, and 3D pose regression.</p><hr><h3>Advancing Question Answering on Handwritten Documents: A  State-of-the-Art Recognition-Based Model for HW-SQuAD</h3>
<p><a href='http://arxiv.org/abs/2406.17437v1'>http://arxiv.org/abs/2406.17437v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Paper proposes a novel recognition-based approach for question-answering handwritten documents
- Model uses transformer-based document retrieval and ensemble methods
- Achieves state-of-the-art results on HW-SQuAD and BenthamQA datasets
- Code and trained models will be publicly available

Summary: The paper presents a novel recognition-based approach that uses transformer-based document retrieval and ensemble methods to improve question-answering on handwritten documents, achieving state-of-the-art results and releasing code and models.</p><hr><h3>Mind the Graph When Balancing Data for Fairness or Robustness</h3>
<p><a href='http://arxiv.org/abs/2406.17433v1'>http://arxiv.org/abs/2406.17433v1</a></p>
<p><b>Compressor summary</b>: This paper studies how data balancing can affect fairness and robustness in machine learning, and emphasizes the need to consider the causal graph for effective mitigation strategies.</p><hr><h3>Towards Probing Speech-Specific Risks in Large Multimodal Models: A  Taxonomy, Benchmark, and Insights</h3>
<p><a href='http://arxiv.org/abs/2406.17430v1'>http://arxiv.org/abs/2406.17430v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a taxonomy of speech-specific risks, such as sarcasm, imitation, and biases, and evaluates LMMs' effectiveness in detecting them.</p><hr><h3>A Critical Analysis of the Theoretical Framework of the Extreme Learning  Machine</h3>
<p><a href='http://arxiv.org/abs/2406.17427v1'>http://arxiv.org/abs/2406.17427v1</a></p>
<p><b>Compressor summary</b>: The Extreme Learning Machine (ELM) lacks rigorous mathematical justification, and we refute its proofs, create a counterexample dataset, and offer alternative foundational statements.</p><hr><h3>Leave No Document Behind: Benchmarking Long-Context LLMs with Extended  Multi-Doc QA</h3>
<p><a href='http://arxiv.org/abs/2406.17419v1'>http://arxiv.org/abs/2406.17419v1</a></p>
<p><b>Compressor summary</b>: Loong is a novel benchmark for evaluating large language models in realistic long-context scenarios through extended multi-document question answering with diverse tasks and context lengths.</p><hr><h3>SE-VGAE: Unsupervised Disentangled Representation Learning for  Interpretable Architectural Layout Design Graph Generation</h3>
<p><a href='http://arxiv.org/abs/2406.17418v1'>http://arxiv.org/abs/2406.17418v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new framework, SE-VGAE, that uses unsupervised disentangled representation learning to generate and interpret architectural layout graphs from floor plan images.</p><hr><h3>Variable Layer-Wise Quantization: A Simple and Effective Approach to  Quantize LLMs</h3>
<p><a href='http://arxiv.org/abs/2406.17415v1'>http://arxiv.org/abs/2406.17415v1</a></p>
<p><b>Compressor summary</b>: The authors propose a variable quantization approach for large language models, where different layers are quantized at varying bit levels based on their importance, resulting in minimal performance drop and compressed model size.</p><hr><h3>Consensus Learning with Deep Sets for Essential Matrix Estimation</h3>
<p><a href='http://arxiv.org/abs/2406.17414v1'>http://arxiv.org/abs/2406.17414v1</a></p>
<p><b>Compressor summary</b>: Our method, based on Deep Sets, estimates the essential matrix by identifying outliers and modeling noise in point matches, outperforming complex networks.</p><hr><h3>Depth-Guided Semi-Supervised Instance Segmentation</h3>
<p><a href='http://arxiv.org/abs/2406.17413v1'>http://arxiv.org/abs/2406.17413v1</a></p>
<p><b>Compressor summary</b>: The Depth-Guided Semi-Supervised Instance Segmentation framework uses depth maps to generate precise contours for distinct instances, overcoming limitations of RGB information, and achieves better performance than previous methods.</p><hr><h3>Less can be more: representational vs. stereotypical gender bias in  facial expression recognition</h3>
<p><a href='http://arxiv.org/abs/2406.17405v1'>http://arxiv.org/abs/2406.17405v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how demographic biases, especially stereotypical ones, in facial expression recognition datasets affect machine learning models' predictions.</p><hr><h3>Make Some Noise: Unlocking Language Model Parallel Inference Capability  through Noisy Training</h3>
<p><a href='http://arxiv.org/abs/2406.17404v1'>http://arxiv.org/abs/2406.17404v1</a></p>
<p><b>Compressor summary</b>: The Make Some Noise (MSN) training framework improves parallel decoding and inference speed of large language models without sacrificing performance by introducing noise and using a tree-based retrieval-augmented decoding strategy.</p><hr><h3>GradCheck: Analyzing classifier guidance gradients for conditional  diffusion sampling</h3>
<p><a href='http://arxiv.org/abs/2406.17399v1'>http://arxiv.org/abs/2406.17399v1</a></p>
<p><b>Compressor summary</b>: The study analyzes how to improve the quality of samples from a specific type of model using different techniques, focusing on the stability of gradients for better guidance.</p><hr><h3>SyncNoise: Geometrically Consistent Noise Prediction for Text-based 3D  Scene Editing</h3>
<p><a href='http://arxiv.org/abs/2406.17396v1'>http://arxiv.org/abs/2406.17396v1</a></p>
<p><b>Compressor summary</b>: SyncNoise is a novel approach for consistent 3D scene editing using 2D diffusion models, achieving high-quality results with global and local consistency across multiple viewpoints.</p><hr><h3>Native Design Bias: Studying the Impact of English Nativeness on  Language Model Performance</h3>
<p><a href='http://arxiv.org/abs/2406.17385v1'>http://arxiv.org/abs/2406.17385v1</a></p>
<p><b>Compressor summary</b>: The study shows that LLMs give lower-quality or factually incorrect responses to non-native English speakers more frequently than to native speakers, and this difference persists across different regions.</p><hr><h3>Automatic infant 2D pose estimation from videos: comparing seven deep  neural network methods</h3>
<p><a href='http://arxiv.org/abs/2406.17382v1'>http://arxiv.org/abs/2406.17382v1</a></p>
<p><b>Compressor summary</b>: This paper tests seven human pose estimation methods on infant videos and finds ViTPose to be the best performer for understanding infant movement in natural settings.</p><hr><h3>Forget but Recall: Incremental Latent Rectification in Continual  Learning</h3>
<p><a href='http://arxiv.org/abs/2406.17381v1'>http://arxiv.org/abs/2406.17381v1</a></p>
<p><b>Compressor summary</b>: ILR is a new continual learning approach that uses rectifier units to correct and update old task representations in deep neural networks, improving performance on several benchmarks.</p><hr><h3>A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns  Well with The Key Tokens</h3>
<p><a href='http://arxiv.org/abs/2406.17378v1'>http://arxiv.org/abs/2406.17378v1</a></p>
<p><b>Compressor summary</b>: The text describes an interesting phenomenon in large language models where the text embeddings can be aligned with key tokens, and shows its potential applications in information retrieval and understanding fuzzy concepts.</p><hr><h3>A Three-Pronged Approach to Cross-Lingual Adaptation with Multilingual  LLMs</h3>
<p><a href='http://arxiv.org/abs/2406.17377v1'>http://arxiv.org/abs/2406.17377v1</a></p>
<p><b>Compressor summary</b>: The text explores three cross-lingual methods to adapt an English-dominated LLM to Indic languages and finds that additional supervisory signals and continued pre-training in one low-resource language help improve performance.</p><hr><h3>An Empirical Study on the Characteristics of Bias upon Context Length  Variation for Bangla</h3>
<p><a href='http://arxiv.org/abs/2406.17375v1'>http://arxiv.org/abs/2406.17375v1</a></p>
<p><b>Compressor summary</b>: The authors create a Bangla dataset for measuring gender bias in pretrained language models and show that context length affects bias metrics, calling for more nuanced analysis.</p><hr><h3>Generalizability of experimental studies</h3>
<p><a href='http://arxiv.org/abs/2406.17374v1'>http://arxiv.org/abs/2406.17374v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a mathematical formalization of experimental studies in machine learning and develops a quantifiable measure of generalizability, which is applied to two benchmarks and can be used with a provided Python module.</p><hr><h3>Leveraging Synthetic Audio Data for End-to-End Low-Resource Speech  Translation</h3>
<p><a href='http://arxiv.org/abs/2406.17363v1'>http://arxiv.org/abs/2406.17363v1</a></p>
<p><b>Compressor summary</b>: The paper presents Irish-to-English speech translation systems using Whisper with various data augmentation techniques to improve performance.</p><hr><h3>Stacked Confusion Reject Plots (SCORE)</h3>
<p><a href='http://arxiv.org/abs/2406.17346v1'>http://arxiv.org/abs/2406.17346v1</a></p>
<p><b>Compressor summary</b>: SCORE is a new tool for machine learning applications that helps users understand how uncertain their predictions are by using stacked confusion matrices and reject curves.</p><hr><h3>NerfBaselines: Consistent and Reproducible Evaluation of Novel View  Synthesis Methods</h3>
<p><a href='http://arxiv.org/abs/2406.17345v1'>http://arxiv.org/abs/2406.17345v1</a></p>
<p><b>Compressor summary</b>: The paper introduces NerfBaselines, a framework to simplify installation and evaluation of novel view synthesis methods, and provides a web platform for comparison.</p><hr><h3>Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers</h3>
<p><a href='http://arxiv.org/abs/2406.17343v1'>http://arxiv.org/abs/2406.17343v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Q-DiT, a technique to improve image synthesis quality and efficiency for diffusion transformers by fine-grained quantization and other techniques.</p><hr><h3>Masked Generative Extractor for Synergistic Representation and 3D  Generation of Point Clouds</h3>
<p><a href='http://arxiv.org/abs/2406.17342v1'>http://arxiv.org/abs/2406.17342v1</a></p>
<p><b>Compressor summary</b>: Point-MAGE is a framework that leverages generative modeling and representation learning for point cloud data, achieving state-of-the-art performance in various tasks.</p><hr><h3>Generative Modelling of Structurally Constrained Graphs</h3>
<p><a href='http://arxiv.org/abs/2406.17341v1'>http://arxiv.org/abs/2406.17341v1</a></p>
<p><b>Compressor summary</b>: ConStruct is a novel framework that allows hard-constraining graph diffusion models to incorporate specific properties such as planarity or acyclicity, ensuring valid graphs for practical applications.</p><hr><h3>Dual-Space Knowledge Distillation for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.17328v1'>http://arxiv.org/abs/2406.17328v1</a></p>
<p><b>Compressor summary</b>: DSKD is a novel framework to compress large language models by unifying their output spaces and aligning their representations using cross-model attention.</p><hr><h3>The State-Action-Reward-State-Action Algorithm in Spatial Prisoner's  Dilemma Game</h3>
<p><a href='http://arxiv.org/abs/2406.17326v1'>http://arxiv.org/abs/2406.17326v1</a></p>
<p><b>Compressor summary</b>: The study applies reinforcement learning (RL) to evolutionary game theory, using the SARSA algorithm to model how cooperative behavior emerges and changes among self-interested agents.</p><hr><h3>Delving into the Utilisation of ChatGPT in Scientific Publications in  Astronomy</h3>
<p><a href='http://arxiv.org/abs/2406.17324v1'>http://arxiv.org/abs/2406.17324v1</a></p>
<p><b>Compressor summary</b>: The study examines the use of large language models in astronomy papers and finds a significant increase in words favored by ChatGPT, suggesting widespread adoption and need for ethical guidelines.</p><hr><h3>XAMI -- A Benchmark Dataset for Artefact Detection in XMM-Newton Optical  Images</h3>
<p><a href='http://arxiv.org/abs/2406.17323v1'>http://arxiv.org/abs/2406.17323v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Scattered light artefacts in astronomical observations are problematic and need automated detection
- A dataset of images with different types of artefacts from the XMM-Newton space telescope is presented
- A hybrid model combining CNNs and transformers is used to detect and mask artefacts using instance segmentation
- The method and dataset can help advance artefact detection in astronomical observations

Summary:
The authors present a new dataset and a hybrid model that can detect and mask scattered light artefacts in astronomical images from the XMM-Newton space telescope using instance segmentation.</p><hr><h3>ALPBench: A Benchmark for Active Learning Pipelines on Tabular Data</h3>
<p><a href='http://arxiv.org/abs/2406.17322v1'>http://arxiv.org/abs/2406.17322v1</a></p>
<p><b>Compressor summary</b>: ALPBench is a tool for benchmarking and comparing active learning pipelines, which consists of 86 real-world datasets and supports various query strategies and learning algorithms.</p><hr><h3>DMF-Net: Image-Guided Point Cloud Completion with Dual-Channel Modality  Fusion and Shape-Aware Upsampling Transformer</h3>
<p><a href='http://arxiv.org/abs/2406.17319v1'>http://arxiv.org/abs/2406.17319v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new dual-channel modality fusion network (DMF-Net) for completing partial point clouds using image guidance, which performs better than existing methods.</p><hr><h3>Not All Preference Pairs Are Created Equal: A Recipe for  Annotation-Efficient Iterative Preference Learning</h3>
<p><a href='http://arxiv.org/abs/2406.17312v1'>http://arxiv.org/abs/2406.17312v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to select which response pairs to annotate for iterative preference learning, considering uncertainty and distribution shifts, to achieve better performance with less annotation cost.</p><hr><h3>Zero-Shot Long-Form Video Understanding through Screenplay</h3>
<p><a href='http://arxiv.org/abs/2406.17309v1'>http://arxiv.org/abs/2406.17309v1</a></p>
<p><b>Compressor summary</b>: MM-Screenplayer is an advanced system that converts long videos into textual screenplays by organizing them into scenes and using a "Look Back" strategy to validate uncertain information, achieving high scores in a challenge.</p><hr><h3>Retrieval Augmented Instruction Tuning for Open NER with Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2406.17305v1'>http://arxiv.org/abs/2406.17305v1</a></p>
<p><b>Compressor summary</b>: The paper explores using Retrieval Augmented Instruction Tuning (RA-IT) for information extraction in open named entity recognition tasks, showing its effectiveness across languages and data sizes.</p><hr><h3>Leveraging LLMs for Dialogue Quality Measurement</h3>
<p><a href='http://arxiv.org/abs/2406.17304v1'>http://arxiv.org/abs/2406.17304v1</a></p>
<p><b>Compressor summary</b>: The paper explores using large language models (LLMs) for evaluating dialogue quality, finding that larger models, algorithmic example selection, chain-of-thought reasoning, and fine-tuning improve performance.</p><hr><h3>CausalScore: An Automatic Reference-Free Metric for Assessing Response  Relevance in Open-Domain Dialogue Systems</h3>
<p><a href='http://arxiv.org/abs/2406.17300v1'>http://arxiv.org/abs/2406.17300v1</a></p>
<p><b>Compressor summary</b>: CausalScore is a novel metric that measures the relevance of responses in open-domain dialogues by estimating the causal strength between dialogue histories and responses, outperforming existing metrics in aligning with human judgements.</p><hr><h3>Towards Efficient and Scalable Training of Differentially Private Deep  Learning</h3>
<p><a href='http://arxiv.org/abs/2406.17298v1'>http://arxiv.org/abs/2406.17298v1</a></p>
<p><b>Compressor summary</b>: The paper investigates the high computational cost of differentially private deep learning training and compares methods to reduce it.</p><hr><h3>Towards Open-set Camera 3D Object Detection</h3>
<p><a href='http://arxiv.org/abs/2406.17297v1'>http://arxiv.org/abs/2406.17297v1</a></p>
<p><b>Compressor summary</b>: OS-Det3D is a two-stage framework that uses a novel 3D Object Discovery Network and Joint Objectness Selection module to improve camera 3D object detection for both known and unknown objects.</p><hr><h3>BlockLLM: Memory-Efficient Adaptation of LLMs by Selecting and  Optimizing the Right Coordinate Blocks</h3>
<p><a href='http://arxiv.org/abs/2406.17296v1'>http://arxiv.org/abs/2406.17296v1</a></p>
<p><b>Compressor summary</b>: BlockLLM reduces memory requirements for training large language models by selecting and updating a small subset of parameters without changing the model architecture or training procedure.</p><hr><h3>Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.17294v1'>http://arxiv.org/abs/2406.17294v1</a></p>
<p><b>Compressor summary</b>: The authors create MathV360K, a diverse multimodal dataset for image instruction fine-tuning, and introduce Math-LLaVA, a model that improves multimodal mathematical reasoning with this dataset.</p><hr><h3>Predicting the Big Five Personality Traits in Chinese Counselling  Dialogues Using Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.17287v1'>http://arxiv.org/abs/2406.17287v1</a></p>
<p><b>Compressor summary</b>: The study shows that Large Language Models can predict personality traits from counseling dialogues using role-play simulations and questionnaires, outperforming previous methods.</p><hr><h3>A Recursive Encoding for Cuneiform Signs</h3>
<p><a href='http://arxiv.org/abs/2406.17283v1'>http://arxiv.org/abs/2406.17283v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a recursive encoding system for cuneiform signs that simplifies sign lookup, allows for computer processing, and enables new methods of rendering and displaying signs and tablets.</p><hr><h3>BERT, Neural Information Retrieval, Boolean Retrieval, Negation  Retrieval</h3>
<p><a href='http://arxiv.org/abs/2406.17282v1'>http://arxiv.org/abs/2406.17282v1</a></p>
<p><b>Compressor summary</b>: SetBERT is a small and effective BERT-based model for enhancing logic-structured queries by using inversed-contrastive loss and outperforming BERT-base.</p><hr><h3>Distance Recomputator and Topology Reconstructor for Graph Neural  Networks</h3>
<p><a href='http://arxiv.org/abs/2406.17281v1'>http://arxiv.org/abs/2406.17281v1</a></p>
<p><b>Compressor summary</b>: The paper presents two new methods to improve Graph Neural Networks (GNNs) by dynamically adjusting node distances and local graph structures for better representation and aggregation of complex and dynamic graphs.</p><hr><h3>OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure</h3>
<p><a href='http://arxiv.org/abs/2406.17276v1'>http://arxiv.org/abs/2406.17276v1</a></p>
<p><b>Compressor summary</b>: Speculative decoding uses adaptive draft trees to generate multiple tokens per step, improving inference efficiency of autoregressive language models.</p><hr><h3>Can We Trust the Performance Evaluation of Uncertainty Estimation  Methods in Text Summarization?</h3>
<p><a href='http://arxiv.org/abs/2406.17274v1'>http://arxiv.org/abs/2406.17274v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a comprehensive benchmark for evaluating uncertainty estimation in text summarization, incorporating various NLG metrics and human annotations.</p><hr><h3>A Comprehensive Solution to Connect Speech Encoder and Large Language  Model for ASR</h3>
<p><a href='http://arxiv.org/abs/2406.17272v1'>http://arxiv.org/abs/2406.17272v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a solution to improve speech recognition by connecting speech encoders to large language models with fine-tuning schemes, modality alignment enhancement, and methods to reduce insertion errors.</p><hr><h3>DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning  Graph</h3>
<p><a href='http://arxiv.org/abs/2406.17271v1'>http://arxiv.org/abs/2406.17271v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method called DARG to generate dynamic and diverse evaluation data for Large Language Models, revealing their performance and bias patterns under different complexity levels.</p><hr><h3>Image-Guided Outdoor LiDAR Perception Quality Assessment for Autonomous  Driving</h3>
<p><a href='http://arxiv.org/abs/2406.17265v1'>http://arxiv.org/abs/2406.17265v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new algorithm for assessing LiDAR point cloud quality in outdoor autonomous driving environments using both image data and ground truth annotations, improving detection performance.</p><hr><h3>Efficient, Multimodal, and Derivative-Free Bayesian Inference With  Fisher-Rao Gradient Flows</h3>
<p><a href='http://arxiv.org/abs/2406.17263v1'>http://arxiv.org/abs/2406.17263v1</a></p>
<p><b>Compressor summary</b>: The paper proposes GMKI, an efficient derivative-free sampler for handling multi-modal distributions in Bayesian inference for large-scale inverse problems.</p><hr><h3>D2LLM: Decomposed and Distilled Large Language Models for Semantic  Search</h3>
<p><a href='http://arxiv.org/abs/2406.17262v1'>http://arxiv.org/abs/2406.17262v1</a></p>
<p><b>Compressor summary</b>: D2LLM combines a bi-encoder and an interaction module to achieve both efficiency and nuanced understanding in semantic search, outperforming five baselines on three tasks.</p><hr><h3>TRAWL: Tensor Reduced and Approximated Weights for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.17261v1'>http://arxiv.org/abs/2406.17261v1</a></p>
<p><b>Compressor summary</b>: TRAWL optimizes large language models through tensor decomposition to improve performance without retraining or extra data, making AI systems more efficient and sustainable.</p><hr><h3>Mitigating Hallucination in Fictional Character Role-Play</h3>
<p><a href='http://arxiv.org/abs/2406.17260v1'>http://arxiv.org/abs/2406.17260v1</a></p>
<p><b>Compressor summary</b>: The authors present a method to reduce hallucination in role-playing dialogues by adjusting the influence of large language models' world knowledge with a confidence threshold, and demonstrate its effectiveness on a new dataset.</p><hr><h3>Leveraging Parameter-Efficient Transfer Learning for Multi-Lingual  Text-to-Speech Adaptation</h3>
<p><a href='http://arxiv.org/abs/2406.17257v1'>http://arxiv.org/abs/2406.17257v1</a></p>
<p><b>Compressor summary</b>: Key points:
- TTS models face challenges in synthesizing speech for multiple languages
- Standard approach uses transformers and large multilingual datasets
- Paper proposes using PETL methods (adapters, hypernetworks) for better performance with less parameters

Summary:
The paper introduces a new method to improve TTS models for multiple languages by using PETL techniques that require fewer parameters and achieve similar or better results than standard fine-tuning.</p><hr><h3>Disentangled Motion Modeling for Video Frame Interpolation</h3>
<p><a href='http://arxiv.org/abs/2406.17256v1'>http://arxiv.org/abs/2406.17256v1</a></p>
<p><b>Compressor summary</b>: The paper proposes MoMo, a diffusion-based approach for video frame interpolation that enhances visual quality by focusing on intermediate motion modeling, using a novel diffusion U-Net architecture and achieving superior perceptual quality with reduced computational demands.</p><hr><h3>MPCODER: Multi-user Personalized Code Generator with Explicit and  Implicit Style Representation Learning</h3>
<p><a href='http://arxiv.org/abs/2406.17255v1'>http://arxiv.org/abs/2406.17255v1</a></p>
<p><b>Compressor summary</b>: MPCoder uses explicit and implicit residual learning to generate personalized, multi-user code with a contrastive adapter and a novel evaluation metric.</p><hr><h3>Scalp Diagnostic System With Label-Free Segmentation and Training-Free  Image Translation</h3>
<p><a href='http://arxiv.org/abs/2406.17254v1'>http://arxiv.org/abs/2406.17254v1</a></p>
<p><b>Compressor summary</b>: ScalpVision is an AI-driven system that uses innovative methods to segment hair and generate data for diagnosing scalp diseases and alopecia.</p><hr><h3>How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?</h3>
<p><a href='http://arxiv.org/abs/2406.17253v1'>http://arxiv.org/abs/2406.17253v1</a></p>
<p><b>Compressor summary</b>: The study explores how different levels of "perplexingness" affect the ability to update large language models with new knowledge, and introduces a novel dataset to investigate this phenomenon.</p><hr><h3>TopoGCL: Topological Graph Contrastive Learning</h3>
<p><a href='http://arxiv.org/abs/2406.17251v1'>http://arxiv.org/abs/2406.17251v1</a></p>
<p><b>Compressor summary</b>: Graph contrastive learning improves representations by incorporating latent shape properties of graphs at multiple resolutions, enhancing performance in unsupervised graph classification.</p><hr><h3>Unlocking Continual Learning Abilities in Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.17245v1'>http://arxiv.org/abs/2406.17245v1</a></p>
<p><b>Compressor summary</b>: Key points:
- LMs struggle with catastrophic forgetting in continual learning (CL)
- MIGU is a rehearsal-free and task-label-free method that updates parameters with large output magnitudes in linear layers
- MIGU is universal, effective, and can integrate with existing CL types

Summary:
MIGU is a novel method that improves LMs' continual learning performance by updating parameters based on output magnitude distribution in linear layers, without rehearsal or task labels.</p><hr><h3>What Do the Circuits Mean? A Knowledge Edit View</h3>
<p><a href='http://arxiv.org/abs/2406.17241v1'>http://arxiv.org/abs/2406.17241v1</a></p>
<p><b>Compressor summary</b>: The authors propose a novel method to learn the meanings of circuit representations in GPT2-XL using knowledge editing and explore their properties, such as size, composition, and overlap with other datasets.</p><hr><h3>Expansive Synthesis: Generating Large-Scale Datasets from Minimal  Samples</h3>
<p><a href='http://arxiv.org/abs/2406.17238v1'>http://arxiv.org/abs/2406.17238v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an Expansive Synthesis model that generates large-quality datasets from minimal samples by using expander graph mappings, feature interpolation, Koopman operator, and optimal transport, achieving performance on par with classifiers trained on full-scale datasets.</p><hr><h3>LIPE: Learning Personalized Identity Prior for Non-rigid Image Editing</h3>
<p><a href='http://arxiv.org/abs/2406.17236v1'>http://arxiv.org/abs/2406.17236v1</a></p>
<p><b>Compressor summary</b>: LIPE is a two-stage framework that learns a personalized identity prior for text-based non-rigid image editing, improving consistency and quality in editing results.</p><hr><h3>Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human  Belief Networks</h3>
<p><a href='http://arxiv.org/abs/2406.17232v1'>http://arxiv.org/abs/2406.17232v1</a></p>
<p><b>Compressor summary</b>: The study explored how integrating human belief networks into large language models can improve their alignment with human opinions on related topics.</p><hr><h3>CogMG: Collaborative Augmentation Between Large Language Model and  Knowledge Graph</h3>
<p><a href='http://arxiv.org/abs/2406.17231v1'>http://arxiv.org/abs/2406.17231v1</a></p>
<p><b>Compressor summary</b>: The CogMG framework uses knowledge graphs to improve question-answering by large language models, reducing hallucinations and increasing accuracy.</p><hr><h3>Large Language Models are Interpretable Learners</h3>
<p><a href='http://arxiv.org/abs/2406.17224v1'>http://arxiv.org/abs/2406.17224v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using large language models and symbolic programs to create interpretable and expressive predictive models for classification tasks, and introduces IL-Bench, a collection of diverse tasks for evaluation.</p><hr><h3>Facial Identity Anonymization via Intrinsic and Extrinsic Attention  Distraction</h3>
<p><a href='http://arxiv.org/abs/2406.17219v1'>http://arxiv.org/abs/2406.17219v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new face anonymization method that distracts both intrinsic and extrinsic identity attention, allowing for flexible manipulation of appearance and geometry structure to protect privacy better than existing methods.</p><hr><h3>Machine Unlearning Fails to Remove Data Poisoning Attacks</h3>
<p><a href='http://arxiv.org/abs/2406.17216v1'>http://arxiv.org/abs/2406.17216v1</a></p>
<p><b>Compressor summary</b>: Existing machine unlearning methods fail to effectively remove the effects of data poisoning on deep learning models, highlighting the need for more rigorous evaluation metrics and provable guarantees.</p><hr><h3>Detecting Frames in News Headlines and Lead Images in U.S. Gun Violence  Coverage</h3>
<p><a href='http://arxiv.org/abs/2406.17213v1'>http://arxiv.org/abs/2406.17213v1</a></p>
<p><b>Compressor summary</b>: The study combines article text and image features to identify news frames, finding that relevant images improve frame prediction and concreteness affects image usefulness.</p><hr><h3>Contrastive General Graph Matching with Adaptive Augmentation Sampling</h3>
<p><a href='http://arxiv.org/abs/2406.17199v1'>http://arxiv.org/abs/2406.17199v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new method called Graph-centric Contrastive framework for Graph Matching (GCGM) that uses graph augmentations for contrastive learning without side information and outperforms existing self-supervised methods in pattern recognition tasks.</p><hr><h3>Geometric Median (GM) Matching for Robust Data Pruning</h3>
<p><a href='http://arxiv.org/abs/2406.17188v1'>http://arxiv.org/abs/2406.17188v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new algorithm, Geometric Median Matching, for selecting informative subsets from large noisy datasets to train deep learning models efficiently and robustly.</p>