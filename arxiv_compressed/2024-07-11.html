
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-07-11</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-07-11 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning</h3>
<p><a href='http://arxiv.org/abs/2407.07094v1'>http://arxiv.org/abs/2407.07094v1</a></p>
<p><b>Compressor summary</b>: AnyTaskTune is a fine-tuning method for large language models that improves performance on diverse domain-specific tasks by defining and enhancing targeted sub-tasks with specialized datasets.</p><hr><h3>FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive  Distillation</h3>
<p><a href='http://arxiv.org/abs/2407.07093v1'>http://arxiv.org/abs/2407.07093v1</a></p>
<p><b>Compressor summary</b>: The paper introduces FBI-LLM, a large-scale binary language model that matches the performance of full-precision models and could enable specialized hardware for 1-bit LLMs.</p><hr><h3>V-VIPE: Variational View Invariant Pose Embedding</h3>
<p><a href='http://arxiv.org/abs/2407.07092v1'>http://arxiv.org/abs/2407.07092v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method (V-VIPE) to represent 3D human pose in canonical coordinate space using a variational autoencoder, enabling various downstream tasks like retrieval and classification.</p><hr><h3>Fine-Tuning Linear Layers Only Is a Simple yet Effective Way for Task  Arithmetic</h3>
<p><a href='http://arxiv.org/abs/2407.07089v1'>http://arxiv.org/abs/2407.07089v1</a></p>
<p><b>Compressor summary</b>: Task arithmetic improves model efficiency by fine-tuning only linear layers, enhancing weight disentanglement and understanding the roles of representation and task-specific models.</p><hr><h3>Safe and Reliable Training of Learning-Based Aerospace Controllers</h3>
<p><a href='http://arxiv.org/abs/2407.07088v1'>http://arxiv.org/abs/2407.07088v1</a></p>
<p><b>Compressor summary</b>: The paper presents new methods to train and verify deep reinforcement learning controllers for safety-critical domains, using k-induction, neural Lyapunov Barrier certificates, and reachability-based approaches.</p><hr><h3>CopyBench: Measuring Literal and Non-Literal Reproduction of  Copyright-Protected Text in Language Model Generation</h3>
<p><a href='http://arxiv.org/abs/2407.07087v1'>http://arxiv.org/abs/2407.07087v1</a></p>
<p><b>Compressor summary</b>: CopyBench is a benchmark to measure literal and non-literal copying in language models using copyrighted fiction books, showing that larger models have more copying issues.</p><hr><h3>Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks  with Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.07086v1'>http://arxiv.org/abs/2407.07086v1</a></p>
<p><b>Compressor summary</b>: The Hypothetical Minds agent uses a cognitively-inspired architecture with a Theory of Mind module to generate and refine hypotheses about other agents' strategies, improving performance in multi-agent reinforcement learning tasks.</p><hr><h3>Can Learned Optimization Make Reinforcement Learning Less Difficult?</h3>
<p><a href='http://arxiv.org/abs/2407.07082v1'>http://arxiv.org/abs/2407.07082v1</a></p>
<p><b>Compressor summary</b>: The paper proposes OPEN, a method that meta-learns an update rule to improve reinforcement learning by addressing its non-stationarity, plasticity loss, and exploration needs.</p><hr><h3>Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary  and Instruction Capabilities</h3>
<p><a href='http://arxiv.org/abs/2407.07080v1'>http://arxiv.org/abs/2407.07080v1</a></p>
<p><b>Compressor summary</b>: The paper introduces two Hebrew language models, DictaLM2.0 and DictaLM2.0-Instruct, trained on a large corpus of Hebrew and English data, and presents a new benchmark suite to evaluate them on various tasks.</p><hr><h3>MoSt-DSA: Modeling Motion and Structural Interactions for Direct  Multi-Frame Interpolation in DSA Images</h3>
<p><a href='http://arxiv.org/abs/2407.07078v1'>http://arxiv.org/abs/2407.07078v1</a></p>
<p><b>Compressor summary</b>: The text introduces MoSt-DSA, a deep learning method for interpolating DSA images that reduces radiation dose by using AI instead of image count and achieves state-of-the-art performance in various aspects.</p><hr><h3>ConceptExpress: Harnessing Diffusion Models for Single-image  Unsupervised Concept Extraction</h3>
<p><a href='http://arxiv.org/abs/2407.07077v1'>http://arxiv.org/abs/2407.07077v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Unsupervised Concept Extraction, a novel task of learning multiple concepts from a single image without human annotations, using pretrained diffusion models.</p><hr><h3>Lookback Lens: Detecting and Mitigating Contextual Hallucinations in  Large Language Models Using Only Attention Maps</h3>
<p><a href='http://arxiv.org/abs/2407.07071v1'>http://arxiv.org/abs/2407.07071v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a simple model to detect contextual hallucinations in LLMs by using the ratio of attention weights on the context versus newly generated tokens, and shows that it can reduce hallucinations in various tasks and models.</p><hr><h3>Explainable Hyperdimensional Computing for Balancing Privacy and  Transparency in Additive Manufacturing Monitoring</h3>
<p><a href='http://arxiv.org/abs/2407.07066v1'>http://arxiv.org/abs/2407.07066v1</a></p>
<p><b>Compressor summary</b>: The study proposes a framework that uses differential privacy and hyperdimensional computing to monitor additive manufacturing processes while protecting sensitive data and maintaining accuracy.</p><hr><h3>Internet of Agents: Weaving a Web of Heterogeneous Agents for  Collaborative Intelligence</h3>
<p><a href='http://arxiv.org/abs/2407.07061v1'>http://arxiv.org/abs/2407.07061v1</a></p>
<p><b>Compressor summary</b>: The Internet of Agents (IoA) is a novel framework that enables effective collaboration among diverse, LLM-based agents by providing a flexible and scalable platform with instant messaging-like architecture, agent integration protocol, and dynamic mechanisms for teaming and conversation flow control.</p><hr><h3>CAPformer: Compression-Aware Pre-trained Transformer for Low-Light Image  Enhancement</h3>
<p><a href='http://arxiv.org/abs/2407.07056v1'>http://arxiv.org/abs/2407.07056v1</a></p>
<p><b>Compressor summary</b>: The study presents CAPformer, a method that learns to enhance low-light images while considering JPEG compression effects and using Brightness-Guided Self-Attention.</p><hr><h3>Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning  Instruction Using Language Model</h3>
<p><a href='http://arxiv.org/abs/2407.07053v1'>http://arxiv.org/abs/2407.07053v1</a></p>
<p><b>Compressor summary</b>: The authors create a new benchmark to test LMMs on abstract image understanding, spatial relations reasoning, and visual element induction using synthetic data generated by language models.</p><hr><h3>CorMulT: A Semi-supervised Modality Correlation-aware Multimodal  Transformer for Sentiment Analysis</h3>
<p><a href='http://arxiv.org/abs/2407.07046v1'>http://arxiv.org/abs/2407.07046v1</a></p>
<p><b>Compressor summary</b>: CorMulT is a two-stage semi-supervised model that learns modality correlations and uses them to improve multimodal sentiment analysis performance.</p><hr><h3>Simple and Interpretable Probabilistic Classifiers for Knowledge Graphs</h3>
<p><a href='http://arxiv.org/abs/2407.07045v1'>http://arxiv.org/abs/2407.07045v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a simple probabilistic model for learning classifiers from incomplete data in Knowledge Graphs, which can be converted into axioms and initialized with expert knowledge.</p><hr><h3>ProtoSAM - One Shot Medical Image Segmentation With Foundational Models</h3>
<p><a href='http://arxiv.org/abs/2407.07042v1'>http://arxiv.org/abs/2407.07042v1</a></p>
<p><b>Compressor summary</b>: ProtoSAM is a one-shot medical image segmentation framework that combines prototypical networks and SAM, achieving state-of-the-art results on several datasets.</p><hr><h3>Hiding Local Manipulations on SAR Images: a Counter-Forensic Attack</h3>
<p><a href='http://arxiv.org/abs/2407.07041v1'>http://arxiv.org/abs/2407.07041v1</a></p>
<p><b>Compressor summary</b>: The paper shows how an expert can use a complex method to hide tampering in SAR images and fool forensic detectors.</p><hr><h3>Decoding Climate Disagreement: A Graph Neural Network-Based Approach to  Understanding Social Media Dynamics</h3>
<p><a href='http://arxiv.org/abs/2407.07038v1'>http://arxiv.org/abs/2407.07038v1</a></p>
<p><b>Compressor summary</b>: The ClimateSent-GAT Model uses Graph Attention Networks to classify disagreements in Reddit comment-reply pairs about climate change, improving on existing methods and helping communicate better.</p><hr><h3>Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era  of Foundation Models</h3>
<p><a href='http://arxiv.org/abs/2407.07035v1'>http://arxiv.org/abs/2407.07035v1</a></p>
<p><b>Compressor summary</b>: This survey reviews Vision-and-Language Navigation (VLN) methods and future opportunities using a principled framework for embodied planning and reasoning, with a focus on the role of foundation models.</p><hr><h3>Trajectory Data Mining and Trip Travel Time Prediction on Specific Roads</h3>
<p><a href='http://arxiv.org/abs/2407.07030v1'>http://arxiv.org/abs/2407.07030v1</a></p>
<p><b>Compressor summary</b>: The text describes a pipeline for mining trajectories from sensors data and using various machine learning approaches to predict travel time on common routes in Islamabad, Pakistan.</p><hr><h3>Resolving Sentiment Discrepancy for Multimodal Sentiment Detection via  Semantics Completion and Decomposition</h3>
<p><a href='http://arxiv.org/abs/2407.07026v1'>http://arxiv.org/abs/2407.07026v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a CoDe network that complements image and text representations with OCR text semantics, decomposes them with projection and contrastive learning, and fuses them for multimodal sentiment analysis to address sentiments discrepancy.</p><hr><h3>Exploring Scalability of Self-Training for Open-Vocabulary Temporal  Action Localization</h3>
<p><a href='http://arxiv.org/abs/2407.07024v1'>http://arxiv.org/abs/2407.07024v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-training method using unlabeled YouTube videos to improve open-vocabulary temporal action localization, and introduces a new evaluation protocol.</p><hr><h3>Less is More: Efficient Brain-Inspired Learning for Autonomous Driving  Trajectory Prediction</h3>
<p><a href='http://arxiv.org/abs/2407.07020v1'>http://arxiv.org/abs/2407.07020v1</a></p>
<p><b>Compressor summary</b>: The Human-Like Trajectory Prediction model (HLTP++) improves autonomous driving by mimicking human cognitive processes and using a novel teacher-student knowledge distillation framework with a new efficient neural network, achieving better trajectory prediction than existing models in various scenarios.</p><hr><h3>End-To-End Causal Effect Estimation from Unstructured Natural Language  Data</h3>
<p><a href='http://arxiv.org/abs/2407.07018v1'>http://arxiv.org/abs/2407.07018v1</a></p>
<p><b>Compressor summary</b>: The paper introduces NATURAL, a family of causal effect estimators using large language models, which can efficiently estimate causal effects from unstructured text data.</p><hr><h3>Induction Heads as an Essential Mechanism for Pattern Matching in  In-context Learning</h3>
<p><a href='http://arxiv.org/abs/2407.07011v1'>http://arxiv.org/abs/2407.07011v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how induction heads contribute to learning and performing tasks using few examples (few-shot learning) and shows their importance for abstract pattern recognition and natural language processing tasks.</p><hr><h3>Explainable AI for Enhancing Efficiency of DL-based Channel Estimation</h3>
<p><a href='http://arxiv.org/abs/2407.07009v1'>http://arxiv.org/abs/2407.07009v1</a></p>
<p><b>Compressor summary</b>: The text describes a novel AI-based decision-making framework for wireless communications that uses perturbation to identify relevant inputs and improve performance and trustworthiness.</p><hr><h3>Empirical analysis of Biding Precedent efficiency in the Brazilian  Supreme Court via Similar Case Retrieval</h3>
<p><a href='http://arxiv.org/abs/2407.07004v1'>http://arxiv.org/abs/2407.07004v1</a></p>
<p><b>Compressor summary</b>: The text analyzes the impact of five Brazilian legal binding precedents on the Federal Supreme Court's rulings and compares different methods of natural language processing for similar case retrieval, finding that the reasons for their ineffectiveness are varied and case-dependent.</p><hr><h3>Learning to Complement and to Defer to Multiple Users</h3>
<p><a href='http://arxiv.org/abs/2407.07003v1'>http://arxiv.org/abs/2407.07003v1</a></p>
<p><b>Compressor summary</b>: LECODU is a novel method for integrating AI and humans in classification tasks that optimizes accuracy and collaboration costs by combining learning to complement and defer strategies with estimating the optimal number of users.</p><hr><h3>Metron: Holistic Performance Evaluation Framework for LLM Inference  Systems</h3>
<p><a href='http://arxiv.org/abs/2407.07000v1'>http://arxiv.org/abs/2407.07000v1</a></p>
<p><b>Compressor summary</b>: Metron is a new framework that evaluates large language models' performance for real-time applications by considering fluidity-index, a metric that reflects the LLM inference process impact on user experience.</p><hr><h3>Improved Block Merging for 3D Point Cloud Instance Segmentation</h3>
<p><a href='http://arxiv.org/abs/2407.06991v1'>http://arxiv.org/abs/2407.06991v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new 3D instance segmentation method that corrects labelled points in processed blocks using label propagation and improves accuracy without requiring overlap between blocks.</p><hr><h3>Segment-Based Interactive Machine Translation for Pre-trained Models</h3>
<p><a href='http://arxiv.org/abs/2407.06990v1'>http://arxiv.org/abs/2407.06990v1</a></p>
<p><b>Compressor summary</b>: The study explores using large language models mBART and mT5 in interactive machine translation, finding that mBART performs similarly to state-of-the-art models.</p><hr><h3>PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and  Tuning Methods</h3>
<p><a href='http://arxiv.org/abs/2407.06985v1'>http://arxiv.org/abs/2407.06985v1</a></p>
<p><b>Compressor summary</b>: The paper introduces PEER, a multi-agent framework for domain-specific problem-solving that integrates question decomposition, information retrieval, summarization, and self-assistant, achieving high performance with lower cost and better data privacy than GPT-4.</p><hr><h3>Category-level Object Detection, Pose Estimation and Reconstruction from  Stereo Images</h3>
<p><a href='http://arxiv.org/abs/2407.06984v1'>http://arxiv.org/abs/2407.06984v1</a></p>
<p><b>Compressor summary</b>: CODERS is a one-stage method for 3D object understanding from stereo images that improves robot manipulation by detecting objects, estimating their pose, and reconstructing them with an implicit stereo matching module and a transform-decoder architecture.</p><hr><h3>Can virtual staining for high-throughput screening generalize?</h3>
<p><a href='http://arxiv.org/abs/2407.06979v1'>http://arxiv.org/abs/2407.06979v1</a></p>
<p><b>Compressor summary</b>: This study investigates how well virtual staining models trained on imaging data from three cell types and two conditions in drug screening can generalize to other scenarios, finding that non-toxic condition training improves performance and there is variability in generalization across cell types.</p><hr><h3>Parameter-Efficient and Memory-Efficient Tuning for Vision Transformer:  A Disentangled Approach</h3>
<p><a href='http://arxiv.org/abs/2407.06964v1'>http://arxiv.org/abs/2407.06964v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a lightweight and efficient method to adapt pre-trained Vision Transformers for downstream tasks by using a query module and a customized classification head that avoids heavy intermediate features and memory-heavy training.</p><hr><h3>Joint prototype and coefficient prediction for 3D instance segmentation</h3>
<p><a href='http://arxiv.org/abs/2407.06958v1'>http://arxiv.org/abs/2407.06958v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new 3D instance segmentation method that learns coefficients and prototypes, produces overcomplete predictions, and achieves faster and more reliable performance than existing methods.</p><hr><h3>Spanish TrOCR: Leveraging Transfer Learning for Language Adaptation</h3>
<p><a href='http://arxiv.org/abs/2407.06950v1'>http://arxiv.org/abs/2407.06950v1</a></p>
<p><b>Compressor summary</b>: The study adapts a state-of-the-art English OCR model, TrOCR, to Spanish using two methods and creates a resource-efficient pipeline for generating OCR datasets in any language.</p><hr><h3>Self-Recognition in Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.06946v1'>http://arxiv.org/abs/2407.06946v1</a></p>
<p><b>Compressor summary</b>: The authors propose a test to check if language models can recognize themselves using security questions and find no evidence of self-recognition in any of the examined models.</p><hr><h3>Raply: A profanity-mitigated rap generator</h3>
<p><a href='http://arxiv.org/abs/2407.06941v1'>http://arxiv.org/abs/2407.06941v1</a></p>
<p><b>Compressor summary</b>: Raply is a GPT-2 model that generates rap lyrics with rhymes and less offensive content by using a dataset without profanities.</p><hr><h3>RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2407.06938v1'>http://arxiv.org/abs/2407.06938v1</a></p>
<p><b>Compressor summary</b>: RodinHD is a method that creates realistic 3D avatars from portraits by addressing challenges like hairstyles, sharp details, and texture cues using novel data scheduling and cross-attention techniques.</p><hr><h3>HumanRefiner: Benchmarking Abnormal Human Generation and Refining with  Coarse-to-fine Pose-Reversible Guidance</h3>
<p><a href='http://arxiv.org/abs/2407.06937v1'>http://arxiv.org/abs/2407.06937v1</a></p>
<p><b>Compressor summary</b>: AbHuman is a large benchmark for synthesized human images with anomalies, and HumanRefiner is a plug-and-play approach to improve text-to-image generation by refining human anomalies.</p><hr><h3>Integrating Ontology Design with the CRISP-DM in the context of  Cyber-Physical Systems Maintenance</h3>
<p><a href='http://arxiv.org/abs/2407.06930v1'>http://arxiv.org/abs/2407.06930v1</a></p>
<p><b>Compressor summary</b>: The text introduces a method that combines expert-driven ontology design with CRISP-DM data mining process to build and update application-specific ontologies for corrective maintenance of Cyber-Physical Systems, using an anomaly detection case study as an example.</p><hr><h3>Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in  Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.06917v1'>http://arxiv.org/abs/2407.06917v1</a></p>
<p><b>Compressor summary</b>: The study introduces GlobalBias, a dataset to analyze how large language models propagate harmful stereotypes across various gender-by-ethnicity groups and find that larger models have higher levels of biased outputs.</p><hr><h3>Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion  Representation of Religion in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.06908v1'>http://arxiv.org/abs/2407.06908v1</a></p>
<p><b>Compressor summary</b>: The text discusses how emotions reveal our values and guide our actions, and explores how different religions are represented in LLMs, finding that some are more nuanced while others are stereotyped or stigmatized due to cultural bias and lack of NLP literature on religion.</p><hr><h3>Hypergraph based Understanding for Document Semantic Entity Recognition</h3>
<p><a href='http://arxiv.org/abs/2407.06904v1'>http://arxiv.org/abs/2407.06904v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Semantic entity recognition is a task to identify semantic types of text in documents.
- Existing models focus on entity categories but ignore entity boundaries.
- HGA is a novel framework that uses hypergraph attention to capture both entity boundaries and categories.
- HGALayoutLM is a new model based on HGA and GraphLayoutLM that achieves state-of-the-art results on several datasets.

Summary:
The paper proposes HGA, a hypergraph attention framework for semantic entity recognition that improves performance by focusing on both entity boundaries and categories, and HGALayoutLM, a new model based on it that sets new state-of-the-art results on several datasets.</p><hr><h3>Measuring Sustainability Intention of ESG Fund Disclosure using Few-Shot  Learning</h3>
<p><a href='http://arxiv.org/abs/2407.06893v1'>http://arxiv.org/abs/2407.06893v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method and system to classify and score sustainable funds' prospectuses based on their language specificity and transparency, using few-shot learners and a ratio metric, to help regulators, investors, and advisors assess ESG claims.</p><hr><h3>A Complete Set of Quadratic Constraints For Repeated ReLU</h3>
<p><a href='http://arxiv.org/abs/2407.06888v1'>http://arxiv.org/abs/2407.06888v1</a></p>
<p><b>Compressor summary</b>: The paper presents a complete set of quadratic constraints for the repeated ReLU that bounds its performance and stability in neural networks, including a less conservative Lipschitz bound compared to the standard approach.</p><hr><h3>Aligning Cyber Space with Physical World: A Comprehensive Survey on  Embodied AI</h3>
<p><a href='http://arxiv.org/abs/2407.06886v1'>http://arxiv.org/abs/2407.06886v1</a></p>
<p><b>Compressor summary</b>: This paper surveys recent advancements in Embodied AI, focusing on perception, interaction, embodied agents, and sim-to-real adaptation using Multi-modal Large Models (MLMs) and World Models (WMs).</p><hr><h3>Rethinking Image-to-Video Adaptation: An Object-centric Perspective</h3>
<p><a href='http://arxiv.org/abs/2407.06871v1'>http://arxiv.org/abs/2407.06871v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new image-to-video adaptation method that uses object discovery and slot attention to compress videos into object-centric tokens, enabling efficient temporal reasoning for video tasks with fewer parameters and better performance.</p><hr><h3>ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context</h3>
<p><a href='http://arxiv.org/abs/2407.06866v1'>http://arxiv.org/abs/2407.06866v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how user context affects GPT-3.5's refusal guardrails and finds biases based on demographics, identity, and political ideology.</p><hr><h3>Beyond Aesthetics: Cultural Competence in Text-to-Image Models</h3>
<p><a href='http://arxiv.org/abs/2407.06863v1'>http://arxiv.org/abs/2407.06863v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The text introduces a framework to evaluate cultural competence of Text-to-Image (T2I) models using structured knowledge bases and large language models
- The framework builds CUBE, a benchmark with cultural artifacts from 8 countries across 3 concepts: cuisine, landmarks, and art
- The evaluation reveals significant gaps in cultural awareness of existing T2I models and provides insights into their cultural diversity

Summary:
The text presents a framework to assess how well Text-to-Image models represent different cultures using a new benchmark called CUBE, which covers 8 countries and 3 concepts.</p><hr><h3>Window-to-Window BEV Representation Learning for Limited FoV Cross-View  Geo-localization</h3>
<p><a href='http://arxiv.org/abs/2407.06861v1'>http://arxiv.org/abs/2407.06861v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a novel method (W2W-BEV) for cross-view geo-localization that learns a bird's eye view (BEV) representation from the ground query image
- The method adaptively matches BEV features to ground windows using context-aware window matching strategy and cross-attention
- The method improves the accuracy under challenging conditions of unknown orientation and limited field of view

Summary:
The paper introduces W2W-BEV, a new method for cross-view geo-localization that learns a BEV representation from the ground image by matching BEV features to ground windows with context-aware strategy and attention, achieving better results under difficult conditions.</p><hr><h3>TE-SSL: Time and Event-aware Self Supervised Learning for Alzheimer's  Disease Progression Analysis</h3>
<p><a href='http://arxiv.org/abs/2407.06852v1'>http://arxiv.org/abs/2407.06852v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new framework, TE-SSL, that uses time-to-event and event data as supervisory signals to improve disease progression analysis using deep learning and representation learning strategies.</p><hr><h3>Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders</h3>
<p><a href='http://arxiv.org/abs/2407.06851v1'>http://arxiv.org/abs/2407.06851v1</a></p>
<p><b>Compressor summary</b>: This paper explores using sentence encoders to detect and classify unsafe prompts for Large Language Models, introducing new datasets and a metric to measure their effectiveness.</p><hr><h3>TeVAE: A Variational Autoencoder Approach for Discrete Online Anomaly  Detection in Variable-state Multivariate Time-series Data</h3>
<p><a href='http://arxiv.org/abs/2407.06849v1'>http://arxiv.org/abs/2407.06849v1</a></p>
<p><b>Compressor summary</b>: The paper presents TeVAE, an automatic online anomaly detection system for complex real-world data that can minimize false positives and detect root causes.</p><hr><h3>Dynamic Correlation Learning and Regularization for Multi-Label  Confidence Calibration</h3>
<p><a href='http://arxiv.org/abs/2407.06844v1'>http://arxiv.org/abs/2407.06844v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new task and algorithm for calibrating confidence scores in multi-label recognition problems, addressing semantic confusion and category correlations using dynamic correlation learning and regularization.</p><hr><h3>Chat-Edit-3D: Interactive 3D Scene Editing via Text Prompts</h3>
<p><a href='http://arxiv.org/abs/2407.06842v1'>http://arxiv.org/abs/2407.06842v1</a></p>
<p><b>Compressor summary</b>: The paper presents CE3D, a dialogue-based 3D scene editing approach that uses a large language model to interpret user input and autonomously invokes visual expert models, while also enabling flexible integration of existing visual models using Hash-Atlas.</p><hr><h3>HTD-Mamba: Efficient Hyperspectral Target Detection with Pyramid State  Space Model</h3>
<p><a href='http://arxiv.org/abs/2407.06841v1'>http://arxiv.org/abs/2407.06841v1</a></p>
<p><b>Compressor summary</b>: The paper introduces HTD-Mamba, a self-supervised method for hyperspectral target detection that uses spectrally contrastive learning and spatial-encoded spectral augmentation to address challenges caused by limited prior knowledge and spectral variations.</p><hr><h3>VRDSynth: Synthesizing Programs for Multilingual Visually Rich Document  Information Extraction</h3>
<p><a href='http://arxiv.org/abs/2407.06826v1'>http://arxiv.org/abs/2407.06826v1</a></p>
<p><b>Compressor summary</b>: VRDSynth is a program synthesis method that automatically extracts entity relations from multilingual visually rich documents using a domain-specific language, outperforming pre-trained models in multiple languages and reducing memory footprint.</p><hr><h3>Cue Point Estimation using Object Detection</h3>
<p><a href='http://arxiv.org/abs/2407.06823v1'>http://arxiv.org/abs/2407.06823v1</a></p>
<p><b>Compressor summary</b>: Key points:
- A novel method for automatic cue point estimation in music mixing
- Based on a pre-trained object detection transformer fine-tuned on a large cue point dataset
- Does not require low-level musical information analysis and adheres to high-level music structure

Summary:
The authors present a new computer vision method for finding cue points in music, which uses a pre-trained transformer and a large annotated dataset, and works well with dance music structure.</p><hr><h3>AstroSpy: On detecting Fake Images in Astronomy via Joint Image-Spectral  Representations</h3>
<p><a href='http://arxiv.org/abs/2407.06817v1'>http://arxiv.org/abs/2407.06817v1</a></p>
<p><b>Compressor summary</b>: AstroSpy is a hybrid model that uses spatial and spectral information to identify real and fake astronomical images, improving authenticity detection in the field of astronomy.</p><hr><h3>Historical Review of Variants of Informal Semantics for Logic Programs  under Answer Set Semantics: GL'88, GL'91, GK'14, D-V'12</h3>
<p><a href='http://arxiv.org/abs/2407.06814v1'>http://arxiv.org/abs/2407.06814v1</a></p>
<p><b>Compressor summary</b>: The note discusses the history of informal semantics for logic programming using answer set semantics, comparing two popular paradigms: Answer Set Programming and ASP-Prolog.</p><hr><h3>Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy</h3>
<p><a href='http://arxiv.org/abs/2407.06813v1'>http://arxiv.org/abs/2407.06813v1</a></p>
<p><b>Compressor summary</b>: The authors aim to create an AI agent that can excel at diplomacy by combining strategic planning, social reasoning, and self-improvement through self-play games.</p><hr><h3>ED-VAE: Entropy Decomposition of ELBO in Variational Autoencoders</h3>
<p><a href='http://arxiv.org/abs/2407.06797v1'>http://arxiv.org/abs/2407.06797v1</a></p>
<p><b>Compressor summary</b>: The Entropy Decomposed Variational Autoencoder (ED-VAE) is a new method that improves the quality of samples and latent representations by explicitly including entropy and cross-entropy components in the ELBO formulation.</p><hr><h3>Countermeasures Against Adversarial Examples in Radio Signal  Classification</h3>
<p><a href='http://arxiv.org/abs/2407.06796v1'>http://arxiv.org/abs/2407.06796v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to defend wireless networks against adversarial attacks in modulation classification using neural rejection, label smoothing, and noise injection.</p><hr><h3>CycleSAM: One-Shot Surgical Scene Segmentation using Cycle-Consistent  Feature Matching to Prompt SAM</h3>
<p><a href='http://arxiv.org/abs/2407.06795v1'>http://arxiv.org/abs/2407.06795v1</a></p>
<p><b>Compressor summary</b>: CycleSAM is a method that improves one-shot surgical scene segmentation by using trained image-mask pairs, spatial cycle-consistency constraints, and a surgical-specific ResNet50 encoder to overcome limitations of the Segment-Anything Model.</p><hr><h3>ERQ: Error Reduction for Post-Training Quantization of Vision  Transformers</h3>
<p><a href='http://arxiv.org/abs/2407.06794v1'>http://arxiv.org/abs/2407.06794v1</a></p>
<p><b>Compressor summary</b>: ERQ is a new method that reduces quantization error in vision transformers by strategically updating weights and activations with full-precision, achieving better compression efficiency than existing methods.</p><hr><h3>Fuzzy color model and clustering algorithm for color clustering problem</h3>
<p><a href='http://arxiv.org/abs/2407.06782v1'>http://arxiv.org/abs/2407.06782v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a fuzzy color model and a novel fuzzy clustering algorithm to efficiently cluster arbitrary color data with uncertainty and vagueness.</p><hr><h3>CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient  Object Detection</h3>
<p><a href='http://arxiv.org/abs/2407.06780v1'>http://arxiv.org/abs/2407.06780v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method called Conditional Dropout and Language-driven Quality Assessment to improve dual-modal salient object detection by handling noisy inputs and missing modalities, which leads to better performance than existing models.</p><hr><h3>Using Pretrained Large Language Model with Prompt Engineering to Answer  Biomedical Questions</h3>
<p><a href='http://arxiv.org/abs/2407.06779v1'>http://arxiv.org/abs/2407.06779v1</a></p>
<p><b>Compressor summary</b>: The authors describe their system for answering biomedical questions using pre-trained LLMs, prompt engineering, and post-processing techniques, achieving competitive scores on BioASQ 2024 tasks.</p><hr><h3>A new validity measure for fuzzy c-means clustering</h3>
<p><a href='http://arxiv.org/abs/2407.06774v1'>http://arxiv.org/abs/2407.06774v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new way to measure how well fuzzy clusters are separated, using the overlap between them, and shows it works well on some examples.</p><hr><h3>Temporal Convolution Derived Multi-Layered Reservoir Computing</h3>
<p><a href='http://arxiv.org/abs/2407.06771v1'>http://arxiv.org/abs/2407.06771v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new reservoir computing method with improved input mapping and network architectures that reduce error and uncertainty in predicting chaotic and non-chaotic time series compared to existing methods.</p><hr><h3>A Generalization Bound for Nearly-Linear Networks</h3>
<p><a href='http://arxiv.org/abs/2407.06765v1'>http://arxiv.org/abs/2407.06765v1</a></p>
<p><b>Compressor summary</b>: The authors propose new generalization bounds for nonlinear networks that consider them as perturbations of linear ones and require no training data to evaluate.</p><hr><h3>Explicit Modelling of Theory of Mind for Belief Prediction in Nonverbal  Social Interactions</h3>
<p><a href='http://arxiv.org/abs/2407.06762v1'>http://arxiv.org/abs/2407.06762v1</a></p>
<p><b>Compressor summary</b>: MToMnet is a neural network that predicts human beliefs and their changes during interactions using multiple inputs like videos, gaze, and body language.</p><hr><h3>Frequency and Generalisation of Periodic Activation Functions in  Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2407.06756v1'>http://arxiv.org/abs/2407.06756v1</a></p>
<p><b>Compressor summary</b>: The paper examines why periodic activation functions improve sample efficiency in deep RL and finds they learn high frequency representations, but have worse generalization on noisy states and can be mitigated by weight decay regularization.</p><hr><h3>iASiS: Towards Heterogeneous Big Data Analysis for Personalized Medicine</h3>
<p><a href='http://arxiv.org/abs/2407.06748v1'>http://arxiv.org/abs/2407.06748v1</a></p>
<p><b>Compressor summary</b>: The IASIS project aims to turn big biomedical data into actionable information for decision makers by integrating and analyzing data from various sources using advanced methods and generating insights for public health activities and personalized care.</p><hr><h3>Positive-Unlabelled Learning for Improving Image-based Recommender  System Explainability</h3>
<p><a href='http://arxiv.org/abs/2407.06740v1'>http://arxiv.org/abs/2407.06740v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new way to train image-based explainer for recommender systems using positive-unlabelled learning to improve explainability with user-personalized negative examples.</p><hr><h3>LVLM-empowered Multi-modal Representation Learning for Visual Place  Recognition</h3>
<p><a href='http://arxiv.org/abs/2407.06730v1'>http://arxiv.org/abs/2407.06730v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new VPR method that fuses image and text features using attention mechanisms, improving robustness against viewpoint and appearance changes.</p><hr><h3>Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting  Region Captions</h3>
<p><a href='http://arxiv.org/abs/2407.06723v1'>http://arxiv.org/abs/2407.06723v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new annotation method for image captioning using labelled graphs to describe scenes with compositionality and hierarchical information, improving performance on downstream models.</p><hr><h3>A Simple Architecture for Enterprise Large Language Model Applications  based on Role based security and Clearance Levels using Retrieval-Augmented  Generation or Mixture of Experts</h3>
<p><a href='http://arxiv.org/abs/2407.06718v1'>http://arxiv.org/abs/2407.06718v1</a></p>
<p><b>Compressor summary</b>: The study presents a simple architecture for secure Enterprise applications using LLMs, RAG, and MoE to filter documents and experts based on user roles and security clearance levels.</p><hr><h3>Improving the Transferability of Adversarial Examples by Feature  Augmentation</h3>
<p><a href='http://arxiv.org/abs/2407.06714v1'>http://arxiv.org/abs/2407.06714v1</a></p>
<p><b>Compressor summary</b>: The paper proposes FAUG, a feature augmentation attack that improves adversarial transferability by injecting random noise into model intermediate features without extra computation costs.</p><hr><h3>MDP Geometry, Normalization and Value Free Solvers</h3>
<p><a href='http://arxiv.org/abs/2407.06712v1'>http://arxiv.org/abs/2407.06712v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a geometric approach to analyze MDP algorithms and shows how to split them into classes with similar dynamics, enabling the creation of new optimal policy-finding methods.</p><hr><h3>Top-K Pairwise Ranking: Bridging the Gap Among Ranking-Based Measures  for Multi-Label Classification</h3>
<p><a href='http://arxiv.org/abs/2407.06709v1'>http://arxiv.org/abs/2407.06709v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Top-K Pairwise Ranking (TKPR), a new measure for multi-label ranking tasks, and develops an empirical surrogate risk minimization framework with theoretical guarantees.</p><hr><h3>Self-supervised visual learning from interactions with objects</h3>
<p><a href='http://arxiv.org/abs/2407.06704v1'>http://arxiv.org/abs/2407.06704v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to enhance self-supervised learning of visual representations by incorporating actions performed on objects, leading to better recognition of object categories.</p><hr><h3>Consistent Document-Level Relation Extraction via Counterfactuals</h3>
<p><a href='http://arxiv.org/abs/2407.06699v1'>http://arxiv.org/abs/2407.06699v1</a></p>
<p><b>Compressor summary</b>: The paper presents CovEReD, a method to generate counterfactual data for document-level relation extraction models, which helps evaluate and reduce factual biases in these models.</p><hr><h3>PSPU: Enhanced Positive and Unlabeled Learning by Leveraging Pseudo  Supervision</h3>
<p><a href='http://arxiv.org/abs/2407.06698v1'>http://arxiv.org/abs/2407.06698v1</a></p>
<p><b>Compressor summary</b>: PSPU improves over PU learning by using pseudo-supervision from confident samples and a consistency loss to reduce overfitting and perform better on various datasets.</p><hr><h3>Certified Continual Learning for Neural Network Regression</h3>
<p><a href='http://arxiv.org/abs/2407.06697v1'>http://arxiv.org/abs/2407.06697v1</a></p>
<p><b>Compressor summary</b>: The paper proposes certified continual learning, an approach to preserve the verified correctness of neural networks when they are re-trained over time for different tasks.</p><hr><h3>Hierarchical Average-Reward Linearly-solvable Markov Decision Processes</h3>
<p><a href='http://arxiv.org/abs/2407.06690v1'>http://arxiv.org/abs/2407.06690v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new hierarchical reinforcement learning method for LMDPs that learns low- and high-level tasks simultaneously using state space partitions, improving average-reward performance significantly.</p><hr><h3>A Predictive Model Based on Transformer with Statistical Feature  Embedding in Manufacturing Sensor Dataset</h3>
<p><a href='http://arxiv.org/abs/2407.06682v1'>http://arxiv.org/abs/2407.06682v1</a></p>
<p><b>Compressor summary</b>: The study presents a new predictive model using Transformer and feature embedding to improve fault detection and virtual metrology in manufacturing processes with limited sensor data.</p><hr><h3>Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of  Modules</h3>
<p><a href='http://arxiv.org/abs/2407.06677v1'>http://arxiv.org/abs/2407.06677v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new Transformer architecture called mixture-of-modules (MoM) that breaks the depth-ordered convention by dynamically selecting modules to compute tokens, achieving better performance and reduced redundancy in parameterization.</p><hr><h3>Games played by Exponential Weights Algorithms</h3>
<p><a href='http://arxiv.org/abs/2407.06676v1'>http://arxiv.org/abs/2407.06676v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes how exponential weights algorithm with constant learning rates behaves in repeated games and shows convergence properties to certain Nash equilibria.</p><hr><h3>CTRL-F: Pairing Convolution with Transformer for Image Classification  via Multi-Level Feature Cross-Attention and Representation Learning Fusion</h3>
<p><a href='http://arxiv.org/abs/2407.06673v1'>http://arxiv.org/abs/2407.06673v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a hybrid network that combines convolution and transformers for image classification
- The hybrid network consists of a convolution branch and a multi-level feature cross-attention module
- The cross-attention module processes different levels of features from the convolution branch and exchanges knowledge through attention
- The paper introduces novel representation fusion techniques to fuse the local and global responses
- The proposed model achieves state-of-the-art performance on image classification tasks with limited or large data

Summary:
The paper presents CTRL-F, a hybrid network that integrates convolution and transformers for image classification. It uses multi-level feature cross-attention to learn from different feature levels and novel representation fusion techniques to combine local and global responses. CTRL-F outperforms existing models on various image classification datasets.</p><hr><h3>Collaborative Design of AI-Enhanced Learning Activities</h3>
<p><a href='http://arxiv.org/abs/2407.06660v1'>http://arxiv.org/abs/2407.06660v1</a></p>
<p><b>Compressor summary</b>: The text describes an intervention program that helps educators learn about AI and how to integrate it into their teaching practices in creative ways, considering ethical and pedagogical aspects.</p><hr><h3>TriQXNet: Forecasting Dst Index from Solar Wind Data Using an  Interpretable Parallel Classical-Quantum Framework with Uncertainty  Quantification</h3>
<p><a href='http://arxiv.org/abs/2407.06658v1'>http://arxiv.org/abs/2407.06658v1</a></p>
<p><b>Compressor summary</b>: TriQXNet is a novel hybrid classical-quantum neural network that predicts the disturbance storm-time index, helping to mitigate the impacts of geomagnetic storms on infrastructure.</p><hr><h3>Teacher agency in the age of generative AI: towards a framework of  hybrid intelligence for learning design</h3>
<p><a href='http://arxiv.org/abs/2407.06655v1'>http://arxiv.org/abs/2407.06655v1</a></p>
<p><b>Compressor summary</b>: Generative AI (genAI) affects teachers' agency in education, but hybrid intelligence combining human and artificial intelligence could enhance learning design and teacher influence.</p><hr><h3>SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language  Model Pre-training</h3>
<p><a href='http://arxiv.org/abs/2407.06654v1'>http://arxiv.org/abs/2407.06654v1</a></p>
<p><b>Compressor summary</b>: The proposed soft deduplication method reduces the sampling weight of duplicated data in large language models' pre-training datasets, improving training efficiency and downstream accuracy while preserving dataset integrity.</p><hr><h3>Toward Motion Robustness: A masked attention regularization framework in  remote photoplethysmography</h3>
<p><a href='http://arxiv.org/abs/2407.06653v1'>http://arxiv.org/abs/2407.06653v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework called MAR-rPPG that improves facial video-based remote photoplethysmography (rPPG) measurement by addressing ROI localization and motion artifacts issues with masked attention regularization and an enhanced EREA network.</p><hr><h3>A Word Order Synchronization Metric for Evaluating Simultaneous  Interpretation and Translation</h3>
<p><a href='http://arxiv.org/abs/2407.06650v1'>http://arxiv.org/abs/2407.06650v1</a></p>
<p><b>Compressor summary</b>: The authors propose an evaluation metric for simultaneous interpretation and machine translation that focuses on maintaining word order synchronization between languages, using rank correlation coefficients and cross-lingual pre-trained models.</p><hr><h3>Variational Learning ISTA</h3>
<p><a href='http://arxiv.org/abs/2407.06646v1'>http://arxiv.org/abs/2407.06646v1</a></p>
<p><b>Compressor summary</b>: The paper proposes two variants of LISTA, A-DLISTA and VLISTA, to solve compressed sensing problems with varying sensing matrices by jointly learning sparse representations and reconstructions while accounting for uncertainty in the dictionaries.</p><hr><h3>Entropy Law: The Story Behind Data Compression and LLM Performance</h3>
<p><a href='http://arxiv.org/abs/2407.06645v1'>http://arxiv.org/abs/2407.06645v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a data selection method for large language models based on an "entropy law" that connects model performance to data compression ratio and first-epoch training loss, which helps improve model learning efficiency and diversity.</p><hr><h3>Powerful and Flexible: Personalized Text-to-Image Generation via  Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2407.06642v1'>http://arxiv.org/abs/2407.06642v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a reinforcement learning framework for personalized text-to-image generation that preserves visual details and structure, outperforming existing methods.</p><hr><h3>Ensembled Cold-Diffusion Restorations for Unsupervised Anomaly Detection</h3>
<p><a href='http://arxiv.org/abs/2407.06635v1'>http://arxiv.org/abs/2407.06635v1</a></p>
<p><b>Compressor summary</b>: The text presents a new method that combines generative models and synthetic anomalies to improve unsupervised anomaly detection in medical images, such as brain MRI.</p><hr><h3>Masked Video and Body-worn IMU Autoencoder for Egocentric Action  Recognition</h3>
<p><a href='http://arxiv.org/abs/2407.06628v1'>http://arxiv.org/abs/2407.06628v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new method for action recognition that combines body-worn IMUs with egocentric videos, using self-supervised pretraining and graph-based modeling to achieve state-of-the-art performance and robustness.</p><hr><h3>Reasoning about unpredicted change and explicit time</h3>
<p><a href='http://arxiv.org/abs/2407.06622v1'>http://arxiv.org/abs/2407.06622v1</a></p>
<p><b>Compressor summary</b>: The text proposes an approach to explain time-stamped observations using simple events called surprises, which represent changes in fluents, and discusses how to minimize them.</p><hr><h3>Mobius: An High Efficient Spatial-Temporal Parallel Training Paradigm  for Text-to-Video Generation Task</h3>
<p><a href='http://arxiv.org/abs/2407.06617v1'>http://arxiv.org/abs/2407.06617v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Mobius, a parallel training paradigm for text-to-video generation that saves memory and time compared to traditional 3D-Unet.</p><hr><h3>Sparse-DeRF: Deblurred Neural Radiance Fields from Sparse View</h3>
<p><a href='http://arxiv.org/abs/2407.06613v1'>http://arxiv.org/abs/2407.06613v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Sparse-DeRF, a method to construct deblurred neural radiance fields from limited blurry images using regularization techniques that improve the quality of the results.</p><hr><h3>CEIA: CLIP-Based Event-Image Alignment for Open-World Event-Based  Understanding</h3>
<p><a href='http://arxiv.org/abs/2407.06611v1'>http://arxiv.org/abs/2407.06611v1</a></p>
<p><b>Compressor summary</b>: CEIA is a framework that learns to align event and image data using contrastive learning to overcome the lack of paired event-text data for open-world event-based understanding, achieving versatility and performance in various multi-modal applications.</p><hr><h3>Tailored Design of Audio-Visual Speech Recognition Models using  Branchformers</h3>
<p><a href='http://arxiv.org/abs/2407.06606v1'>http://arxiv.org/abs/2407.06606v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel audio-visual framework that uses Branchformer architecture to design parameter-efficient systems for speech recognition in noisy environments.</p><hr><h3>Integrating Clinical Knowledge into Concept Bottleneck Models</h3>
<p><a href='http://arxiv.org/abs/2407.06600v1'>http://arxiv.org/abs/2407.06600v1</a></p>
<p><b>Compressor summary</b>: The text describes a method to improve concept bottleneck models by integrating clinical knowledge, making them more aligned with human decision-making and better at classifying medical images in different settings.</p><hr><h3>TVR-Ranking: A Dataset for Ranked Video Moment Retrieval with Imprecise  Queries</h3>
<p><a href='http://arxiv.org/abs/2407.06597v1'>http://arxiv.org/abs/2407.06597v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Ranked Video Moment Retrieval (RVMR), a task that requires finding and ranking video moments from queries in natural language, and presents the TVR-Ranking dataset with relevance annotations for evaluating RVMR models.</p><hr><h3>D-MASTER: Mask Annealed Transformer for Unsupervised Domain Adaptation  in Breast Cancer Detection from Mammograms</h3>
<p><a href='http://arxiv.org/abs/2407.06585v1'>http://arxiv.org/abs/2407.06585v1</a></p>
<p><b>Compressor summary</b>: D-MASTER is a transformer-based framework that adapts to different domains for breast cancer detection from mammograms by masking and reconstructing multi-scale features, improving sensitivity and reducing false positives.</p><hr><h3>Vision language models are blind</h3>
<p><a href='http://arxiv.org/abs/2407.06581v1'>http://arxiv.org/abs/2407.06581v1</a></p>
<p><b>Compressor summary</b>: VLMs struggle with simple visual tasks that humans find easy, indicating a lack of fine detail perception or complete blindness in their vision.</p><hr><h3>NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in  Text Classification</h3>
<p><a href='http://arxiv.org/abs/2407.06579v1'>http://arxiv.org/abs/2407.06579v1</a></p>
<p><b>Compressor summary</b>: The paper introduces NoisyAG-News, a benchmark dataset for text classification with instance-dependent noise patterns, and shows that pre-trained language models struggle to handle such real-world noise.</p><hr><h3>Virtual Personas for Language Models via an Anthology of Backstories</h3>
<p><a href='http://arxiv.org/abs/2407.06576v1'>http://arxiv.org/abs/2407.06576v1</a></p>
<p><b>Compressor summary</b>: Anthology is a method that conditions large language models to adopt virtual personas based on life narratives, improving the representation of diverse human traits in behavioral studies.</p><hr><h3>Attack GAN (AGAN ): A new Security Evaluation Tool for Perceptual  Encryption</h3>
<p><a href='http://arxiv.org/abs/2407.06570v1'>http://arxiv.org/abs/2407.06570v1</a></p>
<p><b>Compressor summary</b>: AGAN is a new attack method that exposes vulnerabilities in perceptual encryption techniques, breaking image privacy protection.</p><hr><h3>FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal  Reinforcement for Enhanced Financial Decision Making</h3>
<p><a href='http://arxiv.org/abs/2407.06567v1'>http://arxiv.org/abs/2407.06567v1</a></p>
<p><b>Compressor summary</b>: FinCon is a large language model-based framework for enhanced financial decision-making with conceptual verbal reinforcement and a risk-control component.</p><hr><h3>Robust and Explainable Framework to Address Data Scarcity in Diagnostic  Imaging</h3>
<p><a href='http://arxiv.org/abs/2407.06566v1'>http://arxiv.org/abs/2407.06566v1</a></p>
<p><b>Compressor summary</b>: ETSEF is a novel framework that combines transfer, self-supervised, and ensemble learning with data enhancement techniques to improve automatic medical diagnostics using limited data samples.</p><hr><h3>Combining Knowledge Graphs and Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.06564v1'>http://arxiv.org/abs/2407.06564v1</a></p>
<p><b>Compressor summary</b>: The text discusses the benefits and challenges of combining natural language processing, large language models, and knowledge graphs for enhancing artificial intelligence applications.</p><hr><h3>OffsetBias: Leveraging Debiased Data for Tuning Evaluators</h3>
<p><a href='http://arxiv.org/abs/2407.06551v1'>http://arxiv.org/abs/2407.06551v1</a></p>
<p><b>Compressor summary</b>: The text describes a study that identifies six types of biases in evaluating generated responses using large language models, proposes a collection of test cases for each bias, and introduces methods to improve the robustness of these models.</p><hr><h3>Deciphering Assamese Vowel Harmony with Featural InfoWaveGAN</h3>
<p><a href='http://arxiv.org/abs/2407.06547v1'>http://arxiv.org/abs/2407.06547v1</a></p>
<p><b>Compressor summary</b>: The Featural InfoWaveGAN model can learn Assamese vowel harmony from raw speech data, capturing its complexities and showing feature learning.</p><hr><h3>Exploring the Causality of End-to-End Autonomous Driving</h3>
<p><a href='http://arxiv.org/abs/2407.06546v1'>http://arxiv.org/abs/2407.06546v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to debug and understand the factors influencing end-to-end autonomous driving models, making them more transparent and trustworthy.</p><hr><h3>Multiple Instance Verification</h3>
<p><a href='http://arxiv.org/abs/2407.06544v1'>http://arxiv.org/abs/2407.06544v1</a></p>
<p><b>Compressor summary</b>: The paper introduces cross-attention pooling (CAP), a novel approach for multiple-instance verification that uses two new attention functions to better distinguish between similar instances in a target bag, outperforming existing methods.</p><hr><h3>DriftGAN: Using historical data for Unsupervised Recurring Drift  Detection</h3>
<p><a href='http://arxiv.org/abs/2407.06543v1'>http://arxiv.org/abs/2407.06543v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes an unsupervised GAN-based method to detect concept drifts and identify their history
- The method reduces the time and data needed to retrain the model for recurring drifts
- The method outperforms state-of-the-art models and is applied to a real-world astrophysics use case

Summary:
The paper presents an unsupervised GAN method that detects concept drifts, tracks their history, and improves the model's performance for recurring drifts in less time and data, demonstrating its effectiveness on an astrophysics problem.</p><hr><h3>LIONs: An Empirically Optimized Approach to Align Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.06542v1'>http://arxiv.org/abs/2407.06542v1</a></p>
<p><b>Compressor summary</b>: The text discusses a three-stage training pipeline to improve language models' alignment, instruction-following, and conversational abilities by using various techniques and surpassing official instruct models.</p><hr><h3>General and Task-Oriented Video Segmentation</h3>
<p><a href='http://arxiv.org/abs/2407.06540v1'>http://arxiv.org/abs/2407.06540v1</a></p>
<p><b>Compressor summary</b>: GvSeg is a versatile framework for various video segmentation tasks that considers the diversity of targets and adapts to task-specific requirements, outperforming existing methods.</p><hr><h3>Enhancing Low-Resource NMT with a Multilingual Encoder and Knowledge  Distillation: A Case Study</h3>
<p><a href='http://arxiv.org/abs/2407.06538v1'>http://arxiv.org/abs/2407.06538v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a framework that combines a multilingual encoder-based seq2seq model with knowledge distillation to improve translation for low-resource Indic languages not supported by mBART-50.</p><hr><h3>Efficient and Accurate Memorable Conversation Model using DPO based on  sLLM</h3>
<p><a href='http://arxiv.org/abs/2407.06537v1'>http://arxiv.org/abs/2407.06537v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an efficient and accurate conversation model for multi-session dialog systems that uses memory management techniques to improve response generation performance and resource utilization.</p><hr><h3>LETS-C: Leveraging Language Embedding for Time Series Classification</h3>
<p><a href='http://arxiv.org/abs/2407.06533v1'>http://arxiv.org/abs/2407.06533v1</a></p>
<p><b>Compressor summary</b>: The paper proposes LETS-C, a lightweight and accurate time series classifier that uses language embeddings and a simple CNN+MLP head instead of fine-tuning large language models.</p><hr><h3>Decomposition Betters Tracking Everything Everywhere</h3>
<p><a href='http://arxiv.org/abs/2407.06531v1'>http://arxiv.org/abs/2407.06531v1</a></p>
<p><b>Compressor summary</b>: DecoMotion is a new test-time optimization method that decomposes video content into static scenes and dynamic objects, improving robustness and appearance in motion estimation.</p><hr><h3>Advanced Financial Fraud Detection Using GNN-CL Model</h3>
<p><a href='http://arxiv.org/abs/2407.06529v1'>http://arxiv.org/abs/2407.06529v1</a></p>
<p><b>Compressor summary</b>: The GNN-CL model combines graph neural networks, convolutional neural networks and long short-term memory to improve financial fraud detection accuracy by analyzing complex transaction patterns and using intelligent purification mechanisms and reinforcement learning strategies.</p><hr><h3>Graph Neural Networks and Deep Reinforcement Learning Based Resource  Allocation for V2X Communications</h3>
<p><a href='http://arxiv.org/abs/2407.06518v1'>http://arxiv.org/abs/2407.06518v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method that combines Graph Neural Networks with Deep Reinforcement Learning to efficiently allocate resources for Vehicle-to-Vehicle and Vehicle-to-Infrastructure communication in Internet of Vehicles technology.</p><hr><h3>VQA-Diff: Exploiting VQA and Diffusion for Zero-Shot Image-to-3D Vehicle  Asset Generation in Autonomous Driving</h3>
<p><a href='http://arxiv.org/abs/2407.06516v1'>http://arxiv.org/abs/2407.06516v1</a></p>
<p><b>Compressor summary</b>: VQA-Diff is a novel framework that uses real-world knowledge from large language models and image prior knowledge from diffusion models to generate photorealistic 3D vehicle assets for autonomous driving, achieving robust zero-shot prediction and appearance control.</p><hr><h3>Computer vision tasks for intelligent aerospace missions: An overview</h3>
<p><a href='http://arxiv.org/abs/2407.06513v1'>http://arxiv.org/abs/2407.06513v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Computer vision tasks are vital for aerospace missions
- Traditional methods are not robust enough for harsh space conditions
- Deep learning-based perception technologies outperform traditional methods and offer great potential
- The survey explores techniques, datasets, and strategies for DL-based aerospace perception
- The challenges and future directions of this field are discussed

Summary:
The text surveys deep learning-based computer vision techniques for aerospace missions, which overcome the limitations of traditional methods and offer great potential, but also face challenges and need further research.</p><hr><h3>LuSNAR:A Lunar Segmentation, Navigation and Reconstruction Dataset based  on Muti-sensor for Autonomous Exploration</h3>
<p><a href='http://arxiv.org/abs/2407.06512v1'>http://arxiv.org/abs/2407.06512v1</a></p>
<p><b>Compressor summary</b>: The paper introduces LuSNAR, a multi-task, multi-scene, and multi-label lunar dataset for evaluating autonomous perception and navigation systems on the moon.</p><hr><h3>Economic span selection of bridge based on deep reinforcement learning</h3>
<p><a href='http://arxiv.org/abs/2407.06507v1'>http://arxiv.org/abs/2407.06507v1</a></p>
<p><b>Compressor summary</b>: The text describes how a deep Q-network algorithm can be used to optimize the economic span of a bridge, reducing its construction cost.</p><hr><h3>Reprogramming Distillation for Medical Foundation Models</h3>
<p><a href='http://arxiv.org/abs/2407.06504v1'>http://arxiv.org/abs/2407.06504v1</a></p>
<p><b>Compressor summary</b>: Reprogramming Distillation is a novel framework that reprograms the foundation model's feature space for downstream tasks and establishes connections between the reprogrammed knowledge and student models for personalized lightweight deployment.</p><hr><h3>Preference-Guided Reinforcement Learning for Efficient Exploration</h3>
<p><a href='http://arxiv.org/abs/2407.06503v1'>http://arxiv.org/abs/2407.06503v1</a></p>
<p><b>Compressor summary</b>: LOPE is a preference-guided RL framework that improves exploration efficiency in hard-exploration tasks by using human feedback as guidance, avoiding learning a separate reward model.</p><hr><h3>STORYSUMM: Evaluating Faithfulness in Story Summarization</h3>
<p><a href='http://arxiv.org/abs/2407.06501v1'>http://arxiv.org/abs/2407.06501v1</a></p>
<p><b>Compressor summary</b>: The paper introduces STORYSUMM, a new dataset for evaluating faithfulness in summarization methods, and shows that current automatic metrics are not accurate enough for this task.</p><hr><h3>It's Our Loss: No Privacy Amplification for Hidden State DP-SGD With  Non-Convex Loss</h3>
<p><a href='http://arxiv.org/abs/2407.06496v1'>http://arxiv.org/abs/2407.06496v1</a></p>
<p><b>Compressor summary</b>: The paper shows that for some loss functions, the final iterate of DP-SGD leaks as much information as all intermediate iterates combined, and thus privacy amplification is not possible for these cases.</p><hr><h3>A Generative Approach to Control Complex Physical Systems</h3>
<p><a href='http://arxiv.org/abs/2407.06494v1'>http://arxiv.org/abs/2407.06494v1</a></p>
<p><b>Compressor summary</b>: DiffPhyCon is a novel method for controlling complex physical systems that minimizes energy and control objectives, explores globally, and can discover near-optimal control sequences, outperforming classical and deep learning approaches.</p><hr><h3>VideoEval: Comprehensive Benchmark Suite for Low-Cost Evaluation of  Video Foundation Model</h3>
<p><a href='http://arxiv.org/abs/2407.06491v1'>http://arxiv.org/abs/2407.06491v1</a></p>
<p><b>Compressor summary</b>: VideoEval is a new benchmark suite that evaluates video foundation models on task adaptability and representation power, revealing their weaknesses and potential improvements.</p><hr><h3>Towards Understanding Multi-Task Learning (Generalization) of LLMs via  Detecting and Exploring Task-Specific Neurons</h3>
<p><a href='http://arxiv.org/abs/2407.06488v1'>http://arxiv.org/abs/2407.06488v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how large language models learn multiple tasks by identifying and analyzing task-sensitive neurons, and proposes a continuous fine-tuning method based on these findings.</p><hr><h3>Optimal Decision Making Through Scenario Simulations Using Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.06486v1'>http://arxiv.org/abs/2407.06486v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a dynamic framework that integrates an optimization function within LLMs' decision-making process, allowing them to offer tailored, optimal solutions to complex problems.</p><hr><h3>CrowdTransfer: Enabling Crowd Knowledge Transfer in AIoT Community</h3>
<p><a href='http://arxiv.org/abs/2407.06485v1'>http://arxiv.org/abs/2407.06485v1</a></p>
<p><b>Compressor summary</b>: The text introduces Crowd Knowledge Transfer (CrowdTransfer), a new approach to improve Artificial Intelligence of Things (AIoT) performance by sharing prior knowledge from multiple agents, and discusses its applications and challenges.</p><hr><h3>Composable Interventions for Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.06483v1'>http://arxiv.org/abs/2407.06483v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a framework to study and compare different test-time interventions applied sequentially to language models, revealing their interactions and limitations.</p><hr><h3>Interaction Matters: An Evaluation Framework for Interactive Dialogue  Assessment on English Second Language Conversations</h3>
<p><a href='http://arxiv.org/abs/2407.06479v1'>http://arxiv.org/abs/2407.06479v1</a></p>
<p><b>Compressor summary</b>: The text introduces an evaluation framework that measures interactivity in English as a Second Language (ESL) speakers' dialogues using micro-level features and machine learning models.</p><hr><h3>Sketch-Guided Scene Image Generation</h3>
<p><a href='http://arxiv.org/abs/2407.06469v1'>http://arxiv.org/abs/2407.06469v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The study proposes a new method for generating scene images from sketch inputs using diffusion models and identity embeddings.
- The method decomposes the task into object-level generation and scene-level construction.
- The method preserves the details of the foreground objects while blending them with the background.

Summary:
The study presents a novel method that uses diffusion models and identity embeddings to generate high-quality scene images from sketch inputs by decomposing the task and preserving object details.</p><hr><h3>AnatoMask: Enhancing Medical Image Segmentation with  Reconstruction-guided Self-masking</h3>
<p><a href='http://arxiv.org/abs/2407.06468v1'>http://arxiv.org/abs/2407.06468v1</a></p>
<p><b>Compressor summary</b>: AnatoMask is a novel self-supervised learning method for 3D medical image segmentation that dynamically masks and reconstructs anatomically significant regions to improve pretraining efficiency.</p><hr><h3>SideSeeing: A multimodal dataset and collection of tools for sidewalk  assessment</h3>
<p><a href='http://arxiv.org/abs/2407.06464v1'>http://arxiv.org/abs/2407.06464v1</a></p>
<p><b>Compressor summary</b>: SideSeeing is an initiative that provides tools and datasets for assessing the built environment, using synchronized video and sensor data from chest-mounted mobile devices to evaluate sidewalk accessibility near hospitals in Brazil and the USA.</p>