
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-01-02</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-01-02 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Principled Gradient-based Markov Chain Monte Carlo for Text Generation</h3>
<p>Li Du,Afra Amini,Lucas Torroba Hennigen,Xinyan Velocity Yu,Jason Eisner,Holden Lee,Ryan Cotterell</p>
<p><a href='http://arxiv.org/abs/2312.17710v1'>http://arxiv.org/abs/2312.17710v1</a></p>
<p><b>Compressor summary</b>: The paper proposes new faithful gradient-based sampling algorithms for energy-based text generation that improve fluency and adherence to control objectives.</p><hr><h3>TuPy-E: detecting hate speech in Brazilian Portuguese social media with  a novel dataset and comprehensive analysis of models</h3>
<p>Felipe Oliveira,Victoria Reis,Nelson Ebecken</p>
<p><a href='http://arxiv.org/abs/2312.17704v1'>http://arxiv.org/abs/2312.17704v1</a></p>
<p><b>Compressor summary</b>: TuPy-E is a large annotated Portuguese corpus for hate speech detection that uses an open-source approach and advanced techniques like BERT models.</p><hr><h3>Multiscale Vision Transformers meet Bipartite Matching for efficient  single-stage Action Localization</h3>
<p>Ioanna Ntinou,Enrique Sanchez,Georgios Tzimiropoulos</p>
<p><a href='http://arxiv.org/abs/2312.17686v1'>http://arxiv.org/abs/2312.17686v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a single-stage method for action localization using a vision transformer with bipartite matching loss, improving performance and speed over two-stage methods.</p><hr><h3>FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video  Synthesis</h3>
<p>Feng Liang,Bichen Wu,Jialiang Wang,Licheng Yu,Kunpeng Li,Yinan Zhao,Ishan Misra,Jia-Bin Huang,Peizhao Zhang,Peter Vajda,Diana Marculescu</p>
<p><a href='http://arxiv.org/abs/2312.17681v1'>http://arxiv.org/abs/2312.17681v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a video-to-video synthesis framework that leverages spatial and temporal information to maintain consistency across video frames, while being flexible, efficient, and high-quality.</p><hr><h3>Data Augmentation for Supervised Graph Outlier Detection with Latent  Diffusion Models</h3>
<p>Kay Liu,Hengrui Zhang,Ziqing Hu,Fangxin Wang,Philip S. Yu</p>
<p><a href='http://arxiv.org/abs/2312.17679v1'>http://arxiv.org/abs/2312.17679v1</a></p>
<p><b>Compressor summary</b>: GODM is a novel data augmentation method that uses diffusion models to generate synthetic graph data for supervised graph outlier detection, mitigating class imbalance issues.</p><hr><h3>Benchmarking the CoW with the TopCoW Challenge: Topology-Aware  Anatomical Segmentation of the Circle of Willis for CTA and MRA</h3>
<p>Kaiyuan Yang,Fabio Musio,Yihui Ma,Norman Juchler,Johannes C. Paetzold,Rami Al-Maskari,Luciano Höher,Hongwei Bran Li,Ibrahim Ethem Hamamci,Anjany Sekuboyina,Suprosanna Shit,Houjing Huang,Diana Waldmannstetter,Florian Kofler,Fernando Navarro,Martin Menten,Ivan Ezhov,Daniel Rueckert,Iris Vos,Ynte Ruigrok,Birgitta Velthuis,Hugo Kuijf,Julien Hämmerli,Catherine Wurster,Philippe Bijlenga,Laura Westphal,Jeroen Bisschop,Elisa Colombo,Hakim Baazaoui,Andrew Makmur,James Hallinan,Bene Wiestler,Jan S. Kirschke,Roland Wiest,Emmanuel Montagnon,Laurent Letourneau-Guillon,Adrian Galdran,Francesco Galati,Daniele Falcetta,Maria A. Zuluaga,Chaolong Lin,Haoran Zhao,Zehan Zhang,Sinyoung Ra,Jongyun Hwang,Hyunjin Park,Junqiang Chen,Marek Wodzinski,Henning Müller,Pengcheng Shi,Wei Liu,Ting Ma,Cansu Yalçin,Rachika E. Hamadache,Joaquim Salvi,Xavier Llado,Uma Maria Lal-Trehan Estrada,Valeriia Abramova,Luca Giancardo,Arnau Oliver,Jialu Liu,Haibin Huang,Yue Cui,Zehang Lin,Yusheng Liu,Shunzhi Zhu,Tatsat R. Patel,Vincent M. Tutino,Maysam Orouskhani,Huayu Wang,Mahmud Mossa-Basha,Chengcheng Zhu,Maximilian R. Rokuss,Yannick Kirchhoff,Nico Disch,Julius Holzschuh,Fabian Isensee,Klaus Maier-Hein,Yuki Sato,Sven Hirsch,Susanne Wegener,Bjoern Menze</p>
<p><a href='http://arxiv.org/abs/2312.17670v1'>http://arxiv.org/abs/2312.17670v1</a></p>
<p><b>Compressor summary</b>: The TopCoW Challenge 2023 aimed to improve the characterization of the Circle of Willis (a network of brain arteries) using a public dataset with annotated images from MRA and CTA modalities, attracting over 140 participants worldwide.</p><hr><h3>AIJack: Security and Privacy Risk Simulator for Machine Learning</h3>
<p>Hideaki Takahashi</p>
<p><a href='http://arxiv.org/abs/2312.17667v1'>http://arxiv.org/abs/2312.17667v1</a></p>
<p><b>Compressor summary</b>: AIJack is an open-source library that helps evaluate security and privacy risks in machine learning models.</p><hr><h3>Shape-IoU: More Accurate Metric considering Bounding Box Shape and Scale</h3>
<p>Hao Zhang,Shuaijie Zhang</p>
<p><a href='http://arxiv.org/abs/2312.17663v1'>http://arxiv.org/abs/2312.17663v1</a></p>
<p><b>Compressor summary</b>: The article introduces a new bounding box regression method that considers the shape and scale of the boxes themselves, improving object detection performance and achieving state-of-the-art results in various tasks.</p><hr><h3>Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language  Models</h3>
<p>Yuqing Wang,Yun Zhao</p>
<p><a href='http://arxiv.org/abs/2312.17661v1'>http://arxiv.org/abs/2312.17661v1</a></p>
<p><b>Compressor summary</b>: Gemini is a multimodal large language model that performs well in complex commonsense reasoning tasks across different domains and modalities.</p><hr><h3>Normalization of Lithuanian Text Using Regular Expressions</h3>
<p>Pijus Kasparaitis</p>
<p><a href='http://arxiv.org/abs/2312.17660v2'>http://arxiv.org/abs/2312.17660v2</a></p>
<p><b>Compressor summary</b>: The paper presents a taxonomy for Lithuanian language semiotic classes, rules for detecting and expanding non-standard words, and evaluates their accuracy in different data sets.</p><hr><h3>Solar Radiation Prediction in the UTEQ based on Machine Learning Models</h3>
<p>Jordy Anchundia Troncoso,Ángel Torres Quijije,Byron Oviedo,Cristian Zambrano-Vega</p>
<p><a href='http://arxiv.org/abs/2312.17659v1'>http://arxiv.org/abs/2312.17659v1</a></p>
<p><b>Compressor summary</b>: The research compares various machine learning algorithms for predicting solar radiation at UTEQ using meteorological variables and finds Gradient Boosting and Random Forest to be the best performers.</p><hr><h3>Visual Point Cloud Forecasting enables Scalable Autonomous Driving</h3>
<p>Zetong Yang,Li Chen,Yanan Sun,Hongyang Li</p>
<p><a href='http://arxiv.org/abs/2312.17655v1'>http://arxiv.org/abs/2312.17655v1</a></p>
<p><b>Compressor summary</b>: The paper introduces ViDAR, a pre-training model for visual autonomous driving that uses visual point cloud forecasting to capture semantics, 3D geometry, and temporal dynamics and improves downstream tasks.</p><hr><h3>Bridging Modality Gap for Visual Grounding with Effecitve Cross-modal  Distillation</h3>
<p>Jiaxi Wang,Wenhui Hu,Xueyang Liu,Beihu Wu,Yuting Qiu,YingYing Cai</p>
<p><a href='http://arxiv.org/abs/2312.17648v1'>http://arxiv.org/abs/2312.17648v1</a></p>
<p><b>Compressor summary</b>: EpmVG is a framework that uses cross-modal distillation to align images and texts in a multimodal pre-trained model for better visual grounding.</p><hr><h3>Research on the Laws of Multimodal Perception and Cognition from a  Cross-cultural Perspective -- Taking Overseas Chinese Gardens as an Example</h3>
<p>Ran Chen,Xueqi Yao,Jing Zhao,Shuhan Xu,Sirui Zhang,Yijun Mao</p>
<p><a href='http://arxiv.org/abs/2312.17642v1'>http://arxiv.org/abs/2312.17642v1</a></p>
<p><b>Compressor summary</b>: The study analyzes perceptual and cognitive interactions in overseas Chinese gardens using social media data, deep learning, and multi-agent systems, revealing new insights into aesthetic experience and cultural communication.</p><hr><h3>Decision-focused predictions via pessimistic bilevel optimization: a  computational study</h3>
<p>Víctor Bucarey,Sophia Calderón,Gonzalo Muñoz,Frederic Semet</p>
<p><a href='http://arxiv.org/abs/2312.17640v1'>http://arxiv.org/abs/2312.17640v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a method to optimize decisions under uncertainty by minimizing regret, and shows its advantages on shortest-path problems.</p><hr><h3>XAI for In-hospital Mortality Prediction via Multimodal ICU Data</h3>
<p>Xingqiao Li,Jindong Gu,Zhiyong Wang,Yancheng Yuan,Bo Du,Fengxiang He</p>
<p><a href='http://arxiv.org/abs/2312.17624v1'>http://arxiv.org/abs/2312.17624v1</a></p>
<p><b>Compressor summary</b>: The paper proposes X-MMP, a multimodal AI system for predicting ICU mortality that provides explainability and visualization of its decisions and features.</p><hr><h3>Large Language Models for Generative Information Extraction: A Survey</h3>
<p>Derong Xu,Wei Chen,Wenjun Peng,Chao Zhang,Tong Xu,Xiangyu Zhao,Xian Wu,Yefeng Zheng,Enhong Chen</p>
<p><a href='http://arxiv.org/abs/2312.17617v1'>http://arxiv.org/abs/2312.17617v1</a></p>
<p><b>Compressor summary</b>: The text is about a systematic review of recent advancements in using large language models for information extraction tasks, and provides insights and future directions.</p><hr><h3>One-Shot Multi-Rate Pruning of Graph Convolutional Networks</h3>
<p>Hichem Sahbi</p>
<p><a href='http://arxiv.org/abs/2312.17615v1'>http://arxiv.org/abs/2312.17615v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method to create lightweight Graph Convolutional Networks by jointly training network topology and weights using a variational approach that aligns weight distribution with an a priori distribution, achieving better performance in skeleton-based recognition tasks especially at high pruning rates.</p><hr><h3>P2M2-Net: Part-Aware Prompt-Guided Multimodal Point Cloud Completion</h3>
<p>Linlian Jiang,Pan Chen,Ye Wang,Tieru Wu,Rui Ma</p>
<p><a href='http://arxiv.org/abs/2312.17611v1'>http://arxiv.org/abs/2312.17611v1</a></p>
<p><b>Compressor summary</b>: The P2M2-Net framework uses text prompts to guide a Transformer network in completing missing regions of 3D point clouds with controllable and diverse results.</p><hr><h3>Towards Faithful Explanations for Text Classification with Robustness  Improvement and Explanation Guided Training</h3>
<p>Dongfang Li,Baotian Hu,Qingcai Chen,Shan He</p>
<p><a href='http://arxiv.org/abs/2312.17591v1'>http://arxiv.org/abs/2312.17591v1</a></p>
<p><b>Compressor summary</b>: REGEX is a method to improve text classification model explanations by enhancing robustness and similarity between attention and feature attributions.</p><hr><h3>Interpretable and Explainable Machine Learning Methods for Predictive  Process Monitoring: A Systematic Literature Review</h3>
<p>Nijat Mehdiyev,Maxim Majlatow,Peter Fettke</p>
<p><a href='http://arxiv.org/abs/2312.17584v1'>http://arxiv.org/abs/2312.17584v1</a></p>
<p><b>Compressor summary</b>: The paper reviews literature on making machine learning models in predictive process mining explainable and interpretable, discussing challenges, methods, and future directions.</p><hr><h3>Action-Item-Driven Summarization of Long Meeting Transcripts</h3>
<p>Logan Golia,Jugal Kalita</p>
<p><a href='http://arxiv.org/abs/2312.17581v1'>http://arxiv.org/abs/2312.17581v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new algorithm that generates meeting summaries based on action items, divides transcripts into topic-based sections, and outperforms the current state-of-the-art model by 4.98%.</p><hr><h3>Informative Rays Selection for Few-Shot Neural Radiance Fields</h3>
<p>Marco Orsingher,Anthony Dell'Eva,Paolo Zani,Paolo Medici,Massimo Bertozzi</p>
<p><a href='http://arxiv.org/abs/2312.17561v1'>http://arxiv.org/abs/2312.17561v1</a></p>
<p><b>Compressor summary</b>: KeyNeRF is a method for training NeRF with few input views by selecting key informative rays using view and pixel-level selection algorithms.</p><hr><h3>A Fully Automated Pipeline Using Swin Transformers for Deep  Learning-Based Blood Segmentation on Head CT Scans After Aneurysmal  Subarachnoid Hemorrhage</h3>
<p>Sergio Garcia Garcia,Santiago Cepeda,Ignacio Arrese,Rosario Sarabia</p>
<p><a href='http://arxiv.org/abs/2312.17553v1'>http://arxiv.org/abs/2312.17553v1</a></p>
<p><b>Compressor summary</b>: The researchers developed and validated an artificial intelligence tool that can automatically segment blood in subarachnoid hemorrhage patients on CT scans, improving accuracy and efficiency.</p><hr><h3>Building Efficient Universal Classifiers with Natural Language Inference</h3>
<p>Moritz Laurer,Wouter van Atteveldt,Andreu Casas,Kasper Welbers</p>
<p><a href='http://arxiv.org/abs/2312.17543v1'>http://arxiv.org/abs/2312.17543v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a BERT-like universal classifier based on NLI that can do any text classification task without fine-tuning or few-shot learning, and shares the code for building it.</p><hr><h3>Distance Guided Generative Adversarial Network for Explainable Binary  Classifications</h3>
<p>Xiangyu Xiong,Yue Sun,Xiaohong Liu,Wei Ke,Chan-Tong Lam,Jiangang Chen,Mingfeng Jiang,Mingwei Wang,Hui Xie,Tong Tong,Qinquan Gao,Hao Chen,Tao Tan</p>
<p><a href='http://arxiv.org/abs/2312.17538v1'>http://arxiv.org/abs/2312.17538v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new data augmentation method called DisGAN, which generates diverse samples in the hyperplane space for binary classification using vertical and horizontal distances.</p><hr><h3>Olapa-MCoT: Enhancing the Chinese Mathematical Reasoning Capability of  LLMs</h3>
<p>Shaojie Zhu,Zhaobin Wang,Chengxiang Zhuo,Hui Lu,Bo Hu,Zang Li</p>
<p><a href='http://arxiv.org/abs/2312.17535v1'>http://arxiv.org/abs/2312.17535v1</a></p>
<p><b>Compressor summary</b>: Olapa-MCoT is a LLM that improves CoT and Chinese mathematical reasoning with SimRRHF algorithm and data relearning, achieving 36% improvement over llama2-13B.</p><hr><h3>Enhancing Quantitative Reasoning Skills of Large Language Models through  Dimension Perception</h3>
<p>Yuncheng Huang,Qianyu He,Jiaqing Liang,Sihang Jiang,Yanghua Xiao,Yunwen Chen</p>
<p><a href='http://arxiv.org/abs/2312.17532v1'>http://arxiv.org/abs/2312.17532v1</a></p>
<p><b>Compressor summary</b>: The authors present a framework to improve LLMs' quantitative reasoning by enhancing their dimension perception, which is crucial for understanding quantities with units and improving performance on related benchmarks.</p><hr><h3>RS-DGC: Exploring Neighborhood Statistics for Dynamic Gradient  Compression on Remote Sensing Image Interpretation</h3>
<p>Weiying Xie,Zixuan Wang,Jitao Ma,Daixun Li,Yunsong Li</p>
<p><a href='http://arxiv.org/abs/2312.17530v1'>http://arxiv.org/abs/2312.17530v1</a></p>
<p><b>Compressor summary</b>: RS-DGC is a dynamic gradient compression technique for distributed deep learning in remote sensing applications that leverages neighborhood statistics to sparsify gradients and reduce communication costs while maintaining performance.</p><hr><h3>Noise-free Optimization in Early Training Steps for Image  Super-Resolution</h3>
<p>MinKyu Lee,Jae-Pil Heo</p>
<p><a href='http://arxiv.org/abs/2312.17526v1'>http://arxiv.org/abs/2312.17526v1</a></p>
<p><b>Compressor summary</b>: The paper investigates deep-learning-based single image super-resolution methods and proposes a new optimization method that improves their stability and performance by estimating the optimal centroid of high-resolution images and removing inherent noise.</p><hr><h3>Overview of the PromptCBLUE Shared Task in CHIP2023</h3>
<p>Wei Zhu,Xiaoling Wang,Mosha Chen,Buzhou Tang</p>
<p><a href='http://arxiv.org/abs/2312.17522v1'>http://arxiv.org/abs/2312.17522v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a shared task that tests Chinese large language models in medical natural language processing using two tracks: prompt tuning and in-context learning.</p><hr><h3>Embedded feature selection in LSTM networks with multi-objective  evolutionary ensemble learning for time series forecasting</h3>
<p>Raquel Espinosa,Fernando Jiménez,José Palma</p>
<p><a href='http://arxiv.org/abs/2312.17517v1'>http://arxiv.org/abs/2312.17517v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel feature selection method for time series forecasting using LSTM networks, an evolutionary algorithm, and ensemble learning, which improves generalization and reduces overfitting.</p><hr><h3>Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in  the Avalon Game</h3>
<p>Zijing Shi,Meng Fang,Shunfeng Zheng,Shilong Deng,Ling Chen,Yali Du</p>
<p><a href='http://arxiv.org/abs/2312.17515v1'>http://arxiv.org/abs/2312.17515v1</a></p>
<p><b>Compressor summary</b>: The study explores how large language models can collaborate in ad hoc teamwork scenarios using CodeAct, an agent that improves communication and adaptability by combining memory and code-driven reasoning.</p><hr><h3>Leveraging Open-Vocabulary Diffusion to Camouflaged Instance  Segmentation</h3>
<p>Tuan-Anh Vu,Duc Thanh Nguyen,Qing Guo,Binh-Son Hua,Nhat Minh Chung,Ivor W. Tsang,Sai-Kit Yeung</p>
<p><a href='http://arxiv.org/abs/2312.17505v1'>http://arxiv.org/abs/2312.17505v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a text-to-image diffusion model that leverages cross-domain features for camouflaged object segmentation, outperforming existing methods on benchmark datasets.</p><hr><h3>HiBid: A Cross-Channel Constrained Bidding System with Budget Allocation  by Hierarchical Offline Deep Reinforcement Learning</h3>
<p>Hao Wang,Bo Tang,Chi Harold Liu,Shangqin Mao,Jiahong Zhou,Zipeng Dai,Yaqi Sun,Qianlong Xie,Xingxing Wang,Dong Wang</p>
<p><a href='http://arxiv.org/abs/2312.17503v1'>http://arxiv.org/abs/2312.17503v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a hierarchical offline DRL framework for cross-channel constrained bidding with budget allocation
- The framework consists of a high-level planner and a low-level executor with CPC-guided action selection mechanism
- HiBid outperforms six baselines and is deployed on Meituan advertising platform

Summary:
The paper introduces HiBid, an offline DRL framework that optimizes cross-channel bidding with budget allocation and CPC constraints, and shows its effectiveness and real-world application.</p><hr><h3>Integrating Chemical Language and Molecular Graph in Multimodal Fused  Deep Learning for Drug Property Prediction</h3>
<p>Xiaohua Lu,Liangxu Xie,Lei Xu,Rongzhi Mao,Shan Chang,Xiaojun Xu</p>
<p><a href='http://arxiv.org/abs/2312.17495v1'>http://arxiv.org/abs/2312.17495v1</a></p>
<p><b>Compressor summary</b>: The text describes a multimodal deep learning model that predicts molecular properties better than mono-modal models by using different representations of drug molecules and fusion methods.</p><hr><h3>QGFace: Quality-Guided Joint Training For Mixed-Quality Face Recognition</h3>
<p>Youzhe Song,Feng Wang</p>
<p><a href='http://arxiv.org/abs/2312.17494v1'>http://arxiv.org/abs/2312.17494v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for mixed-quality face recognition that applies different learning methods to HQ and LQ images, using classification-based methods for HQ data and self-supervised contrastive learning for LQ data.</p><hr><h3>HEAP: Unsupervised Object Discovery and Localization with Contrastive  Grouping</h3>
<p>Xin Zhang,Jinheng Xie,Yuan Yuan,Michael Bi Mi,Robby T. Tan</p>
<p><a href='http://arxiv.org/abs/2312.17492v1'>http://arxiv.org/abs/2312.17492v1</a></p>
<p><b>Compressor summary</b>: HEAP is a novel framework that uses cross-attention and contrastive losses to group patches into regions for efficient hierarchical image decomposition and improved object discovery and differentiation.</p><hr><h3>Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models  through Intervention without Tuning</h3>
<p>Zhongzhi Chen,Xingwu Sun,Xianfeng Jiao,Fengzong Lian,Zhanhui Kang,Di Wang,Cheng-Zhong Xu</p>
<p><a href='http://arxiv.org/abs/2312.17484v1'>http://arxiv.org/abs/2312.17484v1</a></p>
<p><b>Compressor summary</b>: Truth Forest is a method to make LLMs more truthful by finding hidden truth representations using orthogonal probes and Random Peek technique, improving performance on TruthfulQA dataset.</p><hr><h3>MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining</h3>
<p>Jacob Portes,Alex Trott,Sam Havens,Daniel King,Abhinav Venigalla,Moin Nadeem,Nikhil Sardana,Daya Khudia,Jonathan Frankle</p>
<p><a href='http://arxiv.org/abs/2312.17482v1'>http://arxiv.org/abs/2312.17482v1</a></p>
<p><b>Compressor summary</b>: MosaicBERT is a fast, optimized BERT-style encoder architecture that allows efficient pretraining with minimal costs.</p><hr><h3>Culturally-Attuned Moral Machines: Implicit Learning of Human Value  Systems by AI through Inverse Reinforcement Learning</h3>
<p>Nigini Oliveira,Jasmine Li,Koosha Khalvati,Rodolfo Cortes Barragan,Katharina Reinecke,Andrew N. Meltzoff,Rajesh P. N. Rao</p>
<p><a href='http://arxiv.org/abs/2312.17479v1'>http://arxiv.org/abs/2312.17479v1</a></p>
<p><b>Compressor summary</b>: The authors propose using inverse reinforcement learning to help AI agents learn the cultural values and norms of the community they operate in by observing human behavior in a virtual world.</p><hr><h3>Exploring the Sensitivity of LLMs' Decision-Making Capabilities:  Insights from Prompt Variation and Hyperparameters</h3>
<p>Manikanta Loya,Divya Anand Sinha,Richard Futrell</p>
<p><a href='http://arxiv.org/abs/2312.17476v1'>http://arxiv.org/abs/2312.17476v1</a></p>
<p><b>Compressor summary</b>: The study examines how variations in prompts and hyperparameters affect large language models' decision making abilities, finding that they can exhibit a human-like exploration-exploitation tradeoff.</p><hr><h3>EHR Interaction Between Patients and AI: NoteAid EHR Interaction</h3>
<p>Xiaocheng Zhang,Zonghai Yao,Hong Yu</p>
<p><a href='http://arxiv.org/abs/2312.17475v1'>http://arxiv.org/abs/2312.17475v1</a></p>
<p><b>Compressor summary</b>: The paper presents an approach using generative large language models to help patients understand their Electronic Health Records by providing explanations and answering questions, and evaluates its performance on two novel tasks.</p><hr><h3>FerKD: Surgical Label Adaptation for Efficient Distillation</h3>
<p>Zhiqiang Shen</p>
<p><a href='http://arxiv.org/abs/2312.17473v1'>http://arxiv.org/abs/2312.17473v1</a></p>
<p><b>Compressor summary</b>: FerKD is a novel knowledge distillation framework that improves convergence speed and accuracy by calibrating less-confident regions, mixing similar image regions, and using hard ground truth labels.</p><hr><h3>Out of the Ordinary: Spectrally Adapting Regression for Covariate Shift</h3>
<p>Benjamin Eyre,Elliot Creager,David Madras,Vardan Papyan,Richard Zemel</p>
<p><a href='http://arxiv.org/abs/2312.17463v1'>http://arxiv.org/abs/2312.17463v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a simple spectral adaptation method to improve the performance of neural regression models on out-of-distribution data.</p><hr><h3>Tracking with Human-Intent Reasoning</h3>
<p>Jiawen Zhu,Zhi-Qi Cheng,Jun-Yan He,Chenyang Li,Bin Luo,Huchuan Lu,Yifeng Geng,Xuansong Xie</p>
<p><a href='http://arxiv.org/abs/2312.17448v1'>http://arxiv.org/abs/2312.17448v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new tracking task called Instruction Tracking, which uses a Large Vision-Language Model to provide implicit tracking instructions and achieve competitive performance on referring video object segmentation benchmarks.</p><hr><h3>ClST: A Convolutional Transformer Framework for Automatic Modulation  Recognition by Knowledge Distillation</h3>
<p>Dongbin Hou,Lixin Li,Wensheng Lin,Junli Liang,Zhu Han</p>
<p><a href='http://arxiv.org/abs/2312.17446v1'>http://arxiv.org/abs/2312.17446v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new neural network (ClST) and a knowledge distillation method (SKD) to improve automatic modulation recognition (AMR) using deep learning, especially on miniaturized devices.</p><hr><h3>SMoT: Think in State Machine</h3>
<p>Jia Liu,Jie Shuai</p>
<p><a href='http://arxiv.org/abs/2312.17445v1'>http://arxiv.org/abs/2312.17445v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Current prompting approach for language model inference relies on LLM's autonomous exploration of reasoning paths, which can be inefficient and prone to errors
- SMoT introduces a paradigm that uses predefined state machines to guide LLM's reasoning, eliminating fruitless exploration
- SMoT also uses a multi-agent mechanism to enhance the accuracy of reasoning by assigning different objectives to agents
- SMoT achieves an extraordinary accuracy of 98% on an array reasoning task

Summary:
SMoT is a novel paradigm that improves LLM's problem-solving by using predefined state machines and multi-agent mechanisms, resulting in high accuracy on array reasoning.</p><hr><h3>Video Understanding with Large Language Models: A Survey</h3>
<p>Yunlong Tang,Jing Bi,Siting Xu,Luchuan Song,Susan Liang,Teng Wang,Daoan Zhang,Jie An,Jingyang Lin,Rongyi Zhu,Ali Vosoughi,Chao Huang,Zeliang Zhang,Feng Zheng,Jianguo Zhang,Ping Luo,Jiebo Luo,Chenliang Xu</p>
<p><a href='http://arxiv.org/abs/2312.17432v1'>http://arxiv.org/abs/2312.17432v1</a></p>
<p><b>Compressor summary</b>: This survey summarizes recent advancements in using large language models for video understanding, exploring their capabilities, types, tasks, datasets, applications, and limitations.</p><hr><h3>Commonsense for Zero-Shot Natural Language Video Localization</h3>
<p>Meghana Holla,Ismini Lourentzou</p>
<p><a href='http://arxiv.org/abs/2312.17429v1'>http://arxiv.org/abs/2312.17429v1</a></p>
<p><b>Compressor summary</b>: CORONET is a framework that uses commonsense reasoning to improve zero-shot Natural Language-Video Localization by bridging the gap between videos and generated pseudo-queries with Graph Convolution Networks and cross-attention mechanisms.</p><hr><h3>ChangeNet: Multi-Temporal Asymmetric Change Detection Dataset</h3>
<p>Deyi Ji,Siqi Gao,Mingyuan Tao,Hongtao Lu,Feng Zhao</p>
<p><a href='http://arxiv.org/abs/2312.17428v1'>http://arxiv.org/abs/2312.17428v1</a></p>
<p><b>Compressor summary</b>: ChangeNet is a large-scale practical-oriented dataset for multi-temporal change detection with realistic perspective distortions and six annotated categories, covering various complex scenes from 100 cities.</p><hr><h3>Context-based Transfer and Efficient Iterative Learning for Unbiased  Scene Graph Generation</h3>
<p>Qishen Chen,Xinyu Lyu,Haonan Zhang,Pengpeng Zeng,Lianli Gao,Jingkuan Song</p>
<p><a href='http://arxiv.org/abs/2312.17425v1'>http://arxiv.org/abs/2312.17425v1</a></p>
<p><b>Compressor summary</b>: CITrans is a plug-and-play method for scene graph generation that uses context-restricted transfer and efficient iterative learning to improve data transfer and training efficiency.</p><hr><h3>Generative Posterior Networks for Approximately Bayesian Epistemic  Uncertainty Estimation</h3>
<p>Melrose Roderick,Felix Berkenkamp,Fatemeh Sheikholeslami,Zico Kolter</p>
<p><a href='http://arxiv.org/abs/2312.17411v1'>http://arxiv.org/abs/2312.17411v1</a></p>
<p><b>Compressor summary</b>: Generative Posterior Networks (GPNs) are a new generative model that uses unlabeled data to estimate epistemic uncertainty in high-dimensional problems by approximating the Bayesian posterior distribution.</p><hr><h3>Comparing roughness descriptors for distinct terrain surfaces in point  cloud data</h3>
<p>Lei Fan,Yang Zhao</p>
<p><a href='http://arxiv.org/abs/2312.17407v1'>http://arxiv.org/abs/2312.17407v1</a></p>
<p><b>Compressor summary</b>: This study compares five ways of measuring surface roughness and shows that using multiple methods can improve accuracy in analyzing different terrains.</p><hr><h3>Parameter Optimization with Conscious Allocation (POCA)</h3>
<p>Joshua Inman,Tanmay Khandait,Giulia Pedrielli,Lalitha Sankar</p>
<p><a href='http://arxiv.org/abs/2312.17404v1'>http://arxiv.org/abs/2312.17404v1</a></p>
<p><b>Compressor summary</b>: POCA is a new hyperband-based algorithm that adaptively allocates budget to hyperparameter configurations using Bayesian sampling, and outperforms its competitors in finding optimal configurations for machine learning models.</p>