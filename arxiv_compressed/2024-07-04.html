
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-07-04</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-07-04 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Planetarium: A Rigorous Benchmark for Translating Text to Structured  Planning Languages</h3>
<p><a href='http://arxiv.org/abs/2407.03321v1'>http://arxiv.org/abs/2407.03321v1</a></p>
<p><b>Compressor summary</b>: The paper introduces \benchmarkName, a new benchmark to evaluate language models' ability to generate PDDL code from natural language descriptions of planning tasks, addressing challenges in existing evaluation methods.</p><hr><h3>InternLM-XComposer-2.5: A Versatile Large Vision Language Model  Supporting Long-Contextual Input and Output</h3>
<p><a href='http://arxiv.org/abs/2407.03320v1'>http://arxiv.org/abs/2407.03320v1</a></p>
<p><b>Compressor summary</b>: The text introduces IXC-2.5, a large-vision language model with long-context capabilities and improved vision-language comprehension for various applications, achieving GPT-4V level performance with less resources.</p><hr><h3>BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate  Hallucinations</h3>
<p><a href='http://arxiv.org/abs/2407.03314v1'>http://arxiv.org/abs/2407.03314v1</a></p>
<p><b>Compressor summary</b>: The paper introduces BACON, a graph structure that enhances vision language models' abilities in various tasks by simplifying complex visual scenes into basic elements.</p><hr><h3>Universal Length Generalization with Turing Programs</h3>
<p><a href='http://arxiv.org/abs/2407.03310v1'>http://arxiv.org/abs/2407.03310v1</a></p>
<p><b>Compressor summary</b>: Turing Programs enable length generalization on various algorithmic tasks by decomposing them into steps resembling a Turing Machine's computation.</p><hr><h3>A Review of the Applications of Deep Learning-Based Emergent  Communication</h3>
<p><a href='http://arxiv.org/abs/2407.03302v1'>http://arxiv.org/abs/2407.03302v1</a></p>
<p><b>Compressor summary</b>: This paper reviews how emergent communication research can be applied to various fields like machine learning and linguistics by studying how language-like systems arise in multi-agent reinforcement learning environments.</p><hr><h3>DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents</h3>
<p><a href='http://arxiv.org/abs/2407.03300v1'>http://arxiv.org/abs/2407.03300v1</a></p>
<p><b>Compressor summary</b>: DisCo-Diff models introduce discrete latent variables to simplify the learning process of diffusion models and improve their performance on various tasks, including image synthesis and molecular docking.</p><hr><h3>Improved Noise Schedule for Diffusion Training</h3>
<p><a href='http://arxiv.org/abs/2407.03297v1'>http://arxiv.org/abs/2407.03297v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new way to adjust the noise in diffusion models during training to improve efficiency and performance.</p><hr><h3>Biomechanics-informed Non-rigid Medical Image Registration and its  Inverse Material Property Estimation with Linear and Nonlinear Elasticity</h3>
<p><a href='http://arxiv.org/abs/2407.03292v1'>http://arxiv.org/abs/2407.03292v1</a></p>
<p><b>Compressor summary</b>: The paper presents a biomechanical-constrained non-rigid medical image registration algorithm using physics-informed neural networks (PINNs) that generalizes linear elasticity to nonlinear models and solves the inverse parameter estimation problem under PINNs.</p><hr><h3>VCHAR:Variance-Driven Complex Human Activity Recognition framework with  Generative Representation</h3>
<p><a href='http://arxiv.org/abs/2407.03291v1'>http://arxiv.org/abs/2407.03291v1</a></p>
<p><b>Compressor summary</b>: VCHAR is a novel framework for recognizing complex human activities in smart environments without precise labeling of atomic activities, providing video-based explanations for better user comprehension.</p><hr><h3>LLM Internal States Reveal Hallucination Risk Faced With a Query</h3>
<p><a href='http://arxiv.org/abs/2407.03282v1'>http://arxiv.org/abs/2407.03282v1</a></p>
<p><b>Compressor summary</b>: The paper explores how Large Language Models can estimate their own hallucination risk before generating responses and analyzes the internal mechanisms involved in this process.</p><hr><h3>Evaluating Automatic Metrics with Incremental Machine Translation  Systems</h3>
<p><a href='http://arxiv.org/abs/2407.03277v1'>http://arxiv.org/abs/2407.03277v1</a></p>
<p><b>Compressor summary</b>: The text describes a dataset of commercial translations from 12 directions over six years, which can be used to evaluate MT metrics based on their preference for newer translations.</p><hr><h3>For a semiotic AI: Bridging computer vision and visual semiotics for  computational observation of large scale facial image archives</h3>
<p><a href='http://arxiv.org/abs/2407.03268v1'>http://arxiv.org/abs/2407.03268v1</a></p>
<p><b>Compressor summary</b>: FRESCO is a framework to study how images on social media platforms affect society by analyzing them at three levels and using a new metric called the FRESCO score.</p><hr><h3>A Unified Framework for 3D Scene Understanding</h3>
<p><a href='http://arxiv.org/abs/2407.03263v1'>http://arxiv.org/abs/2407.03263v1</a></p>
<p><b>Compressor summary</b>: UniSeg3D is a 3D segmentation framework that handles six tasks with one model, enhancing understanding of 3D scenes by sharing knowledge across tasks.</p><hr><h3>Magnetic Hysteresis Modeling with Neural Operators</h3>
<p><a href='http://arxiv.org/abs/2407.03261v1'>http://arxiv.org/abs/2407.03261v1</a></p>
<p><b>Compressor summary</b>: The paper proposes neural operators for modeling magnetic hysteresis and shows they outperform traditional methods and generalize well to novel input fields.</p><hr><h3>Modern Neighborhood Components Analysis: A Deep Tabular Baseline Two  Decades Later</h3>
<p><a href='http://arxiv.org/abs/2407.03257v1'>http://arxiv.org/abs/2407.03257v1</a></p>
<p><b>Compressor summary</b>: The paper proposes ModernNCA, a modified version of Neighborhood Component Analysis, which improves semantic similarity learning for tabular data and surpasses most deep tabular models in accuracy and efficiency.</p><hr><h3>STF: Sentence Transformer Fine-Tuning For Topic Categorization With  Limited Data</h3>
<p><a href='http://arxiv.org/abs/2407.03253v1'>http://arxiv.org/abs/2407.03253v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The text proposes Sentence Transformers Fine-tuning (STF), a system for classifying topics from tweets using pretrained models and fine-tuning.
- STF outperforms state-of-the-art approaches and does not need much labeled data.
- The main contribution is the application of pretrained sentence transformers language models.

Summary:
The text introduces STF, a system that uses pretrained models and fine-tuning to classify tweet topics accurately, overcoming the limitations of existing methods.</p><hr><h3>ACTRESS: Active Retraining for Semi-supervised Visual Grounding</h3>
<p><a href='http://arxiv.org/abs/2407.03251v1'>http://arxiv.org/abs/2407.03251v1</a></p>
<p><b>Compressor summary</b>: ACTRESS is a new approach for semi-supervised visual grounding that uses active sampling and selective retraining to improve performance with sparse labeled data.</p><hr><h3>Visual Grounding with Attention-Driven Constraint Balancing</h3>
<p><a href='http://arxiv.org/abs/2407.03243v1'>http://arxiv.org/abs/2407.03243v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Attention-Driven Constraint Balancing (AttBalance), a framework that improves visual grounding tasks using transformer-based models and attention mechanisms, achieving state-of-the-art results.</p><hr><h3>Cyclic Refiner: Object-Aware Temporal Representation Learning for  Multi-View 3D Detection and Tracking</h3>
<p><a href='http://arxiv.org/abs/2407.03240v1'>http://arxiv.org/abs/2407.03240v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a cyclic learning mechanism for multi-view 3D detection and tracking tasks that suppresses irrelevant regions in historical frames and improves object awareness, resulting in consistent performance gains over baselines.</p><hr><h3>CATT: Character-based Arabic Tashkeel Transformer</h3>
<p><a href='http://arxiv.org/abs/2407.03236v1'>http://arxiv.org/abs/2407.03236v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new approach to train ATD models, which improve Arabic text comprehension and processing by using finetuned transformers and the Noisy-Student method, achieving state-of-the-art results.</p><hr><h3>Self-Evaluation as a Defense Against Adversarial Attacks on LLMs</h3>
<p><a href='http://arxiv.org/abs/2407.03234v1'>http://arxiv.org/abs/2407.03234v1</a></p>
<p><b>Compressor summary</b>: The study shows that adding a space to LLMs' inputs can cause them to generate unsafe or biased outputs, highlighting the need for better alignment methods.</p><hr><h3>Single Character Perturbations Break LLM Alignment</h3>
<p><a href='http://arxiv.org/abs/2407.03232v1'>http://arxiv.org/abs/2407.03232v1</a></p>
<p><b>Compressor summary</b>: A study shows that adding a space to input can trick LLMs into generating unsafe outputs, highlighting the need for better model alignment.</p><hr><h3>Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and  Schema Pruning</h3>
<p><a href='http://arxiv.org/abs/2407.03227v1'>http://arxiv.org/abs/2407.03227v1</a></p>
<p><b>Compressor summary</b>: The text proposes a method to improve Text-to-SQL semantic parsing using Large Language Models and in-context learning with few-shot examples and approximate SQL query generation.</p><hr><h3>MHNet: Multi-view High-order Network for Diagnosing Neurodevelopmental  Disorders Using Resting-state fMRI</h3>
<p><a href='http://arxiv.org/abs/2407.03217v1'>http://arxiv.org/abs/2407.03217v1</a></p>
<p><b>Compressor summary</b>: MHNet is a novel deep learning model that uses hierarchical and high-order features from multi-view brain functional networks derived from rs-fMRI data for neurodevelopmental disorder prediction, outperforming state-of-the-art methods.</p><hr><h3>Learning Disentangled Representation in Object-Centric Models for Visual  Dynamics Prediction via Transformers</h3>
<p><a href='http://arxiv.org/abs/2407.03216v1'>http://arxiv.org/abs/2407.03216v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel object-centric model that learns disentangled representations and discovers blocks to predict dynamic visual states, achieving better accuracy and interpretability in various settings.</p><hr><h3>How Does Quantization Affect Multilingual LLMs?</h3>
<p><a href='http://arxiv.org/abs/2407.03211v1'>http://arxiv.org/abs/2407.03211v1</a></p>
<p><b>Compressor summary</b>: Quantization affects multilingual LLMs differently and negatively impacts non-Latin script languages and challenging tasks.</p><hr><h3>Combining AI Control Systems and Human Decision Support via Robustness  and Criticality</h3>
<p><a href='http://arxiv.org/abs/2407.03210v1'>http://arxiv.org/abs/2407.03210v1</a></p>
<p><b>Compressor summary</b>: The text discusses using adversarial explanations and autoencoders to improve AI decision-making, robustness, and human oversight in real-world applications.</p><hr><h3>Category-Aware Dynamic Label Assignment with High-Quality Oriented  Proposal</h3>
<p><a href='http://arxiv.org/abs/2407.03205v1'>http://arxiv.org/abs/2407.03205v1</a></p>
<p><b>Compressor summary</b>: The text introduces a method to detect oriented objects in aerial images using a complex plane OBB representation, a conformer RPN head, and a category-aware dynamic label assignment.</p><hr><h3>Expressive Gaussian Human Avatars from Monocular RGB Video</h3>
<p><a href='http://arxiv.org/abs/2407.03204v1'>http://arxiv.org/abs/2407.03204v1</a></p>
<p><b>Compressor summary</b>: EVA is a method to create more realistic and expressive digital human avatars from monocular video by combining a sculpted 3D model with SMPL-X and improving alignment, density control, and confidence prediction.</p><hr><h3>SegVG: Transferring Object Bounding Box to Segmentation for Visual  Grounding</h3>
<p><a href='http://arxiv.org/abs/2407.03200v1'>http://arxiv.org/abs/2407.03200v1</a></p>
<p><b>Compressor summary</b>: The paper proposes SegVG, a method that uses segmentation signals from box annotations for Visual Grounding and mitigates domain discrepancy with Triple Alignment module, achieving SOTA performance.</p><hr><h3>DyFADet: Dynamic Feature Aggregation for Temporal Action Detection</h3>
<p><a href='http://arxiv.org/abs/2407.03197v1'>http://arxiv.org/abs/2407.03197v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a dynamic feature aggregation module for temporal action detection models that adapts kernel weights and receptive fields at different timestamps, improving performance on various benchmarks.</p><hr><h3>Prediction Instability in Machine Learning Ensembles</h3>
<p><a href='http://arxiv.org/abs/2407.03194v1'>http://arxiv.org/abs/2407.03194v1</a></p>
<p><b>Compressor summary</b>: The paper proves that ensembles have prediction instabilities and suggests balancing information use with risk management.</p><hr><h3>Multiple-Resolution Tokenization for Time Series Forecasting with an  Application to Pricing</h3>
<p><a href='http://arxiv.org/abs/2407.03185v1'>http://arxiv.org/abs/2407.03185v1</a></p>
<p><b>Compressor summary</b>: The authors propose a transformer-based time series forecasting method that uses multiple resolutions, cross-series information, and novel modules to improve performance on a real-world pricing problem.</p><hr><h3>Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through  Self-Correction in Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.03181v1'>http://arxiv.org/abs/2407.03181v1</a></p>
<p><b>Compressor summary</b>: The Divergent CoT (DCoT) method improves the performance of large language models by requiring them to generate multiple divergent reasoning chains in a single inference step, enabling self-correction.</p><hr><h3>Motion meets Attention: Video Motion Prompts</h3>
<p><a href='http://arxiv.org/abs/2407.03179v1'>http://arxiv.org/abs/2407.03179v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a modified Sigmoid function with learnable parameters as an attention mechanism to highlight salient motion features in videos for action recognition tasks.</p><hr><h3>IMC 2024 Methods & Solutions Review</h3>
<p><a href='http://arxiv.org/abs/2407.03172v1'>http://arxiv.org/abs/2407.03172v1</a></p>
<p><b>Compressor summary</b>: The paper presents an advanced ensemble technique for 3D image reconstruction from 2D images, developed by the authors who participated in Kaggle's Image Matching Challenge and conducted a review of top-performing methods.</p><hr><h3>Investigating Decoder-only Large Language Models for Speech-to-text  Translation</h3>
<p><a href='http://arxiv.org/abs/2407.03169v1'>http://arxiv.org/abs/2407.03169v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a decoder-only architecture for speech-to-text translation using large language models and shows its effectiveness on two benchmarks, while analyzing the impact of different fine-tuning techniques and task formulation.</p><hr><h3>LivePortrait: Efficient Portrait Animation with Stitching and  Retargeting Control</h3>
<p><a href='http://arxiv.org/abs/2407.03168v1'>http://arxiv.org/abs/2407.03168v1</a></p>
<p><b>Compressor summary</b>: LivePortrait is a video-driven portrait animation framework that uses implicit keypoints to create lifelike videos from single images, with improved efficiency and controllability compared to diffusion-based methods.</p><hr><h3>Consistent Point Orientation for Manifold Surfaces via Boundary  Integration</h3>
<p><a href='http://arxiv.org/abs/2407.03165v1'>http://arxiv.org/abs/2407.03165v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method to generate consistent normals for point clouds using a boundary energy derived from the Dirichlet energy of the generalized winding number field, which improves robustness to noise and complex structures.</p><hr><h3>Global Context Modeling in YOLOv8 for Pediatric Wrist Fracture Detection</h3>
<p><a href='http://arxiv.org/abs/2407.03163v1'>http://arxiv.org/abs/2407.03163v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a YOLOv8 model with a Global Context block that improves fracture detection and reaches state-of-the-art performance on a wrist X-ray dataset.</p><hr><h3>Let the Code LLM Edit Itself When You Edit the Code</h3>
<p><a href='http://arxiv.org/abs/2407.03157v1'>http://arxiv.org/abs/2407.03157v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Positional Integrity Encoding (PIE) for efficient and accurate code prediction in real-time editing scenarios, reducing computational overhead by over 85%.</p><hr><h3>Reinforcement Learning for Sequence Design Leveraging Protein Language  Models</h3>
<p><a href='http://arxiv.org/abs/2407.03154v1'>http://arxiv.org/abs/2407.03154v1</a></p>
<p><b>Compressor summary</b>: The authors propose using protein language models as a reward function to generate new protein sequences with reinforcement learning, while periodically finetuning a proxy model.</p><hr><h3>Stereo Risk: A Continuous Modeling Approach to Stereo Matching</h3>
<p><a href='http://arxiv.org/abs/2407.03152v1'>http://arxiv.org/abs/2407.03152v1</a></p>
<p><b>Compressor summary</b>: Stereo Risk is a new deep-learning method for stereo matching that uses continuous risk minimization to estimate scene depth better than existing methods.</p><hr><h3>Enhancing Translation Accuracy of Large Language Models through  Continual Pre-Training on Parallel Data</h3>
<p><a href='http://arxiv.org/abs/2407.03145v1'>http://arxiv.org/abs/2407.03145v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a two-phase training method for language translation using large pre-trained models, which improves accuracy for aligned source and target sentence orders and spoken language, especially with added tags and interleaved sentences.</p><hr><h3>Machine Learning Models for Improved Tracking from Range-Doppler Map  Images</h3>
<p><a href='http://arxiv.org/abs/2407.03140v1'>http://arxiv.org/abs/2407.03140v1</a></p>
<p><b>Compressor summary</b>: The authors propose machine learning models for target detection and uncertainty estimation in RDM images to improve GMTI radar tracking performance.</p><hr><h3>Towards Efficient Pixel Labeling for Industrial Anomaly Detection and  Localization</h3>
<p><a href='http://arxiv.org/abs/2407.03130v1'>http://arxiv.org/abs/2407.03130v1</a></p>
<p><b>Compressor summary</b>: ADClick is a novel interactive image segmentation algorithm that generates anomaly masks for real defective images with high accuracy, using only a few manual clicks per image and innovative residual features and language prompts.</p><hr><h3>Social Bias Evaluation for Large Language Models Requires Prompt  Variations</h3>
<p><a href='http://arxiv.org/abs/2407.03129v1'>http://arxiv.org/abs/2407.03129v1</a></p>
<p><b>Compressor summary</b>: The paper explores how different prompts affect large language models' social biases and task performance, showing that they are highly sensitive to prompts and have tradeoffs between them.</p><hr><h3>Foundations and Frontiers of Graph Learning Theory</h3>
<p><a href='http://arxiv.org/abs/2407.03125v1'>http://arxiv.org/abs/2407.03125v1</a></p>
<p><b>Compressor summary</b>: The article summarizes the theoretical foundations and recent advances in graph learning models, focusing on their expressiveness, generalization, optimization, and unique phenomena.</p><hr><h3>Can machine learning solve the challenge of adaptive learning and the  individualization of learning paths? A field experiment in an online learning  platform</h3>
<p><a href='http://arxiv.org/abs/2407.03118v1'>http://arxiv.org/abs/2407.03118v1</a></p>
<p><b>Compressor summary</b>: The authors test an algorithm for personalized learning paths using convolutional neural networks on a large digital self-learning platform, and find that it does not significantly improve learners' effort or performance compared to group-based or individual non-adaptive treatments.</p><hr><h3>$L_p$-norm Distortion-Efficient Adversarial Attack</h3>
<p><a href='http://arxiv.org/abs/2407.03115v1'>http://arxiv.org/abs/2407.03115v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new adversarial attack method that reduces $L_0$-norm distortion while maintaining low $L_2$-norm loss, making the perturbations sparse and imperceptible to humans.</p><hr><h3>How Reliable and Stable are Explanations of XAI Methods?</h3>
<p><a href='http://arxiv.org/abs/2407.03108v1'>http://arxiv.org/abs/2407.03108v1</a></p>
<p><b>Compressor summary</b>: This paper evaluates the reliability and stability of various explainable AI (XAI) methods using a diabetes dataset and four machine learning models, finding eXirt as the most reliable XAI method and all others sensitive to perturbations except one.</p><hr><h3>Anti-Collapse Loss for Deep Metric Learning Based on Coding Rate Metric</h3>
<p><a href='http://arxiv.org/abs/2407.03106v1'>http://arxiv.org/abs/2407.03106v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new loss function called Anti-Collapse Loss for deep metric learning that improves feature representation, avoids embedding space collapse, and enhances model performance.</p><hr><h3>On Generalization for Generative Flow Networks</h3>
<p><a href='http://arxiv.org/abs/2407.03105v1'>http://arxiv.org/abs/2407.03105v1</a></p>
<p><b>Compressor summary</b>: GFlowNets are a learning paradigm that samples from an unnormalized probability distribution and can learn complex patterns, but the paper investigates how they generalize to novel, longer trajectories.</p><hr><h3>KeyVideoLLM: Towards Large-scale Video Keyframe Selection</h3>
<p><a href='http://arxiv.org/abs/2407.03104v1'>http://arxiv.org/abs/2407.03104v1</a></p>
<p><b>Compressor summary</b>: KeyVideoLLM is a method to efficiently select keyframes from videos for large language models, improving data management, speed, and video question-answering performance.</p><hr><h3>Cactus: Towards Psychological Counseling Conversations using Cognitive  Behavioral Theory</h3>
<p><a href='http://arxiv.org/abs/2407.03103v1'>http://arxiv.org/abs/2407.03103v1</a></p>
<p><b>Compressor summary</b>: Cactus is a realistic dialogue dataset for training open-source language models as psychological counselors using Cognitive Behavioral Therapy techniques.</p><hr><h3>Conformal Prediction for Causal Effects of Continuous Treatments</h3>
<p><a href='http://arxiv.org/abs/2407.03094v1'>http://arxiv.org/abs/2407.03094v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for predicting causal effects of continuous treatments using conformal prediction, accounting for uncertainty in propensity score estimation, and demonstrates its effectiveness on synthetic and real datasets.</p><hr><h3>Stable Heterogeneous Treatment Effect Estimation across  Out-of-Distribution Populations</h3>
<p><a href='http://arxiv.org/abs/2407.03082v1'>http://arxiv.org/abs/2407.03082v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new framework (SBRL-HAP) for estimating treatment effects that works well both on in-distribution and out-of-distribution data, addressing selection bias and distribution shift issues.</p><hr><h3>Artificial Inductive Bias for Synthetic Tabular Data Generation in  Data-Scarce Scenarios</h3>
<p><a href='http://arxiv.org/abs/2407.03080v1'>http://arxiv.org/abs/2407.03080v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel way to generate realistic synthetic tabular data using Deep Generative Models with artificial inductive bias from transfer learning and meta-learning techniques, improving the quality of the synthetic data in limited real-data environments.</p><hr><h3>A Case Study on Context-Aware Neural Machine Translation with Multi-Task  Learning</h3>
<p><a href='http://arxiv.org/abs/2407.03076v1'>http://arxiv.org/abs/2407.03076v1</a></p>
<p><b>Compressor summary</b>: The paper investigates multi-task learning for document-level neural machine translation to make the model sensitive to the choice of context and shows better performance in low-resource settings, but struggles with generating source from context.</p><hr><h3>Warm-up Free Policy Optimization: Improved Regret in Linear Markov  Decision Processes</h3>
<p><a href='http://arxiv.org/abs/2407.03065v1'>http://arxiv.org/abs/2407.03065v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a policy optimization algorithm that eliminates a warm-up phase and achieves rate-optimal regret in different reinforcement learning settings.</p><hr><h3>ALTER: Augmentation for Large-Table-Based Reasoning</h3>
<p><a href='http://arxiv.org/abs/2407.03061v1'>http://arxiv.org/abs/2407.03061v1</a></p>
<p><b>Compressor summary</b>: ALTER is a framework that enhances large language models' table-based reasoning by augmenting NL questions and tables with relevant information.</p><hr><h3>FairJob: A Real-World Dataset for Fairness in Online Systems</h3>
<p><a href='http://arxiv.org/abs/2407.03059v1'>http://arxiv.org/abs/2407.03059v1</a></p>
<p><b>Compressor summary</b>: The text introduces a fairness-aware dataset for job recommendation in advertising, which preserves predictive power and addresses the challenge of balancing fairness and utility in high-impact domains.</p><hr><h3>Improving Zero-shot Generalization of Learned Prompts via Unsupervised  Knowledge Distillation</h3>
<p><a href='http://arxiv.org/abs/2407.03056v1'>http://arxiv.org/abs/2407.03056v1</a></p>
<p><b>Compressor summary</b>: Key points:
- KDPL is a novel approach to prompt learning based on unsupervised knowledge distillation from more powerful models
- It eliminates the need for labeled examples during adaptation and improves generalization of learned prompts
- It can transfer knowledge even without knowing training class names

Summary:
KDPL is an unsupervised method that learns to adapt prompts from stronger models, enhancing zero-shot generalization and transferability without labels.</p><hr><h3>Improving Conversational Abilities of Quantized Large Language Models  via Direct Preference Alignment</h3>
<p><a href='http://arxiv.org/abs/2407.03051v1'>http://arxiv.org/abs/2407.03051v1</a></p>
<p><b>Compressor summary</b>: The paper proposes QDPO, a new technique that aligns quantized large language models with their full-precision versions, improving chatbot performance and efficiency.</p><hr><h3>Enhancements for Real-Time Monte-Carlo Tree Search in General Video Game  Playing</h3>
<p><a href='http://arxiv.org/abs/2407.03049v1'>http://arxiv.org/abs/2407.03049v1</a></p>
<p><b>Compressor summary</b>: The paper proposes eight enhancements for Monte-Carlo Tree Search (MCTS) in General Video Game Playing (GVGP), which improve win percentages and approach competitive levels with existing agents.</p><hr><h3>SlerpFace: Face Template Protection via Spherical Linear Interpolation</h3>
<p><a href='http://arxiv.org/abs/2407.03043v1'>http://arxiv.org/abs/2407.03043v1</a></p>
<p><b>Compressor summary</b>: The paper proposes SlerpFace, a novel face template protection technique that rotates and drops out features of face templates to prevent identity-preserving synthetic face image attacks using diffusion models.</p><hr><h3>Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction  Tuning for Large Language Model</h3>
<p><a href='http://arxiv.org/abs/2407.03040v1'>http://arxiv.org/abs/2407.03040v1</a></p>
<p><b>Compressor summary</b>: The R2S framework uses CoD-Chain of Dialogue logic to guide LLMs in generating knowledge-intensive multi-turn dialogues for instruction tuning, covering diverse domains like Wikipedia (English), Science (Chinese), and Artifacts (Chinese).</p><hr><h3>SAFT: Towards Out-of-Distribution Generalization in Fine-Tuning</h3>
<p><a href='http://arxiv.org/abs/2407.03036v1'>http://arxiv.org/abs/2407.03036v1</a></p>
<p><b>Compressor summary</b>: SAFT is a simple method that improves CLIP's performance on out-of-distribution data by only updating important parameters during fine-tuning.</p><hr><h3>ISWSST: Index-space-wave State Superposition Transformers for  Multispectral Remotely Sensed Imagery Semantic Segmentation</h3>
<p><a href='http://arxiv.org/abs/2407.03033v1'>http://arxiv.org/abs/2407.03033v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new method called ISWSST that uses quantum mechanics ideas to improve semantic segmentation of multispectral imagery, addressing several issues in the current approaches and achieving better accuracy.</p><hr><h3>Strategies for Arabic Readability Modeling</h3>
<p><a href='http://arxiv.org/abs/2407.03032v1'>http://arxiv.org/abs/2407.03032v1</a></p>
<p><b>Compressor summary</b>: The paper presents experimental results on Arabic readability assessment using various methods, achieving good scores by combining different techniques.</p><hr><h3>Exploiting Dialect Identification in Automatic Dialectal Text  Normalization</h3>
<p><a href='http://arxiv.org/abs/2407.03020v1'>http://arxiv.org/abs/2407.03020v1</a></p>
<p><b>Compressor summary</b>: The paper introduces the task of CODAfication, which normalizes Dialectal Arabic into a standardized written form, and presents new models and methods to improve its performance.</p><hr><h3>An Organism Starts with a Single Pix-Cell: A Neural Cellular Diffusion  for High-Resolution Image Synthesis</h3>
<p><a href='http://arxiv.org/abs/2407.03018v1'>http://arxiv.org/abs/2407.03018v1</a></p>
<p><b>Compressor summary</b>: The paper introduces GeCA, a novel generative model inspired by biological evolution that improves retinal disease classification in Fundus and OCT images.</p><hr><h3>Context-Aware Video Instance Segmentation</h3>
<p><a href='http://arxiv.org/abs/2407.03010v1'>http://arxiv.org/abs/2407.03010v1</a></p>
<p><b>Compressor summary</b>: The paper presents CAVIS, a framework that uses contextual information to improve object tracking and instance matching in video segmentation tasks, achieving state-of-the-art results, especially on difficult videos.</p><hr><h3>Model Guidance via Explanations Turns Image Classifiers into  Segmentation Models</h3>
<p><a href='http://arxiv.org/abs/2407.03009v1'>http://arxiv.org/abs/2407.03009v1</a></p>
<p><b>Compressor summary</b>: The text discusses how heatmaps from image classification networks can be used for weakly supervised segmentation and improved interpretability, and proposes a novel semi-supervised segmentation method using differentiable heatmap architectures.</p><hr><h3>Align and Aggregate: Compositional Reasoning with Video Alignment and  Answer Aggregation for Video Question-Answering</h3>
<p><a href='http://arxiv.org/abs/2407.03008v1'>http://arxiv.org/abs/2407.03008v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a model-agnostic framework for Video Question-Answering that enhances compositional reasoning by integrating video aligner and answer aggregator modules, and evaluates it on various datasets using new metrics and an automatic question decomposition pipeline.</p><hr><h3>What Affects the Stability of Tool Learning? An Empirical Study on the  Robustness of Tool Learning Frameworks</h3>
<p><a href='http://arxiv.org/abs/2407.03007v1'>http://arxiv.org/abs/2407.03007v1</a></p>
<p><b>Compressor summary</b>: This paper investigates how different factors affect the performance of tool learning methods in large language models and offers insights for improving their practical use.</p><hr><h3>Frequency-Controlled Diffusion Model for Versatile Text-Guided  Image-to-Image Translation</h3>
<p><a href='http://arxiv.org/abs/2407.03006v1'>http://arxiv.org/abs/2407.03006v1</a></p>
<p><b>Compressor summary</b>: The paper presents FCDiffusion, a diffusion-based framework for text-guided image-to-image translation using Discrete Cosine Transform to filter latent features and control different aspects of the translation.</p><hr><h3>Human-like Linguistic Biases in Neural Speech Models: Phonetic  Categorization and Phonotactic Constraints in Wav2Vec2.0</h3>
<p><a href='http://arxiv.org/abs/2407.03005v1'>http://arxiv.org/abs/2407.03005v1</a></p>
<p><b>Compressor summary</b>: The study examines how Wav2Vec2, a deep neural speech model, processes and resolves phonotactic constraints in ambiguous sounds and finds that this ability emerges early in the model's Transformer module.</p><hr><h3>SemioLLM: Assessing Large Language Models for Semiological Analysis in  Epilepsy Research</h3>
<p><a href='http://arxiv.org/abs/2407.03004v1'>http://arxiv.org/abs/2407.03004v1</a></p>
<p><b>Compressor summary</b>: The study semioLLM evaluates how well large language models can diagnose epilepsy using text descriptions of seizures, revealing both their strengths and limitations for clinical applications.</p><hr><h3>VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values</h3>
<p><a href='http://arxiv.org/abs/2407.03000v1'>http://arxiv.org/abs/2407.03000v1</a></p>
<p><b>Compressor summary</b>: VIVA is a benchmark that tests vision-language models' ability to use human values to make decisions in real-world situations, revealing their limitations and potential improvements.</p><hr><h3>Are Large Language Models Consistent over Value-laden Questions?</h3>
<p><a href='http://arxiv.org/abs/2407.02996v1'>http://arxiv.org/abs/2407.02996v1</a></p>
<p><b>Compressor summary</b>: The study examines value consistency of large language models across various scenarios and topics, finding them relatively consistent except on controversial topics.</p><hr><h3>Graph and Skipped Transformer: Exploiting Spatial and Temporal Modeling  Capacities for Efficient 3D Human Pose Estimation</h3>
<p><a href='http://arxiv.org/abs/2407.02990v1'>http://arxiv.org/abs/2407.02990v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new efficient method for 3D Human Pose Estimation using a Graph and Skipped Transformer architecture that exploits spatio-temporal information and achieves superior performance with reduced computational complexity.</p><hr><h3>YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision</h3>
<p><a href='http://arxiv.org/abs/2407.02988v1'>http://arxiv.org/abs/2407.02988v1</a></p>
<p><b>Compressor summary</b>: The paper reviews YOLO object detection algorithms, focusing on their improvements and suitability for edge deployment.</p><hr><h3>LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content  Moderation of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.02987v1'>http://arxiv.org/abs/2407.02987v1</a></p>
<p><b>Compressor summary</b>: LoRA-Guard is a method to adapt guardrails for content moderation on resource-constrained devices like mobile phones by sharing knowledge between LLMs and guardrail models.</p><hr><h3>Semantically Rich Local Dataset Generation for Explainable AI in  Genomics</h3>
<p><a href='http://arxiv.org/abs/2407.02984v1'>http://arxiv.org/abs/2407.02984v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using Genetic Programming to generate datasets with semantic variability for interpreting black box deep learning models in gene regulation, achieving good diversity and outperforming a random baseline.</p><hr><h3>Mast Kalandar at SemEval-2024 Task 8: On the Trail of Textual Origins:  RoBERTa-BiLSTM Approach to Detect AI-Generated Text</h3>
<p><a href='http://arxiv.org/abs/2407.02978v1'>http://arxiv.org/abs/2407.02978v1</a></p>
<p><b>Compressor summary</b>: This paper presents a model to classify AI-generated or human text and evaluates its effectiveness, addressing concerns about machine-generated text misuse in various contexts.</p><hr><h3>Large Language Models as Evaluators for Scientific Synthesis</h3>
<p><a href='http://arxiv.org/abs/2407.02977v1'>http://arxiv.org/abs/2407.02977v1</a></p>
<p><b>Compressor summary</b>: The study tests how well LLMs like GPT-4 and Mistral can evaluate scientific summaries by comparing their judgments to human annotators, finding weak correlation between them.</p><hr><h3>Unified Anomaly Detection methods on Edge Device using Knowledge  Distillation and Quantization</h3>
<p><a href='http://arxiv.org/abs/2407.02968v1'>http://arxiv.org/abs/2407.02968v1</a></p>
<p><b>Compressor summary</b>: This paper proposes and tests lightweight multi-class anomaly detection models for visual inspection systems, showing that they can be deployed on edge devices with low latency and memory requirements.</p><hr><h3>FSM: A Finite State Machine Based Zero-Shot Prompting Paradigm for  Multi-Hop Question Answering</h3>
<p><a href='http://arxiv.org/abs/2407.02964v1'>http://arxiv.org/abs/2407.02964v1</a></p>
<p><b>Compressor summary</b>: The proposed Finite State Machine prompting method enhances large language models' reasoning capabilities for complex tasks by iteratively decomposing questions into sub-questions and self-correcting, improving accuracy and trustworthiness.</p><hr><h3>Towards a Scalable Reference-Free Evaluation of Generative Models</h3>
<p><a href='http://arxiv.org/abs/2407.02961v1'>http://arxiv.org/abs/2407.02961v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a fast and interpretable method (FKEA) to evaluate the diversity of generated data using random Fourier features and kernel approximation, which can handle large-scale generative models.</p><hr><h3>3D Multimodal Image Registration for Plant Phenotyping</h3>
<p><a href='http://arxiv.org/abs/2407.02946v1'>http://arxiv.org/abs/2407.02946v1</a></p>
<p><b>Compressor summary</b>: The text describes a novel 3D image registration method that uses depth information from a time-of-flight camera to accurately align images from different cameras, improving the assessment of plant phenotypes.</p><hr><h3>VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using  Learned Priors</h3>
<p><a href='http://arxiv.org/abs/2407.02945v1'>http://arxiv.org/abs/2407.02945v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a method for extrapolated view synthesis in urban scenes using LiDAR and prior knowledge, improving rendering quality for views outside the training camera distribution.</p><hr><h3>Probing the Feasibility of Multilingual Speaker Anonymization</h3>
<p><a href='http://arxiv.org/abs/2407.02937v1'>http://arxiv.org/abs/2407.02937v1</a></p>
<p><b>Compressor summary</b>: The study applies a multilingual anonymization system to nine languages, achieving robust results with varying quality of speech synthesis components.</p><hr><h3>GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.02936v1'>http://arxiv.org/abs/2407.02936v1</a></p>
<p><b>Compressor summary</b>: GraCoRe is a benchmark for evaluating large language models' graph comprehension and reasoning abilities across various types of graphs and tasks, revealing insights into their performance and limitations.</p><hr><h3>PosMLP-Video: Spatial and Temporal Relative Position Encoding for  Efficient Video Recognition</h3>
<p><a href='http://arxiv.org/abs/2407.02934v1'>http://arxiv.org/abs/2407.02934v1</a></p>
<p><b>Compressor summary</b>: PosMLP-Video is a lightweight MLP-like backbone for video recognition that uses relative positional encoding and spatio-temporal factorized positional MLP blocks to achieve competitive performance on image understanding tasks.</p><hr><h3>EgoFlowNet: Non-Rigid Scene Flow from Point Clouds with Ego-Motion  Support</h3>
<p><a href='http://arxiv.org/abs/2407.02920v1'>http://arxiv.org/abs/2407.02920v1</a></p>
<p><b>Compressor summary</b>: EgoFlowNet is a point-level scene flow estimation network that predicts a binary mask and uses all input points for ego-motion and scene flow estimation, improving performance over existing methods on realistic KITTI scenes.</p><hr><h3>Free-SurGS: SfM-Free 3D Gaussian Splatting for Surgical Scene  Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2407.02918v1'>http://arxiv.org/abs/2407.02918v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for real-time 3D reconstruction of surgical scenes without using Structure-from-Motion, by leveraging optical flow priors and scene consistency checks.</p><hr><h3>Towards Negotiative Dialogue for the Talkamatic Dialogue Manager</h3>
<p><a href='http://arxiv.org/abs/2407.02917v1'>http://arxiv.org/abs/2407.02917v1</a></p>
<p><b>Compressor summary</b>: The paper explores various aspects of negotiation-based conversations using a preliminary version of the Talkamatic Dialogue Manager.</p><hr><h3>The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design  Trade-Offs in Ensemble Learning Systems</h3>
<p><a href='http://arxiv.org/abs/2407.02914v1'>http://arxiv.org/abs/2407.02914v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes how ensemble learning in machine learning affects accuracy and energy consumption, and suggests designing small ensembles with subset-based training, majority voting, and energy-efficient algorithms for a green AI approach.</p><hr><h3>SFC: Achieve Accurate Fast Convolution under Low-precision Arithmetic</h3>
<p><a href='http://arxiv.org/abs/2407.02913v1'>http://arxiv.org/abs/2407.02913v1</a></p>
<p><b>Compressor summary</b>: SFC is a new algorithm that improves quantized convolution efficiency by extending DFT with symbolic computing and introducing correction terms, achieving 3.68x reduction in multiplication and maintaining accuracy.</p><hr><h3>Domain-independent detection of known anomalies</h3>
<p><a href='http://arxiv.org/abs/2407.02910v1'>http://arxiv.org/abs/2407.02910v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Industrial quality inspection faces challenges in detecting anomalies with sparse data and unseen objects.
- Hybrid task of domain generalization on sparse classes introduced.
- Three new datasets based on MVTec AD modified and presented.
- Embedding-based approaches (SEMLP and Labeled PatchCore) designed and tested.
- SEMLP achieves best performance with 87.2% AUROC on average.

Summary:
The paper introduces a new hybrid task and datasets for anomaly detection in industrial quality inspection, and proposes embedding-based methods that outperform existing approaches with 87.2% accuracy.</p><hr><h3>Single Image Rolling Shutter Removal with Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2407.02906v1'>http://arxiv.org/abs/2407.02906v1</a></p>
<p><b>Compressor summary</b>: RS-Diffusion is a new method to correct Rolling Shutter artifacts in single images using diffusion techniques and patch-attention, and introduces a new dataset with ground-truth data.</p><hr><h3>Translatotron-V(ison): An End-to-End Model for In-Image Machine  Translation</h3>
<p><a href='http://arxiv.org/abs/2407.02894v1'>http://arxiv.org/abs/2407.02894v1</a></p>
<p><b>Compressor summary</b>: Translatotron-V is an end-to-end image translation model that uses target text decoding and visual tokenization to reduce the language alignment burden and improve performance while preserving visual features.</p><hr><h3>An Uncertainty-guided Tiered Self-training Framework for Active  Source-free Domain Adaptation in Prostate Segmentation</h3>
<p><a href='http://arxiv.org/abs/2407.02893v1'>http://arxiv.org/abs/2407.02893v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method called UGTST that selectively annotates a few target domain samples to improve prostate segmentation in cross-center medical images using deep learning models.</p><hr><h3>GPTQT: Quantize Large Language Models Twice to Push the Efficiency</h3>
<p><a href='http://arxiv.org/abs/2407.02891v1'>http://arxiv.org/abs/2407.02891v1</a></p>
<p><b>Compressor summary</b>: The paper presents a novel post-training quantization method, GPTQT, that reduces memory usage and speeds up processing in large language models by using progressive two-step linear and binary coding.</p><hr><h3>Explicitly Guided Information Interaction Network for Cross-modal Point  Cloud Completion</h3>
<p><a href='http://arxiv.org/abs/2407.02887v1'>http://arxiv.org/abs/2407.02887v1</a></p>
<p><b>Compressor summary</b>: EGIInet is a novel framework that efficiently combines 2D and 3D information for point cloud completion using a unified encoding process and an explicitly guided information interaction strategy, achieving state-of-the-art results with fewer parameters.</p><hr><h3>ShiftAddAug: Augment Multiplication-Free Tiny Neural Network with Hybrid  Computation</h3>
<p><a href='http://arxiv.org/abs/2407.02881v1'>http://arxiv.org/abs/2407.02881v1</a></p>
<p><b>Compressor summary</b>: ShiftAddAug improves the accuracy of neural networks using multiplication-free operators by augmenting them with costly multiplication and a novel weight sharing method, achieving significant gains in image classification and semantic segmentation tasks.</p><hr><h3>Knowledge Composition using Task Vectors with Learned Anisotropic  Scaling</h3>
<p><a href='http://arxiv.org/abs/2407.02880v1'>http://arxiv.org/abs/2407.02880v1</a></p>
<p><b>Compressor summary</b>: aTLAS algorithm combines pre-trained model components to enhance knowledge composition and transfer using linear combinations of parameter blocks with different learned coefficients.</p><hr><h3>Membership Inference Attacks Against Time-Series Models</h3>
<p><a href='http://arxiv.org/abs/2407.02870v1'>http://arxiv.org/abs/2407.02870v1</a></p>
<p><b>Compressor summary</b>: The authors propose new features for assessing privacy risks in time-series models using seasonality and trend components estimated from health data.</p><hr><h3>Fast maneuver recovery from aerial observation: trajectory clustering  and outliers rejection</h3>
<p><a href='http://arxiv.org/abs/2407.02863v1'>http://arxiv.org/abs/2407.02863v1</a></p>
<p><b>Compressor summary</b>: The text describes a data-driven approach to model realistic and diverse behaviors of road users in multi-agent simulations, using clustering methods on raw data from different environments.</p><hr><h3>A Self-Supervised Task for Fault Detection in Satellite Multivariate  Time Series</h3>
<p><a href='http://arxiv.org/abs/2407.02861v1'>http://arxiv.org/abs/2407.02861v1</a></p>
<p><b>Compressor summary</b>: The proposed method uses Physics-Informed Real NVP neural networks with self-supervised training to enhance fault detection in satellite multivariate time series, showing significant performance improvements and potential for other applications.</p><hr><h3>Early-Stage Anomaly Detection: A Study of Model Performance on Complete  vs. Partial Flows</h3>
<p><a href='http://arxiv.org/abs/2407.02856v1'>http://arxiv.org/abs/2407.02856v1</a></p>
<p><b>Compressor summary</b>: Machine learning models perform worse on incomplete network data and need at least 7 packets in the test set for reliable anomaly detection.</p><hr><h3>Universal Gloss-level Representation for Gloss-free Sign Language  Translation and Production</h3>
<p><a href='http://arxiv.org/abs/2407.02854v1'>http://arxiv.org/abs/2407.02854v1</a></p>
<p><b>Compressor summary</b>: UniGloR is a self-supervised method for sign language translation and production that works without gloss annotations and achieves good results on multiple tasks.</p><hr><h3>Plant Doctor: A hybrid machine learning and image segmentation software  to quantify plant damage in video footage</h3>
<p><a href='http://arxiv.org/abs/2407.02853v1'>http://arxiv.org/abs/2407.02853v1</a></p>
<p><b>Compressor summary</b>: Plant Doctor is an AI system that uses video footage to diagnose and track leaf damage in urban street plants, helping control disease spread in cities.</p><hr><h3>Multi-Task Domain Adaptation for Language Grounding with 3D Objects</h3>
<p><a href='http://arxiv.org/abs/2407.02846v1'>http://arxiv.org/abs/2407.02846v1</a></p>
<p><b>Compressor summary</b>: The text proposes a novel method called DA4LG for language grounding with 3D objects, which uses multi-task learning to align vision and language across domains.</p><hr><h3>MindBench: A Comprehensive Benchmark for Mind Map Structure Recognition  and Analysis</h3>
<p><a href='http://arxiv.org/abs/2407.02842v1'>http://arxiv.org/abs/2407.02842v1</a></p>
<p><b>Compressor summary</b>: MindBench is a new benchmark for structured document analysis that includes various tasks and challenges to improve current models' performance.</p><hr><h3>Comparing Feature-based and Context-aware Approaches to PII  Generalization Level Prediction</h3>
<p><a href='http://arxiv.org/abs/2407.02837v1'>http://arxiv.org/abs/2407.02837v1</a></p>
<p><b>Compressor summary</b>: The paper proposes two methods to protect personal data in texts: a feature-based one and a context-aware one using Multilingual-BERT, which performs better and considers semantic relationships.</p><hr><h3>A Pairwise DomMix Attentive Adversarial Network for Unsupervised Domain  Adaptive Object Detection</h3>
<p><a href='http://arxiv.org/abs/2407.02835v1'>http://arxiv.org/abs/2407.02835v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new unsupervised domain adaptation method for object detection that uses a pairwise attentive adversarial network with a Domain Mixup module to align features from different domains and improve adaptation.</p><hr><h3>Aspect-Based Sentiment Analysis Techniques: A Comparative Study</h3>
<p><a href='http://arxiv.org/abs/2407.02834v1'>http://arxiv.org/abs/2407.02834v1</a></p>
<p><b>Compressor summary</b>: The text discusses how Aspect-based Sentiment Analysis, a method that analyzes specific aspects of customer feedback, is important for businesses to understand market trends and improve their competitive edge.</p><hr><h3>Style Alignment based Dynamic Observation Method for UAV-View  Geo-localization</h3>
<p><a href='http://arxiv.org/abs/2407.02832v1'>http://arxiv.org/abs/2407.02832v1</a></p>
<p><b>Compressor summary</b>: Key points:
- UAV-view geo-localization is to match drone images with satellite images for localization
- The paper proposes a style alignment method and a dynamic observation module
- The method transforms visual style, reduces noise, and uses deconstruction loss
- The method achieves state-of-the-art performance on benchmarked datasets

Summary:
The paper presents a novel method for UAV-view geo-localization that aligns visual styles, controls noise, and uses deconstruction loss to outperform previous methods.</p><hr><h3>A Radiometric Correction based Optical Modeling Approach to Removing  Reflection Noise in TLS Point Clouds of Urban Scenes</h3>
<p><a href='http://arxiv.org/abs/2407.02830v1'>http://arxiv.org/abs/2407.02830v1</a></p>
<p><b>Compressor summary</b>: This paper presents an algorithm that removes reflection noise from TLS point clouds, improving 3D vision tasks in urban environments with reflective surfaces.</p><hr><h3>Convergence of Implicit Gradient Descent for Training Two-Layer  Physics-Informed Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2407.02827v1'>http://arxiv.org/abs/2407.02827v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes implicit gradient descent for training two-layer physics-informed neural networks and shows it converges faster and more reliably than common gradient descent.</p><hr><h3>Representation learning with CGAN for casual inference</h3>
<p><a href='http://arxiv.org/abs/2407.02825v1'>http://arxiv.org/abs/2407.02825v1</a></p>
<p><b>Compressor summary</b>: This paper introduces a new method for finding representation learning functions using CGANs for causal inference when two distributions are balanced.</p><hr><h3>Effect of a Process Mining based Pre-processing Step in Prediction of  the Critical Health Outcomes</h3>
<p><a href='http://arxiv.org/abs/2407.02821v1'>http://arxiv.org/abs/2407.02821v1</a></p>
<p><b>Compressor summary</b>: The concatenation pre-processing algorithm improves data quality, process model fit, and critical health outcome predictions by reducing dataset complexities in healthcare datasets.</p><hr><h3>Investigating the Contextualised Word Embedding Dimensions Responsible  for Contextual and Temporal Semantic Changes</h3>
<p><a href='http://arxiv.org/abs/2407.02820v1'>http://arxiv.org/abs/2407.02820v1</a></p>
<p><b>Compressor summary</b>: The paper studies how sense-aware contextual word embeddings encode semantic changes in different contexts and dimensions, using fine-tuned language models and various analyses.</p><hr><h3>Efficient Training of Language Models with Compact and Consistent Next  Token Distributions</h3>
<p><a href='http://arxiv.org/abs/2407.02819v1'>http://arxiv.org/abs/2407.02819v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a faster way to train language models by pre-aggregating the corpus with a collapsed n-gram distribution, which improves model quality and convergence rate.</p><hr><h3>Images Speak Louder than Words: Understanding and Mitigating Bias in  Vision-Language Model from a Causal Mediation Perspective</h3>
<p><a href='http://arxiv.org/abs/2407.02814v1'>http://arxiv.org/abs/2407.02814v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework that uses causal mediation analysis to understand and reduce biases in vision-language models by focusing on image features, which have the largest impact on bias.</p><hr><h3>Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm  and Compiler Co-Design</h3>
<p><a href='http://arxiv.org/abs/2407.02813v1'>http://arxiv.org/abs/2407.02813v1</a></p>
<p><b>Compressor summary</b>: The text proposes a method called Dy-DCA that uses a dynamic deep neural network and content-aware data processing to improve video quality and efficiency, reducing model number and optimizing compilation for real-time performance on mobile devices.</p><hr><h3>SPLITZ: Certifiable Robustness via Split Lipschitz Randomized Smoothing</h3>
<p><a href='http://arxiv.org/abs/2407.02811v1'>http://arxiv.org/abs/2407.02811v1</a></p>
<p><b>Compressor summary</b>: The paper introduces 	extit{SPLITZ}, a novel method that splits classifiers into two parts, constrains the Lipschitz constant of one part and smooths the other, to improve robustness against adversarial examples.</p><hr><h3>Euler's Elastica Based Cartoon-Smooth-Texture Image Decomposition</h3>
<p><a href='http://arxiv.org/abs/2407.02794v1'>http://arxiv.org/abs/2407.02794v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new method to decompose grayscale images into three parts: structural, smooth, and oscillatory, using regularization terms and an efficient algorithm.</p><hr><h3>52B to 1T: Lessons Learned via Tele-FLM Series</h3>
<p><a href='http://arxiv.org/abs/2407.02783v1'>http://arxiv.org/abs/2407.02783v1</a></p>
<p><b>Compressor summary</b>: The report explores the potential of very large language models (>50B parameters) through supervised fine-tuning and progressive growth experiments, and shares an open-source 1T model checkpoint.</p><hr><h3>Croppable Knowledge Graph Embedding</h3>
<p><a href='http://arxiv.org/abs/2407.02779v1'>http://arxiv.org/abs/2407.02779v1</a></p>
<p><b>Compressor summary</b>: MED is a framework for training one KGE model that can serve multiple scenarios with different dimensional requirements by cropping sub-models without additional training, using mutual learning, evolutionary improvement, and dynamic loss weighting to enhance performance.</p><hr><h3>Foster Adaptivity and Balance in Learning with Noisy Labels</h3>
<p><a href='http://arxiv.org/abs/2407.02778v1'>http://arxiv.org/abs/2407.02778v1</a></p>
<p><b>Compressor summary</b>: Our proposed method, SED, handles label noise in a self-adaptive and class-balanced way by using a novel sample selection strategy, mean-teacher model, sample re-weighting mechanism, and consistency regularization.</p><hr><h3>A Framework for Quantum Finite-State Languages with Density Mapping</h3>
<p><a href='http://arxiv.org/abs/2407.02776v1'>http://arxiv.org/abs/2407.02776v1</a></p>
<p><b>Compressor summary</b>: The text introduces a framework for building and simulating quantum finite-state automata (QFAs) using predefined construction methods and improving accuracy on noisy quantum computers.</p><hr><h3>MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language  Models</h3>
<p><a href='http://arxiv.org/abs/2407.02775v1'>http://arxiv.org/abs/2407.02775v1</a></p>
<p><b>Compressor summary</b>: MLKD-BERT is a novel method that improves knowledge distillation by exploring multi-level knowledge and adjusting student attention heads, leading to better performance and faster inference on BERT models.</p><hr><h3>Automatic gradient descent with generalized Newton's method</h3>
<p><a href='http://arxiv.org/abs/2407.02772v1'>http://arxiv.org/abs/2407.02772v1</a></p>
<p><b>Compressor summary</b>: The generalized Newton's method (GeN) is a Hessian-informed optimizer that automatically selects the learning rate for faster convergence without tuning, and performs well on language and vision tasks.</p>