
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-09-06</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-09-06 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>DC-Solver: Improving Predictor-Corrector Diffusion Sampler via Dynamic  Compensation</h3>
<p><a href='http://arxiv.org/abs/2409.03755v1'>http://arxiv.org/abs/2409.03755v1</a></p>
<p><b>Compressor summary</b>: The paper introduces DC-Solver, a fast diffusion probabilistic model sampler that uses dynamic compensation to mitigate misalignment and improve sampling quality on unconditional and conditional tasks with different resolutions.</p><hr><h3>WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild</h3>
<p><a href='http://arxiv.org/abs/2409.03753v1'>http://arxiv.org/abs/2409.03753v1</a></p>
<p><b>Compressor summary</b>: WildVis is an interactive tool for large-scale conversation analysis that combines fast search and visualization with optimizations to handle millions of data points.</p><hr><h3>Attention Heads of Large Language Models: A Survey</h3>
<p><a href='http://arxiv.org/abs/2409.03752v1'>http://arxiv.org/abs/2409.03752v1</a></p>
<p><b>Compressor summary</b>: The paper surveys existing research on understanding the internal mechanisms of large language models, focusing on attention heads and their functions, using a four-stage framework inspired by human thought processes.</p><hr><h3>Dynamics of Supervised and Reinforcement Learning in the Non-Linear  Perceptron</h3>
<p><a href='http://arxiv.org/abs/2409.03749v1'>http://arxiv.org/abs/2409.03749v1</a></p>
<p><b>Compressor summary</b>: This paper studies how different learning rules and input data affect the learning dynamics of nonlinear perceptrons in binary classification tasks.</p><hr><h3>ArtiFade: Learning to Generate High-quality Subject from Blemished  Images</h3>
<p><a href='http://arxiv.org/abs/2409.03745v1'>http://arxiv.org/abs/2409.03745v1</a></p>
<p><b>Compressor summary</b>: ArtiFade is a text-to-image method that removes artifacts from blemished images using fine-tuning on a specialized dataset, preserving the original generative capabilities of the diffusion model.</p><hr><h3>Understanding Data Importance in Machine Learning Attacks: Does Valuable  Data Pose Greater Harm?</h3>
<p><a href='http://arxiv.org/abs/2409.03741v1'>http://arxiv.org/abs/2409.03741v1</a></p>
<p><b>Compressor summary</b>: This paper explores how valuable data samples in machine learning are more vulnerable to certain attacks, like membership inference and model stealing, and suggests using sample characteristics to improve membership metrics for better defense mechanisms.</p><hr><h3>Differentiable Discrete Event Simulation for Queuing Network Control</h3>
<p><a href='http://arxiv.org/abs/2409.03740v1'>http://arxiv.org/abs/2409.03740v1</a></p>
<p><b>Compressor summary</b>: The authors propose a scalable framework for policy optimization based on differentiable discrete event simulation, which improves sample efficiency and stability for queueing network control using reinforcement learning techniques.</p><hr><h3>LLM-CI: Assessing Contextual Integrity Norms in Language Models</h3>
<p><a href='http://arxiv.org/abs/2409.03735v1'>http://arxiv.org/abs/2409.03735v1</a></p>
<p><b>Compressor summary</b>: LLM-CI is an open-source framework that assesses privacy norms in large language models by using a Contextual Integrity-based methodology and multi-prompt assessment to account for prompt sensitivity.</p><hr><h3>Safety vs. Performance: How Multi-Objective Learning Reduces Barriers to  Market Entry</h3>
<p><a href='http://arxiv.org/abs/2409.03734v1'>http://arxiv.org/abs/2409.03734v1</a></p>
<p><b>Compressor summary</b>: The text discusses how reputational damage affects market entry barriers for large language models and proposes a multi-objective framework to study this issue.</p><hr><h3>Planning In Natural Language Improves LLM Search For Code Generation</h3>
<p><a href='http://arxiv.org/abs/2409.03733v1'>http://arxiv.org/abs/2409.03733v1</a></p>
<p><b>Compressor summary</b>: PLANSEARCH is a novel search algorithm that improves LLM inference by generating diverse natural language observations and plans for solving problems, leading to significant performance gains on coding benchmarks.</p><hr><h3>Geometry Image Diffusion: Fast and Data-Efficient Text-to-3D with  Image-Based Surface Representation</h3>
<p><a href='http://arxiv.org/abs/2409.03718v1'>http://arxiv.org/abs/2409.03718v1</a></p>
<p><b>Compressor summary</b>: GIMDiffusion is a model that converts textual descriptions into 3D objects using 2D images, leveraging Text-to-Image models for efficient and accurate generation of high-quality 3D shapes.</p><hr><h3>RAG based Question-Answering for Contextual Response Prediction System</h3>
<p><a href='http://arxiv.org/abs/2409.03708v1'>http://arxiv.org/abs/2409.03708v1</a></p>
<p><b>Compressor summary</b>: This paper presents a framework using Large Language Models with Retrieval Augmented Generation for question-answering in customer service, improving accuracy and relevance over current BERT-based methods.</p><hr><h3>A Different Level Text Protection Mechanism With Differential Privacy</h3>
<p><a href='http://arxiv.org/abs/2409.03707v1'>http://arxiv.org/abs/2409.03707v1</a></p>
<p><b>Compressor summary</b>: The article presents a word importance extraction method using BERT and evaluates its effectiveness in protecting long texts from perturbations.</p><hr><h3>LAST: Language Model Aware Speech Tokenization</h3>
<p><a href='http://arxiv.org/abs/2409.03701v1'>http://arxiv.org/abs/2409.03701v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new way to train speech tokenizers using objectives from textual language models, which improves performance in spoken language modeling and speech-to-text tasks and allows using the same pre-trained model for both speech and text inputs.</p><hr><h3>Classification and Prediction of Heart Diseases using Machine Learning  Algorithms</h3>
<p><a href='http://arxiv.org/abs/2409.03697v1'>http://arxiv.org/abs/2409.03697v1</a></p>
<p><b>Compressor summary</b>: The research compared various machine learning algorithms and found that K-Nearest Neighbor was the best at predicting heart disease using the UCI heart disease repository data set.</p><hr><h3>A New First-Order Meta-Learning Algorithm with Convergence Guarantees</h3>
<p><a href='http://arxiv.org/abs/2409.03682v1'>http://arxiv.org/abs/2409.03682v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new first-order variant of MAML for learning new tasks using prior experience, and shows its convergence, smoothness properties, and improved performance in a synthetic experiment.</p><hr><h3>TRACE-cs: Trustworthy Reasoning for Contrastive Explanations in Course  Scheduling Problems</h3>
<p><a href='http://arxiv.org/abs/2409.03671v1'>http://arxiv.org/abs/2409.03671v1</a></p>
<p><b>Compressor summary</b>: TRACE-cs is a system that combines logic and language models to answer scheduling questions and provide explanations.</p>