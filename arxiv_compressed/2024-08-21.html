
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-08-21</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-08-21 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Prompt-Guided Image-Adaptive Neural Implicit Lookup Tables for  Interpretable Image Enhancement</h3>
<p><a href='http://arxiv.org/abs/2408.11055v1'>http://arxiv.org/abs/2408.11055v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework for interpretable image enhancement using learnable filters with expressive parameters and guiding prompts, achieving better results than predefined methods.</p><hr><h3>Accelerating Goal-Conditioned RL Algorithms and Research</h3>
<p><a href='http://arxiv.org/abs/2408.11052v1'>http://arxiv.org/abs/2408.11052v1</a></p>
<p><b>Compressor summary</b>: The paper introduces JaxGCRL, a high-performance codebase for self-supervised goal-conditioned reinforcement learning, which enables faster training and experimentation on diverse environments.</p><hr><h3>FLAME: Learning to Navigate with Multimodal LLM in Urban Environments</h3>
<p><a href='http://arxiv.org/abs/2408.11051v1'>http://arxiv.org/abs/2408.11051v1</a></p>
<p><b>Compressor summary</b>: FLAME is a novel Multimodal LLM-based agent that efficiently handles urban VLN tasks by adapting to multiple observations using a three-phase tuning technique and synthesizing augmented datasets.</p><hr><h3>MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context  Generation with Speculative Decoding</h3>
<p><a href='http://arxiv.org/abs/2408.11049v1'>http://arxiv.org/abs/2408.11049v1</a></p>
<p><b>Compressor summary</b>: MagicDec is a technique that improves low-latency, high-throughput inference for large language models by using speculative decoding and intelligent drafting strategies.</p><hr><h3>Inside the Black Box: Detecting Data Leakage in Pre-trained Language  Encoders</h3>
<p><a href='http://arxiv.org/abs/2408.11046v1'>http://arxiv.org/abs/2408.11046v1</a></p>
<p><b>Compressor summary</b>: The paper explores privacy risks of pre-trained language models, finding that membership leakage can occur even with black-box outputs, which is a greater risk than previously thought.</p><hr><h3>GraphFSA: A Finite State Automaton Framework for Algorithmic Learning on  Graphs</h3>
<p><a href='http://arxiv.org/abs/2408.11042v1'>http://arxiv.org/abs/2408.11042v1</a></p>
<p><b>Compressor summary</b>: GraphFSA is a framework for machine learning that learns finite state automata to run on each node of a given graph, enabling representation of complex graph algorithms with strong generalization and extrapolation abilities.</p><hr><h3>Transfusion: Predict the Next Token and Diffuse Images with One  Multi-Modal Model</h3>
<p><a href='http://arxiv.org/abs/2408.11039v1'>http://arxiv.org/abs/2408.11039v1</a></p>
<p><b>Compressor summary</b>: Transfusion trains a multi-modal transformer using language modeling and diffusion over mixed text and image data, achieving good results on uni- and cross-modal benchmarks.</p><hr><h3>Atmospheric Transport Modeling of CO$_2$ with Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2408.11032v1'>http://arxiv.org/abs/2408.11032v1</a></p>
<p><b>Compressor summary</b>: This study tests four deep neural networks for their ability to accurately model CO2 distribution in the atmosphere, showing promising results with SwinTransformer achieving near perfect emulation skill.</p><hr><h3>OpenScan: A Benchmark for Generalized Open-Vocabulary 3D Scene  Understanding</h3>
<p><a href='http://arxiv.org/abs/2408.11030v1'>http://arxiv.org/abs/2408.11030v1</a></p>
<p><b>Compressor summary</b>: This paper introduces GOV-3D, a challenging open vocabulary task that requires understanding 3D scenes with fine-grained attributes, and presents OpenScan, a new benchmark for it.</p><hr><h3>Scaling Law with Learning Rate Annealing</h3>
<p><a href='http://arxiv.org/abs/2408.11029v1'>http://arxiv.org/abs/2408.11029v1</a></p>
<p><b>Compressor summary</b>: The text describes a scaling law that relates cross-entropy loss curves of neural language models to learning rate annealing, which can accurately predict loss dynamics during training with low computational cost.</p><hr><h3>Athena: Safe Autonomous Agents with Verbal Contrastive Learning</h3>
<p><a href='http://arxiv.org/abs/2408.11021v1'>http://arxiv.org/abs/2408.11021v1</a></p>
<p><b>Compressor summary</b>: The Athena framework teaches language models to be safe and trustworthy by using past examples of safe and unsafe actions and guiding them with feedback on risky behavior.</p><hr><h3>While GitHub Copilot Excels at Coding, Does It Ensure Responsible  Output?</h3>
<p><a href='http://arxiv.org/abs/2408.11006v1'>http://arxiv.org/abs/2408.11006v1</a></p>
<p><b>Compressor summary</b>: The paper presents targeted attacks on LLM-based Code Completion Tools, revealing significant vulnerabilities in their security frameworks and handling of code.</p><hr><h3>MegaFusion: Extend Diffusion Models towards Higher-resolution Image  Generation without Further Tuning</h3>
<p><a href='http://arxiv.org/abs/2408.11001v1'>http://arxiv.org/abs/2408.11001v1</a></p>
<p><b>Compressor summary</b>: MegaFusion is a novel diffusion-based text-to-image generation approach that efficiently produces high-resolution images with improved semantic accuracy and less object replication, using a coarse-to-fine strategy and adapting to different resolutions.</p><hr><h3>SenPa-MAE: Sensor Parameter Aware Masked Autoencoder for Multi-Satellite  Self-Supervised Pretraining</h3>
<p><a href='http://arxiv.org/abs/2408.11000v1'>http://arxiv.org/abs/2408.11000v1</a></p>
<p><b>Compressor summary</b>: SenPa-MAE is a transformer model that learns to encode sensor parameters from multispectral images, enabling cross-sensor learning for Earth observation.</p><hr><h3>Disentangling segmental and prosodic factors to non-native speech  comprehensibility</h3>
<p><a href='http://arxiv.org/abs/2408.10997v1'>http://arxiv.org/abs/2408.10997v1</a></p>
<p><b>Compressor summary</b>: The text presents an accent conversion system that separates and combines segmental, voice, and prosodic features to improve the comprehensibility and social perception of non-native speech.</p><hr><h3>CTP-LLM: Clinical Trial Phase Transition Prediction Using Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2408.10995v1'>http://arxiv.org/abs/2408.10995v1</a></p>
<p><b>Compressor summary</b>: The study proposes CTP-LLM, a large language model that predicts phase transitions in clinical trials using trial protocol texts, achieving 67% accuracy overall and 75% in predicting final approval from Phase~III.</p><hr><h3>Facial Demorphing via Identity Preserving Image Decomposition</h3>
<p><a href='http://arxiv.org/abs/2408.10993v1'>http://arxiv.org/abs/2408.10993v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a reference-free demorphing technique that can recover the original face images from morphs by decomposing them into identity-preserving features.</p><hr><h3>The fusion of phonography and ideographic characters into virtual  Chinese characters -- Based on Chinese and English</h3>
<p><a href='http://arxiv.org/abs/2408.10979v1'>http://arxiv.org/abs/2408.10979v1</a></p>
<p><b>Compressor summary</b>: The text proposes creating a new writing system combining the advantages of Chinese and English, such as reducing vocabulary, allowing quick word recognition, and facilitating learning of complex concepts.</p><hr><h3>Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical  Planning and Control</h3>
<p><a href='http://arxiv.org/abs/2408.10970v1'>http://arxiv.org/abs/2408.10970v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a hybrid AI algorithm that combines recurrent switching linear dynamical systems (rSLDS) with active inference to learn discrete abstractions for planning and control, achieving fast system identification and non-trivial planning on the Continuous Mountain Car task.</p><hr><h3>NLP for The Greek Language: A Longer Survey</h3>
<p><a href='http://arxiv.org/abs/2408.10962v1'>http://arxiv.org/abs/2408.10962v1</a></p>
<p><b>Compressor summary</b>: The paper surveys three decades of research on NLP tools and resources for the Greek language, including Ancient Greek and dialects, to help researchers and students working with Greek in these fields.</p><hr><h3>Multichannel Attention Networks with Ensembled Transfer Learning to  Recognize Bangla Handwritten Charecter</h3>
<p><a href='http://arxiv.org/abs/2408.10955v1'>http://arxiv.org/abs/2408.10955v1</a></p>
<p><b>Compressor summary</b>: The study used a CNN with ensemble transfer learning and a multichannel attention network to improve Bengali handwritten character recognition, achieving 92% accuracy on raw data and 98% on preprocessed data.</p><hr><h3>Wave-Mask/Mix: Exploring Wavelet-Based Augmentations for Time Series  Forecasting</h3>
<p><a href='http://arxiv.org/abs/2408.10951v1'>http://arxiv.org/abs/2408.10951v1</a></p>
<p><b>Compressor summary</b>: WaveMask and WaveMix are novel data augmentation techniques for time series forecasting that use the discrete wavelet transform to preserve temporal dependencies while adjusting frequency elements.</p><hr><h3>GAIM: Attacking Graph Neural Networks via Adversarial Influence  Maximization</h3>
<p><a href='http://arxiv.org/abs/2408.10948v1'>http://arxiv.org/abs/2408.10948v1</a></p>
<p><b>Compressor summary</b>: GAIM is a black-box adversarial attack on GNNs that uses node feature perturbations and optimizes them with a surrogate model to maximize influence on target nodes, supporting both untargeted and label-oriented attacks.</p><hr><h3>Dr.Academy: A Benchmark for Evaluating Questioning Capability in  Education for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.10947v1'>http://arxiv.org/abs/2408.10947v1</a></p>
<p><b>Compressor summary</b>: The text discusses evaluating large language models' questioning abilities as teachers using a benchmark based on Anderson and Krathwohl's taxonomy and four metrics, showing their potential in teaching different subjects.</p><hr><h3>Large Language Model Driven Recommendation</h3>
<p><a href='http://arxiv.org/abs/2408.10946v1'>http://arxiv.org/abs/2408.10946v1</a></p>
<p><b>Compressor summary</b>: The chapter explores how large language models can enhance recommendation systems by using natural language interactions and general reasoning to personalize recommendations based on diverse user preferences.</p><hr><h3>HiRED: Attention-Guided Token Dropping for Efficient Inference of  High-Resolution Vision-Language Models in Resource-Constrained Environments</h3>
<p><a href='http://arxiv.org/abs/2408.10945v1'>http://arxiv.org/abs/2408.10945v1</a></p>
<p><b>Compressor summary</b>: HiRED is a token-dropping scheme that improves efficiency in high-resolution VLMs by selectively dropping excessive visual tokens based on attention scores.</p><hr><h3>SysBench: Can Large Language Models Follow System Messages?</h3>
<p><a href='http://arxiv.org/abs/2408.10943v1'>http://arxiv.org/abs/2408.10943v1</a></p>
<p><b>Compressor summary</b>: SysBench is a benchmark that evaluates how well large language models follow system messages, which are crucial for customizing the models to specific scenarios.</p><hr><h3>Robust Regression with Ensembles Communicating over Noisy Channels</h3>
<p><a href='http://arxiv.org/abs/2408.10942v1'>http://arxiv.org/abs/2408.10942v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Machine learning models need distributed settings due to size limitations
- Distributed inference tasks face reliability challenges from noisy channels and low-precision devices
- The problem is studied for ensemble regression methods (bagging and gradient boosting)
- Optimization methods are developed for aggregation coefficients based on noise parameters
- Effectiveness is shown on synthetic and real datasets

Summary:
The paper proposes optimization methods for distributed ensemble regression with noisy channels, and evaluates their effectiveness on various datasets.</p><hr><h3>A Closer Look at Data Augmentation Strategies for Finetuning-Based  Low/Few-Shot Object Detection</h3>
<p><a href='http://arxiv.org/abs/2408.10940v1'>http://arxiv.org/abs/2408.10940v1</a></p>
<p><b>Compressor summary</b>: The paper studies how different data augmentation methods affect object detection performance and energy efficiency, and proposes using the Efficiency Factor to balance these trade-offs.</p><hr><h3>Conformalized Interval Arithmetic with Symmetric Calibration</h3>
<p><a href='http://arxiv.org/abs/2408.10939v1'>http://arxiv.org/abs/2408.10939v1</a></p>
<p><b>Compressor summary</b>: The paper proposes new methods for conformal prediction of sum or average labels over specific sets, with valid coverage guarantees and improved performance in some applications.</p><hr><h3>Large Point-to-Gaussian Model for Image-to-3D Generation</h3>
<p><a href='http://arxiv.org/abs/2408.10935v1'>http://arxiv.org/abs/2408.10935v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel Point-to-Gaussian model that leverages point clouds and image features to generate high-quality 3D assets from images more efficiently than existing methods.</p><hr><h3>SDI-Net: Toward Sufficient Dual-View Interaction for Low-light Stereo  Image Enhancement</h3>
<p><a href='http://arxiv.org/abs/2408.10934v1'>http://arxiv.org/abs/2408.10934v1</a></p>
<p><b>Compressor summary</b>: SDI-Net is a new method for enhancing low-light stereo images by fully exploiting the correlation between binocular views using an attention mechanism.</p><hr><h3>The Evolution of Reinforcement Learning in Quantitative Finance</h3>
<p><a href='http://arxiv.org/abs/2408.10932v1'>http://arxiv.org/abs/2408.10932v1</a></p>
<p><b>Compressor summary</b>: The text reviews recent advancements in reinforcement learning (RL) applications within finance, highlighting its potential to enhance traditional solutions using machine learning techniques.</p><hr><h3>LBC: Language-Based-Classifier for Out-Of-Variable Generalization</h3>
<p><a href='http://arxiv.org/abs/2408.10923v1'>http://arxiv.org/abs/2408.10923v1</a></p>
<p><b>Compressor summary</b>: The text describes a new Language-Based-Classifier (LBC) that leverages pre-trained knowledge from Large Language Models to outperform traditional machine learning models on Out-of-Variable tasks involving tabular data.</p><hr><h3>MTFinEval:A Multi-domain Chinese Financial Benchmark with Eurypalynous  questions</h3>
<p><a href='http://arxiv.org/abs/2408.10921v1'>http://arxiv.org/abs/2408.10921v1</a></p>
<p><b>Compressor summary</b>: The paper introduces MTFinEval, a new benchmark to measure the basic economic knowledge and generalization ability of LLMs, which can help select suitable LLMs for production scenarios.</p><hr><h3>Recurrent Neural Networks Learn to Store and Generate Sequences using  Non-Linear Representations</h3>
<p><a href='http://arxiv.org/abs/2408.10920v1'>http://arxiv.org/abs/2408.10920v1</a></p>
<p><b>Compressor summary</b>: The paper shows that gated RNNs do not always learn linear representations of input tokens, but rather their order of magnitude, challenging the strong Linear Representation Hypothesis.</p><hr><h3>CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network</h3>
<p><a href='http://arxiv.org/abs/2408.10919v1'>http://arxiv.org/abs/2408.10919v1</a></p>
<p><b>Compressor summary</b>: CrossFi is a siamese network that uses an attention mechanism and a weight-net to improve Wi-Fi sensing across various scenarios, achieving state-of-the-art results in gesture recognition and other tasks.</p><hr><h3>CHECKWHY: Causal Fact Verification via Argument Structure</h3>
<p><a href='http://arxiv.org/abs/2408.10918v1'>http://arxiv.org/abs/2408.10918v1</a></p>
<p><b>Compressor summary</b>: CheckWhy is a dataset for verifying causal relations in claims using logical reasoning and evidence, which is challenging for current language models.</p><hr><h3>To Code, or Not To Code? Exploring Impact of Code in Pre-training</h3>
<p><a href='http://arxiv.org/abs/2408.10914v1'>http://arxiv.org/abs/2408.10914v1</a></p>
<p><b>Compressor summary</b>: The study shows that using code data in pre-training improves general language understanding and reasoning skills for large language models beyond coding tasks.</p><hr><h3>BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General  Role-Playing Language Model</h3>
<p><a href='http://arxiv.org/abs/2408.10903v1'>http://arxiv.org/abs/2408.10903v1</a></p>
<p><b>Compressor summary</b>: The BEYOND DIALOGUE framework improves role-playing by aligning dialogue with profile traits using beyond dialogue tasks, reasoning outcomes, and fine-grained alignment, achieving better performance than existing methods.</p><hr><h3>Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs</h3>
<p><a href='http://arxiv.org/abs/2408.10902v1'>http://arxiv.org/abs/2408.10902v1</a></p>
<p><b>Compressor summary</b>: Soda-Eval is a new dataset to evaluate chatbots based on Soda, revealing issues with coherence and commonsense knowledge, and showing that fine-tuning LLMs improves performance.</p><hr><h3>Analytical and Empirical Study of Herding Effects in Recommendation  Systems</h3>
<p><a href='http://arxiv.org/abs/2408.10895v1'>http://arxiv.org/abs/2408.10895v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a mathematical model to study how rating aggregation rules and review selection mechanisms can correct assessment errors caused by herding effects in online product ratings, and tests the model on Amazon and TripAdvisor datasets.</p><hr><h3>On Learning Action Costs from Input Plans</h3>
<p><a href='http://arxiv.org/abs/2408.10889v1'>http://arxiv.org/abs/2408.10889v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new problem of learning action costs from unlabeled input plans for optimal planning and presents an algorithm, $LACFIP^k$, that solves it with theoretical and empirical evidence.</p><hr><h3>Low-Quality Image Detection by Hierarchical VAE</h3>
<p><a href='http://arxiv.org/abs/2408.10885v1'>http://arxiv.org/abs/2408.10885v1</a></p>
<p><b>Compressor summary</b>: The study proposes a method to detect and explain low-quality images in unsupervised ways using hierarchical variational autoencoders.</p><hr><h3>DAAD: Dynamic Analysis and Adaptive Discriminator for Fake News  Detection</h3>
<p><a href='http://arxiv.org/abs/2408.10883v1'>http://arxiv.org/abs/2408.10883v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new method for detecting fake news using dynamic analysis and adaptive discriminators, which are more flexible than existing methods.</p><hr><h3>DBHP: Trajectory Imputation in Multi-Agent Sports Using Derivative-Based  Hybrid Prediction</h3>
<p><a href='http://arxiv.org/abs/2408.10878v1'>http://arxiv.org/abs/2408.10878v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a DBHP framework that uses neural networks with Set Transformers to predict missing trajectory data for multiple agents, considering their physical constraints and dynamics, and achieves better results than existing methods in real-world scenarios.</p><hr><h3>V-RoAst: A New Dataset for Visual Road Assessment</h3>
<p><a href='http://arxiv.org/abs/2408.10872v1'>http://arxiv.org/abs/2408.10872v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an approach using Vision Language Models to assess road safety from crowdsourced images, overcoming limitations of traditional CNNs and providing a scalable, cost-effective, and automated solution for low-resource settings.</p><hr><h3>Multi-agent Multi-armed Bandits with Stochastic Sharable Arm Capacities</h3>
<p><a href='http://arxiv.org/abs/2408.10865v1'>http://arxiv.org/abs/2408.10865v1</a></p>
<p><b>Compressor summary</b>: The paper proposes new algorithms for a multi-player bandit problem with stochastic arrivals and decentralized learning, using explore then commit framework.</p><hr><h3>Feature Selection from Differentially Private Correlations</h3>
<p><a href='http://arxiv.org/abs/2408.10862v1'>http://arxiv.org/abs/2408.10862v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates a new method for private feature selection in high-dimensional datasets and shows it performs better than existing methods.</p><hr><h3>Knowledge Sharing and Transfer via Centralized Reward Agent for  Multi-Task Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2408.10858v1'>http://arxiv.org/abs/2408.10858v1</a></p>
<p><b>Compressor summary</b>: The authors propose a multi-task reinforcement learning framework that uses a centralized reward agent to distill knowledge from various tasks and improve learning efficiency.</p><hr><h3>Perception-guided Jailbreak against Text-to-Image Models</h3>
<p><a href='http://arxiv.org/abs/2408.10848v1'>http://arxiv.org/abs/2408.10848v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to generate natural attack prompts for Text-to-Image models that can produce inappropriate images by substituting unsafe words with similar but safe phrases.</p><hr><h3>Aligning Object Detector Bounding Boxes with Human Preference</h3>
<p><a href='http://arxiv.org/abs/2408.10844v1'>http://arxiv.org/abs/2408.10844v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how to align object detections with human preference for box size and proposes an asymmetric bounding box regression loss to encourage large boxes over small ones.</p><hr><h3>Benchmarking Large Language Models for Math Reasoning Tasks</h3>
<p><a href='http://arxiv.org/abs/2408.10839v1'>http://arxiv.org/abs/2408.10839v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a benchmark to compare seven algorithms for mathematical reasoning using LLMs across five datasets and four large models, exploring efficiency and performance trade-offs and practical applications.</p><hr><h3>Multilevel CNNs for Parametric PDEs based on Adaptive Finite Elements</h3>
<p><a href='http://arxiv.org/abs/2408.10838v1'>http://arxiv.org/abs/2408.10838v1</a></p>
<p><b>Compressor summary</b>: The paper presents a neural network that approximates parameter-to-solution maps for high-dimensional partial differential equations using adaptively refined finite element meshes and a reliable error estimator, achieving accuracy and complexity similar to low-rank tensor regression methods.</p><hr><h3>Trustworthy Compression? Impact of AI-based Codecs on Biometrics for Law  Enforcement</h3>
<p><a href='http://arxiv.org/abs/2408.10823v1'>http://arxiv.org/abs/2408.10823v1</a></p>
<p><b>Compressor summary</b>: AI image compression can affect biometric recognition, especially for iris and fabric/tattoo images, while fingerprint recognition remains robust.</p><hr><h3>Navigating Spatio-Temporal Heterogeneity: A Graph Transformer Approach  for Traffic Forecasting</h3>
<p><a href='http://arxiv.org/abs/2408.10822v1'>http://arxiv.org/abs/2408.10822v1</a></p>
<p><b>Compressor summary</b>: STGormer is a new neural network model for traffic forecasting that effectively uses spatial and temporal information in traffic data to improve prediction accuracy.</p><hr><h3>Constructing a High Temporal Resolution Global Lakes Dataset via  Swin-Unet with Applications to Area Prediction</h3>
<p><a href='http://arxiv.org/abs/2408.10821v1'>http://arxiv.org/abs/2408.10821v1</a></p>
<p><b>Compressor summary</b>: The paper introduces GLAKES-Additional, a database with biennial data on over 150,000 lakes worldwide, which helps monitor lake dynamics and attribute area changes to climate and land use factors using advanced machine learning models.</p><hr><h3>Exploiting Large Language Models Capabilities for Question Answer-Driven  Knowledge Graph Completion Across Static and Temporal Domains</h3>
<p><a href='http://arxiv.org/abs/2408.10819v1'>http://arxiv.org/abs/2408.10819v1</a></p>
<p><b>Compressor summary</b>: GS-KGC is a novel generative completion framework for knowledge graphs that uses question-answering, subgraph extraction, and contextual information to discover new triples and facts beyond existing KGs.</p><hr><h3>Learning Randomized Algorithms with Transformers</h3>
<p><a href='http://arxiv.org/abs/2408.10818v1'>http://arxiv.org/abs/2408.10818v1</a></p>
<p><b>Compressor summary</b>: The paper shows how randomization can improve transformer models in various tasks by learning random parameters that enhance robustness and performance.</p><hr><h3>Beyond English-Centric LLMs: What Language Do Multilingual Language  Models Think in?</h3>
<p><a href='http://arxiv.org/abs/2408.10811v1'>http://arxiv.org/abs/2408.10811v1</a></p>
<p><b>Compressor summary</b>: The study explores if non-English-centric LLMs have dual latent languages and how they handle cultural conflicts between them when generating text in different languages.</p><hr><h3>ColBERT Retrieval and Ensemble Response Scoring for Language Model  Question Answering</h3>
<p><a href='http://arxiv.org/abs/2408.10808v1'>http://arxiv.org/abs/2408.10808v1</a></p>
<p><b>Compressor summary</b>: The paper presents methods to improve small language models' performance in telecom question answering, achieving leading marks of 81.9% (Phi-2) and 57.3% (Falcon-7B).</p><hr><h3>Inverse Deep Learning Ray Tracing for Heliostat Surface Prediction</h3>
<p><a href='http://arxiv.org/abs/2408.10802v1'>http://arxiv.org/abs/2408.10802v1</a></p>
<p><b>Compressor summary</b>: Inverse Deep Learning Ray Tracing (iDLR) is a novel method for predicting heliostat surfaces in Concentrating Solar Power plants using target images, improving safety and efficiency by accounting for non-ideal surface conditions.</p><hr><h3>Universal Novelty Detection Through Adaptive Contrastive Learning</h3>
<p><a href='http://arxiv.org/abs/2408.10798v1'>http://arxiv.org/abs/2408.10798v1</a></p>
<p><b>Compressor summary</b>: The paper proposes AutoAugOOD, a probabilistic auto-negative pair generation method that leverages contrastive learning to create a universal novelty detector that adapts well to different distribution shifts and settings.</p><hr><h3>Adversarial Attack for Explanation Robustness of Rationalization Models</h3>
<p><a href='http://arxiv.org/abs/2408.10795v1'>http://arxiv.org/abs/2408.10795v1</a></p>
<p><b>Compressor summary</b>: UAT2E is a method that tests the robustness of explainable AI models by inserting malicious triggers into inputs without changing their predictions, revealing their vulnerability in providing trustworthy explanations.</p><hr><h3>Tapping in a Remote Vehicle's onboard LLM to Complement the Ego  Vehicle's Field-of-View</h3>
<p><a href='http://arxiv.org/abs/2408.10794v1'>http://arxiv.org/abs/2408.10794v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Advanced automotive systems use CPS for driver assistance but have limitations in occluded scenarios
- V2I and V2V communication can share data about surrounding objects to improve safety
- LLMs can enhance voice assistants and communicate what other vehicles see
- GPT-4V and GPT-4o are powerful LLMs that can understand traffic situations and spot participants

Summary:
The text discusses how CPS, V2I/V2V communication, and LLMs can improve automotive systems by overcoming occlusions, sharing data, and enhancing voice assistants. It also evaluates GPT-4V and GPT-4o as LLMs that can understand traffic situations and spot participants.</p><hr><h3>Learning Part-aware 3D Representations by Fusing 2D Gaussians and  Superquadrics</h3>
<p><a href='http://arxiv.org/abs/2408.10789v1'>http://arxiv.org/abs/2408.10789v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a part-aware 3D reconstruction method using a hybrid representation of superquadrics and 2D Gaussians to parse objects or scenes into semantic parts from multi-view image inputs, achieving accurate geometry reconstruction and high-quality rendering.</p><hr><h3>Understanding the Skills Gap between Higher Education and Industry in  the UK in Artificial Intelligence Sector</h3>
<p><a href='http://arxiv.org/abs/2408.10788v1'>http://arxiv.org/abs/2408.10788v1</a></p>
<p><b>Compressor summary</b>: The paper examines UK university AI courses' alignment with industry demands and finds they are strong in programming and machine learning but weak in data science and maths.</p><hr><h3>LightMDETR: A Lightweight Approach for Low-Cost Open-Vocabulary Object  Detection Training</h3>
<p><a href='http://arxiv.org/abs/2408.10787v1'>http://arxiv.org/abs/2408.10787v1</a></p>
<p><b>Compressor summary</b>: LightMDETR is a lightweight version of MDETR that uses a Deep Fusion Encoder to represent image and text modalities for improved object detection and classification with less computational cost.</p><hr><h3>Just a Hint: Point-Supervised Camouflaged Object Detection</h3>
<p><a href='http://arxiv.org/abs/2408.10777v1'>http://arxiv.org/abs/2408.10777v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for detecting camouflaged objects using only one point supervision and unsupervised contrastive learning to address the challenges of ambiguous boundaries and feature instability.</p><hr><h3>Generative AI in Industrial Machine Vision -- A Review</h3>
<p><a href='http://arxiv.org/abs/2408.10775v1'>http://arxiv.org/abs/2408.10775v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Machine vision enhances automation, quality control, and operational efficiency in industrial settings using visual data
- Generative AI is a promising approach to improve pattern recognition in machine vision
- A literature review based on the PRISMA guidelines was conducted to analyze over 1,200 papers on generative AI in industrial machine vision
- The main use of generative AI in machine vision is data augmentation for tasks such as classification and object detection
- The paper identifies application challenges and data requirements for successful generative AI implementation in machine vision

Summary:
The paper reviews recent advances and applications of generative AI in industrial machine vision, focusing on data augmentation for tasks like classification and object detection, and highlighting the challenges and data needs for its successful use.</p><hr><h3>Flexora: Flexible Low Rank Adaptation for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.10774v1'>http://arxiv.org/abs/2408.10774v1</a></p>
<p><b>Compressor summary</b>: Flexora improves fine-tuning of large language models by flexibly selecting important layers for different downstream tasks using hyperparameter optimization.</p><hr><h3>Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion  for Efficient Inference Intervention in Large Language Model</h3>
<p><a href='http://arxiv.org/abs/2408.10764v1'>http://arxiv.org/abs/2408.10764v1</a></p>
<p><b>Compressor summary</b>: Otter inserts extra parameters into transformer models to predict calibration signals for safer, more reliable language generation while saving time and space.</p><hr><h3>SAM-COD: SAM-guided Unified Framework for Weakly-Supervised Camouflaged  Object Detection</h3>
<p><a href='http://arxiv.org/abs/2408.10760v1'>http://arxiv.org/abs/2408.10760v1</a></p>
<p><b>Compressor summary</b>: SAM-COD is a unified framework that supports various weakly-supervised labels for camouflaged object detection, using a prompt adapter, response filter, semantic matcher, and prompt-adaptive knowledge distillation to improve performance.</p><hr><h3>Generating Synthetic Fair Syntax-agnostic Data by Learning and  Distilling Fair Representation</h3>
<p><a href='http://arxiv.org/abs/2408.10755v1'>http://arxiv.org/abs/2408.10755v1</a></p>
<p><b>Compressor summary</b>: The text discusses a technique to generate fair data using knowledge distillation, which improves fairness, quality, and utility of synthetic data compared to existing methods.</p><hr><h3>TrackNeRF: Bundle Adjusting NeRF from Sparse and Noisy Views via Feature  Tracks</h3>
<p><a href='http://arxiv.org/abs/2408.10739v1'>http://arxiv.org/abs/2408.10739v1</a></p>
<p><b>Compressor summary</b>: TrackNeRF improves NeRFs by enforcing global 3D consistency among feature tracks for more accurate reconstruction with sparse and noisy views.</p><hr><h3>Towards Efficient Large Language Models for Scientific Text: A Review</h3>
<p><a href='http://arxiv.org/abs/2408.10729v1'>http://arxiv.org/abs/2408.10729v1</a></p>
<p><b>Compressor summary</b>: The text discusses how large language models can improve scientific tasks, but their high resource requirements call for methods to make them more cost-effective, either by reducing model size or enhancing data quality.</p><hr><h3>Crafting Tomorrow's Headlines: Neural News Generation and Detection in  English, Turkish, Hungarian, and Persian</h3>
<p><a href='http://arxiv.org/abs/2408.10724v1'>http://arxiv.org/abs/2408.10724v1</a></p>
<p><b>Compressor summary</b>: The authors introduce a benchmark dataset for neural news detection in four languages, using outputs from various multilingual generators and classifiers, to study the interpretability and robustness of machine-generated text detectors.</p><hr><h3>MEGen: Generative Backdoor in Large Language Models via Model Editing</h3>
<p><a href='http://arxiv.org/abs/2408.10722v1'>http://arxiv.org/abs/2408.10722v1</a></p>
<p><b>Compressor summary</b>: The paper proposes MEGen, an editing-based generative backdoor for NLP tasks, which can insert a trigger into an LLM to output dangerous information when activated.</p><hr><h3>Towards Foundation Models for the Industrial Forecasting of Chemical  Kinetics</h3>
<p><a href='http://arxiv.org/abs/2408.10720v1'>http://arxiv.org/abs/2408.10720v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new machine learning method (MLP-Mixer) to improve modeling of stiff chemical reactions in fluid dynamics, comparing it with traditional methods on a benchmark dataset.</p><hr><h3>Accelerated training of deep learning surrogate models for surface  displacement and flow, with application to MCMC-based history matching of CO2  storage operations</h3>
<p><a href='http://arxiv.org/abs/2408.10717v1'>http://arxiv.org/abs/2408.10717v1</a></p>
<p><b>Compressor summary</b>: Key points:
- New surrogate modeling framework for CO2 storage operations history matching using deep learning
- Inexpensive flow-only simulations combined with coupled runs
- Effective rock compressibility, residual U-Net architectures for saturation, pressure and surface displacement predictions
- Median relative error less than 4% for all variables
- Surrogate models incorporated into hierarchical Markov chain Monte Carlo history matching workflow
- High prior uncertainty and data types affect history matching results

Summary:
The authors present a new deep learning framework for history matching of CO2 storage operations using surrogate models that predict saturation, pressure and surface displacement from flow-only simulations and coupled runs. They use effective rock compressibility and residual U-Net architectures to achieve low error rates and incorporate uncertainty in the geomodels and data types.</p><hr><h3>Fine-Tuning a Local LLaMA-3 Large Language Model for Automated  Privacy-Preserving Physician Letter Generation in Radiation Oncology</h3>
<p><a href='http://arxiv.org/abs/2408.10715v1'>http://arxiv.org/abs/2408.10715v1</a></p>
<p><b>Compressor summary</b>: The study shows that fine-tuning LLaMA models for radiation oncology using QLoRA algorithm improves the quality of physician letters, which are valuable in clinical practice but time-consuming to generate manually.</p><hr><h3>Offline Model-Based Reinforcement Learning with Anti-Exploration</h3>
<p><a href='http://arxiv.org/abs/2408.10713v1'>http://arxiv.org/abs/2408.10713v1</a></p>
<p><b>Compressor summary</b>: MoMo is a new offline reinforcement learning algorithm that uses anti-exploration techniques to improve performance and handle out-of-distribution states without large ensembles of models.</p><hr><h3>Investigating Context Effects in Similarity Judgements in Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2408.10711v1'>http://arxiv.org/abs/2408.10711v1</a></p>
<p><b>Compressor summary</b>: This paper investigates how large language models are influenced by order bias in similarity judgments, similar to humans, and discusses the implications for AI applications that rely on human values and expectations.</p><hr><h3>Coarse-to-Fine Detection of Multiple Seams for Robotic Welding</h3>
<p><a href='http://arxiv.org/abs/2408.10710v1'>http://arxiv.org/abs/2408.10710v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a novel framework for multiple weld seams extraction using RGB images and 3D point clouds
- The method uses region growth to fine-edge extract the weld seams within the region of interest
- The method is accelerated by a pre-trained deep learning model
- The method is tested on various workpieces with linear and curved weld seams and shows potential for industrial applications

Summary:
The paper presents a new method that uses RGB images and 3D point clouds to extract multiple weld seams accurately and efficiently, using region growth and a pre-trained deep learning model.</p><hr><h3>Variable Assignment Invariant Neural Networks for Learning Logic  Programs</h3>
<p><a href='http://arxiv.org/abs/2408.10709v1'>http://arxiv.org/abs/2408.10709v1</a></p>
<p><b>Compressor summary</b>: The paper presents a technique to learn rules from state transitions using neural networks that are invariant to variable permutation and naming, improving generalization and scalability.</p><hr><h3>Large Language Models for Multimodal Deformable Image Registration</h3>
<p><a href='http://arxiv.org/abs/2408.10703v1'>http://arxiv.org/abs/2408.10703v1</a></p>
<p><b>Compressor summary</b>: LLM-Morph uses pre-trained Large Language Models to align deep features from different modal medical images for coarse-to-fine Multimodal Deformable Image Registration.</p><hr><h3>AnyGraph: Graph Foundation Model in the Wild</h3>
<p><a href='http://arxiv.org/abs/2408.10700v1'>http://arxiv.org/abs/2408.10700v1</a></p>
<p><b>Compressor summary</b>: AnyGraph is a unified graph foundation model that leverages Graph Mixture-of-Experts to handle structure and feature heterogeneity, enabling fast adaptation and favorable scaling in various graph tasks.</p><hr><h3>MsMemoryGAN: A Multi-scale Memory GAN for Palm-vein Adversarial  Purification</h3>
<p><a href='http://arxiv.org/abs/2408.10694v1'>http://arxiv.org/abs/2408.10694v1</a></p>
<p><b>Compressor summary</b>: The paper proposes MsMemoryGAN, a model that filters adversarial perturbations from vein images by reconstructing them using memory modules and a learnable metric.</p><hr><h3>Unconditional Truthfulness: Learning Conditional Dependency for  Uncertainty Quantification of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.10692v1'>http://arxiv.org/abs/2408.10692v1</a></p>
<p><b>Compressor summary</b>: The text proposes a method to measure uncertainty in language model generations by learning the conditional dependency between generation steps using a regression model.</p><hr><h3>Fine-Tuning and Deploying Large Language Models Over Edges: Issues and  Approaches</h3>
<p><a href='http://arxiv.org/abs/2408.10691v1'>http://arxiv.org/abs/2408.10691v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Large language models are versatile but require fine-tuning and resources for deployment
- Traditional fine-tuning methods need more GPU memory than mainstream hardware can offer
- Memory-efficient methods are needed to reduce energy consumption, costs, and environmental impact
- The paper reviews memory-efficient fine-tuning methods and model compression techniques for deploying LLMs over the network edge

Summary:
The paper surveys memory-efficient methods and model compression techniques to enable sustainable deployment of large language models over the network edge.</p><hr><h3>Genesis: Towards the Automation of Systems Biology Research</h3>
<p><a href='http://arxiv.org/abs/2408.10689v1'>http://arxiv.org/abs/2408.10689v1</a></p>
<p><b>Compressor summary</b>: The Genesis project develops robot scientists that use advanced AI and hardware to rapidly improve biological models in system biology.</p><hr><h3>Rejection in Abstract Argumentation: Harder Than Acceptance?</h3>
<p><a href='http://arxiv.org/abs/2408.10683v1'>http://arxiv.org/abs/2408.10683v1</a></p>
<p><b>Compressor summary</b>: This paper introduces rejection conditions for argumentation frameworks, which allow flexible rejection of arguments based on logic programs and have high expressiveness and complexity.</p><hr><h3>Towards Robust Knowledge Unlearning: An Adversarial Framework for  Assessing and Improving Unlearning Robustness in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.10682v1'>http://arxiv.org/abs/2408.10682v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Dynamic Unlearning Attack to proactively assess vulnerabilities of unlearned models and Latent Adversarial Unlearning to enhance their robustness against adversarial attacks.</p><hr><h3>HMoE: Heterogeneous Mixture of Experts for Language Modeling</h3>
<p><a href='http://arxiv.org/abs/2408.10681v1'>http://arxiv.org/abs/2408.10681v1</a></p>
<p><b>Compressor summary</b>: HMoE is a new model that uses experts with different sizes and capacities to handle variable input data, leading to better performance and efficiency compared to traditional homogeneous MoE models.</p><hr><h3>Towards Rehearsal-Free Multilingual ASR: A LoRA-based Case Study on  Whisper</h3>
<p><a href='http://arxiv.org/abs/2408.10680v1'>http://arxiv.org/abs/2408.10680v1</a></p>
<p><b>Compressor summary</b>: The study explores strategies to improve multilingual speech models on new languages without original data while preserving performance on the original languages, using LoRA-based methods and a learnable rank coefficient.</p><hr><h3>DemMamba: Alignment-free Raw Video Demoireing with Frequency-assisted  Spatio-Temporal Mamba</h3>
<p><a href='http://arxiv.org/abs/2408.10679v1'>http://arxiv.org/abs/2408.10679v1</a></p>
<p><b>Compressor summary</b>: DemMamba is an alignment-free video demoireing network that uses Mamba to model moire patterns' spatial and temporal relationships, improving performance and visual quality.</p><hr><h3>Representation Norm Amplification for Out-of-Distribution Detection in  Long-Tail Learning</h3>
<p><a href='http://arxiv.org/abs/2408.10676v1'>http://arxiv.org/abs/2408.10676v1</a></p>
<p><b>Compressor summary</b>: The paper proposes RNA, a method that improves OOD detection and classification by using representation norm as a new dimension and training in a way that creates a noticeable difference between ID and OOD data.</p><hr><h3>Neural Exploratory Landscape Analysis</h3>
<p><a href='http://arxiv.org/abs/2408.10672v1'>http://arxiv.org/abs/2408.10672v1</a></p>
<p><b>Compressor summary</b>: The paper presents NeurELA, an end-to-end framework that profiles landscape features for MetaBBO algorithms, enhancing their performance and autonomy.</p><hr><h3>A Noncontact Technique for Wave Measurement Based on Thermal  Stereography and Deep Learning</h3>
<p><a href='http://arxiv.org/abs/2408.10670v1'>http://arxiv.org/abs/2408.10670v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new technique combining thermal stereography and deep learning to measure waves noncontactly and accurately, overcoming challenges posed by water's optical properties.</p><hr><h3>REInstruct: Building Instruction Data from Unlabeled Corpus</h3>
<p><a href='http://arxiv.org/abs/2408.10663v1'>http://arxiv.org/abs/2408.10663v1</a></p>
<p><b>Compressor summary</b>: REInstruct is a method to automatically create instruction data for language models using unlabeled texts and rewriting techniques, achieving high performance without reliance on proprietary LLMs or human annotation.</p><hr><h3>UIE-UnFold: Deep Unfolding Network with Color Priors and Vision  Transformer for Underwater Image Enhancement</h3>
<p><a href='http://arxiv.org/abs/2408.10653v1'>http://arxiv.org/abs/2408.10653v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a deep unfolding network (DUN) that integrates color priors and inter-stage feature transformation to improve underwater image enhancement (UIE), offering more accurate, reliable, and explainable results than existing methods.</p><hr><h3>Inferring Underwater Topography with FINN</h3>
<p><a href='http://arxiv.org/abs/2408.10649v1'>http://arxiv.org/abs/2408.10649v1</a></p>
<p><b>Compressor summary</b>: The study shows that FINN, a novel hybrid model combining physics and machine learning, can accurately reconstruct underwater topography from wave dynamics data, outperforming conventional ML and physics-aware ML models.</p><hr><h3>Beneath the Surface of Consistency: Exploring Cross-lingual Knowledge  Representation Sharing in LLMs</h3>
<p><a href='http://arxiv.org/abs/2408.10646v1'>http://arxiv.org/abs/2408.10646v1</a></p>
<p><b>Compressor summary</b>: The study explores how well large language models can represent and share factual knowledge across different languages, finding that script similarity affects representation sharing and improving accuracy.</p><hr><h3>Minor SFT loss for LLM fine-tune to increase performance and reduce  model deviation</h3>
<p><a href='http://arxiv.org/abs/2408.10642v1'>http://arxiv.org/abs/2408.10642v1</a></p>
<p><b>Compressor summary</b>: The text introduces a paradigm for aligning large language models to human preference using supervised fine tuning and reinforcement learning from human feedback, and proposes a new training metric and loss function for supervised fine tuning.</p><hr><h3>A Review of Human-Object Interaction Detection</h3>
<p><a href='http://arxiv.org/abs/2408.10641v1'>http://arxiv.org/abs/2408.10641v1</a></p>
<p><b>Compressor summary</b>: This paper reviews image-based human-object interaction detection methods, datasets, and challenges, as well as recent advancements in zero-shot, weakly supervised learning, and large-scale language models.</p><hr><h3>Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search</h3>
<p><a href='http://arxiv.org/abs/2408.10635v1'>http://arxiv.org/abs/2408.10635v1</a></p>
<p><b>Compressor summary</b>: Strategist is a new method that uses LLMs to improve skills for multi-agent games through self-play simulations and reflection, achieving better performance than existing methods in action planning and dialogue generation.</p><hr><h3>Interactive Counterfactual Generation for Univariate Time Series</h3>
<p><a href='http://arxiv.org/abs/2408.10633v1'>http://arxiv.org/abs/2408.10633v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Interactive method for generating counterfactual explanations for univariate time series classification tasks
- Uses 2D projections and decision boundary maps to improve interpretability
- Allows users to manipulate projected data points and explore hypothetical scenarios
- Validated on ECG5000 dataset, shows significant improvements in interpretability and user understanding

Summary:
The paper presents an interactive method that uses 2D projections and decision boundary maps to help users generate counterfactual explanations for univariate time series classification tasks, improving interpretability and user understanding.</p><hr><h3>LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for  Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.10631v1'>http://arxiv.org/abs/2408.10631v1</a></p>
<p><b>Compressor summary</b>: LLM-Barber is a one-shot pruning framework that optimizes large language models by rebuilding the sparsity mask without retraining or weight reconstruction, achieving state-of-the-art results in perplexity and zero-shot performance.</p><hr><h3>Finding the DeepDream for Time Series: Activation Maximization for  Univariate Time Series</h3>
<p><a href='http://arxiv.org/abs/2408.10628v1'>http://arxiv.org/abs/2408.10628v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Sequence Dreaming, a method to improve interpretability of neural networks for time series data by generating sequences that reflect the critical features identified by the model.</p><hr><h3>WRIM-Net: Wide-Ranging Information Mining Network for Visible-Infrared  Person Re-Identification</h3>
<p><a href='http://arxiv.org/abs/2408.10624v1'>http://arxiv.org/abs/2408.10624v1</a></p>
<p><b>Compressor summary</b>: WRIM-Net is a network that mines modality-invariant information for visible-infrared person re-identification by using multi-dimension interactive information mining and auxiliary-information-based contrastive learning.</p><hr><h3>TextMastero: Mastering High-Quality Scene Text Editing in Diverse  Languages and Styles</h3>
<p><a href='http://arxiv.org/abs/2408.10623v1'>http://arxiv.org/abs/2408.10623v1</a></p>
<p><b>Compressor summary</b>: TextMastero is a multilingual scene text editing method using latent diffusion models with glyph conditioning and latent guidance modules to improve accuracy and style similarity.</p><hr><h3>Novel Change Detection Framework in Remote Sensing Imagery Using  Diffusion Models and Structural Similarity Index (SSIM)</h3>
<p><a href='http://arxiv.org/abs/2408.10619v1'>http://arxiv.org/abs/2408.10619v1</a></p>
<p><b>Compressor summary</b>: The Diffusion Based Change Detector combines diffusion models and SSIM to create accurate and interpretable change maps for remote sensing data.</p><hr><h3>A toolbox for calculating objective image properties in aesthetics  research</h3>
<p><a href='http://arxiv.org/abs/2408.10616v1'>http://arxiv.org/abs/2408.10616v1</a></p>
<p><b>Compressor summary</b>: The Aesthetics Toolbox is an open-access, easy-to-use web tool that allows users to calculate various quantitative image properties related to visual aesthetics research using standardized and consistent methods.</p><hr><h3>Enhancing Robustness in Large Language Models: Prompting for Mitigating  the Impact of Irrelevant Information</h3>
<p><a href='http://arxiv.org/abs/2408.10615v1'>http://arxiv.org/abs/2408.10615v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method to help large language models better handle irrelevant information in math problems, improving their reasoning skills.</p><hr><h3>Generalizable Facial Expression Recognition</h3>
<p><a href='http://arxiv.org/abs/2408.10614v1'>http://arxiv.org/abs/2408.10614v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel FER pipeline that uses CLIP features and learned sigmoid masks to extract expression features, improving zero-shot generalization on unseen test sets.</p><hr><h3>On the Approximability of Stationary Processes using the ARMA Model</h3>
<p><a href='http://arxiv.org/abs/2408.10610v1'>http://arxiv.org/abs/2408.10610v1</a></p>
<p><b>Compressor summary</b>: The authors study how well ARMA models can approximate stationary random variables using Hardy space functions and find some limitations and possibilities.</p><hr><h3>PerturBench: Benchmarking Machine Learning Models for Cellular  Perturbation Analysis</h3>
<p><a href='http://arxiv.org/abs/2408.10609v1'>http://arxiv.org/abs/2408.10609v1</a></p>
<p><b>Compressor summary</b>: PerturBench is a framework that helps predict cell perturbation effects, standardize benchmarking, and improve model evaluation in single-cell research.</p><hr><h3>Promoting Equality in Large Language Models: Identifying and Mitigating  the Implicit Bias based on Bayesian Theory</h3>
<p><a href='http://arxiv.org/abs/2408.10608v1'>http://arxiv.org/abs/2408.10608v1</a></p>
<p><b>Compressor summary</b>: Bayesian-Theory based Bias Removal (BTBR) is a framework that identifies and removes implicit biases from large language models trained on biased data.</p><hr><h3>MUSES: 3D-Controllable Image Generation via Multi-Modal Agent  Collaboration</h3>
<p><a href='http://arxiv.org/abs/2408.10605v1'>http://arxiv.org/abs/2408.10605v1</a></p>
<p><b>Compressor summary</b>: MUSES is a novel AI system that generates 3D-controllable images from user queries using a progressive workflow with three key components and outperforms existing methods on newly constructed benchmarks.</p><hr><h3>Multilingual Non-Factoid Question Answering with Silver Answers</h3>
<p><a href='http://arxiv.org/abs/2408.10604v1'>http://arxiv.org/abs/2408.10604v1</a></p>
<p><b>Compressor summary</b>: MuNfQuAD is a multilingual QA dataset with non-factoid questions from BBC news articles that can be answered using silver answers and has potential for low-resource languages.</p><hr><h3>Breast tumor classification based on self-supervised contrastive  learning from ultrasound videos</h3>
<p><a href='http://arxiv.org/abs/2408.10600v1'>http://arxiv.org/abs/2408.10600v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method using self-supervised learning and triplet networks to diagnose breast tumors from unlabeled ultrasound videos, achieving high performance with very few labeled samples.</p><hr><h3>An Efficient Sign Language Translation Using Spatial Configuration and  Motion Dynamics with LLMs</h3>
<p><a href='http://arxiv.org/abs/2408.10593v1'>http://arxiv.org/abs/2408.10593v1</a></p>
<p><b>Compressor summary</b>: SpaMo is a novel Sign Language Translation framework that leverages spatial and motion features from visual encoders and large language models for better translation without relying on domain-specific fine-tuning.</p><hr><h3>Hologram Reasoning for Solving Algebra Problems with Geometry Diagrams</h3>
<p><a href='http://arxiv.org/abs/2408.10592v1'>http://arxiv.org/abs/2408.10592v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a hologram reasoning scheme to solve Algebra Problems with Geometry Diagrams (APGDs) using a graph model pool and deep reinforcement learning, achieving high accuracy and interpretability.</p><hr><h3>DEGAS: Detailed Expressions on Full-Body Gaussian Avatars</h3>
<p><a href='http://arxiv.org/abs/2408.10588v1'>http://arxiv.org/abs/2408.10588v1</a></p>
<p><b>Compressor summary</b>: DEGAS is a method that uses 3D Gaussian Splatting and a conditional variational autoencoder to create realistic full-body avatars with facial expressions driven by audio and body motion.</p><hr><h3>MUSE: Mamba is Efficient Multi-scale Learner for Text-video Retrieval</h3>
<p><a href='http://arxiv.org/abs/2408.10575v1'>http://arxiv.org/abs/2408.10575v1</a></p>
<p><b>Compressor summary</b>: MUSE is a TVR method that uses multi-scale representations to better match videos with natural language queries, using a feature pyramid and an efficient Mamba structure.</p><hr><h3>Putting People in LLMs' Shoes: Generating Better Answers via Question  Rewriter</h3>
<p><a href='http://arxiv.org/abs/2408.10573v1'>http://arxiv.org/abs/2408.10573v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a method to improve the quality of answers generated by large language models by rewriting user questions, without needing human annotations.</p><hr><h3>Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2408.10571v1'>http://arxiv.org/abs/2408.10571v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a Prompt-Agnostic Adversarial Perturbation (PAP) method that protects personal images and artistic styles from customized text-to-image generation models by generating perturbations based on the prompt distribution.</p><hr><h3>SparseGrow: Addressing Growth-Induced Forgetting in Task-Agnostic  Continual Learning</h3>
<p><a href='http://arxiv.org/abs/2408.10566v1'>http://arxiv.org/abs/2408.10566v1</a></p>
<p><b>Compressor summary</b>: SparseGrow is a novel approach to overcome growth-induced forgetting (GIFt) in continual learning by using data-driven sparse layer expansion and on-data initialization to balance adaptability and knowledge retention.</p><hr><h3>Speech Representation Learning Revisited: The Necessity of Separate  Learnable Parameters and Robust Data Augmentation</h3>
<p><a href='http://arxiv.org/abs/2408.10557v1'>http://arxiv.org/abs/2408.10557v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Other HuBERT, a modified speech modeling method that separates content and other information, and shows its effectiveness in encoding and learning the latter with robust data augmentation.</p><hr><h3>Hokoff: Real Game Dataset from Honor of Kings and its Offline  Reinforcement Learning Benchmarks</h3>
<p><a href='http://arxiv.org/abs/2408.10556v1'>http://arxiv.org/abs/2408.10556v1</a></p>
<p><b>Compressor summary</b>: Hokoff is a new dataset and framework for offline RL and MARL research, based on a complex real-world MOBA game, that reveals the limitations of existing methods.</p><hr><h3>Target-Prompt Online Graph Collaborative Learning for Temporal QoS  Prediction</h3>
<p><a href='http://arxiv.org/abs/2408.10555v1'>http://arxiv.org/abs/2408.10555v1</a></p>
<p><b>Compressor summary</b>: TOGCL is a framework that uses graph attention networks and Transformer encoders to predict QoS in service-oriented architecture by modeling user-service interactions and long-term dependencies.</p><hr><h3>AI-Based IVR</h3>
<p><a href='http://arxiv.org/abs/2408.10549v1'>http://arxiv.org/abs/2408.10549v1</a></p>
<p><b>Compressor summary</b>: The article proposes using AI to improve IVR systems in call centers, reducing operator workload and improving customer service, especially for non-English languages like Kazakh.</p><hr><h3>Language Modeling on Tabular Data: A Survey of Foundations, Techniques  and Evolution</h3>
<p><a href='http://arxiv.org/abs/2408.10548v1'>http://arxiv.org/abs/2408.10548v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Tabular data is a common and challenging data type that requires advanced modeling techniques
- Transformer architectures inspired by natural language processing have revolutionized tabular data analysis
- The paper reviews different aspects of language modeling for tabular data, including data structures, datasets, models, and challenges

Summary:
The paper provides a comprehensive survey of language modeling techniques for tabular data, covering various aspects such as data types, models, and tasks. It highlights the impact of transformer architectures on tabular data analysis and identifies future research directions.</p><hr><h3>Diff-PCC: Diffusion-based Neural Compression for 3D Point Clouds</h3>
<p><a href='http://arxiv.org/abs/2408.10543v1'>http://arxiv.org/abs/2408.10543v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Diff-PCC, a diffusion-based point cloud compression method that uses dual-space latent representation and diffusion generator to produce high-quality reconstructions with state-of-the-art compression performance.</p><hr><h3>Training Matting Models without Alpha Labels</h3>
<p><a href='http://arxiv.org/abs/2408.10539v1'>http://arxiv.org/abs/2408.10539v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to perform deep image matting using rough annotations like trimaps, and a directional distance consistency loss to infer alpha values at transition areas, achieving comparable or better results than fine-label-supervised methods.</p><hr><h3>Surgical Workflow Recognition and Blocking Effectiveness Detection in  Laparoscopic Liver Resections with Pringle Maneuver</h3>
<p><a href='http://arxiv.org/abs/2408.10538v1'>http://arxiv.org/abs/2408.10538v1</a></p>
<p><b>Compressor summary</b>: The text describes an AI-assisted monitoring system for laparoscopic liver resection that uses a novel dataset, PmLR50, to improve workflow recognition and detect ischemic injury caused by the Pringle maneuver.</p><hr><h3>FAGStyle: Feature Augmentation on Geodesic Surface for Zero-shot  Text-guided Diffusion Image Style Transfer</h3>
<p><a href='http://arxiv.org/abs/2408.10533v1'>http://arxiv.org/abs/2408.10533v1</a></p>
<p><b>Compressor summary</b>: FAGStyle is a text-guided image style transfer method that uses sliding window crop, feature augmentation, and self-correlation consistency to achieve high-quality stylization while preserving content.</p><hr><h3>NutrifyAI: An AI-Powered System for Real-Time Food Detection,  Nutritional Analysis, and Personalized Meal Recommendations</h3>
<p><a href='http://arxiv.org/abs/2408.10532v1'>http://arxiv.org/abs/2408.10532v1</a></p>
<p><b>Compressor summary</b>: The paper presents a computer vision-based food detection and nutrition analysis system that offers personalized meal recommendations through mobile and web applications.</p><hr><h3>NoMatterXAI: Generating "No Matter What" Alterfactual Examples for  Explaining Black-Box Text Classification Models</h3>
<p><a href='http://arxiv.org/abs/2408.10528v1'>http://arxiv.org/abs/2408.10528v1</a></p>
<p><b>Compressor summary</b>: The paper proposes MoMatterXAI, a novel algorithm that generates alterfactual explanations for text classification tasks to ensure AI models' decisions are not biased against specific attributes.</p><hr><h3>EdgeNAT: Transformer for Efficient Edge Detection</h3>
<p><a href='http://arxiv.org/abs/2408.10527v1'>http://arxiv.org/abs/2408.10527v1</a></p>
<p><b>Compressor summary</b>: Key points:
- EdgeNAT is a one-stage transformer-based edge detector with DiNAT as the encoder
- It captures global and local features efficiently
- It uses a novel SCAF-MLA decoder to enhance feature representation
- It achieves state-of-the-art performance on RGB and depth images

Summary:
EdgeNAT is a new edge detector that uses transformers with DiNAT encoder and SCAF-MLA decoder to extract object boundaries and meaningful edges, outperforming existing methods on multiple datasets.</p><hr><h3>XCB: an effective contextual biasing approach to bias cross-lingual  phrases in speech recognition</h3>
<p><a href='http://arxiv.org/abs/2408.10524v1'>http://arxiv.org/abs/2408.10524v1</a></p>
<p><b>Compressor summary</b>: The XCB module improves the recognition of uncommon phrases in bilingual settings by enhancing the dominant language model with auxiliary language biasing and a specific loss, without increasing inference overhead.</p><hr><h3>BAUST Lipi: A BdSL Dataset with Deep Learning Based Bangla Sign Language  Recognition</h3>
<p><a href='http://arxiv.org/abs/2408.10518v1'>http://arxiv.org/abs/2408.10518v1</a></p>
<p><b>Compressor summary</b>: The authors introduce a new Bangla sign language dataset and a hybrid CNN model, achieving 97.92% accuracy in recognizing Bangla signs.</p><hr><h3>Integrating Multi-Modal Input Token Mixer Into Mamba-Based Decision  Models: Decision MetaMamba</h3>
<p><a href='http://arxiv.org/abs/2408.10517v1'>http://arxiv.org/abs/2408.10517v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Decision MetaMamba, a model that enhances offline reinforcement learning by combining patterns from short sequences and using a State Space Model to selectively combine information from distant sequences.</p><hr><h3>Data Augmentation Integrating Dialogue Flow and Style to Adapt Spoken  Dialogue Systems to Low-Resource User Groups</h3>
<p><a href='http://arxiv.org/abs/2408.10516v1'>http://arxiv.org/abs/2408.10516v1</a></p>
<p><b>Compressor summary</b>: The study proposes a data augmentation framework using LLM and PLM to create personalized dialogue data for SDSs engaging with diverse user groups, enhancing their performance.</p><hr><h3>Approximate Estimation of High-dimension Execution Skill for Dynamic  Agents in Continuous Domains</h3>
<p><a href='http://arxiv.org/abs/2408.10512v1'>http://arxiv.org/abs/2408.10512v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a particle-filter-based estimator for human agent's execution error that can handle arbitrary shapes and changes over time, improving AI decision-making assistance.</p><hr><h3>Single-cell Curriculum Learning-based Deep Graph Embedding Clustering</h3>
<p><a href='http://arxiv.org/abs/2408.10511v1'>http://arxiv.org/abs/2408.10511v1</a></p>
<p><b>Compressor summary</b>: scCLG, a novel method for scRNA-seq data clustering, uses ChebAE and selective training to handle challenges such as boundary nodes and high-quality node variation, achieving superior performance compared to existing approaches.</p><hr><h3>QPO: Query-dependent Prompt Optimization via Multi-Loop Offline  Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2408.10504v1'>http://arxiv.org/abs/2408.10504v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Query-dependent Prompt Optimization (QPO), a method that uses reinforcement learning to fine-tune a small language model to generate optimal prompts for large language models, improving their performance on various tasks without frequent interactions.</p><hr><h3>Adaptive Knowledge Distillation for Classification of Hand Images using  Explainable Vision Transformers</h3>
<p><a href='http://arxiv.org/abs/2408.10503v1'>http://arxiv.org/abs/2408.10503v1</a></p>
<p><b>Compressor summary</b>: The paper explores using vision transformers to classify hand images based on their unique features, explains their internal workings, and introduces adaptive distillation methods to transfer knowledge across domains without forgetting.</p><hr><h3>QUITO-X: An Information Bottleneck-based Compression Algorithm with  Cross-Attention</h3>
<p><a href='http://arxiv.org/abs/2408.10497v1'>http://arxiv.org/abs/2408.10497v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new method to compress prompts for generative LLM using cross-attention and information bottleneck theory, achieving better performance and lower latency.</p><hr><h3>GPT-based Textile Pilling Classification Using 3D Point Cloud Data</h3>
<p><a href='http://arxiv.org/abs/2408.10496v1'>http://arxiv.org/abs/2408.10496v1</a></p>
<p><b>Compressor summary</b>: The TextileNet8 dataset is a novel 3D point cloud data set for textile pilling assessment with high accuracy and can be used to train the PointGPT+NN model, which outperforms other models.</p><hr><h3>Clustering by Mining Density Distributions and Splitting Manifold  Structure</h3>
<p><a href='http://arxiv.org/abs/2408.10493v1'>http://arxiv.org/abs/2408.10493v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a new top-down approach for spectral clustering that improves efficiency and adaptability by using local structures, density-based splitting, and a novel similarity measure for micro-clusters.</p><hr><h3>Achieving the Tightest Relaxation of Sigmoids for Formal Verification</h3>
<p><a href='http://arxiv.org/abs/2408.10491v1'>http://arxiv.org/abs/2408.10491v1</a></p>
<p><b>Compressor summary</b>: The paper introduces $\alpha$-sig, a method that improves the convex relaxation of sigmoid activation functions for formal verification by using tunable hyperplanes to create element-wise tight bounds.</p><hr><h3>Is the Lecture Engaging for Learning? Lecture Voice Sentiment Analysis  for Knowledge Graph-Supported Intelligent Lecturing Assistant (ILA) System</h3>
<p><a href='http://arxiv.org/abs/2408.10492v1'>http://arxiv.org/abs/2408.10492v1</a></p>
<p><b>Compressor summary</b>: The paper presents an ILA system that uses a knowledge graph and real-time voice analysis to help instructors improve their lectures' effectiveness and student learning.</p><hr><h3>Analysis of Plan-based Retrieval for Grounded Text Generation</h3>
<p><a href='http://arxiv.org/abs/2408.10490v1'>http://arxiv.org/abs/2408.10490v1</a></p>
<p><b>Compressor summary</b>: The paper explores how planning can help instruction-tuned LLMs reduce hallucinations in long-form text generation by guiding retrieval of relevant facts.</p><hr><h3>Event Stream based Sign Language Translation: A High-Definition  Benchmark Dataset and A New Algorithm</h3>
<p><a href='http://arxiv.org/abs/2408.10488v1'>http://arxiv.org/abs/2408.10488v1</a></p>
<p><b>Compressor summary</b>: This paper introduces Event-CSL, a high-resolution event stream sign language dataset, and a novel baseline method using Mamba model for improved AI-assisted sign language translation in various conditions.</p><hr><h3>MambaEVT: Event Stream based Visual Object Tracking using State Space  Model</h3>
<p><a href='http://arxiv.org/abs/2408.10487v1'>http://arxiv.org/abs/2408.10487v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new event camera-based visual tracking framework that uses Mamba networks for feature extraction, interaction, and dynamic template update to improve accuracy and efficiency.</p><hr><h3>PRformer: Pyramidal Recurrent Transformer for Multivariate Time Series  Forecasting</h3>
<p><a href='http://arxiv.org/abs/2408.10483v1'>http://arxiv.org/abs/2408.10483v1</a></p>
<p><b>Compressor summary</b>: The paper introduces PRformer, a model that combines Pyramid RNN embeddings with Transformer encoder to improve temporal sequence representation and performance on time series prediction tasks.</p><hr><h3>An End-to-End Reinforcement Learning Based Approach for Micro-View  Order-Dispatching in Ride-Hailing</h3>
<p><a href='http://arxiv.org/abs/2408.10479v1'>http://arxiv.org/abs/2408.10479v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a one-stage reinforcement learning method for order-dispatching in ride-hailing services, improving matching efficiency and user experience.</p><hr><h3>Enhancing One-shot Pruned Pre-trained Language Models through  Sparse-Dense-Sparse Mechanism</h3>
<p><a href='http://arxiv.org/abs/2408.10473v1'>http://arxiv.org/abs/2408.10473v1</a></p>
<p><b>Compressor summary</b>: SDS is a pruning framework that enhances the performance of pre-trained language models by optimizing weight distribution through two rounds of sparse pruning.</p><hr><h3>Tracing Privacy Leakage of Language Models to Training Data via Adjusted  Influence Functions</h3>
<p><a href='http://arxiv.org/abs/2408.10468v1'>http://arxiv.org/abs/2408.10468v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Large Language Models can leak sensitive information from training data
- Influence Functions can trace privacy leakage back to the data, but have limitations
- Heuristically Adjusted IF (HAIF) improves tracing accuracy by reducing weights of tokens with large gradient norms
- HAIF is tested on two datasets and outperforms existing methods

Summary:
The paper proposes HAIF, a method to improve the ability of Language Models to trace privacy leakage back to their training data by adjusting the influence of tokens with large gradient norms. HAIF shows better performance than existing Influence Functions on various datasets and scenarios.</p><hr><h3>Learning Multimodal Latent Space with EBM Prior and MCMC Inference</h3>
<p><a href='http://arxiv.org/abs/2408.10467v1'>http://arxiv.org/abs/2408.10467v1</a></p>
<p><b>Compressor summary</b>: The text proposes a method that combines an energy-based model prior with Markov Chain Monte Carlo inference for better multimodal generation across different modalities.</p><hr><h3>Transfer Operator Learning with Fusion Frame</h3>
<p><a href='http://arxiv.org/abs/2408.10458v1'>http://arxiv.org/abs/2408.10458v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new framework that improves transfer learning in solving PDEs by combining fusion frame theory with POD-DeepONet, leading to better performance across different problems.</p>