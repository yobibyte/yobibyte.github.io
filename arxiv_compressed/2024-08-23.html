
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-08-23</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-08-23 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>DreamCinema: Cinematic Transfer with Free Camera and 3D Character</h3>
<p><a href='http://arxiv.org/abs/2408.12601v1'>http://arxiv.org/abs/2408.12601v1</a></p>
<p><b>Compressor summary</b>: DreamCinema is a framework that uses AI to create high-quality, user-friendly films with 3D characters and smooth cinematography.</p><hr><h3>Controllable Text Generation for Large Language Models: A Survey</h3>
<p><a href='http://arxiv.org/abs/2408.12599v1'>http://arxiv.org/abs/2408.12599v1</a></p>
<p><b>Compressor summary</b>: This paper reviews controllable text generation techniques for large language models, discussing different methods, applications, and challenges in meeting complex user needs.</p><hr><h3>ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor  Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2408.12598v1'>http://arxiv.org/abs/2408.12598v1</a></p>
<p><b>Compressor summary</b>: ND-SDF is a novel technique that learns to adaptively use samples for accurate 3D surface reconstruction and rendering with improved quality and preservation of geometric details.</p><hr><h3>Non-Homophilic Graph Pre-Training and Prompt Learning</h3>
<p><a href='http://arxiv.org/abs/2408.12594v1'>http://arxiv.org/abs/2408.12594v1</a></p>
<p><b>Compressor summary</b>: ProNoG is a novel pre-training and prompt learning framework for non-homophilic graphs that considers node-specific characteristics and reduces labeling requirements.</p><hr><h3>Differentiable Logic Programming for Distant Supervision</h3>
<p><a href='http://arxiv.org/abs/2408.12591v1'>http://arxiv.org/abs/2408.12591v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new NeSy method that learns with distant supervision by differentiably reasoning about logical implications using neural network outputs and logic programs embedded in matrices, achieving better accuracy and faster learning than existing methods.</p><hr><h3>xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed  Representations</h3>
<p><a href='http://arxiv.org/abs/2408.12590v1'>http://arxiv.org/abs/2408.12590v1</a></p>
<p><b>Compressor summary</b>: xGen-VideoSyn-1 is a text-to-video model that uses latent diffusion, video variational autoencoder, and diffusion transformer to generate realistic scenes from textual descriptions.</p><hr><h3>Real-Time Video Generation with Pyramid Attention Broadcast</h3>
<p><a href='http://arxiv.org/abs/2408.12588v1'>http://arxiv.org/abs/2408.12588v1</a></p>
<p><b>Compressor summary</b>: PAB is a fast and easy way to generate videos using DiT models, by sharing attention information in a smart way across different steps.</p><hr><h3>Identifying the Best Arm in the Presence of Global Environment Shifts</h3>
<p><a href='http://arxiv.org/abs/2408.12581v1'>http://arxiv.org/abs/2408.12581v1</a></p>
<p><b>Compressor summary</b>: The paper introduces new methods for identifying the best arm in non-stationary stochastic bandits with global environmental shifts, and shows they outperform existing solutions.</p><hr><h3>RuleAlign: Making Large Language Models Better Physicians with  Diagnostic Rule Alignment</h3>
<p><a href='http://arxiv.org/abs/2408.12579v1'>http://arxiv.org/abs/2408.12579v1</a></p>
<p><b>Compressor summary</b>: The RuleAlign framework helps large language models become better at diagnosing patients by aligning them with specific diagnostic rules, using a medical dialogue dataset and preference learning.</p><hr><h3>A Percolation Model of Emergence: Analyzing Transformers Trained on a  Formal Language</h3>
<p><a href='http://arxiv.org/abs/2408.12578v1'>http://arxiv.org/abs/2408.12578v1</a></p>
<p><b>Compressor summary</b>: The authors propose a definition for "emergence" in neural networks as the sudden learning of specific capabilities due to acquiring certain structures from the data-generating process, and empirically show this phenomenon in a Transformer model using a context-sensitive formal language.</p><hr><h3>MuMA-ToM: Multi-modal Multi-Agent Theory of Mind</h3>
<p><a href='http://arxiv.org/abs/2408.12574v1'>http://arxiv.org/abs/2408.12574v1</a></p>
<p><b>Compressor summary</b>: MuMA-ToM is a benchmark for evaluating AI's ability to reason about human mental states in complex social interactions using multiple sources of information.</p><hr><h3>Jamba-1.5: Hybrid Transformer-Mamba Models at Scale</h3>
<p><a href='http://arxiv.org/abs/2408.12570v1'>http://arxiv.org/abs/2408.12570v1</a></p>
<p><b>Compressor summary</b>: Jamba-1.5 is a hybrid language model with high throughput and low memory usage, fine-tuned for conversation and instruction-following, and supported by ExpertsInt8 quantization technique.</p><hr><h3>Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune  CNNs and Transformers</h3>
<p><a href='http://arxiv.org/abs/2408.12568v1'>http://arxiv.org/abs/2408.12568v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to prune large neural networks by optimizing attribution methods, achieving higher compression rates and performance on image classification tasks.</p><hr><h3>ssProp: Energy-Efficient Training for Convolutional Neural Networks with  Scheduled Sparse Back Propagation</h3>
<p><a href='http://arxiv.org/abs/2408.12561v1'>http://arxiv.org/abs/2408.12561v1</a></p>
<p><b>Compressor summary</b>: Our energy-efficient convolution module uses channel-wise sparsity and gradient selection schedulers to reduce computations, improve model performance, and lower the carbon footprint of deep learning training.</p><hr><h3>Comparing YOLOv5 Variants for Vehicle Detection: A Performance Analysis</h3>
<p><a href='http://arxiv.org/abs/2408.12550v1'>http://arxiv.org/abs/2408.12550v1</a></p>
<p><b>Compressor summary</b>: The study compares five YOLOv5 variants for vehicle detection in various environments, evaluating their performance in detecting different types of vehicles under different conditions using precision, recall, F1-score, and mean Average Precision metrics.</p><hr><h3>Human-In-The-Loop Machine Learning for Safe and Ethical Autonomous  Vehicles: Principles, Challenges, and Opportunities</h3>
<p><a href='http://arxiv.org/abs/2408.12548v1'>http://arxiv.org/abs/2408.12548v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Machine Learning (ML) is important for Autonomous Vehicles (AVs) but faces challenges in complex scenarios
- Human-In-The-Loop Machine Learning (HITL-ML) integrates human capabilities to improve ML effectiveness and safety
- HITL-ML includes Curriculum Learning, Human-In-The-Loop Reinforcement Learning, Active Learning, and ethical principles

Summary:
HITL-ML is a promising approach to enhance the performance and safety of AVs by leveraging human input in ML tasks, such as training, optimization, annotation, and ethics.</p><hr><h3>Towards Evaluating and Building Versatile Large Language Models for  Medicine</h3>
<p><a href='http://arxiv.org/abs/2408.12547v1'>http://arxiv.org/abs/2408.12547v1</a></p>
<p><b>Compressor summary</b>: The study introduces MedS-Bench, a benchmark for evaluating large language models in clinical tasks, and MedS-Ins, a dataset for instruction tuning to improve their performance.</p><hr><h3>Dynamics of Meta-learning Representation in the Teacher-student Scenario</h3>
<p><a href='http://arxiv.org/abs/2408.12545v1'>http://arxiv.org/abs/2408.12545v1</a></p>
<p><b>Compressor summary</b>: This paper investigates the dynamics of meta-learning in non-linear two-layer neural networks using statistical physics and highlights the role of hyper-parameters in the formation of shared representations and generalization.</p><hr><h3>Deep Learning Improvements for Sparse Spatial Field Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2408.12531v1'>http://arxiv.org/abs/2408.12531v1</a></p>
<p><b>Compressor summary</b>: The authors improve upon a previous machine learning method for reconstructing global spatial fields from sparse data, achieving better results in Earth Sciences and Fluid Dynamics simulations.</p><hr><h3>Show-o: One Single Transformer to Unify Multimodal Understanding and  Generation</h3>
<p><a href='http://arxiv.org/abs/2408.12528v1'>http://arxiv.org/abs/2408.12528v1</a></p>
<p><b>Compressor summary</b>: Show-o is a unified transformer that combines autoregressive and diffusion modeling for multimodal understanding and generation across various vision-language tasks.</p><hr><h3>Exploiting Student Parallelism for Low-latency GPU Inference of  BERT-like Models in Online Services</h3>
<p><a href='http://arxiv.org/abs/2408.12526v1'>http://arxiv.org/abs/2408.12526v1</a></p>
<p><b>Compressor summary</b>: Academus is a system that uses student parallelism and distillation techniques to reduce online inference latency of BERT-like models without sacrificing accuracy.</p><hr><h3>PCGRL+: Scaling, Control and Generalization in Reinforcement Learning  Level Generators</h3>
<p><a href='http://arxiv.org/abs/2408.12525v1'>http://arxiv.org/abs/2408.12525v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new PCGRL framework using Jax to speed up training and improve scalability, as well as introduce randomized level sizes and pinpoints to enhance designer control, and evaluate the generalization ability of learned generators on large maps.</p><hr><h3>Advanced atom-level representations for protein flexibility prediction  utilizing graph neural networks</h3>
<p><a href='http://arxiv.org/abs/2408.12519v1'>http://arxiv.org/abs/2408.12519v1</a></p>
<p><b>Compressor summary</b>: The authors propose using graph neural networks to learn atomic-level protein representations and predict protein flexibility from 3D structures, outperforming previous methods on a large test set.</p><hr><h3>The Russian-focused embedders' exploration: ruMTEB benchmark and Russian  embedding model design</h3>
<p><a href='http://arxiv.org/abs/2408.12503v1'>http://arxiv.org/abs/2408.12503v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new Russian embedding model, compares it with existing models, and introduces a benchmark for Russian NLP tasks.</p><hr><h3>MEDCO: Medical Education Copilots Based on A Multi-Agent Framework</h3>
<p><a href='http://arxiv.org/abs/2408.12496v1'>http://arxiv.org/abs/2408.12496v1</a></p>
<p><b>Compressor summary</b>: MEDCO is a novel multi-agent-based copilot system for medical education that simulates real-world training environments and enhances student performance and learning behaviors.</p><hr><h3>GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender  Bias in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.12494v1'>http://arxiv.org/abs/2408.12494v1</a></p>
<p><b>Compressor summary</b>: GenderCARE is a framework to assess and reduce gender bias in large language models by introducing criteria, benchmarks, and debiasing techniques.</p><hr><h3>AI in radiological imaging of soft-tissue and bone tumours: a systematic  review evaluating against CLAIM and FUTURE-AI guidelines</h3>
<p><a href='http://arxiv.org/abs/2408.12491v1'>http://arxiv.org/abs/2408.12491v1</a></p>
<p><b>Compressor summary</b>: The systematic review examines radiology-based AI methods for diagnosing and prognosing soft-tissue and bone tumours, finding that they perform poorly on current guidelines and need improvement in design, development, evaluation, and data reproducibility.</p><hr><h3>Not All Samples Should Be Utilized Equally: Towards Understanding and  Improving Dataset Distillation</h3>
<p><a href='http://arxiv.org/abs/2408.12483v1'>http://arxiv.org/abs/2408.12483v1</a></p>
<p><b>Compressor summary</b>: The paper explores sample difficulty in dataset distillation, proposes a theoretical explanation for matching-based methods, and introduces the Sample Difficulty Correction approach to improve dataset quality.</p><hr><h3>Vintern-1B: An Efficient Multimodal Large Language Model for Vietnamese</h3>
<p><a href='http://arxiv.org/abs/2408.12480v1'>http://arxiv.org/abs/2408.12480v1</a></p>
<p><b>Compressor summary</b>: The report introduces Vintern-1B, a multimodal large language model for Vietnamese tasks that combines Qwen2 and InternViT models, fine-tuned on a large dataset, and optimized for on-device applications.</p><hr><h3>Predicting Solar Energy Generation with Machine Learning based on AQI  and Weather Features</h3>
<p><a href='http://arxiv.org/abs/2408.12476v1'>http://arxiv.org/abs/2408.12476v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Paper proposes a solar energy prediction model using Machine Learning and Deep Learning
- Model considers Air Quality Index and weather features as influencing factors
- Model uses power transform normalization and zero-inflated modeling
- Achieves high accuracy and low error with Conv2D Long Short-Term Memory model

Summary:
The paper presents a Machine Learning and Deep Learning based solar energy prediction model that considers Air Quality Index and weather features, using novel techniques like power transform normalization and zero-inflated modeling, and achieves high accuracy with Conv2D Long Short-Term Memory model.</p><hr><h3>Envisioning Class Entity Reasoning by Large Language Models for Few-shot  Learning</h3>
<p><a href='http://arxiv.org/abs/2408.12469v1'>http://arxiv.org/abs/2408.12469v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new framework that combines abstract class semantics and concrete class entities from language models to improve few-shot learning by extracting semantic-aware visual patterns and refining class prototypes.</p><hr><h3>Smartphone-based Eye Tracking System using Edge Intelligence and Model  Optimisation</h3>
<p><a href='http://arxiv.org/abs/2408.12463v1'>http://arxiv.org/abs/2408.12463v1</a></p>
<p><b>Compressor summary</b>: The authors developed two new eye-tracking techniques for video-type visuals using deep learning models and optimized them for smartphones' resource constraints.</p><hr><h3>Finding Closure: A Closer Look at the Gestalt Law of Closure in  Convolutional Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2408.12460v1'>http://arxiv.org/abs/2408.12460v1</a></p>
<p><b>Compressor summary</b>: The study investigates whether neural networks use Closure, a human visual skill for filling in missing parts of objects, by testing various Convolutional Neural Networks (CNNs) with curated datasets and reveals mixed results, suggesting some CNNs exhibit the Closure effect.</p><hr><h3>Enhancing Multi-hop Reasoning through Knowledge Erasure in Large  Language Model Editing</h3>
<p><a href='http://arxiv.org/abs/2408.12456v1'>http://arxiv.org/abs/2408.12456v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Large language models have internal inaccuracies and outdated knowledge
- Current knowledge editing techniques are good for single-hop reasoning but not for multi-hop reasoning
- The proposed KELE method uses erasure and injection functions to improve multi-hop reasoning

Summary:
The paper proposes a novel knowledge editing method, KELE, that addresses the limitations of current methods in improving large language models' multi-hop reasoning skills by using erasure and injection functions.</p><hr><h3>Relaxed Rotational Equivariance via $G$-Biases in Vision</h3>
<p><a href='http://arxiv.org/abs/2408.12454v1'>http://arxiv.org/abs/2408.12454v1</a></p>
<p><b>Compressor summary</b>: RREConv is a method to handle rotational symmetry breaking in data by using learnable biases under the group order to relax strict group constraints.</p><hr><h3>A Riemannian Approach for Spatiotemporal Analysis and Generation of 4D  Tree-shaped Structures</h3>
<p><a href='http://arxiv.org/abs/2408.12443v1'>http://arxiv.org/abs/2408.12443v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new approach to model, analyze, and generate tree-like 4D objects that change shape and structure over time by representing them as elastic trajectories in a square root velocity function tree space with a Riemannian metric and statistical models.</p><hr><h3>Adapting MIMO video restoration networks to low latency constraints</h3>
<p><a href='http://arxiv.org/abs/2408.12439v1'>http://arxiv.org/abs/2408.12439v1</a></p>
<p><b>Compressor summary</b>: The paper proposes solutions to improve MIMO video restoration by increasing temporal receptive field and smoothing discontinuities at stack transitions, achieving state-of-the-art low-latency performance on a new drone footage benchmark.</p><hr><h3>Positional Description for Numerical Normalization</h3>
<p><a href='http://arxiv.org/abs/2408.12430v1'>http://arxiv.org/abs/2408.12430v1</a></p>
<p><b>Compressor summary</b>: The Positional Description Scheme (PDS) improves language models' arithmetic processing by simplifying number normalization and reducing errors in text-to-speech and speech recognition tasks.</p><hr><h3>FlexEdit: Marrying Free-Shape Masks to VLLM for Flexible Image Editing</h3>
<p><a href='http://arxiv.org/abs/2408.12429v1'>http://arxiv.org/abs/2408.12429v1</a></p>
<p><b>Compressor summary</b>: FlexEdit is an image editing method that uses free-shape masks and language instructions to achieve state-of-the-art performance in LLM-based image editing.</p><hr><h3>Enhanced Infield Agriculture with Interpretable Machine Learning  Approaches for Crop Classification</h3>
<p><a href='http://arxiv.org/abs/2408.12426v1'>http://arxiv.org/abs/2408.12426v1</a></p>
<p><b>Compressor summary</b>: The text discusses various image classification techniques for crop identification in agriculture and evaluates their accuracy, model size, prediction time, and explainability using Xception as the best performer.</p><hr><h3>Multi-Knowledge Fusion Network for Time Series Representation Learning</h3>
<p><a href='http://arxiv.org/abs/2408.12423v1'>http://arxiv.org/abs/2408.12423v1</a></p>
<p><b>Compressor summary</b>: The text proposes a hybrid method that combines prior knowledge with relational structure of multivariate time series data to improve forecast accuracy and uncertainty estimation.</p><hr><h3>Dataset | Mindset = Explainable AI | Interpretable AI</h3>
<p><a href='http://arxiv.org/abs/2408.12420v1'>http://arxiv.org/abs/2408.12420v1</a></p>
<p><b>Compressor summary</b>: XAI is a subset of IAI, which involves a mindset of abstraction and focuses on post-hoc analysis of a dataset, while IAI encompasses both outward and inward reasons for interpreting AI.</p><hr><h3>CODE: Confident Ordinary Differential Editing</h3>
<p><a href='http://arxiv.org/abs/2408.12418v1'>http://arxiv.org/abs/2408.12418v1</a></p>
<p><b>Compressor summary</b>: CODE is a novel image editing method that uses diffusion models and ordinary differential equations to enhance images based on noisy or out-of-distribution guidance while maintaining realism and fidelity.</p><hr><h3>An Evaluation of Deep Learning Models for Stock Market Trend Prediction</h3>
<p><a href='http://arxiv.org/abs/2408.12408v1'>http://arxiv.org/abs/2408.12408v1</a></p>
<p><b>Compressor summary</b>: The study explores various deep learning models for short-term stock market trend prediction using daily and hourly prices, finding that xLSTM-TS performs best.</p><hr><h3>Multi-Source Knowledge-Based Hybrid Neural Framework for Time Series  Representation Learning</h3>
<p><a href='http://arxiv.org/abs/2408.12409v1'>http://arxiv.org/abs/2408.12409v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a hybrid method that combines domain knowledge and relational structure inference to improve forecasting of complex dynamical systems with high-dimensional multivariate time series data.</p><hr><h3>Multi-Style Facial Sketch Synthesis through Masked Generative Modeling</h3>
<p><a href='http://arxiv.org/abs/2408.12400v1'>http://arxiv.org/abs/2408.12400v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new model for generating high-quality multi-stylized sketch portraits from images using semi-supervised learning and feature extraction, achieving better performance than previous methods.</p><hr><h3>Cross-Domain Foundation Model Adaptation: Pioneering Computer Vision  Models for Geophysical Data Analysis</h3>
<p><a href='http://arxiv.org/abs/2408.12396v1'>http://arxiv.org/abs/2408.12396v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The study explores using computer vision foundation models (FMs) for geoscience tasks
- The workflow fine-tunes existing FMs for different geoscientific data types
- The experiments show the effectiveness and advantages of cross-domain FMs adaptation

Summary:
The study adapts computer vision foundation models to various geoscientific data analysis tasks, demonstrating their feasibility and benefits.</p><hr><h3>Sampling Strategies based on Wisdom of Crowds for Amazon Deforestation  Detection</h3>
<p><a href='http://arxiv.org/abs/2408.12381v1'>http://arxiv.org/abs/2408.12381v1</a></p>
<p><b>Compressor summary</b>: ForestEyes is a Citizen Science project using Machine Learning models to monitor deforestation and improve its effectiveness by selecting optimal samples from the training set based on user entropy-increasing strategy.</p><hr><h3>UMERegRobust -- Universal Manifold Embedding Compatible Features for  Robust Point Cloud Registration</h3>
<p><a href='http://arxiv.org/abs/2408.12380v1'>http://arxiv.org/abs/2408.12380v1</a></p>
<p><b>Compressor summary</b>: The paper presents UMERegRobust, a robust registration pipeline that handles partial overlap and differently sampled point clouds using UME framework, and shows its superior performance on KITTI and RotKITTI benchmarks.</p><hr><h3>Cell-ontology guided transcriptome foundation model</h3>
<p><a href='http://arxiv.org/abs/2408.12373v1'>http://arxiv.org/abs/2408.12373v1</a></p>
<p><b>Compressor summary</b>: The text describes a new transcriptome foundation model (scCello) that leverages cell ontology information to learn gene co-expression patterns and improve biological tasks such as identifying novel cell types or predicting drug responses.</p><hr><h3>RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for  Enhanced Query Precision in Tabular Question Answering</h3>
<p><a href='http://arxiv.org/abs/2408.12369v1'>http://arxiv.org/abs/2408.12369v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework that uses full-text search to improve query accuracy and user experience when querying complex databases with natural language.</p><hr><h3>Robust Principal Component Analysis via Discriminant Sample Weight  Learning</h3>
<p><a href='http://arxiv.org/abs/2408.12366v1'>http://arxiv.org/abs/2408.12366v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a robust PCA method that learns discriminative sample weights to mitigate the impact of outliers on feature extraction.</p><hr><h3>CLEANANERCorp: Identifying and Correcting Incorrect Labels in the  ANERcorp Dataset</h3>
<p><a href='http://arxiv.org/abs/2408.12362v1'>http://arxiv.org/abs/2408.12362v1</a></p>
<p><b>Compressor summary</b>: The paper investigates label errors in an Arabic Named Entity Recognition dataset, corrects them, and proposes a cleaner version called CLEANANERCorp for better model training and evaluation.</p><hr><h3>Class-balanced Open-set Semi-supervised Object Detection for Medical  Images</h3>
<p><a href='http://arxiv.org/abs/2408.12355v1'>http://arxiv.org/abs/2408.12355v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an open-set semi-supervised object detection method for medical images that handles class imbalance and utilizes out-of-distribution information to improve detections.</p><hr><h3>GarmentAligner: Text-to-Garment Generation via Retrieval-augmented  Multi-level Corrections</h3>
<p><a href='http://arxiv.org/abs/2408.12352v1'>http://arxiv.org/abs/2408.12352v1</a></p>
<p><b>Compressor summary</b>: GarmentAligner is a text-to-garment diffusion model that uses retrieval augmentation, automatic component extraction, and multi-level correction losses to generate accurate and aligned garments from texts.</p><hr><h3>VTON-HandFit: Virtual Try-on for Arbitrary Hand Pose Guided by Hand  Priors Embedding</h3>
<p><a href='http://arxiv.org/abs/2408.12340v1'>http://arxiv.org/abs/2408.12340v1</a></p>
<p><b>Compressor summary</b>: VTON-HandFit is a method that uses hand priors to improve virtual try-on performance, especially for hand occlusion cases.</p><hr><h3>Fine-tuning Smaller Language Models for Question Answering over  Financial Documents</h3>
<p><a href='http://arxiv.org/abs/2408.12337v1'>http://arxiv.org/abs/2408.12337v1</a></p>
<p><b>Compressor summary</b>: Smaller language models can learn financial reasoning by fine-tuning with larger teacher models and generating programs to encode calculations.</p><hr><h3>Enhanced Expressivity in Graph Neural Networks with Lanczos-Based Linear  Constraints</h3>
<p><a href='http://arxiv.org/abs/2408.12334v1'>http://arxiv.org/abs/2408.12334v1</a></p>
<p><b>Compressor summary</b>: Our method improves link prediction tasks for graph neural networks by embedding subgraphs in the Laplacian matrix's eigenbasis using a novel Learnable Lanczos algorithm with Linear Constraints, achieving significant speedup and performance improvement with less training data.</p><hr><h3>Graph Retrieval Augmented Trustworthiness Reasoning</h3>
<p><a href='http://arxiv.org/abs/2408.12333v1'>http://arxiv.org/abs/2408.12333v1</a></p>
<p><b>Compressor summary</b>: GRATR is a framework that uses a dynamic trustworthiness graph and retrieval-augmented generation to improve trust reasoning in multiplayer games, outperforming baseline methods by 30% or more.</p><hr><h3>Interactive DualChecker for Mitigating Hallucinations in Distilling  Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2408.12326v1'>http://arxiv.org/abs/2408.12326v1</a></p>
<p><b>Compressor summary</b>: DualChecker is a framework that improves knowledge distillation between teacher and student models using an interactive dynamic checker system to mitigate hallucinations and enhance performance in machine learning tasks.</p><hr><h3>Improving Factuality in Large Language Models via Decoding-Time  Hallucinatory and Truthful Comparators</h3>
<p><a href='http://arxiv.org/abs/2408.12325v1'>http://arxiv.org/abs/2408.12325v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a CDT framework to reduce unfaithful hallucinations in LLMs by comparing their responses to truthful ones using multi-task fine-tuning and mixture of experts strategies.</p><hr><h3>MaVEn: An Effective Multi-granularity Hybrid Visual Encoding Framework  for Multimodal Large Language Model</h3>
<p><a href='http://arxiv.org/abs/2408.12321v1'>http://arxiv.org/abs/2408.12321v1</a></p>
<p><b>Compressor summary</b>: MaVEn is a framework that improves multimodal language models' ability to reason with multiple images by combining coarse-grained visual symbols with fine-grained features and using a dynamic reduction mechanism.</p><hr><h3>PolyRouter: A Multi-LLM Querying System</h3>
<p><a href='http://arxiv.org/abs/2408.12320v1'>http://arxiv.org/abs/2408.12320v1</a></p>
<p><b>Compressor summary</b>: PolyRouter is a system that combines different large language models to answer queries efficiently, cheaply, and with high quality.</p><hr><h3>Adapt CLIP as Aggregation Instructor for Image Dehazing</h3>
<p><a href='http://arxiv.org/abs/2408.12317v1'>http://arxiv.org/abs/2408.12317v1</a></p>
<p><b>Compressor summary</b>: CLIPHaze is a hybrid framework that combines Mamba and CLIP to improve dehazing by using parallel state space model and window-based self-attention, along with a novel aggregation module that adapts to different haze types.</p><hr><h3>Unrolled Decomposed Unpaired Learning for Controllable Low-Light Video  Enhancement</h3>
<p><a href='http://arxiv.org/abs/2408.12316v1'>http://arxiv.org/abs/2408.12316v1</a></p>
<p><b>Compressor summary</b>: The paper proposes UDU-Net, a network that enhances low-light videos by decomposing the signal into spatial and temporal factors and updating them iteratively using expert knowledge and human feedback.</p><hr><h3>Large Language Models Are Self-Taught Reasoners: Enhancing LLM  Applications via Tailored Problem-Solving Demonstrations</h3>
<p><a href='http://arxiv.org/abs/2408.12315v1'>http://arxiv.org/abs/2408.12315v1</a></p>
<p><b>Compressor summary</b>: The paper proposes SELF-TAUGHT, a framework that creates customized demonstrations for large language models to improve their performance in various domains, such as clinical diagnosis.</p><hr><h3>MakeupAttack: Feature Space Black-box Backdoor Attack on Face  Recognition via Makeup Transfer</h3>
<p><a href='http://arxiv.org/abs/2408.12312v1'>http://arxiv.org/abs/2408.12312v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Backdoor attacks threaten face recognition systems
- Existing attacks are simple and visible
- MakeupAttack is a novel feature space attack via makeup transfer
- It only requires model queries and can bypass defenses

Summary:
MakeupAttack is a new backdoor attack on face recognition that uses subtle makeup features to manipulate models without full access or detection.</p><hr><h3>Deep Learning with CNNs: A Compact Holistic Tutorial with Focus on  Supervised Regression (Preprint)</h3>
<p><a href='http://arxiv.org/abs/2408.12308v1'>http://arxiv.org/abs/2408.12308v1</a></p>
<p><b>Compressor summary</b>: This tutorial provides a comprehensive and accessible introduction to Deep Learning with CNNs and supervised regression, emphasizing the connections between learning theory, statistics, and machine learning.</p><hr><h3>Leveraging Unlabeled Data Sharing through Kernel Function Approximation  in Offline Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2408.12307v1'>http://arxiv.org/abs/2408.12307v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an algorithm that uses unlabelled data in offline reinforcement learning with kernel function approximation and proves its complexity properties.</p><hr><h3>Tipta uzmanlik sinavinda (tus) büyük dil modelleri insanlardan  daha mi başarili?</h3>
<p><a href='http://arxiv.org/abs/2408.12305v1'>http://arxiv.org/abs/2408.12305v1</a></p>
<p><b>Compressor summary</b>: The study shows that artificial intelligence models can answer medical questions accurately and may improve medical education and assessment.</p><hr><h3>OPTDTALS: Approximate Logic Synthesis via Optimal Decision Trees  Approach</h3>
<p><a href='http://arxiv.org/abs/2408.12304v1'>http://arxiv.org/abs/2408.12304v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new Approximate Logic Synthesis method using optimal decision trees to balance circuit complexity and accuracy, outperforming existing methods.</p>