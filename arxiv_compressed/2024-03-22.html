
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-03-22</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-03-22 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>On Pretraining Data Diversity for Self-Supervised Learning</h3>
<p><a href='http://arxiv.org/abs/2403.13808v1'>http://arxiv.org/abs/2403.13808v1</a></p>
<p><b>Compressor summary</b>: Increasing data diversity in self-supervised learning improves performance only when the distribution distance to downstream data is small.</p><hr><h3>Editing Massive Concepts in Text-to-Image Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2403.13807v1'>http://arxiv.org/abs/2403.13807v1</a></p>
<p><b>Compressor summary</b>: EMCID is a two-stage method that edits massive concepts in text-to-image diffusion models, addressing risks such as outdated content and copyright infringement, while offering scalability for real-world applications.</p><hr><h3>RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition</h3>
<p><a href='http://arxiv.org/abs/2403.13805v1'>http://arxiv.org/abs/2403.13805v1</a></p>
<p><b>Compressor summary</b>: CLIP and MLLMs have complementary strengths for vision-language recognition tasks; RAR combines them to improve accuracy on fine-grained, few-shot, and zero-shot recognition.</p><hr><h3>RadSplat: Radiance Field-Informed Gaussian Splatting for Robust  Real-Time Rendering with 900+ FPS</h3>
<p><a href='http://arxiv.org/abs/2403.13806v1'>http://arxiv.org/abs/2403.13806v1</a></p>
<p><b>Compressor summary</b>: RadSplat is a lightweight real-time rendering method that leverages radiance fields for scene representation and optimization, pruning points for compactness, and test-time filtering for speed, achieving state-of-the-art results on complex scenes.</p><hr><h3>Learning from Models and Data for Visual Grounding</h3>
<p><a href='http://arxiv.org/abs/2403.13804v1'>http://arxiv.org/abs/2403.13804v1</a></p>
<p><b>Compressor summary</b>: SynGround is a framework that enhances vision-and-language models by combining data-driven learning, knowledge transfer, and mask-attention consistency to improve grounding capabilities and performance on pointing games.</p><hr><h3>Bounding Box Stability against Feature Dropout Reflects Detector  Generalization across Environments</h3>
<p><a href='http://arxiv.org/abs/2403.13803v1'>http://arxiv.org/abs/2403.13803v1</a></p>
<p><b>Compressor summary</b>: The authors propose a box stability score (BoS) that reflects the stability of bounding boxes in object detection, which correlates with detector accuracy and can be used to assess detectors without test ground truths.</p><hr><h3>ZigMa: Zigzag Mamba Diffusion Model</h3>
<p><a href='http://arxiv.org/abs/2403.13802v1'>http://arxiv.org/abs/2403.13802v1</a></p>
<p><b>Compressor summary</b>: The study proposes Zigzag Mamba, a method that improves speed and memory utilization for visual data generation by addressing spatial continuity issues in the State-Space Model Mamba.</p><hr><h3>TimeRewind: Rewinding Time with Image-and-Events Video Diffusion</h3>
<p><a href='http://arxiv.org/abs/2403.13800v1'>http://arxiv.org/abs/2403.13800v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method to generate videos by rewinding time from a single image using neuromorphic event cameras and diffusion models, showing promising results for capturing missed moments in computer vision and photography.</p><hr><h3>Reverse Training to Nurse the Reversal Curse</h3>
<p><a href='http://arxiv.org/abs/2403.13799v1'>http://arxiv.org/abs/2403.13799v1</a></p>
<p><b>Compressor summary</b>: The paper proposes reverse training for large language models to improve their ability to handle reverse relations, such as "B is a feature of A", by doubling the available tokens and training in both forward and reverse directions.</p><hr><h3>Hierarchical NeuroSymbolic Approach for Action Quality Assessment</h3>
<p><a href='http://arxiv.org/abs/2403.13798v1'>http://arxiv.org/abs/2403.13798v1</a></p>
<p><b>Compressor summary</b>: The text introduces a neuro-symbolic approach for action quality assessment using computer vision that is more transparent, unbiased, and informative than existing neural models, and applies it to diving.</p><hr><h3>Bridge the Modality and Capacity Gaps in Vision-Language Model Selection</h3>
<p><a href='http://arxiv.org/abs/2403.13797v1'>http://arxiv.org/abs/2403.13797v1</a></p>
<p><b>Compressor summary</b>: SWAB is a method that uses optimal transport to transfer statistics between open-source and target datasets, improving zero-shot image classification by selecting the best Pre-Trained VLM from the VLM Zoo based on text data only.</p><hr><h3>Evaluating Frontier Models for Dangerous Capabilities</h3>
<p><a href='http://arxiv.org/abs/2403.13793v1'>http://arxiv.org/abs/2403.13793v1</a></p>
<p><b>Compressor summary</b>: The authors evaluate Gemini 1.0 AI models on four potential dangerous capabilities and find no strong evidence of risk, but highlight early warning signs.</p><hr><h3>DepthFM: Fast Monocular Depth Estimation with Flow Matching</h3>
<p><a href='http://arxiv.org/abs/2403.13788v1'>http://arxiv.org/abs/2403.13788v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a generative method for monocular depth estimation that uses image diffusion as a prior and flow matching to efficiently map from input images to depth maps, achieving state-of-the-art results with low computational cost.</p><hr><h3>RewardBench: Evaluating Reward Models for Language Modeling</h3>
<p><a href='http://arxiv.org/abs/2403.13787v1'>http://arxiv.org/abs/2403.13787v1</a></p>
<p><b>Compressor summary</b>: RewardBench is a benchmark dataset and code-base to evaluate reward models in language models alignment, revealing their strengths and weaknesses in chat, reasoning, and safety tasks.</p><hr><h3>Chain-of-Interaction: Enhancing Large Language Models for Psychiatric  Behavior Understanding by Dyadic Contexts</h3>
<p><a href='http://arxiv.org/abs/2403.13786v1'>http://arxiv.org/abs/2403.13786v1</a></p>
<p><b>Compressor summary</b>: The Chain-of-Interaction (CoI) prompting method helps large language models understand patient behaviors during motivational interviewing by considering the coding scheme, therapist question strategies, and dyadic interactions between patients and therapists.</p><hr><h3>Towards an extension of Fault Trees in the Predictive Maintenance  Scenario</h3>
<p><a href='http://arxiv.org/abs/2403.13785v1'>http://arxiv.org/abs/2403.13785v1</a></p>
<p><b>Compressor summary</b>: The paper presents an extension of Fault Trees to model and analyze Predictive Maintenance problems in modern systems.</p><hr><h3>The Model Openness Framework: Promoting Completeness and Openness for  Reproducibility, Transparency and Usability in AI</h3>
<p><a href='http://arxiv.org/abs/2403.13784v1'>http://arxiv.org/abs/2403.13784v1</a></p>
<p><b>Compressor summary</b>: The Model Openness Framework (MOF) is a system to rate and promote openness in generative AI models, addressing concerns about transparency, reproducibility, bias, and safety.</p><hr><h3>Sparse Implementation of Versatile Graph-Informed Layers</h3>
<p><a href='http://arxiv.org/abs/2403.13781v1'>http://arxiv.org/abs/2403.13781v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a sparse implementation of Graph-Informed layers for Graph Neural Networks that reduces memory usage and improves computational efficiency, enabling deeper and more scalable models on large graphs.</p><hr><h3>Information-Theoretic Distillation for Reference-less Summarization</h3>
<p><a href='http://arxiv.org/abs/2403.13780v1'>http://arxiv.org/abs/2403.13780v1</a></p>
<p><b>Compressor summary</b>: InfoSumm is a novel framework that distills a powerful summarizer using an information-theoretic objective without relying on large-scale language models or human references.</p><hr><h3>Describe-and-Dissect: Interpreting Neurons in Vision Networks with  Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.13771v1'>http://arxiv.org/abs/2403.13771v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Describe-and-Dissect (DnD), a method that uses multimodal deep learning to generate natural language descriptions of hidden neurons in vision networks without training data or predefined concepts, and shows its superiority over prior work.</p><hr><h3>Towards Principled Representation Learning from Videos for Reinforcement  Learning</h3>
<p><a href='http://arxiv.org/abs/2403.13765v1'>http://arxiv.org/abs/2403.13765v1</a></p>
<p><b>Compressor summary</b>: The paper investigates the theoretical and empirical aspects of learning latent state representations from video data for decision-making tasks, finding that temporal contrastive learning and forward modeling perform well in settings with only iid noise but struggle with exogenous noise.</p><hr><h3>Practical End-to-End Optical Music Recognition for Pianoform Music</h3>
<p><a href='http://arxiv.org/abs/2403.13763v1'>http://arxiv.org/abs/2403.13763v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new format for optical music recognition (OMR) models, called Linearized MusicXML, which allows them to read input images and produce a linear sequence of tokens compatible with industry standards, and creates a benchmark dataset based on the OpenScore Lieder corpus.</p><hr><h3>HierCode: A Lightweight Hierarchical Codebook for Zero-shot Chinese Text  Recognition</h3>
<p><a href='http://arxiv.org/abs/2403.13761v1'>http://arxiv.org/abs/2403.13761v1</a></p>
<p><b>Compressor summary</b>: HierCode is a lightweight codebook that uses a multi-hot encoding strategy to represent Chinese characters hierarchically and facilitate zero-shot recognition of out-of-vocabulary characters, achieving state-of-the-art performance with fast inference speed.</p><hr><h3>Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge  Augmentation in Vision Language Model</h3>
<p><a href='http://arxiv.org/abs/2403.13756v1'>http://arxiv.org/abs/2403.13756v1</a></p>
<p><b>Compressor summary</b>: The authors propose a model that uses a pre-trained vision language model to improve the understanding of patient gait videos by learning from text, video, and numerical data, achieving better performance than previous methods.</p><hr><h3>Different Tokenization Schemes Lead to Comparable Performance in Spanish  Number Agreement</h3>
<p><a href='http://arxiv.org/abs/2403.13754v1'>http://arxiv.org/abs/2403.13754v1</a></p>
<p><b>Compressor summary</b>: The study examines how different ways of splitting words affect Spanish plural nouns' agreement, finding that a method based on word structure performs similarly to other methods and is not necessary for good performance.</p><hr><h3>Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph  Representational Learning</h3>
<p><a href='http://arxiv.org/abs/2403.13749v1'>http://arxiv.org/abs/2403.13749v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new graph isomorphism test hierarchy, $r$-loopy Weisfeiler-Leman (r-WL), that can count homomorphisms of cactus graphs and a corresponding GNN framework, r-MPNN, which performs well on various datasets.</p><hr><h3>Leveraging High-Resolution Features for Improved Deep Hashing-based  Image Retrieval</h3>
<p><a href='http://arxiv.org/abs/2403.13747v1'>http://arxiv.org/abs/2403.13747v1</a></p>
<p><b>Compressor summary</b>: The authors propose HHNet, a method that uses High-Resolution Networks (HRNets) to learn high-resolution features for efficient image retrieval, outperforming existing methods on various datasets.</p><hr><h3>Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific  Adaptation</h3>
<p><a href='http://arxiv.org/abs/2403.13745v1'>http://arxiv.org/abs/2403.13745v1</a></p>
<p><b>Compressor summary</b>: MOTIA is a diffusion-based method that adapts to input videos and uses learned patterns for effective video outpainting, achieving superior results with minimal tuning.</p><hr><h3>Uncertainty-Aware Explanations Through Probabilistic Self-Explainable  Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2403.13740v1'>http://arxiv.org/abs/2403.13740v1</a></p>
<p><b>Compressor summary</b>: Prob-PSENN is a probabilistic reformulation of Deep Neural Networks that offers transparent, flexible, and uncertain prediction explanations using probability distributions over prototypes.</p><hr><h3>EthioLLM: Multilingual Large Language Models for Ethiopian Languages  with Task Evaluation</h3>
<p><a href='http://arxiv.org/abs/2403.13737v1'>http://arxiv.org/abs/2403.13737v1</a></p>
<p><b>Compressor summary</b>: The paper introduces EthioLLM, multilingual large language models for five Ethiopian languages and English, and Ethiobenchmark, a new benchmark dataset for various NLP tasks, to improve the state of low-resource language NLP in Ethiopia.</p><hr><h3>M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via  Multiplier Induced Loss Landscape Scheduling</h3>
<p><a href='http://arxiv.org/abs/2403.13728v1'>http://arxiv.org/abs/2403.13728v1</a></p>
<p><b>Compressor summary</b>: The authors propose a probabilistic graphical model to optimize neural network parameters and weight multipliers jointly, using a hypervolume based likelihood that promotes descent of each loss term, resulting in a multiplier-free method that saves computational resources and outperforms other methods.</p><hr><h3>Probabilistic Forecasting with Stochastic Interpolants and FÃ¶llmer  Processes</h3>
<p><a href='http://arxiv.org/abs/2403.13724v1'>http://arxiv.org/abs/2403.13724v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Framework for probabilistic forecasting based on generative modeling of dynamical systems
- Stochastic interpolants enable construction of a generative model between base and target distributions
- Fictitious non-physical stochastic dynamics produces samples from target conditional distribution
- Drift coefficient can be learned efficiently by square loss regression
- Approach works on complex, high-dimensional problems like fluid dynamics and video prediction

Summary:
The paper presents a generative modeling framework for probabilistic forecasting of dynamical systems using stochastic interpolants and a non-physical stochastic dynamics that learns the drift coefficient from data and handles complex problems.</p><hr><h3>Research Re: search & Re-search</h3>
<p><a href='http://arxiv.org/abs/2403.13705v1'>http://arxiv.org/abs/2403.13705v1</a></p>
<p><b>Compressor summary</b>: The text discusses two types of search algorithms (depth-first and best-first) for minimax games and challenges the prevailing opinion that best-first algorithms are more efficient but less practical than depth-first algorithms.</p><hr><h3>Fostc3net:A Lightweight YOLOv5 Based On the Network Structure  Optimization</h3>
<p><a href='http://arxiv.org/abs/2403.13703v1'>http://arxiv.org/abs/2403.13703v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a lightweight YOLOv5 technique for detecting transmission line objects on mobile devices with improved efficiency and accuracy by integrating C3Ghost and FasterNet modules and using wIoUv3 loss function.</p><hr><h3>SPTNet: An Efficient Alternative Framework for Generalized Category  Discovery with Spatial Prompt Tuning</h3>
<p><a href='http://arxiv.org/abs/2403.13684v1'>http://arxiv.org/abs/2403.13684v1</a></p>
<p><b>Compressor summary</b>: SPTNet adapts both model and data representation for generalized category discovery, using a novel spatial prompt tuning method that focuses on object parts and achieves state-of-the-art performance with minimal additional parameters.</p><hr><h3>DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses</h3>
<p><a href='http://arxiv.org/abs/2403.13683v1'>http://arxiv.org/abs/2403.13683v1</a></p>
<p><b>Compressor summary</b>: The Deep Voxel Matching Network (DVMNet) is a method that computes the relative pose of an object between two images in 3D space using voxels and a least-squares problem, achieving more accurate results and lower computational cost than existing methods.</p><hr><h3>PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned  Language Model for Indian Legal Case Documents</h3>
<p><a href='http://arxiv.org/abs/2403.13681v1'>http://arxiv.org/abs/2403.13681v1</a></p>
<p><b>Compressor summary</b>: The paper introduces PARAMANU-AYN, a legal language model based on Indian laws and constitution that can perform various legal tasks without much data or fine-tuning.</p><hr><h3>RoleInteract: Evaluating the Social Interaction of Role-Playing Agents</h3>
<p><a href='http://arxiv.org/abs/2403.13679v1'>http://arxiv.org/abs/2403.13679v1</a></p>
<p><b>Compressor summary</b>: RoleInteract is a new benchmark to evaluate social intelligence in AI conversational agents that mimic diverse characters and human behaviors.</p><hr><h3>AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and  GPT-2 in Wild Audiovisual Contexts</h3>
<p><a href='http://arxiv.org/abs/2403.13678v1'>http://arxiv.org/abs/2403.13678v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel audio-visual approach to improve facial action unit detection by using advanced features, adaptive fusion, and context-aware modeling.</p><hr><h3>Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into  Vision Transformers</h3>
<p><a href='http://arxiv.org/abs/2403.13677v1'>http://arxiv.org/abs/2403.13677v1</a></p>
<p><b>Compressor summary</b>: The proposed Retina Vision Transformer (RetinaViT) model incorporates low spatial frequency components in the input to improve visual scene formation and achieve better performance than the original ViT on ImageNet-1K dataset.</p><hr><h3>Machine Learning Optimized Approach for Parameter Selection in MESHFREE  Simulations</h3>
<p><a href='http://arxiv.org/abs/2403.13672v1'>http://arxiv.org/abs/2403.13672v1</a></p>
<p><b>Compressor summary</b>: The paper presents an ML-optimized approach to optimize parameters in meshfree simulation software, improving its usability and performance for various applications.</p><hr><h3>DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance</h3>
<p><a href='http://arxiv.org/abs/2403.13667v1'>http://arxiv.org/abs/2403.13667v1</a></p>
<p><b>Compressor summary</b>: The paper introduces DCM, a new dataset that combines camera movement, dance, and music, and proposes DanceCamera3D, a transformer-based model for synthesizing dance camera movements.</p><hr><h3>Grounding Spatial Relations in Text-Only Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.13666v1'>http://arxiv.org/abs/2403.13666v1</a></p>
<p><b>Compressor summary</b>: Text-only Language Models can learn spatial relations with locations, outperforming Vision-and-Language Models on a verbalized VSR dataset.</p><hr><h3>T-Pixel2Mesh: Combining Global and Local Transformer for 3D Mesh  Generation from a Single Image</h3>
<p><a href='http://arxiv.org/abs/2403.13663v1'>http://arxiv.org/abs/2403.13663v1</a></p>
<p><b>Compressor summary</b>: T-Pixel2Mesh is a new method to reconstruct 3D shapes from images using Transformers that improves details and works better with real-world data.</p><hr><h3>ProMamba: Prompt-Mamba for polyp segmentation</h3>
<p><a href='http://arxiv.org/abs/2403.13660v1'>http://arxiv.org/abs/2403.13660v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new segmentation model for polyp detection in medical images that uses Prompt-Mamba, which combines Vision-Mamba and prompt technologies to achieve high accuracy and generalization across different datasets.</p><hr><h3>Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics  Instability Detection</h3>
<p><a href='http://arxiv.org/abs/2403.13658v1'>http://arxiv.org/abs/2403.13658v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel multimodal variational autoencoder (CardioVAE) that integrates chest X-ray and electrocardiogram data to predict cardiac hemodynamic instability, improving performance and interpretability over single-modality methods.</p><hr><h3>Learning User Embeddings from Human Gaze for Personalised Saliency  Prediction</h3>
<p><a href='http://arxiv.org/abs/2403.13653v1'>http://arxiv.org/abs/2403.13653v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to learn user embeddings for personalized saliency prediction using natural images and eye tracking data, improving performance and generalization.</p><hr><h3>ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer</h3>
<p><a href='http://arxiv.org/abs/2403.13652v1'>http://arxiv.org/abs/2403.13652v1</a></p>
<p><b>Compressor summary</b>: ZoDi is a zero-shot domain adaptation method that uses diffusion models to synthesize target-like images and train a model with both source and synthesized images, improving image segmentation performance without target images.</p><hr><h3>Meta-Point Learning and Refining for Category-Agnostic Pose Estimation</h3>
<p><a href='http://arxiv.org/abs/2403.13647v1'>http://arxiv.org/abs/2403.13647v1</a></p>
<p><b>Compressor summary</b>: The proposed method predicts potential keypoints for arbitrary objects using learnable embeddings, and refines them with support keypoints and a slacked regression loss.</p><hr><h3>H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation</h3>
<p><a href='http://arxiv.org/abs/2403.13642v1'>http://arxiv.org/abs/2403.13642v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new model, H-vmunet, for medical image segmentation that improves the 2D-selective-scan (SS2D) module by adding higher-order interactions and a Local-SS2D module to enhance local feature learning.</p><hr><h3>Do Not Worry if You Do Not Have Data: Building Pretrained Language  Models Using Translationese</h3>
<p><a href='http://arxiv.org/abs/2403.13638v1'>http://arxiv.org/abs/2403.13638v1</a></p>
<p><b>Compressor summary</b>: The paper explores using machine translation to create synthetic data for pre-training language models in low-resource languages, showing improved performance on NLU and NLG tasks with efficient filtering and extended pretraining.</p><hr><h3>Enhancing Law Enforcement Training: A Gamified Approach to Detecting  Terrorism Financing</h3>
<p><a href='http://arxiv.org/abs/2403.13625v1'>http://arxiv.org/abs/2403.13625v1</a></p>
<p><b>Compressor summary</b>: The study describes how using learning and training methods with gamification can help law enforcement and other professionals better understand and combat cyber-crime and terrorism financing.</p><hr><h3>Does Differentially Private Synthetic Data Lead to Synthetic  Discoveries?</h3>
<p><a href='http://arxiv.org/abs/2403.13612v1'>http://arxiv.org/abs/2403.13612v1</a></p>
<p><b>Compressor summary</b>: The study evaluates the effectiveness of various methods for generating private synthetic biomedical data, focusing on the Mann-Whitney U test's ability to maintain validity and power when applied to such data.</p><hr><h3>VL-Mamba: Exploring State Space Models for Multimodal Learning</h3>
<p><a href='http://arxiv.org/abs/2403.13600v1'>http://arxiv.org/abs/2403.13600v1</a></p>
<p><b>Compressor summary</b>: The paper introduces VL-Mamba, a more efficient multimodal language model based on state space models that can handle long sequences with fast inference and linear scaling.</p><hr><h3>Llama meets EU: Investigating the European Political Spectrum through  the Lens of LLMs</h3>
<p><a href='http://arxiv.org/abs/2403.13592v1'>http://arxiv.org/abs/2403.13592v1</a></p>
<p><b>Compressor summary</b>: The authors study the political leanings and knowledge of Llama Chat, a large language model, when fine-tuned on EU politics and suggest it could be used for research purposes.</p><hr><h3>Teacher-Student Training for Debiasing: General Permutation Debiasing  for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.13590v1'>http://arxiv.org/abs/2403.13590v1</a></p>
<p><b>Compressor summary</b>: The paper proposes efficient methods to transfer knowledge from debiased large language models to smaller, more reliable ones, improving performance while reducing parameters and computational cost.</p><hr><h3>ReGround: Improving Textual and Spatial Grounding at No Cost</h3>
<p><a href='http://arxiv.org/abs/2403.13589v1'>http://arxiv.org/abs/2403.13589v1</a></p>
<p><b>Compressor summary</b>: The image diffusion model that integrates gated self-attention into the U-Net shows a bias towards spatial cues over text cues, but this can be improved by changing the network architecture without fine-tuning.</p><hr><h3>Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for  Counselor Reflection Generation</h3>
<p><a href='http://arxiv.org/abs/2403.13578v1'>http://arxiv.org/abs/2403.13578v1</a></p>
<p><b>Compressor summary</b>: The paper proposes two new bandit methods to improve natural language generation by jointly optimizing multiple text qualities in counselor reflection generation.</p><hr><h3>Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer</h3>
<p><a href='http://arxiv.org/abs/2403.13570v1'>http://arxiv.org/abs/2403.13570v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new learning approach for creating realistic 4D head avatars using pseudo multi-view videos and a vision transformer backbone, outperforming previous methods in various aspects.</p><hr><h3>eRST: A Signaled Graph Theory of Discourse Relations and Organization</h3>
<p><a href='http://arxiv.org/abs/2403.13560v1'>http://arxiv.org/abs/2403.13560v1</a></p>
<p><b>Compressor summary</b>: The article introduces eRST, a new discourse analysis framework that improves on RST by including more relation types, implicit and explicit signals, and provides tools and a large corpus to support it.</p><hr><h3>Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban  Environments</h3>
<p><a href='http://arxiv.org/abs/2403.13556v1'>http://arxiv.org/abs/2403.13556v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve LiDAR-based 3D object detection in urban environments by using open-vocabulary learning and multi-sensor data with pre-trained vision-language models, achieving better novel object recall and reducing bias towards camera-proximal objects.</p><hr><h3>Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute  Editing</h3>
<p><a href='http://arxiv.org/abs/2403.13551v1'>http://arxiv.org/abs/2403.13551v1</a></p>
<p><b>Compressor summary</b>: Ground-A-Score is a model-agnostic image editing method that incorporates grounding during score distillation to accurately reflect complex text prompts and preserve object integrity in edited images.</p><hr><h3>Diversity-aware Channel Pruning for StyleGAN Compression</h3>
<p><a href='http://arxiv.org/abs/2403.13548v1'>http://arxiv.org/abs/2403.13548v1</a></p>
<p><b>Compressor summary</b>: Our method prunes channels based on their sensitivities to latent vector perturbations, improving sample diversity and FID scores in compressed StyleGAN models without extra training cost.</p><hr><h3>Integrating Large Language Models for Severity Classification in Traffic  Incident Management: A Machine Learning Approach</h3>
<p><a href='http://arxiv.org/abs/2403.13547v1'>http://arxiv.org/abs/2403.13547v1</a></p>
<p><b>Compressor summary</b>: The study shows that using features from large language models can improve or match the accuracy of predicting traffic incident severity using conventional machine learning algorithms.</p><hr><h3>Next day fire prediction via semantic segmentation</h3>
<p><a href='http://arxiv.org/abs/2403.13545v1'>http://arxiv.org/abs/2403.13545v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a deep learning method for predicting fire risk in an area based on images representing daily snapshots and various features.</p><hr><h3>What explains the success of cross-modal fine-tuning with ORCA?</h3>
<p><a href='http://arxiv.org/abs/2403.13537v1'>http://arxiv.org/abs/2403.13537v1</a></p>
<p><b>Compressor summary</b>: ORCA is a cross-modal fine-tuning technique that performs well on 1D tasks but not 2D tasks, and model fine-tuning is more important than embedder training for its success.</p><hr><h3>IDAdapter: Learning Mixed Features for Tuning-Free Personalization of  Text-to-Image Models</h3>
<p><a href='http://arxiv.org/abs/2403.13535v1'>http://arxiv.org/abs/2403.13535v1</a></p>
<p><b>Compressor summary</b>: IDAdapter is a new method that creates personalized and diverse avatars from a single face image, using textual and visual inputs and preserving identity details.</p><hr><h3>Compress3D: a Compressed Latent Space for 3D Generation from a Single  Image</h3>
<p><a href='http://arxiv.org/abs/2403.13524v1'>http://arxiv.org/abs/2403.13524v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a triplane autoencoder with a 3D-aware cross-attention mechanism and a diffusion model for efficient compression and high-speed generation of 3D models from images, achieving superior performance compared to existing methods.</p><hr><h3>Have You Poisoned My Data? Defending Neural Networks against Data  Poisoning</h3>
<p><a href='http://arxiv.org/abs/2403.13523v1'>http://arxiv.org/abs/2403.13523v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Large amounts of training data lead to potential threats like poisoning attacks
- Paper proposes a new way to detect and filter poisoned datapoints in transfer learning setting
- New characteristic vector representation captures intrinsic properties of data distribution
- Experiments show that the proposal outperforms existing defenses

Summary: The paper presents a novel approach using characteristic vectors to defend against clean-label poisoning attacks on neural networks in transfer learning settings and shows its effectiveness in experiments.</p><hr><h3>REAL: Representation Enhanced Analytic Learning for Exemplar-free  Class-incremental Learning</h3>
<p><a href='http://arxiv.org/abs/2403.13522v1'>http://arxiv.org/abs/2403.13522v1</a></p>
<p><b>Compressor summary</b>: REAL is a novel method for exemplar-free class-incremental learning that enhances representation by combining supervised and self-supervised learning, distillation, and analytic learning.</p><hr><h3>Motion Generation from Fine-grained Textual Descriptions</h3>
<p><a href='http://arxiv.org/abs/2403.13518v1'>http://arxiv.org/abs/2403.13518v1</a></p>
<p><b>Compressor summary</b>: Text2motion aims to create motion sequences from fine-grained textual descriptions, using FineHumanML3D dataset and FineMotionDiffuse model trained with GPT-3.5-turbo.</p><hr><h3>How Gender Interacts with Political Values: A Case Study on Czech BERT  Models</h3>
<p><a href='http://arxiv.org/abs/2403.13514v1'>http://arxiv.org/abs/2403.13514v1</a></p>
<p><b>Compressor summary</b>: The study examines political biases in Czech neural language models and finds no systematic alignment with values, but rather superficial imitation of training data patterns.</p><hr><h3>What if...?: Counterfactual Inception to Mitigate Hallucination Effects  in Large Multimodal Models</h3>
<p><a href='http://arxiv.org/abs/2403.13513v1'>http://arxiv.org/abs/2403.13513v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Counterfactual Inception, a method to reduce hallucination in large multimodal models by injecting counterfactual thoughts using misaligned keywords, and Dual-modality Verification Process for selecting optimal keywords.</p><hr><h3>Scale Decoupled Distillation</h3>
<p><a href='http://arxiv.org/abs/2403.13512v1'>http://arxiv.org/abs/2403.13512v1</a></p>
<p><b>Compressor summary</b>: SDD improves logit knowledge distillation by separating global logits into local ones and transferring unambiguous, fine-grained knowledge to the student.</p><hr><h3>FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based  LLMs</h3>
<p><a href='http://arxiv.org/abs/2403.13507v1'>http://arxiv.org/abs/2403.13507v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new adversarial attack on video-based language models that can trick them into generating wrong or nonsensical answers by adding subtle perturbations to videos.</p><hr><h3>VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis</h3>
<p><a href='http://arxiv.org/abs/2403.13501v1'>http://arxiv.org/abs/2403.13501v1</a></p>
<p><b>Compressor summary</b>: The paper introduces VSTAR, a method that improves the generation of longer, more dynamic videos from text using generative temporal nursing with video synopsis prompting and temporal attention regularization.</p><hr><h3>Improved Baselines for Data-efficient Perceptual Augmentation of LLMs</h3>
<p><a href='http://arxiv.org/abs/2403.13499v1'>http://arxiv.org/abs/2403.13499v1</a></p>
<p><b>Compressor summary</b>: The text describes how large language models can be used for various computer vision tasks by connecting them to perceptual backbones, and presents an experimental evaluation of different interfacing mechanisms that improves performance and reduces training time.</p><hr><h3>An Entropy-based Text Watermarking Detection Method</h3>
<p><a href='http://arxiv.org/abs/2403.13485v1'>http://arxiv.org/abs/2403.13485v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an Entropy-based Watermark Detection (EWD) for large language models that considers token entropy during detection and improves performance in low-entropy scenarios.</p><hr><h3>A Unified Optimal Transport Framework for Cross-Modal Retrieval with  Noisy Labels</h3>
<p><a href='http://arxiv.org/abs/2403.13480v1'>http://arxiv.org/abs/2403.13480v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new framework for cross-modal retrieval that uses optimal transport to align semantics, correct noisy labels, and narrow the heterogeneous gap between modalities.</p><hr><h3>Deepfake Detection without Deepfakes: Generalization via Synthetic  Frequency Patterns Injection</h3>
<p><a href='http://arxiv.org/abs/2403.13479v1'>http://arxiv.org/abs/2403.13479v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method for training deepfake detectors that can recognize various techniques by injecting crafted frequency patterns into pristine images, improving their generalization capabilities.</p><hr><h3>Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion</h3>
<p><a href='http://arxiv.org/abs/2403.13470v1'>http://arxiv.org/abs/2403.13470v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes using diffusion models to complete 3D LiDAR scenes from a single scan
- The method operates directly on the points, not on range images extracted from LiDAR
- A regularization loss is added to stabilize the noise prediction during denoising
- The results show more detailed and complete scenes than existing methods

Summary:
The paper presents a diffusion model approach that completes 3D LiDAR scenes directly on points, with a regularization loss to reduce noise, achieving better results than previous scene completion methods.</p><hr><h3>Progressive trajectory matching for medical dataset distillation</h3>
<p><a href='http://arxiv.org/abs/2403.13469v1'>http://arxiv.org/abs/2403.13469v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method to create a synthetic medical image dataset from the original one while preserving useful information, improving training stability, diversity, and performance using progressive trajectory matching and dynamic overlap mitigation.</p><hr><h3>An AI-Assisted Skincare Routine Recommendation System in XR</h3>
<p><a href='http://arxiv.org/abs/2403.13466v1'>http://arxiv.org/abs/2403.13466v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper presents an AI-assisted skin care recommendation system integrated into an XR platform
- The system uses a CNN model to analyse skin type and recommend personalised products based on facial image and questionnaire data
- The system achieves high accuracy in classifying skin issues and can provide immersive and engaging experiences to users

Summary:
The paper proposes an AI-powered XR platform that analyses users' skin type and recommends personalised products using a facial image and a questionnaire, providing immersive and engaging skincare experiences.</p><hr><h3>HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal  Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.13447v1'>http://arxiv.org/abs/2403.13447v1</a></p>
<p><b>Compressor summary</b>: HyperLLaVA improves multimodal task performance by dynamically adjusting visual and language experts using HyperNetworks, unlike static tuning in LLaVA.</p><hr><h3>MedCycle: Unpaired Medical Report Generation via Cycle-Consistency</h3>
<p><a href='http://arxiv.org/abs/2403.13444v1'>http://arxiv.org/abs/2403.13444v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for generating medical reports from X-ray images without paired data, using cycle-consistent mapping functions and report auto-encoding, leading to improved results in chest X-ray report generation.</p><hr><h3>Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking</h3>
<p><a href='http://arxiv.org/abs/2403.13443v1'>http://arxiv.org/abs/2403.13443v1</a></p>
<p><b>Compressor summary</b>: Fast-Poly is a fast and effective 3D multi-object tracking method that addresses object rotational anisotropy, enhances local computation densification, and leverages parallelization for improved accuracy and speed on large-scale datasets.</p><hr><h3>Robustness Verifcation in Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2403.13441v1'>http://arxiv.org/abs/2403.13441v1</a></p>
<p><b>Compressor summary</b>: The paper studies formal verification problems for neural networks using symbolic specifications and provides a theoretical framework to analyze their complexities in a semi-linear setting.</p><hr><h3>Advancing 6D Pose Estimation in Augmented Reality -- Overcoming  Projection Ambiguity with Uncontrolled Imagery</h3>
<p><a href='http://arxiv.org/abs/2403.13434v1'>http://arxiv.org/abs/2403.13434v1</a></p>
<p><b>Compressor summary</b>: The study presents a new method for accurate 6D pose estimation in Augmented Reality from uncontrolled RGB images, improving 3D object overlaying and applications in manufacturing and robotics.</p><hr><h3>Agent Group Chat: An Interactive Group Chat Simulacra For Better  Eliciting Collective Emergent Behavior</h3>
<p><a href='http://arxiv.org/abs/2403.13433v1'>http://arxiv.org/abs/2403.13433v1</a></p>
<p><b>Compressor summary</b>: The Agent Group Chat simulation explores how language influences human collective behavior by creating diverse narrative scenarios and measuring the disorder of agents' interactions.</p><hr><h3>MTP: Advancing Remote Sensing Foundation Model via Multi-Task  Pretraining</h3>
<p><a href='http://arxiv.org/abs/2403.13430v1'>http://arxiv.org/abs/2403.13430v1</a></p>
<p><b>Compressor summary</b>: This study proposes Multi-Task Pretraining for foundation models in Remote Sensing, which improves downstream tasks by addressing task discrepancy and achieving competitive performance with fewer parameters.</p><hr><h3>Diversified and Personalized Multi-rater Medical Image Segmentation</h3>
<p><a href='http://arxiv.org/abs/2403.13417v1'>http://arxiv.org/abs/2403.13417v1</a></p>
<p><b>Compressor summary</b>: The D-Persona framework aims to achieve both diversified and personalized results in multi-rater medical image segmentation by training a Probabilistic U-Net model and using attention-based projection heads.</p><hr><h3>Cell Tracking in C. elegans with Cell Position Heatmap-Based Alignment  and Pairwise Detection</h3>
<p><a href='http://arxiv.org/abs/2403.13412v1'>http://arxiv.org/abs/2403.13412v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a cell tracking method for C. elegans that handles large migrations due to head movement, inconsistent detections, and low-contrast images by using non-rigid alignment and pairwise detection.</p><hr><h3>S2DM: Sector-Shaped Diffusion Models for Video Generation</h3>
<p><a href='http://arxiv.org/abs/2403.13408v1'>http://arxiv.org/abs/2403.13408v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new video generation method called Sector-Shaped Diffusion Model (S2DM) that maintains consistency and continuity across video frames by using sector-shaped diffusion regions and optical flow as temporal conditions.</p><hr><h3>DOR3D-Net: Dense Ordinal Regression Network for 3D Hand Pose Estimation</h3>
<p><a href='http://arxiv.org/abs/2403.13405v1'>http://arxiv.org/abs/2403.13405v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel network for 3D hand pose estimation using ordinal regression, which improves accuracy by reducing noise and outliers in large-scale regression offset values.</p><hr><h3>Unifying Local and Global Multimodal Features for Place Recognition in  Aliased and Low-Texture Environments</h3>
<p><a href='http://arxiv.org/abs/2403.13395v1'>http://arxiv.org/abs/2403.13395v1</a></p>
<p><b>Compressor summary</b>: The paper proposes UMF, a multi-modal model that improves SLAM performance in challenging environments by cross-attention between vision and LiDAR features and re-ranking based on local feature matching.</p><hr><h3>Robust image segmentation model based on binary level set</h3>
<p><a href='http://arxiv.org/abs/2403.13392v1'>http://arxiv.org/abs/2403.13392v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a robust image segmentation model that uses intensity inhomogeneity and binary level set to handle noise and improve performance.</p><hr><h3>IIDM: Image-to-Image Diffusion Model for Semantic Image Synthesis</h3>
<p><a href='http://arxiv.org/abs/2403.13378v1'>http://arxiv.org/abs/2403.13378v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel diffusion model for semantic image synthesis that uses segmentation masks and style reference images, and improves the generation quality with refinement, color-transfer, and model ensembles techniques.</p><hr><h3>Correlation Clustering of Organoid Images</h3>
<p><a href='http://arxiv.org/abs/2403.13376v1'>http://arxiv.org/abs/2403.13376v1</a></p>
<p><b>Compressor summary</b>: The paper presents methods for finding similar patterns in microscopy images of heterogeneous organoids using models, algorithms, and clustering techniques.</p><hr><h3>Few-shot Oriented Object Detection with Memorable Contrastive Learning  in Remote Sensing Images</h3>
<p><a href='http://arxiv.org/abs/2403.13375v1'>http://arxiv.org/abs/2403.13375v1</a></p>
<p><b>Compressor summary</b>: FOMC is a novel FSOD method for remote sensing images that uses oriented bounding boxes and supervised contrastive learning to improve detection performance for arbitrary-oriented objects with limited annotations.</p><hr><h3>LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.13372v1'>http://arxiv.org/abs/2403.13372v1</a></p>
<p><b>Compressor summary</b>: LlamaFactory is a framework that helps users fine-tune large language models efficiently and effectively using a web UI without coding.</p><hr><h3>Counting Network for Learning from Majority Label</h3>
<p><a href='http://arxiv.org/abs/2403.13370v1'>http://arxiv.org/abs/2403.13370v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new problem in multi-class learning, called Learning from the Majority Label, and proposes a counting network to solve it effectively.</p><hr><h3>Clinical information extraction for Low-resource languages with Few-shot  learning using Pre-trained language models and Prompting</h3>
<p><a href='http://arxiv.org/abs/2403.13369v1'>http://arxiv.org/abs/2403.13369v1</a></p>
<p><b>Compressor summary</b>: The authors evaluate domain-adapted and prompted lightweight models for medical information extraction from German doctor's letters, achieving high accuracy with minimal training data and ensuring interpretability of predictions using Shapley values.</p><hr><h3>Computational Models to Study Language Processing in the Human Brain: A  Survey</h3>
<p><a href='http://arxiv.org/abs/2403.13368v1'>http://arxiv.org/abs/2403.13368v1</a></p>
<p><b>Compressor summary</b>: The paper discusses using computational language models in brain research, evaluates their performance, and emphasizes the importance of diverse data and strict experiments.</p><hr><h3>AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in  Text-to-Image Generation</h3>
<p><a href='http://arxiv.org/abs/2403.13352v1'>http://arxiv.org/abs/2403.13352v1</a></p>
<p><b>Compressor summary</b>: AGFSync improves image generation by using VLM to assess and provide feedback to T2I diffusion models in a closed AI-driven loop, leading to better quality images and performance on benchmarks.</p><hr><h3>OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and  Pruning</h3>
<p><a href='http://arxiv.org/abs/2403.13351v1'>http://arxiv.org/abs/2403.13351v1</a></p>
<p><b>Compressor summary</b>: Orthogonal Capsule Network (OrthCaps) reduces redundancy and improves routing performance in CapsNet using pruning, sparse attention routing, and orthogonal weight matrices, achieving competitive results with significantly fewer parameters.</p><hr><h3>Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified  Anomaly Detection</h3>
<p><a href='http://arxiv.org/abs/2403.13349v1'>http://arxiv.org/abs/2403.13349v1</a></p>
<p><b>Compressor summary</b>: HGAD is a novel anomaly detection method that uses hierarchical Gaussian mixture modeling to overcome the limitations of existing normalizing flow-based approaches, achieving better performance on real-world datasets.</p><hr><h3>vid-TLDR: Training Free Token merging for Light-weight Video Transformer</h3>
<p><a href='http://arxiv.org/abs/2403.13347v1'>http://arxiv.org/abs/2403.13347v1</a></p>
<p><b>Compressor summary</b>: The authors propose vid-TLDR, a lightweight video Transformer that merges background tokens and focuses on salient regions using attention maps and token dropping, to improve efficiency without sacrificing performance.</p><hr><h3>TiBiX: Leveraging Temporal Information for Bidirectional X-ray and  Report Generation</h3>
<p><a href='http://arxiv.org/abs/2403.13343v1'>http://arxiv.org/abs/2403.13343v1</a></p>
<p><b>Compressor summary</b>: TiBiX is a method that uses temporal information to generate reports and images for chest X-rays, improving the quality of medical information.</p><hr><h3>FissionFusion: Fast Geometric Generation and Hierarchical Souping for  Medical Image Analysis</h3>
<p><a href='http://arxiv.org/abs/2403.13341v1'>http://arxiv.org/abs/2403.13341v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a hierarchical merging approach to improve medical imaging performance and robustness by aggregating models based on hyperparameter configurations, leading to better results than model soups on in-domain and out-of-distribution tasks.</p><hr><h3>Adaptive Critical Subgraph Mining for Cognitive Impairment Conversion  Prediction with T1-MRI-based Brain Network</h3>
<p><a href='http://arxiv.org/abs/2403.13338v1'>http://arxiv.org/abs/2403.13338v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Brain-SubGNN, a graph representation network that mines and enhances critical subgraphs based on T1-MRI to improve understanding and diagnosis of early-stage dementia.</p><hr><h3>Learning Novel View Synthesis from Heterogeneous Low-light Captures</h3>
<p><a href='http://arxiv.org/abs/2403.13337v1'>http://arxiv.org/abs/2403.13337v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to decompose illumination, reflectance, and noise from input views under low-light conditions, enabling better synthesis of novel views with improved visual quality.</p><hr><h3>Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text  Detection</h3>
<p><a href='http://arxiv.org/abs/2403.13335v1'>http://arxiv.org/abs/2403.13335v1</a></p>
<p><b>Compressor summary</b>: The study shows that combining multiple transformer-based models with adaptive ensemble algorithms can significantly improve the accuracy of detecting fake text generated by large language models on different types of data.</p><hr><h3>Hyacinth6B: A large language model for Traditional Chinese</h3>
<p><a href='http://arxiv.org/abs/2403.13334v1'>http://arxiv.org/abs/2403.13334v1</a></p>
<p><b>Compressor summary</b>: The study aims to create a lightweight language model, Hyacinth6B, that performs well without high computational costs by using the LoRA method for efficient finetuning.</p><hr><h3>AMP: Autoregressive Motion Prediction Revisited with Next Token  Prediction for Autonomous Driving</h3>
<p><a href='http://arxiv.org/abs/2403.13331v1'>http://arxiv.org/abs/2403.13331v1</a></p>
<p><b>Compressor summary</b>: The paper introduces an autoregressive method for motion prediction in autonomous driving using GPT-style next token prediction, factorized attention modules, and position encoding styles to capture spatial-temporal relations.</p><hr><h3>Efficient scene text image super-resolution with semantic guidance</h3>
<p><a href='http://arxiv.org/abs/2403.13330v1'>http://arxiv.org/abs/2403.13330v1</a></p>
<p><b>Compressor summary</b>: SGENet is an efficient framework for scene text recognition that combines super-resolution and semantic guidance branches to improve accuracy while keeping low computational costs.</p><hr><h3>Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation  for Natural Camera Motion</h3>
<p><a href='http://arxiv.org/abs/2403.13327v1'>http://arxiv.org/abs/2403.13327v1</a></p>
<p><b>Compressor summary</b>: Our method adapts to handheld video camera motion and improves scene reconstruction using detailed image formation modeling and differentiable rendering with velocities from visual-inertial odometry.</p><hr><h3>Out-of-Distribution Detection Using Peer-Class Generated by Large  Language Model</h3>
<p><a href='http://arxiv.org/abs/2403.13324v1'>http://arxiv.org/abs/2403.13324v1</a></p>
<p><b>Compressor summary</b>: ODPC uses language models to generate peer classes for OOD detection, improving reliability and security of machine learning models.</p><hr><h3>DD-RobustBench: An Adversarial Robustness Benchmark for Dataset  Distillation</h3>
<p><a href='http://arxiv.org/abs/2403.13322v1'>http://arxiv.org/abs/2403.13322v1</a></p>
<p><b>Compressor summary</b>: The authors present a comprehensive benchmark for evaluating the adversarial robustness of compressed datasets created by various distillation methods, using different attacks and datasets.</p><hr><h3>HyperFusion: A Hypernetwork Approach to Multimodal Integration of  Tabular and Medical Imaging Data for Predictive Modeling</h3>
<p><a href='http://arxiv.org/abs/2403.13319v1'>http://arxiv.org/abs/2403.13319v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Paper proposes a hypernetwork framework to fuse medical imaging and tabular data for multimodal tasks in healthcare
- Method conditions image processing on EHR values and measurements
- Outperforms single-modality models and existing fusion methods on brain MRI tasks

Summary: The paper introduces a novel hypernetwork approach to merge medical imaging and tabular data, which improves multimodal healthcare applications by enhancing image processing with EHR information.</p><hr><h3>PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models  with Abstract Visual Patterns</h3>
<p><a href='http://arxiv.org/abs/2403.13315v1'>http://arxiv.org/abs/2403.13315v1</a></p>
<p><b>Compressor summary</b>: The paper introduces PuzzleVQA, a dataset to test large multimodal models on abstract patterns, and finds that they struggle with visual perception and inductive reasoning, suggesting limitations in emulating human cognition.</p><hr><h3>Polaris: A Safety-focused LLM Constellation Architecture for Healthcare</h3>
<p><a href='http://arxiv.org/abs/2403.13313v1'>http://arxiv.org/abs/2403.13313v1</a></p>
<p><b>Compressor summary</b>: Polaris is a safety-focused system of large language models that can engage in real-time voice conversations with patients, performing better than previous models and human nurses on medical safety and bedside manner.</p><hr><h3>LeanReasoner: Boosting Complex Logical Reasoning with Lean</h3>
<p><a href='http://arxiv.org/abs/2403.13312v1'>http://arxiv.org/abs/2403.13312v1</a></p>
<p><b>Compressor summary</b>: The authors use Lean, a theorem proving framework, to improve LLMs' logical reasoning skills by formalizing problems into theorems and solving them with Lean's symbolic solver and library of proofs.</p><hr><h3>Multi-Robot Connected Fermat Spiral Coverage</h3>
<p><a href='http://arxiv.org/abs/2403.13311v1'>http://arxiv.org/abs/2403.13311v1</a></p>
<p><b>Compressor summary</b>: The Multi-Robot Connected Fermat Spiral (MCFS) algorithm helps multiple robots navigate around obstacles for efficient area coverage and smooth paths, improving performance in complex environments.</p><hr><h3>LaserHuman: Language-guided Scene-aware Human Motion Generation in Free  Environment</h3>
<p><a href='http://arxiv.org/abs/2403.13307v1'>http://arxiv.org/abs/2403.13307v1</a></p>
<p><b>Compressor summary</b>: LaserHuman is a new dataset for generating realistic human motions from natural language descriptions in various 3D environments, with a novel multi-conditional diffusion model that outperforms existing methods.</p><hr><h3>DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced  Data Generation and Perception</h3>
<p><a href='http://arxiv.org/abs/2403.13304v1'>http://arxiv.org/abs/2403.13304v1</a></p>
<p><b>Compressor summary</b>: DetDiffusion is a novel method that combines generative and perceptive models to create high-quality synthetic images for object detection tasks using segmentation and perception-aware attributes.</p><hr><h3>Rotary Position Embedding for Vision Transformer</h3>
<p><a href='http://arxiv.org/abs/2403.13298v1'>http://arxiv.org/abs/2403.13298v1</a></p>
<p><b>Compressor summary</b>: RoPE enhances Vision Transformer performance on image tasks by improving resolution and precision, while being computationally efficient.</p><hr><h3>Building Optimal Neural Architectures using Interpretable Knowledge</h3>
<p><a href='http://arxiv.org/abs/2403.13293v1'>http://arxiv.org/abs/2403.13293v1</a></p>
<p><b>Compressor summary</b>: AutoBuild is a scheme that learns to assign importance scores to neural architecture modules using latent embeddings, enabling high-quality network construction without costly search.</p><hr><h3>Text-to-3D Shape Generation</h3>
<p><a href='http://arxiv.org/abs/2403.13289v1'>http://arxiv.org/abs/2403.13289v1</a></p>
<p><b>Compressor summary</b>: Text-to-3D shape generation is a rapidly advancing field with many challenges, and this report surveys the existing methods and suggests future research directions.</p><hr><h3>AdaViPro: Region-based Adaptive Visual Prompt for Large-Scale Models  Adapting</h3>
<p><a href='http://arxiv.org/abs/2403.13282v1'>http://arxiv.org/abs/2403.13282v1</a></p>
<p><b>Compressor summary</b>: AdaViPro is a method that learns to add and remove prompts in different regions of an image to fine-tune pre-trained models efficiently.</p><hr><h3>AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient  Fine-Tuning of Large Models</h3>
<p><a href='http://arxiv.org/abs/2403.13269v1'>http://arxiv.org/abs/2403.13269v1</a></p>
<p><b>Compressor summary</b>: AFLoRA is a new fine-tuning method that uses low-rank matrices to adapt pre-trained models with fewer parameters, less computation, and better performance on the GLUE benchmark.</p><hr><h3>Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural  Network</h3>
<p><a href='http://arxiv.org/abs/2403.13268v1'>http://arxiv.org/abs/2403.13268v1</a></p>
<p><b>Compressor summary</b>: Unifews is a novel method that jointly sparsifies edges and weights of graph neural networks, reducing computational cost without sacrificing accuracy.</p><hr><h3>SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large  Vision Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.13263v1'>http://arxiv.org/abs/2403.13263v1</a></p>
<p><b>Compressor summary</b>: The paper introduces SC-Tune, a novel fine-tuning method for Large Vision Language Models that improves their self-consistency and object-level comprehension abilities.</p><hr><h3>Self-Supervised Class-Agnostic Motion Prediction with Spatial and  Temporal Consistency Regularizations</h3>
<p><a href='http://arxiv.org/abs/2403.13261v1'>http://arxiv.org/abs/2403.13261v1</a></p>
<p><b>Compressor summary</b>: The text describes a new method for autonomous driving systems to predict motion using unlabeled LiDAR point clouds and coarse pseudo motion labels, outperforming existing self-supervised methods.</p><hr><h3>SAMCT: Segment Any CT Allowing Labor-Free Task-Indicator Prompts</h3>
<p><a href='http://arxiv.org/abs/2403.13258v1'>http://arxiv.org/abs/2403.13258v1</a></p>
<p><b>Compressor summary</b>: The paper introduces SAMCT, a modified segment anything model for medical imaging that improves performance by adding a U-shaped CNN image encoder, cross-branch interaction, and task-indicator prompt encoder to address the lack of medical knowledge and insufficient feature extraction.</p><hr><h3>Arcee's MergeKit: A Toolkit for Merging Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.13257v1'>http://arxiv.org/abs/2403.13257v1</a></p>
<p><b>Compressor summary</b>: MergeKit is an open-source library that helps merge pre-trained language models to improve performance and versatility without additional training.</p><hr><h3>Document Author Classification Using Parsed Language Structure</h3>
<p><a href='http://arxiv.org/abs/2403.13253v1'>http://arxiv.org/abs/2403.13253v1</a></p>
<p><b>Compressor summary</b>: The paper explores using grammatical structure information extracted by a statistical natural language parser to detect authorship of texts, and tests the method on The Federalist Papers and Sanditon.</p><hr><h3>Facilitating Pornographic Text Detection for Open-Domain Dialogue  Systems via Knowledge Distillation of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.13250v1'>http://arxiv.org/abs/2403.13250v1</a></p>
<p><b>Compressor summary</b>: CensorChat is a dialogue monitoring dataset that uses knowledge distillation of large language models to annotate and develop text classifiers for detecting pornographic content in human-machine interaction dialogues.</p><hr><h3>A Unified and General Framework for Continual Learning</h3>
<p><a href='http://arxiv.org/abs/2403.13249v1'>http://arxiv.org/abs/2403.13249v1</a></p>
<p><b>Compressor summary</b>: This paper introduces a unified framework for Continual Learning methods, reveals their common mathematical structures, and proposes refresh learning, an innovative technique inspired by neuroscience to enhance CL performance.</p><hr><h3>Mora: Enabling Generalist Video Generation via A Multi-Agent Framework</h3>
<p><a href='http://arxiv.org/abs/2403.13248v1'>http://arxiv.org/abs/2403.13248v1</a></p>
<p><b>Compressor summary</b>: Mora is a new multi-agent framework that mimics Sora's generalist video generation capabilities using several advanced visual AI agents for various tasks.</p><hr><h3>Divide-Conquer Transformer Learning for Predicting Electric Vehicle  Charging Events Using Smart Meter Data</h3>
<p><a href='http://arxiv.org/abs/2403.13246v1'>http://arxiv.org/abs/2403.13246v1</a></p>
<p><b>Compressor summary</b>: The authors develop a home EV charging prediction method using historical smart meter data, which can help with load scheduling and energy management, and achieve high accuracy.</p><hr><h3>Instruction Multi-Constraint Molecular Generation Using a  Teacher-Student Large Language Model</h3>
<p><a href='http://arxiv.org/abs/2403.13244v1'>http://arxiv.org/abs/2403.13244v1</a></p>
<p><b>Compressor summary</b>: The text introduces TSMMG, a large language model that generates molecules based on natural language descriptions and performs well across various tasks and styles.</p><hr><h3>Tackling Noisy Labels with Network Parameter Additive Decomposition</h3>
<p><a href='http://arxiv.org/abs/2403.13241v1'>http://arxiv.org/abs/2403.13241v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to separate clean and noisy data memorization in deep networks using additive parameter decomposition, improving generalization and reducing overfitting.</p><hr><h3>SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual  Summarization</h3>
<p><a href='http://arxiv.org/abs/2403.13240v1'>http://arxiv.org/abs/2403.13240v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a summarize-and-translate pipeline for cross-lingual summarization, which leverages existing resources and shows competitive performance with few-shot fine-tuning.</p><hr><h3>Beyond Skeletons: Integrative Latent Mapping for Coherent 4D Sequence  Generation</h3>
<p><a href='http://arxiv.org/abs/2403.13238v1'>http://arxiv.org/abs/2403.13238v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new framework to generate coherent 4D sequences of 3D shapes with dynamic evolution of shape and color using diffusion models and latent mapping.</p><hr><h3>Technical Report: Competition Solution For BetterMixture</h3>
<p><a href='http://arxiv.org/abs/2403.13233v1'>http://arxiv.org/abs/2403.13233v1</a></p>
<p><b>Compressor summary</b>: The paper presents a third-place solution for the BetterMixture challenge, which uses Ke-Data-Juicer to optimize and filter data for large language models.</p><hr><h3>Diffusion Model for Data-Driven Black-Box Optimization</h3>
<p><a href='http://arxiv.org/abs/2403.13219v1'>http://arxiv.org/abs/2403.13219v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to optimize complex designs using diffusion models, which generate near-optimal solutions based on noisy rewards or human preferences, preserving latent structures and achieving sub-optimality error bounds.</p><hr><h3>Self-Attention Based Semantic Decomposition in Vector Symbolic  Architectures</h3>
<p><a href='http://arxiv.org/abs/2403.13218v1'>http://arxiv.org/abs/2403.13218v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new variant of the resonator network that uses self-attention based update rules for faster and better associative memory tasks like pattern recognition, scene decomposition, and object reasoning.</p><hr><h3>From Representational Harms to Quality-of-Service Harms: A Case Study on  Llama 2 Safety Safeguards</h3>
<p><a href='http://arxiv.org/abs/2403.13213v1'>http://arxiv.org/abs/2403.13213v1</a></p>
<p><b>Compressor summary</b>: The paper investigates the effectiveness of safety measures in large language models and shows that safety responses can still encode harmful assumptions, leading to trade-offs between helpfulness and safety.</p>