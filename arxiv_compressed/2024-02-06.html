
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-02-06</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-02-06 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Test-Time Adaptation for Depth Completion</h3>
<p><a href='http://arxiv.org/abs/2402.03312v1'>http://arxiv.org/abs/2402.03312v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for adapting depth completion models in real-time without needing source data, which improves performance by 21.1% on average.</p><hr><h3>HASSOD: Hierarchical Adaptive Self-Supervised Object Detection</h3>
<p><a href='http://arxiv.org/abs/2402.03311v1'>http://arxiv.org/abs/2402.03311v1</a></p>
<p><b>Compressor summary</b>: HASSOD is a novel self-supervised object detection method that adapts to the number of objects per image and understands their compositions, achieving better performance and interpretability than existing methods.</p><hr><h3>V-IRL: Grounding Virtual Intelligence in Real Life</h3>
<p><a href='http://arxiv.org/abs/2402.03310v1'>http://arxiv.org/abs/2402.03310v1</a></p>
<p><b>Compressor summary</b>: V-IRL is a platform that lets AI agents interact with a virtual but realistic version of the physical world to improve their abilities in perception, decision-making, and interaction.</p><hr><h3>AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion</h3>
<p><a href='http://arxiv.org/abs/2402.03309v1'>http://arxiv.org/abs/2402.03309v1</a></p>
<p><b>Compressor summary</b>: The text describes a new method (AONeuS) that uses acoustic-optical neural fusion to create high-resolution 3D underwater scenes from limited data, improving on existing RGB and sonar methods.</p><hr><h3>4D Gaussian Splatting: Towards Efficient Novel View Synthesis for  Dynamic Scenes</h3>
<p><a href='http://arxiv.org/abs/2402.03307v1'>http://arxiv.org/abs/2402.03307v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for synthesizing views in dynamic scenes using anisotropic 4D Gaussians that can capture complex motion dynamics and achieve real-time rendering speeds.</p><hr><h3>Do Diffusion Models Learn Semantically Meaningful and Efficient  Representations?</h3>
<p><a href='http://arxiv.org/abs/2402.03305v1'>http://arxiv.org/abs/2402.03305v1</a></p>
<p><b>Compressor summary</b>: The text explains how diffusion models learn to generate 2D Gaussian bumps by traversing three phases of latent representations and demonstrates that they cannot factorize localization in x and y positions, suggesting the need for better inductive biases.</p><hr><h3>Nevermind: Instruction Override and Moderation in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.03303v1'>http://arxiv.org/abs/2402.03303v1</a></p>
<p><b>Compressor summary</b>: The paper benchmarks different-sized LLMs on instruction following in conflicting situations and finds that larger models perform better, while instruction following conflicts with safety filters or guidelines.</p><hr><h3>Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining</h3>
<p><a href='http://arxiv.org/abs/2402.03302v1'>http://arxiv.org/abs/2402.03302v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new Mamba-based model, Swin-UMamba, for medical image segmentation that leverages ImageNet pretraining to achieve superior performance over existing methods.</p><hr><h3>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open  Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.03300v1'>http://arxiv.org/abs/2402.03300v1</a></p>
<p><b>Compressor summary</b>: DeepSeekMath 7B is a language model pre-trained with math-related web data and optimized for mathematical reasoning using GRPO, achieving high scores on MATH benchmark without external tools.</p><hr><h3>GUARD: Role-playing to Generate Natural-language Jailbreakings to Test  Guideline Adherence of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.03299v1'>http://arxiv.org/abs/2402.03299v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a system (GUARD) to generate jailbreaks for testing LLMs' safety and ethical behavior
- GUARD uses a role-playing method, a knowledge graph, and a guideline-following setting
- GUARD is tested on various LLMs across different modalities and shows effectiveness

Summary:
The paper introduces GUARD, a system that generates jailbreaks for testing the safety and ethics of LLMs using roles, a knowledge graph, and guidelines. It demonstrates its performance on different LLMs and modalities.</p><hr><h3>Ginger: An Efficient Curvature Approximation with Linear Complexity for  General Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.03295v1'>http://arxiv.org/abs/2402.03295v1</a></p>
<p><b>Compressor summary</b>: Ginger is a method to improve second-order optimization in deep learning by efficiently computing the inverse of the generalized Gauss-Newton matrix.</p><hr><h3>Flora: Low-Rank Adapters Are Secretly Gradient Compressors</h3>
<p><a href='http://arxiv.org/abs/2402.03293v1'>http://arxiv.org/abs/2402.03293v1</a></p>
<p><b>Compressor summary</b>: Flora uses random projections to enable high-rank weight updates for neural networks, reducing memory usage without compromising performance.</p><hr><h3>Zero-shot Object-Level OOD Detection with Context-Aware Inpainting</h3>
<p><a href='http://arxiv.org/abs/2402.03292v1'>http://arxiv.org/abs/2402.03292v1</a></p>
<p><b>Compressor summary</b>: RONIN is a method for detecting out-of-distribution objects by using a diffusion model to inpaint the object with the predicted in-distribution label, making it easier to distinguish between in-distribution and out-of-distribution samples.</p><hr><h3>InstanceDiffusion: Instance-level Control for Image Generation</h3>
<p><a href='http://arxiv.org/abs/2402.03290v1'>http://arxiv.org/abs/2402.03290v1</a></p>
<p><b>Compressor summary</b>: InstanceDiffusion is a text-to-image model that allows precise control over individual objects in an image using various location methods and outperforms existing state-of-the-art models.</p><hr><h3>Make Every Move Count: LLM-based High-Quality RTL Code Generation Using  MCTS</h3>
<p><a href='http://arxiv.org/abs/2402.03289v1'>http://arxiv.org/abs/2402.03289v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an automated transformer decoding algorithm that uses Monte Carlo tree-search for generating RTL code with improved PPA efficiency and correctness.</p><hr><h3>A Lennard-Jones Layer for Distribution Normalization</h3>
<p><a href='http://arxiv.org/abs/2402.03287v1'>http://arxiv.org/abs/2402.03287v1</a></p>
<p><b>Compressor summary</b>: The Lennard-Jones layer (LJL) is a method to equalize the density of 2D and 3D point clouds by simulating interactions between points and adjusting their distribution without retraining neural networks.</p><hr><h3>Training-Free Consistent Text-to-Image Generation</h3>
<p><a href='http://arxiv.org/abs/2402.03286v1'>http://arxiv.org/abs/2402.03286v1</a></p>
<p><b>Compressor summary</b>: ConsiStory is a training-free method for consistent subject generation in text-to-image models using shared activations, subject-driven attention, and feature injection.</p><hr><h3>Deal, or no deal (or who knows)? Forecasting Uncertainty in  Conversations using Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.03284v1'>http://arxiv.org/abs/2402.03284v1</a></p>
<p><b>Compressor summary</b>: FortUne Dial is a task that evaluates language models' ability to represent uncertainty in conversations, using two types of uncertainty representations and fine-tuning strategies to improve calibration.</p><hr><h3>A Framework for Partially Observed Reward-States in RLHF</h3>
<p><a href='http://arxiv.org/abs/2402.03282v1'>http://arxiv.org/abs/2402.03282v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new model for reinforcement learning with human feedback (RLHF) that considers partially observed rewards and different types of feedback, and presents efficient algorithms and generalizations based on this model.</p><hr><h3>Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information  Seeking in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.03271v1'>http://arxiv.org/abs/2402.03271v1</a></p>
<p><b>Compressor summary</b>: UoT is a new algorithm that helps language models ask better questions to seek information and solve tasks more effectively in uncertain situations.</p><hr><h3>Multiclass Classification Procedure for Detecting Attacks on MQTT-IoT  Protocol</h3>
<p><a href='http://arxiv.org/abs/2402.03270v1'>http://arxiv.org/abs/2402.03270v1</a></p>
<p><b>Compressor summary</b>: The text discusses how machine learning techniques can improve intrusion detection systems in IoT networks using a dataset with MQTT protocol frames under attack.</p><hr><h3>Understanding the Reasoning Ability of Language Models From the  Perspective of Reasoning Paths Aggregation</h3>
<p><a href='http://arxiv.org/abs/2402.03268v1'>http://arxiv.org/abs/2402.03268v1</a></p>
<p><b>Compressor summary</b>: Pre-trained language models reason by aggregating indirect reasoning paths from knowledge graphs and math word problems, which can be improved by augmenting unlabeled random walk reasoning paths.</p><hr><h3>MobilityGPT: Enhanced Human Mobility Modeling with a GPT model</h3>
<p><a href='http://arxiv.org/abs/2402.03264v1'>http://arxiv.org/abs/2402.03264v1</a></p>
<p><b>Compressor summary</b>: MobilityGPT is a geospatially-aware generative model that uses GPT to create realistic human mobility trajectories with controllable generation, semantic sequence similarity, and road connectivity constraints.</p><hr><h3>Learning Best-in-Class Policies for the Predict-then-Optimize Framework</h3>
<p><a href='http://arxiv.org/abs/2402.03256v1'>http://arxiv.org/abs/2402.03256v1</a></p>
<p><b>Compressor summary</b>: PG losses are new decision-aware surrogate losses that approximate the downstream loss and perform well in misspecified settings with non-central symmetric noise.</p><hr><h3>Fair Active Ranking from Pairwise Preferences</h3>
<p><a href='http://arxiv.org/abs/2402.03252v1'>http://arxiv.org/abs/2402.03252v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a fair ranking method that minimizes the error in groups of items using pairwise comparisons and an oracle, adapting to different fairness preferences by adjusting parameters.</p><hr><h3>CLIP Can Understand Depth</h3>
<p><a href='http://arxiv.org/abs/2402.03251v1'>http://arxiv.org/abs/2402.03251v1</a></p>
<p><b>Compressor summary</b>: The paper adapts CLIP for monocular depth estimation by jointly training a compact decoder and a tiny embedding matrix named mirror, improving its performance without fine-tuning and refining CLIP's prior knowledge.</p><hr><h3>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</h3>
<p><a href='http://arxiv.org/abs/2402.03246v1'>http://arxiv.org/abs/2402.03246v1</a></p>
<p><b>Compressor summary</b>: SGS-SLAM is a novel system that combines semantic understanding with 3D Gaussian representations for accurate scene interpretation and high-quality visualizations in real time.</p><hr><h3>Skill Set Optimization: Reinforcing Language Model Behavior via  Transferable Skills</h3>
<p><a href='http://arxiv.org/abs/2402.03244v1'>http://arxiv.org/abs/2402.03244v1</a></p>
<p><b>Compressor summary</b>: Skill Set Optimization (SSO) is a method to improve LLM actor performance by constructing and refining sets of transferable skills using subgoals and instructions.</p><hr><h3>PINN-BO: A Black-box Optimization Algorithm using Physics-Informed  Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.03243v1'>http://arxiv.org/abs/2402.03243v1</a></p>
<p><b>Compressor summary</b>: PINN-BO uses Physics-Informed Neural Networks with Partial Differential Equations to improve black-box optimization efficiency and sample quality.</p><hr><h3>JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance  Skill Matching</h3>
<p><a href='http://arxiv.org/abs/2402.03242v1'>http://arxiv.org/abs/2402.03242v1</a></p>
<p><b>Compressor summary</b>: JobSkape is a framework for generating synthetic job postings to improve skill-to-taxonomy matching using large language models.</p><hr><h3>FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action  Recognition</h3>
<p><a href='http://arxiv.org/abs/2402.03241v1'>http://arxiv.org/abs/2402.03241v1</a></p>
<p><b>Compressor summary</b>: FROSTER is a framework that uses residual feature distillation to adapt CLIP for open-vocabulary action recognition while preserving its generalization capability.</p><hr><h3>ActiveAnno3D -- An Active Learning Framework for Multi-Modal 3D Object  Detection</h3>
<p><a href='http://arxiv.org/abs/2402.03235v1'>http://arxiv.org/abs/2402.03235v1</a></p>
<p><b>Compressor summary</b>: The paper proposes ActiveAnno3D, an active learning framework for 3D object detection that selects informative data samples for labeling and minimizes labeling costs.</p><hr><h3>Smart Flow Matching: On The Theory of Flow Matching Algorithms with  Applications</h3>
<p><a href='http://arxiv.org/abs/2402.03232v1'>http://arxiv.org/abs/2402.03232v1</a></p>
<p><b>Compressor summary</b>: The paper derives a new loss and algorithm for training vector field models that improves over standard Conditional Flow Matching with smaller variance and better learning results.</p><hr><h3>CT-based Anatomical Segmentation for Thoracic Surgical Planning: A  Benchmark Study for 3D U-shaped Deep Learning Models</h3>
<p><a href='http://arxiv.org/abs/2402.03230v1'>http://arxiv.org/abs/2402.03230v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The text introduces a benchmark study for different 3D U-shaped models in medical image segmentation for thoracic surgery planning
- It compares the impact of attention mechanisms, resolution stages, and network configurations on accuracy and complexity
- STUNet performs best among the models tested

Summary:
The text summarizes a benchmark study that evaluates various 3D U-shaped deep learning models for segmenting thoracic anatomy from CT scans and compares their performance and features.</p><hr><h3>IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of  brain MR images</h3>
<p><a href='http://arxiv.org/abs/2402.03227v1'>http://arxiv.org/abs/2402.03227v1</a></p>
<p><b>Compressor summary</b>: IGUANe is a 3D deep learning model that harmonizes brain MR images from multiple sites by integrating an arbitrary number of domains and preserving individual information related to age and Alzheimer's disease.</p><hr><h3>FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion</h3>
<p><a href='http://arxiv.org/abs/2402.03226v1'>http://arxiv.org/abs/2402.03226v1</a></p>
<p><b>Compressor summary</b>: FuseMoE is a novel framework that combines different types of data and handles missing information to improve machine learning models' performance in various tasks, especially in medical settings.</p><hr><h3>English Prompts are Better for NLI-based Zero-Shot Emotion  Classification than Target-Language Prompts</h3>
<p><a href='http://arxiv.org/abs/2402.03223v1'>http://arxiv.org/abs/2402.03223v1</a></p>
<p><b>Compressor summary</b>: The paper explores the best language for prompting emotion labels on non-English texts using multilingual large language models, and finds that English prompts are consistently better than the target language.</p><hr><h3>"Define Your Terms" : Enhancing Efficient Offensive Speech  Classification with Definition</h3>
<p><a href='http://arxiv.org/abs/2402.03221v1'>http://arxiv.org/abs/2402.03221v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a joint embedding model for detecting different types of offensive speech on social media using limited data and showing promising results.</p><hr><h3>BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity  Text Embeddings Through Self-Knowledge Distillation</h3>
<p><a href='http://arxiv.org/abs/2402.03216v1'>http://arxiv.org/abs/2402.03216v1</a></p>
<p><b>Compressor summary</b>: The paper introduces M3-Embedding, a versatile model for multi-lingual, cross-lingual, and various functionalities of information retrieval, with new state-of-the-art results and novel training techniques.</p><hr><h3>Organic or Diffused: Can We Distinguish Human Art from AI-generated  Images?</h3>
<p><a href='http://arxiv.org/abs/2402.03214v1'>http://arxiv.org/abs/2402.03214v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates different methods for distinguishing AI-generated images from human art across various settings and scenarios, finding that a hybrid approach combining both human and automated detectors is most effective.</p><hr><h3>Light and Optimal Schr√∂dinger Bridge Matching</h3>
<p><a href='http://arxiv.org/abs/2402.03207v1'>http://arxiv.org/abs/2402.03207v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an optimal Schr"odinger bridge matching procedure that recovers the SB process with a single step and arbitrary transport plan, and relates it to energy-based modeling objectives.</p><hr><h3>Guidance with Spherical Gaussian Constraint for Conditional Diffusion</h3>
<p><a href='http://arxiv.org/abs/2402.03201v1'>http://arxiv.org/abs/2402.03201v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Diffusion with Spherical Gaussian constraint (DSG), a method that improves conditional diffusion models by constraining guidance steps within the data manifold, leading to better sample quality and faster sampling processes.</p><hr><h3>Isotropy, Clusters, and Classifiers</h3>
<p><a href='http://arxiv.org/abs/2402.03191v1'>http://arxiv.org/abs/2402.03191v1</a></p>
<p><b>Compressor summary</b>: The paper argues that isotropy in embedding spaces, a property that has been debated recently, cannot coexist with clustered data, which also harms linear classification tasks.</p><hr><h3>Unified Hallucination Detection for Multimodal Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.03190v1'>http://arxiv.org/abs/2402.03190v1</a></p>
<p><b>Compressor summary</b>: The paper introduces MHaluBench, a benchmark for evaluating multimodal hallucination detection methods, and UNIHD, a framework that uses auxiliary tools to detect hallucinations in large language models.</p><hr><h3>Towards mitigating uncann(eye)ness in face swaps via gaze-centric loss  terms</h3>
<p><a href='http://arxiv.org/abs/2402.03188v1'>http://arxiv.org/abs/2402.03188v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new loss equation for face swapping that improves the realism of the eyes, reducing uncanny valley effects and making it harder to detect deepfakes.</p><hr><h3>How Good is a Single Basin?</h3>
<p><a href='http://arxiv.org/abs/2402.03187v1'>http://arxiv.org/abs/2402.03187v1</a></p>
<p><b>Compressor summary</b>: The study shows that connected ensembles with more interaction negatively affect performance but can be improved by re-discovering multi-basin deep ensembles through distillation.</p><hr><h3>Empowering Time Series Analysis with Large Language Models: A Survey</h3>
<p><a href='http://arxiv.org/abs/2402.03182v1'>http://arxiv.org/abs/2402.03182v1</a></p>
<p><b>Compressor summary</b>: The text reviews methods that use large language models (LLMs) for time series analysis, discussing their challenges, motivations, and applications in various domains.</p><hr><h3>C-RAG: Certified Generation Risks for Retrieval-Augmented Language  Models</h3>
<p><a href='http://arxiv.org/abs/2402.03181v1'>http://arxiv.org/abs/2402.03181v1</a></p>
<p><b>Compressor summary</b>: This paper proposes C-RAG, a framework to certify and reduce generation risks in retrieval-augmented language models by grounding external knowledge and providing theoretical guarantees.</p><hr><h3>CIDAR: Culturally Relevant Instruction Dataset For Arabic</h3>
<p><a href='http://arxiv.org/abs/2402.03177v1'>http://arxiv.org/abs/2402.03177v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CIDAR, an open Arabic instruction-tuning dataset that reflects the diverse cultures of the Arab region and addresses the biases in existing instruction datasets towards Western culture.</p><hr><h3>The Matrix: A Bayesian learning model for LLMs</h3>
<p><a href='http://arxiv.org/abs/2402.03175v1'>http://arxiv.org/abs/2402.03175v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a Bayesian learning model to understand how Large Language Models work by approximating an ideal generative text model based on predicting the next token.</p><hr><h3>Multi: Multimodal Understanding Leaderboard with Text and Images</h3>
<p><a href='http://arxiv.org/abs/2402.03173v1'>http://arxiv.org/abs/2402.03173v1</a></p>
<p><b>Compressor summary</b>: Multi is a comprehensive benchmark for multimodal large language models that tests their understanding of complex figures, tables, and scientific questions in realistic examination styles.</p><hr><h3>Accurate and Well-Calibrated ICD Code Assignment Through Attention Over  Diverse Label Embeddings</h3>
<p><a href='http://arxiv.org/abs/2402.03172v1'>http://arxiv.org/abs/2402.03172v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new automated method for assigning ICD codes to clinical texts using a Transformer-based encoder and label embeddings, achieving better performance than previous models.</p><hr><h3>Homograph Attacks on Maghreb Sentiment Analyzers</h3>
<p><a href='http://arxiv.org/abs/2402.03171v1'>http://arxiv.org/abs/2402.03171v1</a></p>
<p><b>Compressor summary</b>: Homograph attacks severely reduce sentiment analysis accuracy for Arabic dialects, highlighting the need for ethical and responsible machine learning.</p><hr><h3>Is Mamba Capable of In-Context Learning?</h3>
<p><a href='http://arxiv.org/abs/2402.03170v1'>http://arxiv.org/abs/2402.03170v1</a></p>
<p><b>Compressor summary</b>: Mamba is a new model that performs as well as transformers in learning from sequential data.</p><hr><h3>RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein  Segmentation and Classification</h3>
<p><a href='http://arxiv.org/abs/2402.03166v1'>http://arxiv.org/abs/2402.03166v1</a></p>
<p><b>Compressor summary</b>: RRWNet is an automated framework that uses a neural network to segment retinal blood vessels and correct errors in classification, improving the accuracy of disease biomarkers.</p><hr><h3>Decidable Reasoning About Time in Finite-Domain Situation Calculus  Theories</h3>
<p><a href='http://arxiv.org/abs/2402.03164v1'>http://arxiv.org/abs/2402.03164v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new approach to represent time in cyber-physical systems using clocks as real-valued fluents, making the reachability problem decidable and enabling Golog program realization.</p><hr><h3>Linguistic features for sentence difficulty prediction in ABSA</h3>
<p><a href='http://arxiv.org/abs/2402.03163v1'>http://arxiv.org/abs/2402.03163v1</a></p>
<p><b>Compressor summary</b>: The paper explores what makes sentences difficult for aspect-based sentiment analysis by analyzing different data sets and using a combination of classifiers and linguistic features to measure difficulty.</p><hr><h3>Direct-a-Video: Customized Video Generation with User-Directed Camera  Movement and Object Motion</h3>
<p><a href='http://arxiv.org/abs/2402.03162v1'>http://arxiv.org/abs/2402.03162v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Direct-a-Video, a system that enables independent control of object motion and camera movement in text-to-video models using cross-attention layers and self-supervised training.</p><hr><h3>Video-LaVIT: Unified Video-Language Pre-training with Decoupled  Visual-Motional Tokenization</h3>
<p><a href='http://arxiv.org/abs/2402.03161v1'>http://arxiv.org/abs/2402.03161v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an efficient method to pre-train LLMs on videos by decomposing them into keyframes and motions, enabling unified generative pre-training of video, image, and text content.</p><hr><h3>Optimal and Near-Optimal Adaptive Vector Quantization</h3>
<p><a href='http://arxiv.org/abs/2402.03158v1'>http://arxiv.org/abs/2402.03158v1</a></p>
<p><b>Compressor summary</b>: The paper presents new algorithms that improve the efficiency and scalability of adaptive vector quantization (AVQ), enabling its wider use in machine learning optimization.</p><hr><h3>A Multi-step Loss Function for Robust Learning of the Dynamics in  Model-based Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2402.03146v1'>http://arxiv.org/abs/2402.03146v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a multi-step objective for training one-step models in model-based reinforcement learning, which improves trajectory prediction and handling of noisy data.</p><hr><h3>Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for  Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.03142v1'>http://arxiv.org/abs/2402.03142v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Neural network pruning is important for reducing model complexity and memory usage
- Existing pruning algorithms have limitations
- KEN is a novel, universal, and unstructured pruning algorithm based on KDE
- KEN preserves the most significant parameters and restores others to pre-training state
- KEN achieves better or equal performance with 25% parameter reduction or more
- KEN_viz is an explainable tool that visualizes the optimized model composition and subnetwork selection

Summary:
KEN is a new pruning algorithm that uses KDE to selectively preserve significant parameters in transformer models, achieving better or equal performance with less memory and an explainable tool called KEN_viz.</p><hr><h3>Boosting Long-Delayed Reinforcement Learning with Auxiliary  Short-Delayed Task</h3>
<p><a href='http://arxiv.org/abs/2402.03141v1'>http://arxiv.org/abs/2402.03141v1</a></p>
<p><b>Compressor summary</b>: AD-RL improves reinforcement learning in delayed scenarios by using a short-delayed auxiliary task to learn the value function faster and more efficiently for the long-delayed main task.</p><hr><h3>Enhancing Neural Subset Selection: Integrating Background Information  into Set Representations</h3>
<p><a href='http://arxiv.org/abs/2402.03139v1'>http://arxiv.org/abs/2402.03139v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a new method for selecting subsets from larger sets in drug discovery using neural networks that considers the superset's information, which improves performance over existing methods.</p><hr><h3>Just Cluster It: An Approach for Exploration in High-Dimensions using  Clustering and Pre-Trained Representations</h3>
<p><a href='http://arxiv.org/abs/2402.03138v1'>http://arxiv.org/abs/2402.03138v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a clustering-based exploration method for 3-D environments using random or pre-trained representations, which outperforms other exploration methods.</p><hr><h3>Sociolinguistically Informed Interpretability: A Case Study on Hinglish  Emotion Classification</h3>
<p><a href='http://arxiv.org/abs/2402.03137v1'>http://arxiv.org/abs/2402.03137v1</a></p>
<p><b>Compressor summary</b>: The study investigates if pre-trained language models can learn associations between language choice and emotional expression in Hinglish, finding that they do but may overgeneralize this in some cases.</p><hr><h3>Mastering Zero-Shot Interactions in Cooperative and Competitive  Simultaneous Games</h3>
<p><a href='http://arxiv.org/abs/2402.03136v1'>http://arxiv.org/abs/2402.03136v1</a></p>
<p><b>Compressor summary</b>: Albatross is a novel algorithm that uses simulated self-play and planning to learn how to interact with agents of any strength in simultaneous games, achieving better results than AlphaZero and previous methods in both competitive and cooperative scenarios.</p><hr><h3>Constrained Decoding for Cross-lingual Label Projection</h3>
<p><a href='http://arxiv.org/abs/2402.03131v1'>http://arxiv.org/abs/2402.03131v1</a></p>
<p><b>Compressor summary</b>: The authors propose a constrained decoding method for label projection in zero-shot cross-lingual transfer, improving translation quality and performance over existing methods.</p><hr><h3>How Free is Parameter-Free Stochastic Optimization?</h3>
<p><a href='http://arxiv.org/abs/2402.03126v1'>http://arxiv.org/abs/2402.03126v1</a></p>
<p><b>Compressor summary</b>: The paper explores if there are completely parameter-free methods for stochastic optimization, and shows that simple hyperparameter search can achieve this in non-convex settings, while providing a partially parameter-free method in convex settings.</p><hr><h3>Good Teachers Explain: Explanation-Enhanced Knowledge Distillation</h3>
<p><a href='http://arxiv.org/abs/2402.03119v1'>http://arxiv.org/abs/2402.03119v1</a></p>
<p><b>Compressor summary</b>: The paper introduces e$^2$KD, a method that improves knowledge distillation by aligning teacher and student explanations, leading to better accuracy, agreement, and robustness.</p><hr><h3>Discovering interpretable models of scientific image data with deep  learning</h3>
<p><a href='http://arxiv.org/abs/2402.03115v1'>http://arxiv.org/abs/2402.03115v1</a></p>
<p><b>Compressor summary</b>: The paper proposes methods to create interpretable models from complex image data using disentangled representation learning, sparse neural networks, and symbolic regression, and shows their usefulness in bioimaging for cell state classification.</p><hr><h3>Infrared Spectra Prediction for Diazo Groups Utilizing a Machine  Learning Approach with Structural Attention Mechanism</h3>
<p><a href='http://arxiv.org/abs/2402.03112v1'>http://arxiv.org/abs/2402.03112v1</a></p>
<p><b>Compressor summary</b>: The text describes a machine learning model that uses Structural Attention Mechanism to improve the prediction and interpretation of infrared spectra, especially for diazo compounds, by focusing on chemical information near functional groups.</p><hr><h3>Non-Stationary Latent Auto-Regressive Bandits</h3>
<p><a href='http://arxiv.org/abs/2402.03110v1'>http://arxiv.org/abs/2402.03110v1</a></p>
<p><b>Compressor summary</b>: A new model and algorithm for non-stationary multi-armed bandits with latent auto-regressive rewards are proposed and shown to perform better than standard UCB in various settings.</p><hr><h3>Intent-based Prompt Calibration: Enhancing prompt optimization with  synthetic boundary cases</h3>
<p><a href='http://arxiv.org/abs/2402.03099v1'>http://arxiv.org/abs/2402.03099v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new method for automatic prompt engineering that uses calibration and synthetic data generation to improve the performance of Large Language Models on real-world tasks.</p><hr><h3>Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object  Detector</h3>
<p><a href='http://arxiv.org/abs/2402.03094v1'>http://arxiv.org/abs/2402.03094v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method, CD-ViTO, for cross-domain few-shot object detection that improves the performance of open-set detectors by adding novel components and outperforms existing methods on both out-of-domain and in-domain datasets.</p><hr><h3>AI-Enhanced Virtual Reality in Medicine: A Comprehensive Survey</h3>
<p><a href='http://arxiv.org/abs/2402.03093v1'>http://arxiv.org/abs/2402.03093v1</a></p>
<p><b>Compressor summary</b>: The paper examines how artificial intelligence and virtual reality are transforming medical care and services through three categories of applications: visualization enhancement, medical data processing, and intervention assistance.</p><hr><h3>Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual  Text Processing</h3>
<p><a href='http://arxiv.org/abs/2402.03082v1'>http://arxiv.org/abs/2402.03082v1</a></p>
<p><b>Compressor summary</b>: The authors provide a comprehensive analysis of recent advancements and challenges in the field of visual text processing, covering various tasks, features, learning paradigms, and datasets.</p><hr><h3>Multilingual transformer and BERTopic for short text topic modeling: The  case of Serbian</h3>
<p><a href='http://arxiv.org/abs/2402.03067v1'>http://arxiv.org/abs/2402.03067v1</a></p>
<p><b>Compressor summary</b>: BERTopic performs well in topic modeling for partially preprocessed Serbian tweets, providing more informative topics than LDA and NMF.</p><hr><h3>Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes  Uncertainty</h3>
<p><a href='http://arxiv.org/abs/2402.03055v1'>http://arxiv.org/abs/2402.03055v1</a></p>
<p><b>Compressor summary</b>: PAC is a new reinforcement learning algorithm that combines stochastic policies and critics, using PAC-Bayes analysis to model and adapt uncertainty, leading to better exploration and control.</p><hr><h3>Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for  Semantic Representations</h3>
<p><a href='http://arxiv.org/abs/2402.03053v1'>http://arxiv.org/abs/2402.03053v1</a></p>
<p><b>Compressor summary</b>: The paper presents Malaysian language models Llama2 and Mistral fine-tuned for embedding tasks, showing their effectiveness in Semantic Similarity and Retrieval-Augmented Generation.</p><hr><h3>EasyInstruct: An Easy-to-use Instruction Processing Framework for Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.03049v1'>http://arxiv.org/abs/2402.03049v1</a></p>
<p><b>Compressor summary</b>: EasyInstruct is an open-source framework that simplifies instruction processing for large language models and encourages more research on instruction data.</p><hr><h3>PFDM: Parser-Free Virtual Try-on via Diffusion Model</h3>
<p><a href='http://arxiv.org/abs/2402.03047v1'>http://arxiv.org/abs/2402.03047v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Virtual try-on improves garment shopping experiences but needs accurate segmentation masks
- PFDM is a parser-free virtual try-on method based on diffusion model
- PFDM uses pseudo-images, Garment Fusion Attention, and large-scale dataset to synthesize high-fidelity images

Summary:
PFDM is a novel parser-free virtual try-on method that can seamlessly wear garments on the target person using diffusion model, pseudo-images, and Garment Fusion Attention, achieving high-fidelity results.</p><hr><h3>Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement  Learning</h3>
<p><a href='http://arxiv.org/abs/2402.03046v1'>http://arxiv.org/abs/2402.03046v1</a></p>
<p><b>Compressor summary</b>: Open RL Benchmark is a community-driven repository of fully tracked Reinforcement Learning experiments that allows easy comparison and reproducibility of RL algorithms.</p><hr><h3>SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach</h3>
<p><a href='http://arxiv.org/abs/2402.03043v1'>http://arxiv.org/abs/2402.03043v1</a></p>
<p><b>Compressor summary</b>: The paper proposes SIDU-TXT, an explainable AI method that provides word-level explanations for text classification models, and evaluates its performance on image and text datasets using a comprehensive framework.</p><hr><h3>InteractiveVideo: User-Centric Controllable Video Generation with  Synergistic Multimodal Instructions</h3>
<p><a href='http://arxiv.org/abs/2402.03040v1'>http://arxiv.org/abs/2402.03040v1</a></p>
<p><b>Compressor summary</b>: InteractiveVideo is a framework that allows users to interactively generate videos using various input mechanisms and refine the result through user instructions.</p><hr><h3>Automatic Combination of Sample Selection Strategies for Few-Shot  Learning</h3>
<p><a href='http://arxiv.org/abs/2402.03038v1'>http://arxiv.org/abs/2402.03038v1</a></p>
<p><b>Compressor summary</b>: This paper investigates how different sample selection methods affect few-shot learning performance and proposes a new method (ACSESS) that combines them for better results.</p><hr><h3>Data-induced multiscale losses and efficient multirate gradient descent  schemes</h3>
<p><a href='http://arxiv.org/abs/2402.03021v1'>http://arxiv.org/abs/2402.03021v1</a></p>
<p><b>Compressor summary</b>: The paper explores how multiscale data affects deep learning and proposes a new gradient descent method that adapts learning rates based on data variations.</p><hr><h3>Taylor Videos for Action Recognition</h3>
<p><a href='http://arxiv.org/abs/2402.03019v1'>http://arxiv.org/abs/2402.03019v1</a></p>
<p><b>Compressor summary</b>: Taylor video is a new format that highlights dominant motions in each frame using Taylor series expansion, improving action recognition performance when combined with RGB or optical flow videos.</p><hr><h3>Toward Green and Human-Like Artificial Intelligence: A Complete Survey  on Contemporary Few-Shot Learning Approaches</h3>
<p><a href='http://arxiv.org/abs/2402.03017v1'>http://arxiv.org/abs/2402.03017v1</a></p>
<p><b>Compressor summary</b>: The text provides a comprehensive overview of Few-Shot Learning, its taxonomy, applications, and future research directions.</p><hr><h3>Whom to Trust? Elective Learning for Distributed Gaussian Process  Regression</h3>
<p><a href='http://arxiv.org/abs/2402.03014v1'>http://arxiv.org/abs/2402.03014v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm, Pri-GP, that improves cooperative learning in multi-agent systems by allowing agents to selectively request predictions from trustworthy neighbors and ensuring reliable predictions.</p><hr><h3>On the Impact of Output Perturbation on Fairness in Binary Linear  Classification</h3>
<p><a href='http://arxiv.org/abs/2402.03011v1'>http://arxiv.org/abs/2402.03011v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how output perturbation affects individual and group fairness in binary linear classification under differential privacy.</p><hr><h3>UniMem: Towards a Unified View of Long-Context Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.03009v1'>http://arxiv.org/abs/2402.03009v1</a></p>
<p><b>Compressor summary</b>: UniMem is a unified framework for improving large language models' ability to process long contexts by enhancing their memory capabilities, and UniMix integrates the strengths of 16 existing methods based on this framework.</p><hr><h3>On the development of a practical Bayesian optimisation algorithm for  expensive experiments and simulations with changing environmental conditions</h3>
<p><a href='http://arxiv.org/abs/2402.03006v1'>http://arxiv.org/abs/2402.03006v1</a></p>
<p><b>Compressor summary</b>: The article presents ENVBO, an algorithm that optimizes systems in changing environments with controllable and uncontrollable parameters by conditioning on measurements of the latter, demonstrating its effectiveness in a wind farm simulator example.</p><hr><h3>[Citation needed] Data usage and citation practices in medical imaging  conferences</h3>
<p><a href='http://arxiv.org/abs/2402.03003v1'>http://arxiv.org/abs/2402.03003v1</a></p>
<p><b>Compressor summary</b>: The authors developed two tools to detect dataset usage in medical imaging papers and found that there is a high concentration of usage of a limited set of datasets and inconsistent citation practices.</p><hr><h3>Careful with that Scalpel: Improving Gradient Surgery with an EMA</h3>
<p><a href='http://arxiv.org/abs/2402.02998v1'>http://arxiv.org/abs/2402.02998v1</a></p>
<p><b>Compressor summary</b>: Bloop is a method for combining auxiliary objectives with training losses in deep learning models using gradient surgery and moving averages, which improves performance on NLP and vision tasks.</p><hr><h3>Text-Guided Image Clustering</h3>
<p><a href='http://arxiv.org/abs/2402.02996v1'>http://arxiv.org/abs/2402.02996v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Text-Guided Image Clustering, which uses generated text from captioning and VQA models to cluster images, and introduces a novel approach to inject task or domain knowledge for clustering.</p><hr><h3>Decoding-time Realignment of Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.02992v1'>http://arxiv.org/abs/2402.02992v1</a></p>
<p><b>Compressor summary</b>: DeRa is a method to improve language models by adjusting their alignment without retraining, making them more efficient and less prone to errors and biases.</p><hr><h3>A Safety-Adapted Loss for Pedestrian Detection in Automated Driving</h3>
<p><a href='http://arxiv.org/abs/2402.02986v1'>http://arxiv.org/abs/2402.02986v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new training strategy for object detectors in automated driving that considers the criticality of pedestrians to prevent dangerous misdetections.</p><hr><h3>Unsupervised semantic segmentation of high-resolution UAV imagery for  road scene parsing</h3>
<p><a href='http://arxiv.org/abs/2402.02985v1'>http://arxiv.org/abs/2402.02985v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an unsupervised road parsing framework that uses a vision language model and a computer vision model to process UAV images without manual annotations, achieving high accuracy and flexibility.</p><hr><h3>Evaluating Datalog Tools for Meta-reasoning over OWL 2 QL</h3>
<p><a href='http://arxiv.org/abs/2402.02978v1'>http://arxiv.org/abs/2402.02978v1</a></p>
<p><b>Compressor summary</b>: The paper compares different logic programming tools for meta-querying in ontologies under the Metamodeling Semantic Entailment Regime (MSER) using Datalog, a practical approach for sizeable ontologies.</p><hr><h3>Variational Flow Models: Flowing in Your Style</h3>
<p><a href='http://arxiv.org/abs/2402.02977v1'>http://arxiv.org/abs/2402.02977v1</a></p>
<p><b>Compressor summary</b>: The paper introduces variational inference methods for posterior flows, a class of stochastic processes, and proposes a training-free method to transform linear flows into straight constant-speed flows for faster sampling and improved accuracy.</p><hr><h3>Boosting, Voting Classifiers and Randomized Sample Compression Schemes</h3>
<p><a href='http://arxiv.org/abs/2402.02976v1'>http://arxiv.org/abs/2402.02976v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a randomized boosting algorithm that improves the theoretical performance of voting classifiers by reducing logarithmic dependencies in the generalization error.</p><hr><h3>Putting Context in Context: the Impact of Discussion Structure on Text  Classification</h3>
<p><a href='http://arxiv.org/abs/2402.02975v1'>http://arxiv.org/abs/2402.02975v1</a></p>
<p><b>Compressor summary</b>: The authors investigate how different types of contextual information, such as linguistic, structural, and temporal, can improve stance detection in text classification using a transformer-based model on a large dataset.</p><hr><h3>Retrieval-Augmented Score Distillation for Text-to-3D Generation</h3>
<p><a href='http://arxiv.org/abs/2402.02972v1'>http://arxiv.org/abs/2402.02972v1</a></p>
<p><b>Compressor summary</b>: RetDream improves text-to-3D generation by using a retrieval-based approach to enhance the quality and geometry of generated scenes, while adapting the diffusion model's 2D prior for view consistency.</p><hr><h3>Delving into Multi-modal Multi-task Foundation Models for Road Scene  Understanding: From Learning Paradigm Perspectives</h3>
<p><a href='http://arxiv.org/abs/2402.02968v1'>http://arxiv.org/abs/2402.02968v1</a></p>
<p><b>Compressor summary</b>: Foundation models improve intelligent vehicle capabilities by processing and fusing diverse data modalities and tasks, with potential applications in various learning paradigms.</p><hr><h3>Mixed Noise and Posterior Estimation with Conditional DeepGEM</h3>
<p><a href='http://arxiv.org/abs/2402.02964v1'>http://arxiv.org/abs/2402.02964v1</a></p>
<p><b>Compressor summary</b>: The authors present a new method for estimating posterior and noise parameters in Bayesian inverse problems using an expectation maximization algorithm and a learned conditional normalizing flow.</p><hr><h3>AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a  Single High-Resolution Image</h3>
<p><a href='http://arxiv.org/abs/2402.02956v1'>http://arxiv.org/abs/2402.02956v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework called AdaTreeFormer that uses a shared encoder with hierarchical feature extraction and attention mechanisms to estimate tree density from aerial or satellite images in different domains.</p><hr><h3>Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine  Workers</h3>
<p><a href='http://arxiv.org/abs/2402.02951v1'>http://arxiv.org/abs/2402.02951v1</a></p>
<p><b>Compressor summary</b>: DynaBRO is a fault-tolerant distributed machine learning method that can handle dynamic Byzantine behaviors without requiring knowledge of the number of malicious machines or losing convergence speed.</p><hr><h3>Kernel PCA for Out-of-Distribution Detection</h3>
<p><a href='http://arxiv.org/abs/2402.02949v1'>http://arxiv.org/abs/2402.02949v1</a></p>
<p><b>Compressor summary</b>: Key points:
- OoD detection is important for DNN reliability
- PCA fails to separate OoD and InD features in nonlinear subspace
- KPCA with non-linear kernels improves OoD-InD separability
- Reconstruction error in KPCA subspace is used for efficient detection
- Empirical results show superior efficiency and efficacy of KPCA-based detector

Summary:
The authors propose a Kernel PCA (KPCA)-based method for Out-of-Distribution (OoD) detection in Deep Neural Networks, which uses non-linear kernels to enhance the separability between OoD and In-Distribution features and achieves efficient and accurate detection with low reconstruction error.</p><hr><h3>HoughToRadon Transform: New Neural Network Layer for Features  Improvement in Projection Space</h3>
<p><a href='http://arxiv.org/abs/2402.02946v1'>http://arxiv.org/abs/2402.02946v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new layer for neural networks, HoughToRadon Transform, which improves speed and accuracy in semantic image segmentation tasks by modifying feature maps after Hough Transform.</p><hr><h3>Exploring the Synergies of Hybrid CNNs and ViTs Architectures for  Computer Vision: A survey</h3>
<p><a href='http://arxiv.org/abs/2402.02941v1'>http://arxiv.org/abs/2402.02941v1</a></p>
<p><b>Compressor summary</b>: The text is a comprehensive review of hybrid CNN-ViT architectures in computer vision, exploring their synergies, challenges, and future directions.</p><hr><h3>InterpretCC: Conditional Computation for Inherently Interpretable Neural  Networks</h3>
<p><a href='http://arxiv.org/abs/2402.02933v1'>http://arxiv.org/abs/2402.02933v1</a></p>
<p><b>Compressor summary</b>: InterpretCC is a family of interpretable neural networks that adaptively activate features to provide trustworthy explanations, actionable interpretations, and accurate predictions for human-facing domains.</p><hr><h3>Instance Segmentation XXL-CT Challenge of a Historic Airplane</h3>
<p><a href='http://arxiv.org/abs/2402.02928v1'>http://arxiv.org/abs/2402.02928v1</a></p>
<p><b>Compressor summary</b>: The text describes a challenge to test machine learning-based image segmentation for identifying parts of a historic airplane in XL-CT images.</p><hr><h3>Automated Cognate Detection as a Supervised Link Prediction Task with  Cognate Transformer</h3>
<p><a href='http://arxiv.org/abs/2402.02926v1'>http://arxiv.org/abs/2402.02926v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a transformer-based method for automated cognate detection in historical linguistics, which uses labeled information and multiple sequence alignments to improve accuracy and efficiency.</p><hr><h3>Pixel-Wise Color Constancy via Smoothness Techniques in Multi-Illuminant  Scenes</h3>
<p><a href='http://arxiv.org/abs/2402.02922v1'>http://arxiv.org/abs/2402.02922v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new color constancy method for images with multiple light sources, which learns pixel-wise illumination maps and preserves smoothness and natural appearance using total variation loss, bilateral filter, and label-smoothing techniques.</p><hr><h3>A Computational Model for the Assessment of Mutual Intelligibility Among  Closely Related Languages</h3>
<p><a href='http://arxiv.org/abs/2402.02915v1'>http://arxiv.org/abs/2402.02915v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a computer-assisted method using a linguistic model and semantic vectors to study mutual intelligibility between closely related languages, such as German, Dutch, and English.</p><hr><h3>DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage  Temporal Convolutional Network</h3>
<p><a href='http://arxiv.org/abs/2402.02910v1'>http://arxiv.org/abs/2402.02910v1</a></p>
<p><b>Compressor summary</b>: This study develops a waist-mounted sensor that accurately recognizes four exercises in an older adult rehabilitation program, improving on existing methods.</p><hr><h3>ViewFusion: Learning Composable Diffusion Models for Novel View  Synthesis</h3>
<p><a href='http://arxiv.org/abs/2402.02906v1'>http://arxiv.org/abs/2402.02906v1</a></p>
<p><b>Compressor summary</b>: ViewFusion is an end-to-end generative approach to novel view synthesis that adapts to multiple scenes and object classes, uses variable number of views, and works well in undetermined conditions, but has limitations in inference speed and dataset size.</p><hr><h3>LLM Agents in Interaction: Measuring Personality Consistency and  Linguistic Alignment in Interacting Populations of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.02896v1'>http://arxiv.org/abs/2402.02896v1</a></p>
<p><b>Compressor summary</b>: The text explores how personality profiles affect the behaviour of large language models in naturalistic dialogues and calls for more research on crafting human-like personas for interactive AI agents.</p><hr><h3>Motion-Aware Video Frame Interpolation</h3>
<p><a href='http://arxiv.org/abs/2402.02892v1'>http://arxiv.org/abs/2402.02892v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel video frame interpolation method that uses a hierarchical pyramid module to estimate intermediate optical flow, reducing complexity and improving accuracy.</p><hr><h3>Black-Box Approximation and Optimization with Hierarchical Tucker  Decomposition</h3>
<p><a href='http://arxiv.org/abs/2402.02890v1'>http://arxiv.org/abs/2402.02890v1</a></p>
<p><b>Compressor summary</b>: HTBB is a new black-box approximation and optimization method that uses low-rank hierarchical Tucker decomposition and outperforms existing gradient-free methods and tensor train decomposition for high-dimensional problems.</p><hr><h3>Time-, Memory- and Parameter-Efficient Visual Adaptation</h3>
<p><a href='http://arxiv.org/abs/2402.02887v1'>http://arxiv.org/abs/2402.02887v1</a></p>
<p><b>Compressor summary</b>: Our proposed adaptation method for foundation models does not backpropagate gradients through the backbone, reducing training-time, memory usage, and achieving state-of-the-art accuracy-parameter trade-offs on VTAB benchmark.</p><hr><h3>A Review on Building Blocks of Decentralized Artificial Intelligence</h3>
<p><a href='http://arxiv.org/abs/2402.02885v1'>http://arxiv.org/abs/2402.02885v1</a></p>
<p><b>Compressor summary</b>: Key points:
- AI is transforming our lives but has ethical issues
- Decentralized AI (DEAI) is an alternative to centralized AI (CEAI)
- The paper reviews 71 studies on DEAI and its building blocks

Summary:
The paper explores decentralized AI as a way to address ethical challenges of AI, by reviewing existing work and identifying its components.</p><hr><h3>Approximate Attributions for Off-the-Shelf Siamese Transformers</h3>
<p><a href='http://arxiv.org/abs/2402.02883v1'>http://arxiv.org/abs/2402.02883v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method for attributing Siamese encoders' decisions to linguistic aspects and compare exact and approximate attributions for understanding their behavior.</p><hr><h3>How do Large Language Models Learn In-Context? Query and Key Matrices of  In-Context Heads are Two Towers for Metric Learning</h3>
<p><a href='http://arxiv.org/abs/2402.02872v1'>http://arxiv.org/abs/2402.02872v1</a></p>
<p><b>Compressor summary</b>: The text explores how in-context learning works by merging demonstration features and using attention weights to transfer label information, and proposes a hypothesis with experiments on different models.</p><hr><h3>Statistics without Interpretation: A Sober Look at Explainable Machine  Learning</h3>
<p><a href='http://arxiv.org/abs/2402.02870v1'>http://arxiv.org/abs/2402.02870v1</a></p>
<p><b>Compressor summary</b>: Explanation algorithms are complex, but need clear interpretations to avoid errors; papers should clarify how to use and understand them.</p><hr><h3>Fine-tuning Reinforcement Learning Models is Secretly a Forgetting  Mitigation Problem</h3>
<p><a href='http://arxiv.org/abs/2402.02868v1'>http://arxiv.org/abs/2402.02868v1</a></p>
<p><b>Compressor summary</b>: Fine-tuning RL models can cause forgetting of pre-trained capabilities, leading to poor transfer performance; however, knowledge retention techniques can mitigate this issue and improve results.</p><hr><h3>EEVEE: An Easy Annotation Tool for Natural Language Processing</h3>
<p><a href='http://arxiv.org/abs/2402.02864v1'>http://arxiv.org/abs/2402.02864v1</a></p>
<p><b>Compressor summary</b>: EEVEE is an easy-to-use web-based tool for creating NLP datasets with support for various tasks.</p><hr><h3>Deep autoregressive density nets vs neural ensembles for model-based  offline reinforcement learning</h3>
<p><a href='http://arxiv.org/abs/2402.02858v1'>http://arxiv.org/abs/2402.02858v1</a></p>
<p><b>Compressor summary</b>: The authors propose a single autoregressive model for model-based reinforcement learning without ensembles, achieving good performance on D4RL benchmark while analyzing model properties.</p><hr><h3>Enhancing Compositional Generalization via Compositional Feature  Alignment</h3>
<p><a href='http://arxiv.org/abs/2402.02851v1'>http://arxiv.org/abs/2402.02851v1</a></p>
<p><b>Compressor summary</b>: Compositional Feature Alignment (CFA) is a two-stage technique that improves the compositional generalization of pretrained models by learning orthogonal class and domain features and finetuning the encoder with them.</p><hr><h3>Comparing Knowledge Sources for Open-Domain Scientific Claim  Verification</h3>
<p><a href='http://arxiv.org/abs/2402.02844v1'>http://arxiv.org/abs/2402.02844v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates open-domain claim verification systems on biomedical and health claims using different knowledge sources and retrieval techniques.</p><hr><h3>With a Little Help from my (Linguistic) Friends: Topic Segmentation of  Multi-party Casual Conversations</h3>
<p><a href='http://arxiv.org/abs/2402.02837v1'>http://arxiv.org/abs/2402.02837v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a formal approach to segment dialogues into topic segments and analyse their interactions to understand topic organization in open-domain conversations.</p><hr><h3>Shortened LLaMA: A Simple Depth Pruning for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.02834v1'>http://arxiv.org/abs/2402.02834v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a depth pruning method for large language models that improves inference speeds, especially when memory is limited, compared to width pruning methods.</p><hr><h3>PowerGraph: A power grid benchmark dataset for graph neural networks</h3>
<p><a href='http://arxiv.org/abs/2402.02827v1'>http://arxiv.org/abs/2402.02827v1</a></p>
<p><b>Compressor summary</b>: PowerGraph is a new dataset for training and explaining graph neural networks on cascading failure events in power grids, which could improve GNN applications across various disciplines.</p><hr><h3>SynthVision -- Harnessing Minimal Input for Maximal Output in Computer  Vision Models using Synthetic Image data</h3>
<p><a href='http://arxiv.org/abs/2402.02826v1'>http://arxiv.org/abs/2402.02826v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Computer vision models can detect Human Papilloma Virus Genital warts using synthetic data generated by diffusion models
- The model achieved high accuracy, precision, recall, and F1 Score for both HPV and normal cases
- The approach is fast and innovative for urgent medical situations

Summary:
The study shows how diffusion models can create realistic synthetic images for training a computer vision model to detect genital warts caused by Human Papilloma Virus with high accuracy and reliability.</p><hr><h3>Evading Data Contamination Detection for Language Models is (too) Easy</h3>
<p><a href='http://arxiv.org/abs/2402.02823v1'>http://arxiv.org/abs/2402.02823v1</a></p>
<p><b>Compressor summary</b>: The text discusses the problem of deliberate contamination of large language models' performance measurements by malicious providers and proposes a new method to detect it.</p><hr><h3>Revisiting VAE for Unsupervised Time Series Anomaly Detection: A  Frequency Perspective</h3>
<p><a href='http://arxiv.org/abs/2402.02820v1'>http://arxiv.org/abs/2402.02820v1</a></p>
<p><b>Compressor summary</b>: Frequency-enhanced Conditional Variational Autoencoder (FCVAE) is a novel unsupervised method for detecting anomalies in time series data that integrates global and local frequency features to capture both long-periodic and short-periodic patterns, achieving better performance than existing VAE-based methods.</p><hr><h3>State estimation of urban air pollution with statistical, physical, and  super-learning graph models</h3>
<p><a href='http://arxiv.org/abs/2402.02812v1'>http://arxiv.org/abs/2402.02812v1</a></p>
<p><b>Compressor summary</b>: The authors propose different reconstruction methods for real-time urban air pollution maps using city graphs and super-learning models, and test their performance in Paris.</p><hr><h3>Multi-scale fMRI time series analysis for understanding  neurodegeneration in MCI</h3>
<p><a href='http://arxiv.org/abs/2402.02811v1'>http://arxiv.org/abs/2402.02811v1</a></p>
<p><b>Compressor summary</b>: The study uses a novel deep learning technique to analyze multi-scale views of resting-state fMRI volumes and classify mild cognitive impairment from healthy controls, revealing differences in brain network activity and dynamics.</p><hr><h3>Are Sounds Sound for Phylogenetic Reconstruction?</h3>
<p><a href='http://arxiv.org/abs/2402.02807v1'>http://arxiv.org/abs/2402.02807v1</a></p>
<p><b>Compressor summary</b>: The text compares the performance of using lexical cognates and sound correspondences for language family tree reconstruction and finds that cognate-based approaches are more accurate.</p><hr><h3>Graph-enhanced Large Language Models in Asynchronous Plan Reasoning</h3>
<p><a href='http://arxiv.org/abs/2402.02805v1'>http://arxiv.org/abs/2402.02805v1</a></p>
<p><b>Compressor summary</b>: The study investigates if large language models can plan asynchronously and presents a new technique called Plan Like a Graph that improves their performance but reveals their limitations in complex tasks.</p><hr><h3>KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language  Models</h3>
<p><a href='http://arxiv.org/abs/2402.02801v1'>http://arxiv.org/abs/2402.02801v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Paper proposes KS-Lottery, a method to find effective parameters for multilingual fine-tuning of LLMs using Kolmogorov-Smirnov Test
- Theoretically proves that KS-Lottery can find certified winning tickets in the embedding layer
- Experiments show that KS-Lottery achieves comparable performance with full fine-tuning using much fewer parameters

Summary:
KS-Lottery is a new method to efficiently fine-tune LLMs for multilingual tasks by identifying a small subset of parameters that perform well, using a statistical test and a theoretical guarantee.</p><hr><h3>Extreme Two-View Geometry From Object Poses with Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2402.02800v1'>http://arxiv.org/abs/2402.02800v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new method to estimate camera poses from extreme viewpoint differences by using object priors and diffusion models to synthesize novel-view images and match them.</p><hr><h3>Joint Attention-Guided Feature Fusion Network for Saliency Detection of  Surface Defects</h3>
<p><a href='http://arxiv.org/abs/2402.02797v1'>http://arxiv.org/abs/2402.02797v1</a></p>
<p><b>Compressor summary</b>: JAFFNet is a feature fusion network for saliency detection of surface defects, which adapts to different scales and backgrounds by using joint attention and dense receptive fields.</p><hr><h3>Rethinking Optimization and Architecture for Tiny Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.02791v1'>http://arxiv.org/abs/2402.02791v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The study explores how to optimize tiny language models (1B parameters) for mobile devices.
- It proposes several design formulas and trains PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on multilingual corpora.
- The experiments show significant improvement over baselines and even some state-of-the-art models.

Summary:
The study optimizes tiny language models for mobile devices using design formulas and achieves better performance than many larger models.</p><hr><h3>Stable and Robust Deep Learning By Hyperbolic Tangent Exponential Linear  Unit (TeLU)</h3>
<p><a href='http://arxiv.org/abs/2402.02790v1'>http://arxiv.org/abs/2402.02790v1</a></p>
<p><b>Compressor summary</b>: The Hyperbolic Tangent Exponential Linear Unit (TeLU) is a novel activation function for neural networks that improves stability and robustness over existing functions like ReLU, GELU, and Mish.</p><hr><h3>From Partial to Strictly Incremental Constituent Parsing</h3>
<p><a href='http://arxiv.org/abs/2402.02782v1'>http://arxiv.org/abs/2402.02782v1</a></p>
<p><b>Compressor summary</b>: The paper explores how to build incremental parsers that output trees using prefix information, guided by left-to-right language models and tree-decoding modules, and compares them to non-incremental and partially incremental models.</p><hr><h3>Contrastive Diffuser: Planning Towards High Return States via  Contrastive Learning</h3>
<p><a href='http://arxiv.org/abs/2402.02772v1'>http://arxiv.org/abs/2402.02772v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CDiffuser, a novel diffusion-based reinforcement learning method that uses return contrast to improve the base distribution and generate trajectories towards high-return states.</p><hr><h3>Learning from Teaching Regularization: Generalizable Correlations Should  be Easy to Imitate</h3>
<p><a href='http://arxiv.org/abs/2402.02769v1'>http://arxiv.org/abs/2402.02769v1</a></p>
<p><b>Compressor summary</b>: LoT is a new technique that improves generalization in deep neural networks by training auxiliary student learners who teach the main model more abstract and generalizable correlations.</p><hr><h3>Transmission Line Detection Based on Improved Hough Transform</h3>
<p><a href='http://arxiv.org/abs/2402.02761v1'>http://arxiv.org/abs/2402.02761v1</a></p>
<p><b>Compressor summary</b>: Our improved stochastic Hough transform technique can accurately and quickly detect transmission lines in UAV images by using the Hessian matrix for initial processing and enhancing boundary search and pixel row segmentation.</p><hr><h3>KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache</h3>
<p><a href='http://arxiv.org/abs/2402.02750v1'>http://arxiv.org/abs/2402.02750v1</a></p>
<p><b>Compressor summary</b>: The paper proposes KIVI, a 2bit KV cache quantization algorithm that reduces memory usage and enables larger batch sizes for large language models.</p><hr><h3>Standard Gaussian Process is All You Need for High-Dimensional Bayesian  Optimization</h3>
<p><a href='http://arxiv.org/abs/2402.02746v1'>http://arxiv.org/abs/2402.02746v1</a></p>
<p><b>Compressor summary</b>: Standard Bayesian Optimization with Gaussian process regression often performs well in high-dimensional optimization problems, contrary to common belief.</p><hr><h3>Glocal Hypergradient Estimation with Koopman Operator</h3>
<p><a href='http://arxiv.org/abs/2402.02741v1'>http://arxiv.org/abs/2402.02741v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to optimize hyperparameters called "glocal hypergradient estimation", which combines reliability and efficiency by using Koopman operator theory to approximate global hypergradients from local ones.</p><hr><h3>Improving Robustness of LiDAR-Camera Fusion Model against Weather  Corruption from Fusion Strategy Perspective</h3>
<p><a href='http://arxiv.org/abs/2402.02738v1'>http://arxiv.org/abs/2402.02738v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates and proposes a method to improve the robustness of LiDAR-camera fusion models for 3D object detection in various weather conditions.</p><hr><h3>Using Motion Cues to Supervise Single-Frame Body Pose and Shape  Estimation in Low Data Regimes</h3>
<p><a href='http://arxiv.org/abs/2402.02736v1'>http://arxiv.org/abs/2402.02736v1</a></p>
<p><b>Compressor summary</b>: The text describes a method to improve human body pose and shape estimation from a single camera using unannotated videos for extra supervision when there is not enough annotated data available.</p><hr><h3>InVA: Integrative Variational Autoencoder for Harmonization of  Multi-modal Neuroimaging Data</h3>
<p><a href='http://arxiv.org/abs/2402.02734v1'>http://arxiv.org/abs/2402.02734v1</a></p>
<p><b>Compressor summary</b>: The paper introduces InVA, a novel method that uses variational autoencoders to efficiently borrow information from multiple images and capture complex non-linear associations for predictive inference.</p><hr><h3>ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer</h3>
<p><a href='http://arxiv.org/abs/2402.02733v1'>http://arxiv.org/abs/2402.02733v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel method to re-age faces and transfer portrait styles in non-photorealistic images in one generative step, using existing networks trained within the same domain.</p><hr><h3>A Generative Approach to Surrogate-based Black-box Attacks</h3>
<p><a href='http://arxiv.org/abs/2402.02732v1'>http://arxiv.org/abs/2402.02732v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a generative surrogate method for black-box attacks on DNNs, which learns the distribution of samples near the target's decision boundaries and crafts adversarial examples with imperceptible differences.</p><hr><h3>Innovative Cybersickness Detection: Exploring Head Movement Patterns in  Virtual Reality</h3>
<p><a href='http://arxiv.org/abs/2402.02725v1'>http://arxiv.org/abs/2402.02725v1</a></p>
<p><b>Compressor summary</b>: This research shows that analyzing head movement patterns can help detect cybersickness with high accuracy using machine learning algorithms.</p><hr><h3>FDNet: Frequency Domain Denoising Network For Cell Segmentation in  Astrocytes Derived From Induced Pluripotent Stem Cells</h3>
<p><a href='http://arxiv.org/abs/2402.02724v1'>http://arxiv.org/abs/2402.02724v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new dataset and method (FDNet) for segmenting astrocytes from microscopy images, which helps to study neuronal metabolism and predict differentiation progress in iPSCs.</p><hr><h3>Discounted Adaptive Online Prediction</h3>
<p><a href='http://arxiv.org/abs/2402.02720v1'>http://arxiv.org/abs/2402.02720v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an adaptive online learning algorithm that can gracefully forget old information and improve over gradient descent for tasks where the future may differ from the past.</p><hr><h3>Understanding the planning of LLM agents: A survey</h3>
<p><a href='http://arxiv.org/abs/2402.02716v1'>http://arxiv.org/abs/2402.02716v1</a></p>
<p><b>Compressor summary</b>: This survey examines how large language models can enhance the planning abilities of autonomous agents, reviewing existing works and categorizing them into different directions, such as task decomposition and memory.</p><hr><h3>Position Paper: What Can Large Language Models Tell Us about Time Series  Analysis</h3>
<p><a href='http://arxiv.org/abs/2402.02713v1'>http://arxiv.org/abs/2402.02713v1</a></p>
<p><b>Compressor summary</b>: The paper discusses the potential of large language models to revolutionize time series analysis and improve decision-making.</p><hr><h3>Architectural Strategies for the optimization of Physics-Informed Neural  Networks</h3>
<p><a href='http://arxiv.org/abs/2402.02711v1'>http://arxiv.org/abs/2402.02711v1</a></p>
<p><b>Compressor summary</b>: Gaussian activations and preconditioned neural architectures improve the training of physics-informed neural networks for solving partial differential equations.</p><hr><h3>Representation Surgery for Multi-Task Model Merging</h3>
<p><a href='http://arxiv.org/abs/2402.02705v1'>http://arxiv.org/abs/2402.02705v1</a></p>
<p><b>Compressor summary</b>: The paper introduces "Surgery," a method to reduce representation bias in multi-task learning by adjusting the merged model's representation to match individual models.</p><hr><h3>Understanding What Affects Generalization Gap in Visual Reinforcement  Learning: Theory and Empirical Evidence</h3>
<p><a href='http://arxiv.org/abs/2402.02701v1'>http://arxiv.org/abs/2402.02701v1</a></p>
<p><b>Compressor summary</b>: This paper analyzes the factors affecting the generalization gap in visual reinforcement learning with distractors and shows that minimizing representation distance between training and testing environments is crucial.</p><hr><h3>Sample Complexity Characterization for Linear Contextual MDPs</h3>
<p><a href='http://arxiv.org/abs/2402.02700v1'>http://arxiv.org/abs/2402.02700v1</a></p>
<p><b>Compressor summary</b>: The paper studies CMDPs with time-varying environments using two linear function approximation models and proposes novel algorithms with guaranteed suboptimality gap and polynomial sample complexity.</p><hr><h3>Beyond Expectations: Learning with Stochastic Dominance Made Practical</h3>
<p><a href='http://arxiv.org/abs/2402.02698v1'>http://arxiv.org/abs/2402.02698v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a general framework for learning with stochastic dominance, which extends the concept to compare arbitrary random variables and finds optimal solutions efficiently.</p><hr><h3>Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit  Models for High-dimensional Gaussian Mixtures</h3>
<p><a href='http://arxiv.org/abs/2402.02697v1'>http://arxiv.org/abs/2402.02697v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes the connections between implicit deep equilibrium models and explicit neural networks using random matrix theory, showing how their spectral behavior depends on activation functions and weight variances.</p><hr><h3>Causal Feature Selection for Responsible Machine Learning</h3>
<p><a href='http://arxiv.org/abs/2402.02696v1'>http://arxiv.org/abs/2402.02696v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Responsible ML addresses issues like interpretability, fairness, robustness, and domain generalization.
- Feature selection is important for responsible ML tasks.
- Causal feature selection identifies features with causal impacts on outcomes and distinguishes correlation from causation.

Summary:
The survey discusses how causal feature selection can enhance responsible ML by ensuring ethical and social values, reliability, and trustworthiness in high-stakes applications.</p><hr><h3>Exploiting Class Probabilities for Black-box Sentence-level Attacks</h3>
<p><a href='http://arxiv.org/abs/2402.02695v1'>http://arxiv.org/abs/2402.02695v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm that uses class probabilities for black-box sentence-level attacks, which are adversarial sentences that fool text classifiers.</p><hr><h3>Statistical Guarantees for Link Prediction using Graph Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.02692v1'>http://arxiv.org/abs/2402.02692v1</a></p>
<p><b>Compressor summary</b>: The paper develops a linear GNN architecture (LG-GNN) that accurately predicts edges in graphs generated by a graphon and provides theoretical guarantees for its performance.</p><hr><h3>Poisson Process for Bayesian Optimization</h3>
<p><a href='http://arxiv.org/abs/2402.02687v1'>http://arxiv.org/abs/2402.02687v1</a></p>
<p><b>Compressor summary</b>: PoPBO is a novel, efficient, and robust Bayesian Optimization method that uses a ranking-based surrogate model based on the Poisson process to handle noisy and intractable function responses in optimization problems.</p><hr><h3>Equivariant Symmetry Breaking Sets</h3>
<p><a href='http://arxiv.org/abs/2402.02681v1'>http://arxiv.org/abs/2402.02681v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework for systematically breaking symmetry in equivariant neural networks using symmetry breaking sets, which are data efficient and applicable to any group.</p><hr><h3>Large Language Models are Geographically Biased</h3>
<p><a href='http://arxiv.org/abs/2402.02680v1'>http://arxiv.org/abs/2402.02680v1</a></p>
<p><b>Compressor summary</b>: The authors study geographic biases in large language models (LLMs) and show that they are correlated with socioeconomic conditions, affecting predictions on sensitive topics.</p><hr><h3>Counterfactual Explanations of Black-box Machine Learning Models using  Causal Discovery with Applications to Credit Rating</h3>
<p><a href='http://arxiv.org/abs/2402.02678v1'>http://arxiv.org/abs/2402.02678v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new XAI framework that uses counterfactual probabilities and prior information to explain black-box models without assuming a known causal graph.</p><hr><h3>Verifiable evaluations of machine learning models using zkSNARKs</h3>
<p><a href='http://arxiv.org/abs/2402.02675v1'>http://arxiv.org/abs/2402.02675v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a method to verify model evaluations of private neural networks using zero-knowledge proofs, improving transparency and trust in commercial machine learning.</p><hr><h3>Utility-Based Reinforcement Learning: Unifying Single-objective and  Multi-objective Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2402.02665v1'>http://arxiv.org/abs/2402.02665v1</a></p>
<p><b>Compressor summary</b>: The paper proposes extending the utility-based paradigm from multi-objective reinforcement learning to single-objective reinforcement learning for various benefits and challenges.</p><hr><h3>Counterfactual Fairness Is Not Demographic Parity, and Other  Observations</h3>
<p><a href='http://arxiv.org/abs/2402.02663v1'>http://arxiv.org/abs/2402.02663v1</a></p>
<p><b>Compressor summary</b>: The author challenges the claim that counterfactual fairness and demographic parity are equivalent, and clarifies common misconceptions about counterfactual fairness.</p><hr><h3>Image-Caption Encoding for Improving Zero-Shot Generalization</h3>
<p><a href='http://arxiv.org/abs/2402.02662v1'>http://arxiv.org/abs/2402.02662v1</a></p>
<p><b>Compressor summary</b>: The paper proposes ICE, a method that uses generated captions to improve OOD generalization of vision-language models for image classification.</p><hr><h3>Multi-step Problem Solving Through a Verifier: An Empirical Analysis on  Model-induced Process Supervision</h3>
<p><a href='http://arxiv.org/abs/2402.02658v1'>http://arxiv.org/abs/2402.02658v1</a></p>
<p><b>Compressor summary</b>: The paper proposes MiPS, a method to automate data curation for process supervision, which improves problem-solving performance by sampling and scoring intermediate step completions using a reasoning model.</p><hr><h3>RACER: An LLM-powered Methodology for Scalable Analysis of  Semi-structured Mental Health Interviews</h3>
<p><a href='http://arxiv.org/abs/2402.02656v1'>http://arxiv.org/abs/2402.02656v1</a></p>
<p><b>Compressor summary</b>: The RACER system uses a large language model to analyze semi-structured interviews in healthcare research, producing themes that match human evaluators' results with high agreement.</p><hr><h3>VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based  Machine Reading Comprehension</h3>
<p><a href='http://arxiv.org/abs/2402.02655v1'>http://arxiv.org/abs/2402.02655v1</a></p>
<p><b>Compressor summary</b>: The paper describes VlogQA, a corpus of Vietnamese spoken language for machine reading comprehension tasks, based on YouTube videos about food and travel, and reports promising results using deep learning models.</p><hr><h3>Learning with Mixture of Prototypes for Out-of-Distribution Detection</h3>
<p><a href='http://arxiv.org/abs/2402.02653v1'>http://arxiv.org/abs/2402.02653v1</a></p>
<p><b>Compressor summary</b>: PALM is a method that improves OOD detection by modeling each class with multiple prototypes and learning more faithful sample embeddings, achieving state-of-the-art performance on CIFAR-100.</p><hr><h3>Vision-Language Models Provide Promptable Representations for  Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2402.02651v1'>http://arxiv.org/abs/2402.02651v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new method for embodied reinforcement learning that uses vision-language models as promptable representations, which improves performance on complex RL tasks in Minecraft and Habitat compared to generic image embeddings or instruction-following methods.</p><hr><h3>Densely Decoded Networks with Adaptive Deep Supervision for Medical  Image Segmentation</h3>
<p><a href='http://arxiv.org/abs/2402.02649v1'>http://arxiv.org/abs/2402.02649v1</a></p>
<p><b>Compressor summary</b>: The paper proposes densely decoded networks (ddn) with crutch connections for refined dense prediction in medical image segmentation, and adaptive deep supervision (ads) for robust feature extraction using layer-wise effective receptive fields (lerf).</p><hr><h3>Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses</h3>
<p><a href='http://arxiv.org/abs/2402.02648v1'>http://arxiv.org/abs/2402.02648v1</a></p>
<p><b>Compressor summary</b>: The paper discusses the issues with large language models' responses to knowledge-intensive questions and proposes a new prompting method, Recursive Chain of Feedback (R-CoF), to improve reliability and validity.</p>