
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2023-12-07</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2023-12-07 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context  Learning</h3>
<p>Xinshun Wang,Zhongbin Fang,Xia Li,Xiangtai Li,Chen Chen,Mengyuan Liu</p>
<p><a href='http://arxiv.org/abs/2312.03703v1'>http://arxiv.org/abs/2312.03703v1</a></p>
<p><b>Compressor summary</b>: Skeleton-in-Context (SiC) is a framework for in-context learning of skeleton sequence modeling that can handle multiple tasks simultaneously, adapt to new tasks, and achieve state-of-the-art performance.</p><hr><h3>Self-conditioned Image Generation via Generating Representations</h3>
<p>Tianhong Li,Dina Katabi,Kaiming He</p>
<p><a href='http://arxiv.org/abs/2312.03701v1'>http://arxiv.org/abs/2312.03701v1</a></p>
<p><b>Compressor summary</b>: RCG is a new image generation method that uses self-supervised representation distribution and achieves high quality results without human annotations.</p><hr><h3>OneLLM: One Framework to Align All Modalities with Language</h3>
<p>Jiaming Han,Kaixiong Gong,Yiyuan Zhang,Jiaqi Wang,Kaipeng Zhang,Dahua Lin,Yu Qiao,Peng Gao,Xiangyu Yue</p>
<p><a href='http://arxiv.org/abs/2312.03700v1'>http://arxiv.org/abs/2312.03700v1</a></p>
<p><b>Compressor summary</b>: The paper introduces OneLLM, a unified framework for aligning eight modalities to language, and presents a multimodal instruction dataset for evaluating its performance on various tasks.</p><hr><h3>PROMISE: A Framework for Model-Driven Stateful Prompt Orchestration</h3>
<p>Wenyuan Wu,Jasmin Heierli,Max Meisterhans,Adrian Moser,Andri Färber,Mateusz Dolata,Elena Gavagnin,Alexandre de Spindler,Gerhard Schwabe</p>
<p><a href='http://arxiv.org/abs/2312.03699v1'>http://arxiv.org/abs/2312.03699v1</a></p>
<p><b>Compressor summary</b>: PROMISE is a framework that helps create and control complex language-based interactions with information systems, improving their effectiveness and efficiency.</p><hr><h3>Intrinsic Harmonization for Illumination-Aware Compositing</h3>
<p>Chris Careaga,Yağız Aksoy,S. Mahdi H. Miangoleh</p>
<p><a href='http://arxiv.org/abs/2312.03698v1'>http://arxiv.org/abs/2312.03698v1</a></p>
<p><b>Compressor summary</b>: The authors propose a self-supervised method for image harmonization that adjusts shading and albedo to match lighting between foreground and background in composited images.</p><hr><h3>On the Role of Edge Dependency in Graph Generative Models</h3>
<p>Sudhanshu Chanpuriya,Cameron Musco,Konstantinos Sotiropoulos,Charalampos Tsourakakis</p>
<p><a href='http://arxiv.org/abs/2312.03691v1'>http://arxiv.org/abs/2312.03691v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new evaluation framework for graph generative models that considers model-generated graph overlap, categorize them into three complexity levels, derive theoretical bounds on their output quality, introduce new models based on dense subgraph discovery, and show competitive results with popular models.</p><hr><h3>Evaluating and Mitigating Discrimination in Language Model Decisions</h3>
<p>Alex Tamkin,Amanda Askell,Liane Lovitt,Esin Durmus,Nicholas Joseph,Shauna Kravec,Karina Nguyen,Jared Kaplan,Deep Ganguli</p>
<p><a href='http://arxiv.org/abs/2312.03689v1'>http://arxiv.org/abs/2312.03689v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method for evaluating the potential discriminatory impact of language models in various use cases by generating diverse prompts with different demographic information, and suggest ways to reduce discrimination through prompt engineering.</p><hr><h3>What Planning Problems Can A Relational Neural Network Solve?</h3>
<p>Jiayuan Mao,Tomás Lozano-Pérez,Joshua B. Tenenbaum,Leslie Pack Kaelbling</p>
<p><a href='http://arxiv.org/abs/2312.03682v1'>http://arxiv.org/abs/2312.03682v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes how relational neural networks, such as graph neural networks and transformers, can be used to learn goal-conditioned policies for planning problems, and identifies three classes of planning problems based on circuit width and depth.</p><hr><h3>Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching</h3>
<p>Lennart Bastian,Yizheng Xie,Nassir Navab,Zorah Lähner</p>
<p><a href='http://arxiv.org/abs/2312.03678v1'>http://arxiv.org/abs/2312.03678v1</a></p>
<p><b>Compressor summary</b>: The proposed method combines different basis functions to create a hybrid spectral space for shape correspondence, improving performance on non-isometric deformations and noisy data.</p><hr><h3>GeoShapley: A Game Theory Approach to Measuring Spatial Effects in  Machine Learning Models</h3>
<p>Ziqi Li</p>
<p><a href='http://arxiv.org/abs/2312.03675v1'>http://arxiv.org/abs/2312.03675v1</a></p>
<p><b>Compressor summary</b>: GeoShapley is a game theory-based approach for measuring the importance of location and its synergies with other features in various machine learning models, and it can be applied to both statistical and black-box models.</p><hr><h3>WarpDiffusion: Efficient Diffusion Model for High-Fidelity Virtual  Try-on</h3>
<p>xujie zhang,Xiu Li,Michael Kampffmeyer,Xin Dong,Zhenyu Xie,Feida Zhu,Haoye Dong,Xiaodan Liang</p>
<p><a href='http://arxiv.org/abs/2312.03667v1'>http://arxiv.org/abs/2312.03667v1</a></p>
<p><b>Compressor summary</b>: WarpDiffusion is a novel method that improves Virtual Try-On by combining warping-based and diffusion-based techniques with attention mechanisms to enhance realism and retain garment details.</p><hr><h3>Generative agent-based modeling with actions grounded in physical,  social, or digital space using Concordia</h3>
<p>Alexander Sasha Vezhnevets,John P. Agapiou,Avia Aharon,Ron Ziv,Jayd Matyas,Edgar A. Duéñez-Guzmán,William A. Cunningham,Simon Osindero,Danny Karmon,Joel Z. Leibo</p>
<p><a href='http://arxiv.org/abs/2312.03664v1'>http://arxiv.org/abs/2312.03664v1</a></p>
<p><b>Compressor summary</b>: Concordia is a library that facilitates constructing and working with Generative Agent-Based Models, which use Large Language Models to apply common sense, control technologies, and communicate in simulations of physical or digital environments.</p><hr><h3>Reason2Drive: Towards Interpretable and Chain-based Reasoning for  Autonomous Driving</h3>
<p>Ming Nie,Renyuan Peng,Chunwei Wang,Xinyue Cai,Jianhua Han,Hang Xu,Li Zhang</p>
<p><a href='http://arxiv.org/abs/2312.03661v1'>http://arxiv.org/abs/2312.03661v1</a></p>
<p><b>Compressor summary</b>: Reason2Drive is a new dataset for studying interpretable reasoning in complex driving environments using large vision-language models.</p><hr><h3>Interpretability Illusions in the Generalization of Simplified Models</h3>
<p>Dan Friedman,Andrew Lampinen,Lucas Dixon,Danqi Chen,Asma Ghandeharioun</p>
<p><a href='http://arxiv.org/abs/2312.03656v1'>http://arxiv.org/abs/2312.03656v1</a></p>
<p><b>Compressor summary</b>: The simplified representations of deep learning models may not accurately capture their behavior outside the training data and may lead to wrong conclusions about their generalization abilities.</p><hr><h3>MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit  Assignment</h3>
<p>Ziyan Wang,Yali Du,Yudi Zhang,Meng Fang,Biwei Huang</p>
<p><a href='http://arxiv.org/abs/2312.03644v1'>http://arxiv.org/abs/2312.03644v1</a></p>
<p><b>Compressor summary</b>: MACCA is a method to accurately assign credit to individual agents in offline multi-agent reinforcement learning by modeling the causal relationships between rewards using a Dynamic Bayesian Network, which works well in both discrete and continuous action settings.</p><hr><h3>Transformer-Powered Surrogates Close the ICF Simulation-Experiment Gap  with Extremely Limited Data</h3>
<p>Matthew L. Olson,Shusen Liu,Jayaraman J. Thiagarajan,Bogdan Kustowski,Weng-Keen Wong,Rushil Anirudh</p>
<p><a href='http://arxiv.org/abs/2312.03642v1'>http://arxiv.org/abs/2312.03642v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new transformer-based method that combines graph hyper-parameter optimization with multi-modal data to improve prediction accuracy in simulation and real-world scenarios.</p><hr><h3>MotionCtrl: A Unified and Flexible Motion Controller for Video  Generation</h3>
<p>Zhouxia Wang,Ziyang Yuan,Xintao Wang,Tianshui Chen,Menghan Xia,Ping Luo,Ying Shan</p>
<p><a href='http://arxiv.org/abs/2312.03641v1'>http://arxiv.org/abs/2312.03641v1</a></p>
<p><b>Compressor summary</b>: The paper introduces MotionCtrl, a novel motion controller for video generation that independently controls camera and object motion, enabling more fine-grained control and diverse combinations of motions.</p><hr><h3>Not All Large Language Models (LLMs) Succumb to the "Reversal Curse": A  Comparative Study of Deductive Logical Reasoning in BERT and GPT Models</h3>
<p>Jingye Yang,Da Wu,Kai Wang</p>
<p><a href='http://arxiv.org/abs/2312.03633v1'>http://arxiv.org/abs/2312.03633v1</a></p>
<p><b>Compressor summary</b>: The study found that while bidirectional LLM BERT can avoid the reversal curse, both encoder and decoder models struggle with logical reasoning involving three sets.</p><hr><h3>MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations</h3>
<p>Assaf Ben-Kish,Moran Yanuka,Morris Alper,Raja Giryes,Hadar Averbuch-Elor</p>
<p><a href='http://arxiv.org/abs/2312.03631v1'>http://arxiv.org/abs/2312.03631v1</a></p>
<p><b>Compressor summary</b>: MOCHa uses reinforcement learning to reduce hallucinations and improve caption quality in image captioning without strong supervision, and introduces OpenCHAIR, a new benchmark for evaluating open-vocabulary hallucinations.</p><hr><h3>Boosting Segment Anything Model Towards Open-Vocabulary Learning</h3>
<p>Xumeng Han,Longhui Wei,Xuehui Yu,Zhiyang Dou,Xin He,Kuiran Wang,Zhenjun Han,Qi Tian</p>
<p><a href='http://arxiv.org/abs/2312.03628v1'>http://arxiv.org/abs/2312.03628v1</a></p>
<p><b>Compressor summary</b>: Sambor is a new model that improves SAM by adding the ability to detect objects based on human inputs and category names, using a novel module and an open-set region proposal network.</p><hr><h3>TokenCompose: Grounding Diffusion with Token-level Supervision</h3>
<p>Zirui Wang,Zhizhou Sha,Zheng Ding,Yilin Wang,Zhuowen Tu</p>
<p><a href='http://arxiv.org/abs/2312.03626v1'>http://arxiv.org/abs/2312.03626v1</a></p>
<p><b>Compressor summary</b>: TokenCompose is a Latent Diffusion Model that improves text-to-image generation by introducing token-wise consistency terms between image content and object segmentation maps during finetuning, achieving better multi-category instance composition and photorealism.</p><hr><h3>Physical Symbolic Optimization</h3>
<p>Wassim Tenachi,Rodrigo Ibata,Foivos I. Diakogiannis</p>
<p><a href='http://arxiv.org/abs/2312.03612v1'>http://arxiv.org/abs/2312.03612v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a method that uses reinforcement learning to generate equations with physical units, achieving better results than other methods in noisy conditions.</p><hr><h3>DreamComposer: Controllable 3D Object Generation via Multi-View  Conditions</h3>
<p>Yunhan Yang,Yukun Huang,Xiaoyang Wu,Yuan-Chen Guo,Song-Hai Zhang,Hengshuang Zhao,Tong He,Xihui Liu</p>
<p><a href='http://arxiv.org/abs/2312.03611v1'>http://arxiv.org/abs/2312.03611v1</a></p>
<p><b>Compressor summary</b>: The paper introduces DreamComposer, a framework that improves existing view-aware diffusion models by using multiple views of an object to generate high-quality novel views for 3D object reconstruction and other tasks.</p><hr><h3>Automated Multimodal Data Annotation via Calibration With Indoor  Positioning System</h3>
<p>Ryan Rubel,Andrew Dudash,Mohammad Goli,James O'Hara,Karl Wunderlich</p>
<p><a href='http://arxiv.org/abs/2312.03608v1'>http://arxiv.org/abs/2312.03608v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to automatically label LiDAR and camera data for object detection in indoor settings using an IPS, which is much faster than manual annotation.</p><hr><h3>DiffusionSat: A Generative Foundation Model for Satellite Imagery</h3>
<p>Samar Khanna,Patrick Liu,Linqi Zhou,Chenlin Meng,Robin Rombach,Marshall Burke,David Lobell,Stefano Ermon</p>
<p><a href='http://arxiv.org/abs/2312.03606v1'>http://arxiv.org/abs/2312.03606v1</a></p>
<p><b>Compressor summary</b>: The paper introduces DiffusionSat, a large generative model for satellite images that uses metadata and diffusion techniques to generate realistic samples and solve various tasks.</p><hr><h3>MMM: Generative Masked Motion Model</h3>
<p>Ekkasit Pinyoanuntapong,Pu Wang,Minwoo Lee,Chen Chen</p>
<p><a href='http://arxiv.org/abs/2312.03596v1'>http://arxiv.org/abs/2312.03596v1</a></p>
<p><b>Compressor summary</b>: MMM is a novel motion generation method that uses a tokenizer and a transformer to capture dependencies between motion and text tokens, allowing for high-fidelity, high-speed, and editable motion generation.</p><hr><h3>A Task is Worth One Word: Learning with Task Prompts for High-Quality  Versatile Image Inpainting</h3>
<p>Junhao Zhuang,Yanhong Zeng,Wenran Liu,Chun Yuan,Kai Chen</p>
<p><a href='http://arxiv.org/abs/2312.03594v1'>http://arxiv.org/abs/2312.03594v1</a></p>
<p><b>Compressor summary</b>: PowerPaint is a model that excels at context-aware image inpainting and text-guided object inpainting by using learnable task prompts and tailored fine-tuning strategies.</p><hr><h3>Language-Informed Visual Concept Learning</h3>
<p>Sharon Lee,Yunzhi Zhang,Shangzhe Wu,Jiajun Wu</p>
<p><a href='http://arxiv.org/abs/2312.03587v1'>http://arxiv.org/abs/2312.03587v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses learning a language-informed visual concept representation from large pre-trained vision-language models and using it to generate images with novel compositions of visual concepts.</p><hr><h3>Foundation Model Assisted Weakly Supervised Semantic Segmentation</h3>
<p>Xiaobo Yang,Xiaojin Gong</p>
<p><a href='http://arxiv.org/abs/2312.03585v1'>http://arxiv.org/abs/2312.03585v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework using pre-trained models CLIP and SAM to generate segmentation seeds for weakly supervised semantic segmentation, achieving state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014.</p><hr><h3>Context Diffusion: In-Context Aware Image Generation</h3>
<p>Ivona Najdenkoska,Animesh Sinha,Abhimanyu Dubey,Dhruv Mahajan,Vignesh Ramanathan,Filip Radenovic</p>
<p><a href='http://arxiv.org/abs/2312.03584v1'>http://arxiv.org/abs/2312.03584v1</a></p>
<p><b>Compressor summary</b>: Context Diffusion is a framework for generating images from contextual examples and text prompts, improving image quality and adaptability.</p><hr><h3>Improving Bias Mitigation through Bias Experts in Natural Language  Understanding</h3>
<p>Eojin Jeon,Mingyu Lee,Juhyeong Park,Yeachan Kim,Wing-Lam Mok,SangKeun Lee</p>
<p><a href='http://arxiv.org/abs/2312.03577v1'>http://arxiv.org/abs/2312.03577v1</a></p>
<p><b>Compressor summary</b>: The text discusses a new debiasing framework for models that uses binary classifiers called bias experts to improve bias identification and mitigate its negative effects on performance.</p><hr><h3>DocBinFormer: A Two-Level Transformer Network for Effective Document  Image Binarization</h3>
<p>Risab Biswas,Swalpa Kumar Roy,Ning Wang,Umapada Pal,Guang-Bin Huang</p>
<p><a href='http://arxiv.org/abs/2312.03568v1'>http://arxiv.org/abs/2312.03568v1</a></p>
<p><b>Compressor summary</b>: The DocBinFormer is a new transformer-based architecture for effective document image binarization that captures global and local features using two-level vision transformers, outperforming existing methods on several benchmarks.</p><hr><h3>XAIQA: Explainer-Based Data Augmentation for Extractive Question  Answering</h3>
<p>Joel Stremmel,Ardavan Saeedi,Hamid Hassanzadeh,Sanjit Batra,Jeffrey Hertzberg,Jaime Murillo,Eran Halperin</p>
<p><a href='http://arxiv.org/abs/2312.03567v1'>http://arxiv.org/abs/2312.03567v1</a></p>
<p><b>Compressor summary</b>: XAIQA is a novel method that generates synthetic QA pairs from electronic health records data for extractive QA systems, outperforming existing approaches in semantic matches and clinical abbreviations, and improving GPT-4's performance on difficult questions.</p><hr><h3>Enhancing Kinship Verification through Multiscale Retinex and Combined  Deep-Shallow features</h3>
<p>El Ouanas Belabbaci,Mohammed Khammari,Ammar Chouchane,Mohcene Bessaoudi,Abdelmalik Ouamane,Yassine Himeur,Shadi Atalla,Wathiq Mansoor</p>
<p><a href='http://arxiv.org/abs/2312.03562v1'>http://arxiv.org/abs/2312.03562v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new method for verifying family relationships from facial images using Multiscale Retinex, deep and shallow texture descriptors, and Logistic Regression, achieving promising results on three kinship datasets.</p><hr><h3>When an Image is Worth 1,024 x 1,024 Words: A Case Study in  Computational Pathology</h3>
<p>Wenhui Wang,Shuming Ma,Hanwen Xu,Naoto Usuyama,Jiayu Ding,Hoifung Poon,Furu Wei</p>
<p><a href='http://arxiv.org/abs/2312.03558v1'>http://arxiv.org/abs/2312.03558v1</a></p>
<p><b>Compressor summary</b>: Key points:
- LongViT is a vision Transformer for gigapixel images
- It splits the image into millions of patches and uses LongNet to model them
- It can handle computation and memory constraints
- It is applied in computational pathology for cancer diagnosis and prognosis
- It outperforms previous methods

Summary:
LongViT is a new vision Transformer that can process gigapixel images in a fast and efficient way, enabling better cancer diagnosis and prognosis in computational pathology.</p><hr><h3>Personalized Face Inpainting with Diffusion Models by Parallel Visual  Attention</h3>
<p>Jianjin Xu,Saman Motamed,Praneetha Vaddamanu,Chen Henry Wu,Christian Haene,Jean-Charles Bazin,Fernando de la Torre</p>
<p><a href='http://arxiv.org/abs/2312.03556v1'>http://arxiv.org/abs/2312.03556v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method called Parallel Visual Attention (PVA) that uses attention modules and an identity encoder to improve face inpainting results, preserving identity and semantic attributes, and reducing computational complexity compared to existing techniques.</p><hr><h3>Holmes: Towards Distributed Training Across Clusters with Heterogeneous  NIC Environment</h3>
<p>Fei Yang,Shuang Peng,Ning Sun,Fangyu Wang,Ke Tan,Fu Wu,Jiezhong Qiu,Aimin Pan</p>
<p><a href='http://arxiv.org/abs/2312.03549v1'>http://arxiv.org/abs/2312.03549v1</a></p>
<p><b>Compressor summary</b>: Holmes is a novel LLM training framework for heterogeneous NIC environments that uses data and model parallelism strategies, intelligent tasklet scheduling, and pipeline parallel techniques to achieve high training efficiency.</p><hr><h3>Texture-Semantic Collaboration Network for ORSI Salient Object Detection</h3>
<p>Gongyang Li,Zhen Bai,Zhi Liu</p>
<p><a href='http://arxiv.org/abs/2312.03548v1'>http://arxiv.org/abs/2312.03548v1</a></p>
<p><b>Compressor summary</b>: The Texture-Semantic Collaboration Network (TSCNet) is a novel approach for salient object detection in optical remote sensing images that leverages both texture and semantic cues to address the challenges of multiple, small, low-illumination, and irregularly shaped objects.</p><hr><h3>GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging  Cross-Modal Attention with Large Language Models</h3>
<p>Haicheng Liao,Huanming Shen,Zhenning Li,Chengyue Wang,Guofa Li,Yiming Bie,Chengzhong Xu</p>
<p><a href='http://arxiv.org/abs/2312.03543v1'>http://arxiv.org/abs/2312.03543v1</a></p>
<p><b>Compressor summary</b>: The paper presents a CAVG model that uses multiple encoders and LLMs to improve visual grounding in autonomous vehicles, achieving high accuracy and efficiency in various scenarios.</p><hr><h3>FoodFusion: A Latent Diffusion Model for Realistic Food Image Generation</h3>
<p>Olivia Markham,Yuhao Chen,Chi-en Amy Tai,Alexander Wong</p>
<p><a href='http://arxiv.org/abs/2312.03540v1'>http://arxiv.org/abs/2312.03540v1</a></p>
<p><b>Compressor summary</b>: FoodFusion is a Latent Diffusion model that generates realistic and diverse food images from textual descriptions using a large curated dataset and data cleaning methods.</p><hr><h3>Low-shot Object Learning with Mutual Exclusivity Bias</h3>
<p>Anh Thai,Ahmad Humayun,Stefan Stojanov,Zixuan Huang,Bikram Boote,James M. Rehg</p>
<p><a href='http://arxiv.org/abs/2312.03533v1'>http://arxiv.org/abs/2312.03533v1</a></p>
<p><b>Compressor summary</b>: The paper proposes LSME, a new object learning task based on mutual exclusivity bias, and presents a dataset, baselines, and a top-performing method for it.</p><hr><h3>Personalized Pose Forecasting</h3>
<p>Maria Priisalu,Ted Kronvall,Cristian Sminchisescu</p>
<p><a href='http://arxiv.org/abs/2312.03528v1'>http://arxiv.org/abs/2312.03528v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new way to adapt human motion prediction models to individual movement patterns, which is important for systems like delivery robots that interact with the same person over time.</p><hr><h3>On the Diversity and Realism of Distilled Dataset: An Efficient Dataset  Distillation Paradigm</h3>
<p>Peng Sun,Bei Shi,Daiwei Yu,Tao Lin</p>
<p><a href='http://arxiv.org/abs/2312.03526v1'>http://arxiv.org/abs/2312.03526v1</a></p>
<p><b>Compressor summary</b>: The authors propose RDED, a new data distillation method that addresses the challenges of large-scale and high-resolution datasets by focusing on realism, diversity, and efficiency.</p><hr><h3>Sig-Networks Toolkit: Signature Networks for Longitudinal Language  Modelling</h3>
<p>Talia Tseriotou,Ryan Sze-Yin Chan,Adam Tsakalidis,Iman Munire Bilal,Elena Kochkina,Terry Lyons,Maria Liakata</p>
<p><a href='http://arxiv.org/abs/2312.03523v1'>http://arxiv.org/abs/2312.03523v1</a></p>
<p><b>Compressor summary</b>: Sig-Networks is a new open-source toolkit that uses Signature-based Neural Network models to perform well in temporal NLP tasks like counselling conversations, rumour stance switch and mood changes in social media threads.</p><hr><h3>Active Wildfires Detection and Dynamic Escape Routes Planning for Humans  through Information Fusion between Drones and Satellites</h3>
<p>Chang Liu,Tamas Sziranyi</p>
<p><a href='http://arxiv.org/abs/2312.03519v1'>http://arxiv.org/abs/2312.03519v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using UAV vision and satellite image analysis for detecting wildfires, extracting road networks, and planning dynamic escape routes for people in distress during wilderness rescues.</p><hr><h3>FRDiff: Feature Reuse for Exquisite Zero-shot Acceleration of Diffusion  Models</h3>
<p>Junhyuk So,Jungwon Lee,Eunhyeok Park</p>
<p><a href='http://arxiv.org/abs/2312.03517v1'>http://arxiv.org/abs/2312.03517v1</a></p>
<p><b>Compressor summary</b>: The paper introduces FRDiff, a technique that uses feature reuse and reduced score function evaluations to speed up diffusion models without compromising quality.</p><hr><h3>Kandinsky 3.0 Technical Report</h3>
<p>Vladimir Arkhipkin,Andrei Filatov,Viacheslav Vasilev,Anastasia Maltseva,Said Azizov,Igor Pavlov,Julia Agafonova,Andrey Kuznetsov,Denis Dimitrov</p>
<p><a href='http://arxiv.org/abs/2312.03511v1'>http://arxiv.org/abs/2312.03511v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Kandinsky 3.0, an improved text-to-image generation model with a larger backbone, encoder, and no diffusion mapping, which enhances quality and domain adaptability.</p><hr><h3>Towards Sobolev Training</h3>
<p>Neil Kichler,Sher Afghan,Uwe Naumann</p>
<p><a href='http://arxiv.org/abs/2312.03510v1'>http://arxiv.org/abs/2312.03510v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to create accurate and efficient surrogate models for complex phenomena by using sensitivity information during learning and pruning, which can be applied beyond quantitative finance.</p><hr><h3>Gravitational cell detection and tracking in fluorescence microscopy  data</h3>
<p>Nikomidisz Eftimiu,Michal Kozubek</p>
<p><a href='http://arxiv.org/abs/2312.03509v1'>http://arxiv.org/abs/2312.03509v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new computer vision technique using gravitational force fields for detecting, segmenting, and tracking cells in fluorescence microscopy images, which can be faster and more explainable than machine learning methods.</p><hr><h3>Improving the Generalization of Segmentation Foundation Model under  Distribution Shift via Weakly Supervised Adaptation</h3>
<p>Haojie Zhang,Yongyi Su,Xun Xu,Kui Jia</p>
<p><a href='http://arxiv.org/abs/2312.03502v1'>http://arxiv.org/abs/2312.03502v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses a new self-training strategy to improve the image segmentation model SAM's robustness and efficiency under different distribution shifts, outperforming existing methods.</p><hr><h3>Speculative Exploration on the Concept of Artificial Agents Conducting  Autonomous Research</h3>
<p>Shiro Takagi</p>
<p><a href='http://arxiv.org/abs/2312.03497v1'>http://arxiv.org/abs/2312.03497v1</a></p>
<p><b>Compressor summary</b>: The paper explores the concept of artificial agents capable of conducting research, discussing their core components and challenges, and suggesting prototyping as a first step to overcome them.</p><hr><h3>Learning From Scenarios for Stochastic Repairable Scheduling</h3>
<p>Kim van den Houten,David M. J. Tax,Esteban Freydell,Mathijs de Weerdt</p>
<p><a href='http://arxiv.org/abs/2312.03492v1'>http://arxiv.org/abs/2312.03492v1</a></p>
<p><b>Compressor summary</b>: Decision-focused learning adapts to stochastic scheduling problems with uncertain processing times by using historical realizations and outperforms existing methods.</p><hr><h3>Exploring Answer Information Methods for Question Generation with  Transformers</h3>
<p>Talha Chafekar,Aafiya Hussain,Grishma Sharma,Deepak Sharma</p>
<p><a href='http://arxiv.org/abs/2312.03483v1'>http://arxiv.org/abs/2312.03483v1</a></p>
<p><b>Compressor summary</b>: The authors experiment with different methods to incorporate target answers into question generation for RNN models and find that answer prompting without additional modes performs best.</p><hr><h3>AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing  Evaluation Suite</h3>
<p>Jonas Groschwitz,Shay B. Cohen,Lucia Donatelli,Meaghan Fowlie</p>
<p><a href='http://arxiv.org/abs/2312.03480v1'>http://arxiv.org/abs/2312.03480v1</a></p>
<p><b>Compressor summary</b>: GrAPES is a challenge set that tests AMR parsers on various aspects of sentence meaning, revealing their strengths and weaknesses.</p><hr><h3>Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D  Diffusion</h3>
<p>Weitao Du,Jiujiu Chen,Xuecang Zhang,Zhiming Ma,Shengchao Liu</p>
<p><a href='http://arxiv.org/abs/2312.03475v1'>http://arxiv.org/abs/2312.03475v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new method called MoleculeJAE that can learn the geometry and topology of molecules using self-supervised learning, improving drug discovery with better geometrical representation.</p><hr><h3>Search Strategies for Self-driving Laboratories with Pending Experiments</h3>
<p>Hao Wen,Jakob Zeitler,Connor Rupnow</p>
<p><a href='http://arxiv.org/abs/2312.03466v1'>http://arxiv.org/abs/2312.03466v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses optimizing Bayesian search strategies for self-driving laboratories with asynchronous parallel experiments and delayed feedback.</p><hr><h3>Subnetwork-to-go: Elastic Neural Network with Dynamic Training and  Customizable Inference</h3>
<p>Kai Li,Yi Luo</p>
<p><a href='http://arxiv.org/abs/2312.03464v1'>http://arxiv.org/abs/2312.03464v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new way to train a large neural network and extract smaller subnetworks from it during inference based on size or complexity constraints, which improves performance and reduces training time compared to training separate subnetworks from scratch.</p><hr><h3>DBCopilot: Scaling Natural Language Querying to Massive Databases</h3>
<p>Tianshu Wang,Hongyu Lin,Xianpei Han,Le Sun,Xiaoyang Chen,Hao Wang,Zhenyu Zeng</p>
<p><a href='http://arxiv.org/abs/2312.03463v1'>http://arxiv.org/abs/2312.03463v1</a></p>
<p><b>Compressor summary</b>: DBCopilot is a framework that simplifies database interactions by routing natural language questions through massive databases using a compact neural network router and leveraging large language models for SQL generation.</p><hr><h3>HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian  Splatting</h3>
<p>Yuheng Jiang,Zhehao Shen,Penghao Wang,Zhuo Su,Yu Hong,Yingliang Zhang,Jingyi Yu,Lan Xu</p>
<p><a href='http://arxiv.org/abs/2312.03461v1'>http://arxiv.org/abs/2312.03461v1</a></p>
<p><b>Compressor summary</b>: HiFi4G is a technique that uses 3D Gaussians to render realistic human performance from dense footage, enabling efficient compression and non-rigid tracking.</p><hr><h3>F3-Pruning: A Training-Free and Generalized Pruning Strategy towards  Faster and Finer Text-to-Video Synthesis</h3>
<p>Sitong Su,Jianzhi Liu,Lianli Gao,Jingkuan Song</p>
<p><a href='http://arxiv.org/abs/2312.03459v1'>http://arxiv.org/abs/2312.03459v1</a></p>
<p><b>Compressor summary</b>: The authors propose F3-Pruning, a training-free and generalized pruning strategy for inferencing large T2V models faster without losing quality.</p><hr><h3>Think from Words(TFW): Initiating Human-Like Cognition in Large Language  Models Through Think from Words for Japanese Text-level Classification</h3>
<p>Chengguang Gan,Qinghao Zhang,Tatsunori Mori</p>
<p><a href='http://arxiv.org/abs/2312.03458v1'>http://arxiv.org/abs/2312.03458v1</a></p>
<p><b>Compressor summary</b>: The study introduces "Think from Words" (TFW) and "TFW with Extra word-level information" (TFW Extra), two methods that aim to improve Large Language Models' (LLMs) text comprehension by starting at the word level and using additional word-level data, and evaluates their effectiveness on six Japanese datasets.</p><hr><h3>Data-driven Crop Growth Simulation on Time-varying Generated Images  using Multi-conditional Generative Adversarial Networks</h3>
<p>Lukas Drees,Dereje T. Demie,Madhuri R. Paul,Johannes Leonhardt,Sabine J. Seidel,Thomas F. Döring,Ribana Roscher</p>
<p><a href='http://arxiv.org/abs/2312.03443v1'>http://arxiv.org/abs/2312.03443v1</a></p>
<p><b>Compressor summary</b>: The paper presents a two-stage framework for realistic image prediction and plant phenotyping using conditional Wasserstein generative adversarial networks, which can integrate multiple growth-influencing conditions and help precision agriculture by revealing spatial crop development over time.</p><hr><h3>High-Quality Facial Geometry and Appearance Capture at Home</h3>
<p>Yuxuan Han,Junfeng Lyu,Feng Xu</p>
<p><a href='http://arxiv.org/abs/2312.03442v1'>http://arxiv.org/abs/2312.03442v1</a></p>
<p><b>Compressor summary</b>: This paper presents a new, easy-to-use method for capturing high-quality 3D face scans with skin, hair, eyes, and mouth interior using a single smartphone flashlight sequence in a dim room.</p><hr><h3>UFineBench: Towards Text-based Person Retrieval with Ultra-fine  Granularity</h3>
<p>Jialong Zuo,Hanyu Zhou,Ying Nie,Feng Zhang,Tianyu Guo,Nong Sang,Yunhe Wang,Changxin Gao</p>
<p><a href='http://arxiv.org/abs/2312.03441v1'>http://arxiv.org/abs/2312.03441v1</a></p>
<p><b>Compressor summary</b>: The paper introduces UFineBench, a new benchmark for text-based person retrieval with ultra-fine granularity, and presents a new dataset (UFine6926), an evaluation paradigm (UFine3C), and an efficient algorithm (CFAM) to address the problem of coarse-grained annotations.</p><hr><h3>Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle</h3>
<p>Youtian Lin,Zuozhuo Dai,Siyu Zhu,Yao Yao</p>
<p><a href='http://arxiv.org/abs/2312.03431v1'>http://arxiv.org/abs/2312.03431v1</a></p>
<p><b>Compressor summary</b>: Gaussian-Flow is a fast point-based approach for dynamic scene reconstruction and real-time rendering from videos using a novel Dual-Domain Deformation Model.</p><hr><h3>ShareCMP: Polarization-Aware RGB-P Semantic Segmentation</h3>
<p>Zhuoyan Liu,Bo Wang,Lizhi Wang,Chenyu Mao,Ye Li</p>
<p><a href='http://arxiv.org/abs/2312.03430v1'>http://arxiv.org/abs/2312.03430v1</a></p>
<p><b>Compressor summary</b>: The ShareCMP framework improves RGB-Polarization semantic segmentation for underwater scenarios with less parameters and better performance.</p><hr><h3>Artist-Friendly Relightable and Animatable Neural Heads</h3>
<p>Yingyan Xu,Prashanth Chandran,Sebastian Weiss,Markus Gross,Gaspard Zoss,Derek Bradley</p>
<p><a href='http://arxiv.org/abs/2312.03420v1'>http://arxiv.org/abs/2312.03420v1</a></p>
<p><b>Compressor summary</b>: The text describes a new method to create realistic and animatable digital heads that can be relit in any environment and perform various expressions.</p><hr><h3>Run LoRA Run: Faster and Lighter LoRA Implementations</h3>
<p>Daria Cherniuk,Aleksandr Mikhalev,Ivan Oseledets</p>
<p><a href='http://arxiv.org/abs/2312.03415v1'>http://arxiv.org/abs/2312.03415v1</a></p>
<p><b>Compressor summary</b>: LoRA is a technique that speeds up neural network training by using low-rank adapters, and the RunLoRA framework optimizes this technique for efficiency.</p><hr><h3>Compressed Context Memory For Online Language Model Interaction</h3>
<p>Jang-Hyun Kim,Junyoung Yeom,Sangdoo Yun,Hyun Oh Song</p>
<p><a href='http://arxiv.org/abs/2312.03414v1'>http://arxiv.org/abs/2312.03414v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a method to compress and store context for Transformer language models in online scenarios, reducing memory and computation while maintaining performance.</p><hr><h3>Approximating Solutions to the Knapsack Problem using the Lagrangian  Dual Framework</h3>
<p>Mitchell Keegan,Mahdi Abolghasemi</p>
<p><a href='http://arxiv.org/abs/2312.03413v1'>http://arxiv.org/abs/2312.03413v1</a></p>
<p><b>Compressor summary</b>: The paper proposes neural network models that use the Lagrangian Dual Framework to approximate Knapsack Problem solutions, improving constraint satisfaction while maintaining strong optimization performance.</p><hr><h3>DeepPyramid+: Medical Image Segmentation using Pyramid View Fusion and  Deformable Pyramid Reception</h3>
<p>Negin Ghamsarian,Sebastian Wolf,Martin Zinkernagel,Klaus Schoeffmann,Raphael Sznitman</p>
<p><a href='http://arxiv.org/abs/2312.03409v1'>http://arxiv.org/abs/2312.03409v1</a></p>
<p><b>Compressor summary</b>: DeepPyramid+ is a neural network architecture that tackles various challenges in medical image and surgical video segmentation using Pyramid View Fusion and Deformable Pyramid Reception modules.</p><hr><h3>Open-sourced Data Ecosystem in Autonomous Driving: the Present and  Future</h3>
<p>Hongyang Li,Yang Li,Huijie Wang,Jia Zeng,Pinlong Cai,Huilin Xu,Dahua Lin,Junchi Yan,Feng Xu,Lu Xiong,Jingdong Wang,Futang Zhu,Kai Yan,Chunjing Xu,Tiancai Wang,Beipeng Mu,Shaoqing Ren,Zhihui Peng,Yu Qiao</p>
<p><a href='http://arxiv.org/abs/2312.03408v1'>http://arxiv.org/abs/2312.03408v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses a comprehensive review of over seventy open-source autonomous driving datasets, assessing their characteristics and challenges for the evolution of the industry ecosystem.</p><hr><h3>SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting</h3>
<p>Chao Chen,Tian Zhou,Yanjun Zhao,Hui Liu,Liang Sun,Rong Jin</p>
<p><a href='http://arxiv.org/abs/2312.03406v1'>http://arxiv.org/abs/2312.03406v1</a></p>
<p><b>Compressor summary</b>: SVQ is a sparse vector quantization method that improves spatiotemporal forecasting tasks by balancing details and noise reduction using a two-layer MLP and a randomly fixed or learnable matrix, achieving state-of-the-art results in various fields.</p><hr><h3>Generalized Contrastive Divergence: Joint Training of Energy-Based Model  and Diffusion Model through Inverse Reinforcement Learning</h3>
<p>Sangwoong Yoon,Dohyun Kwon,Himchan Hwang,Yung-Kyun Noh,Frank C. Park</p>
<p><a href='http://arxiv.org/abs/2312.03397v1'>http://arxiv.org/abs/2312.03397v1</a></p>
<p><b>Compressor summary</b>: GCD trains an energy-based model and a sampler together, generalizing Contrastive Divergence by using a trainable sampler instead of MCMC, and can improve both models' performance.</p><hr><h3>Action Scene Graphs for Long-Form Understanding of Egocentric Videos</h3>
<p>Ivan Rodin,Antonino Furnari,Kyle Min,Subarna Tripathi,Giovanni Maria Farinella</p>
<p><a href='http://arxiv.org/abs/2312.03391v1'>http://arxiv.org/abs/2312.03391v1</a></p>
<p><b>Compressor summary</b>: Egocentric Action Scene Graphs (EASGs) are a new way to understand long egocentric videos, using graphs to describe actions, objects, and relationships over time.</p><hr><h3>An Infinite-Width Analysis on the Jacobian-Regularised Training of a  Neural Network</h3>
<p>Taeyoung Kim,Hongseok Yang</p>
<p><a href='http://arxiv.org/abs/2312.03386v1'>http://arxiv.org/abs/2312.03386v1</a></p>
<p><b>Compressor summary</b>: The paper extends infinite-width analysis to the Jacobian of deep neural networks, showing that MLPs and their Jacobians converge to Gaussian processes in the infinite-width limit.</p><hr><h3>A Text-to-Text Model for Multilingual Offensive Language Identification</h3>
<p>Tharindu Ranasinghe,Marcos Zampieri</p>
<p><a href='http://arxiv.org/abs/2312.03379v1'>http://arxiv.org/abs/2312.03379v1</a></p>
<p><b>Compressor summary</b>: The paper introduces pre-trained encoder-decoder models for offensive language detection that outperform existing transformer-based models and achieve state-of-the-art results in multiple languages.</p><hr><h3>Riemannian Complex Matrix Convolution Network for PolSAR Image  Classification</h3>
<p>Junfei Shi,Wei Wang,Haiyan Jin,Mengmeng Nie,Shanshan Ji</p>
<p><a href='http://arxiv.org/abs/2312.03378v1'>http://arxiv.org/abs/2312.03378v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new deep learning method for PolSAR image classification that directly uses the complex matrix as input, learns its structure in Riemannian space, and improves performance over existing methods.</p><hr><h3>Evaluating the point cloud of individual trees generated from images  based on Neural Radiance fields (NeRF) method</h3>
<p>Hongyu Huang,Guoji Tian,Chongcheng Chen</p>
<p><a href='http://arxiv.org/abs/2312.03372v1'>http://arxiv.org/abs/2312.03372v1</a></p>
<p><b>Compressor summary</b>: The study uses Neural Radiance Fields (NeRF) to reconstruct three-dimensional trees from two-dimensional images, showing its efficiency and adaptability but with lower resolution and accuracy compared to photogrammetric methods.</p><hr><h3>Lazy-k: Decoding for Constrained Token Classification</h3>
<p>Arthur Hemmer,Mickaël Coustaty,Nicola Bartolo,Jérôme Brachat,Jean-Marc Ogier</p>
<p><a href='http://arxiv.org/abs/2312.03367v1'>http://arxiv.org/abs/2312.03367v1</a></p>
<p><b>Compressor summary</b>: The authors study how to improve probabilistic models for information extraction by combining them with constrained decoding methods, proposing a new method called Lazy-$k$, and showing its benefits over existing approaches.</p><hr><h3>KhabarChin: Automatic Detection of Important News in the Persian  Language</h3>
<p>Hamed Hematian Hemati,Arash Lagzian,Moein Salimi Sartakhti,Hamid Beigy,Ehsaneddin Asgari</p>
<p><a href='http://arxiv.org/abs/2312.03361v1'>http://arxiv.org/abs/2312.03361v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Khabarchin, a new dataset for detecting important news in Persian language, and proposes learning-based models to tackle this task.</p><hr><h3>Teaching Specific Scientific Knowledge into Large Language Models  through Additional Training</h3>
<p>Kan Hatakeyama-Sato,Yasuhiko Igarashi,Shun Katakami,Yuta Nabae,Teruaki Hayakawa</p>
<p><a href='http://arxiv.org/abs/2312.03360v1'>http://arxiv.org/abs/2312.03360v1</a></p>
<p><b>Compressor summary</b>: The paragraph discusses using additional training to embed specialized scientific knowledge into a large language model, addressing challenges such as text scarcity and hyperparameter optimization.</p><hr><h3>RING-NeRF: A Versatile Architecture based on Residual Implicit Neural  Grids</h3>
<p>Doriand Petit,Steve Bourgeois,Dumitru Pavel,Vincent Gay-Bellile,Florian Chabot,Loic Barthe</p>
<p><a href='http://arxiv.org/abs/2312.03357v1'>http://arxiv.org/abs/2312.03357v1</a></p>
<p><b>Compressor summary</b>: The RING-NeRF architecture uses Residual Implicit Neural Grids to control the level of detail and achieve fast training and state-of-the-art performance in 3D reconstruction and new view synthesis tasks.</p><hr><h3>PointMoment:Mixed-Moment-based Self-Supervised Representation Learning  for 3D Point Clouds</h3>
<p>Xin Cao,Xinxin Han,Yifan Wang,Mengna Yang,Kang Li</p>
<p><a href='http://arxiv.org/abs/2312.03350v1'>http://arxiv.org/abs/2312.03350v1</a></p>
<p><b>Compressor summary</b>: PointMoment is a novel self-supervised representation learning framework for point clouds that uses a high-order mixed moment loss function to reduce feature redundancy and improve downstream tasks such as 3D point cloud classification and segmentation.</p><hr><h3>Interpretable Mechanistic Representations for Meal-level Glycemic  Control in the Wild</h3>
<p>Ke Alexander Wang,Emily B. Fox</p>
<p><a href='http://arxiv.org/abs/2312.03344v1'>http://arxiv.org/abs/2312.03344v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Paper proposes hybrid variational autoencoder to learn interpretable representations of CGM and meal data for diabetes
- Latent space grounded to mechanistic differential equation inputs, reflecting physiological quantities
- Novel method to infer glucose appearance rate from unreliable meal logs
- Unsupervised representation discovers separation between individuals proportional to disease severity
- Embeddings produce clusters better than other features

Summary:
The paper presents a new method to learn interpretable and accurate representations of CGM and meal data for diabetes using a hybrid variational autoencoder that connects latent space to physiological quantities and infers glucose appearance rate. The method reveals disease severity and outperforms other features.</p><hr><h3>Topic and genre in dialogue</h3>
<p>Amandine Decker,Ellen Breitholtz,Christine Howes,Staffan Larsson</p>
<p><a href='http://arxiv.org/abs/2312.03342v1'>http://arxiv.org/abs/2312.03342v1</a></p>
<p><b>Compressor summary</b>: The paper proposes separating and defining genre and topic concepts to improve dialogue system flexibility and reliability.</p><hr><h3>Online Vectorized HD Map Construction using Geometry</h3>
<p>Zhixin Zhang,Yiyuan Zhang,Xiaohan Ding,Fusheng Jin,Xiangyu Yue</p>
<p><a href='http://arxiv.org/abs/2312.03341v1'>http://arxiv.org/abs/2312.03341v1</a></p>
<p><b>Compressor summary</b>: GeMap is a method that learns Euclidean shapes and relations of map instances beyond basic perception, achieving state-of-the-art performance on two datasets.</p><hr><h3>PointJEM: Self-supervised Point Cloud Understanding for Reducing Feature  Redundancy via Joint Entropy Maximization</h3>
<p>Xin Cao,Huan Xia,Xinxin Han,Yifan Wang,Kang Li,Linzhi Su</p>
<p><a href='http://arxiv.org/abs/2312.03339v1'>http://arxiv.org/abs/2312.03339v1</a></p>
<p><b>Compressor summary</b>: PointJEM is a self-supervised point cloud representation learning method that reduces feature redundancy using joint entropy and performs well in downstream tasks.</p><hr><h3>Measuring Misogyny in Natural Language Generation: Preliminary Results  from a Case Study on two Reddit Communities</h3>
<p>Aaron J. Snoswell,Lucinda Nelson,Hao Xue,Flora D. Salim,Nicolas Suzor,Jean Burgess</p>
<p><a href='http://arxiv.org/abs/2312.03330v1'>http://arxiv.org/abs/2312.03330v1</a></p>
<p><b>Compressor summary</b>: The paper argues that generic toxicity classifiers are not suitable for measuring misogyny in natural language generation and proposes using a misogyny-specific lexicon instead.</p><hr><h3>Building Category Graphs Representation with Spatial and Temporal  Attention for Visual Navigation</h3>
<p>Xiaobo Hu,Youfang Lin,HeHe Fan,Shuo Wang,Zhihao Wu,Kai Lv</p>
<p><a href='http://arxiv.org/abs/2312.03327v1'>http://arxiv.org/abs/2312.03327v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a Category Relation Graph (CRG) to learn object layout knowledge and a Temporal-Spatial-Region (TSR) attention architecture to capture object dependencies for visual navigation.</p><hr><h3>GCFA:Geodesic Curve Feature Augmentation via Shape Space Theory</h3>
<p>Yuexing Han,Guanxin Wan,Bing Wang</p>
<p><a href='http://arxiv.org/abs/2312.03325v1'>http://arxiv.org/abs/2312.03325v1</a></p>
<p><b>Compressor summary</b>: The authors propose Geodesic curve feature augmentation (GCFA), a method that projects image features into a shape space and generates new features along a geodesic curve, improving data preprocessing for deep learning models in small sample environments.</p><hr><h3>Background Clustering Pre-training for Few-shot Segmentation</h3>
<p>Zhimiao Yu,Tiancheng Lin,Yi Xu</p>
<p><a href='http://arxiv.org/abs/2312.03322v1'>http://arxiv.org/abs/2312.03322v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new pre-training method for few-shot segmentation called Background Clustering Pre-Training, which separates novel classes from the background and uses clustering and base classes to improve the performance.</p><hr><h3>Complementary Benefits of Contrastive Learning and Self-Training Under  Distribution Shift</h3>
<p>Saurabh Garg,Amrith Setlur,Zachary Chase Lipton,Sivaraman Balakrishnan,Virginia Smith,Aditi Raghunathan</p>
<p><a href='http://arxiv.org/abs/2312.03318v1'>http://arxiv.org/abs/2312.03318v1</a></p>
<p><b>Compressor summary</b>: This paper investigates how combining self-training and contrastive learning techniques improve unsupervised domain adaptation and semi-supervised learning, with varying success depending on the setting.</p><hr><h3>Optimizing Two-Pass Cross-Lingual Transfer Learning: Phoneme Recognition  and Phoneme to Grapheme Translation</h3>
<p>Wonjun Lee,Gary Geunbae Lee,Yunsu Kim</p>
<p><a href='http://arxiv.org/abs/2312.03312v1'>http://arxiv.org/abs/2312.03312v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to improve speech recognition in low-resource languages by enhancing phoneme recognition and translation models with articulatory characteristics and realistic noise generation.</p><hr><h3>Benchmarking Continual Learning from Cognitive Perspectives</h3>
<p>Xiaoqian Liu,Junge Zhang,Mingyi Zhang,Peipei Yang</p>
<p><a href='http://arxiv.org/abs/2312.03309v1'>http://arxiv.org/abs/2312.03309v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a unified evaluation paradigm for continual learning models based on cognitive properties supporting human continual learning, such as adaptability, sensitivity to task variations, and efficiency.</p><hr><h3>Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking  Technique</h3>
<p>Ilya Tyagin,Ilya Safro</p>
<p><a href='http://arxiv.org/abs/2312.03303v1'>http://arxiv.org/abs/2312.03303v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Dyport, a new benchmarking system for evaluating biomedical hypothesis generation systems using curated datasets and dynamic graphs to assess both accuracy and impact.</p><hr><h3>DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction</h3>
<p>Yanlong Li,Chamara Madarasingha,Kanchana Thilakarathna</p>
<p><a href='http://arxiv.org/abs/2312.03298v1'>http://arxiv.org/abs/2312.03298v1</a></p>
<p><b>Compressor summary</b>: DiffPMAE is a self-supervised learning method for point cloud reconstruction that combines Masked Auto-Encoding and Diffusion Model, outperforming many existing techniques on various tasks.</p><hr><h3>Enhancing Molecular Property Prediction via Mixture of Collaborative  Experts</h3>
<p>Xu Yao,Shuang Liang,Songqiao Han,Hailiang Huang</p>
<p><a href='http://arxiv.org/abs/2312.03292v1'>http://arxiv.org/abs/2312.03292v1</a></p>
<p><b>Compressor summary</b>: The GNN-MoCE architecture uses a mixture of collaborative experts to predict biochemical properties from molecular graphs, addressing data scarcity and imbalance in the Molecular Property Prediction task by exploiting task commonalities and enhancing expert diversity and influence.</p><hr><h3>OMNIINPUT: A Model-centric Evaluation Framework through Output  Distribution</h3>
<p>Weitang Liu,Ying Wai Li,Tianle Wang,Yi-Zhuang You,Jingbo Shang</p>
<p><a href='http://arxiv.org/abs/2312.03291v1'>http://arxiv.org/abs/2312.03291v1</a></p>
<p><b>Compressor summary</b>: The OmniInput framework evaluates an AI/ML model's quality on all possible inputs by using a self-constructed test set and analyzing its output distribution.</p><hr><h3>Can language agents be alternatives to PPO? A Preliminary Empirical  Study On OpenAI Gym</h3>
<p>Junjie Sheng,Zixiao Huang,Chuyun Shen,Wenhao Li,Yun Hua,Bo Jin,Hongyuan Zha,Xiangfeng Wang</p>
<p><a href='http://arxiv.org/abs/2312.03290v1'>http://arxiv.org/abs/2312.03290v1</a></p>
<p><b>Compressor summary</b>: The authors investigate if language agents can be alternatives to PPO agents in sequential decision-making tasks by using the TextGym simulator, introducing different levels of scenarios, and proposing a novel EXE agent.</p><hr><h3>Class Incremental Learning for Adversarial Robustness</h3>
<p>Seungju Cho,Hongshin Lee,Changick Kim</p>
<p><a href='http://arxiv.org/abs/2312.03289v1'>http://arxiv.org/abs/2312.03289v1</a></p>
<p><b>Compressor summary</b>: The study proposes ARCIL, a method that combines adversarial robustness and incremental learning, and introduces FPD and LAD losses to address the loss of robustness in this setting, achieving significantly better results than existing methods on three datasets.</p><hr><h3>STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention  Transformer for Skeleton-based Action Recognition</h3>
<p>Nguyen Huu Bao Long</p>
<p><a href='http://arxiv.org/abs/2312.03288v1'>http://arxiv.org/abs/2312.03288v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for skeleton-based action recognition using graph convolutional networks, cross-attention modules, and temporal attention transformers that outperforms previous methods on two datasets.</p><hr><h3>Indirect Gradient Matching for Adversarial Robust Distillation</h3>
<p>Hongsin Lee,Seungju Cho,Changick Kim</p>
<p><a href='http://arxiv.org/abs/2312.03286v1'>http://arxiv.org/abs/2312.03286v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new technique, IGDM, to improve adversarial robustness by transferring input gradient knowledge from a teacher model to a student model, which enhances the performance of existing adversarial distillation methods without additional data augmentation.</p><hr><h3>Anomaly Detection for Scalable Task Grouping in Reinforcement  Learning-based RAN Optimization</h3>
<p>Jimmy Li,Igor Kozlov,Di Wu,Xue Liu,Gregory Dudek</p>
<p><a href='http://arxiv.org/abs/2312.03277v1'>http://arxiv.org/abs/2312.03277v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a scalable framework using reinforcement learning and anomaly detection to optimize cellular RAN across many cell sites with varying traffic patterns, efficiently using computational resources.</p><hr><h3>SO-NeRF: Active View Planning for NeRF using Surrogate Objectives</h3>
<p>Keifer Lee,Shubham Gupta,Sunglyoung Kim,Bhargav Makwana,Chao Chen,Chen Feng</p>
<p><a href='http://arxiv.org/abs/2312.03266v1'>http://arxiv.org/abs/2312.03266v1</a></p>
<p><b>Compressor summary</b>: SOAR is a method for selecting good views for NeRF using interpretable functions and a learned network, improving speed and quality compared to baselines.</p><hr><h3>f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization</h3>
<p>Sina Baharlouei,Shivam Patel,Meisam Razaviyayn</p>
<p><a href='http://arxiv.org/abs/2312.03259v1'>http://arxiv.org/abs/2312.03259v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a stochastic optimization framework for fair machine learning that works with small data batches, has convergence guarantees, and performs well on both training and test data distribution shifts.</p><hr><h3>CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale  Recommendation Models</h3>
<p>Hailin Zhang,Zirui Liu,Boxuan Chen,Yikai Zhao,Tong Zhao,Tong Yang,Bin Cui</p>
<p><a href='http://arxiv.org/abs/2312.03256v1'>http://arxiv.org/abs/2312.03256v1</a></p>
<p><b>Compressor summary</b>: CAFE is a new compression framework for Deep Learning Recommendation Models that uses HotSketch to capture feature importance and hash embedding for non-hot features, achieving better performance than existing methods.</p><hr><h3>Seller-side Outcome Fairness in Online Marketplaces</h3>
<p>Zikun Ye,Reza Yousefi Maragheh,Lalitesh Morishetti,Shanu Vashishtha,Jason Cho,Kaushiki Nag,Sushant Kumar,Kannan Achan</p>
<p><a href='http://arxiv.org/abs/2312.03253v1'>http://arxiv.org/abs/2312.03253v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an optimization model and a gradient-based algorithm to improve seller fairness in online marketplaces by balancing recommendation rewards and a fairness metric.</p><hr><h3>Customizable Combination of Parameter-Efficient Modules for Multi-Task  Learning</h3>
<p>Haowen Wang,Tao Sun,Cong Fan,Jinjie Gu</p>
<p><a href='http://arxiv.org/abs/2312.03248v1'>http://arxiv.org/abs/2312.03248v1</a></p>
<p><b>Compressor summary</b>: C-Poly is a novel approach for improving neural networks' knowledge organization, leading to better cross-task generalization using customized skills and shared skills learned with low-rank techniques.</p><hr><h3>Multicoated and Folded Graph Neural Networks with Strong Lottery Tickets</h3>
<p>Jiale Yan,Hiroaki Ito,Ángel López García-Arias,Yasuyuki Okoshi,Hikari Otsuka,Kazushi Kawamura,Thiem Van Chu,Masato Motomura</p>
<p><a href='http://arxiv.org/abs/2312.03236v1'>http://arxiv.org/abs/2312.03236v1</a></p>
<p><b>Compressor summary</b>: This paper explores subnetworks in graph neural networks (GNNs) using scalar pruning mask methods, discovering untrained recurrent networks with high performance and reducing memory usage by up to 98.7%.</p><hr><h3>Deep Multimodal Fusion for Surgical Feedback Classification</h3>
<p>Rafal Kocielnik,Elyssa Y. Wong,Timothy N. Chu,Lydia Lin,De-An Huang,Jiayun Wang,Anima Anandkumar,Andrew J. Hung</p>
<p><a href='http://arxiv.org/abs/2312.03231v1'>http://arxiv.org/abs/2312.03231v1</a></p>
<p><b>Compressor summary</b>: This paper develops a machine learning model to classify five types of surgical feedback (Anatomic, Technical, Procedural, Praise, Visual Aid) from text, audio, and video inputs to help improve surgical training.</p><hr><h3>Human Body Model based ID using Shape and Pose Parameters</h3>
<p>Aravind Sundaresan,Brian Burns,Indranil Sur,Yi Yao,Xiao Lin,Sujeong Kim</p>
<p><a href='http://arxiv.org/abs/2312.03227v1'>http://arxiv.org/abs/2312.03227v1</a></p>
<p><b>Compressor summary</b>: The HMID system, trained with shape, pose, and biometric losses, improves biometric identification performance on raw images of human bodies in various conditions.</p><hr><h3>Rethinking Object Saliency Ranking: A Novel Whole-flow Processing  Paradigm</h3>
<p>Mengke Song,Linfeng Li,Dunquan Wu,Wenfeng Song,Chenglizhao Chen</p>
<p><a href='http://arxiv.org/abs/2312.03226v1'>http://arxiv.org/abs/2312.03226v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new method for ranking salient objects by importance order, addressing challenges in existing methods such as ill-defined ground truth, multi-task conflicts, and complex model designs.</p><hr><h3>Predicting Scores of Various Aesthetic Attribute Sets by Learning from  Overall Score Labels</h3>
<p>Heng Huang,Xin Jin,Yaqi Liu,Hao Lou,Chaoen Xiao,Shuai Cui,Xinning Li,Dongqing Zou</p>
<p><a href='http://arxiv.org/abs/2312.03222v1'>http://arxiv.org/abs/2312.03222v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new model, F2S, that predicts image aesthetic attributes using feature extractors instead of labels, enabling the learning of meaningful attribute scores from overall scores.</p><hr><h3>Accelerated Gradient Algorithms with Adaptive Subspace Search for  Instance-Faster Optimization</h3>
<p>Yuanshi Liu,Hanzhen Zhao,Yang Xu,Pengyun Yue,Cong Fang</p>
<p><a href='http://arxiv.org/abs/2312.03218v1'>http://arxiv.org/abs/2312.03218v1</a></p>
<p><b>Compressor summary</b>: This paper proposes adaptive gradient-based algorithms that improve complexities for machine learning problems by refining the description of degenerated conditions using two factors and addressing the limitations of existing optimization modeling and analysis.</p><hr><h3>SDSRA: A Skill-Driven Skill-Recombination Algorithm for Efficient Policy  Learning</h3>
<p>Eric H. Jiang,Andrew Lizarraga</p>
<p><a href='http://arxiv.org/abs/2312.03216v1'>http://arxiv.org/abs/2312.03216v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new algorithm, SDSRA, that improves efficiency and policy quality in reinforcement learning tasks by combining skill-based strategies with robust Actor-Critic framework.</p><hr><h3>Bootstrap Your Own Variance</h3>
<p>Polina Turishcheva,Jason Ramapuram,Sinead Williamson,Dan Busbridge,Eeshan Dhekane,Russ Webb</p>
<p><a href='http://arxiv.org/abs/2312.03213v1'>http://arxiv.org/abs/2312.03213v1</a></p>
<p><b>Compressor summary</b>: BYOV combines self-supervised learning and Bayesian methods to estimate uncertainty in model predictions, outperforming a deterministic baseline and providing preliminary evidence of its usefulness.</p><hr><h3>Constrained Bayesian Optimization Under Partial Observations: Balanced  Improvements and Provable Convergence</h3>
<p>Shengbo Wang,Ke Li</p>
<p><a href='http://arxiv.org/abs/2312.03212v1'>http://arxiv.org/abs/2312.03212v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an efficient and provable method for solving expensive partially observable constrained optimization problems using improved acquisition functions and a surrogate model that better represents feasible regions.</p><hr><h3>Cache Me if You Can: Accelerating Diffusion Models through Block Caching</h3>
<p>Felix Wimbauer,Bichen Wu,Edgar Schoenfeld,Xiaoliang Dai,Ji Hou,Zijian He,Artsiom Sanakoyeu,Peizhao Zhang,Sam Tsai,Jonas Kohler,Christian Rupprecht,Daniel Cremers,Peter Vajda,Jialiang Wang</p>
<p><a href='http://arxiv.org/abs/2312.03209v1'>http://arxiv.org/abs/2312.03209v1</a></p>
<p><b>Compressor summary</b>: The authors propose block caching, a technique that reuses outputs from previous layer blocks in diffusion models to speed up image generation while maintaining high visual quality.</p><hr><h3>Satellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact</h3>
<p>Patrick Beukema,Favyen Bastani,Piper Wolters,Henry Herzog,Joe Ferdinando</p>
<p><a href='http://arxiv.org/abs/2312.03207v1'>http://arxiv.org/abs/2312.03207v1</a></p>
<p><b>Compressor summary</b>: The paper introduces three specialized computer vision models for satellite data to monitor global IUU fishing and presents best practices for real-time maritime conservation using the Skylight platform.</p><hr><h3>Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled  Feature Fields</h3>
<p>Shijie Zhou,Haoran Chang,Sicheng Jiang,Zhiwen Fan,Zehao Zhu,Dejia Xu,Pradyumna Chari,Suya You,Zhangyang Wang,Achuta Kadambi</p>
<p><a href='http://arxiv.org/abs/2312.03203v1'>http://arxiv.org/abs/2312.03203v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method to extend NeRF's functionality for semantic tasks using 3D Gaussian Splatting and 2D foundation models, while addressing speed and quality issues.</p><hr><h3>Domain Invariant Representation Learning and Sleep Dynamics Modeling for  Automatic Sleep Staging</h3>
<p>Seungyeon Lee,Thai-Hoang Pham,Zhao Cheng,Ping Zhang</p>
<p><a href='http://arxiv.org/abs/2312.03196v1'>http://arxiv.org/abs/2312.03196v1</a></p>
<p><b>Compressor summary</b>: The study introduces a neural network model called DREAM that improves automatic sleep staging by learning domain generalized representations from diverse physiological signals and modeling sleep dynamics, outperforming existing methods on three datasets.</p><hr><h3>Detecting Rumor Veracity with Only Textual Information by Double-Channel  Structure</h3>
<p>Alex Kim,Sangwon Yoon</p>
<p><a href='http://arxiv.org/abs/2312.03195v1'>http://arxiv.org/abs/2312.03195v1</a></p>
<p><b>Compressor summary</b>: The authors propose a double-channel model for classifying rumors on social media as true, false, or unverifiable based on their informativeness and use it to achieve state-of-the-art results on a dataset.</p><hr><h3>Corporate Bankruptcy Prediction with Domain-Adapted BERT</h3>
<p>Alex Kim,Sangwon Yoon</p>
<p><a href='http://arxiv.org/abs/2312.03194v1'>http://arxiv.org/abs/2312.03194v1</a></p>
<p><b>Compressor summary</b>: The study uses BERT to analyze company disclosures and improve bankruptcy prediction by enhancing the input dataset quality, achieving high accuracy rates.</p>