
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-02-13</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-02-13 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>FAST: Factorizable Attention for Speeding up Transformers</h3>
<p><a href='http://arxiv.org/abs/2402.07901v1'>http://arxiv.org/abs/2402.07901v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a faster, more efficient attention mechanism for transformers using a factorable form of attention that reduces computational and memory complexity while maintaining full representation and all-to-all relationship between tokens.</p><hr><h3>Wavefront Randomization Improves Deconvolution</h3>
<p><a href='http://arxiv.org/abs/2402.07900v1'>http://arxiv.org/abs/2402.07900v1</a></p>
<p><b>Compressor summary</b>: Adding a random mask to an imaging system reduces optical aberrations and improves image quality by making deconvolution less sensitive to noise.</p><hr><h3>A systematic investigation of learnability from single child linguistic  input</h3>
<p><a href='http://arxiv.org/abs/2402.07899v1'>http://arxiv.org/abs/2402.07899v1</a></p>
<p><b>Compressor summary</b>: The study trains six model architectures on five datasets containing subsets of a child's linguistic input to examine their ability to form meaningful syntactic and semantic representations, finding that they consistently match previous results.</p><hr><h3>Suppressing Pink Elephants with Direct Principle Feedback</h3>
<p><a href='http://arxiv.org/abs/2402.07896v1'>http://arxiv.org/abs/2402.07896v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Existing methods for controlling language models are not always suitable for inference time use
- The paper proposes a novel method called Direct Principle Feedback that simplifies Constitutional AI
- The method is tested on the Pink Elephant Problem, where an LLM should avoid discussing a certain entity and focus on another
- The results show that the proposed method performs well compared to other models and GPT-4

Summary:
The paper introduces Direct Principle Feedback, a simplified version of Constitutional AI, for controlling language models at inference time. It demonstrates its effectiveness on the Pink Elephant Problem, where an LLM should avoid mentioning a forbidden entity and discuss a preferred one.</p><hr><h3>Detection of Spider Mites on Labrador Beans through Machine Learning  Approaches Using Custom Datasets</h3>
<p><a href='http://arxiv.org/abs/2402.07895v1'>http://arxiv.org/abs/2402.07895v1</a></p>
<p><b>Compressor summary</b>: The study presents a visual machine learning method for plant disease detection using real-world camera data, showing improved accuracy with a sequential CNN model.</p><hr><h3>MODIPHY: Multimodal Obscured Detection for IoT using PHantom  Convolution-Enabled Faster YOLO</h3>
<p><a href='http://arxiv.org/abs/2402.07894v1'>http://arxiv.org/abs/2402.07894v1</a></p>
<p><b>Compressor summary</b>: YOLO Phantom is a small, efficient object detection model that works well in low-light and occluded scenarios for IoT applications.</p><hr><h3>Label-Efficient Model Selection for Text Generation</h3>
<p><a href='http://arxiv.org/abs/2402.07891v1'>http://arxiv.org/abs/2402.07891v1</a></p>
<p><b>Compressor summary</b>: DiffUse is an efficient method for choosing between text generation models by clustering embeddings and reducing the need for preference annotations.</p><hr><h3>MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement  Learning</h3>
<p><a href='http://arxiv.org/abs/2402.07890v1'>http://arxiv.org/abs/2402.07890v1</a></p>
<p><b>Compressor summary</b>: The paper introduces MAIDCRL, a semi-centralized reinforcement learning method for multi-agent control using convolutional layers, which improves performance and speed on both homogeneous and heterogeneous StarCraft scenarios.</p><hr><h3>WildfireGPT: Tailored Large Language Model for Wildfire Analysis</h3>
<p><a href='http://arxiv.org/abs/2402.07877v1'>http://arxiv.org/abs/2402.07877v1</a></p>
<p><b>Compressor summary</b>: WildfireGPT is a prototype LLM agent that uses climate projections and scientific literature to provide detailed, domain-specific insights on wildfire risks for various end users.</p><hr><h3>Policy Improvement using Language Feedback Models</h3>
<p><a href='http://arxiv.org/abs/2402.07876v1'>http://arxiv.org/abs/2402.07876v1</a></p>
<p><b>Compressor summary</b>: Language Feedback Models (LFMs) improve instruction following by identifying desirable actions from large language models and generalize to unseen environments.</p><hr><h3>Implicit Bias of Policy Gradient in Linear Quadratic Control:  Extrapolation to Unseen Initial States</h3>
<p><a href='http://arxiv.org/abs/2402.07875v1'>http://arxiv.org/abs/2402.07875v1</a></p>
<p><b>Compressor summary</b>: The paper explores how the implicit bias of policy gradient in reinforcement learning affects extrapolation to unseen initial states and suggests selecting initial states wisely for better performance.</p><hr><h3>Scaling Laws for Fine-Grained Mixture of Experts</h3>
<p><a href='http://arxiv.org/abs/2402.07871v1'>http://arxiv.org/abs/2402.07871v1</a></p>
<p><b>Compressor summary</b>: This paper analyzes how Mixture of Experts models can be optimized by adjusting a new hyperparameter called granularity, leading to more efficient and better performing language models than dense Transformers.</p><hr><h3>Nesting Particle Filters for Experimental Design in Dynamical Systems</h3>
<p><a href='http://arxiv.org/abs/2402.07868v1'>http://arxiv.org/abs/2402.07868v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new Bayesian Experimental Design method that optimizes risk-sensitive policies using nested sequential Monte Carlo estimators, outperforming existing methods on dynamical systems.</p><hr><h3>Prismatic VLMs: Investigating the Design Space of Visually-Conditioned  Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.07865v1'>http://arxiv.org/abs/2402.07865v1</a></p>
<p><b>Compressor summary</b>: The authors evaluate, analyze, and improve visually-conditioned language models (VLMs) for visual dialogue and related tasks, providing a unified framework, code, and checkpoints.</p><hr><h3>Lissard: Long and Simple Sequential Reasoning Datasets</h3>
<p><a href='http://arxiv.org/abs/2402.07859v1'>http://arxiv.org/abs/2402.07859v1</a></p>
<p><b>Compressor summary</b>: The paper presents Lissard, a benchmark to test language models' ability to handle long sequences with repetitive rules, and shows that existing models perform worse on these tasks as the sequence length increases.</p><hr><h3>Multiscale Neuroimaging Features for the Identification of Medication  Class and Non-Responders in Mood Disorder Treatment</h3>
<p><a href='http://arxiv.org/abs/2402.07858v1'>http://arxiv.org/abs/2402.07858v1</a></p>
<p><b>Compressor summary</b>: The text discusses using multi spatial scale neuroimaging features to help identify patients with mood disorders who may not respond to standard treatments and find better alternatives.</p><hr><h3>Comparing skill of historical rainfall data based monsoon rainfall  prediction in India with NCEP-NWP forecasts</h3>
<p><a href='http://arxiv.org/abs/2402.07851v1'>http://arxiv.org/abs/2402.07851v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper trains neural networks to forecast rainfall in India using historical data from 1901 to 2022
- The paper compares the neural network predictions with NWP forecasts and persistence estimates
- The paper finds that neural network predictions are more accurate than both alternatives, especially for three day forecasts
- The paper suggests that NWP forecasts can be improved by using more diverse data and better neural network architecture

Summary:
The paper shows how neural networks trained on historical rainfall data in India outperform existing methods in predicting short-term rainfall, and proposes ways to improve NWP forecasts.</p><hr><h3>Generative Modeling of Discrete Joint Distributions by E-Geodesic Flow  Matching on Assignment Manifolds</h3>
<p><a href='http://arxiv.org/abs/2402.07846v1'>http://arxiv.org/abs/2402.07846v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new generative model for discrete distributions using normalizing flows, which gradually assign categories and avoid discretization issues, and can represent complex dependencies in structured data.</p><hr><h3>An Investigation into Using Unsupervised Metrics to Optimise GNNs for  Node Clustering</h3>
<p><a href='http://arxiv.org/abs/2402.07845v1'>http://arxiv.org/abs/2402.07845v1</a></p>
<p><b>Compressor summary</b>: This paper shows that modularity can be used to optimize graph neural networks (GNNs) without ground-truth comparisons and investigates its limitations on synthetic datasets with different information partitioning scenarios.</p><hr><h3>Do Membership Inference Attacks Work on Large Language Models?</h3>
<p><a href='http://arxiv.org/abs/2402.07841v1'>http://arxiv.org/abs/2402.07841v1</a></p>
<p><b>Compressor summary</b>: The paper studies how well membership inference attacks can guess if a text is part of the training data for large language models, finding that these attacks are mostly ineffective due to factors like dataset size and fuzzy boundaries between members and non-members.</p><hr><h3>Towards Meta-Pruning via Optimal Transport</h3>
<p><a href='http://arxiv.org/abs/2402.07839v1'>http://arxiv.org/abs/2402.07839v1</a></p>
<p><b>Compressor summary</b>: Intra-Fusion is a novel neural network pruning method that uses fusion and Optimal Transport to create a more effective sparse model without the need for fine-tuning, and it can also reduce training time.</p><hr><h3>Generalizing across Temporal Domains with Koopman Operators</h3>
<p><a href='http://arxiv.org/abs/2402.07834v1'>http://arxiv.org/abs/2402.07834v1</a></p>
<p><b>Compressor summary</b>: The study proposes Temporal Koopman Networks (TKNets) for addressing the challenging problem of generalizing predictive models to evolving domains using Koopman theory.</p><hr><h3>Aya Model: An Instruction Finetuned Open-Access Multilingual Language  Model</h3>
<p><a href='http://arxiv.org/abs/2402.07827v1'>http://arxiv.org/abs/2402.07827v1</a></p>
<p><b>Compressor summary</b>: Aya is a multilingual language model that performs well on various tasks across 101 languages and introduces new evaluation methods to assess its performance.</p><hr><h3>Understanding fitness landscapes in morpho-evolution via local optima  networks</h3>
<p><a href='http://arxiv.org/abs/2402.07822v1'>http://arxiv.org/abs/2402.07822v1</a></p>
<p><b>Compressor summary</b>: The paper applies Local Optima Network analysis to compare the fitness landscapes of three genetic encodings for robot morpho-evolution and locomotion tasks, providing insights for designing better algorithms.</p><hr><h3>On Computationally Efficient Multi-Class Calibration</h3>
<p><a href='http://arxiv.org/abs/2402.07821v1'>http://arxiv.org/abs/2402.07821v1</a></p>
<p><b>Compressor summary</b>: This work proposes a novel notion of multi-class calibration called projected smooth calibration, which provides strong guarantees for downstream binary classification tasks and can be computed efficiently in polynomial time.</p><hr><h3>A Benchmark Grocery Dataset of Realworld Point Clouds From Single View</h3>
<p><a href='http://arxiv.org/abs/2402.07819v1'>http://arxiv.org/abs/2402.07819v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new large-scale 3D grocery dataset (3DGrocery100) for computer vision applications, addressing the lack of fine-grained and real-world data in this domain.</p><hr><h3>Differentially Private Zeroth-Order Methods for Scalable Large Language  Model Finetuning</h3>
<p><a href='http://arxiv.org/abs/2402.07818v1'>http://arxiv.org/abs/2402.07818v1</a></p>
<p><b>Compressor summary</b>: The paper proposes stagewise DP zeroth-order methods for LLM pretraining that balance privacy, utility, and scalability, and reduces trainable parameters using data-free pruning.</p><hr><h3>Injecting Wiktionary to improve token-level contextual representations  using contrastive learning</h3>
<p><a href='http://arxiv.org/abs/2402.07817v1'>http://arxiv.org/abs/2402.07817v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Contextual word embeddings are sensitive to context but need more supervision
- The paper proposes injecting a lexicon as an alternative source of supervision using Wiktionary
- The paper evaluates the approach on the Word-In-Context task and achieves new state-of-the-art results

Summary:
The paper introduces a novel method to improve contextual word embeddings by using Wiktionary as extra supervision and shows its effectiveness on the Word-In-Context task.</p><hr><h3>PBADet: A One-Stage Anchor-Free Approach for Part-Body Association</h3>
<p><a href='http://arxiv.org/abs/2402.07814v1'>http://arxiv.org/abs/2402.07814v1</a></p>
<p><b>Compressor summary</b>: PBADet is a new method for detecting human body parts and their associations with individuals using multi-scale features without anchors, achieving better accuracy and efficiency than existing methods.</p><hr><h3>Retrieval-Augmented Thought Process as Sequential Decision Making</h3>
<p><a href='http://arxiv.org/abs/2402.07812v1'>http://arxiv.org/abs/2402.07812v1</a></p>
<p><b>Compressor summary</b>: RATP is a method that uses Monte-Carlo Tree Search to improve the thought generation of large language models by leveraging external knowledge and addressing privacy, hallucination, and context handling issues.</p><hr><h3>Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation</h3>
<p><a href='http://arxiv.org/abs/2402.07808v1'>http://arxiv.org/abs/2402.07808v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The task is to estimate a distribution of parameters that can generate data-consistent simulations
- The problem is ill-posed because many source distributions can match the data
- The proposed approach maximizes entropy to retain uncertainty and uses Sliced-Wasserstein distance
- The method works on sample-based tasks and recovers high-entropy source distributions without sacrificing fidelity
- The method is applied to infer parameters of a neuron model from experimental datasets

Summary:
The authors propose a method for inferring uncertain source distributions of simulator parameters using maximum entropy and Sliced-Wasserstein distance, and demonstrate its application to a neuron model.</p><hr><h3>Generalising Planning Environment Redesign</h3>
<p><a href='http://arxiv.org/abs/2402.07799v1'>http://arxiv.org/abs/2402.07799v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a general, metric-agnostic approach to planning environment redesign that can handle various objectives and outperforms existing approaches on benchmarks.</p><hr><h3>From Uncertainty to Precision: Enhancing Binary Classifier Performance  through Calibration</h3>
<p><a href='http://arxiv.org/abs/2402.07790v1'>http://arxiv.org/abs/2402.07790v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Binary classifier performance typically measured by accuracy, which ignores uncertainty
- Calibration important for interpreting model scores as probabilities
- Local Calibration Score introduced as a refined metric to detect score distortions
- Local regressions recommended as effective recalibration tools and visualization facilitators
- Applied to Random Forest classifier for credit default prediction

Summary:
The paper proposes the Local Calibration Score, a new metric to measure and improve calibration of binary classifiers, especially in sensitive domains like finance, using local regressions.</p><hr><h3>Multi-Intent Attribute-Aware Text Matching in Searching</h3>
<p><a href='http://arxiv.org/abs/2402.07788v1'>http://arxiv.org/abs/2402.07788v1</a></p>
<p><b>Compressor summary</b>: The study proposes a multi-intent attribute-aware matching model (MIM) that leverages attributes from both queries and items to improve text matching in searching platforms.</p><hr><h3>Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment  Analysis</h3>
<p><a href='http://arxiv.org/abs/2402.07787v1'>http://arxiv.org/abs/2402.07787v1</a></p>
<p><b>Compressor summary</b>: The paper introduces EMGF, a framework that efficiently integrates diverse linguistic and structural features for improved Aspect-based Sentiment Analysis using multi-anchor triplet learning and orthogonal projection.</p><hr><h3>HYPO: Hyperspherical Out-of-Distribution Generalization</h3>
<p><a href='http://arxiv.org/abs/2402.07785v1'>http://arxiv.org/abs/2402.07785v1</a></p>
<p><b>Compressor summary</b>: HYPO is a novel framework for machine learning models to learn domain-invariant features across different environments by using a hyperspherical space and a prototypical learning objective, which improves out-of-distribution generalization.</p><hr><h3>TELLER: A Trustworthy Framework for Explainable, Generalizable and  Controllable Fake News Detection</h3>
<p><a href='http://arxiv.org/abs/2402.07776v1'>http://arxiv.org/abs/2402.07776v1</a></p>
<p><b>Compressor summary</b>: The text proposes a novel framework for detecting fake news that integrates human expertise, logical predicates, and generalizable rules to achieve explainability, generalizability, and controllability in the detection process.</p><hr><h3>End-to-End Learning for Fair Multiobjective Optimization Under  Uncertainty</h3>
<p><a href='http://arxiv.org/abs/2402.07772v1'>http://arxiv.org/abs/2402.07772v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method for integrating nondifferentiable optimization problems with uncertain parameters and fairness/robustness properties into machine learning models using the Predict-Then-Optimize paradigm.</p><hr><h3>Text Detoxification as Style Transfer in English and Hindi</h3>
<p><a href='http://arxiv.org/abs/2402.07767v1'>http://arxiv.org/abs/2402.07767v1</a></p>
<p><b>Compressor summary</b>: The paper proposes three methods to automatically convert toxic text into non-toxic text while keeping its meaning and fluency, using a dataset with expert-annotated detoxified versions of toxic sentences.</p><hr><h3>Towards an Understanding of Stepwise Inference in Transformers: A  Synthetic Graph Navigation Model</h3>
<p><a href='http://arxiv.org/abs/2402.07757v1'>http://arxiv.org/abs/2402.07757v1</a></p>
<p><b>Compressor summary</b>: The text describes a synthetic graph navigation task to study stepwise inference in autoregressive Transformer models, revealing various phenomena related to reasoning and generalization.</p><hr><h3>Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language  Models</h3>
<p><a href='http://arxiv.org/abs/2402.07754v1'>http://arxiv.org/abs/2402.07754v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Diffusion-of-Thought, a new model that combines diffusion and Chain-of-Thought techniques to improve reasoning in text processing tasks.</p><hr><h3>Predictive Churn with the Set of Good Models</h3>
<p><a href='http://arxiv.org/abs/2402.07745v1'>http://arxiv.org/abs/2402.07745v1</a></p>
<p><b>Compressor summary</b>: The paper studies predictive churn due to model updates and proposes using predictive multiplicity measures to examine expected churn over the Rashomon set of prospective models.</p><hr><h3>Towards Unified Alignment Between Agents, Humans, and Environment</h3>
<p><a href='http://arxiv.org/abs/2402.07744v1'>http://arxiv.org/abs/2402.07744v1</a></p>
<p><b>Compressor summary</b>: The paper introduces $\mathbf{UA}^2$ principles for aligning agents with human intentions, environmental dynamics, and self-constraints to improve their performance in realistic environments.</p><hr><h3>Asking Multimodal Clarifying Questions in Mixed-Initiative  Conversational Search</h3>
<p><a href='http://arxiv.org/abs/2402.07742v1'>http://arxiv.org/abs/2402.07742v1</a></p>
<p><b>Compressor summary</b>: The text proposes adding images to clarifying questions in conversational search systems to improve multimodal query clarification, introduces a new dataset (Melon) and a model (Marto) for this task, and shows significant improvements in retrieval performance.</p><hr><h3>Task-conditioned adaptation of visual features in multi-task policy  learning</h3>
<p><a href='http://arxiv.org/abs/2402.07739v1'>http://arxiv.org/abs/2402.07739v1</a></p>
<p><b>Compressor summary</b>: The paper proposes task-conditioned adapters for multi-task policy learning in autonomous agents, enabling them to flexibly adapt their perception modules based on current tasks without finetuning pre-trained models and using example demonstrations when the task is unknown.</p><hr><h3>Universal link predictor by In-context Learning</h3>
<p><a href='http://arxiv.org/abs/2402.07738v1'>http://arxiv.org/abs/2402.07738v1</a></p>
<p><b>Compressor summary</b>: UniLP is a novel link prediction model that adapts to different graphs without targeted training by combining heuristic and parametric approaches with In-context Learning.</p><hr><h3>Unsupervised Sign Language Translation and Generation</h3>
<p><a href='http://arxiv.org/abs/2402.07726v1'>http://arxiv.org/abs/2402.07726v1</a></p>
<p><b>Compressor summary</b>: USLNet is an unsupervised model that translates and generates sign language from text and video data without parallel sign language data, using reconstruction modules and cross-modality back-translation procedure.</p><hr><h3>LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation</h3>
<p><a href='http://arxiv.org/abs/2402.07721v1'>http://arxiv.org/abs/2402.07721v1</a></p>
<p><b>Compressor summary</b>: LoRA-drop is a method to improve resource efficiency in fine-tuning large pre-trained models by analyzing and retaining LoRA output for important layers.</p><hr><h3>Model Collapse Demystified: The Case of Regression</h3>
<p><a href='http://arxiv.org/abs/2402.07712v1'>http://arxiv.org/abs/2402.07712v1</a></p>
<p><b>Compressor summary</b>: The paper studies how large language models like ChatGPT deteriorate when trained on their own generated data and proposes an adaptive regularization strategy to prevent this.</p><hr><h3>Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA</h3>
<p><a href='http://arxiv.org/abs/2402.07710v1'>http://arxiv.org/abs/2402.07710v1</a></p>
<p><b>Compressor summary</b>: Deep learning methods, especially CNNs, are widely used for analyzing structured grid data like images, but face challenges with sparse and unstructured 3D point clouds from LiDAR and 3D sensors.</p><hr><h3>Signed Distance Field based Segmentation and Statistical Shape Modelling  of the Left Atrial Appendage</h3>
<p><a href='http://arxiv.org/abs/2402.07708v1'>http://arxiv.org/abs/2402.07708v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a pipeline for automatic segmentation, mesh model creation, and statistical shape modelling of the left atrial appendage in patients with atrial fibrillation using deep learning methods.</p><hr><h3>Online Sequential Decision-Making with Unknown Delays</h3>
<p><a href='http://arxiv.org/abs/2402.07703v1'>http://arxiv.org/abs/2402.07703v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The text is about online sequential decision-making with delays using online convex optimization (OCO) framework
- The text proposes three families of delayed algorithms based on approximate solutions for different types of feedback
- The text provides regret bounds and demonstrates efficiency under different norms

Summary:
The text presents three families of OCO algorithms for online sequential decision-making with delays, along with their regret bounds and efficiency in various norms.</p><hr><h3>Boundary Exploration for Bayesian Optimization With Unknown Physical  Constraints</h3>
<p><a href='http://arxiv.org/abs/2402.07692v1'>http://arxiv.org/abs/2402.07692v1</a></p>
<p><b>Compressor summary</b>: BE-CBO is a new Bayesian optimization method that uses neural networks to learn constraints and efficiently explore the boundary between feasible and infeasible regions of the design space.</p><hr><h3>OrderBkd: Textual backdoor attack through repositioning</h3>
<p><a href='http://arxiv.org/abs/2402.07689v1'>http://arxiv.org/abs/2402.07689v1</a></p>
<p><b>Compressor summary</b>: Our new backdoor attack on NLP systems uses repositioning of two words as a trigger, maintains high success rate, and is robust against ONION defense.</p><hr><h3>CyberMetric: A Benchmark Dataset for Evaluating Large Language Models  Knowledge in Cybersecurity</h3>
<p><a href='http://arxiv.org/abs/2402.07688v1'>http://arxiv.org/abs/2402.07688v1</a></p>
<p><b>Compressor summary</b>: CyberMetric is a dataset with 10,000 questions to benchmark LLMs in cybersecurity, showing they often perform better than human experts.</p><hr><h3>Contrastive Multiple Instance Learning for Weakly Supervised Person ReID</h3>
<p><a href='http://arxiv.org/abs/2402.07685v1'>http://arxiv.org/abs/2402.07685v1</a></p>
<p><b>Compressor summary</b>: CMIL is a new weakly supervised ReID framework that leverages contrastive losses and outperforms baselines on three datasets, introducing the WL-MUDD dataset.</p><hr><h3>Auxiliary Tasks to Boost Biaffine Semantic Dependency Parsing</h3>
<p><a href='http://arxiv.org/abs/2402.07682v1'>http://arxiv.org/abs/2402.07682v1</a></p>
<p><b>Compressor summary</b>: The paper proposes auxiliary tasks that improve the biaffine parser's performance on semantic dependency parsing by introducing interdependence between arcs while preserving its O(n^2) complexity.</p><hr><h3>Large Language Models "Ad Referendum": How Good Are They at Machine  Translation in the Legal Domain?</h3>
<p><a href='http://arxiv.org/abs/2402.07681v1'>http://arxiv.org/abs/2402.07681v1</a></p>
<p><b>Compressor summary</b>: The study finds that large language models perform well in translating legal texts, despite lower scores on automatic evaluation metrics, suggesting the importance of human evaluation methods.</p><hr><h3>AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual  Vision Transformer</h3>
<p><a href='http://arxiv.org/abs/2402.07680v1'>http://arxiv.org/abs/2402.07680v1</a></p>
<p><b>Compressor summary</b>: AYDIV is a new framework that aligns LiDAR and camera data to improve long-distance object detection for autonomous driving systems.</p><hr><h3>GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted  Assembly Guidance</h3>
<p><a href='http://arxiv.org/abs/2402.07677v1'>http://arxiv.org/abs/2402.07677v1</a></p>
<p><b>Compressor summary</b>: GBOT is a novel graph-based tracking approach for augmented reality assembly guidance that handles occlusions, complex assembly states, and multiple objects in real time.</p><hr><h3>The Sound of Healthcare: Improving Medical Transcription ASR Accuracy  with Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.07658v1'>http://arxiv.org/abs/2402.07658v1</a></p>
<p><b>Compressor summary</b>: This study shows that Large Language Models can significantly improve the accuracy of Automatic Speech Recognition in medical transcription by enhancing various aspects of transcript quality, including word errors, medical concepts, speaker diarization, and semantic coherence.</p><hr><h3>Detecting the Clinical Features of Difficult-to-Treat Depression using  Synthetic Data from Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.07645v1'>http://arxiv.org/abs/2402.07645v1</a></p>
<p><b>Compressor summary</b>: The authors developed a tool using a large language model to extract and label factors from electronic health records that are associated with difficult-to-treat depression, achieving good performance on real and synthetic data.</p><hr><h3>A Flow-based Credibility Metric for Safety-critical Pedestrian Detection</h3>
<p><a href='http://arxiv.org/abs/2402.07642v1'>http://arxiv.org/abs/2402.07642v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a new metric, c-flow, for evaluating how well object detectors in automated driving can avoid safety-critical mistakes by using optical flow and without needing extra labels.</p><hr><h3>Complete Instances Mining for Weakly Supervised Instance Segmentation</h3>
<p><a href='http://arxiv.org/abs/2402.07633v1'>http://arxiv.org/abs/2402.07633v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel approach for weakly supervised instance segmentation using MaskIoU heads, Complete Instances Mining strategy, and Anti-noise strategy to refine proposals and improve robustness, achieving state-of-the-art performance on PASCAL VOC 2012 and MS COCO datasets.</p><hr><h3>Overconfident and Unconfident AI Hinder Human-AI Collaboration</h3>
<p><a href='http://arxiv.org/abs/2402.07632v1'>http://arxiv.org/abs/2402.07632v1</a></p>
<p><b>Compressor summary</b>: The text discusses how AI's overconfidence or underconfidence can affect human trust, acceptance of AI suggestions, and collaboration outcomes, and suggests that aligning AI's expressed confidence with its actual performance and calibrating human trust is crucial for enhancing human-AI collaboration.</p><hr><h3>G-Retriever: Retrieval-Augmented Generation for Textual Graph  Understanding and Question Answering</h3>
<p><a href='http://arxiv.org/abs/2402.07630v1'>http://arxiv.org/abs/2402.07630v1</a></p>
<p><b>Compressor summary</b>: The paper presents a question-answering framework for textual graphs that integrates GNNs, LLMs, and RAG, and introduces a benchmark and outperforms baselines on various tasks.</p><hr><h3>AutoMathText: Autonomous Data Selection with Language Models for  Mathematical Texts</h3>
<p><a href='http://arxiv.org/abs/2402.07625v1'>http://arxiv.org/abs/2402.07625v1</a></p>
<p><b>Compressor summary</b>: Our method improves language models' math skills by using meta-prompted models to select high-quality math content from the AutoMathText dataset, achieving significant token efficiency gains.</p><hr><h3>Anchor-based Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.07616v1'>http://arxiv.org/abs/2402.07616v1</a></p>
<p><b>Compressor summary</b>: The Anchor-based LLM (AnLLM) uses a new self-attention network and inference strategy to compress sequence information into an anchor token, reducing cache and improving inference efficiency for large language models.</p><hr><h3>Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping</h3>
<p><a href='http://arxiv.org/abs/2402.07610v1'>http://arxiv.org/abs/2402.07610v1</a></p>
<p><b>Compressor summary</b>: The paper explores how multi-time bootstrapping self-alignment can improve large language models' performance by exploiting data diversity from in-context learning and proposes Step-On-Feet Tuning (SOFT) to enhance zero or one-shot capabilities.</p><hr><h3>Near-Minimax-Optimal Distributional Reinforcement Learning with a  Generative Model</h3>
<p><a href='http://arxiv.org/abs/2402.07598v1'>http://arxiv.org/abs/2402.07598v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new algorithm for distributional reinforcement learning that approximates return distributions using a generative model and proves its minimax-optimality, along with new theoretical results and experimental comparisons.</p><hr><h3>Sheet Music Transformer: End-To-End Optical Music Recognition Beyond  Monophonic Transcription</h3>
<p><a href='http://arxiv.org/abs/2402.07596v1'>http://arxiv.org/abs/2402.07596v1</a></p>
<p><b>Compressor summary</b>: The Sheet Music Transformer is a new model for optical music recognition that can handle complex musical scores without relying on monophonic strategies.</p><hr><h3>Foundational Inference Models for Dynamical Systems</h3>
<p><a href='http://arxiv.org/abs/2402.07594v1'>http://arxiv.org/abs/2402.07594v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method for inferring ordinary differential equations (ODEs) from noisy data using neural networks, and shows its effectiveness on various systems.</p><hr><h3>Identifying architectural design decisions for achieving green ML  serving</h3>
<p><a href='http://arxiv.org/abs/2402.07585v1'>http://arxiv.org/abs/2402.07585v1</a></p>
<p><b>Compressor summary</b>: This paper reviews ML serving architectural design choices and quality characteristics, focusing on energy efficiency for achieving green AI.</p><hr><h3>Topic Modeling as Multi-Objective Contrastive Optimization</h3>
<p><a href='http://arxiv.org/abs/2402.07577v1'>http://arxiv.org/abs/2402.07577v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for neural topic modeling that balances between document-level contrastive learning and evidence lower bound optimization to improve topic coherence and downstream performance.</p><hr><h3>Only the Curve Shape Matters: Training Foundation Models for Zero-Shot  Multivariate Time Series Forecasting through Next Curve Shape Prediction</h3>
<p><a href='http://arxiv.org/abs/2402.07570v1'>http://arxiv.org/abs/2402.07570v1</a></p>
<p><b>Compressor summary</b>: The General Time Transformer (GTT) is a foundation model for zero-shot multivariate time series forecasting that uses a channel-wise framework to predict next curve shapes based on past ones, achieving superior results on unseen datasets and surpassing supervised baselines.</p><hr><h3>Weisfeiler-Leman at the margin: When more expressivity matters</h3>
<p><a href='http://arxiv.org/abs/2402.07568v1'>http://arxiv.org/abs/2402.07568v1</a></p>
<p><b>Compressor summary</b>: The paper explores how subgraph information and margin theory can improve the generalization performance of graph isomorphism algorithms, such as $1$-WL, and message-passing graph neural networks (MPNNs).</p><hr><h3>TransAxx: Efficient Transformers with Approximate Computing</h3>
<p><a href='http://arxiv.org/abs/2402.07545v1'>http://arxiv.org/abs/2402.07545v1</a></p>
<p><b>Compressor summary</b>: The authors propose TransAxx, a framework that enables fast support for approximate arithmetic on ViT models and uses MCTS to generate approximate accelerators for them, achieving significant trade-offs between accuracy and power.</p><hr><h3>Show Me How It's Done: The Role of Explanations in Fine-Tuning Language  Models</h3>
<p><a href='http://arxiv.org/abs/2402.07543v1'>http://arxiv.org/abs/2402.07543v1</a></p>
<p><b>Compressor summary</b>: Fine-tuning language models with explanations improves their performance and enables them to solve tasks they couldn't before, especially for smaller models.</p><hr><h3>BreakGPT: A Large Language Model with Multi-stage Structure for  Financial Breakout Detection</h3>
<p><a href='http://arxiv.org/abs/2402.07536v1'>http://arxiv.org/abs/2402.07536v1</a></p>
<p><b>Compressor summary</b>: BreakGPT is a large language model that improves financial breakout detection accuracy using a multi-stage structure.</p><hr><h3>Morse sequences</h3>
<p><a href='http://arxiv.org/abs/2402.07526v1'>http://arxiv.org/abs/2402.07526v1</a></p>
<p><b>Compressor summary</b>: A Morse sequence is a sequence of expansions and fillings that represents the gradient vector field of a discrete Morse function on a simplicial complex.</p><hr><h3>MAFIA: Multi-Adapter Fused Inclusive LanguAge Models</h3>
<p><a href='http://arxiv.org/abs/2402.07519v1'>http://arxiv.org/abs/2402.07519v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to debias pretrained language models across multiple dimensions using structured knowledge and a large generative model, improving performance on various tasks and languages.</p><hr><h3>Physics-informed machine learning as a kernel method</h3>
<p><a href='http://arxiv.org/abs/2402.07514v1'>http://arxiv.org/abs/2402.07514v1</a></p>
<p><b>Compressor summary</b>: Physics-informed machine learning combines data and physical models for better regression, with faster convergence rates depending on the physical error.</p><hr><h3>The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese</h3>
<p><a href='http://arxiv.org/abs/2402.07513v1'>http://arxiv.org/abs/2402.07513v1</a></p>
<p><b>Compressor summary</b>: The study investigates biases in speech recognition systems for casual Portuguese conversations using Whisper and MMS methods and shows that oversampling techniques reduce stereotypical biases.</p><hr><h3>Secret Collusion Among Generative AI Agents</h3>
<p><a href='http://arxiv.org/abs/2402.07510v1'>http://arxiv.org/abs/2402.07510v1</a></p>
<p><b>Compressor summary</b>: The paper explores privacy and security issues in systems of communicating AI agents, focusing on the potential use of steganography for secret collusion, and proposes a framework to test and monitor these risks.</p><hr><h3>Clustering Dynamics for Improved Speed Prediction Deriving from  Topographical GPS Registrations</h3>
<p><a href='http://arxiv.org/abs/2402.07507v1'>http://arxiv.org/abs/2402.07507v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to predict traffic speeds using sparse GPS data and topographical features, outperforming existing methods in regions with limited data coverage.</p><hr><h3>NeuralSentinel: Safeguarding Neural Network Reliability and  Trustworthiness</h3>
<p><a href='http://arxiv.org/abs/2402.07506v1'>http://arxiv.org/abs/2402.07506v1</a></p>
<p><b>Compressor summary</b>: NeuralSentinel is a tool that validates AI models by combining attack and defence strategies, explainability concepts, and an easy-to-use interface, which was tested on a skin cancer image detector in a Hackathon event.</p><hr><h3>ClusterTabNet: Supervised clustering method for table detection and  table structure recognition</h3>
<p><a href='http://arxiv.org/abs/2402.07502v1'>http://arxiv.org/abs/2402.07502v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new deep learning method for detecting tables in documents using word relations and shows it is more accurate and efficient than existing methods.</p><hr><h3>One Train for Two Tasks: An Encrypted Traffic Classification Framework  Using Supervised Contrastive Learning</h3>
<p><a href='http://arxiv.org/abs/2402.07501v1'>http://arxiv.org/abs/2402.07501v1</a></p>
<p><b>Compressor summary</b>: The paper proposes CLE-TFE, a model that uses contrastive learning and graph data augmentation to improve encrypted traffic classification by jointly training packet-level and flow-level tasks with less computational overhead.</p><hr><h3>Understanding Deep Learning defenses Against Adversarial Examples  Through Visualizations for Dynamic Risk Assessment</h3>
<p><a href='http://arxiv.org/abs/2402.07496v1'>http://arxiv.org/abs/2402.07496v1</a></p>
<p><b>Compressor summary</b>: The text discusses the importance of studying possible attacks on Deep Neural Network models used in critical tasks and visualizing the effectiveness of different defenses against adversarial example attacks.</p><hr><h3>Score-based Diffusion Models via Stochastic Differential Equations -- a  Technical Tutorial</h3>
<p><a href='http://arxiv.org/abs/2402.07487v1'>http://arxiv.org/abs/2402.07487v1</a></p>
<p><b>Compressor summary</b>: The article explains score-based diffusion models using stochastic differential equations (SDE), covering sampling and score matching methods, with proofs and examples for both beginners and practitioners.</p><hr><h3>T-RAG: Lessons from the LLM Trenches</h3>
<p><a href='http://arxiv.org/abs/2402.07483v1'>http://arxiv.org/abs/2402.07483v1</a></p>
<p><b>Compressor summary</b>: The text describes the development and deployment of Tree-RAG, a question answering system using a large language model that incorporates a tree structure to represent entity hierarchies in private enterprise documents.</p><hr><h3>Topological Safeguard for Evasion Attack based on the Interpretability  of Artificial Neural Network Behavior</h3>
<p><a href='http://arxiv.org/abs/2402.07480v1'>http://arxiv.org/abs/2402.07480v1</a></p>
<p><b>Compressor summary</b>: The text discusses a novel evasion attack detector for Deep Learning models that uses Graph Convolutional Neural Networks (GCN) to analyze neuron activations and model topology, aiming to improve cybersecurity against such threats.</p><hr><h3>Food Recommendation as Language Processing (F-RLP): A Personalized and  Contextual Paradigm</h3>
<p><a href='http://arxiv.org/abs/2402.07477v1'>http://arxiv.org/abs/2402.07477v1</a></p>
<p><b>Compressor summary</b>: The text introduces F-RLP, a new framework that combines language models and food-specific data to create better food recommendations.</p><hr><h3>Pushing The Limit of LLM Capacity for Text Classification</h3>
<p><a href='http://arxiv.org/abs/2402.07470v1'>http://arxiv.org/abs/2402.07470v1</a></p>
<p><b>Compressor summary</b>: RGPT is a framework that uses adaptive boosting to create a specialized text classification LLM by recurrently ensembling base learners, which significantly outperforms existing models and humans.</p><hr><h3>Score-Based Physics-Informed Neural Networks for High-Dimensional  Fokker-Planck Equations</h3>
<p><a href='http://arxiv.org/abs/2402.07465v1'>http://arxiv.org/abs/2402.07465v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel score-based method to solve high-dimensional Fokker-Planck equations, overcoming the challenges of curse of dimensionality and numerical errors in existing methods.</p><hr><h3>A Hormetic Approach to the Value-Loading Problem: Preventing the  Paperclip Apocalypse?</h3>
<p><a href='http://arxiv.org/abs/2402.07462v1'>http://arxiv.org/abs/2402.07462v1</a></p>
<p><b>Compressor summary</b>: HALO is a regulatory paradigm for artificial intelligence that uses hormetic analysis to ensure safe and optimal limits of AI behaviors by modeling them as allostatic opponent processes, solving the value-loading problem and weak-to-strong generalization problem.</p><hr><h3>On the Distance from Calibration in Sequential Prediction</h3>
<p><a href='http://arxiv.org/abs/2402.07458v1'>http://arxiv.org/abs/2402.07458v1</a></p>
<p><b>Compressor summary</b>: The paper studies a binary prediction setting with a calibration distance measure that evaluates deviation from perfect calibration and proves an O(sqrt(T)) bound on it for a forecasting algorithm.</p><hr><h3>OS-Copilot: Towards Generalist Computer Agents with Self-Improvement</h3>
<p><a href='http://arxiv.org/abs/2402.07456v1'>http://arxiv.org/abs/2402.07456v1</a></p>
<p><b>Compressor summary</b>: OS-Copilot is a framework that helps create digital agents that can interact with various elements in an operating system, such as the web, files, and applications, and improve their skills on different tasks.</p><hr><h3>Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs</h3>
<p><a href='http://arxiv.org/abs/2402.07453v1'>http://arxiv.org/abs/2402.07453v1</a></p>
<p><b>Compressor summary</b>: We analyze the impact of bandit feedback on multiclass classification loss and show that it increases the optimal mistake bound by a factor of at most $k$, where $k$ is the number of labels, compared to full information. We also reveal nearly optimal bounds for the gap between randomized and deterministic learners, and adaptive and oblivious adversaries in bandit feedback settings, which differ significantly from the full information scenario.</p><hr><h3>TriAug: Out-of-Distribution Detection for Robust Classification of  Imbalanced Breast Lesion in Ultrasound</h3>
<p><a href='http://arxiv.org/abs/2402.07452v1'>http://arxiv.org/abs/2402.07452v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for detecting out-of-distribution samples in breast ultrasound images and improving classification accuracy with triplet state augmentation and balanced sphere loss.</p><hr><h3>AraSpider: Democratizing Arabic-to-SQL</h3>
<p><a href='http://arxiv.org/abs/2402.07448v1'>http://arxiv.org/abs/2402.07448v1</a></p>
<p><b>Compressor summary</b>: The study introduces AraSpider, an Arabic version of Spider dataset, improves Arabic NLP with multilingual translation models, and highlights the importance of context, back translation, and data sharing in NLP research.</p><hr><h3>Quality Does Matter: A Detailed Look at the Quality and Utility of  Web-Mined Parallel Corpora</h3>
<p><a href='http://arxiv.org/abs/2402.07446v1'>http://arxiv.org/abs/2402.07446v1</a></p>
<p><b>Compressor summary</b>: The study evaluates the quality of web-mined corpora for low-resource languages using similarity measures and shows that NMT models can perform well with high-quality portions of these corpora.</p><hr><h3>The I/O Complexity of Attention, or How Optimal is Flash Attention?</h3>
<p><a href='http://arxiv.org/abs/2402.07443v1'>http://arxiv.org/abs/2402.07443v1</a></p>
<p><b>Compressor summary</b>: FlashAttention algorithm optimizes Transformer's self-attention by reducing I/O complexity, and this paper investigates its optimal performance for various memory hierarchies.</p><hr><h3>Game Agent Driven by Free-Form Text Command: Using LLM-based Code  Generation and Behavior Branch</h3>
<p><a href='http://arxiv.org/abs/2402.07442v1'>http://arxiv.org/abs/2402.07442v1</a></p>
<p><b>Compressor summary</b>: This paper introduces a natural language-based text command control system for game agents that can understand and execute free-form commands using a large language model and behavior trees in a Pok'emon game simulation.</p><hr><h3>Intrinsic Task-based Evaluation for Referring Expression Generation</h3>
<p><a href='http://arxiv.org/abs/2402.07432v1'>http://arxiv.org/abs/2402.07432v1</a></p>
<p><b>Compressor summary</b>: The study proposes a new evaluation method for Referring Expression Generation (REG) models that considers referential success and alternative suggestions, improving on previous ratings-based evaluations.</p><hr><h3>SALAD: Smart AI Language Assistant Daily</h3>
<p><a href='http://arxiv.org/abs/2402.07431v1'>http://arxiv.org/abs/2402.07431v1</a></p>
<p><b>Compressor summary</b>: SALAD is an AI-powered app that helps foreigners learn Japanese by providing translations, speech recognition, audio, vocabulary tracking, grammar explanations, and songs using daily data to improve fluency and confidence in communication with native speakers.</p><hr><h3>Particle Filter SLAM for Vehicle Localization</h3>
<p><a href='http://arxiv.org/abs/2402.07429v1'>http://arxiv.org/abs/2402.07429v1</a></p>
<p><b>Compressor summary</b>: The paper presents a Particle Filter SLAM method that combines encoded data, fiber optic gyro information, and lidar technology to enable precise estimation of vehicle motion and environmental perception for simultaneous localization and mapping in robotics.</p><hr><h3>News Recommendation with Attention Mechanism</h3>
<p><a href='http://arxiv.org/abs/2402.07422v1'>http://arxiv.org/abs/2402.07422v1</a></p>
<p><b>Compressor summary</b>: The paper introduces NRAM, a new algorithm for news recommendation with attention mechanism, which could greatly enhance personalization of news content on digital platforms.</p><hr><h3>Conditional Generative Models are Sufficient to Sample from Any Causal  Effect Estimand</h3>
<p><a href='http://arxiv.org/abs/2402.07419v1'>http://arxiv.org/abs/2402.07419v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method to compute causal effects from observational image data using conditional generative models and diffusion techniques, and applies it to evaluate conditional generative models on CelebA dataset.</p><hr><h3>SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy  Adaptation</h3>
<p><a href='http://arxiv.org/abs/2402.07418v1'>http://arxiv.org/abs/2402.07418v1</a></p>
<p><b>Compressor summary</b>: The text introduces a framework called SemTra that uses multi-modal models and a pretrained language model to adapt semantic skills from user input snippets for cross-domain long-horizon tasks.</p><hr><h3>An Empirical Study Into What Matters for Calibrating Vision-Language  Models</h3>
<p><a href='http://arxiv.org/abs/2402.07417v1'>http://arxiv.org/abs/2402.07417v1</a></p>
<p><b>Compressor summary</b>: This study examines how well vision-language models can estimate uncertainty across different settings and shows that temperature scaling improves their calibration, even with few examples.</p><hr><h3>Context-aware Multi-Model Object Detection for Diversely Heterogeneous  Compute Systems</h3>
<p><a href='http://arxiv.org/abs/2402.07415v1'>http://arxiv.org/abs/2402.07415v1</a></p>
<p><b>Compressor summary</b>: SHIFT is a system that adapts object detection models based on contextual information and computational resources, improving energy efficiency and latency in autonomous systems.</p><hr><h3>Auxiliary Reward Generation with Transition Distance Representation  Learning</h3>
<p><a href='http://arxiv.org/abs/2402.07412v1'>http://arxiv.org/abs/2402.07412v1</a></p>
<p><b>Compressor summary</b>: The text proposes a novel representation learning approach for reinforcement learning that generates auxiliary rewards based on the transition distance between states, improving learning efficiency and stability in manipulation tasks.</p><hr><h3>Potential-Based Reward Shaping For Intrinsic Motivation</h3>
<p><a href='http://arxiv.org/abs/2402.07411v1'>http://arxiv.org/abs/2402.07411v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Potential-Based Intrinsic Motivation (PBIM), which preserves optimal policies and prevents suboptimal behavior in complex environments by converting intrinsic motivation rewards into a potential-based form.</p><hr><h3>A Closer Look at the Robustness of Contrastive Language-Image  Pre-Training (CLIP)</h3>
<p><a href='http://arxiv.org/abs/2402.07410v1'>http://arxiv.org/abs/2402.07410v1</a></p>
<p><b>Compressor summary</b>: This paper investigates the safety objectives of CLIP models, focusing on their resilience to visual factor variations, uncertainty estimations, and anomalous input detection, by testing 83 CLIP models and 127 ImageNet classifiers under various conditions.</p><hr><h3>Dlares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs  Between Spanish and English</h3>
<p><a href='http://arxiv.org/abs/2402.07405v1'>http://arxiv.org/abs/2402.07405v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Tois'on de Oro, a bilingual framework for financial natural language processing in Spanish and English, which includes a large curated dataset, a finetuned LLM, and an evaluation benchmark to address the gap in Spanish finance NLP research.</p><hr><h3>Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic  Hierarchy Process and GPT-4 for Automated Decision Support</h3>
<p><a href='http://arxiv.org/abs/2402.07404v1'>http://arxiv.org/abs/2402.07404v1</a></p>
<p><b>Compressor summary</b>: The study introduces a novel framework that combines the Analytic Hierarchy Process and GPT-4 to automate and enhance cybersecurity decision-making processes using AI-driven virtual experts.</p><hr><h3>Make it more specific: A novel uncertainty based airway segmentation  application on 3D U-Net and its variants</h3>
<p><a href='http://arxiv.org/abs/2402.07403v1'>http://arxiv.org/abs/2402.07403v1</a></p>
<p><b>Compressor summary</b>: The paper proposes two new network structures, B-UNet and B-CE-UNet, for improving lung trachea segmentation by adding branch loss and central line loss to learn fine branch features and uncertainty estimation for confidence.</p><hr><h3>Can LLMs Produce Faithful Explanations For Fact-checking? Towards  Faithful Explainable Fact-Checking via Multi-Agent Debate</h3>
<p><a href='http://arxiv.org/abs/2402.07401v1'>http://arxiv.org/abs/2402.07401v1</a></p>
<p><b>Compressor summary</b>: The study investigates how Large Language Models can generate more faithful explanations for fact-checking using a Multi-Agent Debate Refinement framework that improves credibility and trustworthiness.</p><hr><h3>VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language  Models with Autonomous Instruction Optimization</h3>
<p><a href='http://arxiv.org/abs/2402.07398v1'>http://arxiv.org/abs/2402.07398v1</a></p>
<p><b>Compressor summary</b>: VisLingInstruct optimizes instructions and visual features for MMLMs to improve zero-shot performance in multi-modal tasks, achieving significant gains on TextVQA and HatefulMemes datasets.</p><hr><h3>Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy  Induction from Limited Examples</h3>
<p><a href='http://arxiv.org/abs/2402.07386v1'>http://arxiv.org/abs/2402.07386v1</a></p>
<p><b>Compressor summary</b>: Chain-of-Layer is a method for automatically constructing taxonomies from entities using in-context learning and ranking filters to minimize errors.</p><hr><h3>Exploring Perceptual Limitation of Multimodal Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.07384v1'>http://arxiv.org/abs/2402.07384v1</a></p>
<p><b>Compressor summary</b>: This paper studies how small objects in images affect the performance of large language models in answering visual questions and identifies four factors that limit their perception.</p><hr><h3>Unsupervised Discovery of Object-Centric Neural Fields</h3>
<p><a href='http://arxiv.org/abs/2402.07376v1'>http://arxiv.org/abs/2402.07376v1</a></p>
<p><b>Compressor summary</b>: The text introduces uOCF, an unsupervised method for learning 3D object representations from real images, which improves generalization and enables applications like segmentation and scene manipulation.</p><hr><h3>Real-World Atmospheric Turbulence Correction via Domain Adaptation</h3>
<p><a href='http://arxiv.org/abs/2402.07371v1'>http://arxiv.org/abs/2402.07371v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a domain adaptation framework that combines supervised simulated and unsupervised real-world atmospheric turbulence correction to improve image quality and downstream vision tasks.</p><hr><h3>SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked  AutoEncoder</h3>
<p><a href='http://arxiv.org/abs/2402.07370v1'>http://arxiv.org/abs/2402.07370v1</a></p>
<p><b>Compressor summary</b>: The paper proposes SAMAE, a self-supervised face swapping method that enhances model training by masking facial regions, using disentangled features, and addressing shape misalignment issues.</p><hr><h3>Diff-RNTraj: A Structure-aware Diffusion Model for Road  Network-constrained Trajectory Generation</h3>
<p><a href='http://arxiv.org/abs/2402.07369v1'>http://arxiv.org/abs/2402.07369v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Diff-RNTraj, a diffusion model that generates road network-constrained trajectories with road-related information to address privacy concerns and scale limitations in existing trajectory data.</p><hr><h3>Assessing Generalization for Subpopulation Representative Modeling via  In-Context Learning</h3>
<p><a href='http://arxiv.org/abs/2402.07368v1'>http://arxiv.org/abs/2402.07368v1</a></p>
<p><b>Compressor summary</b>: The study examines how well LLM-based SRMs generalize from data and respond to different demographic groups, finding in-context learning helps some groups while hurting others.</p><hr><h3>A Novel Gaussian Min-Max Theorem and its Applications</h3>
<p><a href='http://arxiv.org/abs/2402.07356v1'>http://arxiv.org/abs/2402.07356v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new pair of Gaussian processes that allows extending classical theorems in high-dimensional statistics and machine learning to non-identically-distributed rows.</p><hr><h3>Data Distribution-based Curriculum Learning</h3>
<p><a href='http://arxiv.org/abs/2402.07352v1'>http://arxiv.org/abs/2402.07352v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Data Distribution-based Curriculum Learning (DDCL), a new approach to ordering training samples from easy to hard, which improves the performance and speed of classification for different classifiers and datasets.</p><hr><h3>Antagonistic AI</h3>
<p><a href='http://arxiv.org/abs/2402.07350v1'>http://arxiv.org/abs/2402.07350v1</a></p>
<p><b>Compressor summary</b>: The paper explores the idea of antagonistic AI systems that challenge users and may have benefits, such as helping them build resilience or healthier relationships, while discussing ethical considerations for their responsible design.</p><hr><h3>Measurement Scheduling for ICU Patients with Offline Reinforcement  Learning</h3>
<p><a href='http://arxiv.org/abs/2402.07344v1'>http://arxiv.org/abs/2402.07344v1</a></p>
<p><b>Compressor summary</b>: The study explores new offline reinforcement learning methods to optimize laboratory test scheduling for ICU patients using a preprocessed dataset.</p><hr><h3>Random Geometric Graph Alignment with Graph Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.07340v1'>http://arxiv.org/abs/2402.07340v1</a></p>
<p><b>Compressor summary</b>: The paper studies how one-layer graph neural networks can recover correct vertex alignments between two noisy graphs with random geometric structure and features, outperforming direct assignment methods in high noise levels.</p><hr><h3>Exploring Saliency Bias in Manipulation Detection</h3>
<p><a href='http://arxiv.org/abs/2402.07338v1'>http://arxiv.org/abs/2402.07338v1</a></p>
<p><b>Compressor summary</b>: The text discusses the importance of considering semantics when detecting image manipulations that spread misinformation through social media.</p>