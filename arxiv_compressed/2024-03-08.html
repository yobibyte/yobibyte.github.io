
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-03-08</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-03-08 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like  Speed</h3>
<p><a href='http://arxiv.org/abs/2403.04765v1'>http://arxiv.org/abs/2403.04765v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method that improves the efficiency and accuracy of semi-dense matching across images, with potential applications in image retrieval and 3D reconstruction.</p><hr><h3>Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a  provably efficient algorithm for batch Bayesian Optimization</h3>
<p><a href='http://arxiv.org/abs/2403.04764v1'>http://arxiv.org/abs/2403.04764v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new batch Bayesian Optimization method using Thompson Sampling that minimizes redundancy and has low regret, and shows superior performance on nonconvex test functions.</p><hr><h3>BloomGML: Graph Machine Learning through the Lens of Bilevel  Optimization</h3>
<p><a href='http://arxiv.org/abs/2403.04763v1'>http://arxiv.org/abs/2403.04763v1</a></p>
<p><b>Compressor summary</b>: The paper shows how different graph learning techniques can be viewed as special cases or simplifications of bilevel optimization and presents a flexible class of energy functions for GNN message-passing layers with residual error analysis, called BloomGML.</p><hr><h3>Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing</h3>
<p><a href='http://arxiv.org/abs/2403.04759v1'>http://arxiv.org/abs/2403.04759v1</a></p>
<p><b>Compressor summary</b>: The paper introduces LifeHD, an on-device lifelong learning system for IoT devices using hyperdimensional computing, which improves unsupervised clustering accuracy and energy efficiency compared to existing methods.</p><hr><h3>That's My Point: Compact Object-centric LiDAR Pose Estimation for  Large-scale Outdoor Localisation</h3>
<p><a href='http://arxiv.org/abs/2403.04755v1'>http://arxiv.org/abs/2403.04755v1</a></p>
<p><b>Compressor summary</b>: The paper presents a method to estimate 3D poses from LiDAR scans using minimal storage and clustering, while maintaining accurate localization with an object-matching network.</p><hr><h3>GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural  Networks</h3>
<p><a href='http://arxiv.org/abs/2403.04747v1'>http://arxiv.org/abs/2403.04747v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new aggregation function for graph neural networks (GNNs) that preserves expressivity and improves learning dynamics, potentially leading to self-normalizing GNNs.</p><hr><h3>LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error</h3>
<p><a href='http://arxiv.org/abs/2403.04746v1'>http://arxiv.org/abs/2403.04746v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a biologically inspired method called simulated trial and error (STE) that improves the accuracy and reliability of tool use by large language models, outperforming existing methods like GPT-4.</p><hr><h3>SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker  Assumptions</h3>
<p><a href='http://arxiv.org/abs/2403.04744v1'>http://arxiv.org/abs/2403.04744v1</a></p>
<p><b>Compressor summary</b>: The paper investigates NGCA's complexity in the SQ model and shows that only the moment-matching condition is necessary for hardness, not the chi-squared condition.</p><hr><h3>I Can't Believe It's Not Scene Flow!</h3>
<p><a href='http://arxiv.org/abs/2403.04739v1'>http://arxiv.org/abs/2403.04739v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new evaluation protocol for scene flow methods that accounts for object size, speed, and class, and demonstrates its effectiveness with a simple but powerful baseline method called TrackFlow.</p><hr><h3>SnapNTell: Enhancing Entity-Centric Visual Question Answering with  Retrieval Augmented Multimodal LLM</h3>
<p><a href='http://arxiv.org/abs/2403.04735v1'>http://arxiv.org/abs/2403.04735v1</a></p>
<p><b>Compressor summary</b>: SnapNTell is a new benchmark for entity-centric visual question answering that challenges large language models to provide accurate and detailed responses about various entities across 22 categories.</p><hr><h3>How Far Are We from Intelligent Visual Deductive Reasoning?</h3>
<p><a href='http://arxiv.org/abs/2403.04732v1'>http://arxiv.org/abs/2403.04732v1</a></p>
<p><b>Compressor summary</b>: The text explores vision-based deductive reasoning in VLMs using Raven's Progressive Matrices and finds that current state-of-the-art models struggle with understanding complex visual patterns.</p><hr><h3>Masked Capsule Autoencoders</h3>
<p><a href='http://arxiv.org/abs/2403.04724v1'>http://arxiv.org/abs/2403.04724v1</a></p>
<p><b>Compressor summary</b>: MCAE is a new Capsule Network model that uses self-supervised pretraining with masked image modelling to improve performance on complex data tasks.</p><hr><h3>Rethinking of Encoder-based Warm-start Methods in Hyperparameter  Optimization</h3>
<p><a href='http://arxiv.org/abs/2403.04720v1'>http://arxiv.org/abs/2403.04720v1</a></p>
<p><b>Compressor summary</b>: The research introduces a new encoder-based model for representing tabular datasets in meta-learning tasks, comparing it with Dataset2Vec and highlighting the importance of task-specific representations.</p><hr><h3>Common 7B Language Models Already Possess Strong Math Capabilities</h3>
<p><a href='http://arxiv.org/abs/2403.04706v1'>http://arxiv.org/abs/2403.04706v1</a></p>
<p><b>Compressor summary</b>: The paper demonstrates that a large language model can perform well on math benchmarks with proper pre-training and data scaling, but struggles to reliably generate correct answers without them.</p><hr><h3>ObjectCompose: Evaluating Resilience of Vision-Based Models on  Object-to-Background Compositional Changes</h3>
<p><a href='http://arxiv.org/abs/2403.04701v1'>http://arxiv.org/abs/2403.04701v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to generate diverse object-to-background changes using text-to-image, image-to-text, and image-to-segment models, and evaluates the robustness of vision-based models against these changes.</p><hr><h3>Delving into the Trajectory Long-tail Distribution for Muti-object  Tracking</h3>
<p><a href='http://arxiv.org/abs/2403.04700v1'>http://arxiv.org/abs/2403.04700v1</a></p>
<p><b>Compressor summary</b>: This paper explores the long-tail distribution issue in multiple object tracking data and proposes two data augmentation strategies to mitigate its effects on performance.</p><hr><h3>AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit  Detectors</h3>
<p><a href='http://arxiv.org/abs/2403.04697v1'>http://arxiv.org/abs/2403.04697v1</a></p>
<p><b>Compressor summary</b>: The paper proposes AUFormer, a new method for facial action unit detection using parameter-efficient transfer learning and a novel loss function, achieving state-of-the-art performance without extra data.</p><hr><h3>Fact-Checking the Output of Large Language Models via Token-Level  Uncertainty Quantification</h3>
<p><a href='http://arxiv.org/abs/2403.04696v1'>http://arxiv.org/abs/2403.04696v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to detect hallucinations and fact-check claims in large language models using token-level uncertainty scores, which improve over baselines and are comparable to external fact-checking tools.</p><hr><h3>Analysis of Systems' Performance in Natural Language Processing  Competitions</h3>
<p><a href='http://arxiv.org/abs/2403.04693v1'>http://arxiv.org/abs/2403.04693v1</a></p>
<p><b>Compressor summary</b>: The text describes a universal evaluation methodology for scientific and technological collaborative competitions, which can handle classification and regression problems, account for different difficulties, and provide more accurate performance comparisons.</p><hr><h3>PixArt-Î£: Weak-to-Strong Training of Diffusion Transformer for 4K  Text-to-Image Generation</h3>
<p><a href='http://arxiv.org/abs/2403.04692v1'>http://arxiv.org/abs/2403.04692v1</a></p>
<p><b>Compressor summary</b>: PixArt-\Sigma is a more efficient text-to-image diffusion model that generates higher quality 4K images with better alignment to user prompts and uses less data and parameters than previous models.</p><hr><h3>Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self  Attention at the Threadblock Level</h3>
<p><a href='http://arxiv.org/abs/2403.04690v1'>http://arxiv.org/abs/2403.04690v1</a></p>
<p><b>Compressor summary</b>: The paper proposes efficient kernels for neighborhood attention, a self-attention variant that limits token attention to nearby tokens, improving latency and reducing memory footprint.</p><hr><h3>Greater than the sum of its parts: The role of minority and majority  status in collaborative problem-solving communication</h3>
<p><a href='http://arxiv.org/abs/2403.04671v1'>http://arxiv.org/abs/2403.04671v1</a></p>
<p><b>Compressor summary</b>: The text explores how sociocognitive linguistic patterns differ by race/ethnicity and gender in collaborative problem-solving tasks and discusses the implications of diversity on communication and collaboration.</p><hr><h3>End-to-end Conditional Robust Optimization</h3>
<p><a href='http://arxiv.org/abs/2403.04670v1'>http://arxiv.org/abs/2403.04670v1</a></p>
<p><b>Compressor summary</b>: Contextual Optimization (CO) uses machine learning and optimization to solve uncertain decision problems, and a new approach called Conditional Robust Optimization (CRO) enhances safety and reliability by combining uncertainty quantification with robust optimization, achieving high quality conditional coverage using differentiable optimization methods.</p><hr><h3>The Social Impact of Generative AI: An Analysis on ChatGPT</h3>
<p><a href='http://arxiv.org/abs/2403.04667v1'>http://arxiv.org/abs/2403.04667v1</a></p>
<p><b>Compressor summary</b>: The paper examines the social impacts of ChatGPT and other generative AI models, considering both their benefits and risks for various sectors and proposing ways to promote ethical and human-centered AI development.</p><hr><h3>Telecom Language Models: Must They Be Large?</h3>
<p><a href='http://arxiv.org/abs/2403.04666v1'>http://arxiv.org/abs/2403.04666v1</a></p>
<p><b>Compressor summary</b>: This paper evaluates Phi-2, a small language model that can understand and answer questions about telecom standards with high accuracy by using a Retrieval-Augmented Generation approach.</p><hr><h3>Dynamic Cross Attention for Audio-Visual Person Verification</h3>
<p><a href='http://arxiv.org/abs/2403.04661v1'>http://arxiv.org/abs/2403.04661v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a Dynamic Cross-Attention (DCA) model for audio-visual identity verification that adapts to strong or weak complementary relationships between audio and visual features, achieving state-of-the-art results on Voxceleb1 dataset.</p><hr><h3>Chain of Thought Explanation for Dialogue State Tracking</h3>
<p><a href='http://arxiv.org/abs/2403.04656v1'>http://arxiv.org/abs/2403.04656v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a model called CoTE for dialogue state tracking that generates explanations to improve accuracy and reliability in slot value determination.</p><hr><h3>Audio-Visual Person Verification based on Recursive Fusion of Joint  Cross-Attention</h3>
<p><a href='http://arxiv.org/abs/2403.04654v1'>http://arxiv.org/abs/2403.04654v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a recursive cross-attentional model with BLSTMs for better audio-visual fusion in person verification, improving over unimodal systems.</p><hr><h3>Yi: Open Foundation Models by 01.AI</h3>
<p><a href='http://arxiv.org/abs/2403.04652v1'>http://arxiv.org/abs/2403.04652v1</a></p>
<p><b>Compressor summary</b>: The Yi model family is a series of multidimensional language and vision models that achieve strong performance on various benchmarks due to their high-quality data and super-computing infrastructure.</p><hr><h3>Context-Based Multimodal Fusion</h3>
<p><a href='http://arxiv.org/abs/2403.04650v1'>http://arxiv.org/abs/2403.04650v1</a></p>
<p><b>Compressor summary</b>: The Context-Based Multimodal Fusion model combines modality fusion and data distribution alignment to solve complex multimodal tasks with reduced computational and training data requirements.</p><hr><h3>QAQ: Quality Adaptive Quantization for LLM KV Cache</h3>
<p><a href='http://arxiv.org/abs/2403.04643v1'>http://arxiv.org/abs/2403.04643v1</a></p>
<p><b>Compressor summary</b>: QAQ is a novel compression scheme that adapts to key and value caches in NLP models, allowing for more efficient deployment of LLMs with minimal performance loss.</p><hr><h3>Teaching Large Language Models to Reason with Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2403.04642v1'>http://arxiv.org/abs/2403.04642v1</a></p>
<p><b>Compressor summary</b>: The paper compares different algorithms that use reinforcement learning from human feedback to improve large language models' reasoning capabilities and finds Expert Iteration performs best with similar sample complexity to PPO.</p><hr><h3>CAT: Enhancing Multimodal Large Language Model to Answer Questions in  Dynamic Audio-Visual Scenarios</h3>
<p><a href='http://arxiv.org/abs/2403.04640v1'>http://arxiv.org/abs/2403.04640v1</a></p>
<p><b>Compressor summary</b>: The paper presents CAT, a method that enhances MLLMs for answering questions in complex audio-visual scenarios by aggregating clues, using a mixed multimodal dataset, and optimizing for non-ambiguity responses.</p><hr><h3>MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis</h3>
<p><a href='http://arxiv.org/abs/2403.04639v1'>http://arxiv.org/abs/2403.04639v1</a></p>
<p><b>Compressor summary</b>: The paper introduces MaCMS, the first Magahi-Hindi-English code-mixed sentiment analysis dataset, and analyzes its structure, language preferences, and quality.</p><hr><h3>Entropy Aware Message Passing in Graph Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2403.04636v1'>http://arxiv.org/abs/2403.04636v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an entropy-aware message passing term for GNNs that prevents oversmoothing by preserving node diversity during aggregation.</p><hr><h3>Pix2Gif: Motion-Guided Diffusion for GIF Generation</h3>
<p><a href='http://arxiv.org/abs/2403.04634v1'>http://arxiv.org/abs/2403.04634v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Pix2Gif, a model that generates GIFs from images using text and motion guidance, and uses a new warping module and perceptual loss to ensure quality and coherence.</p><hr><h3>Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI  Collaboration</h3>
<p><a href='http://arxiv.org/abs/2403.04629v1'>http://arxiv.org/abs/2403.04629v1</a></p>
<p><b>Compressor summary</b>: ShapleyBO is a framework that uses game-theoretic Shapley values to interpret and improve Bayesian optimization, enabling better exploration and personalization of wearable robotic devices by humans.</p><hr><h3>In-n-Out: Calibrating Graph Neural Networks for Link Prediction</h3>
<p><a href='http://arxiv.org/abs/2403.04605v1'>http://arxiv.org/abs/2403.04605v1</a></p>
<p><b>Compressor summary</b>: IN-N-OUT is a new method to improve the calibration of graph neural networks (GNNs) for predicting links by labeling edges with true/false labels based on GNN predictions, which leads to better embeddings and more accurate probabilities.</p><hr><h3>Contrastive Continual Learning with Importance Sampling and  Prototype-Instance Relation Distillation</h3>
<p><a href='http://arxiv.org/abs/2403.04599v1'>http://arxiv.org/abs/2403.04599v1</a></p>
<p><b>Compressor summary</b>: The paper proposes CCLIS, a method that uses importance sampling to select replay buffers and prototype-instance relation distillation to maintain knowledge, which improves continual learning by reducing catastrophic forgetting.</p><hr><h3>Embodied Understanding of Driving Scenarios</h3>
<p><a href='http://arxiv.org/abs/2403.04593v1'>http://arxiv.org/abs/2403.04593v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Embodied Language Model (ELM), a framework that enables autonomous agents to understand driving scenes with large spatial and temporal spans by incorporating space-aware pre-training and time-aware token selection.</p><hr><h3>Zero-shot cross-modal transfer of Reinforcement Learning policies  through a Global Workspace</h3>
<p><a href='http://arxiv.org/abs/2403.04588v1'>http://arxiv.org/abs/2403.04588v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Humans can create mental representations from multiple senses and generalize information across domains
- Robotics and RL agents struggle to exploit sensor redundancy and complementarity for robustness and generalization
- A brain-inspired multimodal representation called 'Global Workspace' combines information across modalities and transfers policies between them without extra training

Summary:
The paper proposes a 'Global Workspace', a multimodal representation inspired by the human brain, that can improve RL agents' robustness and generalization by combining and transferring information across different sensors.</p><hr><h3>Unbiased Estimator for Distorted Conics in Camera Calibration</h3>
<p><a href='http://arxiv.org/abs/2403.04583v1'>http://arxiv.org/abs/2403.04583v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for camera calibration using conic features based on moments that can overcome distortion limitations and improve accuracy.</p><hr><h3>Beyond Major Product Prediction: Reproducing Reaction Mechanisms with  Machine Learning Models Trained on a Large-Scale Mechanistic Dataset</h3>
<p><a href='http://arxiv.org/abs/2403.04580v1'>http://arxiv.org/abs/2403.04580v1</a></p>
<p><b>Compressor summary</b>: The authors create a large dataset of organic reaction intermediates and train machine learning models to predict reaction pathways, impurities, and roles of catalysts and reagents.</p><hr><h3>Wiki-TabNER:Advancing Table Interpretation Through Named Entity  Recognition</h3>
<p><a href='http://arxiv.org/abs/2403.04577v1'>http://arxiv.org/abs/2403.04577v1</a></p>
<p><b>Compressor summary</b>: The paper analyses a benchmark dataset for table interpretation tasks, finds it too simple, creates a new more realistic dataset, introduces a novel entity linking problem, and proposes a prompting framework to evaluate large language models on this task.</p><hr><h3>Machine learning and information theory concepts towards an AI  Mathematician</h3>
<p><a href='http://arxiv.org/abs/2403.04571v1'>http://arxiv.org/abs/2403.04571v1</a></p>
<p><b>Compressor summary</b>: The text explores how deep learning lacks reasoning and uncertainty estimation skills compared to human mathematicians, and proposes using information theory to discover interesting conjectures in mathematics.</p><hr><h3>Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit  Feedback and Unknown Transition</h3>
<p><a href='http://arxiv.org/abs/2403.04568v1'>http://arxiv.org/abs/2403.04568v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm for linear mixture MDPs that achieves better regret than previous methods by using a novel least square estimator and self-normalized concentration.</p><hr><h3>Out of the Room: Generalizing Event-Based Dynamic Motion Segmentation  for Complex Scenes</h3>
<p><a href='http://arxiv.org/abs/2403.04562v1'>http://arxiv.org/abs/2403.04562v1</a></p>
<p><b>Compressor summary</b>: The authors propose an event-based method for class-agnostic motion segmentation that works in complex large-scale outdoor environments and achieves state-of-the-art results on indoor and outdoor benchmarks.</p><hr><h3>Reducing self-supervised learning complexity improves weakly-supervised  classification performance in computational pathology</h3>
<p><a href='http://arxiv.org/abs/2403.04558v1'>http://arxiv.org/abs/2403.04558v1</a></p>
<p><b>Compressor summary</b>: The authors explore self-supervised learning methods for breast cancer diagnosis using consumer-grade hardware and show that they can improve classification performance while reducing training time by 90%.</p><hr><h3>Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness  Characterization Methods for Data-Centric AI</h3>
<p><a href='http://arxiv.org/abs/2403.04551v1'>http://arxiv.org/abs/2403.04551v1</a></p>
<p><b>Compressor summary</b>: The text introduces a fine-grained taxonomy of hardness types and a toolkit to benchmark Hardness Characterization Methods (HCMs) for ML model development.</p><hr><h3>Explainable Face Verification via Feature-Guided Gradient  Backpropagation</h3>
<p><a href='http://arxiv.org/abs/2403.04549v1'>http://arxiv.org/abs/2403.04549v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new explanation approach for face recognition systems called FGGB, which generates precise saliency maps to interpret the system's decisions.</p><hr><h3>CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?</h3>
<p><a href='http://arxiv.org/abs/2403.04547v1'>http://arxiv.org/abs/2403.04547v1</a></p>
<p><b>Compressor summary</b>: The study presents a novel algorithm (M4) to reduce biases in CLIP models by balancing data and analyzes its effects on different factors and metrics.</p><hr><h3>Improve Generalization Ability of Deep Wide Residual Network with A  Suitable Scaling Factor</h3>
<p><a href='http://arxiv.org/abs/2403.04545v1'>http://arxiv.org/abs/2403.04545v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how to choose a scaling factor ($\alpha$) for ResNets to avoid generalization issues and improve performance on various tasks.</p><hr><h3>Towards Automatic Composition of ASP Programs from Natural Language  Specifications</h3>
<p><a href='http://arxiv.org/abs/2403.04541v1'>http://arxiv.org/abs/2403.04541v1</a></p>
<p><b>Compressor summary</b>: The paper presents a two-step system for generating ASP programs from natural language, using neural machine translation and CNL2ASP tool.</p><hr><h3>Hyperspectral unmixing for Raman spectroscopy via physics-constrained  autoencoders</h3>
<p><a href='http://arxiv.org/abs/2403.04526v1'>http://arxiv.org/abs/2403.04526v1</a></p>
<p><b>Compressor summary</b>: Autoencoder neural networks improve unmixing accuracy, robustness, and efficiency in hyperspectral Raman spectroscopy for chemical composition analysis.</p><hr><h3>T-TAME: Trainable Attention Mechanism for Explaining Convolutional  Networks and Vision Transformers</h3>
<p><a href='http://arxiv.org/abs/2403.04523v1'>http://arxiv.org/abs/2403.04523v1</a></p>
<p><b>Compressor summary</b>: T-TAME is a method to generate explanations for Vision Transformers and other neural networks in image classification tasks, achieving state-of-the-art performance with less computation than existing techniques.</p><hr><h3>Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge  Graph Completion</h3>
<p><a href='http://arxiv.org/abs/2403.04521v1'>http://arxiv.org/abs/2403.04521v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework, UFKGC, for uncertainity-aware few-shot knowledge graph completion that models uncertainty using Gaussian distributions and improves robustness to noises.</p><hr><h3>Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video  Recommendation</h3>
<p><a href='http://arxiv.org/abs/2403.04511v1'>http://arxiv.org/abs/2403.04511v1</a></p>
<p><b>Compressor summary</b>: The study explores deep filter bubbles on short-video platforms, how they evolve over time, and what factors influence them, while suggesting ways to mitigate their negative effects.</p><hr><h3>Where does In-context Translation Happen in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.04510v1'>http://arxiv.org/abs/2403.04510v1</a></p>
<p><b>Compressor summary</b>: The study explores the point where large language models transition from learning in context to translating, and finds a "task recognition" layer where attention to context is no longer needed.</p><hr><h3>Finding Waldo: Towards Efficient Exploration of NeRF Scene Space</h3>
<p><a href='http://arxiv.org/abs/2403.04508v1'>http://arxiv.org/abs/2403.04508v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new concept of scene exploration with NeRFs, proposes three methods to efficiently discover inputs for novel view synthesis, and shows that the proposed EGPS method outperforms baselines.</p><hr><h3>NLPre: a revised approach towards language-centric benchmarking of  Natural Language Preprocessing systems</h3>
<p><a href='http://arxiv.org/abs/2403.04507v1'>http://arxiv.org/abs/2403.04507v1</a></p>
<p><b>Compressor summary</b>: The text introduces a novel method for evaluating natural language preprocessing tools using a benchmarking system inspired by GLUE and applied to Polish.</p><hr><h3>Improving Matrix Completion by Exploiting Rating Ordinality in Graph  Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2403.04504v1'>http://arxiv.org/abs/2403.04504v1</a></p>
<p><b>Compressor summary</b>: ROGMC is a new method that uses cumulative preference propagation and interest regularization to incorporate rating ordinality in graph neural networks for matrix completion, leading to better recommendations.</p><hr><h3>What makes an image realistic?</h3>
<p><a href='http://arxiv.org/abs/2403.04493v1'>http://arxiv.org/abs/2403.04493v1</a></p>
<p><b>Compressor summary</b>: The text discusses the challenges of quantifying realism in generated data, proposes a new concept called universal critic, and argues that it is different from adversarial critics.</p><hr><h3>Discriminative Sample-Guided and Parameter-Efficient Feature Space  Adaptation for Cross-Domain Few-Shot Learning</h3>
<p><a href='http://arxiv.org/abs/2403.04492v1'>http://arxiv.org/abs/2403.04492v1</a></p>
<p><b>Compressor summary</b>: The paper proposes two improvements for cross-domain few-shot classification: a parameter-efficient adaptation strategy and a variance-aware loss function, achieving better accuracy and efficiency than existing methods.</p><hr><h3>Source Matters: Source Dataset Impact on Model Robustness in Medical  Imaging</h3>
<p><a href='http://arxiv.org/abs/2403.04484v1'>http://arxiv.org/abs/2403.04484v1</a></p>
<p><b>Compressor summary</b>: The study compares the performance of ImageNet and RadImageNet in medical imaging classification, finding that both achieve similar results but ImageNet overfits more to confounders.</p><hr><h3>GraphInstruct: Empowering Large Language Models with Graph Understanding  and Reasoning Capability</h3>
<p><a href='http://arxiv.org/abs/2403.04483v1'>http://arxiv.org/abs/2403.04483v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a benchmark called GraphInstruct to evaluate and enhance the graph understanding abilities of large language models (LLMs)
- The paper constructs two LLM variants, GraphLM and GraphLM+, by instruction-tuning and step mask training respectively
- The paper shows that GraphLM and GraphLM+ outperform other LLMs on 21 classical graph reasoning tasks
- The paper releases the code for generating GraphInstruct publicly

Summary:
The paper introduces GraphInstruct, a benchmark to test and improve LLMs' graph understanding skills, and two enhanced LLM variants that excel on various graph reasoning tasks.</p><hr><h3>On the Topology Awareness and Generalization Performance of Graph Neural  Networks</h3>
<p><a href='http://arxiv.org/abs/2403.04482v1'>http://arxiv.org/abs/2403.04482v1</a></p>
<p><b>Compressor summary</b>: This paper characterizes the topology awareness of graph neural networks (GNNs) and studies its impact on generalization performance and fairness, showing that improving topology awareness can cause unfair generalization in some cases.</p><hr><h3>Do Large Language Model Understand Multi-Intent Spoken Language ?</h3>
<p><a href='http://arxiv.org/abs/2403.04481v1'>http://arxiv.org/abs/2403.04481v1</a></p>
<p><b>Compressor summary</b>: The study uses Large Language Models for understanding spoken language with multiple intentions, creating new datasets and metrics to evaluate their performance.</p><hr><h3>Hyperparameter Tuning MLPs for Probabilistic Time Series Forecasting</h3>
<p><a href='http://arxiv.org/abs/2403.04477v1'>http://arxiv.org/abs/2403.04477v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how specific hyperparameters affect MLP performance in time series forecasting and introduces a large new dataset for this task.</p><hr><h3>TextMonkey: An OCR-Free Large Multimodal Model for Understanding  Document</h3>
<p><a href='http://arxiv.org/abs/2403.04473v1'>http://arxiv.org/abs/2403.04473v1</a></p>
<p><b>Compressor summary</b>: TextMonkey is a text-centric large multimodal model that enhances performance and interpretability in tasks like DocVQA and scene text analysis with improvements on several benchmark datasets.</p><hr><h3>The Shutdown Problem: Three Theorems</h3>
<p><a href='http://arxiv.org/abs/2403.04471v1'>http://arxiv.org/abs/2403.04471v1</a></p>
<p><b>Compressor summary</b>: The shutdown problem explores how to create artificial agents that can competently pursue goals and still shut down when needed without trying to prevent or cause the shutdown button to be pressed, with trade-offs depending on the agent's patience.</p><hr><h3>A Survey of Graph Neural Networks in Real world: Imbalance, Noise,  Privacy and OOD Challenges</h3>
<p><a href='http://arxiv.org/abs/2403.04468v1'>http://arxiv.org/abs/2403.04468v1</a></p>
<p><b>Compressor summary</b>: The text surveys existing Graph Neural Network (GNN) models that address challenges like data imbalance, noise, privacy, and out-of-distribution scenarios to improve their reliability and robustness in real-world applications.</p><hr><h3>Pearl: A Review-driven Persona-Knowledge Grounded Conversational  Recommendation Dataset</h3>
<p><a href='http://arxiv.org/abs/2403.04460v1'>http://arxiv.org/abs/2403.04460v1</a></p>
<p><b>Compressor summary</b>: PEARL is a new conversational recommendation dataset with detailed persona and knowledge that improves recommendation quality and relevance.</p><hr><h3>Low-Resource Court Judgment Summarization for Common Law Systems</h3>
<p><a href='http://arxiv.org/abs/2403.04454v1'>http://arxiv.org/abs/2403.04454v1</a></p>
<p><b>Compressor summary</b>: The authors present CLSum, a dataset for summarizing multi-jurisdictional common law court judgments, and propose LLM-based methods for data augmentation, summary generation, and evaluation.</p><hr><h3>Vlearn: Off-Policy Learning with Efficient State-Value Function  Estimation</h3>
<p><a href='http://arxiv.org/abs/2403.04453v1'>http://arxiv.org/abs/2403.04453v1</a></p>
<p><b>Compressor summary</b>: Vlearn is an off-policy reinforcement learning algorithm that uses only a state-value function as the critic, making it efficient and robust in high-dimensional action spaces.</p><hr><h3>Feedback-Generation for Programming Exercises With GPT-4</h3>
<p><a href='http://arxiv.org/abs/2403.04449v1'>http://arxiv.org/abs/2403.04449v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates GPT-4 Turbo's ability to provide feedback for student programming submissions, finding improvements over GPT-3.5 in correctness and structure but also noting some inconsistencies.</p><hr><h3>FRRI: a novel algorithm for fuzzy-rough rule induction</h3>
<p><a href='http://arxiv.org/abs/2403.04447v1'>http://arxiv.org/abs/2403.04447v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel fuzzy rough rule induction algorithm (FRRI) that creates interpretable white box models by combining fuzzy and rough set theory, and shows its superior performance in accuracy and rule length compared to other methods.</p><hr><h3>Classist Tools: Social Class Correlates with Performance in NLP</h3>
<p><a href='http://arxiv.org/abs/2403.04445v1'>http://arxiv.org/abs/2403.04445v1</a></p>
<p><b>Compressor summary</b>: The text discusses how sociodemographic characteristics like socioeconomic status, ethnicity, and geography affect NLP performance and calls for their inclusion in future language technologies to avoid disadvantaging less-privileged groups.</p><hr><h3>Disentangled Diffusion-Based 3D Human Pose Estimation with Hierarchical  Spatial and Temporal Denoiser</h3>
<p><a href='http://arxiv.org/abs/2403.04444v1'>http://arxiv.org/abs/2403.04444v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method (DDHPose) for 3D human pose estimation that disentangles the pose into length and direction, uses a hierarchical denoiser to model joint spatial and temporal information, and improves performance over previous methods.</p><hr><h3>FriendNet: Detection-Friendly Dehazing Network</h3>
<p><a href='http://arxiv.org/abs/2403.04443v1'>http://arxiv.org/abs/2403.04443v1</a></p>
<p><b>Compressor summary</b>: The paper proposes FriendNet, a novel architecture that combines image restoration and object detection to improve performance in adverse weather conditions for autonomous driving systems.</p><hr><h3>Cooperative Bayesian Optimization for Imperfect Agents</h3>
<p><a href='http://arxiv.org/abs/2403.04442v1'>http://arxiv.org/abs/2403.04442v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a collaborative Bayesian optimization problem where two agents work together to optimize a black-box function with each controlling one variable, using strategic planning and a user model to find the global maximum efficiently.</p><hr><h3>StableDrag: Stable Dragging for Point-based Image Editing</h3>
<p><a href='http://arxiv.org/abs/2403.04437v1'>http://arxiv.org/abs/2403.04437v1</a></p>
<p><b>Compressor summary</b>: StableDrag is a new framework that improves point-based image editing by using a discriminative point tracking method and a confidence-based latent enhancement strategy to address the issues of inaccurate tracking and incomplete supervision.</p><hr><h3>Exploring the Influence of Dimensionality Reduction on Anomaly Detection  Performance in Multivariate Time Series</h3>
<p><a href='http://arxiv.org/abs/2403.04429v1'>http://arxiv.org/abs/2403.04429v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates how dimensionality reduction techniques improve unsupervised time series anomaly detection models' performance and efficiency across different datasets.</p><hr><h3>Promising and worth-to-try future directions for advancing  state-of-the-art surrogates methods of agent-based models in social and  health computational sciences</h3>
<p><a href='http://arxiv.org/abs/2403.04417v1'>http://arxiv.org/abs/2403.04417v1</a></p>
<p><b>Compressor summary</b>: The text discusses how to use surrogate models to reduce computational costs and increase efficiency for large-scale Agent-Based Models (ABMs) in Social Health Computational Sciences.</p><hr><h3>Exploring Continual Learning of Compositional Generalization in NLI</h3>
<p><a href='http://arxiv.org/abs/2403.04400v1'>http://arxiv.org/abs/2403.04400v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new challenge (C2Gen NLI) to test neural models' compositional inference abilities under continual learning, and analyzes how different algorithms and subtask ordering affect performance.</p><hr><h3>MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality  Assessment</h3>
<p><a href='http://arxiv.org/abs/2403.04398v1'>http://arxiv.org/abs/2403.04398v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method called MAGR to reduce forgetting in continual assessment of diverse skills by aligning old and new features with quality scores.</p><hr><h3>Impacts of Color and Texture Distortions on Earth Observation Data in  Deep Learning</h3>
<p><a href='http://arxiv.org/abs/2403.04385v1'>http://arxiv.org/abs/2403.04385v1</a></p>
<p><b>Compressor summary</b>: This paper studies how different visual characteristics of input Earth observation data affect land cover classification models' performance and finds that texture distortions have a greater impact than color distortions.</p><hr><h3>Acceleron: A Tool to Accelerate Research Ideation</h3>
<p><a href='http://arxiv.org/abs/2403.04382v1'>http://arxiv.org/abs/2403.04382v1</a></p>
<p><b>Compressor summary</b>: Acceleron is a tool that uses large language models to help researchers formulate novel research proposals and validate their motivation by identifying gaps in existing literature.</p><hr><h3>Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation</h3>
<p><a href='http://arxiv.org/abs/2403.04381v1'>http://arxiv.org/abs/2403.04381v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to adapt single-view hand pose estimation models to dual views without extra annotations, allowing them to work with different camera settings.</p><hr><h3>Video-Driven Animation of Neural Head Avatars</h3>
<p><a href='http://arxiv.org/abs/2403.04380v1'>http://arxiv.org/abs/2403.04380v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to animate realistic 3D head models using video input without requiring personal information, by using a neural network that translates expression features into animation parameters.</p><hr><h3>Computational Modelling of Plurality and Definiteness in Chinese Noun  Phrases</h3>
<p><a href='http://arxiv.org/abs/2403.04376v1'>http://arxiv.org/abs/2403.04376v1</a></p>
<p><b>Compressor summary</b>: The paper studies how Chinese speakers omit noun markers based on context, builds a corpus with annotations, and tests various machine learning models to predict the missing markers' meanings.</p><hr><h3>From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge  Prediction</h3>
<p><a href='http://arxiv.org/abs/2403.04369v1'>http://arxiv.org/abs/2403.04369v1</a></p>
<p><b>Compressor summary</b>: The FWGB approach uses a legal knowledge graph and attention mechanism to predict confusing criminal charges by focusing on constituent elements that distinguish them.</p><hr><h3>Learning to Remove Wrinkled Transparent Film with Polarized Prior</h3>
<p><a href='http://arxiv.org/abs/2403.04368v1'>http://arxiv.org/abs/2403.04368v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to remove wrinkled transparent films from images using polarized cameras and neural networks, improving image quality and industrial recognition systems performance.</p><hr><h3>Enhancing Court View Generation with Knowledge Injection and Guidance</h3>
<p><a href='http://arxiv.org/abs/2403.04366v1'>http://arxiv.org/abs/2403.04366v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel Knowledge Injection and Guidance (KIG) approach to improve court view generation using pretrained language models, achieving better results especially in handling responsive claims.</p><hr><h3>Multi-step Temporal Modeling for UAV Tracking</h3>
<p><a href='http://arxiv.org/abs/2403.04363v1'>http://arxiv.org/abs/2403.04363v1</a></p>
<p><b>Compressor summary</b>: MT-Track is a new efficient and streamlined framework for UAV tracking that uses temporal modeling in two steps: correlation map generation and refinement, to handle challenges like fast motion and small objects.</p><hr><h3>Spatiotemporal Pooling on Appropriate Topological Maps Represented as  Two-Dimensional Images for EEG Classification</h3>
<p><a href='http://arxiv.org/abs/2403.04353v1'>http://arxiv.org/abs/2403.04353v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new EEG-based motor imagery classification method using topological maps, spatial features, and spatiotemporal pooling to improve accuracy.</p><hr><h3>CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction  Tuning</h3>
<p><a href='http://arxiv.org/abs/2403.04343v1'>http://arxiv.org/abs/2403.04343v1</a></p>
<p><b>Compressor summary</b>: The text proposes a novel algorithm for balancing tasks when tuning large multimodal models using visual instructions.</p><hr><h3>Explainable AI for Embedded Systems Design: A Case Study of Static  Redundant NVM Memory Write Prediction</h3>
<p><a href='http://arxiv.org/abs/2403.04337v1'>http://arxiv.org/abs/2403.04337v1</a></p>
<p><b>Compressor summary</b>: The paper explores using eXplainable Artificial Intelligence (XAI) to identify redundant memory writes in embedded systems, which can improve performance and energy efficiency.</p><hr><h3>Measuring Meaning Composition in the Human Brain with Composition Scores  from Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.04325v1'>http://arxiv.org/abs/2403.04325v1</a></p>
<p><b>Compressor summary</b>: The Composition Score is a new model-based metric that measures how much smaller language units combine to form phrases and sentences' meanings, and it relates to brain regions involved in different aspects of this process.</p><hr><h3>Discriminative Probing and Tuning for Text-to-Image Generation</h3>
<p><a href='http://arxiv.org/abs/2403.04321v1'>http://arxiv.org/abs/2403.04321v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a discriminative adapter for text-to-image generation that improves alignment between generated images and text prompts by enhancing the model's discriminative abilities.</p><hr><h3>Online Adaptation of Language Models with a Memory of Amortized Contexts</h3>
<p><a href='http://arxiv.org/abs/2403.04317v1'>http://arxiv.org/abs/2403.04317v1</a></p>
<p><b>Compressor summary</b>: The paper introduces MAC, an efficient and effective online adaptation framework for large language models that uses amortized feature extraction and memory-augmentation to store new information and answer questions without gradient updates.</p><hr><h3>Can Your Model Tell a Negation from an Implicature? Unravelling  Challenges With Intent Encoders</h3>
<p><a href='http://arxiv.org/abs/2403.04314v1'>http://arxiv.org/abs/2403.04314v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new evaluation toolkit for conversational systems that assesses semantic understanding by measuring negation and implicature, and suggests pre-training with augmented data to improve embedding models.</p><hr><h3>ALTO: An Efficient Network Orchestrator for Compound AI Systems</h3>
<p><a href='http://arxiv.org/abs/2403.04311v1'>http://arxiv.org/abs/2403.04311v1</a></p>
<p><b>Compressor summary</b>: ALTO is a network orchestrator that improves the efficiency and performance of compound AI systems like language models by streaming intermediate outputs between stages.</p><hr><h3>AO-DETR: Anti-Overlapping DETR for X-Ray Prohibited Items Detection</h3>
<p><a href='http://arxiv.org/abs/2403.04309v1'>http://arxiv.org/abs/2403.04309v1</a></p>
<p><b>Compressor summary</b>: The paper proposes AO-DETR, a method to detect overlapping prohibited items in X-ray images using category-specific queries and edge localization, outperforming existing object detectors.</p><hr><h3>HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild</h3>
<p><a href='http://arxiv.org/abs/2403.04307v1'>http://arxiv.org/abs/2403.04307v1</a></p>
<p><b>Compressor summary</b>: HaluEval-Wild is a new benchmark for evaluating large language models' hallucinations in real-world user-LLM interactions by using challenging queries from existing datasets and categorizing them into five types.</p><hr><h3>Effectiveness Assessment of Recent Large Vision-Language Models</h3>
<p><a href='http://arxiv.org/abs/2403.04306v1'>http://arxiv.org/abs/2403.04306v1</a></p>
<p><b>Compressor summary</b>: This article evaluates the performance of large vision-language models (LVLMs) in specialized tasks like object detection and healthcare, as well as general tasks like reasoning and question answering, finding that they are not very effective in either domain.</p><hr><h3>LORS: Low-rank Residual Structure for Parameter-Efficient Network  Stacking</h3>
<p><a href='http://arxiv.org/abs/2403.04303v1'>http://arxiv.org/abs/2403.04303v1</a></p>
<p><b>Compressor summary</b>: LORS reduces parameter usage in deep learning models by allowing stacked modules to share most of their parameters.</p><hr><h3>A$^{3}$lign-DFER: Pioneering Comprehensive Dynamic Affective Alignment  for Dynamic Facial Expression Recognition with CLIP</h3>
<p><a href='http://arxiv.org/abs/2403.04294v1'>http://arxiv.org/abs/2403.04294v1</a></p>
<p><b>Compressor summary</b>: A$^{3}$lign-DFER is a method that aligns text and video in three aspects (affective, dynamic, and bidirectional) to improve CLIP's performance in recognizing facial expressions dynamically.</p><hr><h3>MKF-ADS: A Multi-Knowledge Fused Anomaly Detection System for Automotive</h3>
<p><a href='http://arxiv.org/abs/2403.04293v1'>http://arxiv.org/abs/2403.04293v1</a></p>
<p><b>Compressor summary</b>: The paper proposes MKF-IDS, an anomaly-based intrusion detection system for CAN bus in ITSs, using spatial-temporal correlation with attention mechanism and patch sparse-transformer modules to improve safety and security.</p><hr><h3>A challenge in A(G)I, cybernetics revived in the Ouroboros Model as one  algorithm for all thinking</h3>
<p><a href='http://arxiv.org/abs/2403.04292v1'>http://arxiv.org/abs/2403.04292v1</a></p>
<p><b>Compressor summary</b>: The paper discusses AI challenges in image categorization and generation, proposes incorporating cybernetics and analog control processes for improved cognition and abstraction, and introduces the Ouroboros Model as a versatile algorithmic backbone for general cognition.</p><hr><h3>Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model  with Proxy</h3>
<p><a href='http://arxiv.org/abs/2403.04283v1'>http://arxiv.org/abs/2403.04283v1</a></p>
<p><b>Compressor summary</b>: Proxy-RLHF is a new method that lowers the computational cost of aligning large language models with human values by decoupling generation and alignment processes using a proxy model trained with reinforcement learning.</p><hr><h3>A New Benchmark for Evaluating Automatic Speech Recognition in the  Arabic Call Domain</h3>
<p><a href='http://arxiv.org/abs/2403.04280v1'>http://arxiv.org/abs/2403.04280v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a comprehensive benchmark for Arabic speech recognition in telephone conversations, considering dialectal diversity and call quality challenges.</p><hr><h3>Controllable Generation with Text-to-Image Diffusion Models: A Survey</h3>
<p><a href='http://arxiv.org/abs/2403.04279v1'>http://arxiv.org/abs/2403.04279v1</a></p>
<p><b>Compressor summary</b>: The text reviews controllable generation with text-to-image diffusion models, covering their theoretical foundations and practical advancements in various condition categories.</p><hr><h3>Active Generalized Category Discovery</h3>
<p><a href='http://arxiv.org/abs/2403.04272v1'>http://arxiv.org/abs/2403.04272v1</a></p>
<p><b>Compressor summary</b>: AGCD uses adaptive sampling to select valuable novel samples for labeling, and a stable label mapping algorithm to ensure consistent training across different stages, improving GCD performance in the low-labeling regime.</p><hr><h3>Competitive Facility Location under Random Utilities and Routing  Constraints</h3>
<p><a href='http://arxiv.org/abs/2403.04264v1'>http://arxiv.org/abs/2403.04264v1</a></p>
<p><b>Compressor summary</b>: The paper studies a facility location problem with routing constraints in a competitive market, proposes new cuts for solving it, and develops exact and heuristic methods that outperform existing approaches.</p><hr><h3>Advancing Biomedical Text Mining with Community Challenges</h3>
<p><a href='http://arxiv.org/abs/2403.04261v1'>http://arxiv.org/abs/2403.04261v1</a></p>
<p><b>Compressor summary</b>: Biomedical text mining is a rapidly growing field that leverages advanced technology to analyze vast amounts of diverse text data from various sources, with community challenges promoting innovation and collaboration in Chinese biomedical research.</p><hr><h3>Depth-aware Test-Time Training for Zero-shot Video Object Segmentation</h3>
<p><a href='http://arxiv.org/abs/2403.04258v1'>http://arxiv.org/abs/2403.04258v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a test-time training method for zero-shot video object segmentation that predicts consistent depth maps and uses momentum-based weight initialization and looping-based training scheme to achieve better results.</p><hr><h3>Mastering Memory Tasks with World Models</h3>
<p><a href='http://arxiv.org/abs/2403.04253v1'>http://arxiv.org/abs/2403.04253v1</a></p>
<p><b>Compressor summary</b>: Recall to Imagine (R2I) integrates state space models into world models of reinforcement learning agents to improve long-term memory and credit assignment, achieving superhuman performance in complex memory tasks and faster convergence than DreamerV3.</p><hr><h3>UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed  Entities</h3>
<p><a href='http://arxiv.org/abs/2403.04247v1'>http://arxiv.org/abs/2403.04247v1</a></p>
<p><b>Compressor summary</b>: The paper introduces negative seed entities to improve Entity Set Expansion (ESE) for ultra-fine-grained semantic classes, proposes two frameworks to assess models in this task, and suggests three strategies to enhance model performance.</p><hr><h3>Regularized DeepIV with Model Selection</h3>
<p><a href='http://arxiv.org/abs/2403.04236v1'>http://arxiv.org/abs/2403.04236v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Regularized DeepIV (RDIV), a minimax-oracle-free method that avoids limitations in instrumental variable estimation using machine learning and provides rigorous guarantees for the popular DeepIV method with Tikhonov regularization.</p><hr><h3>DEEP-ICL: Definition-Enriched Experts for Language Model In-Context  Learning</h3>
<p><a href='http://arxiv.org/abs/2403.04233v1'>http://arxiv.org/abs/2403.04233v1</a></p>
<p><b>Compressor summary</b>: DEEP-ICL is a new method that uses task definitions to improve in-context learning without relying on large language models.</p><hr><h3>Single-Image HDR Reconstruction Assisted Ghost Suppression and Detail  Preservation Network for Multi-Exposure HDR Imaging</h3>
<p><a href='http://arxiv.org/abs/2403.04228v1'>http://arxiv.org/abs/2403.04228v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for reconstructing HDR images from multiple low dynamic range images in dynamic scenes, using single-frame HDR reconstruction and enhanced stop image techniques to preserve details and avoid ghosting artifacts.</p><hr><h3>3DTextureTransformer: Geometry Aware Texture Generation for Arbitrary  Mesh Topology</h3>
<p><a href='http://arxiv.org/abs/2403.04225v1'>http://arxiv.org/abs/2403.04225v1</a></p>
<p><b>Compressor summary</b>: The 3DTextureTransformer is a novel framework that generates high-quality textures for arbitrary mesh topologies using a hybrid approach of geometric deep learning and StyleGAN-like architecture, achieving state-of-the-art performance in this domain.</p><hr><h3>Aligners: Decoupling LLMs and Alignment</h3>
<p><a href='http://arxiv.org/abs/2403.04224v1'>http://arxiv.org/abs/2403.04224v1</a></p>
<p><b>Compressor summary</b>: The paper proposes to train aligner models that can quickly and safely align any large language model for a given criterion using synthetic data.</p><hr><h3>Self-Evaluation of Large Language Model based on Glass-box Features</h3>
<p><a href='http://arxiv.org/abs/2403.04222v1'>http://arxiv.org/abs/2403.04222v1</a></p>
<p><b>Compressor summary</b>: The study explores how open-source Large Language Models can evaluate their own output using glass-box features, such as softmax distribution, and incorporating reference features to improve quality evaluation.</p><hr><h3>Why Online Reinforcement Learning is Causal</h3>
<p><a href='http://arxiv.org/abs/2403.04221v1'>http://arxiv.org/abs/2403.04221v1</a></p>
<p><b>Compressor summary</b>: The paper explores how causal modelling can enhance reinforcement learning in online and offline settings, especially when learning from other agents' experiences.</p><hr><h3>Persona Extraction Through Semantic Similarity for Emotional Support  Conversation Generation</h3>
<p><a href='http://arxiv.org/abs/2403.04212v1'>http://arxiv.org/abs/2403.04212v1</a></p>
<p><b>Compressor summary</b>: The text proposes a new framework called PESS that can automatically infer persona from dialogues, which helps improve emotional support in conversations with chatbots.</p><hr><h3>GRAWA: Gradient-based Weighted Averaging for Distributed Training of  Deep Learning Models</h3>
<p><a href='http://arxiv.org/abs/2403.04206v1'>http://arxiv.org/abs/2403.04206v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm for fast distributed training of deep learning models with better convergence and quality, and shows its effectiveness in theoretical and experimental studies.</p><hr><h3>On the Essence and Prospect: An Investigation of Alignment Approaches  for Big Models</h3>
<p><a href='http://arxiv.org/abs/2403.04204v1'>http://arxiv.org/abs/2403.04204v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Big AI models have impressive results but also potential risks
- Alignment technologies aim to make models conform to human preferences and values
- Survey paper investigates value alignment approaches and their challenges, categories, connections, and frontiers

Summary:
The paper surveys various methods and issues related to aligning AI models with human values, exploring historical context, mathematical essence, existing techniques, emerging topics, and future directions.</p><hr><h3>ACC-ViT : Atrous Convolution's Comeback in Vision Transformers</h3>
<p><a href='http://arxiv.org/abs/2403.04200v1'>http://arxiv.org/abs/2403.04200v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Atrous Attention, a hybrid of regional and sparse attention, which adapts to local and global information while preserving hierarchical relations. It also presents ACC-ViT, a vision transformer backbone that achieves high accuracy on ImageNet-1K with fewer parameters than existing models.</p><hr><h3>CN-RMA: Combined Network with Ray Marching Aggregation for 3D Indoors  Object Detection from Multi-view Images</h3>
<p><a href='http://arxiv.org/abs/2403.04198v1'>http://arxiv.org/abs/2403.04198v1</a></p>
<p><b>Compressor summary</b>: The paper proposes CN-RMA, a new method for detecting 3D indoor objects from multiple images, by combining 3D reconstruction and object detection networks to handle occlusion issues.</p><hr><h3>Large Language Models are In-Context Molecule Learners</h3>
<p><a href='http://arxiv.org/abs/2403.04197v1'>http://arxiv.org/abs/2403.04197v1</a></p>
<p><b>Compressor summary</b>: ICMA is a new method for adapting large language models to the molecule-caption translation task using context examples, improving their performance without extra pre-training or data.</p><hr><h3>Fill-and-Spill: Deep Reinforcement Learning Policy Gradient Methods for  Reservoir Operation Decision and Control</h3>
<p><a href='http://arxiv.org/abs/2403.04195v1'>http://arxiv.org/abs/2403.04195v1</a></p>
<p><b>Compressor summary</b>: The text discusses applying deep reinforcement learning techniques to determine optimal reservoir operation policies, focusing on two methods that perform well for a case study of Folsom Reservoir in California.</p><hr><h3>SAM-PD: How Far Can SAM Take Us in Tracking and Segmenting Anything in  Videos by Prompt Denoising</h3>
<p><a href='http://arxiv.org/abs/2403.04194v1'>http://arxiv.org/abs/2403.04194v1</a></p>
<p><b>Compressor summary</b>: The paper explores using the Segment Anything Model (SAM) for video object segmentation by iteratively refining bounding box prompts to handle position, size, and occlusion variations.</p><hr><h3>Generative AI for Synthetic Data Generation: Methods, Challenges and the  Future</h3>
<p><a href='http://arxiv.org/abs/2403.04190v1'>http://arxiv.org/abs/2403.04190v1</a></p>
<p><b>Compressor summary</b>: This paper explores how large language models can create realistic synthetic data for low-resource AI challenges, detailing methods, evaluations, and applications, while acknowledging limitations and suggesting future directions.</p><hr><h3>YYDS: Visible-Infrared Person Re-Identification with Coarse Descriptions</h3>
<p><a href='http://arxiv.org/abs/2403.04183v1'>http://arxiv.org/abs/2403.04183v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Refer-VI-ReID, a method to match visible images using infrared samples and textual descriptions, with a new Y-Y-shape structure and a cross-modal re-ranking algorithm that improves performance on three datasets.</p><hr><h3>Metric-aware LLM inference</h3>
<p><a href='http://arxiv.org/abs/2403.04182v1'>http://arxiv.org/abs/2403.04182v1</a></p>
<p><b>Compressor summary</b>: Metric-aware LLM inference is a new method to optimize NLP task performance by adjusting inference strategies based on evaluation metrics.</p><hr><h3>RATSF: Empowering Customer Service Volume Management through  Retrieval-Augmented Time-Series Forecasting</h3>
<p><a href='http://arxiv.org/abs/2403.04180v1'>http://arxiv.org/abs/2403.04180v1</a></p>
<p><b>Compressor summary</b>: RATSF is a framework that uses RACA, a cross-attention module, to improve customer service volume forecasting by leveraging historical data effectively in non-stationary scenarios.</p><hr><h3>Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation</h3>
<p><a href='http://arxiv.org/abs/2403.04178v1'>http://arxiv.org/abs/2403.04178v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a TTS system that incorporates stress into translations, addressing the challenge of language diversity and accessibility in India's education sector.</p><hr><h3>Image Coding for Machines with Edge Information Learning Using Segment  Anything</h3>
<p><a href='http://arxiv.org/abs/2403.04173v1'>http://arxiv.org/abs/2403.04173v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a new method for image compression called SA-ICM that encodes and decodes only the edge information of object parts in an image
- SA-ICM is based on a LIC model trained using edge information created by Segment Anything, which can be applied to various image recognition tasks and video compression models
- SA-ICM has advantages in terms of privacy protection, robustness, and performance for image recognition and video compression

Summary:
The paper introduces SA-ICM, a novel image compression technique that preserves only the edge information of object parts, which improves image recognition and video compression tasks while protecting privacy.</p><hr><h3>SDPL: Shifting-Dense Partition Learning for UAV-View Geo-Localization</h3>
<p><a href='http://arxiv.org/abs/2403.04172v1'>http://arxiv.org/abs/2403.04172v1</a></p>
<p><b>Compressor summary</b>: SDPL is a part-based representation learning method for cross-view geo-localization that divides images into multiple parts to explore contextual information while maintaining global structure and handling position shifting and scale variations.</p><hr><h3>ProMISe: Promptable Medical Image Segmentation using SAM</h3>
<p><a href='http://arxiv.org/abs/2403.04164v1'>http://arxiv.org/abs/2403.04164v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve medical image segmentation using adaptive prompts and pattern shifting without fine-tuning the large SAM model, achieving competitive results.</p><hr><h3>Noisy Spiking Actor Network for Exploration</h3>
<p><a href='http://arxiv.org/abs/2403.04162v1'>http://arxiv.org/abs/2403.04162v1</a></p>
<p><b>Compressor summary</b>: NoisySAN is a novel exploration strategy for deep RL using spiking neural networks that introduces and reduces noise to achieve better performance on various tasks.</p><hr><h3>SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS</h3>
<p><a href='http://arxiv.org/abs/2403.04161v1'>http://arxiv.org/abs/2403.04161v1</a></p>
<p><b>Compressor summary</b>: SWAP-Score is a novel training-free metric that measures network expressivity and has strong correlation with performance, outperforming existing metrics in Neural Architecture Search.</p><hr><h3>DA-Net: A Disentangled and Adaptive Network for Multi-Source  Cross-Lingual Transfer Learning</h3>
<p><a href='http://arxiv.org/abs/2403.04158v1'>http://arxiv.org/abs/2403.04158v1</a></p>
<p><b>Compressor summary</b>: DA-Net is a new method to transfer knowledge from multiple languages to another language by disentangling inputs and adapting class distributions, improving performance on three tasks and 38 languages.</p><hr><h3>Stabilizing Policy Gradients for Stochastic Differential Equations via  Consistency with Perturbation Process</h3>
<p><a href='http://arxiv.org/abs/2403.04154v1'>http://arxiv.org/abs/2403.04154v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a method to train deep neural networks with policy gradient and SDEs for generating samples with high rewards.
- The method constrains the SDE to be consistent with its perturbation process, which covers the entire space and is easy to sample.
- The method improves stability and sample complexity of policy gradients and applies it to structure-based drug design.

Summary:
The paper presents a stable and efficient method to train deep neural networks with SDEs and policy gradient for generating high-reward samples, and demonstrates its effectiveness on structure-based drug design.</p><hr><h3>Dual-path Frequency Discriminators for Few-shot Anomaly Detection</h3>
<p><a href='http://arxiv.org/abs/2403.04151v1'>http://arxiv.org/abs/2403.04151v1</a></p>
<p><b>Compressor summary</b>: Key points:
- FSAD is important for industrial manufacturing but existing methods have limitations
- The paper proposes a DFD network that uses frequency domain to detect and locate subtle anomalies at image-level and feature-level
- DFD outperforms state-of-the-art methods on benchmarks

Summary:
The paper presents a novel FSAD method, DFD, that leverages frequency domain to detect and locate inconspicuous anomalies in industrial manufacturing using a dual-path feature discrimination module.</p><hr><h3>MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection</h3>
<p><a href='http://arxiv.org/abs/2403.04149v1'>http://arxiv.org/abs/2403.04149v1</a></p>
<p><b>Compressor summary</b>: The paper proposes MAP, a method to protect intellectual property of deep learning models by pruning target-related parameters without accessing unauthorized data.</p><hr><h3>Contrastive Augmented Graph2Graph Memory Interaction for Few Shot  Continual Learning</h3>
<p><a href='http://arxiv.org/abs/2403.04140v1'>http://arxiv.org/abs/2403.04140v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new method for FSCIL that uses G2G interaction to preserve local geometric structure and mitigate catastrophic forgetting in few-shot learning.</p><hr><h3>Unsupervised Learning of Harmonic Analysis Based on Neural HSMM with  Code Quality Templates</h3>
<p><a href='http://arxiv.org/abs/2403.04135v1'>http://arxiv.org/abs/2403.04135v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for unsupervised learning of harmonic analysis using a hidden semi-Markov model and chord quality templates, which can recognize tonic without prior knowledge.</p><hr><h3>Towards learning-based planning:The nuPlan benchmark for real-world  autonomous driving</h3>
<p><a href='http://arxiv.org/abs/2403.04133v1'>http://arxiv.org/abs/2403.04133v1</a></p>
<p><b>Compressor summary</b>: nuPlan is a new dataset and benchmark for testing machine learning-based planners in autonomous vehicles across diverse driving situations.</p><hr><h3>Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</h3>
<p><a href='http://arxiv.org/abs/2403.04132v1'>http://arxiv.org/abs/2403.04132v1</a></p>
<p><b>Compressor summary</b>: Chatbot Arena is an open platform that evaluates large language models based on human preferences using pairwise comparisons and crowdsourcing, providing a credible and widely cited leaderboard for LLMs.</p><hr><h3>An Explainable AI Framework for Artificial Intelligence of Medical  Things</h3>
<p><a href='http://arxiv.org/abs/2403.04130v1'>http://arxiv.org/abs/2403.04130v1</a></p>
<p><b>Compressor summary</b>: The text describes a custom Explainable Artificial Intelligence framework that uses multiple techniques to improve transparency and accuracy in healthcare systems, particularly in brain tumor detection.</p><hr><h3>Scalable and Robust Transformer Decoders for Interpretable Image  Classification with Foundation Models</h3>
<p><a href='http://arxiv.org/abs/2403.04125v1'>http://arxiv.org/abs/2403.04125v1</a></p>
<p><b>Compressor summary</b>: ComFe is an interpretable image classification approach that uses transformer-decoder and mixture modelling to identify and use consistent image components for accurate predictions.</p><hr><h3>Privacy-preserving Fine-tuning of Large Language Models through Flatness</h3>
<p><a href='http://arxiv.org/abs/2403.04124v1'>http://arxiv.org/abs/2403.04124v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework to improve both privacy and generalization of large language models by controlling the flatness of their loss landscape.</p><hr><h3>Can Large Language Models Reason and Plan?</h3>
<p><a href='http://arxiv.org/abs/2403.04121v1'>http://arxiv.org/abs/2403.04121v1</a></p>
<p><b>Compressor summary</b>: LLMs do not seem to be able to correct their own mistakes like humans can.</p><hr><h3>A data-centric approach to class-specific bias in image data  augmentation</h3>
<p><a href='http://arxiv.org/abs/2403.04120v1'>http://arxiv.org/abs/2403.04120v1</a></p>
<p><b>Compressor summary</b>: Data augmentation improves computer vision models but may introduce class-specific biases; this study examines these biases across various datasets and model types, suggesting a nuanced approach to model selection and a refined method for managing DA-induced biases.</p>