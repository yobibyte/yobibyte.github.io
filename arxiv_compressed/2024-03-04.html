
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-03-04</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-03-04 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>DistriFusion: Distributed Parallel Inference for High-Resolution  Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2402.19481v1'>http://arxiv.org/abs/2402.19481v1</a></p>
<p><b>Compressor summary</b>: DistriFusion is a method that speeds up image synthesis with diffusion models by using parallelism, asynchronous communication, and context reuse across multiple GPUs without sacrificing quality.</p><hr><h3>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</h3>
<p><a href='http://arxiv.org/abs/2402.19479v1'>http://arxiv.org/abs/2402.19479v1</a></p>
<p><b>Compressor summary</b>: The authors propose an automatic approach to create a large video dataset with high-quality captions using multimodal inputs and cross-modality teacher models, and show its effectiveness on three downstream tasks.</p><hr><h3>Learning a Generalized Physical Face Model From Data</h3>
<p><a href='http://arxiv.org/abs/2402.19477v1'>http://arxiv.org/abs/2402.19477v1</a></p>
<p><b>Compressor summary</b>: The paper presents a simulation-free method for learning a generalized physical face model from 3D face data, enabling easy fitting to any identity and realistic physics-based facial animation with minimal artist input and network training.</p><hr><h3>The All-Seeing Project V2: Towards General Relation Comprehension of the  Open World</h3>
<p><a href='http://arxiv.org/abs/2402.19474v1'>http://arxiv.org/abs/2402.19474v1</a></p>
<p><b>Compressor summary</b>: The All-Seeing Project V2 introduces a new model and dataset for understanding object relations in images, improving relation comprehension in multi-modal large language models.</p><hr><h3>Retrieval-Augmented Generation for AI-Generated Content: A Survey</h3>
<p><a href='http://arxiv.org/abs/2402.19473v1'>http://arxiv.org/abs/2402.19473v1</a></p>
<p><b>Compressor summary</b>: The paper reviews retrieval-augmented generation (RAG), a technique that enhances artificial intelligence generated content (AIGC) by integrating information retrieval to improve accuracy and robustness, and surveys its applications, benchmarks, and future research directions.</p><hr><h3>Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid  Progress</h3>
<p><a href='http://arxiv.org/abs/2402.19472v1'>http://arxiv.org/abs/2402.19472v1</a></p>
<p><b>Compressor summary</b>: Lifelong benchmarks mitigate overfitting in machine learning by expanding test sets and using Sort & Search framework for efficient evaluation of models.</p><hr><h3>Loose LIPS Sink Ships: Asking Questions in Battleship with  Language-Informed Program Sampling</h3>
<p><a href='http://arxiv.org/abs/2402.19471v1'>http://arxiv.org/abs/2402.19471v1</a></p>
<p><b>Compressor summary</b>: The study uses a game-based task to investigate how people ask questions with limited resources and compares language models' abilities to generate informative questions that mirror human performance.</p><hr><h3>Teaching Large Language Models an Unseen Language on the Fly</h3>
<p><a href='http://arxiv.org/abs/2402.19167v1'>http://arxiv.org/abs/2402.19167v1</a></p>
<p><b>Compressor summary</b>: We propose DiPMT++, a framework that teaches large language models to translate new languages using only a dictionary and 5K parallel sentences, improving translation quality for Zhuang and enabling human-assisted translation for unseen languages.</p><hr><h3>MemoNav: Working Memory Model for Visual Navigation</h3>
<p><a href='http://arxiv.org/abs/2402.19161v1'>http://arxiv.org/abs/2402.19161v1</a></p>
<p><b>Compressor summary</b>: MemoNav is a memory model for image-goal navigation that uses short-term, long-term, and working memory to efficiently explore unfamiliar environments and navigate to goals indicated by images.</p><hr><h3>Effective Message Hiding with Order-Preserving Mechanisms</h3>
<p><a href='http://arxiv.org/abs/2402.19160v1'>http://arxiv.org/abs/2402.19160v1</a></p>
<p><b>Compressor summary</b>: StegaFormer is a new method for hiding secret messages in images using MLPs that preserves the order of message bits and fuses them with image features, achieving better recovery accuracy, message capacity, and imperceptibility.</p><hr><h3>Trajectory Consistency Distillation</h3>
<p><a href='http://arxiv.org/abs/2402.19159v1'>http://arxiv.org/abs/2402.19159v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Trajectory Consistency Distillation (TCD) to improve the Latent Consistency Model (LCM) for text-to-image synthesis by addressing errors in three areas and using strategic stochastic sampling.</p><hr><h3>Beyond Language Models: Byte Models are Digital World Simulators</h3>
<p><a href='http://arxiv.org/abs/2402.19155v1'>http://arxiv.org/abs/2402.19155v1</a></p>
<p><b>Compressor summary</b>: bGPT is a powerful model that uses next byte prediction to simulate and diagnose various aspects of the digital world, achieving high accuracy in tasks such as converting music notation and executing CPU operations.</p><hr><h3>Typographic Attacks in Large Multimodal Models Can be Alleviated by More  Informative Prompts</h3>
<p><a href='http://arxiv.org/abs/2402.19150v1'>http://arxiv.org/abs/2402.19150v1</a></p>
<p><b>Compressor summary</b>: Typographic Attack is a security vulnerability for LMMs, but they can partially distinguish visual contents and typos in images, and using more informative texts or prompts can improve their performance.</p><hr><h3>A SAM-guided Two-stream Lightweight Model for Anomaly Detection</h3>
<p><a href='http://arxiv.org/abs/2402.19145v1'>http://arxiv.org/abs/2402.19145v1</a></p>
<p><b>Compressor summary</b>: The paper presents a lightweight and efficient anomaly detection model that uses Segment Anything (SAM) to localize unseen anomalies and diverse patterns, achieving high performance on various datasets.</p><hr><h3>Weakly Supervised Monocular 3D Detection with a Single-View Image</h3>
<p><a href='http://arxiv.org/abs/2402.19144v1'>http://arxiv.org/abs/2402.19144v1</a></p>
<p><b>Compressor summary</b>: SKD-WM3D is a weakly supervised monocular 3D detection framework that uses self-knowledge distillation to achieve precise 3D object localization from a single image without any 3D annotations or extra training data.</p><hr><h3>ProtoP-OD: Explainable Object Detection with Prototypical Parts</h3>
<p><a href='http://arxiv.org/abs/2402.19142v1'>http://arxiv.org/abs/2402.19142v1</a></p>
<p><b>Compressor summary</b>: The paper introduces prototypical parts, a way to make detection transformers more interpretable by constructing local features that align with object classes and allowing visual inspection of the model's perception.</p><hr><h3>Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale  Annotations</h3>
<p><a href='http://arxiv.org/abs/2402.19133v1'>http://arxiv.org/abs/2402.19133v1</a></p>
<p><b>Compressor summary</b>: The paper explores using eye-tracking recordings as an alternative to manual annotations for evaluating explainability methods in NLP tasks, and compares different language models and languages.</p><hr><h3>BigGait: Learning Gait Representation You Want by Large Vision Models</h3>
<p><a href='http://arxiv.org/abs/2402.19122v1'>http://arxiv.org/abs/2402.19122v1</a></p>
<p><b>Compressor summary</b>: BigGait is a novel framework that uses large vision models to learn implicit gait features in an unsupervised way, improving gait recognition performance and reducing annotation costs.</p><hr><h3>VIXEN: Visual Text Comparison Network for Image Difference Captioning</h3>
<p><a href='http://arxiv.org/abs/2402.19119v1'>http://arxiv.org/abs/2402.19119v1</a></p>
<p><b>Compressor summary</b>: VIXEN summarizes textual differences between images to detect manipulation.</p><hr><h3>Continuous Sign Language Recognition Based on Motor attention mechanism  and frame-level Self-distillation</h3>
<p><a href='http://arxiv.org/abs/2402.19118v1'>http://arxiv.org/abs/2402.19118v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel motor attention mechanism to capture dynamic changes in sign language expression and applies self-distillation to improve feature extraction for continuous sign language recognition (CSLR), achieving state-of-the-art results on three datasets.</p><hr><h3>How to Understand "Support"? An Implicit-enhanced Causal Inference  Approach for Weakly-supervised Phrase Grounding</h3>
<p><a href='http://arxiv.org/abs/2402.19116v1'>http://arxiv.org/abs/2402.19116v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a method called Implicit-Enhanced Causal Inference to improve weakly-supervised phrase grounding by modeling implicit relations and using intervention and counterfactual techniques, leading to better performance than existing models and multimodal LLMs.</p><hr><h3>DeepEraser: Deep Iterative Context Mining for Generic Text Eraser</h3>
<p><a href='http://arxiv.org/abs/2402.19108v1'>http://arxiv.org/abs/2402.19108v1</a></p>
<p><b>Compressor summary</b>: DeepEraser is a recurrent deep network that erases text from images using iterative refinements and custom mask generation, achieving strong results on several benchmarks.</p><hr><h3>Whispers that Shake Foundations: Analyzing and Mitigating False Premise  Hallucinations in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.19103v1'>http://arxiv.org/abs/2402.19103v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes how false premises cause language models to generate incorrect text and proposes FAITH, a method to limit attention heads and reduce hallucinations.</p><hr><h3>FlatNAS: optimizing Flatness in Neural Architecture Search for  Out-of-Distribution Robustness</h3>
<p><a href='http://arxiv.org/abs/2402.19102v1'>http://arxiv.org/abs/2402.19102v1</a></p>
<p><b>Compressor summary</b>: FlatNAS is a novel NAS method that optimizes NN performance, OOD robustness, and parameter count using only in-distribution data.</p><hr><h3>TEncDM: Understanding the Properties of Diffusion Model in the Space of  Language Model Encodings</h3>
<p><a href='http://arxiv.org/abs/2402.19097v1'>http://arxiv.org/abs/2402.19097v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new text diffusion model (TEncDM) that uses language model encodings and a Transformer decoder for better text generation and reduces the number of denoising steps.</p><hr><h3>Leveraging Representations from Intermediate Encoder-blocks for  Synthetic Image Detection</h3>
<p><a href='http://arxiv.org/abs/2402.19091v1'>http://arxiv.org/abs/2402.19091v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Synthetic image generation poses risks for online information integrity and safety
- Existing SID methods focus on high-level visual semantics but need fine-grained details
- The method uses intermediate CLIP layers and a learnable vector space to improve SID performance
- The method outperforms state-of-the-art by 10.6% with minimal training time

Summary:
The authors propose a novel synthetic image detection method that leverages intermediate CLIP layers and a forgery-aware vector space, achieving significant improvement over existing methods with fast training.</p><hr><h3>Best Arm Identification with Resource Constraints</h3>
<p><a href='http://arxiv.org/abs/2402.19090v1'>http://arxiv.org/abs/2402.19090v1</a></p>
<p><b>Compressor summary</b>: The text studies a problem where an agent needs to find the best arm under limited resources and proposes a novel algorithm that converges quickly, with different rates depending on resource uncertainty.</p><hr><h3>Survey in Characterization of Semantic Change</h3>
<p><a href='http://arxiv.org/abs/2402.19088v1'>http://arxiv.org/abs/2402.19088v1</a></p>
<p><b>Compressor summary</b>: The text discusses how language evolution affects computational linguistics algorithms, and surveys existing methods to characterize semantic changes in words.</p><hr><h3>Controllable Preference Optimization: Toward Controllable  Multi-Objective Alignment</h3>
<p><a href='http://arxiv.org/abs/2402.19085v1'>http://arxiv.org/abs/2402.19085v1</a></p>
<p><b>Compressor summary</b>: The paper proposes controllable preference optimization (CPO) to align AI models with human preferences on multiple objectives, reducing trade-offs and improving performance.</p><hr><h3>VideoMAC: Video Masked Autoencoders Meet ConvNets</h3>
<p><a href='http://arxiv.org/abs/2402.19082v1'>http://arxiv.org/abs/2402.19082v1</a></p>
<p><b>Compressor summary</b>: VideoMAC combines video masked autoencoders with ConvNets to improve visual representation learning for videos, outperforming ViT-based approaches on downstream tasks.</p><hr><h3>Smooth Tchebycheff Scalarization for Multi-Objective Optimization</h3>
<p><a href='http://arxiv.org/abs/2402.19078v1'>http://arxiv.org/abs/2402.19078v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new smooth Tchebycheff scalarization approach for gradient-based multi-objective optimization that has good theoretical properties and lower computational complexity than existing methods.</p><hr><h3>Pointing out the Shortcomings of Relation Extraction Models with  Semantically Motivated Adversarials</h3>
<p><a href='http://arxiv.org/abs/2402.19076v1'>http://arxiv.org/abs/2402.19076v1</a></p>
<p><b>Compressor summary</b>: The text discusses how large language models struggle with relation extraction tasks when entity mentions are replaced or modified, suggesting they rely on shortcut features rather than semantic understanding.</p><hr><h3>TimeXer: Empowering Transformers for Time Series Forecasting with  Exogenous Variables</h3>
<p><a href='http://arxiv.org/abs/2402.19072v1'>http://arxiv.org/abs/2402.19072v1</a></p>
<p><b>Compressor summary</b>: TimeXer is a novel framework that leverages external information to enhance the forecasting of endogenous variables using the Transformer architecture with self-attention and cross-attention mechanisms.</p><hr><h3>VEnvision3D: A Synthetic Perception Dataset for 3D Multi-Task Model  Research</h3>
<p><a href='http://arxiv.org/abs/2402.19059v1'>http://arxiv.org/abs/2402.19059v1</a></p>
<p><b>Compressor summary</b>: VEnvision3D is a large synthetic 3D perception dataset for multi-task learning, aiming to facilitate the development of unified foundation models in computer vision research.</p><hr><h3>Exploring the Efficacy of Large Language Models in Summarizing Mental  Health Counseling Sessions: A Benchmark Study</h3>
<p><a href='http://arxiv.org/abs/2402.19052v1'>http://arxiv.org/abs/2402.19052v1</a></p>
<p><b>Compressor summary</b>: The study evaluates the performance of large language models in summarizing various counseling components using MentalCLOUDS dataset, finding that Mistral performs best overall with room for improvement.</p><hr><h3>Theoretical Foundations of Deep Selective State-Space Models</h3>
<p><a href='http://arxiv.org/abs/2402.19047v1'>http://arxiv.org/abs/2402.19047v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Structured state-space models (SSMs) like S4 are effective for sequential data modeling
- Deep SSMs with multiplicative interactions can outperform attention-based transformers in accuracy and efficiency
- The paper provides theoretical grounding using Rough Path Theory to explain the success of selective state-space models like Mamba

Summary:
The paper explains how deep SSMs with selectivity mechanisms, which project hidden states from input signatures, can surpass attention-based transformers in sequential data modeling and provides a theoretical framework using Rough Path Theory.</p><hr><h3>Atmospheric Turbulence Removal with Video Sequence Deep Visual Priors</h3>
<p><a href='http://arxiv.org/abs/2402.19041v1'>http://arxiv.org/abs/2402.19041v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Atmospheric turbulence distorts visual imagery
- Model-based methods have artefacts, deep learning methods need diverse datasets
- Self-supervised learning method uses accelerated DIP with temporal information
- Method improves visual quality of raw or pre-processed sequences

Summary:
The paper proposes a self-supervised learning method that uses accelerated DIP and temporal information to improve the visual quality of sequences affected by atmospheric turbulence distortions.</p><hr><h3>Progressive Contrastive Learning with Multi-Prototype for Unsupervised  Visible-Infrared Person Re-identification</h3>
<p><a href='http://arxiv.org/abs/2402.19026v1'>http://arxiv.org/abs/2402.19026v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for matching people in infrared and visible images without annotations, using progressive contrastive learning with multi-prototype to address disparity and retain natural feature variety.</p><hr><h3>Combination of Weak Learners eXplanations to Improve Random Forest  eXplicability Robustness</h3>
<p><a href='http://arxiv.org/abs/2402.19025v1'>http://arxiv.org/abs/2402.19025v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve the consistency of explanations for predictions made by combining multiple weak learners using discriminative averaging, and shows its effectiveness on SHAP and Random Forest ensembles.</p><hr><h3>Enhancing Visual Document Understanding with Contrastive Learning in  Large Visual-Language Models</h3>
<p><a href='http://arxiv.org/abs/2402.19014v1'>http://arxiv.org/abs/2402.19014v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a contrastive learning framework, DoCo, to improve visual representation in text-rich scenarios for large visual-language models, enhancing their performance in visual document understanding tasks.</p><hr><h3>Generating, Reconstructing, and Representing Discrete and Continuous  Data: Generalized Diffusion with Learnable Encoding-Decoding</h3>
<p><a href='http://arxiv.org/abs/2402.19009v1'>http://arxiv.org/abs/2402.19009v1</a></p>
<p><b>Compressor summary</b>: DiLED is a generalized diffusion model with learnable encoder-decoder that enhances performance and broad applicability across different data types.</p><hr><h3>DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in  Dynamic Environments</h3>
<p><a href='http://arxiv.org/abs/2402.19007v1'>http://arxiv.org/abs/2402.19007v1</a></p>
<p><b>Compressor summary</b>: The DOZE dataset provides a more realistic challenge for autonomous agents to navigate in dynamic environments with diverse objects and obstacles.</p><hr><h3>RSAM-Seg: A SAM-based Approach with Prior Knowledge Integration for  Remote Sensing Image Semantic Segmentation</h3>
<p><a href='http://arxiv.org/abs/2402.19004v1'>http://arxiv.org/abs/2402.19004v1</a></p>
<p><b>Compressor summary</b>: RSAM-Seg is a modified SAM model that improves image segmentation for remote sensing tasks and can help identify missing data in ground truth.</p><hr><h3>GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction</h3>
<p><a href='http://arxiv.org/abs/2402.19002v1'>http://arxiv.org/abs/2402.19002v1</a></p>
<p><b>Compressor summary</b>: GoalNet predicts pedestrian goals and trajectories using scene context and observed trajectory, outperforming previous methods by a large margin.</p><hr><h3>Analysis of the Two-Step Heterogeneous Transfer Learning for Laryngeal  Blood Vessel Classification: Issue and Improvement</h3>
<p><a href='http://arxiv.org/abs/2402.19001v1'>http://arxiv.org/abs/2402.19001v1</a></p>
<p><b>Compressor summary</b>: The study explores how using intermediate domains affects two-step transfer learning for classifying medical images and proposes a step-wise fine-tuning method to improve performance.</p><hr><h3>COFT-AD: COntrastive Fine-Tuning for Few-Shot Anomaly Detection</h3>
<p><a href='http://arxiv.org/abs/2402.18998v1'>http://arxiv.org/abs/2402.18998v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for few-shot anomaly detection that uses pre-trained models, contrastive training, and cross-instance pairs to learn suitable representations for the task.</p><hr><h3>Negative-Binomial Randomized Gamma Markov Processes for Heterogeneous  Overdispersed Count Time Series</h3>
<p><a href='http://arxiv.org/abs/2402.18995v1'>http://arxiv.org/abs/2402.18995v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new dynamical system that improves count time series modeling by capturing overdispersed behaviors, learning latent structure, and enabling fast inference and prediction.</p><hr><h3>Theoretically Achieving Continuous Representation of Oriented Bounding  Boxes</h3>
<p><a href='http://arxiv.org/abs/2402.18975v1'>http://arxiv.org/abs/2402.18975v1</a></p>
<p><b>Compressor summary</b>: This paper introduces Continuous OBB, a novel representation method that solves the discontinuity issue in Oriented Object Detection, and shows its effectiveness on the DOTA dataset using Faster-RCNN as a baseline model.</p><hr><h3>Graph Generation via Spectral Diffusion</h3>
<p><a href='http://arxiv.org/abs/2402.18974v1'>http://arxiv.org/abs/2402.18974v1</a></p>
<p><b>Compressor summary</b>: GRASP is a fast and accurate graph generative model that uses spectral decomposition, denoising, and node features to create realistic graphs.</p><hr><h3>OHTA: One-shot Hand Avatar via Data-driven Implicit Priors</h3>
<p><a href='http://arxiv.org/abs/2402.18969v1'>http://arxiv.org/abs/2402.18969v1</a></p>
<p><b>Compressor summary</b>: The paper introduces OHTA, a method that creates realistic and personalized hand avatars from one image using data-driven hand priors, and shows its applications in various scenarios.</p><hr><h3>Towards Out-of-Distribution Detection for breast cancer classification  in Point-of-Care Ultrasound Imaging</h3>
<p><a href='http://arxiv.org/abs/2402.18960v1'>http://arxiv.org/abs/2402.18960v1</a></p>
<p><b>Compressor summary</b>: The study compares three methods for detecting unreliable assessments in medical image analysis using deep learning and finds that the ensemble method performs best.</p><hr><h3>Boosting Semi-Supervised Object Detection in Remote Sensing Images With  Active Teaching</h3>
<p><a href='http://arxiv.org/abs/2402.18958v1'>http://arxiv.org/abs/2402.18958v1</a></p>
<p><b>Compressor summary</b>: The authors propose a novel active learning method (SSOD-AT) that uses an RoI comparison module to generate high-confidence pseudo-labels for object detection in remote sensing images and improves performance over state-of-the-art methods.</p><hr><h3>WWW: A Unified Framework for Explaining What, Where and Why of Neural  Networks by Interpretation of Neuron Concepts</h3>
<p><a href='http://arxiv.org/abs/2402.18956v1'>http://arxiv.org/abs/2402.18956v1</a></p>
<p><b>Compressor summary</b>: The paper proposes WWW, a framework that explains neural network decisions by discovering concepts, creating localized concept maps and heatmaps, and predicting uncertainty.</p><hr><h3>Percept, Chat, and then Adapt: Multimodal Knowledge Transfer of  Foundation Models for Open-World Video Recognition</h3>
<p><a href='http://arxiv.org/abs/2402.18951v1'>http://arxiv.org/abs/2402.18951v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Open-world video recognition is hard due to environment variations
- Foundation models have rich knowledge but need proper application
- PCA pipeline transfers external multimodal knowledge to improve recognition
- PCA has three stages: Percept, Chat, and Adapt
- PCA achieves state-of-the-art results on three benchmarks

Summary:
PCA is a generic pipeline that uses foundation models' knowledge to enhance open-world video recognition, by transferring visual and textual information in three stages.</p><hr><h3>PopALM: Popularity-Aligned Language Models for Social Media Trendy  Response Prediction</h3>
<p><a href='http://arxiv.org/abs/2402.18950v1'>http://arxiv.org/abs/2402.18950v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to predict popular user replies on social media using reinforcement learning and curriculum learning.</p><hr><h3>Real-Time Adaptive Safety-Critical Control with Gaussian Processes in  High-Order Uncertain Models</h3>
<p><a href='http://arxiv.org/abs/2402.18946v1'>http://arxiv.org/abs/2402.18946v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an adaptive online learning framework with a sparse GP model and a safety filter based on HOCBFs to ensure safe control in non-stationary environments.</p><hr><h3>Modality-Agnostic Structural Image Representation Learning for  Deformable Multi-Modality Medical Image Registration</h3>
<p><a href='http://arxiv.org/abs/2402.18933v1'>http://arxiv.org/abs/2402.18933v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to learn effective structural image representations for multimodality image registration using Deep Neighbourhood Self-similarity and anatomy-aware contrastive learning, improving discrimination and accuracy compared to existing methods.</p><hr><h3>Navigating Beyond Dropout: An Intriguing Solution Towards Generalizable  Image Super Resolution</h3>
<p><a href='http://arxiv.org/abs/2402.18929v1'>http://arxiv.org/abs/2402.18929v1</a></p>
<p><b>Compressor summary</b>: Our paper investigates the effects of Dropout and proposes a new training strategy for Blind Super-Resolution that preserves fine details better than Dropout.</p><hr><h3>Edge Computing Enabled Real-Time Video Analysis via Adaptive  Spatial-Temporal Semantic Filtering</h3>
<p><a href='http://arxiv.org/abs/2402.18927v1'>http://arxiv.org/abs/2402.18927v1</a></p>
<p><b>Compressor summary</b>: The paper presents a real-time video analysis system using edge computing that adapts to network conditions and object detection with reinforcement learning methods.</p><hr><h3>PCDepth: Pattern-based Complementary Learning for Monocular Depth  Estimation by Best of Both Worlds</h3>
<p><a href='http://arxiv.org/abs/2402.18925v1'>http://arxiv.org/abs/2402.18925v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Event cameras record scene dynamics with high temporal resolution and low-level illumination
- Existing methods fuse intensity and event data at pixel level, ignoring high-level patterns
- PCDepth discretizes the scene into high-level patterns and integrates them across modalities
- PCDepth achieves more accurate monocular depth estimation than existing methods, especially in nighttime scenarios

Summary:
PCDepth is a novel approach that leverages high-level patterns from event cameras and intensity images for better monocular depth estimation, outperforming state-of-the-art methods in low-light conditions.</p><hr><h3>Inappropriate Pause Detection In Dysarthric Speech Using Large-Scale  Speech Recognition</h3>
<p><a href='http://arxiv.org/abs/2402.18923v1'>http://arxiv.org/abs/2402.18923v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a large-scale speech recognition model with an inappropriate pause prediction layer to detect and assess inappropriate pauses in dysarthric speech, which affects stroke patients' speech intelligibility.</p><hr><h3>A Simple yet Effective Network based on Vision Transformer for  Camouflaged Object and Salient Object Detection</h3>
<p><a href='http://arxiv.org/abs/2402.18922v1'>http://arxiv.org/abs/2402.18922v1</a></p>
<p><b>Compressor summary</b>: The authors propose a simple and versatile network (SENet) for camouflaged object detection and salient object detection, using a vision Transformer encoder-decoder structure, a local information capture module, and a dynamic weighted loss function.</p><hr><h3>Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation</h3>
<p><a href='http://arxiv.org/abs/2402.18920v1'>http://arxiv.org/abs/2402.18920v1</a></p>
<p><b>Compressor summary</b>: The paper presents a unified framework that predicts point-wise correspondences and shape interpolation between 3D shapes using a combination of deep functional maps and classical surface deformation models, achieving better performance than previous methods.</p><hr><h3>Decompose-and-Compose: A Compositional Approach to Mitigating Spurious  Correlation</h3>
<p><a href='http://arxiv.org/abs/2402.18919v1'>http://arxiv.org/abs/2402.18919v1</a></p>
<p><b>Compressor summary</b>: DaC improves image classification robustness by identifying causal components, intervening on images, and retraining models to address correlation shift caused by compositional nature of images.</p><hr><h3>SNE-RoadSegV2: Advancing Heterogeneous Feature Fusion and Fallibility  Awareness for Freespace Detection</h3>
<p><a href='http://arxiv.org/abs/2402.18918v1'>http://arxiv.org/abs/2402.18918v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a novel heterogeneous feature fusion network for freespace detection with improved accuracy and efficiency, addressing limitations in previous techniques.</p><hr><h3>Stop Relying on No-Choice and Do not Repeat the Moves: Optimal,  Efficient and Practical Algorithms for Assortment Optimization</h3>
<p><a href='http://arxiv.org/abs/2402.18917v1'>http://arxiv.org/abs/2402.18917v1</a></p>
<p><b>Compressor summary</b>: The paper proposes efficient algorithms for assortment optimization with user choices modeled by Plackett Luce, and provides a novel concentration guarantee using Pairwise Rank-Breaking to minimize regret.</p><hr><h3>AdaMergeX: Cross-Lingual Transfer with Large Language Models via  Adaptive Adapter Merging</h3>
<p><a href='http://arxiv.org/abs/2402.18913v1'>http://arxiv.org/abs/2402.18913v1</a></p>
<p><b>Compressor summary</b>: The paper introduces AdaMergeX, a cross-lingual transfer method that uses adaptive adapter merging to improve performance on target tasks by decoupling task ability and language ability.</p><hr><h3>DIGIC: Domain Generalizable Imitation Learning by Causal Discovery</h3>
<p><a href='http://arxiv.org/abs/2402.18910v1'>http://arxiv.org/abs/2402.18910v1</a></p>
<p><b>Compressor summary</b>: The paper proposes DIGIC, a framework that uses causal discovery to identify causal features for domain generalizable imitation learning with single-domain data, outperforming cross-domain variation-based methods.</p><hr><h3>Updating Language Models with Unstructured Facts: Towards Practical  Knowledge Editing</h3>
<p><a href='http://arxiv.org/abs/2402.18909v1'>http://arxiv.org/abs/2402.18909v1</a></p>
<p><b>Compressor summary</b>: Unstructured Knowledge Editing (UKE) is a new benchmark that evaluates language models' ability to update their knowledge using unstructured texts, which is more practical than current methods based on structured facts.</p><hr><h3>On the Convergence of Differentially-Private Fine-tuning: To Linearly  Probe or to Fully Fine-tune?</h3>
<p><a href='http://arxiv.org/abs/2402.18905v1'>http://arxiv.org/abs/2402.18905v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes the training dynamics of differentially private (DP) linear probing and full fine-tuning, explores sequential fine-tuning, provides theoretical insights into DP fine-tuning convergence, and establishes a utility curve for privacy budget allocation.</p><hr><h3>Aligning Knowledge Graph with Visual Perception for Object-goal  Navigation</h3>
<p><a href='http://arxiv.org/abs/2402.18892v1'>http://arxiv.org/abs/2402.18892v1</a></p>
<p><b>Compressor summary</b>: AKGVP is a method that improves object-goal navigation by aligning knowledge graph with visual perception using continuous modeling and natural language pre-training.</p><hr><h3>BP-DeepONet: A new method for cuffless blood pressure estimation using  the physcis-informed DeepONet</h3>
<p><a href='http://arxiv.org/abs/2402.18886v1'>http://arxiv.org/abs/2402.18886v1</a></p>
<p><b>Compressor summary</b>: The study proposes a novel framework using physics-informed DeepONet approach to predict continuous and accurate arterial blood pressure waveforms without invasive methods.</p><hr><h3>Supervised Contrastive Representation Learning: Landscape Analysis with  Unconstrained Features</h3>
<p><a href='http://arxiv.org/abs/2402.18884v1'>http://arxiv.org/abs/2402.18884v1</a></p>
<p><b>Compressor summary</b>: This paper studies how over-parameterized deep neural networks behave under supervised contrastive loss and reveals their structural patterns using an analytical approach.</p><hr><h3>Dose Prediction Driven Radiotherapy Paramters Regression via Intra- and  Inter-Relation Modeling</h3>
<p><a href='http://arxiv.org/abs/2402.18879v1'>http://arxiv.org/abs/2402.18879v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a two-stage framework using CNN and transformer to predict dose maps and radiotherapy parameters, incorporating intra-relation and inter-relation models for accurate parameter regression.</p><hr><h3>Principal Component Analysis as a Sanity Check for Bayesian  Phylolinguistic Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2402.18877v1'>http://arxiv.org/abs/2402.18877v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a simple method to check if the tree model assumption is valid for reconstructing language evolution, by projecting the tree onto a space generated by principal component analysis.</p><hr><h3>Loss-aware Curriculum Learning for Heterogeneous Graph Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.18875v1'>http://arxiv.org/abs/2402.18875v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a loss-aware training schedule (LTS) that improves the performance and robustness of heterogeneous graph neural networks by progressively incorporating data with varying quality.</p><hr><h3>Reducing Hallucinations in Entity Abstract Summarization with  Facts-Template Decomposition</h3>
<p><a href='http://arxiv.org/abs/2402.18873v1'>http://arxiv.org/abs/2402.18873v1</a></p>
<p><b>Compressor summary</b>: SlotSum is an explainable framework for entity abstract summarization that decomposes the summary into facts and template, enabling error detection and rectification with external knowledge.</p><hr><h3>Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming</h3>
<p><a href='http://arxiv.org/abs/2402.18866v1'>http://arxiv.org/abs/2402.18866v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Dr. Strategy, a new MBRL agent with a novel dreaming strategy that uses latent landmarks to improve sample efficiency and performance in complex navigation tasks.</p><hr><h3>Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient  Tuning</h3>
<p><a href='http://arxiv.org/abs/2402.18865v1'>http://arxiv.org/abs/2402.18865v1</a></p>
<p><b>Compressor summary</b>: The paper investigates the trade-off between plasticity and stability in large language models' continual learning, proposing a method called I-LoRA that improves performance on domain-specific tasks.</p><hr><h3>Probabilistic Lipschitzness and the Stable Rank for Comparing  Explanation Models</h3>
<p><a href='http://arxiv.org/abs/2402.18863v1'>http://arxiv.org/abs/2402.18863v1</a></p>
<p><b>Compressor summary</b>: This paper investigates how different explainability models affect the quality of post hoc explanations for neural networks, using probabilistic Lipschitzness and stable rank as metrics to compare their robustness.</p><hr><h3>Taking Second-life Batteries from Exhausted to Empowered using  Experiments, Data Analysis, and Health Estimation</h3>
<p><a href='http://arxiv.org/abs/2402.18859v1'>http://arxiv.org/abs/2402.18859v1</a></p>
<p><b>Compressor summary</b>: The study proposes health monitoring algorithms for reusing retired electric vehicle batteries in grid energy storage, achieving promising results with a machine learning-based model and an adaptive online algorithm.</p><hr><h3>Rethinking Multi-domain Generalization with A General Learning Objective</h3>
<p><a href='http://arxiv.org/abs/2402.18853v1'>http://arxiv.org/abs/2402.18853v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new learning objective for multi-domain generalization (mDG) that uses Y-mapping to relax constraints and improve performance in various tasks.</p><hr><h3>Applications of 0-1 Neural Networks in Prescription and Prediction</h3>
<p><a href='http://arxiv.org/abs/2402.18851v1'>http://arxiv.org/abs/2402.18851v1</a></p>
<p><b>Compressor summary</b>: Prescriptive networks (PNNs) are shallow neural networks that use mixed integer programming to optimize personalized healthcare policies, offering greater interpretability than deep neural networks and better performance in a case study of postpartum hypertension treatment.</p><hr><h3>Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP  Models on Accuracy and Semantic Coherence</h3>
<p><a href='http://arxiv.org/abs/2402.18849v1'>http://arxiv.org/abs/2402.18849v1</a></p>
<p><b>Compressor summary</b>: This study presents an LSB-NLP hybrid framework that combines image steganography and NLP to improve the accuracy and robustness of extracting hidden text, especially Chinese characters.</p><hr><h3>SwitchLight: Co-design of Physics-driven Architecture and Pre-training  Framework for Human Portrait Relighting</h3>
<p><a href='http://arxiv.org/abs/2402.18848v1'>http://arxiv.org/abs/2402.18848v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a co-designed method for realistic human portrait relighting using a physics-guided architecture and a self-supervised pre-training strategy.</p><hr><h3>Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling</h3>
<p><a href='http://arxiv.org/abs/2402.18846v1'>http://arxiv.org/abs/2402.18846v1</a></p>
<p><b>Compressor summary</b>: MFRNP is a novel framework for multi-fidelity surrogate modeling that improves accuracy by optimizing lower fidelity decoders for information sharing and modeling residual between aggregated outputs and ground truth.</p><hr><h3>Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey</h3>
<p><a href='http://arxiv.org/abs/2402.18844v1'>http://arxiv.org/abs/2402.18844v1</a></p>
<p><b>Compressor summary</b>: This paper reviews deep learning methods for 3D human pose estimation and mesh recovery, covering single-person and multi-person approaches, explicit and implicit models, and comparing results on several datasets.</p><hr><h3>ViewFusion: Towards Multi-View Consistency via Interpolated Denoising</h3>
<p><a href='http://arxiv.org/abs/2402.18842v1'>http://arxiv.org/abs/2402.18842v1</a></p>
<p><b>Compressor summary</b>: ViewFusion is a new algorithm that enhances diffusion models for better multi-view consistency in image generation without needing training or fine-tuning.</p><hr><h3>Extended Flow Matching: a Method of Conditional Generation with  Generalized Continuity Equation</h3>
<p><a href='http://arxiv.org/abs/2402.18839v1'>http://arxiv.org/abs/2402.18839v1</a></p>
<p><b>Compressor summary</b>: The paper develops a new method for conditional generation using Flow Matching, which improves over existing guidance-based methods by ensuring continuous matching of matrix fields instead of vector fields.</p><hr><h3>When does word order matter and when doesn't it?</h3>
<p><a href='http://arxiv.org/abs/2402.18838v1'>http://arxiv.org/abs/2402.18838v1</a></p>
<p><b>Compressor summary</b>: The paper suggests that language models are insensitive to word order in NLU tasks because linguistic redundancy provides overlapping information, and this insensitivity varies across tasks.</p><hr><h3>A Model-Based Approach for Improving Reinforcement Learning Efficiency  Leveraging Expert Observations</h3>
<p><a href='http://arxiv.org/abs/2402.18836v1'>http://arxiv.org/abs/2402.18836v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to use expert observations with deep reinforcement learning for better sample efficiency and performance on continuous control tasks.</p><hr><h3>Utilizing Local Hierarchy with Adversarial Training for Hierarchical  Text Classification</h3>
<p><a href='http://arxiv.org/abs/2402.18825v1'>http://arxiv.org/abs/2402.18825v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new framework (HiAdv) that uses a local hierarchy to improve text classification, especially for complex taxonomic structures and rare classes.</p><hr><h3>Batch size invariant Adam</h3>
<p><a href='http://arxiv.org/abs/2402.18824v1'>http://arxiv.org/abs/2402.18824v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a modified version of Adam that works well with distributed training and does not require strong assumptions about gradient variance.</p><hr><h3>Debiased Novel Category Discovering and Localization</h3>
<p><a href='http://arxiv.org/abs/2402.18821v1'>http://arxiv.org/abs/2402.18821v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an object detection model that can discover and localize novel classes without bias towards seen objects, using Debiased Region Mining and semi-supervised contrastive learning.</p><hr><h3>Dual Operating Modes of In-Context Learning</h3>
<p><a href='http://arxiv.org/abs/2402.18819v1'>http://arxiv.org/abs/2402.18819v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a probabilistic model to explain the dual operating modes of in-context learning (ICL) and analyzes its behavior, offering insights into ICL's risk dynamics and performance with biased labels.</p><hr><h3>Gradient Alignment for Cross-Domain Face Anti-Spoofing</h3>
<p><a href='http://arxiv.org/abs/2402.18817v1'>http://arxiv.org/abs/2402.18817v1</a></p>
<p><b>Compressor summary</b>: The paper introduces GAC-FAS, a novel learning objective for face anti-spoofing that ensures convergence to an optimal flat minimum without additional modules and achieves state-of-the-art performance on cross-domain datasets.</p><hr><h3>How do Large Language Models Handle Multilingualism?</h3>
<p><a href='http://arxiv.org/abs/2402.18815v1'>http://arxiv.org/abs/2402.18815v1</a></p>
<p><b>Compressor summary</b>: The authors study how large language models process multilingual inputs, propose a framework for it, and develop a method to detect language-specific neurons in LLMs.</p><hr><h3>BFRFormer: Transformer-based generator for Real-World Blind Face  Restoration</h3>
<p><a href='http://arxiv.org/abs/2402.18811v1'>http://arxiv.org/abs/2402.18811v1</a></p>
<p><b>Compressor summary</b>: The paper proposes BFRFormer, a Transformer-based method for restoring blind face images with more identity-preserving details, using wavelet discriminator and aggregated attention module to address limitations of convolutional neural networks.</p><hr><h3>On the Decision-Making Abilities in Role-Playing using Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2402.18807v1'>http://arxiv.org/abs/2402.18807v1</a></p>
<p><b>Compressor summary</b>: This paper evaluates how well large language models can make decisions in role-playing tasks based on different personality types and provides metrics to improve their decision-making abilities.</p><hr><h3>To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair  Training on Shared Models</h3>
<p><a href='http://arxiv.org/abs/2402.18803v1'>http://arxiv.org/abs/2402.18803v1</a></p>
<p><b>Compressor summary</b>: The paper proposes generalization error bounds for fair machine learning that leverage the majority group's larger sample size to reduce performance disparities between groups.</p><hr><h3>BlockEcho: Retaining Long-Range Dependencies for Imputing Block-Wise  Missing Data</h3>
<p><a href='http://arxiv.org/abs/2402.18800v1'>http://arxiv.org/abs/2402.18800v1</a></p>
<p><b>Compressor summary</b>: BlockEcho is a novel matrix completion method that uses Matrix Factorization within Generative Adversarial Networks to improve imputation of block-wise missing data, especially at higher rates.</p><hr><h3>Enhancing the "Immunity" of Mixture-of-Experts Networks for Adversarial  Defense</h3>
<p><a href='http://arxiv.org/abs/2402.18787v1'>http://arxiv.org/abs/2402.18787v1</a></p>
<p><b>Compressor summary</b>: The Immunity method enhances a modified Mixture-of-Experts architecture with Random Switch Gates and MI/Position Stability-based losses to improve adversarial robustness of DNNs.</p><hr><h3>OpticalDR: A Deep Optical Imaging Model for Privacy-Protective  Depression Recognition</h3>
<p><a href='http://arxiv.org/abs/2402.18786v1'>http://arxiv.org/abs/2402.18786v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new imaging system that anonymizes facial images for depression recognition while preserving disease-related features and achieving state-of-the-art privacy protection performance.</p><hr><h3>Brain-inspired and Self-based Artificial Intelligence</h3>
<p><a href='http://arxiv.org/abs/2402.18784v1'>http://arxiv.org/abs/2402.18784v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a Brain-inspired and Self-based Artificial Intelligence paradigm that emphasizes the role of self in shaping human-level AI models and robotic applications, aiming for real Artificial General Intelligence.</p><hr><h3>A Quantitative Evaluation of Score Distillation Sampling Based  Text-to-3D</h3>
<p><a href='http://arxiv.org/abs/2402.18780v1'>http://arxiv.org/abs/2402.18780v1</a></p>
<p><b>Compressor summary</b>: The text describes a new method for evaluating and improving 3D content generation from text prompts using objective metrics and a novel baseline model that reduces artifacts.</p><hr><h3>NARUTO: Neural Active Reconstruction from Uncertain Target Observations</h3>
<p><a href='http://arxiv.org/abs/2402.18771v1'>http://arxiv.org/abs/2402.18771v1</a></p>
<p><b>Compressor summary</b>: NARUTO is a neural system that uses uncertainty learning to create high-quality environment maps and efficiently explore them for active reconstruction tasks.</p><hr><h3>Advancing Generative AI for Portuguese with Open Decoder Gervásio PT*</h3>
<p><a href='http://arxiv.org/abs/2402.18766v1'>http://arxiv.org/abs/2402.18766v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Gerv'asio PT*, a new open source decoder model that sets a new state of the art for neural decoding of Portuguese with instruction data sets.</p><hr><h3>Disentangling the Causes of Plasticity Loss in Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2402.18762v1'>http://arxiv.org/abs/2402.18762v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how to maintain neural network trainability by combining multiple mechanisms of plasticity loss mitigation, such as layer normalization and weight decay, in various settings.</p>