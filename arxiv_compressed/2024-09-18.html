
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-09-18</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-09-18 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Phidias: A Generative Model for Creating 3D Content from Text, Image,  and 3D Conditions with Reference-Augmented Diffusion</h3>
<p><a href='http://arxiv.org/abs/2409.11406v1'>http://arxiv.org/abs/2409.11406v1</a></p>
<p><b>Compressor summary</b>: Phidias is a novel generative model that uses diffusion and reference-augmented 3D generation to improve quality, generalization, and controllability in 3D modeling.</p><hr><h3>AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs</h3>
<p><a href='http://arxiv.org/abs/2409.11404v1'>http://arxiv.org/abs/2409.11404v1</a></p>
<p><b>Compressor summary</b>: The paper introduces seven synthetic datasets for underrepresented Arabic dialects, a benchmark for evaluating LLMs on dialect comprehension and generation, and highlights challenges in capturing diverse Arabic dialects and cultural contexts.</p><hr><h3>NVLM: Open Frontier-Class Multimodal LLMs</h3>
<p><a href='http://arxiv.org/abs/2409.11402v1'>http://arxiv.org/abs/2409.11402v1</a></p>
<p><b>Compressor summary</b>: NVLM 1.0 is a state-of-the-art multimodal language model that outperforms leading models on vision-language tasks, with a novel architecture and curation of high-quality datasets.</p><hr><h3>Says Who? Effective Zero-Shot Annotation of Focalization</h3>
<p><a href='http://arxiv.org/abs/2409.11390v1'>http://arxiv.org/abs/2409.11390v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates Large Language Models' performance in identifying narrative focalization and shows their potential for studying literary texts.</p><hr><h3>Normalization in Proportional Feature Spaces</h3>
<p><a href='http://arxiv.org/abs/2409.11389v1'>http://arxiv.org/abs/2409.11389v1</a></p>
<p><b>Compressor summary</b>: The text discusses feature normalization methods for data analysis and modeling, focusing on uniform and proportional features and their comparisons, with some examples of normalization and similarity measures.</p><hr><h3>Ultrasound Image Enhancement with the Variance of Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2409.11380v1'>http://arxiv.org/abs/2409.11380v1</a></p>
<p><b>Compressor summary</b>: The paper presents a novel method that uses adaptive beamforming and denoising diffusion to enhance ultrasound images by balancing contrast, resolution, and speckle preservation.</p><hr><h3>Diversify and Conquer: Diversity-Centric Data Selection with Iterative  Refinement</h3>
<p><a href='http://arxiv.org/abs/2409.11378v1'>http://arxiv.org/abs/2409.11378v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a data selection method for fine-tuning large language models that focuses on diversity rather than quality, using k-means clustering and iterative refinement to improve performance across various tasks.</p><hr><h3>Machine Learning on Dynamic Functional Connectivity: Promise, Pitfalls,  and Interpretations</h3>
<p><a href='http://arxiv.org/abs/2409.11377v1'>http://arxiv.org/abs/2409.11377v1</a></p>
<p><b>Compressor summary</b>: The text summarizes the authors' study of using existing fMRI data to understand human cognition/behavior, evaluate current deep models for cognitive task recognition and disease diagnosis, and provide guidelines for selecting suitable machine learning backbones for new neuroimaging applications.</p><hr><h3>Towards Time Series Reasoning with LLMs</h3>
<p><a href='http://arxiv.org/abs/2409.11376v1'>http://arxiv.org/abs/2409.11376v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a multi-modal time-series language model that extracts and reasons about time-series information using a lightweight encoder and chain-of-thought augmentation, achieving zero-shot performance across various domains.</p><hr><h3>Multi-OCT-SelfNet: Integrating Self-Supervised Learning with  Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease  Classification</h3>
<p><a href='http://arxiv.org/abs/2409.11375v1'>http://arxiv.org/abs/2409.11375v1</a></p>
<p><b>Compressor summary</b>: The authors propose a self-supervised framework using large language models and SwinV2 to improve retinal disease diagnosis from multi-modal data, enhancing generalization and performance on smaller datasets.</p><hr><h3>OSV: One Step is Enough for High-Quality Image to Video Generation</h3>
<p><a href='http://arxiv.org/abs/2409.11367v1'>http://arxiv.org/abs/2409.11367v1</a></p>
<p><b>Compressor summary</b>: The authors propose a two-stage training framework that combines consistency distillation with GAN training to accelerate video diffusion, leading to high-quality videos in one step and outperforming existing methods.</p><hr><h3>CoCA: Regaining Safety-awareness of Multimodal Large Language Models  with Constitutional Calibration</h3>
<p><a href='http://arxiv.org/abs/2409.11365v1'>http://arxiv.org/abs/2409.11365v1</a></p>
<p><b>Compressor summary</b>: The paper explores how to enhance the safety-awareness of multimodal language models against malicious image inputs using a technique called CoCA.</p><hr><h3>CORE-Bench: Fostering the Credibility of Published Research Through a  Computational Reproducibility Agent Benchmark</h3>
<p><a href='http://arxiv.org/abs/2409.11363v1'>http://arxiv.org/abs/2409.11363v1</a></p>
<p><b>Compressor summary</b>: CORE-Bench is a benchmark for measuring AI agents' accuracy in performing computational reproducibility tasks across three disciplines, aiming to improve scientific processes and agent development.</p><hr><h3>Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think</h3>
<p><a href='http://arxiv.org/abs/2409.11355v1'>http://arxiv.org/abs/2409.11355v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a faster and more accurate monocular depth estimator by fixing an inference pipeline flaw, fine-tuning the model with task-specific losses, and applying the method to Stable Diffusion.</p><hr><h3>THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation  in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2409.11353v1'>http://arxiv.org/abs/2409.11353v1</a></p>
<p><b>Compressor summary</b>: THaMES is a framework for detecting and mitigating hallucinations in large language models using various strategies and automated test set generation.</p><hr><h3>CLIP Adaptation by Intra-modal Overlap Reduction</h3>
<p><a href='http://arxiv.org/abs/2409.11338v1'>http://arxiv.org/abs/2409.11338v1</a></p>
<p><b>Compressor summary</b>: The text analyses how CLIP model's high cosine similarity between paired and unpaired images affects few-shot classification and proposes a lightweight adapter to reduce this overlap, improving performance and robustness.</p><hr><h3>Reducing Catastrophic Forgetting in Online Class Incremental Learning  Using Self-Distillation</h3>
<p><a href='http://arxiv.org/abs/2409.11329v1'>http://arxiv.org/abs/2409.11329v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-distillation method to solve catastrophic forgetting in continual learning and improves it with a memory update technique that prioritizes storing misclassified samples.</p><hr><h3>TopoMaskV2: Enhanced Instance-Mask-Based Formulation for the Road  Topology Problem</h3>
<p><a href='http://arxiv.org/abs/2409.11325v1'>http://arxiv.org/abs/2409.11325v1</a></p>
<p><b>Compressor summary</b>: TopoMask is a new method that uses mask-based instances and attention-based transformers to improve centerline prediction from road images, achieving state-of-the-art performance on OpenLane-V2 dataset.</p><hr><h3>LPT++: Efficient Training on Mixture of Long-tailed Experts</h3>
<p><a href='http://arxiv.org/abs/2409.11323v1'>http://arxiv.org/abs/2409.11323v1</a></p>
<p><b>Compressor summary</b>: LPT++ is a framework for long-tailed classification that combines fine-tuning, model ensemble, and three core components to improve Vision Transformers' performance with minimal additional parameters.</p><hr><h3>SOAP: Improving and Stabilizing Shampoo using Adam</h3>
<p><a href='http://arxiv.org/abs/2409.11321v1'>http://arxiv.org/abs/2409.11321v1</a></p>
<p><b>Compressor summary</b>: SOAP is a computationally efficient optimization algorithm that combines the benefits of Shampoo and Adam, reducing the number of iterations and wall clock time for large-scale language model pre-training.</p><hr><h3>fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D  Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2409.11315v1'>http://arxiv.org/abs/2409.11315v1</a></p>
<p><b>Compressor summary</b>: The paper introduces the fMRI-3D dataset, a collection of fMRI data and 3D object images with text captions, and presents MinD-3D, a framework to reconstruct 3D objects from fMRI signals using a generative transformer decoder.</p><hr><h3>SpMis: An Investigation of Synthetic Spoken Misinformation Detection</h3>
<p><a href='http://arxiv.org/abs/2409.11308v1'>http://arxiv.org/abs/2409.11308v1</a></p>
<p><b>Compressor summary</b>: The text discusses the advances and challenges of speech generation technology, especially in detecting misinformation in synthetic spoken content, and introduces an open-source dataset (SpMis) to study this issue.</p><hr><h3>GS-Net: Generalizable Plug-and-Play 3D Gaussian Splatting Module</h3>
<p><a href='http://arxiv.org/abs/2409.11307v1'>http://arxiv.org/abs/2409.11307v1</a></p>
<p><b>Compressor summary</b>: GS-Net is a plug-and-play module that improves 3D Gaussian Splatting by densifying Gaussian ellipsoids from sparse point clouds, achieving better generalization and rendering quality on novel viewpoints using the CARLA-NVS dataset.</p><hr><h3>Beyond LoRA: Exploring Efficient Fine-Tuning Techniques for Time Series  Foundational Models</h3>
<p><a href='http://arxiv.org/abs/2409.11302v1'>http://arxiv.org/abs/2409.11302v1</a></p>
<p><b>Compressor summary</b>: The text discusses using Parameter-Efficient Fine-Tuning techniques for time series models in healthcare applications, particularly forecasting vital signs of sepsis patients, and shows that some methods outperform existing approaches while fine-tuning fewer parameters.</p><hr><h3>Navigating Process Mining: A Case study using pm4py</h3>
<p><a href='http://arxiv.org/abs/2409.11294v1'>http://arxiv.org/abs/2409.11294v1</a></p>
<p><b>Compressor summary</b>: The paper uses pm4py to analyze road traffic fine management processes and discover their models, patterns, and limitations using various process-mining techniques.</p><hr><h3>Neural Networks for Vehicle Routing Problem</h3>
<p><a href='http://arxiv.org/abs/2409.11290v1'>http://arxiv.org/abs/2409.11290v1</a></p>
<p><b>Compressor summary</b>: The text discusses using neural networks as a new tool for optimizing vehicle routes, presenting a novel graphical neural network model and demonstrating its efficiency through tests.</p><hr><h3>Leveraging Distillation Techniques for Document Understanding: A Case  Study with FLAN-T5</h3>
<p><a href='http://arxiv.org/abs/2409.11282v1'>http://arxiv.org/abs/2409.11282v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to transfer knowledge from a proprietary LLM to a more accessible one, enabling better document understanding with limited resources.</p><hr><h3>Machine Learning and Theory Ladenness -- A Phenomenological Account</h3>
<p><a href='http://arxiv.org/abs/2409.11277v1'>http://arxiv.org/abs/2409.11277v1</a></p>
<p><b>Compressor summary</b>: This text discusses how machine learning methods are influenced by the domain theories they are applied to, arguing that both theory-dependent and theory-independent perspectives are oversimplified.</p><hr><h3>Task Arithmetic for Language Expansion in Speech Translation</h3>
<p><a href='http://arxiv.org/abs/2409.11274v1'>http://arxiv.org/abs/2409.11274v1</a></p>
<p><b>Compressor summary</b>: The authors propose an augmented task arithmetic method for expanding speech-text multimodal foundation models to new language pairs by using a language control model to prevent confusion and improve translation quality.</p><hr><h3>LOLA -- An Open-Source Massively Multilingual Large Language Model</h3>
<p><a href='http://arxiv.org/abs/2409.11272v1'>http://arxiv.org/abs/2409.11272v1</a></p>
<p><b>Compressor summary</b>: LOLA is a large language model that works well across many languages by using expert-routing and sparse architecture.</p><hr><h3>Geometry Aware Meta-Learning Neural Network for Joint Phase and Precoder  Optimization in RIS</h3>
<p><a href='http://arxiv.org/abs/2409.11270v1'>http://arxiv.org/abs/2409.11270v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a neural network that optimizes the precoder and phase shifts for reconfigurable intelligent surface-assisted systems, achieving better performance in terms of rate, power consumption, and convergence speed.</p><hr><h3>The Art of Storytelling: Multi-Agent Generative AI for Dynamic  Multimodal Narratives</h3>
<p><a href='http://arxiv.org/abs/2409.11261v1'>http://arxiv.org/abs/2409.11261v1</a></p>
<p><b>Compressor summary</b>: The paper presents an education tool that uses Generative AI to create interactive stories for children by combining narrative co-creation, text-to-speech, and text-to-video generation.</p><hr><h3>Temporal As a Plugin: Unsupervised Video Denoising with Pre-Trained  Image Denoisers</h3>
<p><a href='http://arxiv.org/abs/2409.11256v1'>http://arxiv.org/abs/2409.11256v1</a></p>
<p><b>Compressor summary</b>: TAP is an unsupervised video denoising method that uses a pre-trained image denoiser with tunable temporal modules to harness temporal information and improve denoising performance.</p><hr><h3>Norm of Mean Contextualized Embeddings Determines their Variance</h3>
<p><a href='http://arxiv.org/abs/2409.11253v1'>http://arxiv.org/abs/2409.11253v1</a></p>
<p><b>Compressor summary</b>: The study analyzes how the norm and variance of contextualized embeddings vary by context and layer in Transformer models, finding a trade-off relationship and a decomposition into within-cluster and between-cluster variances.</p><hr><h3>WER We Stand: Benchmarking Urdu ASR Models</h3>
<p><a href='http://arxiv.org/abs/2409.11252v1'>http://arxiv.org/abs/2409.11252v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates different ASR models for Urdu, comparing their performance on read and conversational speech using WER and error analysis, and highlighting the challenges of developing robust ASR systems for low-resource languages.</p><hr><h3>Linear Recency Bias During Training Improves Transformers' Fit to  Reading Times</h3>
<p><a href='http://arxiv.org/abs/2409.11250v1'>http://arxiv.org/abs/2409.11250v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates a modified Transformer model with ALiBi, which simulates memory decay and improves its fit to human reading times and sentence processing difficulty.</p><hr><h3>Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded  Attributions and Learning to Refuse</h3>
<p><a href='http://arxiv.org/abs/2409.11242v1'>http://arxiv.org/abs/2409.11242v1</a></p>
<p><b>Compressor summary</b>: The authors introduce Trust-Score, a metric to evaluate the trustworthiness of LLMs in RAG systems, and propose Trust-Align, a framework to improve LLMs' performance on RAG tasks.</p><hr><h3>Spontaneous Informal Speech Dataset for Punctuation Restoration</h3>
<p><a href='http://arxiv.org/abs/2409.11241v1'>http://arxiv.org/abs/2409.11241v1</a></p>
<p><b>Compressor summary</b>: SponSpeech is a new dataset for punctuation restoration in spontaneous speech, with a filtering pipeline to generate more data and a challenging test set.</p><hr><h3>LLM-as-a-Judge & Reward Model: What They Can and Cannot Do</h3>
<p><a href='http://arxiv.org/abs/2409.11239v1'>http://arxiv.org/abs/2409.11239v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes automated evaluators' performance outside of English, finding that English skills transfer well but LLMs have issues with errors and unwanted language in non-English settings.</p><hr><h3>Cost-informed dimensionality reduction for structural digital twin  technologies</h3>
<p><a href='http://arxiv.org/abs/2409.11236v1'>http://arxiv.org/abs/2409.11236v1</a></p>
<p><b>Compressor summary</b>: The paper presents a decision-theoretic method for dimensionality reduction in structural asset management, balancing misclassification costs and preserving discriminatory information.</p><hr><h3>SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking</h3>
<p><a href='http://arxiv.org/abs/2409.11235v1'>http://arxiv.org/abs/2409.11235v1</a></p>
<p><b>Compressor summary</b>: The paper introduces SLAck, a unified framework that uses semantics, location, and appearance priors to improve open-vocabulary multiple object tracking performance.</p><hr><h3>STCMOT: Spatio-Temporal Cohesion Learning for UAV-Based Multiple Object  Tracking</h3>
<p><a href='http://arxiv.org/abs/2409.11234v1'>http://arxiv.org/abs/2409.11234v1</a></p>
<p><b>Compressor summary</b>: The proposed Spatio-Temporal Cohesion Multiple Object Tracking framework (STCMOT) uses historical embedding features to improve target recognition and location in UAV videos, achieving state-of-the-art performance.</p><hr><h3>Evaluating the Impact of Compression Techniques on Task-Specific  Performance of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2409.11233v1'>http://arxiv.org/abs/2409.11233v1</a></p>
<p><b>Compressor summary</b>: This study evaluates compression methods for large language models, showing that SparseGPT and Wanda maintain perplexity but degrade downstream task performance, and introduces JS Divergence as a better metric while emphasizing the importance of task-specific calibration data.</p><hr><h3>Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT  Problem: Does the LLM Solve the Problem Itself or Call an External SAT  Solver?</h3>
<p><a href='http://arxiv.org/abs/2409.11232v1'>http://arxiv.org/abs/2409.11232v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes how well the OpenAI O1-preview model uses external solvers to solve random K-SAT problems, and investigates if it shows any sign of intelligent behavior or just makes random guesses.</p><hr><h3>Score Forgetting Distillation: A Swift, Data-Free Method for Machine  Unlearning in Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2409.11219v1'>http://arxiv.org/abs/2409.11219v1</a></p>
<p><b>Compressor summary</b>: This paper proposes Score Forgetting Distillation (SFD), an innovative machine unlearning method that promotes the forgetting of undesirable information in diffusion models by aligning the conditional scores of "unsafe" classes with those of "safe" ones, without requiring real data.</p><hr><h3>Exploring ChatGPT-based Augmentation Strategies for Contrastive  Aspect-based Sentiment Analysis</h3>
<p><a href='http://arxiv.org/abs/2409.11218v1'>http://arxiv.org/abs/2409.11218v1</a></p>
<p><b>Compressor summary</b>: The text discusses using ChatGPT, a large language model, for data augmentation in aspect-based sentiment analysis, improving performance with three strategies and contrastive learning.</p><hr><h3>Self-Evolutionary Large Language Models through Uncertainty-Enhanced  Preference Optimization</h3>
<p><a href='http://arxiv.org/abs/2409.11212v1'>http://arxiv.org/abs/2409.11212v1</a></p>
<p><b>Compressor summary</b>: The UPO framework uses uncertainty estimation and reliable feedback sampling to improve large language models' self-evolution and response generation in iterative preference optimization.</p><hr><h3>SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2409.11211v1'>http://arxiv.org/abs/2409.11211v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an optimization strategy to improve 3D Gaussian Splatting, a method for reconstructing 3D scenes from multi-view images, by modeling splat features as outputs of an implicit neural field.</p><hr><h3>Synthetic data augmentation for robotic mobility aids to support blind  and low vision people</h3>
<p><a href='http://arxiv.org/abs/2409.11164v1'>http://arxiv.org/abs/2409.11164v1</a></p>
<p><b>Compressor summary</b>: The study shows that using synthetic data can improve the performance of deep learning-based vision models for robotic mobility aids for blind and low-vision people, but also highlights their limitations compared to real-world data.</p><hr><h3>SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with  Customisable Fairness Calibration</h3>
<p><a href='http://arxiv.org/abs/2409.11149v1'>http://arxiv.org/abs/2409.11149v1</a></p>
<p><b>Compressor summary</b>: SAGED is a benchmarking pipeline that detects and mitigates biases in large language models by using counterfactual branching and baseline calibration.</p><hr><h3>Improving the Efficiency of Visually Augmented Language Models</h3>
<p><a href='http://arxiv.org/abs/2409.11148v1'>http://arxiv.org/abs/2409.11148v1</a></p>
<p><b>Compressor summary</b>: This paper introduces BLIND-VALM, a visually-augmented LM that uses text representations from CLIP instead of images, achieving similar performance to existing methods with less computation and complexity.</p><hr><h3>Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning</h3>
<p><a href='http://arxiv.org/abs/2409.11147v1'>http://arxiv.org/abs/2409.11147v1</a></p>
<p><b>Compressor summary</b>: RGER is a novel method that uses graph kernels to select exemplars for in-context learning based on both semantic and structural similarity, improving the performance of large language models on reasoning tasks.</p><hr><h3>Semformer: Transformer Language Models with Semantic Planning</h3>
<p><a href='http://arxiv.org/abs/2409.11143v1'>http://arxiv.org/abs/2409.11143v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Semformer, a new method for training Transformer language models that uses planning tokens to guide semantic representation prediction, reducing shortcut learning and improving performance on various tasks.</p><hr><h3>Scale generalisation properties of extended scale-covariant and  scale-invariant Gaussian derivative networks on image datasets with spatial  scaling variations</h3>
<p><a href='http://arxiv.org/abs/2409.11140v1'>http://arxiv.org/abs/2409.11140v1</a></p>
<p><b>Compressor summary</b>: The paper analyses scale generalisation in Gaussian derivative networks using new datasets, pooling methods, regularisation, and discrete approximations, and shows their good performance and explainability.</p><hr><h3>Learning Generalized Hamiltonians using fully Symplectic Mappings</h3>
<p><a href='http://arxiv.org/abs/2409.11138v1'>http://arxiv.org/abs/2409.11138v1</a></p>
<p><b>Compressor summary</b>: The text describes how physics informed neural networks, especially Hamiltonian neural networks, can improve the performance of standard neural networks by incorporating physical invariances and conserving energy, and proposes a method to reconstruct and conserve Hamiltonians for generalized non-separable systems using symplectic integrators.</p><hr><h3>Can Graph Reordering Speed Up Graph Neural Network Training? An  Experimental Study</h3>
<p><a href='http://arxiv.org/abs/2409.11129v1'>http://arxiv.org/abs/2409.11129v1</a></p>
<p><b>Compressor summary</b>: Graph reordering optimizes GNN training by improving memory access patterns and reduces training time on different systems and hyperparameters.</p><hr><h3>Genetic Information Analysis of Age-Related Macular Degeneration Fellow  Eye Using Multi-Modal Selective ViT</h3>
<p><a href='http://arxiv.org/abs/2409.11128v1'>http://arxiv.org/abs/2409.11128v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a machine learning method to predict AMD susceptibility genes using fundus and OCT images, as well as medical records, achieving over 80% accuracy.</p><hr><h3>Gradient-free Post-hoc Explainability Using Distillation Aided Learnable  Approach</h3>
<p><a href='http://arxiv.org/abs/2409.11123v1'>http://arxiv.org/abs/2409.11123v1</a></p>
<p><b>Compressor summary</b>: DAX is a framework that generates saliency-based explanations for deep models without using gradients or model specific information, and it performs better than existing methods in various settings.</p><hr><h3>Diversity-grounded Channel Prototypical Learning for Out-of-Distribution  Intent Detection</h3>
<p><a href='http://arxiv.org/abs/2409.11114v1'>http://arxiv.org/abs/2409.11114v1</a></p>
<p><b>Compressor summary</b>: The study proposes a new fine-tuning framework for large language models to enhance intent classification for task-oriented dialogue systems, using semantic matching with prototypes derived from class names.</p><hr><h3>Strategic Insights in Human and Large Language Model Tactics at Word  Guessing Games</h3>
<p><a href='http://arxiv.org/abs/2409.11112v1'>http://arxiv.org/abs/2409.11112v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how players strategize and learn in a word-guessing game over two years and tests large language models' abilities to understand and play the game in different languages.</p><hr><h3>Quantitative Evaluation of MILs' Reliability For WSIs Classification</h3>
<p><a href='http://arxiv.org/abs/2409.11110v1'>http://arxiv.org/abs/2409.11110v1</a></p>
<p><b>Compressor summary</b>: The paper compares the reliability of different models for classifying Whole Slide Images in pathology using three metrics and datasets, and finds the MEAN-POOL-INS model to be the most reliable.</p><hr><h3>Depth-based Privileged Information for Boosting 3D Human Pose Estimation  on RGB</h3>
<p><a href='http://arxiv.org/abs/2409.11104v1'>http://arxiv.org/abs/2409.11104v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to estimate 3D human pose from single RGB images by hallucinating depth information using a heatmap-based estimator and Privileged Information.</p><hr><h3>Fractional Naive Bayes (FNB): non-convex optimization for a parsimonious  weighted selective naive Bayes classifier</h3>
<p><a href='http://arxiv.org/abs/2409.11100v1'>http://arxiv.org/abs/2409.11100v1</a></p>
<p><b>Compressor summary</b>: The text describes a method to improve naive Bayes classification by estimating variable weights and using sparse regularization.</p><hr><h3>MonoKAN: Certified Monotonic Kolmogorov-Arnold Network</h3>
<p><a href='http://arxiv.org/abs/2409.11078v1'>http://arxiv.org/abs/2409.11078v1</a></p>
<p><b>Compressor summary</b>: MonoKAN is a novel ANN architecture that combines the interpretability of KAN with certified partial monotonicity using cubic Hermite splines and positive weights.</p><hr><h3>ShapeAug++: More Realistic Shape Augmentation for Event Data</h3>
<p><a href='http://arxiv.org/abs/2409.11075v1'>http://arxiv.org/abs/2409.11075v1</a></p>
<p><b>Compressor summary</b>: ShapeAug++ is a method that enhances occlusion handling in DVS event data using random polygons and curved movements, leading to improved top-1 accuracy for DVS classification.</p><hr><h3>RoMath: A Mathematical Reasoning Benchmark in Romanian</h3>
<p><a href='http://arxiv.org/abs/2409.11074v1'>http://arxiv.org/abs/2409.11074v1</a></p>
<p><b>Compressor summary</b>: RoMath is a Romanian mathematical reasoning benchmark suite that aims to improve non-English language models and promote multilingual AI development by covering various domains and difficulty levels in mathematics.</p><hr><h3>Improve Machine Learning carbon footprint using Parquet dataset format  and Mixed Precision training for regression algorithms</h3>
<p><a href='http://arxiv.org/abs/2409.11071v1'>http://arxiv.org/abs/2409.11071v1</a></p>
<p><b>Compressor summary</b>: The study found that using mixed precision and carefully chosen hyper-parameters can reduce power consumption in regression ML models, but there was no statistical significance between different techniques or dataset formats.</p><hr><h3>A Reinforcement Learning Environment for Automatic Code Optimization in  the MLIR Compiler</h3>
<p><a href='http://arxiv.org/abs/2409.11068v1'>http://arxiv.org/abs/2409.11068v1</a></p>
<p><b>Compressor summary</b>: The project introduces a new RL environment for the MLIR compiler that uses Multi-Action Reinforcement Learning to optimize code performance and achieves comparable or better results than TensorFlow.</p><hr><h3>HMF: A Hybrid Multi-Factor Framework for Dynamic Intraoperative  Hypotension Prediction</h3>
<p><a href='http://arxiv.org/abs/2409.11064v1'>http://arxiv.org/abs/2409.11064v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel Hybrid Multi-Factor framework using a Transformer encoder to predict intraoperative hypotension as a blood pressure forecasting task, addressing distribution shift and sequence dependencies.</p><hr><h3>OneEncoder: A Lightweight Framework for Progressive Alignment of  Modalities</h3>
<p><a href='http://arxiv.org/abs/2409.11059v1'>http://arxiv.org/abs/2409.11059v1</a></p>
<p><b>Compressor summary</b>: OneEncoder is a lightweight framework for cross-modal alignment learning that efficiently integrates information from image, text, audio, and video modalities using a Universal Projection module.</p><hr><h3>KVPruner: Structural Pruning for Faster and Memory-Efficient Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2409.11057v1'>http://arxiv.org/abs/2409.11057v1</a></p>
<p><b>Compressor summary</b>: KVPruner is a method to improve efficiency and speed of large language models by pruning non-essential key-value channels using global perplexity analysis and requiring minimal recovery training.</p><hr><h3>Large Language Models are Good Multi-lingual Learners : When LLMs Meet  Cross-lingual Prompts</h3>
<p><a href='http://arxiv.org/abs/2409.11056v1'>http://arxiv.org/abs/2409.11056v1</a></p>
<p><b>Compressor summary</b>: MLPrompt is a new method that helps LLMs understand and follow complex rules by translating them into different languages, leading to better performance than existing methods in various tasks.</p><hr><h3>A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language  Models: An Experimental Analysis up to 405B</h3>
<p><a href='http://arxiv.org/abs/2409.11055v1'>http://arxiv.org/abs/2409.11055v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates how different quantization methods affect the performance of large language models on various tasks, finding that larger models generally perform better with similar size quantization as smaller ones, and weight-only methods often yield better results in larger models.</p><hr><h3>A logical alarm for misaligned binary classifiers</h3>
<p><a href='http://arxiv.org/abs/2409.11052v1'>http://arxiv.org/abs/2409.11052v1</a></p>
<p><b>Compressor summary</b>: The text discusses a method to evaluate binary classifiers by using axioms that ensure logical consistency and allow proving malfunctions with unlabeled data, which has applications in safe AI.</p><hr><h3>Down-Sampling Inter-Layer Adapter for Parameter and Computation  Efficient Ultra-Fine-Grained Image Recognition</h3>
<p><a href='http://arxiv.org/abs/2409.11051v1'>http://arxiv.org/abs/2409.11051v1</a></p>
<p><b>Compressor summary</b>: Our method improves ultra-fine-grained image recognition accuracy with less parameters, less floating-point operations, and frozen backbone using down-sampling inter-layer adapters.</p><hr><h3>Towards No-Code Programming of Cobots: Experiments with Code Synthesis  by Large Code Models for Conversational Programming</h3>
<p><a href='http://arxiv.org/abs/2409.11041v1'>http://arxiv.org/abs/2409.11041v1</a></p>
<p><b>Compressor summary</b>: The paper explores using large language models to create natural language instructions for collaborative robots in assembly tasks, finding that they can generate accurate first order code but struggle with higher-order code.</p><hr><h3>Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI</h3>
<p><a href='http://arxiv.org/abs/2409.11032v1'>http://arxiv.org/abs/2409.11032v1</a></p>
<p><b>Compressor summary</b>: A hierarchical framework using large language models can reveal argumentative patterns in textual narratives, as demonstrated by analyzing public opinions on generative AI.</p><hr><h3>Estimating the distribution of numerosity and non-numerical visual  magnitudes in natural scenes using computer vision</h3>
<p><a href='http://arxiv.org/abs/2409.11028v1'>http://arxiv.org/abs/2409.11028v1</a></p>
<p><b>Compressor summary</b>: The authors develop a computer vision pipeline to analyze natural images and find that numerosity perception follows a power law distribution and is correlated with other continuous magnitudes.</p><hr><h3>D2Vformer: A Flexible Time Series Prediction Model Based on Time  Position Embedding</h3>
<p><a href='http://arxiv.org/abs/2409.11024v1'>http://arxiv.org/abs/2409.11024v1</a></p>
<p><b>Compressor summary</b>: D2Vformer is a novel model that uses date2vec to generate time position embeddings and an attention mechanism to make predictions on time series data, outperforming existing methods in various scenarios.</p><hr><h3>GEIC: Universal and Multilingual Named Entity Recognition with Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2409.11022v1'>http://arxiv.org/abs/2409.11022v1</a></p>
<p><b>Compressor summary</b>: Key points:
- LLMs are powerful but not efficient for NER tasks
- GEIC is a new task to use LLMs for extraction and in-context classification
- CascadeNER is a framework that uses two small LLMs in cascading for few-shot and zero-shot NER
- AnythingNER is a new multilingual NER dataset for LLMs
- CascadeNER outperforms baselines on low-resource and fine-grained scenarios

Summary:
The paper proposes CascadeNER, a framework that uses two small LLMs in cascading to perform few-shot and zero-shot NER on the new AnythingNER dataset, achieving state-of-the-art results.</p><hr><h3>MM2Latent: Text-to-facial image generation and editing in GANs with  multimodal assistance</h3>
<p><a href='http://arxiv.org/abs/2409.11010v1'>http://arxiv.org/abs/2409.11010v1</a></p>
<p><b>Compressor summary</b>: The paper introduces MM2Latent, a practical framework for multimodal image generation and editing that improves controllability and efficiency over existing methods.</p><hr><h3>Latent mixed-effect models for high-dimensional longitudinal data</h3>
<p><a href='http://arxiv.org/abs/2409.11008v1'>http://arxiv.org/abs/2409.11008v1</a></p>
<p><b>Compressor summary</b>: The paper introduces LMM-VAE, a scalable and interpretable model that combines linear mixed models and variational autoencoders for modelling longitudinal data.</p><hr><h3>CAST: Cross-modal Alignment Similarity Test for Vision Language Models</h3>
<p><a href='http://arxiv.org/abs/2409.11007v1'>http://arxiv.org/abs/2409.11007v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new test (CAST) to evaluate vision language models' consistency across visual and language inputs, which is important for their generalization abilities.</p><hr><h3>Enhancing Low-Resource Language and Instruction Following Capabilities  of Audio Language Models</h3>
<p><a href='http://arxiv.org/abs/2409.10999v1'>http://arxiv.org/abs/2409.10999v1</a></p>
<p><b>Compressor summary</b>: This paper studies how to improve audio language models for low-resource languages like Thai, and proposes Typhoon-Audio, which performs better than existing models and is comparable to state-of-the-art Gemini-1.5-Pro in English and Thai.</p><hr><h3>Contextual Breach: Assessing the Robustness of Transformer-based QA  Models</h3>
<p><a href='http://arxiv.org/abs/2409.10997v1'>http://arxiv.org/abs/2409.10997v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a dataset with different types of adversarial noises to test how well question-answering models perform under realistic distorted inputs.</p><hr><h3>GINTRIP: Interpretable Temporal Graph Regression using Information  bottleneck and Prototype-based method</h3>
<p><a href='http://arxiv.org/abs/2409.10996v1'>http://arxiv.org/abs/2409.10996v1</a></p>
<p><b>Compressor summary</b>: The paper presents a novel framework for interpreting temporal graph regression models using Information Bottleneck and prototype-based methods, which improves performance and interpretability on traffic datasets.</p><hr><h3>Less is More: A Simple yet Effective Token Reduction Method for  Efficient Multi-modal LLMs</h3>
<p><a href='http://arxiv.org/abs/2409.10994v1'>http://arxiv.org/abs/2409.10994v1</a></p>
<p><b>Compressor summary</b>: TRIM is a new approach to improve the efficiency of Multimodal Large Language Models by reducing image tokens using CLIP Metric, achieving significant computational savings without sacrificing performance.</p><hr><h3>GOSt-MT: A Knowledge Graph for Occupation-related Gender Biases in  Machine Translation</h3>
<p><a href='http://arxiv.org/abs/2409.10989v1'>http://arxiv.org/abs/2409.10989v1</a></p>
<p><b>Compressor summary</b>: The paper presents GOSt-MT, a Knowledge Graph that analyzes gender bias in machine translation across multiple languages by integrating labour data and textual corpora.</p><hr><h3>Relative Representations: Topological and Geometric Perspectives</h3>
<p><a href='http://arxiv.org/abs/2409.10967v1'>http://arxiv.org/abs/2409.10967v1</a></p>
<p><b>Compressor summary</b>: The paper proposes two improvements to relative representations for zero-shot model stitching: normalization and topological densification, leading to better performance on a natural language task.</p><hr><h3>Cross-lingual transfer of multilingual models on low resource African  Languages</h3>
<p><a href='http://arxiv.org/abs/2409.10965v1'>http://arxiv.org/abs/2409.10965v1</a></p>
<p><b>Compressor summary</b>: This study compares monolingual and multilingual NLP models for cross-lingual transfer between Kinyarwanda and Kirundi, finding that multilingual models perform better while still being competitive.</p><hr><h3>Versatile Incremental Learning: Towards Class and Domain-Agnostic  Incremental Learning</h3>
<p><a href='http://arxiv.org/abs/2409.10956v1'>http://arxiv.org/abs/2409.10956v1</a></p>
<p><b>Compressor summary</b>: The paper proposes ICON, a framework for Versatile Incremental Learning (VIL) that tackles class and domain confusion using CAST regularization and an Incremental Classifier to avoid overwriting and accumulate new knowledge effectively.</p><hr><h3>Investigating Context-Faithfulness in Large Language Models: The Roles  of Memory Strength and Evidence Style</h3>
<p><a href='http://arxiv.org/abs/2409.10955v1'>http://arxiv.org/abs/2409.10955v1</a></p>
<p><b>Compressor summary</b>: This study examines how memory strength and evidence presentation affect Large Language Models' context-faithfulness when incorporating external information into their responses.</p><hr><h3>Fair Anomaly Detection For Imbalanced Groups</h3>
<p><a href='http://arxiv.org/abs/2409.10951v1'>http://arxiv.org/abs/2409.10951v1</a></p>
<p><b>Compressor summary</b>: FairAD is a new anomaly detection method that ensures fairness in imbalanced scenarios by using contrastive learning and rebalancing autoencoder modules.</p><hr><h3>Contrasformer: A Brain Network Contrastive Transformer for  Neurodegenerative Condition Identification</h3>
<p><a href='http://arxiv.org/abs/2409.10944v1'>http://arxiv.org/abs/2409.10944v1</a></p>
<p><b>Compressor summary</b>: The text introduces Contrasformer, a novel contrastive brain network Transformer that improves the identification of neurological disorders using prior-knowledge-enhanced contrast graphs and attention mechanisms.</p><hr><h3>Optimizing TinyML: The Impact of Reduced Data Acquisition Rates for Time  Series Classification on Microcontrollers</h3>
<p><a href='http://arxiv.org/abs/2409.10942v1'>http://arxiv.org/abs/2409.10942v1</a></p>
<p><b>Compressor summary</b>: This paper shows how reducing data acquisition rates can improve the efficiency, energy consumption, and latency of TinyML models for time series classification on IoT devices with minimal accuracy loss.</p><hr><h3>Propulsion: Steering LLM with Tiny Fine-Tuning</h3>
<p><a href='http://arxiv.org/abs/2409.10927v1'>http://arxiv.org/abs/2409.10927v1</a></p>
<p><b>Compressor summary</b>: Propulsion is a novel method that efficiently fine-tunes large language models for specific tasks by selectively scaling pre-trained dimensions without modifying the model's parameters, reducing computational overhead and maintaining performance.</p><hr><h3>KALE: An Artwork Image Captioning System Augmented with Heterogeneous  Graph</h3>
<p><a href='http://arxiv.org/abs/2409.10921v1'>http://arxiv.org/abs/2409.10921v1</a></p>
<p><b>Compressor summary</b>: The paper introduces KALE, a model that generates detailed and meaningful captions for fine-art paintings by using artwork metadata as additional knowledge.</p><hr><h3>AMEGO: Active Memory from long EGOcentric videos</h3>
<p><a href='http://arxiv.org/abs/2409.10917v1'>http://arxiv.org/abs/2409.10917v1</a></p>
<p><b>Compressor summary</b>: AMEGO enhances comprehension of long egocentric videos by constructing self-contained representations from them, and it is evaluated on the new Active Memories Benchmark.</p><hr><h3>A Physics Informed Neural Network (PINN) Methodology for Coupled Moving  Boundary PDEs</h3>
<p><a href='http://arxiv.org/abs/2409.10910v1'>http://arxiv.org/abs/2409.10910v1</a></p>
<p><b>Compressor summary</b>: PINN is a framework that uses deep learning to solve physical problems involving moving boundaries, such as solidification of alloys, by integrating physics knowledge and constraints into neural networks.</p><hr><h3>Attention-Seeker: Dynamic Self-Attention Scoring for Unsupervised  Keyphrase Extraction</h3>
<p><a href='http://arxiv.org/abs/2409.10907v1'>http://arxiv.org/abs/2409.10907v1</a></p>
<p><b>Compressor summary</b>: Attention-Seeker is an unsupervised keyphrase extraction method that uses self-attention maps from a Large Language Model to estimate the importance of phrases without manual tuning, achieving state-of-the-art results on four datasets.</p><hr><h3>WaterQualityNeT: Prediction of Seasonal Water Quality of Nepal Using  Hybrid Deep Learning Models</h3>
<p><a href='http://arxiv.org/abs/2409.10898v1'>http://arxiv.org/abs/2409.10898v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Paper presents hybrid deep learning model for predicting Nepal's seasonal water quality
- Model combines CNN and RNN to capture temporal and spatial patterns
- Model outperforms traditional methods and provides reliable tool for water quality control

Summary:
The paper introduces a novel deep learning model that integrates CNN and RNN to forecast Nepal's seasonal water quality from a small dataset, achieving better accuracy than conventional methods.</p><hr><h3>AutoSpec: Automated Generation of Neural Network Specifications</h3>
<p><a href='http://arxiv.org/abs/2409.10897v1'>http://arxiv.org/abs/2409.10897v1</a></p>
<p><b>Compressor summary</b>: AutoSpec is a framework that automatically generates accurate specifications for neural networks in learning-augmented systems, overcoming the limitations of manual specification processes.</p><hr><h3>Shaking the Fake: Detecting Deepfake Videos in Real Time via Active  Probes</h3>
<p><a href='http://arxiv.org/abs/2409.10889v1'>http://arxiv.org/abs/2409.10889v1</a></p>
<p><b>Compressor summary</b>: SFake is a new real-time deepfake detection method that uses mechanical vibrations to identify face swapping in videos.</p><hr><h3>CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation  for Meeting Summarization</h3>
<p><a href='http://arxiv.org/abs/2409.10883v1'>http://arxiv.org/abs/2409.10883v1</a></p>
<p><b>Compressor summary</b>: CREAM is a novel framework that evaluates meeting summarizations without using references by combining chain-of-thought reasoning and key facts alignment with an ELO ranking system.</p><hr><h3>American Sign Language to Text Translation using Transformer and Seq2Seq  with LSTM</h3>
<p><a href='http://arxiv.org/abs/2409.10874v1'>http://arxiv.org/abs/2409.10874v1</a></p>
<p><b>Compressor summary</b>: The study compares the performance of Transformer and Seq2Seq models in sign language translation and tests the impact of adding ResidualLSTM to the Transformer model, which reduces its BLEU Score by 23.37%.</p><hr><h3>Adaptive Large Language Models By Layerwise Attention Shortcuts</h3>
<p><a href='http://arxiv.org/abs/2409.10870v1'>http://arxiv.org/abs/2409.10870v1</a></p>
<p><b>Compressor summary</b>: The paper introduces adaptive computations for LLMs to make AI architectures depth and context adaptive using attention shortcuts, improving performance on various datasets.</p><hr><h3>3DFacePolicy: Speech-Driven 3D Facial Animation with Diffusion Policy</h3>
<p><a href='http://arxiv.org/abs/2409.10848v1'>http://arxiv.org/abs/2409.10848v1</a></p>
<p><b>Compressor summary</b>: 3DFacePolicy is a diffusion policy model for realistic 3D facial animation prediction using audio and vertex states to imitate human emotions.</p><hr><h3>BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion  Generation</h3>
<p><a href='http://arxiv.org/abs/2409.10847v1'>http://arxiv.org/abs/2409.10847v1</a></p>
<p><b>Compressor summary</b>: Bidirectional Autoregressive Diffusion (BAD) combines autoregressive and mask-based models to improve sequence modeling by using a permutation-based corruption technique that preserves structure and enforces causality, leading to better text-to-motion generation.</p><hr><h3>Implicit Reasoning in Deep Time Series Forecasting</h3>
<p><a href='http://arxiv.org/abs/2409.10840v1'>http://arxiv.org/abs/2409.10840v1</a></p>
<p><b>Compressor summary</b>: This study evaluates the reasoning abilities of deep time series forecasting models and finds evidence of effective generalization beyond pattern memorization.</p><hr><h3>Machine Learning for Public Good: Predicting Urban Crime Patterns to  Enhance Community Safety</h3>
<p><a href='http://arxiv.org/abs/2409.10838v1'>http://arxiv.org/abs/2409.10838v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Urban safety is a major concern that can benefit from accurate crime prediction using ML techniques
- The paper uses police dispatch call data from San Jose, CA to categorize calls into priority levels based on time, place, and nature
- Random Forest models are the most effective in identifying dangerous situations with high accuracy and low false negatives

Summary:
The paper applies ML techniques, especially Random Forest models, to predict and categorize crime risks from police dispatch call data in San Jose, CA, improving urban safety outcomes.</p><hr><h3>ReXErr: Synthesizing Clinically Meaningful Errors in Diagnostic  Radiology Reports</h3>
<p><a href='http://arxiv.org/abs/2409.10829v1'>http://arxiv.org/abs/2409.10829v1</a></p>
<p><b>Compressor summary</b>: ReXErr is a method that uses large language models to generate realistic errors in chest X-ray reports for improving radiology reporting quality and reliability.</p>