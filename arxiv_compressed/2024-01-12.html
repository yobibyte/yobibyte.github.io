
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-01-12</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-01-12 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Distilling Vision-Language Models on Millions of Videos</h3>
<p>Yue Zhao,Long Zhao,Xingyi Zhou,Jialin Wu,Chun-Te Chu,Hui Miao,Florian Schroff,Hartwig Adam,Ting Liu,Boqing Gong,Philipp Krähenbühl,Liangzhe Yuan</p>
<p><a href='http://arxiv.org/abs/2401.06129v1'>http://arxiv.org/abs/2401.06129v1</a></p>
<p><b>Compressor summary</b>: The authors create a video-language model using synthesized data and fine-tuning, which improves performance on various benchmarks compared to existing methods.</p><hr><h3>E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image  Translation</h3>
<p>Yifan Gong,Zheng Zhan,Qing Jin,Yanyu Li,Yerlan Idelbayev,Xian Liu,Andrey Zharkov,Kfir Aberman,Sergey Tulyakov,Yanzhi Wang,Jian Ren</p>
<p><a href='http://arxiv.org/abs/2401.06127v1'>http://arxiv.org/abs/2401.06127v1</a></p>
<p><b>Compressor summary</b>: The text describes a novel approach to improve the efficiency of distilling GANs from diffusion models for real-time image editing on mobile devices using innovative techniques like generalized features, Low-Rank Adaptation, and minimal data fine-tuning.</p><hr><h3>Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural  Rendering Priors</h3>
<p>Jack Saunders,Vinay Namboodiri</p>
<p><a href='http://arxiv.org/abs/2401.06126v1'>http://arxiv.org/abs/2401.06126v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method for high-quality visual dubbing using data-efficient neural rendering priors and actor-specific adaptation, which generalizes to limited data and outperforms existing approaches in terms of visual quality and recognizability.</p><hr><h3>Manipulating Feature Visualizations with Gradient Slingshots</h3>
<p>Dilyara Bareeva,Marina M. -C. Höhne,Alexander Warnecke,Lukas Pirch,Klaus-Robert Müller,Konrad Rieck,Kirill Bykov</p>
<p><a href='http://arxiv.org/abs/2401.06122v1'>http://arxiv.org/abs/2401.06122v1</a></p>
<p><b>Compressor summary</b>: The paper explores how adversarial manipulations can falsify neural network explanations and proposes a method to protect them.</p><hr><h3>TOFU: A Task of Fictitious Unlearning for LLMs</h3>
<p>Pratyush Maini,Zhili Feng,Avi Schwarzschild,Zachary C. Lipton,J. Zico Kolter</p>
<p><a href='http://arxiv.org/abs/2401.06121v1'>http://arxiv.org/abs/2401.06121v1</a></p>
<p><b>Compressor summary</b>: TOFU is a benchmark for evaluating the effectiveness of unlearning methods in large language models, using synthetic author profiles and diverse metrics.</p><hr><h3>Extreme Compression of Large Language Models via Additive Quantization</h3>
<p>Vage Egiazarian,Andrei Panferov,Denis Kuznedelev,Elias Frantar,Artem Babenko,Dan Alistarh</p>
<p><a href='http://arxiv.org/abs/2401.06118v1'>http://arxiv.org/abs/2401.06118v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm called Additive Quantization for Language Models (AQLM) that significantly improves the compression and accuracy of large language models, enabling them to run on end-user devices with very low bit counts.</p><hr><h3>Gaussian Shadow Casting for Neural Characters</h3>
<p>Luis Bolanos,Shih-Yang Su,Helge Rhodin</p>
<p><a href='http://arxiv.org/abs/2401.06116v1'>http://arxiv.org/abs/2401.06116v1</a></p>
<p><b>Compressor summary</b>: The proposed Gaussian shadow model enables realistic shadows and shading in neural character models by using a simple analytic formula instead of costly sampling, improving reconstructions and poses in various scenes.</p><hr><h3>Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed  Embeddings</h3>
<p>Hiroaki Yamagiwa,Yusuke Takase,Hidetoshi Shimodaira</p>
<p><a href='http://arxiv.org/abs/2401.06112v1'>http://arxiv.org/abs/2401.06112v1</a></p>
<p><b>Compressor summary</b>: Axis Tour is a novel method that optimizes the order of interpretable semantic axes in word embeddings using Independent Component Analysis (ICA) for improved clarity and performance on downstream tasks.</p><hr><h3>PALP: Prompt Aligned Personalization of Text-to-Image Models</h3>
<p>Moab Arar,Andrey Voynov,Amir Hertz,Omri Avrahami,Shlomi Fruchter,Yael Pritch,Daniel Cohen-Or,Ariel Shamir</p>
<p><a href='http://arxiv.org/abs/2401.06105v1'>http://arxiv.org/abs/2401.06105v1</a></p>
<p><b>Compressor summary</b>: Prompt-aligned personalization is a new method for generating personalized images that align well with complex textual prompts, while preserving subject fidelity and user requirements.</p><hr><h3>Transformers are Multi-State RNNs</h3>
<p>Matanel Oren,Michael Hassid,Yossi Adi,Roy Schwartz</p>
<p><a href='http://arxiv.org/abs/2401.06104v1'>http://arxiv.org/abs/2401.06104v1</a></p>
<p><b>Compressor summary</b>: This paper shows that transformers can be seen as infinite multi-state RNNs and introduces a new conversion policy, TOVA, which improves performance on long range tasks with less cache memory usage.</p><hr><h3>Patchscope: A Unifying Framework for Inspecting Hidden Representations  of Language Models</h3>
<p>Asma Ghandeharioun,Avi Caciularu,Adam Pearce,Lucas Dixon,Mor Geva</p>
<p><a href='http://arxiv.org/abs/2401.06102v1'>http://arxiv.org/abs/2401.06102v1</a></p>
<p><b>Compressor summary</b>: Patchscopes is a framework that explains the hidden representations of large language models in natural language, addressing limitations of prior methods and enabling new applications.</p><hr><h3>A Closer Look at AUROC and AUPRC under Class Imbalance</h3>
<p>Matthew B. A. McDermott,Lasse Hyldig Hansen,Haoran Zhang,Giovanni Angelotti,Jack Gallifant</p>
<p><a href='http://arxiv.org/abs/2401.06091v1'>http://arxiv.org/abs/2401.06091v1</a></p>
<p><b>Compressor summary</b>: The paper challenges the notion that AUPRC is superior to AUROC for binary classification tasks with class imbalance, and shows that AUPRC can be biased and harmful in such cases.</p><hr><h3>PANDORA: A Parallel Dendrogram Construction Algorithm for Single Linkage  Clustering on GPU</h3>
<p>Piyush Sao,Andrey Prokopenko,Damien Lebrun-Grandié</p>
<p><a href='http://arxiv.org/abs/2401.06089v1'>http://arxiv.org/abs/2401.06089v1</a></p>
<p><b>Compressor summary</b>: \pandora is a parallel algorithm that efficiently constructs dendrograms for hierarchical clustering, using a recursive tree contraction method, achieving significant speed-ups on CPUs and GPUs.</p><hr><h3>Autocompletion of Chief Complaints in the Electronic Health Records  using Large Language Models</h3>
<p>K M Sajjadul Islam,Ayesha Siddika Nipu,Praveen Madiraju,Priya Deshpande</p>
<p><a href='http://arxiv.org/abs/2401.06088v1'>http://arxiv.org/abs/2401.06088v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The text is about a study that develops an autocompletion tool for documenting Chief Complaints (CC) in medical records using machine learning models and natural language generation techniques.
- The study compares three variants of BioGPT and a Long Short-Term Memory (LSTM) model, as well as a GPT-4 prompt.
- The study evaluates the models' performance based on perplexity, modified BERTScore, and cosine similarity score.
- The results show that BioGPT-Large performs best among the models and leads to an effective autocompletion tool.

Summary:
The study develops and evaluates an autocompletion tool for generating Chief Complaints in medical records using various machine learning models and natural language generation techniques, finding that BioGPT-Large outperforms other models and produces accurate and well-formatted CC phrases or sentences.</p><hr><h3>XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an  Agent-Based Model of a Sports Betting Exchange</h3>
<p>Chawin Terawong,Dave Cliff</p>
<p><a href='http://arxiv.org/abs/2401.06086v1'>http://arxiv.org/abs/2401.06086v1</a></p>
<p><b>Compressor summary</b>: XGBoost is a machine learning method that learns profitable betting strategies from synthetic data generated by an agent-based model of a sports-betting exchange, and can generalize to outperform the original strategies.</p><hr><h3>Improving Large Language Models via Fine-grained Reinforcement Learning  with Minimum Editing Constraint</h3>
<p>Zhipeng Chen,Kun Zhou,Wayne Xin Zhao,Junchen Wan,Fuzheng Zhang,Di Zhang,Ji-Rong Wen</p>
<p><a href='http://arxiv.org/abs/2401.06081v1'>http://arxiv.org/abs/2401.06081v1</a></p>
<p><b>Compressor summary</b>: RLMEC is a new RL method that uses a generative model to provide token-level rewards for training large language models, improving their performance on complex reasoning tasks and reducing harmful outputs.</p><hr><h3>Secrets of RLHF in Large Language Models Part II: Reward Modeling</h3>
<p>Binghai Wang,Rui Zheng,Lu Chen,Yan Liu,Shihan Dou,Caishuang Huang,Wei Shen,Senjie Jin,Enyu Zhou,Chenyu Shi,Songyang Gao,Nuo Xu,Yuhao Zhou,Xiaoran Fan,Zhiheng Xi,Jun Zhao,Xiao Wang,Tao Ji,Hang Yan,Lixing Shen,Zhan Chen,Tao Gui,Qi Zhang,Xipeng Qiu,Xuanjing Huang,Zuxuan Wu,Yu-Gang Jiang</p>
<p><a href='http://arxiv.org/abs/2401.06080v1'>http://arxiv.org/abs/2401.06080v1</a></p>
<p><b>Compressor summary</b>: The paper proposes methods to improve reward models for reinforcement learning from human feedback by addressing challenges related to preference data quality and generalization.</p><hr><h3>Chain of History: Learning and Forecasting with LLMs for Temporal  Knowledge Graph Completion</h3>
<p>Ruilin Luo,Tianle Gu,Haoling Li,Junzhe Li,Zicheng Lin,Jiayi Li,Yujiu Yang</p>
<p><a href='http://arxiv.org/abs/2401.06072v1'>http://arxiv.org/abs/2401.06072v1</a></p>
<p><b>Compressor summary</b>: The paper presents a novel approach to predict missing event links in future timestamps using LLMs fine-tuned with historical data and structural information, achieving state-of-the-art results.</p><hr><h3>LEGO:Language Enhanced Multi-modal Grounding Model</h3>
<p>Zhaowei Li,Qi Xu,Dong Zhang,Hang Song,Yiqing Cai,Qi Qi,Ran Zhou,Junting Pan,Zefeng Li,Van Tu Vu,Zhida Huang,Tao Wang</p>
<p><a href='http://arxiv.org/abs/2401.06071v1'>http://arxiv.org/abs/2401.06071v1</a></p>
<p><b>Compressor summary</b>: The paper introduces LEGO, a multi-modal model that captures both global and local information across different modalities, enhancing its performance in various tasks requiring fine-grained understanding of input data.</p><hr><h3>DeepSeekMoE: Towards Ultimate Expert Specialization in  Mixture-of-Experts Language Models</h3>
<p>Damai Dai,Chengqi Deng,Chenggang Zhao,R. X. Xu,Huazuo Gao,Deli Chen,Jiashi Li,Wangding Zeng,Xingkai Yu,Y. Wu,Zhenda Xie,Y. K. Li,Panpan Huang,Fuli Luo,Chong Ruan,Zhifang Sui,Wenfeng Liang</p>
<p><a href='http://arxiv.org/abs/2401.06066v1'>http://arxiv.org/abs/2401.06066v1</a></p>
<p><b>Compressor summary</b>: DeepSeekMoE is a new language model architecture that improves expert specialization and reduces computational costs compared to conventional MoE architectures like GShard.</p><hr><h3>Investigating Data Contamination for Pre-training Language Models</h3>
<p>Minhao Jiang,Ken Ziyu Liu,Ming Zhong,Rylan Schaeffer,Siru Ouyang,Jiawei Han,Sanmi Koyejo</p>
<p><a href='http://arxiv.org/abs/2401.06059v1'>http://arxiv.org/abs/2401.06059v1</a></p>
<p><b>Compressor summary</b>: The paper explores how pre-training language models with evaluation data affects their performance on downstream tasks and highlights limitations of current contamination definitions.</p><hr><h3>MatSynth: A Modern PBR Materials Dataset</h3>
<p>Giuseppe Vecchio,Valentin Deschaintre</p>
<p><a href='http://arxiv.org/abs/2401.06056v1'>http://arxiv.org/abs/2401.06056v1</a></p>
<p><b>Compressor summary</b>: MatSynth is a large and diverse dataset of high-quality, public domain materials for use in virtual environments.</p><hr><h3>Fast High Dynamic Range Radiance Fields for Dynamic Scenes</h3>
<p>Guanjun Wu,Taoran Yi,Jiemin Fang,Wenyu Liu,Xinggang Wang</p>
<p><a href='http://arxiv.org/abs/2401.06052v1'>http://arxiv.org/abs/2401.06052v1</a></p>
<p><b>Compressor summary</b>: The paper proposes HDR-HexPlane, a dynamic HDR NeRF framework that can learn 3D scenes from dynamic 2D images with various exposures and render high-quality novel-view images at any time point with desired exposure.</p><hr><h3>Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for  Traffic Forecasting</h3>
<p>Qipeng Qian,Tanwi Mallick</p>
<p><a href='http://arxiv.org/abs/2401.06040v1'>http://arxiv.org/abs/2401.06040v1</a></p>
<p><b>Compressor summary</b>: WavGCRN is a novel method that combines wavelet transformation, graph convolutional recurrent networks, and road network information to improve spatiotemporal traffic forecasting by modeling multiscale structure in traffic data.</p><hr><h3>RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane  Networks</h3>
<p>Partha Ghosh,Soubhik Sanyal,Cordelia Schmid,Bernhard Schölkopf</p>
<p><a href='http://arxiv.org/abs/2401.06035v1'>http://arxiv.org/abs/2401.06035v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new generative model for videos that can handle long-term dependencies, reduce computational complexity, and synthesize high-quality video clips efficiently.</p><hr><h3>LinguAlchemy: Fusing Typological and Geographical Elements for Unseen  Language Generalization</h3>
<p>Muhammad Farid Adilazuarda,Samuel Cahyawijaya,Alham Fikri Aji,Genta Indra Winata,Ayu Purwarianti</p>
<p><a href='http://arxiv.org/abs/2401.06034v1'>http://arxiv.org/abs/2401.06034v1</a></p>
<p><b>Compressor summary</b>: LinguAlchemy improves PLMs' performance on unseen languages by regularizing them with linguistic constraints, making them more inclusive and accessible.</p><hr><h3>GE-AdvGAN: Improving the transferability of adversarial samples by  gradient editing-based adversarial generative model</h3>
<p>Zhiyu Zhu,Huaming Chen,Xinyi Wang,Jiayu Zhang,Zhibo Jin,Kim-Kwang Raymond Choo</p>
<p><a href='http://arxiv.org/abs/2401.06031v1'>http://arxiv.org/abs/2401.06031v1</a></p>
<p><b>Compressor summary</b>: GE-AdvGAN improves the efficiency and transferability of adversarial attacks by optimizing generator training with a novel gradient editing mechanism in GANs.</p><hr><h3>Automatic UAV-based Airport Pavement Inspection Using Mixed Real and  Virtual Scenarios</h3>
<p>Pablo Alonso,Jon Ander Iñiguez de Gordoa,Juan Diego Ortega,Sara García,Francisco Javier Iriarte,Marcos Nieto</p>
<p><a href='http://arxiv.org/abs/2401.06019v1'>http://arxiv.org/abs/2401.06019v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Runway and taxiway pavements deteriorate over time and need regular inspection
- UAV-based vision system using DL can identify pavement defects automatically
- Synthetic dataset generation helps overcome data scarcity for training

Summary:
The paper proposes a vision-based DL method using UAVs to detect pavement defects, and a synthetic dataset generation technique to train the model with limited real data.</p><hr><h3>Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation  in Endoscopic Surgery</h3>
<p>Cui Beilei,Islam Mobarakol,Bai Long,Ren Hongliang</p>
<p><a href='http://arxiv.org/abs/2401.06013v1'>http://arxiv.org/abs/2401.06013v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Surgical-DINO, a low-rank adaptation of DINOv2 for depth estimation in robotic surgery, which significantly outperforms existing models on the SCARED dataset.</p><hr><h3>Attention to detail: inter-resolution knowledge distillation</h3>
<p>Rocío del Amor,Julio Silva-Rodríguez,Adrián Colomer,Valery Naranjo</p>
<p><a href='http://arxiv.org/abs/2401.06010v1'>http://arxiv.org/abs/2401.06010v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Computer vision solutions for gigapixel images in digital pathology face computational limitations due to large image size.
- Knowledge distillation uses soft labels and features from high-resolution images to train a model that works with lower-resolution images.
- Incorporating attention maps, such as grad-CAMs, helps transfer discriminative information across resolutions and improve performance.

Summary:
The authors propose using attention maps, like grad-CAMs, to guide knowledge distillation for computer vision in digital pathology, improving model performance at different image resolutions.</p><hr><h3>Sea ice detection using concurrent multispectral and synthetic aperture  radar imagery</h3>
<p>Martin S J Rogers,Maria Fox,Andrew Fleming,Louisa van Zeeland,Jeremy Wilkinson,J. Scott Hosking</p>
<p><a href='http://arxiv.org/abs/2401.06009v1'>http://arxiv.org/abs/2401.06009v1</a></p>
<p><b>Compressor summary</b>: ViSual_IceD is a CNN that fuses multispectral and SAR imagery for accurate sea ice detection in polar regions, outperforming other models and complementing passive microwave data.</p><hr><h3>TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering</h3>
<p>Linus Franke,Darius Rückert,Laura Fink,Marc Stamminger</p>
<p><a href='http://arxiv.org/abs/2401.06003v1'>http://arxiv.org/abs/2401.06003v1</a></p>
<p><b>Compressor summary</b>: TRIPS is a novel technique that combines ideas from Gaussian Splatting and ADOP to render high-quality images of highly detailed scenes at real-time speeds with a differentiable pipeline.</p><hr><h3>MGARD: A multigrid framework for high-performance, error-controlled data  compression and refactoring</h3>
<p>Qian Gong,Jieyang Chen,Ben Whitney,Xin Liang,Viktor Reshniak,Tania Banerjee,Jaemoon Lee,Anand Rangarajan,Lipeng Wan,Nicolas Vidal,Qing Liu,Ana Gainaru,Norbert Podhorszki,Richard Archibald,Sanjay Ranka,Scott Klasky</p>
<p><a href='http://arxiv.org/abs/2401.05994v1'>http://arxiv.org/abs/2401.05994v1</a></p>
<p><b>Compressor summary</b>: MGARD is a software tool for compressing and managing large scientific data on grids across various computing architectures.</p><hr><h3>UAVD4L: A Large-Scale Dataset for UAV 6-DoF Localization</h3>
<p>Rouwan Wu,Xiaoya Cheng,Juelin Zhu,Xuxiang Liu,Maojun Zhang,Shen Yan</p>
<p><a href='http://arxiv.org/abs/2401.05971v1'>http://arxiv.org/abs/2401.05971v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a large-scale dataset for UAV localization and presents a two-stage pipeline that combines synthetic data generation and visual localization, as well as a hierarchical system for 3D ground target tracking.</p><hr><h3>Spatial-Aware Deep Reinforcement Learning for the Traveling Officer  Problem</h3>
<p>Niklas Strauß,Matthias Schubert</p>
<p><a href='http://arxiv.org/abs/2401.05969v1'>http://arxiv.org/abs/2401.05969v1</a></p>
<p><b>Compressor summary</b>: Spatial-aware deep reinforcement learning (SATOP) method improves the traveling officer problem by exploiting spatial relationships and learning future inter-action correlations, resulting in 22% more fines in Melbourne.</p><hr><h3>A Lightweight Feature Fusion Architecture For Resource-Constrained Crowd  Counting</h3>
<p>Yashwardhan Chaudhuri,Ankit Kumar,Orchid Chetia Phukan,Arun Balaji Buduru</p>
<p><a href='http://arxiv.org/abs/2401.05968v1'>http://arxiv.org/abs/2401.05968v1</a></p>
<p><b>Compressor summary</b>: The paper introduces two lightweight crowd-counting models that use MobileNet and MobileViT backbones, feature fusion, and are computationally efficient compared to previous methods.</p><hr><h3>Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph  Embedding</h3>
<p>Yihua Zhu,Hidetoshi Shimodaira</p>
<p><a href='http://arxiv.org/abs/2401.05967v1'>http://arxiv.org/abs/2401.05967v1</a></p>
<p><b>Compressor summary</b>: OrthogonalE is a novel Knowledge Graph embedding model that uses matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations, improving generality and flexibility over existing methods.</p><hr><h3>An attempt to generate new bridge types from latent space of PixelCNN</h3>
<p>Hongjun Zhang</p>
<p><a href='http://arxiv.org/abs/2401.05964v1'>http://arxiv.org/abs/2401.05964v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to generate new bridge types using generative artificial intelligence, which combines different structural components and can potentially lead to artificial general intelligence.</p><hr><h3>Machine Learning Insides OptVerse AI Solver: Design Principles and  Applications</h3>
<p>Xijun Li,Fangzhou Zhu,Hui-Ling Zhen,Weilin Luo,Meng Lu,Yimin Huang,Zhenan Fan,Zirui Zhou,Yufei Kuang,Zhihai Wang,Zijie Geng,Yang Li,Haoyang Liu,Zhiwu An,Muming Yang,Jianshu Li,Jie Wang,Junchi Yan,Defeng Sun,Tao Zhong,Yong Zhang,Jia Zeng,Mingxuan Yuan,Jianye Hao,Jun Yao,Kun Mao</p>
<p><a href='http://arxiv.org/abs/2401.05960v1'>http://arxiv.org/abs/2401.05960v1</a></p>
<p><b>Compressor summary</b>: The paper presents a study on enhancing Huawei Cloud's OptVerse AI Solver with machine learning techniques to improve efficiency and performance in mathematical programming tasks.</p><hr><h3>LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase</h3>
<p>Chujie Gao,Dongping Chen,Qihui Zhang,Yue Huang,Yao Wan,Lichao Sun</p>
<p><a href='http://arxiv.org/abs/2401.05952v1'>http://arxiv.org/abs/2401.05952v1</a></p>
<p><b>Compressor summary</b>: The text introduces mixcase, a hybrid text form of machine-generated and human-generated content, and MixSet, the first dataset to study mixed modification scenarios in large language models.</p><hr><h3>Universal Vulnerabilities in Large Language Models: In-context Learning  Backdoor Attacks</h3>
<p>Shuai Zhao,Meihuizi Jia,Luu Anh Tuan,Jinming Wen</p>
<p><a href='http://arxiv.org/abs/2401.05949v1'>http://arxiv.org/abs/2401.05949v1</a></p>
<p><b>Compressor summary</b>: In-context learning is an effective NLP paradigm but has security risks as it can be exploited by ICLAttack, a new backdoor attack method that manipulates model behavior without fine-tuning.</p><hr><h3>Learning Cognitive Maps from Transformer Representations for Efficient  Planning in Partially Observed Environments</h3>
<p>Antoine Dedieu,Wolfgang Lehrach,Guangyao Zhou,Dileep George,Miguel Lázaro-Gredilla</p>
<p><a href='http://arxiv.org/abs/2401.05946v1'>http://arxiv.org/abs/2401.05946v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a transformer model with discrete bottlenecks that can learn compressed representations of observations and actions, enabling it to extract interpretable cognitive maps for path planning in partially observed environments.</p><hr><h3>SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully</h3>
<p>Jushi Kai,Tianhang Zhang,Hai Hu,Zhouhan Lin</p>
<p><a href='http://arxiv.org/abs/2401.05930v1'>http://arxiv.org/abs/2401.05930v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to help language models generate text more truthfully by highlighting and hesitating on less probable but factual tokens.</p><hr><h3>Mitigating Unhelpfulness in Emotional Support Conversations with  Multifaceted AI Feedback</h3>
<p>Jiashuo Wang,Chunpu Xu,Chak Tou Leong,Wenjie Li,Jing Li</p>
<p><a href='http://arxiv.org/abs/2401.05928v1'>http://arxiv.org/abs/2401.05928v1</a></p>
<p><b>Compressor summary</b>: Muffin is a framework that uses contrastive learning to reduce unhelpful responses in emotional support systems by considering multiple factors such as empathy, support strategies, and coherence.</p><hr><h3>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians</h3>
<p>Bin Dou,Tianyu Zhang,Yongjia Ma,Zhaohui Wang,Zejian Yuan</p>
<p><a href='http://arxiv.org/abs/2401.05925v1'>http://arxiv.org/abs/2401.05925v1</a></p>
<p><b>Compressor summary</b>: Our method improves 3D scene segmentation speed and quality by optimizing Gaussian points, fusing spatial and semantic features, and using a shallow decoding network.</p><hr><h3>How Teachers Can Use Large Language Models and Bloom's Taxonomy to  Create Educational Quizzes</h3>
<p>Sabina Elkins,Ekaterina Kochmar,Jackie C. K. Cheung,Iulian Serban</p>
<p><a href='http://arxiv.org/abs/2401.05914v1'>http://arxiv.org/abs/2401.05914v1</a></p>
<p><b>Compressor summary</b>: The paper shows how a language model-based question generation system can be used by teachers to create quizzes with learning goals from Bloom's taxonomy, and demonstrates its advantages over handwritten quizzes in terms of quality and metrics.</p><hr><h3>Prompt-based mental health screening from social media text</h3>
<p>Wesley Ramos dos Santos,Ivandre Paraboni</p>
<p><a href='http://arxiv.org/abs/2401.05912v1'>http://arxiv.org/abs/2401.05912v1</a></p>
<p><b>Compressor summary</b>: The article describes how to use GPT 3.5 prompts and a simple text classifier to screen for mental health issues in social media posts, achieving similar results to a more complex BERT method but with less computation.</p><hr><h3>EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with  Epilepsy Medical Knowledge</h3>
<p>Xuyang Zhao,Qibin Zhao,Toshihisa Tanaka</p>
<p><a href='http://arxiv.org/abs/2401.05908v1'>http://arxiv.org/abs/2401.05908v1</a></p>
<p><b>Compressor summary</b>: EpilepsyLLM is a customized language model that provides more accurate and relevant medical information about epilepsy in Japanese by fine-tuning a pre-trained LLM with domain-specific datasets.</p><hr><h3>Efficient Image Deblurring Networks based on Diffusion Models</h3>
<p>Kang Chen,Yuanjie Liu</p>
<p><a href='http://arxiv.org/abs/2401.05907v1'>http://arxiv.org/abs/2401.05907v1</a></p>
<p><b>Compressor summary</b>: Swintormer is a sliding window model for defocus deblurring that uses diffusion, Transformer blocks, and optimized Macs to achieve high performance with low memory usage and improved SNR.</p><hr><h3>PartSTAD: 2D-to-3D Part Segmentation Task Adaptation</h3>
<p>Hyunjin Kim,Minhyuk Sung</p>
<p><a href='http://arxiv.org/abs/2401.05906v1'>http://arxiv.org/abs/2401.05906v1</a></p>
<p><b>Compressor summary</b>: PartSTAD adapts 2D segmentation models for 3D segmentation tasks using finetuning, merging weights, and a foreground segmentation model, achieving significant improvements on PartNet-Mobility dataset.</p><hr><h3>ConKeD: Multiview contrastive descriptor learning for keypoint-based  retinal image registration</h3>
<p>David Rivas-Villar,Álvaro S. Hervella,José Rouco,Jorge Novo</p>
<p><a href='http://arxiv.org/abs/2401.05901v1'>http://arxiv.org/abs/2401.05901v1</a></p>
<p><b>Compressor summary</b>: ConKeD is a novel deep learning approach that learns descriptors for retinal image registration using a multi-positive multi-negative contrastive learning strategy, achieving comparable results to state-of-the-art methods with fewer training samples and detected keypoints.</p><hr><h3>Optimistic Model Rollouts for Pessimistic Offline Policy Optimization</h3>
<p>Yuanzhao Zhai,Yiying Li,Zijian Gao,Xudong Gong,Kele Xu,Dawei Feng,Ding Bo,Huaimin Wang</p>
<p><a href='http://arxiv.org/abs/2401.05899v1'>http://arxiv.org/abs/2401.05899v1</a></p>
<p><b>Compressor summary</b>: ORPO is an offline RL framework that uses optimistic rollouts to improve policy optimization and generalization with synthetic model rollouts.</p><hr><h3>Binary Linear Tree Commitment-based Ownership Protection for Distributed  Machine Learning</h3>
<p>Tianxiu Xie,Keke Gai,Jing Yu,Liehuang Zhu</p>
<p><a href='http://arxiv.org/abs/2401.05895v1'>http://arxiv.org/abs/2401.05895v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel ownership protection model for distributed machine learning using binary linear tree commitment, which ensures computational integrity with efficient proof aggregation and watermarking.</p><hr><h3>LiDAR data acquisition and processing for ecology applications</h3>
<p>Ion Ciobotari,Adriana Príncipe,Maria Alexandra Oliveira,João Nuno Silva</p>
<p><a href='http://arxiv.org/abs/2401.05891v1'>http://arxiv.org/abs/2401.05891v1</a></p>
<p><b>Compressor summary</b>: The text describes a low-cost terrestrial laser scanner (TLS) for ecological data acquisition and its application in two case studies, showing its effectiveness in measuring vegetation structure.</p><hr><h3>Generative Deduplication For Socia Media Data Selection</h3>
<p>Xianming Li,Jing Li</p>
<p><a href='http://arxiv.org/abs/2401.05883v1'>http://arxiv.org/abs/2401.05883v1</a></p>
<p><b>Compressor summary</b>: Generative duplication is a novel approach that removes duplicate text from noisy social media data, mitigating model bias, improving performance, and saving training time.</p><hr><h3>YOIO: You Only Iterate Once by mining and fusing multiple necessary  global information in the optical flow estimation</h3>
<p>Yu Jing,Tan Yujuan,Ren Ao,Liu Duo</p>
<p><a href='http://arxiv.org/abs/2401.05879v1'>http://arxiv.org/abs/2401.05879v1</a></p>
<p><b>Compressor summary</b>: The YOIO framework improves optical flow prediction accuracy in occluded regions using spatiotemporal information from two frames and achieves state-of-the-art results with high efficiency.</p><hr><h3>Safe reinforcement learning in uncertain contexts</h3>
<p>Dominik Baumann,Thomas B. Schön</p>
<p><a href='http://arxiv.org/abs/2401.05876v1'>http://arxiv.org/abs/2401.05876v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a safe learning method for robotic systems that can handle discrete environmental changes without directly measuring them, using multi-class classification and experiments to estimate the context.</p><hr><h3>Enhancing Personality Recognition in Dialogue by Data Augmentation and  Heterogeneous Conversational Graph Networks</h3>
<p>Yahui Fu,Haiyue Song,Tianyu Zhao,Tatsuya Kawahara</p>
<p><a href='http://arxiv.org/abs/2401.05871v1'>http://arxiv.org/abs/2401.05871v1</a></p>
<p><b>Compressor summary</b>: The text describes a new method for improving personality recognition in robots using data augmentation and a specialized network architecture, leading to better human-robot interactions.</p><hr><h3>HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced  Diffusion Models</h3>
<p>Hanzhang Wang,Haoran Wang,Jinze Yang,Zhongrui Yu,Zeke Xie,Lei Tian,Xinyan Xiao,Junjun Jiang,Xianming Liu,Mingming Sun</p>
<p><a href='http://arxiv.org/abs/2401.05870v1'>http://arxiv.org/abs/2401.05870v1</a></p>
<p><b>Compressor summary</b>: HiCAST is a novel approach for arbitrary style transfer that allows flexible and customized stylization by using a Latent Diffusion Model and a Style Adapter, and can also apply to video AST with improved temporal consistency.</p><hr><h3>Towards Boosting Many-to-Many Multilingual Machine Translation with  Large Language Models</h3>
<p>Pengzhi Gao,Zhongjun He,Hua Wu,Haifeng Wang</p>
<p><a href='http://arxiv.org/abs/2401.05861v1'>http://arxiv.org/abs/2401.05861v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method, XConST, to improve multilingual zero-shot translation by using prompt strategies and cross-lingual consistency regularization during instruction finetuning on pretrained LLMs.</p><hr><h3>Inferring Intentions to Speak Using Accelerometer Data In-the-Wild</h3>
<p>Litian Li,Jord Molhoek,Jing Zhou</p>
<p><a href='http://arxiv.org/abs/2401.05849v1'>http://arxiv.org/abs/2401.05849v1</a></p>
<p><b>Compressor summary</b>: The text studies whether AI can recognize intentions to speak from accelerometer data in real-life settings, but finds that it is not reliable enough and more data sources are needed.</p><hr><h3>Revisiting Silhouette: From Micro to Macro Aggregation</h3>
<p>Georgios Vardakas,John Pavlopoulos,Aristidis Likas</p>
<p><a href='http://arxiv.org/abs/2401.05831v1'>http://arxiv.org/abs/2401.05831v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new way to evaluate data clustering quality, called macro-averaging, which is more robust to cluster imbalance and background noise than the standard micro-averaging method.</p><hr><h3>Hallucination Benchmark in Medical Visual Question Answering</h3>
<p>Jinge Wu,Yunsoo Kim,Honghan Wu</p>
<p><a href='http://arxiv.org/abs/2401.05827v1'>http://arxiv.org/abs/2401.05827v1</a></p>
<p><b>Compressor summary</b>: The text discusses the potential of visual assistants for healthcare using large language and vision models, but warns that these models may hallucinate when faced with unfamiliar medical images.</p><hr><h3>Towards Goal-Oriented Agents for Evolving Problems Observed via  Conversation</h3>
<p>Michael Free,Andrew Langworthy,Mary Dimitropoulaki,Simon Thompson</p>
<p><a href='http://arxiv.org/abs/2401.05822v1'>http://arxiv.org/abs/2401.05822v1</a></p>
<p><b>Compressor summary</b>: The text describes a system that trains a chatbot with reinforcement learning to solve evolving problems by conversing with a simulated user about a virtual game.</p><hr><h3>Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents</h3>
<p>Quentin Delfosse,Sebastian Sztwiertnia,Wolfgang Stammer,Mark Rothermel,Kristian Kersting</p>
<p><a href='http://arxiv.org/abs/2401.05821v1'>http://arxiv.org/abs/2401.05821v1</a></p>
<p><b>Compressor summary</b>: SCoBots are transparent agents that use concept bottleneck layers to enable domain experts to understand and regularize their behavior, leading to better human-aligned RL.</p><hr><h3>Implications of Noise in Resistive Memory on Deep Neural Networks for  Image Classification</h3>
<p>Yannick Emonds,Kai Xi,Holger Fröning</p>
<p><a href='http://arxiv.org/abs/2401.05820v1'>http://arxiv.org/abs/2401.05820v1</a></p>
<p><b>Compressor summary</b>: The text explores how image classification with neural networks can tolerate noise in resistive memory operations and proposes methods to improve resilience.</p><hr><h3>Tuning LLMs with Contrastive Alignment Instructions for Machine  Translation in Unseen, Low-resource Languages</h3>
<p>Zhuoyuan Mao,Yen Yu</p>
<p><a href='http://arxiv.org/abs/2401.05811v1'>http://arxiv.org/abs/2401.05811v1</a></p>
<p><b>Compressor summary</b>: The article proposes AlignInstruct, a method that improves machine translation on large language models for unseen and low-resource languages by using cross-lingual supervision.</p><hr><h3>On the representation and methodology for wide and short range head pose  estimation</h3>
<p>Alejandro Cobo,Roberto Valle,José M. Buenaposada,Luis Baumela</p>
<p><a href='http://arxiv.org/abs/2401.05807v1'>http://arxiv.org/abs/2401.05807v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper analyzes different methods and metrics for head pose estimation (HPE) in face processing tasks
- It shows that Euler angles are good for short-range HPE but not for extreme rotations
- It proposes a new cross-data set evaluation methodology and a generalization of the geodesic angular distance metric
- It introduces a wide range HPE benchmark based on the CMU Panoptic data set

Summary:
The paper discusses HPE methods and metrics for different rotation ranges, improves cross-data set evaluation, and introduces a new benchmark.</p><hr><h3>CLIP-Driven Semantic Discovery Network for Visible-Infrared Person  Re-Identification</h3>
<p>Xiaoyan Yu,Neng Dong,Liehuang Zhu,Hao Peng,Dapeng Tao</p>
<p><a href='http://arxiv.org/abs/2401.05806v1'>http://arxiv.org/abs/2401.05806v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method, CLIP-Driven Semantic Discovery Network (CSDN), that uses high-level semantics to bridge the modality gap between visible and infrared images for person re-identification.</p><hr><h3>Graph Spatiotemporal Process for Multivariate Time Series Anomaly  Detection with Missing Values</h3>
<p>Yu Zheng,Huan Yee Koh,Ming Jin,Lianhua Chi,Haishuai Wang,Khoa T. Phan,Yi-Ping Phoebe Chen,Shirui Pan,Wei Xiang</p>
<p><a href='http://arxiv.org/abs/2401.05800v1'>http://arxiv.org/abs/2401.05800v1</a></p>
<p><b>Compressor summary</b>: The text introduces GST-Pro, a framework for detecting anomalies in irregularly-sampled multivariate time series using neural controlled differential equations and a distribution-based scoring mechanism.</p><hr><h3>Designing Heterogeneous LLM Agents for Financial Sentiment Analysis</h3>
<p>Frank Xing</p>
<p><a href='http://arxiv.org/abs/2401.05799v1'>http://arxiv.org/abs/2401.05799v1</a></p>
<p><b>Compressor summary</b>: This paper explores using large language models without fine-tuning for financial sentiment analysis, proposing a framework that leverages their generative power and domain knowledge to improve accuracy and discusses its implications on business and management.</p><hr><h3>Bounds on the price of feedback for mistake-bounded online learning</h3>
<p>Jesse Geneson,Linus Tang</p>
<p><a href='http://arxiv.org/abs/2401.05794v1'>http://arxiv.org/abs/2401.05794v1</a></p>
<p><b>Compressor summary</b>: Key improvements in online learning bounds for various scenarios are presented, including sharpened upper and lower bounds and solved problems.</p><hr><h3>Discovering Low-rank Subspaces for Language-agnostic Multilingual  Representations</h3>
<p>Zhihui Xie,Handong Zhao,Tong Yu,Shuai Li</p>
<p><a href='http://arxiv.org/abs/2401.05792v1'>http://arxiv.org/abs/2401.05792v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to remove language-specific factors from multilingual embeddings using singular value decomposition, which improves semantic tasks like cross-lingual sentence retrieval without finetuning.</p><hr><h3>Evidence to Generate (E2G): A Single-agent Two-step Prompting for  Context Grounded and Retrieval Augmented Reasoning</h3>
<p>Md Rizwan Parvez</p>
<p><a href='http://arxiv.org/abs/2401.05787v1'>http://arxiv.org/abs/2401.05787v1</a></p>
<p><b>Compressor summary</b>: E2G is a novel framework that leverages evidence from context to improve LLMs' reasoning and generation performance across various tasks.</p><hr><h3>EraseDiff: Erasing Data Influence in Diffusion Models</h3>
<p>Jing Wu,Trung Le,Munawar Hayat,Mehrtash Harandi</p>
<p><a href='http://arxiv.org/abs/2401.05779v1'>http://arxiv.org/abs/2401.05779v1</a></p>
<p><b>Compressor summary</b>: The paper introduces an algorithm for diffusion models to remove data while preserving their utility and effectiveness on other data.</p><hr><h3>Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language  Model Systems</h3>
<p>Tianyu Cui,Yanling Wang,Chuanpu Fu,Yong Xiao,Sijia Li,Xinhao Deng,Yunpeng Liu,Qinglin Zhang,Ziyi Qiu,Peiyang Li,Zhixing Tan,Junwu Xiong,Xinyu Kong,Zujie Wen,Ke Xu,Qi Li</p>
<p><a href='http://arxiv.org/abs/2401.05778v1'>http://arxiv.org/abs/2401.05778v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a taxonomy for large language models (LLMs) to analyze and mitigate risks in their four essential modules, and reviews benchmarks for risk assessment.</p><hr><h3>Probing Structured Semantics Understanding and Generation of Language  Models via Question Answering</h3>
<p>Jinxin Liu,Shulin Cao,Jiaxin Shi,Tingjian Zhang,Lei Hou,Juanzi Li</p>
<p><a href='http://arxiv.org/abs/2401.05777v1'>http://arxiv.org/abs/2401.05777v1</a></p>
<p><b>Compressor summary</b>: This paper evaluates how well large language models understand and generate structured logical forms for question answering using different formal languages and suggests generating more training data rather than directly relying on LLMs for answering questions.</p><hr><h3>Knowledge Translation: A New Pathway for Model Compression</h3>
<p>Wujie Sun,Defang Chen,Jiawei Chen,Yan Feng,Chun Chen,Can Wang</p>
<p><a href='http://arxiv.org/abs/2401.05772v1'>http://arxiv.org/abs/2401.05772v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework called Knowledge Translation that uses neural networks to compress deep learning models without re-training or architectural constraints, inspired by language translation.</p><hr><h3>Learn From Zoom: Decoupled Supervised Contrastive Learning For WCE Image  Classification</h3>
<p>Kunpeng Qiu,Zhiying Zhou,Yongxin Guo</p>
<p><a href='http://arxiv.org/abs/2401.05771v1'>http://arxiv.org/abs/2401.05771v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for classifying lesions in Wireless Capsule Endoscopy images using Decoupled Supervised Contrastive Learning and Saliency Augmentor, achieving state-of-the-art results.</p><hr><h3>Evaluating Data Augmentation Techniques for Coffee Leaf Disease  Classification</h3>
<p>Adrian Gheorghiu,Iulian-Marius Tăiatu,Dumitru-Clementin Cercel,Iuliana Marin,Florin Pop</p>
<p><a href='http://arxiv.org/abs/2401.05768v1'>http://arxiv.org/abs/2401.05768v1</a></p>
<p><b>Compressor summary</b>: The paper presents a deep learning-based method for classifying Robusta coffee leaf diseases using the RoCoLe dataset, augmented with synthetic data generated by CycleGAN, improving the performance of the model.</p><hr><h3>Learning Generalizable Models via Disentangling Spurious and Enhancing  Potential Correlations</h3>
<p>Na Wang,Lei Qi,Jintao Guo,Yinghuan Shi,Yang Gao</p>
<p><a href='http://arxiv.org/abs/2401.05752v1'>http://arxiv.org/abs/2401.05752v1</a></p>
<p><b>Compressor summary</b>: This paper proposes two modules to improve domain generalization by disentangling spurious correlations and enhancing potential correlations using sample and feature perspectives, achieving better results for CNNs or MLPs.</p><hr><h3>GO-NeRF: Generating Virtual Objects in Neural Radiance Fields</h3>
<p>Peng Dai,Feitong Tan,Xin Yu,Yinda Zhang,Xiaojuan Qi</p>
<p><a href='http://arxiv.org/abs/2401.05750v1'>http://arxiv.org/abs/2401.05750v1</a></p>
<p><b>Compressor summary</b>: The paper introduces GO-NeRF, a method for creating 3D objects within an existing Neural Radiance Field (NeRF) scene using scene context and producing high-quality, harmonious results with minimal artifacts.</p><hr><h3>A Shocking Amount of the Web is Machine Translated: Insights from  Multi-Way Parallelism</h3>
<p>Brian Thompson,Mehak Preet Dhaliwal,Peter Frisch,Tobias Domhan,Marcello Federico</p>
<p><a href='http://arxiv.org/abs/2401.05749v1'>http://arxiv.org/abs/2401.05749v1</a></p>
<p><b>Compressor summary</b>: The text suggests that machine translation is widely used for creating low-quality translations of web content in multiple languages, especially in lower resource languages.</p><hr><h3>Surface Normal Estimation with Transformers</h3>
<p>Barry Shichen Hu,Siyun Liang,Johannes Paetzold,Huy H. Nguyen,Isao Echizen,Jiapeng Tang</p>
<p><a href='http://arxiv.org/abs/2401.05745v1'>http://arxiv.org/abs/2401.05745v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Use Transformer to predict normals from noisy point clouds
- Previous methods use PointNet variants and surface fitting methods with limitations
- Proposed method is simpler, faster, more robust, and achieves state-of-the-art performance on two datasets

Summary:
The paper proposes a simple and effective Transformer-based model for predicting normals from noisy point clouds, outperforming previous methods that use PointNet variants and surface fitting methods with limitations.</p><hr><h3>Consistent Query Answering for Existential Rules under Tuple-Deletion  Semantics</h3>
<p>Lorenzo Marconi,Riccardo Rosati</p>
<p><a href='http://arxiv.org/abs/2401.05743v1'>http://arxiv.org/abs/2401.05743v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how to answer queries over knowledge bases with existential rules, finding some cases where it is easy or possible to fix inconsistencies using new techniques.</p><hr><h3>LKCA: Large Kernel Convolutional Attention</h3>
<p>Chenghao Li,Boheng Zeng,Yi Lu,Pengbo Shi,Qingzi Chen,Jirui Liu,Lingyun Zhu</p>
<p><a href='http://arxiv.org/abs/2401.05738v1'>http://arxiv.org/abs/2401.05738v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Large Kernel Convolutional Attention (LKCA), a new spatial attention method for visual transformers that simplifies the attention operation and combines the advantages of convolutional neural networks and visual transformers, achieving competitive performance in various visual tasks.</p><hr><h3>An experimental evaluation of Deep Reinforcement Learning algorithms for  HVAC control</h3>
<p>Antonio Manjavacas,Alejandro Campoy-Nieves,Javier Jiménez-Raboso,Miguel Molina-Solana,Juan Gómez-Romero</p>
<p><a href='http://arxiv.org/abs/2401.05737v1'>http://arxiv.org/abs/2401.05737v1</a></p>
<p><b>Compressor summary</b>: This paper evaluates state-of-the-art Deep Reinforcement Learning algorithms for HVAC control, showing their potential in comfort and energy efficiency while highlighting challenges in generalization and incremental learning.</p><hr><h3>Cross-modal Retrieval for Knowledge-based Visual Question Answering</h3>
<p>Paul Lerner,Olivier Ferret,Camille Guinaudeau</p>
<p><a href='http://arxiv.org/abs/2401.05736v1'>http://arxiv.org/abs/2401.05736v1</a></p>
<p><b>Compressor summary</b>: The text discusses how cross-modal retrieval can help recognize named entities in visual question answering tasks by bridging the semantic gap between entities and their depictions.</p><hr><h3>Object-Centric Diffusion for Efficient Video Editing</h3>
<p>Kumara Kahatapitiya,Adil Karjauv,Davide Abati,Fatih Porikli,Yuki M. Asano,Amirhossein Habibian</p>
<p><a href='http://arxiv.org/abs/2401.05735v1'>http://arxiv.org/abs/2401.05735v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes the inefficiencies of diffusion-based video editing, introduces Object-Centric Diffusion (OCD) to reduce computation costs by focusing on foreground regions, and proposes two novel techniques that can significantly speed up video editing without retraining.</p><hr><h3>Enhancing Contrastive Learning with Efficient Combinatorial Positive  Pairing</h3>
<p>Jaeill Kim,Duhun Hwang,Eunjung Lee,Jangwon Suh,Jimyeong Kim,Wonjong Rhee</p>
<p><a href='http://arxiv.org/abs/2401.05730v1'>http://arxiv.org/abs/2401.05730v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a multi-view strategy called ECPP that improves the speed and performance of contrastive and non-contrastive visual representation learning methods.</p><hr><h3>Zero Resource Cross-Lingual Part Of Speech Tagging</h3>
<p>Sahil Chopra</p>
<p><a href='http://arxiv.org/abs/2401.05727v1'>http://arxiv.org/abs/2401.05727v1</a></p>
<p><b>Compressor summary</b>: The text discusses using a hidden Markov model to predict part-of-speech tags in low-resource languages by transferring data from source languages, and finds that this method is effective for zero-resource languages.</p><hr><h3>Kernelized Normalizing Constant Estimation: Bridging Bayesian Quadrature  and Bayesian Optimization</h3>
<p>Xu Cai,Jonathan Scarlett</p>
<p><a href='http://arxiv.org/abs/2401.05716v1'>http://arxiv.org/abs/2401.05716v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how the difficulty of estimating a normalizing constant using black-box function queries depends on the problem parameter $\lambda$, ranging from Bayesian quadrature to Bayesian optimization and considering noisy function evaluations.</p><hr><h3>Dynamic Indoor Fingerprinting Localization based on Few-Shot  Meta-Learning with CSI Images</h3>
<p>Jiyu Jiao,Xiaojun Wang,Chenpei Han,Yuhua Huang,Yizhuo Zhang</p>
<p><a href='http://arxiv.org/abs/2401.05711v1'>http://arxiv.org/abs/2401.05711v1</a></p>
<p><b>Compressor summary</b>: A meta-learning algorithm that uses historical localization tasks to improve adaptability and efficiency in indoor environments reduces data acquisition costs and increases accuracy.</p><hr><h3>The Distributional Reward Critic Architecture for Perturbed-Reward  Reinforcement Learning</h3>
<p>Xi Chen,Zhihui Zhu,Andrew Perrault</p>
<p><a href='http://arxiv.org/abs/2401.05710v1'>http://arxiv.org/abs/2401.05710v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for reinforcement learning with unknown reward perturbations that can recover the true rewards in various settings and outperforms existing methods.</p><hr><h3>CAT-LLM: Prompting Large Language Models with Text Style Definition for  Chinese Article-style Transfer</h3>
<p>Zhen Tao,Dinghao Xi,Zhiyu Li,Liumin Tang,Wei Xu</p>
<p><a href='http://arxiv.org/abs/2401.05707v1'>http://arxiv.org/abs/2401.05707v1</a></p>
<p><b>Compressor summary</b>: The proposed Chinese Article-style Transfer framework (CAT-LLM) leverages Large Language Models to analyze and transfer text features from Chinese articles while preserving the original content's integrity.</p><hr><h3>R-BI: Regularized Batched Inputs enhance Incremental Decoding Framework  for Low-Latency Simultaneous Speech Translation</h3>
<p>Jiaxin Guo,Zhanglin Wu,Zongyao Li,Hengchao Shang,Daimeng Wei,Xiaoyu Chen,Zhiqiang Rao,Shaojun Li,Hao Yang</p>
<p><a href='http://arxiv.org/abs/2401.05700v1'>http://arxiv.org/abs/2401.05700v1</a></p>
<p><b>Compressor summary</b>: Regularized Batched Inputs is a novel method for low-latency simultaneous speech translation that improves input diversity and reduces output errors with suitable regularization techniques.</p><hr><h3>Integrating Physician Diagnostic Logic into Large Language Models:  Preference Learning from Process Feedback</h3>
<p>Chengfeng Dou,Zhi Jin,Wenpin Jiao,Haiyan Zhao,Yongqiang Zhao,Zhenwei Tao</p>
<p><a href='http://arxiv.org/abs/2401.05695v1'>http://arxiv.org/abs/2401.05695v1</a></p>
<p><b>Compressor summary</b>: PLPF is a method to improve medical dialogue generation by integrating diagnostic logic into large language models using rule modeling, preference data generation, and preference alignment.</p><hr><h3>UCorrect: An Unsupervised Framework for Automatic Speech Recognition  Error Correction</h3>
<p>Jiaxin Guo,Minghan Wang,Xiaosong Qiao,Daimeng Wei,Hengchao Shang,Zongyao Li,Zhengzhe Yu,Yinglu Li,Chang Su,Min Zhang,Shimin Tao,Hao Yang</p>
<p><a href='http://arxiv.org/abs/2401.05689v1'>http://arxiv.org/abs/2401.05689v1</a></p>
<p><b>Compressor summary</b>: UCorrect is an unsupervised method for correcting errors in automatic speech recognition (ASR) output without relying on specific training data or fine-tuning, achieving significant word error rate reduction and outperforming popular correction models.</p><hr><h3>Self Expanding Convolutional Neural Networks</h3>
<p>Blaise Appolinary,Alex Deaconu,Sophia Yang</p>
<p><a href='http://arxiv.org/abs/2401.05686v1'>http://arxiv.org/abs/2401.05686v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to dynamically expand CNNs during training using an expansion score, which improves performance, reduces resource use, and is eco-friendly.</p><hr><h3>Exploring Self- and Cross-Triplet Correlations for Human-Object  Interaction Detection</h3>
<p>Weibo Jiang,Weihong Ren,Jiandong Tian,Liangqiong Qu,Zhiyong Wang,Honghai Liu</p>
<p><a href='http://arxiv.org/abs/2401.05676v1'>http://arxiv.org/abs/2401.05676v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for human-object interaction detection that considers both self-triplet and cross-triplet dependencies, as well as leveraging the CLIP model to obtain interaction-aware features.</p><hr><h3>Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for  Text-to-Image Generation</h3>
<p>Seung Hyun Lee,Yinxiao Li,Junjie Ke,Innfarn Yoo,Han Zhang,Jiahui Yu,Qifei Wang,Fei Deng,Glenn Entis,Junfeng He,Gang Li,Sangpil Kim,Irfan Essa,Feng Yang</p>
<p><a href='http://arxiv.org/abs/2401.05675v1'>http://arxiv.org/abs/2401.05675v1</a></p>
<p><b>Compressor summary</b>: Parrot is a new framework for text-to-image generation that uses reinforcement learning to automatically balance multiple quality rewards and improve the generated images.</p><hr><h3>ConcEPT: Concept-Enhanced Pre-Training for Language Models</h3>
<p>Xintao Wang,Zhouhong Gu,Jiaqing Liang,Dakuan Lu,Yanghua Xiao,Wei Wang</p>
<p><a href='http://arxiv.org/abs/2401.05669v1'>http://arxiv.org/abs/2401.05669v1</a></p>
<p><b>Compressor summary</b>: ConcEPT is a novel pre-training method for language models that infuses conceptual knowledge by predicting the concepts of entities in the context, improving performance on tasks like entity typing.</p><hr><h3>EsaCL: Efficient Continual Learning of Sparse Models</h3>
<p>Weijieying Ren,Vasant G Honavar</p>
<p><a href='http://arxiv.org/abs/2401.05667v1'>http://arxiv.org/abs/2401.05667v1</a></p>
<p><b>Compressor summary</b>: EsaCL is a method for efficient continual learning of sparse models that prunes redundant parameters without retraining and uses intelligent data selection to improve data efficiency.</p><hr><h3>Root Cause Analysis on Energy Efficiency with Transfer Entropy Flow</h3>
<p>Jian Ma</p>
<p><a href='http://arxiv.org/abs/2401.05664v1'>http://arxiv.org/abs/2401.05664v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using transfer entropy to diagnose the root causes of low energy efficiency in industrial systems, and tests it on a real compressing air system data set.</p><hr><h3>Unveiling the Tapestry of Automated Essay Scoring: A Comprehensive  Investigation of Accuracy, Fairness, and Generalizability</h3>
<p>Kaixun Yang,Mladen Raković,Yuyang Li,Quanlong Guan,Dragan Gašević,Guanliang Chen</p>
<p><a href='http://arxiv.org/abs/2401.05655v1'>http://arxiv.org/abs/2401.05655v1</a></p>
<p><b>Compressor summary</b>: The study investigates the relationship between AES model accuracy, fairness, and generalizability, finding that prompt-specific models perform better in accuracy but may have more bias towards students of different economic statuses compared to cross-prompt models, while traditional machine learning models with engineered features achieve higher accuracy and fairness than complex neural networks.</p><hr><h3>Towards Conversational Diagnostic AI</h3>
<p>Tao Tu,Anil Palepu,Mike Schaekermann,Khaled Saab,Jan Freyberg,Ryutaro Tanno,Amy Wang,Brenna Li,Mohamed Amin,Nenad Tomasev,Shekoofeh Azizi,Karan Singhal,Yong Cheng,Le Hou,Albert Webson,Kavita Kulkarni,S Sara Mahdavi,Christopher Semturs,Juraj Gottweis,Joelle Barral,Katherine Chou,Greg S Corrado,Yossi Matias,Alan Karthikesalingam,Vivek Natarajan</p>
<p><a href='http://arxiv.org/abs/2401.05654v1'>http://arxiv.org/abs/2401.05654v1</a></p>
<p><b>Compressor summary</b>: AMIE is an AI system that can have diagnostic dialogues with patients and performs better than primary care physicians in some aspects, but it is not yet ready for real-world use.</p><hr><h3>Quantifying Marketing Performance at Channel-Partner Level by Using  Marketing Mix Modeling (MMM) and Shapley Value Regression</h3>
<p>Sean Tang,Sriya Musunuru,Baoshi Zong,Brooks Thornton</p>
<p><a href='http://arxiv.org/abs/2401.05653v1'>http://arxiv.org/abs/2401.05653v1</a></p>
<p><b>Compressor summary</b>: The paper shows how Shapley Value Regression can help measure individual partner's impact on marketing performance in financial services, without the need for complex and costly cooperative game theory testing.</p><hr><h3>On Detecting Cherry-picking in News Coverage Using Large Language Models</h3>
<p>Israa Jaradat,Haiqi Zhang,Chengkai Li</p>
<p><a href='http://arxiv.org/abs/2401.05650v1'>http://arxiv.org/abs/2401.05650v1</a></p>
<p><b>Compressor summary</b>: Cherry is a novel approach that uses language models and multiple news sources to automatically detect cherry-picked statements in news articles by identifying missing important statements.</p><hr><h3>Masked Attribute Description Embedding for Cloth-Changing Person  Re-identification</h3>
<p>Chunlei Peng,Boyu Wang,Decheng Liu,Nannan Wang,Ruimin Hu,Xinbo Gao</p>
<p><a href='http://arxiv.org/abs/2401.05646v1'>http://arxiv.org/abs/2401.05646v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method called MADE that uses attribute descriptions to enhance person re-identification across clothing changes, by masking and embedding them into the Transformer blocks of an image model.</p><hr><h3>MatSAM: Efficient Materials Microstructure Extraction via Visual Large  Model</h3>
<p>Changtai Li,Xu Han,Chao Yao,Xiaojuan Ban</p>
<p><a href='http://arxiv.org/abs/2401.05638v1'>http://arxiv.org/abs/2401.05638v1</a></p>
<p><b>Compressor summary</b>: MatSAM is a general and efficient solution for microstructure extraction in microscopic images based on SAM, using point-based prompts generation to adapt to different materials and microscopy types.</p><hr><h3>Transforming Image Super-Resolution: A ConvFormer-based Efficient  Approach</h3>
<p>Gang Wu,Junjun Jiang,Junpeng Jiang,Xianming Liu</p>
<p><a href='http://arxiv.org/abs/2401.05633v1'>http://arxiv.org/abs/2401.05633v1</a></p>
<p><b>Compressor summary</b>: The ConvFormer-based Super-Resolution network (CFSR) is a lightweight method for image super-resolution that replaces the self-attention module with large kernel convolution and uses an edge-preserving feed-forward network to preserve high-frequency information.</p><hr><h3>Natural Language Processing for Dialects of a Language: A Survey</h3>
<p>Aditya Joshi,Raj Dabre,Diptesh Kanojia,Zhuang Li,Haolan Zhan,Gholamreza Haffari,Doris Dippold</p>
<p><a href='http://arxiv.org/abs/2401.05632v1'>http://arxiv.org/abs/2401.05632v1</a></p>
<p><b>Compressor summary</b>: The text surveys past research on natural language processing (NLP) for dialects, covering various tasks, languages, and methods, with a focus on improving the equity of language technologies.</p><hr><h3>Learning Performance-Oriented Control Barrier Functions Under Complex  Safety Constraints and Limited Actuation</h3>
<p>Shaoru Chen,Mahyar Fazlyab</p>
<p><a href='http://arxiv.org/abs/2401.05629v1'>http://arxiv.org/abs/2401.05629v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-supervised learning framework for finding control barrier functions that maximize safety and accommodate complex constraints in nonlinear control systems.</p><hr><h3>Face-GPS: A Comprehensive Technique for Quantifying Facial Muscle  Dynamics in Videos</h3>
<p>Juni Kim,Zhikang Dong,Pawel Polak</p>
<p><a href='http://arxiv.org/abs/2401.05625v1'>http://arxiv.org/abs/2401.05625v1</a></p>
<p><b>Compressor summary</b>: The authors present a new method that uses geometry, smoothing, and spectral analysis to measure facial muscle activity from videos, which could have various applications in security, medicine, and emotion recognition.</p><hr><h3>The Benefits of a Concise Chain of Thought on Problem-Solving in Large  Language Models</h3>
<p>Matthew Renze,Erhan Guven</p>
<p><a href='http://arxiv.org/abs/2401.05618v1'>http://arxiv.org/abs/2401.05618v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Concise Chain-of-Thought (CCoT) prompting, which reduces response length and per-token cost for GPT-3.5 and GPT-4 on MCQA tasks, but may impair math problem-solving for GPT-3.5.</p><hr><h3>Graph Q-Learning for Combinatorial Optimization</h3>
<p>Victoria M. Dax,Jiachen Li,Kevin Leahy,Mykel J. Kochenderfer</p>
<p><a href='http://arxiv.org/abs/2401.05610v1'>http://arxiv.org/abs/2401.05610v1</a></p>
<p><b>Compressor summary</b>: The paper shows how Graph Neural Networks (GNNs) can be used to optimize discrete solutions in Combinatorial Optimization problems by learning policies through Q-Learning.</p><hr><h3>Scaling Laws for Forgetting When Fine-Tuning Large Language Models</h3>
<p>Damjan Kalajdzievski</p>
<p><a href='http://arxiv.org/abs/2401.05605v1'>http://arxiv.org/abs/2401.05605v1</a></p>
<p><b>Compressor summary</b>: Our study shows that LoRA, a PEFT strategy, causes significant catastrophic forgetting in LLMs and provides scaling laws for its relationship with performance and number of parameters.</p><hr><h3>REBUS: A Robust Evaluation Benchmark of Understanding Symbols</h3>
<p>Andrew Gritsevskiy,Arjun Panickssery,Aaron Kirtland,Derik Kauffman,Hans Gundlach,Irina Gritsevskaya,Joe Cavanagh,Jonathan Chiang,Lydia La Roux,Michelle Hung</p>
<p><a href='http://arxiv.org/abs/2401.05604v1'>http://arxiv.org/abs/2401.05604v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a rebus puzzle benchmark to evaluate multimodal language models' performance, which requires various skills and shows current models' weaknesses in reasoning and explanation.</p><hr><h3>Nucleus subtype classification using inter-modality learning</h3>
<p>Lucas W. Remedios,Shunxing Bao,Samuel W. Remedios,Ho Hin Lee,Leon Y. Cai,Thomas Li,Ruining Deng,Can Cui,Jia Li,Qi Liu,Ken S. Lau,Joseph T. Roland,Mary K. Washington,Lori A. Coburn,Keith T. Wilson,Yuankai Huo,Bennett A. Landman</p>
<p><a href='http://arxiv.org/abs/2401.05602v1'>http://arxiv.org/abs/2401.05602v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using inter-modality learning to classify more cell types on virtual H&E stains by synthesizing them from multiplexed immunofluorescence images and transferring labels from the latter.</p><hr><h3>POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource  Unsupervised Neural Machine Translation</h3>
<p>Shilong Pan,Zhiliang Tian,Liang Ding,Zhen Huang,Zhihua Wen,Dongsheng Li</p>
<p><a href='http://arxiv.org/abs/2401.05596v1'>http://arxiv.org/abs/2401.05596v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method called POMP that uses a dynamic graph of multiple auxiliary languages to improve unsupervised neural machine translation for low-resource languages by mitigating linguistic noise in large language models.</p>