
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-07-18</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-07-18 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge  Bases</h3>
<p><a href='http://arxiv.org/abs/2407.12784v1'>http://arxiv.org/abs/2407.12784v1</a></p>
<p><b>Compressor summary</b>: AgentPoison is a novel backdoor attack that targets long-term memory or RAG knowledge bases of LLM agents, allowing them to be compromised without additional model training or fine-tuning while maintaining normal performance for benign instructions.</p><hr><h3>SMooDi: Stylized Motion Diffusion Model</h3>
<p><a href='http://arxiv.org/abs/2407.12783v1'>http://arxiv.org/abs/2407.12783v1</a></p>
<p><b>Compressor summary</b>: SMooDi is a new model that can generate stylized motion for different content and styles, using text guidance and a lightweight adaptor.</p><hr><h3>Contrastive Adversarial Training for Unsupervised Domain Adaptation</h3>
<p><a href='http://arxiv.org/abs/2407.12782v1'>http://arxiv.org/abs/2407.12782v1</a></p>
<p><b>Compressor summary</b>: CAT is a novel approach that uses labeled source domain samples to improve feature generation for the target domain in adversarial training, addressing challenges related to robustness, generalization, and alignment.</p><hr><h3>VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control</h3>
<p><a href='http://arxiv.org/abs/2407.12781v1'>http://arxiv.org/abs/2407.12781v1</a></p>
<p><b>Compressor summary</b>: The text describes a new method that allows controlling camera movement in text-to-video synthesis using transformer-based models and spatiotemporal embeddings.</p><hr><h3>Generalizable Human Gaussians for Sparse View Synthesis</h3>
<p><a href='http://arxiv.org/abs/2407.12777v1'>http://arxiv.org/abs/2407.12777v1</a></p>
<p><b>Compressor summary</b>: This paper introduces a new method using Gaussian Splatting to accurately render 3D humans from sparse views, by learning generalizable human Gaussians and leveraging 2D UV space of a template.</p><hr><h3>OMG-Net: A Deep Learning Framework Deploying Segment Anything to Detect  Pan-Cancer Mitotic Figures from Haematoxylin and Eosin-Stained Slides</h3>
<p><a href='http://arxiv.org/abs/2407.12773v1'>http://arxiv.org/abs/2407.12773v1</a></p>
<p><b>Compressor summary</b>: The study proposes an AI system to automatically detect mitotic figures in cancer images, improving accuracy and consistency in grading and treatment decisions.</p><hr><h3>LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models</h3>
<p><a href='http://arxiv.org/abs/2407.12772v1'>http://arxiv.org/abs/2407.12772v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a benchmark framework, LMMS-EVAL, for evaluating large multi-modal models, and proposes two new tools, LMMS-EVAL LITE and Multimodal LIVEBENCH, to address the evaluation trilemma of cost, coverage, and contamination.</p><hr><h3>A survey and taxonomy of methods interpreting random forest models</h3>
<p><a href='http://arxiv.org/abs/2407.12759v1'>http://arxiv.org/abs/2407.12759v1</a></p>
<p><b>Compressor summary</b>: This paper reviews methods to interpret random forest models and provides a taxonomy of techniques for choosing appropriate tools based on interpretability aspects.</p><hr><h3>Mutual Information Guided Optimal Transport for Unsupervised  Visible-Infrared Person Re-identification</h3>
<p><a href='http://arxiv.org/abs/2407.12758v1'>http://arxiv.org/abs/2407.12758v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new unsupervised learning method for cross-modality pedestrian image retrieval, using mutual information, three learning principles, and iterative training with optimal transport assignment and prototype-based contrastive learning.</p><hr><h3>LookupViT: Compressing visual information to a limited number of tokens</h3>
<p><a href='http://arxiv.org/abs/2407.12753v1'>http://arxiv.org/abs/2407.12753v1</a></p>
<p><b>Compressor summary</b>: LookupViT is a novel vision transformer block that compresses information from high-resolution tokens to reduce inference cost while maintaining or improving accuracy on various tasks such as image and video classification, and captioning.</p><hr><h3>HDLCopilot: Hardware Design Library Querying with Natural Language</h3>
<p><a href='http://arxiv.org/abs/2407.12749v1'>http://arxiv.org/abs/2407.12749v1</a></p>
<p><b>Compressor summary</b>: HDLCopilot is a natural language-based system that helps hardware engineers find information in PDKs faster and more accurately, using an LLM to understand complex queries and provide relevant results.</p><hr><h3>GroundUp: Rapid Sketch-Based 3D City Massing</h3>
<p><a href='http://arxiv.org/abs/2407.12739v1'>http://arxiv.org/abs/2407.12739v1</a></p>
<p><b>Compressor summary</b>: GroundUp is a tool that helps architects design 3D urban areas by converting their sketches into 3D models and allowing them to revise quickly.</p><hr><h3>CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision  Transformer Inference</h3>
<p><a href='http://arxiv.org/abs/2407.12736v1'>http://arxiv.org/abs/2407.12736v1</a></p>
<p><b>Compressor summary</b>: CHOSEN is a co-design framework that automates ViT deployment on FPGAs, improving performance by using multi-kernel design, approximate non-linear functions, efficient logic block usage, and a novel compiler algorithm.</p><hr><h3>EchoSight: Advancing Visual-Language Models with Wiki Knowledge</h3>
<p><a href='http://arxiv.org/abs/2407.12735v1'>http://arxiv.org/abs/2407.12735v1</a></p>
<p><b>Compressor summary</b>: EchoSight is a multimodal framework that uses retrieval-augmented generation to help large language models answer visual questions requiring encyclopedic knowledge.</p><hr><h3>A LLM Benchmark based on the Minecraft Builder Dialog Agent Task</h3>
<p><a href='http://arxiv.org/abs/2407.12734v1'>http://arxiv.org/abs/2407.12734v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a Minecraft builder task benchmark for evaluating large language models' spatial reasoning and vector math skills.</p><hr><h3>RoDE: Linear Rectified Mixture of Diverse Experts for Food Large  Multi-Modal Models</h3>
<p><a href='http://arxiv.org/abs/2407.12730v1'>http://arxiv.org/abs/2407.12730v1</a></p>
<p><b>Compressor summary</b>: Uni-Food is a large, unified food dataset for vision-language tasks that includes images, categories, ingredients, recipes, and nutritional information, while RoDE is a novel method to improve LMMs by allocating parameters based on task complexity.</p><hr><h3>NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with  Diffusion Model</h3>
<p><a href='http://arxiv.org/abs/2407.12727v1'>http://arxiv.org/abs/2407.12727v1</a></p>
<p><b>Compressor summary</b>: The paper introduces NL2Contact, a model that generates realistic 3D hand-object contacts from natural language descriptions, and ContactDescribe, a dataset for training the model.</p><hr><h3>Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language  Models?</h3>
<p><a href='http://arxiv.org/abs/2407.12725v1'>http://arxiv.org/abs/2407.12725v1</a></p>
<p><b>Compressor summary</b>: The text introduces SarcasmCue, a new framework to improve large language models' sarcasm detection by using different prompting strategies that combine sequential and non-sequential methods.</p><hr><h3>An Evaluation of Continual Learning for Advanced Node Semiconductor  Defect Inspection</h3>
<p><a href='http://arxiv.org/abs/2407.12724v1'>http://arxiv.org/abs/2407.12724v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a meta-learning method for semiconductor defect inspection that adapts to new defect types without forgetting previous knowledge or requiring large datasets.</p><hr><h3>SlimFlow: Training Smaller One-Step Diffusion Models with Rectified Flow</h3>
<p><a href='http://arxiv.org/abs/2407.12718v1'>http://arxiv.org/abs/2407.12718v1</a></p>
<p><b>Compressor summary</b>: SlimFlow is a framework for developing small and efficient one-step diffusion models using rectified flow, addressing challenges like initialization mismatch and distillation issues with Annealing Reflow and Flow-Guided Distillation.</p><hr><h3>A Unifying Post-Processing Framework for Multi-Objective Learn-to-Defer  Problems</h3>
<p><a href='http://arxiv.org/abs/2407.12710v1'>http://arxiv.org/abs/2407.12710v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for developing learn-to-defer systems that work with human experts, optimizing accuracy under various constraints using a generalization of Neyman and Pearson's lemma and showing improved results on COMPAS and ACSIncome datasets.</p><hr><h3>MoME: Mixture of Multimodal Experts for Generalist Multimodal Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.12709v1'>http://arxiv.org/abs/2407.12709v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method called mixture of multimodal experts (MoME) to improve generalist large language models on vision-language tasks by modulating features and incorporating sparsely gated experts.</p><hr><h3>IMAGDressing-v1: Customizable Virtual Dressing</h3>
<p><a href='http://arxiv.org/abs/2407.12705v1'>http://arxiv.org/abs/2407.12705v1</a></p>
<p><b>Compressor summary</b>: The text introduces a new virtual dressing method (IMAGDressing-v1) that allows users to edit and control clothing images in various scenes, using a novel metric (CAMI) and a large dataset (IGPair).</p><hr><h3>Subgraph-Aware Training of Text-based Methods for Knowledge Graph  Completion</h3>
<p><a href='http://arxiv.org/abs/2407.12703v1'>http://arxiv.org/abs/2407.12703v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for knowledge graph completion that leverages the structural properties of the graphs and improves performance over existing methods.</p><hr><h3>TransCAD: A Hierarchical Transformer for CAD Sequence Inference from  Point Clouds</h3>
<p><a href='http://arxiv.org/abs/2407.12702v1'>http://arxiv.org/abs/2407.12702v1</a></p>
<p><b>Compressor summary</b>: TransCAD is a transformer-based model that predicts 3D CAD models from point clouds using a hierarchical learning strategy and a loop refiner, achieving state-of-the-art results with a new metric for CAD sequence evaluation.</p><hr><h3>Calibrated Diverse Ensemble Entropy Minimization for Robust Test-Time  Adaptation in Prostate Cancer Detection</h3>
<p><a href='http://arxiv.org/abs/2407.12697v1'>http://arxiv.org/abs/2407.12697v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method, Diverse Ensemble Entropy Minimization (DEnEM), to improve real-time prostate cancer detection using micro-ultrasound and deep learning, addressing the challenge of data distribution shifts across clinical centers.</p><hr><h3>4Dynamic: Text-to-4D Generation with Hybrid Priors</h3>
<p><a href='http://arxiv.org/abs/2407.12684v1'>http://arxiv.org/abs/2407.12684v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Text-to-4D generation method using text-to-video diffusion model and reference video supervision
- Two stages: static 3D generation and dynamic generation with customized SDS losses and prior-switching strategy
- Dynamic modeling representation for deformation and topology changes
- Better realism, consistency, and quality than existing methods

Summary:
The paper presents a text-to-video diffusion method for text-to-4D generation, which uses reference video supervision to ensure realistic and dynamic motion. It combines customized SDS losses and prior-switching strategy for 3D and temporal consistency, and introduces a dynamic modeling representation for shape and topology changes.</p><hr><h3>In-Situ Infrared Camera Monitoring for Defect and Anomaly Detection in  Laser Powder Bed Fusion: Calibration, Data Mapping, and Feature Extraction</h3>
<p><a href='http://arxiv.org/abs/2407.12682v1'>http://arxiv.org/abs/2407.12682v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new approach to accurately map in-situ data for LPBF defect detection using novel IR features and demonstrate its effectiveness through printing, monitoring, and characterizing various parts.</p><hr><h3>Goldfish: Vision-Language Understanding of Arbitrarily Long Videos</h3>
<p><a href='http://arxiv.org/abs/2407.12679v1'>http://arxiv.org/abs/2407.12679v1</a></p>
<p><b>Compressor summary</b>: Goldfish is a method for comprehending videos of any length using efficient retrieval and MiniGPT4-Video, achieving significant improvements in both long and short video understanding.</p><hr><h3>CoSIGN: Few-Step Guidance of ConSIstency Model to Solve General INverse  Problems</h3>
<p><a href='http://arxiv.org/abs/2407.12676v1'>http://arxiv.org/abs/2407.12676v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve diffusion model-based inverse problem solving by using a pretrained consistency model and enforcing constraints during the sampling process, achieving high reconstruction quality with fewer inference steps.</p><hr><h3>Enhancing the Utility of Privacy-Preserving Cancer Classification using  Synthetic Data</h3>
<p><a href='http://arxiv.org/abs/2407.12669v1'>http://arxiv.org/abs/2407.12669v1</a></p>
<p><b>Compressor summary</b>: This paper explores how to use privacy-preserving techniques, such as synthetic data generation and differentially private learning, to improve deep learning models for breast cancer detection from mammography images while protecting patient data.</p><hr><h3>SG-NeRF: Neural Surface Reconstruction with Scene Graph Optimization</h3>
<p><a href='http://arxiv.org/abs/2407.12667v1'>http://arxiv.org/abs/2407.12667v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper presents a novel approach to handle noisy camera poses in Neural Radiance Fields (NeRFs) using scene graphs and confidence estimation.
- The method uses an IoU loss, a coarse-to-fine strategy, and a new dataset with outlier poses for evaluation.
- The results show the superiority of the proposed approach over existing methods in robustness and quality.

Summary:
The paper proposes a scene graph-based NeRF method that can handle noisy camera poses effectively, using confidence estimation, an IoU loss, and a new dataset.</p><hr><h3>Patch-Level Training for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.12665v1'>http://arxiv.org/abs/2407.12665v1</a></p>
<p><b>Compressor summary</b>: The paper proposes patch-level training for large language models, which reduces computational costs by compressing multiple tokens into a single patch and predicting the next patch instead of the next token.</p><hr><h3>InfoNorm: Mutual Information Shaping of Normals for Sparse-View  Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2407.12661v1'>http://arxiv.org/abs/2407.12661v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a method to regularize geometric modeling for 3D scene reconstruction from multi-view images
- The method encourages mutual information among surface normals of highly correlated scene points
- The method uses semantic and geometric features to identify correlated points
- The method improves the surface reconstruction quality of SDF-based neural surfaces

Summary:
The paper introduces a technique that improves 3D scene reconstruction from multi-view images by regularizing geometric modeling with mutual information among normals of correlated points, based on semantic and geometric features.</p><hr><h3>Fusion Flow-enhanced Graph Pooling Residual Networks for Unmanned Aerial  Vehicles Surveillance in Day and Night Dual Visions</h3>
<p><a href='http://arxiv.org/abs/2407.12647v1'>http://arxiv.org/abs/2407.12647v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method, OF-GPRN, that improves UAV detection in dual-vision images using optical fusion and graph-pooling residual network, achieving an 17.9% higher mAP than ResGCN.</p><hr><h3>Zero-shot Text-guided Infinite Image Synthesis with LLM guidance</h3>
<p><a href='http://arxiv.org/abs/2407.12642v1'>http://arxiv.org/abs/2407.12642v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Text-guided image synthesis has many applications but faces challenges
- The proposed approach uses Large Language Models (LLMs) for global coherence and local context understanding
- The model expands images conditioned on captions from LLMs and visual features
- The model shows superior performance and zero-shot capability with LLM guidance

Summary:
The paper presents a novel text-guided image synthesis method that uses LLMs to generate coherent and contextualized captions for expanding images in arbitrary sizes.</p><hr><h3>Toward INT4 Fixed-Point Training via Exploring Quantization Error for  Gradients</h3>
<p><a href='http://arxiv.org/abs/2407.12637v1'>http://arxiv.org/abs/2407.12637v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an adaptive quantization method for low-bit fixed-point training that minimizes the quantization error for large gradients using an optimal interval and an update algorithm.</p><hr><h3>CerberusDet: Unified Multi-Task Object Detection</h3>
<p><a href='http://arxiv.org/abs/2407.12632v1'>http://arxiv.org/abs/2407.12632v1</a></p>
<p><b>Compressor summary</b>: CerberasDet is a YOLO-based multi-headed framework for efficient object detection across multiple tasks and datasets.</p><hr><h3>A Methodology Establishing Linear Convergence of Adaptive Gradient  Methods under PL Inequality</h3>
<p><a href='http://arxiv.org/abs/2407.12629v1'>http://arxiv.org/abs/2407.12629v1</a></p>
<p><b>Compressor summary</b>: The paper proves that AdaGrad and Adam, two adaptive gradient methods, have linear convergence when the cost function meets the Polyak-{\L}ojasiewicz inequality, using a simple and unified approach for batch and stochastic gradients.</p><hr><h3>Domain-specific or Uncertainty-aware models: Does it really make a  difference for biomedical text classification?</h3>
<p><a href='http://arxiv.org/abs/2407.12626v1'>http://arxiv.org/abs/2407.12626v1</a></p>
<p><b>Compressor summary</b>: This study explores how domain-specific models and uncertainty estimation affect the entropy of a model's output probability distribution in biomedical applications.</p><hr><h3>Rethinking the Architecture Design for Efficient Generic Event Boundary  Detection</h3>
<p><a href='http://arxiv.org/abs/2407.12622v1'>http://arxiv.org/abs/2407.12622v1</a></p>
<p><b>Compressor summary</b>: This paper improves generic event boundary detection (GEBD) models by simplifying their architecture, reducing redundancy, and enhancing spatiotemporal learning for faster and more accurate results in real-world applications.</p><hr><h3>Harnessing the Power of Artificial Intelligence to Vitalize Endangered  Indigenous Languages: Technologies and Experiences</h3>
<p><a href='http://arxiv.org/abs/2407.12620v1'>http://arxiv.org/abs/2407.12620v1</a></p>
<p><b>Compressor summary</b>: The text explores AI and NLP applications for preserving endangered Indigenous languages through community engagement, machine translation, and interactive language models.</p><hr><h3>Missing Modality Prediction for Unpaired Multimodal Learning via Joint  Embedding of Unimodal Models</h3>
<p><a href='http://arxiv.org/abs/2407.12616v1'>http://arxiv.org/abs/2407.12616v1</a></p>
<p><b>Compressor summary</b>: The text proposes a framework that efficiently adapts unimodal models to handle missing data and predict the missing modality's embedding using self-supervised learning.</p><hr><h3>Strawberry detection and counting based on YOLOv7 pruning and  information based tracking algorithm</h3>
<p><a href='http://arxiv.org/abs/2407.12614v1'>http://arxiv.org/abs/2407.12614v1</a></p>
<p><b>Compressor summary</b>: This study developed a fast and accurate machine vision system for monitoring strawberry growth and yield using pruned deep learning models and an enhanced object tracking algorithm.</p><hr><h3>On Diversity in Discriminative Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2407.12599v1'>http://arxiv.org/abs/2407.12599v1</a></p>
<p><b>Compressor summary</b>: The paper presents a neural network architecture that leverages diversity principles to achieve high accuracy in self-supervised and semi-supervised learning tasks.</p><hr><h3>Estimate Epidemiological Parameters given Partial Observations based on  Algebraically Observable PINNs</h3>
<p><a href='http://arxiv.org/abs/2407.12598v1'>http://arxiv.org/abs/2407.12598v1</a></p>
<p><b>Compressor summary</b>: The study proposes a method called algebraically observable PINNs to estimate epidemiological parameters using noisy and partial trajectory data.</p><hr><h3>Enhancing Wrist Abnormality Detection with YOLO: Analysis of  State-of-the-art Single-stage Detection Models</h3>
<p><a href='http://arxiv.org/abs/2407.12597v1'>http://arxiv.org/abs/2407.12597v1</a></p>
<p><b>Compressor summary</b>: The study compares different YOLO models for automated wrist fracture detection and finds that they outperform the commonly used two-stage algorithm, Faster R-CNN, especially for pediatric patients.</p><hr><h3>VisFocus: Prompt-Guided Vision Encoders for OCR-Free Dense Document  Understanding</h3>
<p><a href='http://arxiv.org/abs/2407.12594v1'>http://arxiv.org/abs/2407.12594v1</a></p>
<p><b>Compressor summary</b>: VisFocus is an OCR-free method for visual document understanding that uses attention mechanisms and pre-training to focus on relevant text patches based on the language prompt.</p><hr><h3>EvSign: Sign Language Recognition and Translation with Streaming Events</h3>
<p><a href='http://arxiv.org/abs/2407.12593v1'>http://arxiv.org/abs/2407.12593v1</a></p>
<p><b>Compressor summary</b>: The text describes using event cameras to improve sign language recognition and translation, introducing a new dataset (EvSign) and an efficient transformer-based framework for these tasks.</p><hr><h3>VegeDiff: Latent Diffusion Model for Geospatial Vegetation Forecasting</h3>
<p><a href='http://arxiv.org/abs/2407.12592v1'>http://arxiv.org/abs/2407.12592v1</a></p>
<p><b>Compressor summary</b>: VegeDiff is a diffusion model that probabilistically captures uncertainties in geospatial vegetation forecasting, separately modeling the impacts of dynamic meteorological and static environmental variables, and outperforms existing deterministic methods.</p><hr><h3>Privacy-Preserving Adaptive Re-Identification without Image Transfer</h3>
<p><a href='http://arxiv.org/abs/2407.12589v1'>http://arxiv.org/abs/2407.12589v1</a></p>
<p><b>Compressor summary</b>: Fed-Protoid is a novel method for privacy-preserving person re-identification that adapts models on edge devices using distributed source prototypes and minimizes a customized MMD loss.</p><hr><h3>Embracing Events and Frames with Hierarchical Feature Refinement Network  for Object Detection</h3>
<p><a href='http://arxiv.org/abs/2407.12582v1'>http://arxiv.org/abs/2407.12582v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for object detection using event cameras and frame cameras, which improves performance and robustness under challenging conditions.</p><hr><h3>E5-V: Universal Embeddings with Multimodal Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.12580v1'>http://arxiv.org/abs/2407.12580v1</a></p>
<p><b>Compressor summary</b>: E5-V is a framework that adapts large language models for creating universal multimodal embeddings, improving performance and reducing training costs compared to previous approaches.</p><hr><h3>The Fabrication of Reality and Fantasy: Scene Generation with  LLM-Assisted Prompt Interpretation</h3>
<p><a href='http://arxiv.org/abs/2407.12579v1'>http://arxiv.org/abs/2407.12579v1</a></p>
<p><b>Compressor summary</b>: The paper presents RFBench, a benchmark for evaluating image generation from realistic-fantasy prompts, and RFNet, a training-free method combining diffusion models with LLMs to generate better images.</p><hr><h3>DP-KAN: Differentially Private Kolmogorov-Arnold Networks</h3>
<p><a href='http://arxiv.org/abs/2407.12569v1'>http://arxiv.org/abs/2407.12569v1</a></p>
<p><b>Compressor summary</b>: Kolmogorov-Arnold Network can be trained privately and performs similarly to Multilayer Perceptron in differentially private settings.</p><hr><h3>LTRL: Boosting Long-tail Recognition via Reflective Learning</h3>
<p><a href='http://arxiv.org/abs/2407.12568v1'>http://arxiv.org/abs/2407.12568v1</a></p>
<p><b>Compressor summary</b>: Reflecting learning is a novel learning paradigm that uses reviewing, summarizing, and correcting processes to improve long-tail recognition performance.</p><hr><h3>End-to-end Stroke imaging analysis, using reservoir computing-based  effective connectivity, and interpretable Artificial intelligence</h3>
<p><a href='http://arxiv.org/abs/2407.12553v1'>http://arxiv.org/abs/2407.12553v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a pipeline using reservoir computing and directed graph analysis for efficient brain representation in stroke data derived from MRI, enabling classification of effective connectivity and interpretation of disrupted networks with explainable AI tools.</p><hr><h3>UniTE: A Survey and Unified Pipeline for Pre-training ST Trajectory  Embeddings</h3>
<p><a href='http://arxiv.org/abs/2407.12550v1'>http://arxiv.org/abs/2407.12550v1</a></p>
<p><b>Compressor summary</b>: UniTE is a survey and a unified pipeline for pre-training trajectory embeddings that simplifies their development and analysis.</p><hr><h3>Abstraction Alignment: Comparing Model and Human Conceptual  Relationships</h3>
<p><a href='http://arxiv.org/abs/2407.12543v1'>http://arxiv.org/abs/2407.12543v1</a></p>
<p><b>Compressor summary</b>: Abstraction alignment is a method to measure how well an ML model's learned representations match human-expected abstractions using a human abstraction graph.</p><hr><h3>Towards Collaborative Intelligence: Propagating Intentions and Reasoning  for Multi-Agent Coordination with Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.12532v1'>http://arxiv.org/abs/2407.12532v1</a></p>
<p><b>Compressor summary</b>: The framework trains large language models as collaborative agents for coordinated behaviors in cooperative MARL by sharing private intentions, adapting comprehension strategies, and dynamically re-planning sub-tasks.</p><hr><h3>Crafting the Path: Robust Query Rewriting for Information Retrieval</h3>
<p><a href='http://arxiv.org/abs/2407.12529v1'>http://arxiv.org/abs/2407.12529v1</a></p>
<p><b>Compressor summary</b>: Crafting the Path is a novel structured query rewriting method that improves information retrieval by generating relevant queries using a three-step process and is less dependent on Large Language Models' internal knowledge.</p><hr><h3>On the Complexity of Identification in Linear Structural Causal Models</h3>
<p><a href='http://arxiv.org/abs/2407.12528v1'>http://arxiv.org/abs/2407.12528v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm for identifying causal parameters in linear structural models from observational data and proves that the identification task is computationally hard in general.</p><hr><h3>Struct-X: Enhancing Large Language Models Reasoning with Structured Data</h3>
<p><a href='http://arxiv.org/abs/2407.12522v1'>http://arxiv.org/abs/2407.12522v1</a></p>
<p><b>Compressor summary</b>: Struct-X is a framework that helps large language models use structured data more effectively by encoding it into a topological space and guiding them through five phases to enhance reasoning abilities.</p><hr><h3>Causality-inspired Discriminative Feature Learning in Triple Domains for  Gait Recognition</h3>
<p><a href='http://arxiv.org/abs/2407.12519v1'>http://arxiv.org/abs/2407.12519v1</a></p>
<p><b>Compressor summary</b>: CLTD is a new method for gait recognition that uses attention and projection to separate identity features from non-identity clues in spatial, temporal, and spectral domains.</p><hr><h3>Evaluating the transferability potential of deep learning models for  climate downscaling</h3>
<p><a href='http://arxiv.org/abs/2407.12517v1'>http://arxiv.org/abs/2407.12517v1</a></p>
<p><b>Compressor summary</b>: The paper evaluates how well deep learning models can learn from diverse climate data and transfer their knowledge across different tasks, locations, and variables.</p><hr><h3>On Initializing Transformers with Pre-trained Embeddings</h3>
<p><a href='http://arxiv.org/abs/2407.12514v1'>http://arxiv.org/abs/2407.12514v1</a></p>
<p><b>Compressor summary</b>: The paper explores why random initialization schemes outperform some pre-trained word and sub-word embeddings in transformer models, and suggests standardizing the embeddings' values as a solution.</p><hr><h3>$\textit{GeoHard}$: Towards Measuring Class-wise Hardness through  Modelling Class Semantics</h3>
<p><a href='http://arxiv.org/abs/2407.12512v1'>http://arxiv.org/abs/2407.12512v1</a></p>
<p><b>Compressor summary</b>: This paper introduces class-wise hardness and proposes GeoHard, a metric that measures the difficulty of different classes in natural language understanding tasks by analyzing their semantic embeddings.</p><hr><h3>Fast Context-Based Low-Light Image Enhancement via Neural Implicit  Representations</h3>
<p><a href='http://arxiv.org/abs/2407.12511v1'>http://arxiv.org/abs/2407.12511v1</a></p>
<p><b>Compressor summary</b>: CoLIE is a novel approach that enhances low-light images by mapping coordinates to illumination components and reducing computational overhead, making it more adaptable and practical for various scenes and tasks.</p><hr><h3>MERLIN: Multimodal Embedding Refinement via LLM-based Iterative  Navigation for Text-Video Retrieval-Rerank Pipeline</h3>
<p><a href='http://arxiv.org/abs/2407.12508v1'>http://arxiv.org/abs/2407.12508v1</a></p>
<p><b>Compressor summary</b>: MERLIN is a system that uses large language models to improve text-video retrieval by aligning user queries with video content.</p><hr><h3>Subequivariant Reinforcement Learning in 3D Multi-Entity Physical  Environments</h3>
<p><a href='http://arxiv.org/abs/2407.12505v1'>http://arxiv.org/abs/2407.12505v1</a></p>
<p><b>Compressor summary</b>: This paper introduces Subequivariant Hierarchical Neural Networks (SHNN) for learning policies in complex 3D multi-entity systems, using local entity-level graphs and subequivariant message passing, and proposes a new benchmark (MEBEN) to evaluate them.</p><hr><h3>Case2Code: Learning Inductive Reasoning with Synthetic Data</h3>
<p><a href='http://arxiv.org/abs/2407.12504v1'>http://arxiv.org/abs/2407.12504v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a Case2Code task to evaluate and teach large language models (LLMs) inductive reasoning by synthesizing input-output transformations for executable programs and training LLMs on these synthetic cases.</p><hr><h3>EmoFace: Audio-driven Emotional 3D Face Animation</h3>
<p><a href='http://arxiv.org/abs/2407.12501v1'>http://arxiv.org/abs/2407.12501v1</a></p>
<p><b>Compressor summary</b>: EmoFace is a novel audio-driven method for creating emotionally expressive 3D face animations with natural blinks, eye movements, and lip synchronization, and it comes with a new emotional dataset for MetaHuman models.</p><hr><h3>Automate or Assist? The Role of Computational Models in Identifying  Gendered Discourse in US Capital Trial Transcripts</h3>
<p><a href='http://arxiv.org/abs/2407.12500v1'>http://arxiv.org/abs/2407.12500v1</a></p>
<p><b>Compressor summary</b>: The paper presents a case study of using automated systems to identify gender-biased language in US capital trials for women defendants, finding that they can help lawyers challenge their bias and refine annotation rules, but cannot replace human expertise.</p><hr><h3>Evaluating Linguistic Capabilities of Multimodal LLMs in the Lens of  Few-Shot Learning</h3>
<p><a href='http://arxiv.org/abs/2407.12498v1'>http://arxiv.org/abs/2407.12498v1</a></p>
<p><b>Compressor summary</b>: This study evaluates how well large language models perform on a benchmark with different learning methods and shows that they improve when using image captions or interleaved data, and few-shot learning.</p><hr><h3>Test-Time Adaptation with State-Space Models</h3>
<p><a href='http://arxiv.org/abs/2407.12492v1'>http://arxiv.org/abs/2407.12492v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a probabilistic model that adapts a deployed model to distribution shifts by learning hidden feature dynamics and class prototypes without labels or model backbone access.</p><hr><h3>Hierarchical and Decoupled BEV Perception Learning Framework for  Autonomous Driving</h3>
<p><a href='http://arxiv.org/abs/2407.12491v1'>http://arxiv.org/abs/2407.12491v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new hierarchical BEV perception paradigm for autonomous driving systems, using a library of modules and a user-friendly interface to solve challenges like lengthy development cycles and poor reusability.</p><hr><h3>Towards AI-Powered Video Assistant Referee System for Association  Football</h3>
<p><a href='http://arxiv.org/abs/2407.12483v1'>http://arxiv.org/abs/2407.12483v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a semi-automated system, VARS, that improves football fairness and accuracy by analyzing multi-view videos of fouls and suggesting sanctions.</p><hr><h3>Pretraining Data and Tokenizer for Indic LLM</h3>
<p><a href='http://arxiv.org/abs/2407.12481v1'>http://arxiv.org/abs/2407.12481v1</a></p>
<p><b>Compressor summary</b>: Key points:
- A novel approach to data preparation for multilingual Indic large language model
- Data from open-source and proprietary sources, including Common Crawl, Indic books, news articles, and Wikipedia
- Custom preprocessing pipeline for each Indic language to eliminate redundant and low-quality text content
- Deduplication on Common Crawl data to address redundancy in web pages
- A novel multilingual tokenizer training strategy that outperforms OpenAI Tiktoken tokenizer for Indic languages

Summary:
The authors propose a new method to prepare data for a multilingual Indic large language model using diverse and rich sources, custom preprocessing, deduplication, and a novel tokenizer training strategy.</p><hr><h3>A Novel Dependency Framework for Enhancing Discourse Data Analysis</h3>
<p><a href='http://arxiv.org/abs/2407.12473v1'>http://arxiv.org/abs/2407.12473v1</a></p>
<p><b>Compressor summary</b>: The study uses refined BERT-based parsers to convert PDTB and RST annotations into dependencies, enabling unified analysis of different discourse corpora across languages.</p><hr><h3>Continual Learning for Temporal-Sensitive Question Answering</h3>
<p><a href='http://arxiv.org/abs/2407.12470v1'>http://arxiv.org/abs/2407.12470v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a continual learning framework for question answering with temporal memory and contrastive learning, and creates a new dataset to support the research.</p><hr><h3>Estimating Reaction Barriers with Deep Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2407.12453v1'>http://arxiv.org/abs/2407.12453v1</a></p>
<p><b>Compressor summary</b>: The authors propose using reinforcement learning to find the cheapest way for a system to transition between stable states in its energy landscape.</p><hr><h3>Energy-Guided Diffusion Sampling for Offline-to-Online Reinforcement  Learning</h3>
<p><a href='http://arxiv.org/abs/2407.12448v1'>http://arxiv.org/abs/2407.12448v1</a></p>
<p><b>Compressor summary</b>: EDIS improves offline-to-online RL by using a diffusion model to extract prior knowledge from offline data and energy functions to generate better online data.</p><hr><h3>A Comprehensive Sustainable Framework for Machine Learning and  Artificial Intelligence</h3>
<p><a href='http://arxiv.org/abs/2407.12445v1'>http://arxiv.org/abs/2407.12445v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new framework for Sustainable Machine Learning that considers fairness, privacy, interpretability, and greenhouse gas emissions, and proposes a meta-learning algorithm to help users select optimal model architectures based on their requirements.</p><hr><h3>GraphGuard: Contrastive Self-Supervised Learning for Credit-Card Fraud  Detection in Multi-Relational Dynamic Graphs</h3>
<p><a href='http://arxiv.org/abs/2407.12440v1'>http://arxiv.org/abs/2407.12440v1</a></p>
<p><b>Compressor summary</b>: GraphGuard is a new method that uses graphs and self-supervised learning to detect credit card fraud better than existing methods.</p><hr><h3>Semantic-Aware Representation of Multi-Modal Data for Data Ingress: A  Literature Review</h3>
<p><a href='http://arxiv.org/abs/2407.12438v1'>http://arxiv.org/abs/2407.12438v1</a></p>
<p><b>Compressor summary</b>: The study explores how semantic-aware embeddings can improve information retrieval from large, diverse, and temporal data lakes in various application domains.</p><hr><h3>Variable-Agnostic Causal Exploration for Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2407.12437v1'>http://arxiv.org/abs/2407.12437v1</a></p>
<p><b>Compressor summary</b>: VACERL is a framework that uses causal relationships to guide exploration in RL without assuming environmental causal variables, improving agent performance especially in challenging domains.</p><hr><h3>F-HOI: Toward Fine-grained Semantic-Aligned 3D Human-Object Interactions</h3>
<p><a href='http://arxiv.org/abs/2407.12435v1'>http://arxiv.org/abs/2407.12435v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Semantic-HOI, a new dataset for 3D human object interaction, and proposes F-HOI, a unified model that leverages multimodal instructions to handle diverse HOI tasks.</p><hr><h3>GLARE: Low Light Image Enhancement via Generative Latent Feature based  Codebook Retrieval</h3>
<p><a href='http://arxiv.org/abs/2407.12431v1'>http://arxiv.org/abs/2407.12431v1</a></p>
<p><b>Compressor summary</b>: The paper proposes GLARE, a new Low-Light Image Enhancement network that uses a codebook prior derived from normal-light images and a generative module to align low-light features with normal-light latent representations, resulting in improved performance on various benchmarks and real-world data.</p><hr><h3>GeneralAD: Anomaly Detection Across Domains by Attending to Distorted  Features</h3>
<p><a href='http://arxiv.org/abs/2407.12427v1'>http://arxiv.org/abs/2407.12427v1</a></p>
<p><b>Compressor summary</b>: The paper presents GeneralAD, a framework that detects and generates semantic, near-distribution, and industrial anomalies using Vision Transformers and attention-based discriminators, achieving high performance across various datasets.</p><hr><h3>Sharif-STR at SemEval-2024 Task 1: Transformer as a Regression Model for  Fine-Grained Scoring of Textual Semantic Relations</h3>
<p><a href='http://arxiv.org/abs/2407.12426v1'>http://arxiv.org/abs/2407.12426v1</a></p>
<p><b>Compressor summary</b>: The paper explores using fine-tuning techniques on RoBERTa to improve semantic textual relatedness across different languages, with promising results in Latin languages but challenges in Arabic.</p><hr><h3>Navigating the Noisy Crowd: Finding Key Information for Claim  Verification</h3>
<p><a href='http://arxiv.org/abs/2407.12425v1'>http://arxiv.org/abs/2407.12425v1</a></p>
<p><b>Compressor summary</b>: EACon is a framework that helps verify claims by abstracting evidence and deconstructing the claim into subclaims, improving the performance of large language models.</p><hr><h3>SafePowerGraph: Safety-aware Evaluation of Graph Neural Networks for  Transmission Power Grids</h3>
<p><a href='http://arxiv.org/abs/2407.12421v1'>http://arxiv.org/abs/2407.12421v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Power grids are complex infrastructures that need effective analysis methods
- Machine learning techniques, especially Graph Neural Networks (GNNs), can help with grid analysis problems
- Existing benchmarks and datasets do not consider safety and robustness requirements for power grids
- SafePowerGraph is a new framework and benchmark that integrates multiple simulators and assesses GNN performance under realistic scenarios
- Self-supervised learning and graph attention architectures are important for GNN robustness

Summary:
SafePowerGraph is a novel safety-oriented framework and benchmark for evaluating Graph Neural Networks in power grid analysis, using multiple simulators and diverse scenarios.</p><hr><h3>Dirac--Bianconi Graph Neural Networks -- Enabling Non-Diffusive  Long-Range Graph Predictions</h3>
<p><a href='http://arxiv.org/abs/2407.12419v1'>http://arxiv.org/abs/2407.12419v1</a></p>
<p><b>Compressor summary</b>: DBGNNs are a new type of graph neural network that uses the topological Dirac equation to capture complex graph dynamics and outperforms conventional MPNNs for long-range predictions.</p><hr><h3>Improving the classification of extreme classes by means of loss  regularisation and generalised beta distributions</h3>
<p><a href='http://arxiv.org/abs/2407.12417v1'>http://arxiv.org/abs/2407.12417v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a unimodal regularisation method that improves classification of extreme classes in ordinal problems using the generalised beta distribution and shows superior results compared to other methods.</p><hr><h3>Not All Frequencies Are Created Equal:Towards a Dynamic Fusion of  Frequencies in Time-Series Forecasting</h3>
<p><a href='http://arxiv.org/abs/2407.12415v1'>http://arxiv.org/abs/2407.12415v1</a></p>
<p><b>Compressor summary</b>: Frequency Dynamic Fusion (FreDF) is a novel time series forecasting method that captures long-term dependency by predicting and fusing different frequencies in the Fourier domain, adapting to various scenarios.</p><hr><h3>Analyzing the Generalization and Reliability of Steering Vectors -- ICML  2024</h3>
<p><a href='http://arxiv.org/abs/2407.12404v1'>http://arxiv.org/abs/2407.12404v1</a></p>
<p><b>Compressor summary</b>: The paper investigates the reliability and generalization properties of steering vectors for language models, finding that they have limitations in terms of variable effectiveness and brittleness.</p><hr><h3>TurkishMMLU: Measuring Massive Multitask Language Understanding in  Turkish</h3>
<p><a href='http://arxiv.org/abs/2407.12402v1'>http://arxiv.org/abs/2407.12402v1</a></p>
<p><b>Compressor summary</b>: The paper introduces TurkishMMLU, a multitask, multiple-choice Turkish QA benchmark to evaluate LLMs' understanding of the Turkish language across various subjects in high-school curricula.</p><hr><h3>Geometric Remove-and-Retrain (GOAR): Coordinate-Invariant eXplainable AI  Assessment</h3>
<p><a href='http://arxiv.org/abs/2407.12401v1'>http://arxiv.org/abs/2407.12401v1</a></p>
<p><b>Compressor summary</b>: The paper proposes GOAR, a geometric feature-perturbation approach for XAI that overcomes the limitations of pixel-perturbation methods like ROAR.</p><hr><h3>A Practical Solver for Scalar Data Topological Simplification</h3>
<p><a href='http://arxiv.org/abs/2407.12399v1'>http://arxiv.org/abs/2407.12399v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an optimization method for simplifying scalar data that preserves important features and can handle different topological structures, making it practical for real-life datasets and improving analysis and visualization.</p><hr><h3>Mamba-PTQ: Outlier Channels in Recurrent Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.12397v1'>http://arxiv.org/abs/2407.12397v1</a></p>
<p><b>Compressor summary</b>: This paper explores how post-training quantization affects recurrent language models, identifying activation outliers as a challenge similar to transformer-based models.</p><hr><h3>Efficient Depth-Guided Urban View Synthesis</h3>
<p><a href='http://arxiv.org/abs/2407.12395v1'>http://arxiv.org/abs/2407.12395v1</a></p>
<p><b>Compressor summary</b>: EDUS is a new method for fast and efficient urban view synthesis from sparse input images using noisy predicted geometric priors.</p><hr><h3>PersLLM: A Personified Training Approach for Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.12393v1'>http://arxiv.org/abs/2407.12393v1</a></p>
<p><b>Compressor summary</b>: This study proposes PersLLM, a method to integrate psychology-grounded principles of personality into large language models, enhancing their utility in domains like social simulations and human-machine interactions.</p><hr><h3>Morphosyntactic Analysis for CHILDES</h3>
<p><a href='http://arxiv.org/abs/2407.12389v1'>http://arxiv.org/abs/2407.12389v1</a></p>
<p><b>Compressor summary</b>: The text discusses how AI and ML advances enable researchers to compare language development across 27 languages using new transcription and analysis methods.</p><hr><h3>Reliable and Efficient Concept Erasure of Text-to-Image Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2407.12383v1'>http://arxiv.org/abs/2407.12383v1</a></p>
<p><b>Compressor summary</b>: RECE is a novel approach that efficiently modifies text-to-image models to erase inappropriate concepts without compromising their generation ability or requiring additional fine-tuning.</p><hr><h3>Deep Learning-based Sentiment Analysis of Olympics Tweets</h3>
<p><a href='http://arxiv.org/abs/2407.12376v1'>http://arxiv.org/abs/2407.12376v1</a></p>
<p><b>Compressor summary</b>: The study develops an advanced deep learning model for sentiment analysis of Olympic-related tweets, achieving the highest accuracy with the BERT model.</p><hr><h3>FETCH: A Memory-Efficient Replay Approach for Continual Learning in  Image Classification</h3>
<p><a href='http://arxiv.org/abs/2407.12375v1'>http://arxiv.org/abs/2407.12375v1</a></p>
<p><b>Compressor summary</b>: FETCH is a two-stage compression method that improves accuracy in class-incremental continual learning using compressed replay with GDumb.</p><hr><h3>HIMO: A New Benchmark for Full-Body Human Interacting with Multiple  Objects</h3>
<p><a href='http://arxiv.org/abs/2407.12371v1'>http://arxiv.org/abs/2407.12371v1</a></p>
<p><b>Compressor summary</b>: The HIMO dataset provides a large collection of full-body human interactions with multiple objects, along with textual descriptions and temporal segments, for training models on two novel tasks: HOI synthesis and fine-grained timeline control.</p><hr><h3>Temporal receptive field in dynamic graph learning: A comprehensive  analysis</h3>
<p><a href='http://arxiv.org/abs/2407.12370v1'>http://arxiv.org/abs/2407.12370v1</a></p>
<p><b>Compressor summary</b>: This study analyzes the temporal receptive field in dynamic graph learning, showing its importance for accurate prediction in evolving networks.</p><hr><h3>NavGPT-2: Unleashing Navigational Reasoning Capability for Large  Vision-Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.12366v1'>http://arxiv.org/abs/2407.12366v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to bridge the gap between large language models and vision-and-language navigation tasks by aligning visual content in a frozen language model, enabling better integration of language and navigation policy networks.</p><hr><h3>Conversational Query Reformulation with the Guidance of Retrieved  Documents</h3>
<p><a href='http://arxiv.org/abs/2407.12363v1'>http://arxiv.org/abs/2407.12363v1</a></p>
<p><b>Compressor summary</b>: GuideCQR is a framework that improves conversational search by using guided documents to refine queries, generate expected answers, and filter results, outperforming previous methods and LLM prompts.</p><hr><h3>ProcTag: Process Tagging for Assessing the Efficacy of Document  Instruction Data</h3>
<p><a href='http://arxiv.org/abs/2407.12358v1'>http://arxiv.org/abs/2407.12358v1</a></p>
<p><b>Compressor summary</b>: ProcTag is a data-oriented method that evaluates document instruction datasets by tagging the execution process of instructions, enabling selective sampling or filtering for training large language models on document visual question answering tasks.</p><hr><h3>Evaluating graph-based explanations for AI-based recommender systems</h3>
<p><a href='http://arxiv.org/abs/2407.12357v1'>http://arxiv.org/abs/2407.12357v1</a></p>
<p><b>Compressor summary</b>: Graph-based explanations improve usability but not understanding for AI recommendations compared to textual explanations.</p><hr><h3>LTSim: Layout Transportation-based Similarity Measure for Evaluating  Layout Generation</h3>
<p><a href='http://arxiv.org/abs/2407.12356v1'>http://arxiv.org/abs/2407.12356v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new layout similarity measure based on optimal transport, which can handle various layout differences and is applicable to many layout generation tasks.</p><hr><h3>Invertible Neural Warp for NeRF</h3>
<p><a href='http://arxiv.org/abs/2407.12354v1'>http://arxiv.org/abs/2407.12354v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new method for optimizing camera pose and NeRF using overparameterized representations, rigid warp functions, and invertible neural networks.</p><hr><h3>Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval</h3>
<p><a href='http://arxiv.org/abs/2407.12346v1'>http://arxiv.org/abs/2407.12346v1</a></p>
<p><b>Compressor summary</b>: The proposed object-aware query perturbation framework improves cross-modal image-text retrieval for small objects by generating a key feature subspace of the detected objects and perturbing the queries using this subspace.</p><hr><h3>VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual  Descriptions</h3>
<p><a href='http://arxiv.org/abs/2407.12345v1'>http://arxiv.org/abs/2407.12345v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a novel method for trajectory prediction that uses visual inputs from surround-view cameras and textual descriptions generated by VLM and refined by LLM.
- The method achieves a low latency of 53 ms, making it feasible for real-time processing.
- The method outperforms previous methods with similar performance and creates a new dataset (nuScenes-Text) with rich textual annotations.

Summary:
The paper presents a fast and accurate trajectory prediction method that leverages visual and textual cues from surround-view cameras and VLM/LLM-generated descriptions, and introduces a new dataset with these annotations.</p><hr><h3>The Better Angels of Machine Personality: How Personality Relates to LLM  Safety</h3>
<p><a href='http://arxiv.org/abs/2407.12344v1'>http://arxiv.org/abs/2407.12344v1</a></p>
<p><b>Compressor summary</b>: The paper explores how personality traits affect LLM safety abilities and shows that modifying these traits can improve their performance in various aspects.</p><hr><h3>Word Embedding Dimension Reduction via Weakly-Supervised Feature  Selection</h3>
<p><a href='http://arxiv.org/abs/2407.12342v1'>http://arxiv.org/abs/2407.12342v1</a></p>
<p><b>Compressor summary</b>: WordFS is a new method to reduce word embedding dimensions while maintaining efficiency and effectiveness in various natural language processing tasks.</p><hr><h3>M2DS: Multilingual Dataset for Multi-document Summarisation</h3>
<p><a href='http://arxiv.org/abs/2407.12336v1'>http://arxiv.org/abs/2407.12336v1</a></p>
<p><b>Compressor summary</b>: The text discusses the need for a multilingual dataset for multi-document summarization (M2DS) in today's globalized digital world, which is provided by BBC articles in five languages.</p><hr><h3>Why Do You Grok? A Theoretical Analysis of Grokking Modular Addition</h3>
<p><a href='http://arxiv.org/abs/2407.12332v1'>http://arxiv.org/abs/2407.12332v1</a></p>
<p><b>Compressor summary</b>: The paper explains why some models can generalize well on modular addition problem even after overfitting, by transitioning from kernel-like to gradient descent-like behavior.</p><hr><h3>I2AM: Interpreting Image-to-Image Latent Diffusion Models via  Attribution Maps</h3>
<p><a href='http://arxiv.org/abs/2407.12331v1'>http://arxiv.org/abs/2407.12331v1</a></p>
<p><b>Compressor summary</b>: The paper proposes I2AM, a method to enhance interpretability of image generation models by aggregating patch-level cross-attention scores, enabling detailed attribution analysis and evaluation for reference-based image inpainting tasks.</p><hr><h3>Uncertainty Calibration with Energy Based Instance-wise Scaling in the  Wild Dataset</h3>
<p><a href='http://arxiv.org/abs/2407.12330v1'>http://arxiv.org/abs/2407.12330v1</a></p>
<p><b>Compressor summary</b>: This paper proposes an energy model-based instance-wise calibration method for deep neural networks to improve their uncertainty estimation and reliability in multi-class classification tasks, especially for out-of-distribution inputs.</p><hr><h3>Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language  Models</h3>
<p><a href='http://arxiv.org/abs/2407.12327v1'>http://arxiv.org/abs/2407.12327v1</a></p>
<p><b>Compressor summary</b>: The Spectra LLM suite explores low-bitwidth language models and their performance, training dynamics, and scaling trends, with promising results in size reduction and commonsense reasoning, but challenges in toxicity and perplexity.</p><hr><h3>Out of Length Text Recognition with Sub-String Matching</h3>
<p><a href='http://arxiv.org/abs/2407.12317v1'>http://arxiv.org/abs/2407.12317v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for long text recognition, called OOL Text Recognition with sub-String Matching (SMTR), which uses cross-attention and regularization training to handle arbitrary length text.</p><hr><h3>ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via  Modal Fusion Map</h3>
<p><a href='http://arxiv.org/abs/2407.12315v1'>http://arxiv.org/abs/2407.12315v1</a></p>
<p><b>Compressor summary</b>: ModalChorus is an interactive system for visual probing and alignment of multi-modal embeddings, using a two-stage process with Modal Fusion Map and embedding alignment to enhance modality fusion and intention articulation.</p><hr><h3>MEDFuse: Multimodal EHR Data Fusion with Masked Lab-Test Modeling and  Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2407.12309v1'>http://arxiv.org/abs/2407.12309v1</a></p>
<p><b>Compressor summary</b>: MEDFuse is a framework that fuses structured and unstructured EHR data using multimodal embeddings to improve clinical decision-making, achieving high performance in multi-label classification tasks.</p><hr><h3>Weakly-Supervised 3D Hand Reconstruction with Knowledge Prior and  Uncertainty Guidance</h3>
<p><a href='http://arxiv.org/abs/2407.12307v1'>http://arxiv.org/abs/2407.12307v1</a></p>
<p><b>Compressor summary</b>: The text introduces a weakly-supervised method for 3D hand reconstruction that uses hand knowledge from different sources and uncertainty modeling to train with 2D landmark annotations, improving performance over existing methods.</p><hr><h3>Splatfacto-W: A Nerfstudio Implementation of Gaussian Splatting for  Unconstrained Photo Collections</h3>
<p><a href='http://arxiv.org/abs/2407.12306v1'>http://arxiv.org/abs/2407.12306v1</a></p>
<p><b>Compressor summary</b>: Splatfacto-W is a novel view synthesis method that improves scene consistency in unconstrained images by incorporating per-Gaussian neural color features, per-image appearance embeddings, and a spherical harmonics-based background model.</p><hr><h3>Exploiting Inter-Image Similarity Prior for Low-Bitrate Remote Sensing  Image Compression</h3>
<p><a href='http://arxiv.org/abs/2407.12295v1'>http://arxiv.org/abs/2407.12295v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a codebook-based remote sensing image compression method that leverages VQGAN to generate a discrete codebook and uses a Transformer-based prediction model and a hierarchical prior integration network to enhance the decoding performance.</p><hr><h3>Any Target Can be Offense: Adversarial Example Generation via  Generalized Latent Infection</h3>
<p><a href='http://arxiv.org/abs/2407.12292v1'>http://arxiv.org/abs/2407.12292v1</a></p>
<p><b>Compressor summary</b>: The GAKer method generates adversarial examples that can fool deep neural networks into recognizing any image as a target object, improving attack success rates for both known and unknown classes.</p><hr><h3>JointDreamer: Ensuring Geometry Consistency and Text Congruence in  Text-to-3D Generation via Joint Score Distillation</h3>
<p><a href='http://arxiv.org/abs/2407.12291v1'>http://arxiv.org/abs/2407.12291v1</a></p>
<p><b>Compressor summary</b>: Joint Score Distillation (JSD) improves text-to-3D generation by considering coherence among views and using energy functions to capture view-aware information.</p><hr><h3>Chip Placement with Diffusion</h3>
<p><a href='http://arxiv.org/abs/2407.12282v1'>http://arxiv.org/abs/2407.12282v1</a></p>
<p><b>Compressor summary</b>: The authors propose a diffusion model and a novel architecture for macro placement in digital circuits, which achieves competitive performance and trains at scale using synthetic datasets.</p><hr><h3>ER-FSL: Experience Replay with Feature Subspace Learning for Online  Continual Learning</h3>
<p><a href='http://arxiv.org/abs/2407.12279v1'>http://arxiv.org/abs/2407.12279v1</a></p>
<p><b>Compressor summary</b>: ER-FSL is a novel online continual learning method that uses multiple feature subspaces to learn new data and replays old data in a larger feature space to prevent catastrophic forgetting.</p><hr><h3>Multimodal Reranking for Knowledge-Intensive Visual Question Answering</h3>
<p><a href='http://arxiv.org/abs/2407.12277v1'>http://arxiv.org/abs/2407.12277v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a multi-modal reranker for visual question answering that uses cross-item interaction to improve ranking quality and relevance score modeling of knowledge candidates.</p><hr><h3>When can transformers compositionally generalize in-context?</h3>
<p><a href='http://arxiv.org/abs/2407.12275v1'>http://arxiv.org/abs/2407.12275v1</a></p>
<p><b>Compressor summary</b>: The text discusses how transformers can compose tasks from independent components, but struggle to generalize compositionally unless there's a bottleneck separating task inference and execution.</p><hr><h3>GRIDS: Grouped Multiple-Degradation Restoration with Image Degradation  Similarity</h3>
<p><a href='http://arxiv.org/abs/2407.12273v1'>http://arxiv.org/abs/2407.12273v1</a></p>
<p><b>Compressor summary</b>: GRIDS is a new image restoration method that groups similar degradations and improves efficiency and effectiveness of restoration.</p><hr><h3>RBAD: A Dataset and Benchmark for Retinal Vessels Branching Angle  Detection</h3>
<p><a href='http://arxiv.org/abs/2407.12271v1'>http://arxiv.org/abs/2407.12271v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new method to accurately detect retinal branching angles using image processing and provides an open-source annotation tool and a benchmark dataset for evaluation.</p><hr><h3>UTG: Towards a Unified View of Snapshot and Event Based Models for  Temporal Graphs</h3>
<p><a href='http://arxiv.org/abs/2407.12269v1'>http://arxiv.org/abs/2407.12269v1</a></p>
<p><b>Compressor summary</b>: Unified Temporal Graph (UTG) is a framework that combines snapshot- and event-based machine learning models for temporal graphs, improving their performance and efficiency.</p><hr><h3>Generating 3D House Wireframes with Semantics</h3>
<p><a href='http://arxiv.org/abs/2407.12267v1'>http://arxiv.org/abs/2407.12267v1</a></p>
<p><b>Compressor summary</b>: Key points:
- New approach for generating 3D house wireframes with semantic enrichment using an autoregressive model
- Unified wire-based representation for improved coherence and semantic integration
- Graph-based autoencoder and transformer-based decoder for learning geometric tokens and generating wireframes
- Iterative prediction and decoding for detailed wireframes that can be segmented into components

Summary:
The authors propose a novel autoregressive model that uses a unified wire-based representation to generate 3D house wireframes with semantic enrichment, achieving superior accuracy and novelty.</p><hr><h3>In-Context Probing Approximates Influence Function for Data Valuation</h3>
<p><a href='http://arxiv.org/abs/2407.12259v1'>http://arxiv.org/abs/2407.12259v1</a></p>
<p><b>Compressor summary</b>: In-context probing is a useful method for valuing and selecting training data for large language models, as it approximates the influence functions that estimate the contribution of data to model predictions.</p><hr><h3>Facial Affect Recognition based on Multi Architecture Encoder and  Feature Fusion for the ABAW7 Challenge</h3>
<p><a href='http://arxiv.org/abs/2407.12258v1'>http://arxiv.org/abs/2407.12258v1</a></p>
<p><b>Compressor summary</b>: The paper describes a method for facial expression analysis using Transformer Encoder and visual features, achieving better results than previous methods in a competition.</p><hr><h3>Compound Expression Recognition via Multi Model Ensemble for the ABAW7  Challenge</h3>
<p><a href='http://arxiv.org/abs/2407.12257v1'>http://arxiv.org/abs/2407.12257v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an ensemble learning method using different neural networks to recognize complex human emotional expressions accurately.</p><hr><h3>Dual-Hybrid Attention Network for Specular Highlight Removal</h3>
<p><a href='http://arxiv.org/abs/2407.12255v1'>http://arxiv.org/abs/2407.12255v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel end-to-end network (DHAN-SHR) that uses hybrid attention mechanisms to remove specular highlights from images without relying on additional priors or supervision, achieving state-of-the-art performance and introducing a large-scale benchmark dataset.</p><hr><h3>COKE: Causal Discovery with Chronological Order and Expert Knowledge in  High Proportion of Missing Manufacturing Data</h3>
<p><a href='http://arxiv.org/abs/2407.12254v1'>http://arxiv.org/abs/2407.12254v1</a></p>
<p><b>Compressor summary</b>: COKE is a method that uses expert knowledge and chronological order to construct causal graphs for fault diagnosis and optimization in manufacturing processes without imputing missing data, achieving significant improvement in F1-score compared to benchmark methods.</p><hr><h3>Lacuna Language Learning: Leveraging RNNs for Ranked Text Completion in  Digitized Coptic Manuscripts</h3>
<p><a href='http://arxiv.org/abs/2407.12247v1'>http://arxiv.org/abs/2407.12247v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a bidirectional RNN model for predicting missing characters in ancient manuscripts, which can help scholars rank possible reconstructions but not reconstruct the text definitively.</p><hr><h3>Explaining Deep Neural Networks by Leveraging Intrinsic Methods</h3>
<p><a href='http://arxiv.org/abs/2407.12243v1'>http://arxiv.org/abs/2407.12243v1</a></p>
<p><b>Compressor summary</b>: The thesis aims to improve interpretability of deep neural networks by introducing self-explanatory designs, studying neuron activation phenomena, and analyzing visual analytics applications.</p><hr><h3>Adaptive Cascading Network for Continual Test-Time Adaptation</h3>
<p><a href='http://arxiv.org/abs/2407.12240v1'>http://arxiv.org/abs/2407.12240v1</a></p>
<p><b>Compressor summary</b>: Our method adapts a pre-trained model to different unlabelled domains at test time by updating both feature extractor and classifier using meta-learning and cascading, with new evaluation metrics for real-world scenarios.</p><hr><h3>Motion and Structure from Event-based Normal Flow</h3>
<p><a href='http://arxiv.org/abs/2407.12239v1'>http://arxiv.org/abs/2407.12239v1</a></p>
<p><b>Compressor summary</b>: The text describes a new approach to solve computer vision problems using event cameras, which are more challenging than standard cameras and have better performance in certain scenarios.</p><hr><h3>Urban Traffic Forecasting with Integrated Travel Time and Data  Availability in a Conformal Graph Neural Network Framework</h3>
<p><a href='http://arxiv.org/abs/2407.12238v1'>http://arxiv.org/abs/2407.12238v1</a></p>
<p><b>Compressor summary</b>: The study proposes a new framework using Graph Neural Networks and Adaptive Conformal Prediction to improve traffic flow prediction, outperforming existing models.</p><hr><h3>Base Models for Parabolic Partial Differential Equations</h3>
<p><a href='http://arxiv.org/abs/2407.12234v1'>http://arxiv.org/abs/2407.12234v1</a></p>
<p><b>Compressor summary</b>: The text proposes a meta-learning framework for efficiently solving parabolic PDEs across different scenarios by learning from existing simulations.</p><hr><h3>Conditional Quantile Estimation for Uncertain Watch Time in Short-Video  Recommendation</h3>
<p><a href='http://arxiv.org/abs/2407.12223v1'>http://arxiv.org/abs/2407.12223v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Conditional Quantile Estimation (CQE), a novel technique that uses quantile regression to capture the nuanced distribution of watch time in short video recommendation, improving accuracy and robustness.</p><hr><h3>Questionable practices in machine learning</h3>
<p><a href='http://arxiv.org/abs/2407.12220v1'>http://arxiv.org/abs/2407.12220v1</a></p>
<p><b>Compressor summary</b>: The text discusses the prevalence of questionable research practices in evaluating large language models and their negative impact on reproducibility.</p>