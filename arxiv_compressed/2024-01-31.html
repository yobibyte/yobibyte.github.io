
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
<a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center" width=50%></a>
            <h1>arxiv compressed, 2024-01-31</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-01-31 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>A simple, strong baseline for building damage detection on the xBD  dataset</h3>
<p><a href='http://arxiv.org/abs/2401.17271v1'>http://arxiv.org/abs/2401.17271v1</a></p>
<p><b>Compressor summary</b>: The authors present a simple and easy to apply building damage detection method based on a winning xView2 solution, while showing that both their simplified and the original model struggle to generalize to unseen locations.</p><hr><h3>YOLO-World: Real-Time Open-Vocabulary Object Detection</h3>
<p><a href='http://arxiv.org/abs/2401.17270v1'>http://arxiv.org/abs/2401.17270v1</a></p>
<p><b>Compressor summary</b>: YOLO-World is a new approach that improves YOLO detectors by using vision-language modeling to recognize objects with unknown names efficiently and accurately.</p><hr><h3>Weaver: Foundation Models for Creative Writing</h3>
<p><a href='http://arxiv.org/abs/2401.17268v1'>http://arxiv.org/abs/2401.17268v1</a></p>
<p><b>Compressor summary</b>: Weaver is a family of large language models specialized in content creation that outperforms generalist LLMs on various writing tasks and supports retrieval-augmented generation and function calling.</p><hr><h3>ReacLLaMA: Merging chemical and textual information in chemical  reactivity AI models</h3>
<p><a href='http://arxiv.org/abs/2401.17267v1'>http://arxiv.org/abs/2401.17267v1</a></p>
<p><b>Compressor summary</b>: The text presents two methods to improve chemical reactivity prediction by incorporating procedural text information into a Graphormer model.</p><hr><h3>You Only Need One Step: Fast Super-Resolution with Stable Diffusion via  Scale Distillation</h3>
<p><a href='http://arxiv.org/abs/2401.17258v1'>http://arxiv.org/abs/2401.17258v1</a></p>
<p><b>Compressor summary</b>: YONOS-SR is a new image super-resolution method that uses scale distillation to train a diffusion model, achieving state-of-the-art results with just one DDIM step.</p><hr><h3>Weak-to-Strong Jailbreaking on Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2401.17256v1'>http://arxiv.org/abs/2401.17256v1</a></p>
<p><b>Compressor summary</b>: The paper proposes and demonstrates a weak-to-strong jailbreaking attack on large language models, revealing a safety issue that needs to be addressed when aligning them.</p><hr><h3>LLaMP: Large Language Model Made Powerful for High-fidelity Materials  Knowledge Retrieval and Distillation</h3>
<p><a href='http://arxiv.org/abs/2401.17244v1'>http://arxiv.org/abs/2401.17244v1</a></p>
<p><b>Compressor summary</b>: LLaMP is a multimodal framework that uses reasoning-and-acting agents to reduce hallucination in Large Language Models for materials informatics tasks, such as data retrieval and synthesis procedures.</p><hr><h3>ReAlnet: Achieving More Human Brain-Like Vision via Human Neural  Representational Alignment</h3>
<p><a href='http://arxiv.org/abs/2401.17231v1'>http://arxiv.org/abs/2401.17231v1</a></p>
<p><b>Compressor summary</b>: ReAlnet is a new AI vision model that aligns with human brain activity using non-invasive EEG recordings, improving its performance and robustness.</p><hr><h3>Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding  Space using Contrastive Learning</h3>
<p><a href='http://arxiv.org/abs/2401.17228v1'>http://arxiv.org/abs/2401.17228v1</a></p>
<p><b>Compressor summary</b>: The authors propose a pluralist moral sentence embedding space using contrastive learning, which captures the nuances of moral judgment but needs supervised learning with human labels.</p><hr><h3>MouSi: Poly-Visual-Expert Vision-Language Models</h3>
<p><a href='http://arxiv.org/abs/2401.17221v1'>http://arxiv.org/abs/2401.17221v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using ensemble experts technique to improve vision-language models by synergizing different visual encoders and reducing positional encoding waste.</p><hr><h3>ContactGen: Contact-Guided Interactive 3D Human Generation for Partners</h3>
<p><a href='http://arxiv.org/abs/2401.17212v1'>http://arxiv.org/abs/2401.17212v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method called ContactGen that can generate 3D interactive humans with different poses and contact regions based on an interaction label using a guided diffusion framework.</p><hr><h3>Self-Supervised Representation Learning for Nerve Fiber Distribution  Patterns in 3D-PLI</h3>
<p><a href='http://arxiv.org/abs/2401.17207v1'>http://arxiv.org/abs/2401.17207v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new data-driven method to characterize nerve fiber architecture in 3D brain images using contrastive learning, enabling downstream analysis tasks and improving understanding of the human brain organization.</p><hr><h3>Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT  Semantic Embeddings K-Means-Infused CRF Model</h3>
<p><a href='http://arxiv.org/abs/2401.17206v1'>http://arxiv.org/abs/2401.17206v1</a></p>
<p><b>Compressor summary</b>: This paper reviews Bangla Named Entity Recognition, identifies its limitations, and proposes a Gazetteer and a new NER solution using advanced NLP tools.</p><hr><h3>CPR++: Object Localization via Single Coarse Point Supervision</h3>
<p><a href='http://arxiv.org/abs/2401.17203v1'>http://arxiv.org/abs/2401.17203v1</a></p>
<p><b>Compressor summary</b>: The paper proposes coarse point refinement (CPR) and CPR++ methods to reduce semantic variance in point-based object localization by selecting a semantic centre point and using variance regularization, improving object detection performance on four datasets.</p><hr><h3>NormEnsembleXAI: Unveiling the Strengths and Weaknesses of XAI Ensemble  Techniques</h3>
<p><a href='http://arxiv.org/abs/2401.17200v1'>http://arxiv.org/abs/2401.17200v1</a></p>
<p><b>Compressor summary</b>: The paper compares XAI ensemble methods, proposes a new method called NormEnsembleXAI for improving interpretability, and provides a library to implement it.</p><hr><h3>Single Word Change is All You Need: Designing Attacks and Defenses for  Text Classifiers</h3>
<p><a href='http://arxiv.org/abs/2401.17196v1'>http://arxiv.org/abs/2401.17196v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a metric to measure classifier robustness against single-word changes in text classification, an efficient attack method exploiting this vulnerability, and a defense mechanism to improve robustness.</p><hr><h3>Embracing Language Inclusivity and Diversity in CLIP through Continual  Language Learning</h3>
<p><a href='http://arxiv.org/abs/2401.17186v1'>http://arxiv.org/abs/2401.17186v1</a></p>
<p><b>Compressor summary</b>: The paper proposes CLL-CLIP, a multilingual VL model that learns to update its language knowledge incrementally without forgetting, and evaluates it on image-text retrieval tasks across 36 languages.</p><hr><h3>Transfer Learning for Text Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2401.17181v1'>http://arxiv.org/abs/2401.17181v1</a></p>
<p><b>Compressor summary</b>: Text diffusion could potentially replace autoregressive decoding in large language models, with some tasks showing better performance and faster generation, but more research is needed.</p><hr><h3>GraphViz2Vec: A Structure-aware Feature Generation Model to Improve  Classification in GNNs</h3>
<p><a href='http://arxiv.org/abs/2401.17178v1'>http://arxiv.org/abs/2401.17178v1</a></p>
<p><b>Compressor summary</b>: GraphViz2Vec creates initial embeddings for GNNs using energy diagrams from random walks to capture structural information and improve performance on node and link classification tasks.</p><hr><h3>Zero-Shot Reinforcement Learning via Function Encoders</h3>
<p><a href='http://arxiv.org/abs/2401.17173v1'>http://arxiv.org/abs/2401.17173v1</a></p>
<p><b>Compressor summary</b>: The paper introduces the function encoder, an algorithm that helps reinforcement learning agents transfer between related tasks using a coherent vector representation of the reward or transition function.</p><hr><h3>Conditional and Modal Reasoning in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2401.17169v1'>http://arxiv.org/abs/2401.17169v1</a></p>
<p><b>Compressor summary</b>: The paper investigates how well large language models can reason with conditionals and epistemic modals, which are important for human reasoning.</p><hr><h3>Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool  Utilization in Real-World Complex Scenarios</h3>
<p><a href='http://arxiv.org/abs/2401.17167v1'>http://arxiv.org/abs/2401.17167v1</a></p>
<p><b>Compressor summary</b>: UltraTool is a novel benchmark that evaluates large language models' ability in planning, creating, and using tools in complex real-world scenarios.</p><hr><h3>Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis</h3>
<p><a href='http://arxiv.org/abs/2401.17159v1'>http://arxiv.org/abs/2401.17159v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a Monte Carlo Tree Search (MCTS) based method for automatic synthesis of effective SMT strategies, which improves performance on various SMT logics compared to existing methods and solvers.</p><hr><h3>Large Language Model Evaluation via Matrix Entropy</h3>
<p><a href='http://arxiv.org/abs/2401.17139v1'>http://arxiv.org/abs/2401.17139v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Paper introduces matrix entropy as a new metric for evaluating LLMs
- Matrix entropy measures data compression proficiency in LLMs
- Applicable in single-modal and multi-modal settings
- Reveals scaling laws and alignment quality of LLMs

Summary:
The paper presents matrix entropy, a novel metric that assesses how well LLMs compress data and handle multiple modalities, showing their scalability and alignment performance.</p><hr><h3>Personalized Differential Privacy for Ridge Regression</h3>
<p><a href='http://arxiv.org/abs/2401.17127v1'>http://arxiv.org/abs/2401.17127v1</a></p>
<p><b>Compressor summary</b>: The text introduces a novel method called Personalized-DP Output Perturbation that allows training machine learning models with individual privacy levels, and provides theoretical accuracy guarantees for this approach.</p><hr><h3>Explainable data-driven modeling via mixture of experts: towards  effective blending of grey and black-box models</h3>
<p><a href='http://arxiv.org/abs/2401.17118v1'>http://arxiv.org/abs/2401.17118v1</a></p>
<p><b>Compressor summary</b>: The proposed framework combines diverse local models using a "mixture of experts" rationale, enabling accurate and interpretable predictions for complex systems.</p><hr><h3>Evaluation in Neural Style Transfer: A Review</h3>
<p><a href='http://arxiv.org/abs/2401.17109v1'>http://arxiv.org/abs/2401.17109v1</a></p>
<p><b>Compressor summary</b>: The text discusses the challenges of evaluating Neural Style Transfer (NST) methods, highlighting inconsistencies and limitations, and providing recommendations for a standardized framework to compare and understand results better.</p><hr><h3>MT-Ranker: Reference-free machine translation evaluation by inter-system  ranking</h3>
<p><a href='http://arxiv.org/abs/2401.17099v1'>http://arxiv.org/abs/2401.17099v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new way to evaluate machine translation quality without references by ranking translations in pairs and shows that it correlates better with human judgments and outperforms existing methods on various benchmarks.</p><hr><h3>CharNet: Generalized Approach for High-Complexity Character  Classification</h3>
<p><a href='http://arxiv.org/abs/2401.17098v1'>http://arxiv.org/abs/2401.17098v1</a></p>
<p><b>Compressor summary</b>: CharNet is a simple and effective method for classifying handwritten characters with complex structures.</p><hr><h3>Traffic estimation in unobserved network locations using data-driven  macroscopic models</h3>
<p><a href='http://arxiv.org/abs/2401.17095v1'>http://arxiv.org/abs/2401.17095v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a model called MaTE that uses macroscopic flow theory and multi-source data to estimate traffic flow and travel time accurately, even when sensor measurements are unavailable.</p><hr><h3>StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis</h3>
<p><a href='http://arxiv.org/abs/2401.17093v1'>http://arxiv.org/abs/2401.17093v1</a></p>
<p><b>Compressor summary</b>: StrokeNUWA is a new method that uses stroke tokens on vector graphics to generate better and faster visuals for LLMs.</p><hr><h3>NNOSE: Nearest Neighbor Occupational Skill Extraction</h3>
<p><a href='http://arxiv.org/abs/2401.17092v1'>http://arxiv.org/abs/2401.17092v1</a></p>
<p><b>Compressor summary</b>: The paper proposes NNOSE, a method that leverages multiple occupational skill datasets by retrieving similar skills from external sources and improves skill extraction without additional fine-tuning.</p><hr><h3>Active Generation Network of Human Skeleton for Action Recognition</h3>
<p><a href='http://arxiv.org/abs/2401.17086v1'>http://arxiv.org/abs/2401.17086v1</a></p>
<p><b>Compressor summary</b>: The active generative network (AGN) can create diverse and temporally consistent actions for human action recognition with very little data by adapting motion styles and using uncertainty metrics.</p><hr><h3>SemScore: Automated Evaluation of Instruction-Tuned LLMs based on  Semantic Textual Similarity</h3>
<p><a href='http://arxiv.org/abs/2401.17072v1'>http://arxiv.org/abs/2401.17072v1</a></p>
<p><b>Compressor summary</b>: The paper introduces SemScore, a simple but effective evaluation metric for instruction-tuned LLMs that compares generated responses to gold target responses using semantic textual similarity and shows it outperforms other metrics in correlating with human evaluation.</p><hr><h3>Outline of an Independent Systematic Blackbox Test for ML-based Systems</h3>
<p><a href='http://arxiv.org/abs/2401.17062v1'>http://arxiv.org/abs/2401.17062v1</a></p>
<p><b>Compressor summary</b>: The article suggests a testing procedure for ML models and systems that considers their black box nature and stochastic properties, and provides test results and method extensions.</p><hr><h3>OmniSCV: An Omnidirectional Synthetic Image Generator for Computer  Vision</h3>
<p><a href='http://arxiv.org/abs/2401.17061v1'>http://arxiv.org/abs/2401.17061v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper presents a tool for generating omnidirectional images with semantic and depth information
- The images are synthesized in a virtual environment using various projection models and lenses
- The tool provides pixel-wise ground-truth information for training and testing computer vision algorithms
Summary:
The paper introduces a tool that generates realistic omnidirectional images with semantic and depth data from different projection models and lenses, enabling accurate training and testing of computer vision methods.</p><hr><h3>Atlanta Scaled layouts from non-central panoramas</h3>
<p><a href='http://arxiv.org/abs/2401.17058v1'>http://arxiv.org/abs/2401.17058v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method to reconstruct 3D indoor layouts from non-central panoramas using neural networks and geometry reasoning, outperforming previous methods and solving the problem in Manhattan and Atlanta environments.</p><hr><h3>BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane  Extrapolation</h3>
<p><a href='http://arxiv.org/abs/2401.17053v1'>http://arxiv.org/abs/2401.17053v1</a></p>
<p><b>Compressor summary</b>: BlockFusion is a diffusion-based model that creates 3D scenes using unit blocks and can seamlessly extend them by incorporating new blocks, achieving diverse, high-quality, and geometrically consistent results.</p><hr><h3>Making Parametric Anomaly Detection on Tabular Data Non-Parametric Again</h3>
<p><a href='http://arxiv.org/abs/2401.17052v1'>http://arxiv.org/abs/2401.17052v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new method for anomaly detection on tabular data using deep learning models that leverage retrieval techniques to improve reconstruction accuracy.</p><hr><h3>ViTree: Single-path Neural Tree for Step-wise Interpretable Fine-grained  Visual Categorization</h3>
<p><a href='http://arxiv.org/abs/2401.17050v1'>http://arxiv.org/abs/2401.17050v1</a></p>
<p><b>Compressor summary</b>: ViTree combines vision transformers and neural decision trees to create interpretable, fine-grained visual categorization models that outperform competitors.</p><hr><h3>Explaining Explanations in Probabilistic Logic Programming</h3>
<p><a href='http://arxiv.org/abs/2401.17045v1'>http://arxiv.org/abs/2401.17045v1</a></p>
<p><b>Compressor summary</b>: The authors propose an approach to improve the explanations generated by probabilistic logic programming systems by defining a query-driven inference mechanism that allows for causal analysis and relevance filtering.</p><hr><h3>Scalable Mechanism Design for Multi-Agent Path Finding</h3>
<p><a href='http://arxiv.org/abs/2401.17044v1'>http://arxiv.org/abs/2401.17044v1</a></p>
<p><b>Compressor summary</b>: The text discusses the challenges and solutions for scalable mechanism design in multi-agent path finding, where self-interested agents may misrepresent their goals to achieve better outcomes.</p><hr><h3>CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented  Generation of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2401.17043v1'>http://arxiv.org/abs/2401.17043v1</a></p>
<p><b>Compressor summary</b>: RAG is a technique that improves language models by using external knowledge sources, but current benchmarks are limited; this paper creates a large-scale benchmark with four CRUD application types and evaluates all RAG components in various scenarios.</p><hr><h3>Forecasting VIX using Bayesian Deep Learning</h3>
<p><a href='http://arxiv.org/abs/2401.17042v1'>http://arxiv.org/abs/2401.17042v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a probabilistic deep learning model for volatility index prediction using TCN, Transformers, and uncertainty calibration methods, achieving better performance than traditional models.</p><hr><h3>Taking Action Towards Graceful Interaction: The Effects of Performing  Actions on Modelling Policies for Instruction Clarification Requests</h3>
<p><a href='http://arxiv.org/abs/2401.17039v1'>http://arxiv.org/abs/2401.17039v1</a></p>
<p><b>Compressor summary</b>: This study explores how action taking as a side task affects learning to ask clarification requests in instruction-following interactions and finds that it has limited impact, but uncertainty in predictions can help.</p><hr><h3>Bayesian Optimization with Noise-Free Observations: Improved Regret  Bounds via Random Exploration</h3>
<p><a href='http://arxiv.org/abs/2401.17037v1'>http://arxiv.org/abs/2401.17037v1</a></p>
<p><b>Compressor summary</b>: The paper presents new Bayesian optimization algorithms that use scattered data approximation and random exploration to improve query point distribution and regret bounds, while being easy to implement and performing better than existing methods.</p><hr><h3>Intrinsic Data Constraints and Upper Bounds in Binary Classification  Performance</h3>
<p><a href='http://arxiv.org/abs/2401.17036v1'>http://arxiv.org/abs/2401.17036v1</a></p>
<p><b>Compressor summary</b>: The structure of data organization affects machine learning algorithms' performance, which can reach its theoretical maximum depending on the dataset's characteristics and class overlap.</p><hr><h3>Robust Kernel Sparse Subspace Clustering</h3>
<p><a href='http://arxiv.org/abs/2401.17035v1'>http://arxiv.org/abs/2401.17035v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a robust kernel sparse subspace clustering algorithm for data with gross sparse corruptions and shows its improved performance compared to a linear robust algorithm.</p><hr><h3>Multilayer Graph Approach to Deep Subspace Clustering</h3>
<p><a href='http://arxiv.org/abs/2401.17033v1'>http://arxiv.org/abs/2401.17033v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve deep subspace clustering by using information from multiple layers of the encoder network and integrating them with a multilayer graph, leading to better performance on four datasets.</p><hr><h3>Heterogeneous treatment effect estimation with subpopulation  identification for personalized medicine in opioid use disorder</h3>
<p><a href='http://arxiv.org/abs/2401.17027v1'>http://arxiv.org/abs/2401.17027v1</a></p>
<p><b>Compressor summary</b>: The text introduces SubgroupTE, a neural network framework that estimates treatment effects for diverse subgroups, improving personalized recommendations for conditions like opioid use disorder.</p><hr><h3>Static and Dynamic Synthesis of Bengali and Devanagari Signatures</h3>
<p><a href='http://arxiv.org/abs/2401.17026v1'>http://arxiv.org/abs/2401.17026v1</a></p>
<p><b>Compressor summary</b>: The text describes a method to create synthetic handwriting in Indic scripts using a motor equivalence model and evaluates its effectiveness.</p><hr><h3>MF-MOS: A Motion-Focused Model for Moving Object Segmentation</h3>
<p><a href='http://arxiv.org/abs/2401.17023v1'>http://arxiv.org/abs/2401.17023v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel motion-focused model for LiDAR moving object segmentation that uses both range images and residual maps, achieving state-of-the-art performance on the SemanticKITTI dataset.</p><hr><h3>Evaluation of Out-of-Distribution Detection Performance on Autonomous  Driving Datasets</h3>
<p><a href='http://arxiv.org/abs/2401.17013v1'>http://arxiv.org/abs/2401.17013v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to measure the risk of Deep Neural Networks (DNNs) producing incorrect outputs, by using a Mahalanobis distance-based score that can reduce classification risk at the cost of pixel coverage.</p><hr><h3>Category-wise Fine-Tuning: Resisting Incorrect Pseudo-Labels in  Multi-Label Image Classification with Partial Labels</h3>
<p><a href='http://arxiv.org/abs/2401.16991v1'>http://arxiv.org/abs/2401.16991v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Category-wise Fine-Tuning (CFT), a method that calibrates model predictions for partially labeled image datasets by fine-tuning logistic regressions using known labels and genetic algorithm, achieving state-of-the-art results on three benchmarks.</p><hr><h3>CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement  Learning</h3>
<p><a href='http://arxiv.org/abs/2401.16974v1'>http://arxiv.org/abs/2401.16974v1</a></p>
<p><b>Compressor summary</b>: CORE is a deep reinforcement learning method for discovering causal structures and performing interventions on graphs with up to 10 variables, which outperforms existing approaches in accuracy and efficiency.</p><hr><h3>Deep 3D World Models for Multi-Image Super-Resolution Beyond Optical  Flow</h3>
<p><a href='http://arxiv.org/abs/2401.16972v1'>http://arxiv.org/abs/2401.16972v1</a></p>
<p><b>Compressor summary</b>: EpiMISR is a multi-image super-resolution method that uses epipolar geometry and transformer networks to increase resolution from images with arbitrary camera positions and orientations, outperforming existing methods.</p><hr><h3>Distinguishing Fictional Voices: a Study of Authorship Verification  Models for Quotation Attribution</h3>
<p><a href='http://arxiv.org/abs/2401.16968v1'>http://arxiv.org/abs/2401.16968v1</a></p>
<p><b>Compressor summary</b>: The paper investigates using pretrained Authorship Verification models to identify speakers in English novels based on stylistic and topical information, but finds that it is not always more accurate than semantic-only models.</p><hr><h3>Two Heads Are Better Than One: Integrating Knowledge from Knowledge  Graphs and Large Language Models for Entity Alignment</h3>
<p><a href='http://arxiv.org/abs/2401.16960v1'>http://arxiv.org/abs/2401.16960v1</a></p>
<p><b>Compressor summary</b>: The study proposes a new entity alignment method that combines knowledge graph embeddings with large language model inference to find equivalent entities across different knowledge graphs.</p><hr><h3>Online Resource Allocation with Non-Stationary Customers</h3>
<p><a href='http://arxiv.org/abs/2401.16945v1'>http://arxiv.org/abs/2401.16945v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Novel algorithm for online resource allocation with non-stationary customer arrivals and unknown click-through rates
- Leverages results from stochastic contextual bandit with knapsack and online matching with adversarial arrivals
- Achieves sublinear regret under near-stationary customer arrivals and optimal competitive ratio under general customer arrival distributions
- Numerical experiments show near-optimal revenues for all scenarios

Summary:
The algorithm combines two techniques to allocate resources online efficiently in the presence of non-stationary customers and unknown click-through rates, and demonstrates its effectiveness through numerical experiments.</p><hr><h3>Segmentation and Characterization of Macerated Fibers and Vessels Using  Deep Learning</h3>
<p><a href='http://arxiv.org/abs/2401.16937v1'>http://arxiv.org/abs/2401.16937v1</a></p>
<p><b>Compressor summary</b>: The paper presents a fast and accurate deep learning method for segmenting and characterizing wood cell types in microscopy images, using the YOLOv8 model and providing a user-friendly web application.</p><hr><h3>Multi-modal Representation Learning for Cross-modal Prediction of  Continuous Weather Patterns from Discrete Low-Dimensional Data</h3>
<p><a href='http://arxiv.org/abs/2401.16936v1'>http://arxiv.org/abs/2401.16936v1</a></p>
<p><b>Compressor summary</b>: The text discusses the importance of wind energy for reducing greenhouse gas emissions and proposes a deep learning method to improve wind data analysis by addressing three challenges: data resolution, dimensionality reduction, and extrapolation.</p><hr><h3>Fourier Prompt Tuning for Modality-Incomplete Scene Segmentation</h3>
<p><a href='http://arxiv.org/abs/2401.16923v1'>http://arxiv.org/abs/2401.16923v1</a></p>
<p><b>Compressor summary</b>: The paper introduces MISS, a task for studying modality incompleteness in multi-modal segmentation, and proposes MMS and FPT methods to improve robustness against missing modalities.</p><hr><h3>Energy-conserving equivariant GNN for elasticity of lattice architected  metamaterials</h3>
<p><a href='http://arxiv.org/abs/2401.16914v1'>http://arxiv.org/abs/2401.16914v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a higher-order graph neural network model that predicts the stiffness of periodic lattices, with improved accuracy and efficiency compared to traditional methods.</p><hr><h3>Cross-Lingual Transfer from Related Languages: Treating Low-Resource  Maltese as Multilingual Code-Switching</h3>
<p><a href='http://arxiv.org/abs/2401.16895v1'>http://arxiv.org/abs/2401.16895v1</a></p>
<p><b>Compressor summary</b>: The authors propose a method to improve cross-lingual transfer for Maltese by selectively transliterating words based on their etymology and present results on four downstream tasks.</p><hr><h3>CAFCT: Contextual and Attentional Feature Fusions of Convolutional  Neural Networks and Transformer for Liver Tumor Segmentation</h3>
<p><a href='http://arxiv.org/abs/2401.16886v1'>http://arxiv.org/abs/2401.16886v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes a new model for liver tumor segmentation using CNN and Transformer
- The model has three modules to improve contextual information: AFF, ASPP, and AGs
- The model achieves high IoU and Dice scores on the LiTS dataset

Summary:
The authors present a novel hybrid CNN and Transformer model with three context-enhancing modules for liver tumor segmentation, which significantly improves accuracy on the LiTS dataset.</p><hr><h3>Zero-shot Classification using Hyperdimensional Computing</h3>
<p><a href='http://arxiv.org/abs/2401.16876v1'>http://arxiv.org/abs/2401.16876v1</a></p>
<p><b>Compressor summary</b>: The HDC-ZSC model uses symbol-like representations and attribute encoders to perform zero-shot learning and classification tasks with high accuracy and fewer parameters.</p><hr><h3>A Tournament of Transformation Models: B-Spline-based vs. Mesh-based  Multi-Objective Deformable Image Registration</h3>
<p><a href='http://arxiv.org/abs/2401.16867v1'>http://arxiv.org/abs/2401.16867v1</a></p>
<p><b>Compressor summary</b>: The paper compares B-spline and mesh transformation models for deformable image registration using a multi-objective optimization method and shows their impact on registration outcomes in cervical cancer patients.</p><hr><h3>State Value Generation with Prompt Learning and Self-Training for  Low-Resource Dialogue State Tracking</h3>
<p><a href='http://arxiv.org/abs/2401.16862v1'>http://arxiv.org/abs/2401.16862v1</a></p>
<p><b>Compressor summary</b>: SVAG is a novel framework for low-resource dialogue state tracking that generates state values and domain slot types using self-training and an estimator to improve performance and generalization.</p><hr><h3>Repositioning the Subject within Image</h3>
<p><a href='http://arxiv.org/abs/2401.16861v1'>http://arxiv.org/abs/2401.16861v1</a></p>
<p><b>Compressor summary</b>: The paper introduces SEELE, a framework for dynamic image manipulation using diffusion generative models and task inversion techniques, applied to the novel task of subject repositioning.</p><hr><h3>Checkmating One, by Using Many: Combining Mixture of Experts with MCTS  to Improve in Chess</h3>
<p><a href='http://arxiv.org/abs/2401.16852v1'>http://arxiv.org/abs/2401.16852v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new deep learning chess method that uses a combination of specialized models, MoE, and MCTS to improve playing strength and align with strategic phases of chess.</p><hr><h3>Evaluating ML-Based Anomaly Detection Across Datasets of Varied  Integrity: A Case Study</h3>
<p><a href='http://arxiv.org/abs/2401.16843v1'>http://arxiv.org/abs/2401.16843v1</a></p>
<p><b>Compressor summary</b>: This paper introduces refined datasets for network traffic anomaly detection and evaluates the performance of a Random Forest algorithm across different datasets, finding it robust against data integrity issues.</p><hr><h3>Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition</h3>
<p><a href='http://arxiv.org/abs/2401.16836v1'>http://arxiv.org/abs/2401.16836v1</a></p>
<p><b>Compressor summary</b>: Coseparable Nonnegative Tensor Factorization (NTF) extends coseparable NMF to tensors, preserving multi-dimensional correlations in high-dimensional data and offering a more efficient core representation using alternating index selection methods.</p><hr><h3>EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor  Image Comprehension in Remote Sensing Domain</h3>
<p><a href='http://arxiv.org/abs/2401.16822v1'>http://arxiv.org/abs/2401.16822v1</a></p>
<p><b>Compressor summary</b>: EarthGPT is a new large language model that can interpret various remote sensing (RS) images and perform well in different RS tasks using a large dataset called MMRS.</p><hr><h3>H2O-Danube-1.8B Technical Report</h3>
<p><a href='http://arxiv.org/abs/2401.16818v1'>http://arxiv.org/abs/2401.16818v1</a></p>
<p><b>Compressor summary</b>: H2O-Danube-1.8B is a large language model with strong performance on various benchmarks, released under an open license.</p><hr><h3>Reviving Undersampling for Long-Tailed Learning</h3>
<p><a href='http://arxiv.org/abs/2401.16811v1'>http://arxiv.org/abs/2401.16811v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using balanced undersampling and model ensembling to improve the performance of long-tailed recognition, focusing on harmonic and geometric mean accuracy rather than average accuracy.</p><hr><h3>An Embeddable Implicit IUVD Representation for Part-based 3D Human  Surface Reconstruction</h3>
<p><a href='http://arxiv.org/abs/2401.16810v1'>http://arxiv.org/abs/2401.16810v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new 3D human reconstruction method that uses a combination of parametric body models and neural implicit functions, improving accuracy, speed, and robustness.</p><hr><h3>Encoding Temporal Statistical-space Priors via Augmented Representation</h3>
<p><a href='http://arxiv.org/abs/2401.16808v1'>http://arxiv.org/abs/2401.16808v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes SSAR, a technique to improve time series forecasting by augmenting the representation with statistical prior
- SSAR outperforms five baselines on two data sets using different algorithms
- SSAR is modular and easy to apply

Summary:
The paper introduces SSAR, a simple but effective method that enhances time series forecasting by adding statistical prior to the representation. SSAR beats five competitors on various settings and is flexible to use.</p><hr><h3>Online Algorithm for Node Feature Forecasting in Temporal Graphs</h3>
<p><a href='http://arxiv.org/abs/2401.16800v1'>http://arxiv.org/abs/2401.16800v1</a></p>
<p><b>Compressor summary</b>: The paper introduces an online algorithm called "mspace" that accurately predicts node features in temporal graphs, outperforming existing methods especially when training data is limited.</p><hr><h3>Learnable Prompt as Pseudo-Imputation: Reassessing the Necessity of  Traditional EHR Data Imputation in Downstream Clinical Prediction</h3>
<p><a href='http://arxiv.org/abs/2401.16796v1'>http://arxiv.org/abs/2401.16796v1</a></p>
<p><b>Compressor summary</b>: The paper proposes PAI, a new training protocol that uses learnable prompts to model missing values in EHR without injecting imputed data, improving performance and robustness of EHR analysis models.</p><hr><h3>Performance Insights-based AI-driven Football Transfer Fee Prediction</h3>
<p><a href='http://arxiv.org/abs/2401.16795v1'>http://arxiv.org/abs/2401.16795v1</a></p>
<p><b>Compressor summary</b>: The text describes an AI-based model that predicts the transfer fees of football players, helping clubs to optimize their player acquisition and retention strategies.</p><hr><h3>Accelerated Cloud for Artificial Intelligence (ACAI)</h3>
<p><a href='http://arxiv.org/abs/2401.16791v1'>http://arxiv.org/abs/2401.16791v1</a></p>
<p><b>Compressor summary</b>: ACAI is a cloud-based platform that automates ML workflows, improves productivity, and reduces costs and experiment time by providing data storage, resource provisioning, job scheduling, and experiment tracking.</p><hr><h3>Can Large Language Models be Trusted for Evaluation? Scalable  Meta-Evaluation of LLMs as Evaluators via Agent Debate</h3>
<p><a href='http://arxiv.org/abs/2401.16788v1'>http://arxiv.org/abs/2401.16788v1</a></p>
<p><b>Compressor summary</b>: ScaleEval is a framework that uses multiple LLM agents to help human annotators evaluate other LLMs effectively and efficiently in various tasks and scenarios.</p><hr><h3>Enhancing Efficiency and Robustness in Support Vector Regression with  HawkEye Loss</h3>
<p><a href='http://arxiv.org/abs/2401.16785v1'>http://arxiv.org/abs/2401.16785v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new loss function for support vector regression that is bounded, smooth, and insensitive, improving its robustness and performance on various datasets.</p><hr><h3>Graph Fairness Learning under Distribution Shifts</h3>
<p><a href='http://arxiv.org/abs/2401.16784v1'>http://arxiv.org/abs/2401.16784v1</a></p>
<p><b>Compressor summary</b>: Key points:
- GNNs may be biased and discriminatory based on sensitive attributes
- Graph fairness learning assumes same distribution for training and testing data
- Theoretical analysis identifies factors that influence bias and fairness on graphs
- FatraGNN framework uses graph generator to create diverse and biased graphs for training and minimizes representation distances between groups
- Experiments show improved accuracy and fairness performance of FatraGNN

Summary:
The paper proposes FatraGNN, a framework that ensures fairness on GNNs under distribution shifts by using a graph generator to create diverse and biased graphs for training and reducing group representation distances.</p><hr><h3>Addressing Distribution Shift in Time Series Forecasting with Instance  Normalization Flows</h3>
<p><a href='http://arxiv.org/abs/2401.16777v1'>http://arxiv.org/abs/2401.16777v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new method for time series forecasting that handles distribution shifts, works with various models, and uses a novel invertible network for transformation.</p><hr><h3>Activity Detection for Massive Connectivity in Cell-free Networks with  Unknown Large-scale Fading, Channel Statistics, Noise Variance, and Activity  Probability: A Bayesian Approach</h3>
<p><a href='http://arxiv.org/abs/2401.16775v1'>http://arxiv.org/abs/2401.16775v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Bayesian methods for activity detection in cell-free networks without requiring precise network information, outperforming existing approaches.</p><hr><h3>Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator</h3>
<p><a href='http://arxiv.org/abs/2401.16772v1'>http://arxiv.org/abs/2401.16772v1</a></p>
<p><b>Compressor summary</b>: Discriminator Soft Q Imitation Learning (DSQIL) is a method that combines soft Q-learning with adversarial inverse reinforcement learning to improve imitation learning efficiency and robustness in unknown states.</p><hr><h3>MolPLA: A Molecular Pretraining Framework for Learning Cores, R-Groups  and their Linker Joints</h3>
<p><a href='http://arxiv.org/abs/2401.16771v1'>http://arxiv.org/abs/2401.16771v1</a></p>
<p><b>Compressor summary</b>: MolPLA is a new framework that uses graph contrastive learning to understand molecular structures and help chemists find better R-groups for drug development.</p><hr><h3>Detection and Recovery Against Deep Neural Network Fault Injection  Attacks Based on Contrastive Learning</h3>
<p><a href='http://arxiv.org/abs/2401.16766v1'>http://arxiv.org/abs/2401.16766v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-supervised learning approach to make DNNs more resilient to fault injection attacks during inference.</p><hr><h3>BoostDream: Efficient Refining for High-Quality Text-to-3D Generation  from Multi-View Diffusion</h3>
<p><a href='http://arxiv.org/abs/2401.16764v1'>http://arxiv.org/abs/2401.16764v1</a></p>
<p><b>Compressor summary</b>: BoostDream is a fast and effective method for improving the quality of 3D models generated by text-to-3D approaches, combining 3D model distillation, multi-view SDS loss, and prompt-based guidance.</p><hr><h3>Pick-and-Draw: Training-free Semantic Guidance for Text-to-Image  Personalization</h3>
<p><a href='http://arxiv.org/abs/2401.16762v1'>http://arxiv.org/abs/2401.16762v1</a></p>
<p><b>Compressor summary</b>: Pick-and-Draw is a training-free method to improve text-to-image personalization by using appearance and layout guidance from reference images, enhancing identity consistency and diversity.</p><hr><h3>One-Step Forward and Backtrack: Overcoming Zig-Zagging in Loss-Aware  Quantization Training</h3>
<p><a href='http://arxiv.org/abs/2401.16760v1'>http://arxiv.org/abs/2401.16760v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a loss-aware quantization method that uses a one-step forward and backtrack approach to find more accurate and stable gradient directions for faster model convergence on edge devices.</p><hr><h3>SwapNet: Efficient Swapping for DNN Inference on Edge AI Devices Beyond  the Memory Budget</h3>
<p><a href='http://arxiv.org/abs/2401.16757v1'>http://arxiv.org/abs/2401.16757v1</a></p>
<p><b>Compressor summary</b>: SwapNet is a middleware that efficiently swaps DNN blocks to run large models on edge AI devices with limited memory, reducing latency and maintaining accuracy.</p><hr><h3>Diffusion model for relational inference</h3>
<p><a href='http://arxiv.org/abs/2401.16755v1'>http://arxiv.org/abs/2401.16755v1</a></p>
<p><b>Compressor summary</b>: The Diffusion model for Relational Inference (DiffRI) is a new method that can learn and discover hidden interactions between components of complex systems using observable dynamics, without any supervision.</p><hr><h3>AI Oversight and Human Mistakes: Evidence from Centre Court</h3>
<p><a href='http://arxiv.org/abs/2401.16754v1'>http://arxiv.org/abs/2401.16754v1</a></p>
<p><b>Compressor summary</b>: The introduction of Hawk-Eye review in tennis increased the mistake rate of umpires due to psychological costs of being overruled by AI.</p><hr><h3>MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with  Mutual Scoring of the Unlabeled Images</h3>
<p><a href='http://arxiv.org/abs/2401.16753v1'>http://arxiv.org/abs/2401.16753v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel zero-shot anomaly classification and segmentation method for industrial vision, using mutual scoring of unlabeled test images to exploit cues for anomaly determination without training or prompts.</p><hr><h3>Detecting Racist Text in Bengali: An Ensemble Deep Learning Framework</h3>
<p><a href='http://arxiv.org/abs/2401.16748v1'>http://arxiv.org/abs/2401.16748v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper aims to detect racist comments in Bengali using NLP and deep learning techniques
- They built a novel dataset, annotated it, and validated it
- They achieved an accuracy rate of 87.94% with the Ensemble approach
- They used RNN, LSTM, BERT Embeddings, and MCNN-LSTM models

Summary:
The paper presents a Bengali racist comment detection system using NLP and deep learning, achieving an accuracy of 87.94% with a novel dataset and Ensemble approach.</p><hr><h3>MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2401.16745v1'>http://arxiv.org/abs/2401.16745v1</a></p>
<p><b>Compressor summary</b>: MT-Eval is a benchmark that evaluates large language models' ability to handle complex multi-turn conversations by categorizing interaction patterns and analyzing their strengths and weaknesses.</p><hr><h3>ShaRP: Explaining Rankings with Shapley Values</h3>
<p><a href='http://arxiv.org/abs/2401.16744v1'>http://arxiv.org/abs/2401.16744v1</a></p>
<p><b>Compressor summary</b>: ShaRP is a framework that explains how features contribute to different aspects of ranked outcomes using Shapley values, and can be applied to both score-based and learned ranking models.</p><hr><h3>MESA: Matching Everything by Segmenting Anything</h3>
<p><a href='http://arxiv.org/abs/2401.16741v1'>http://arxiv.org/abs/2401.16741v1</a></p>
<p><b>Compressor summary</b>: MESA is a new method to reduce matching redundancy in computer vision by using SAM's image segmentation and a multi-relational graph to find precise area matches.</p><hr><h3>Engineering A Large Language Model From Scratch</h3>
<p><a href='http://arxiv.org/abs/2401.16736v1'>http://arxiv.org/abs/2401.16736v1</a></p>
<p><b>Compressor summary</b>: Atinuke is a Transformer-based neural network that uses attention mechanisms and advanced matrix operations to perform well on various natural language tasks while maintaining interpretability and robustness.</p><hr><h3>Towards Generating Informative Textual Description for Neurons in  Language Models</h3>
<p><a href='http://arxiv.org/abs/2401.16731v1'>http://arxiv.org/abs/2401.16731v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework to generate human-interpretable textual descriptions for neurons in transformer-based language models using generative language models and an unsupervised approach.</p><hr><h3>Widely Linear Matched Filter: A Lynchpin towards the Interpretability of  Complex-valued CNNs</h3>
<p><a href='http://arxiv.org/abs/2401.16729v1'>http://arxiv.org/abs/2401.16729v1</a></p>
<p><b>Compressor summary</b>: The study introduces a new paradigm for interpreting complex-valued CNNs using matched filtering, showing improved performance and physical meaning compared to standard methods.</p><hr><h3>Recent Advances in Hate Speech Moderation: Multimodality and the Role of  Large Models</h3>
<p><a href='http://arxiv.org/abs/2401.16727v1'>http://arxiv.org/abs/2401.16727v1</a></p>
<p><b>Compressor summary</b>: The text surveys recent advances in hate speech moderation using large language models and multimodal models, while highlighting gaps and challenges in underrepresented languages and contexts.</p><hr><h3>Optimal-Landmark-Guided Image Blending for Face Morphing Attacks</h3>
<p><a href='http://arxiv.org/abs/2401.16722v1'>http://arxiv.org/abs/2401.16722v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new face morphing attack using optimal-landmark-guided image blending and Graph Convolutional Networks to create realistic and effective morphed images that evade face recognition systems.</p><hr><h3>SmartFRZ: An Efficient Training Framework using Attention-Based Layer  Freezing</h3>
<p><a href='http://arxiv.org/abs/2401.16720v1'>http://arxiv.org/abs/2401.16720v1</a></p>
<p><b>Compressor summary</b>: SmartFRZ is a generic and efficient training framework for AI models that uses attention-guided layer freezing to reduce computation and achieve training acceleration without compromising accuracy.</p><hr><h3>LF Tracy: A Unified Single-Pipeline Approach for Salient Object  Detection in Light Field Cameras</h3>
<p><a href='http://arxiv.org/abs/2401.16712v1'>http://arxiv.org/abs/2401.16712v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method called LF Tracy that uses light field cameras and a single pipeline to improve salient object detection, outperforming existing methods with fewer parameters.</p><hr><h3>Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible  Cluster Shapes</h3>
<p><a href='http://arxiv.org/abs/2401.16708v1'>http://arxiv.org/abs/2401.16708v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new model for soft clustering called multivariate beta mixture model, which can adapt to different cluster shapes using a flexible probability density function and shows its effectiveness on synthetic and real datasets.</p><hr><h3>Multi-granularity Correspondence Learning from Long-term Noisy Videos</h3>
<p><a href='http://arxiv.org/abs/2401.16702v1'>http://arxiv.org/abs/2401.16702v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Norton, a method that uses optimal transport to address misalignment between video clips and captions, improving temporal learning and video understanding.</p><hr><h3>Towards Precise 3D Human Pose Estimation with Multi-Perspective  Spatial-Temporal Relational Transformers</h3>
<p><a href='http://arxiv.org/abs/2401.16700v1'>http://arxiv.org/abs/2401.16700v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a multi-stage framework for 3D human pose estimation using transformers and self-attention to capture spatial-temporal correlations from multi-view video data, achieving state-of-the-art results on the Human3.6M dataset.</p><hr><h3>EdgeOL: Efficient in-situ Online Learning on Edge Devices</h3>
<p><a href='http://arxiv.org/abs/2401.16694v1'>http://arxiv.org/abs/2401.16694v1</a></p>
<p><b>Compressor summary</b>: EdgeOL is an edge learning framework that optimizes inference accuracy, fine-tuning speed, and energy efficiency for DNNs in applications like robot-assisted eldercare and object recognition.</p><hr><h3>Calibration-then-Calculation: A Variance Reduced Metric Framework in  Deep Click-Through Rate Prediction Models</h3>
<p><a href='http://arxiv.org/abs/2401.16692v1'>http://arxiv.org/abs/2401.16692v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Calibrated Loss Metric, a new framework that reduces variance in neural network evaluation metrics and improves accuracy in detecting effective modeling improvement.</p><hr><h3>Characterization of Magnetic Labyrinthine Structures through Junctions  and Terminals Detection using Template Matching and CNN</h3>
<p><a href='http://arxiv.org/abs/2401.16688v1'>http://arxiv.org/abs/2401.16688v1</a></p>
<p><b>Compressor summary</b>: The study introduces TM-CNN, a novel technique that combines template matching and convolutional neural networks to accurately detect defects in magnetic labyrinthine patterns, overcoming the limitations of previous methods.</p><hr><h3>The Detection and Understanding of Fictional Discourse</h3>
<p><a href='http://arxiv.org/abs/2401.16678v1'>http://arxiv.org/abs/2401.16678v1</a></p>
<p><b>Compressor summary</b>: The paper explores different datasets for detecting fiction and introduces new features to generalize semantics, aiming to enhance cultural archive knowledge and understand fictional storytelling.</p><hr><h3>Is Artificial Intelligence Providing the Second Revolution for Weather  Forecasting?</h3>
<p><a href='http://arxiv.org/abs/2401.16669v1'>http://arxiv.org/abs/2401.16669v1</a></p>
<p><b>Compressor summary</b>: This study proposes "Three Large Rules" for developing large artificial intelligence weather forecast models, which can revolutionize numerical weather prediction by integrating with traditional models.</p><hr><h3>Fast Dual-Regularized Autoencoder for Sparse Biological Data</h3>
<p><a href='http://arxiv.org/abs/2401.16664v1'>http://arxiv.org/abs/2401.16664v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a fast and accurate shallow autoencoder for predicting drug-target interactions and drug-disease associations using sparse matrix completion.</p><hr><h3>Generalization of LiNGAM that allows confounding</h3>
<p><a href='http://arxiv.org/abs/2401.16661v1'>http://arxiv.org/abs/2401.16661v1</a></p>
<p><b>Compressor summary</b>: LiNGAM-MMI is a method that improves LiNGAM by quantifying and minimizing the effect of confounding on variable order determination using KL divergence and the shortest path problem.</p><hr><h3>Recovering Mental Representations from Large Language Models with Markov  Chain Monte Carlo</h3>
<p><a href='http://arxiv.org/abs/2401.16657v1'>http://arxiv.org/abs/2401.16657v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using Large Language Models as elements of a sampling algorithm to study their mental representations, finding increased efficiency and performance compared to direct prompting.</p><hr><h3>OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on  E-Branchformer</h3>
<p><a href='http://arxiv.org/abs/2401.16658v1'>http://arxiv.org/abs/2401.16658v1</a></p>
<p><b>Compressor summary</b>: The authors improved the performance and efficiency of an open speech model by using a new architecture and released their work for public use.</p><hr><h3>Gradient-Based Language Model Red Teaming</h3>
<p><a href='http://arxiv.org/abs/2401.16656v1'>http://arxiv.org/abs/2401.16656v1</a></p>
<p><b>Compressor summary</b>: GBRT is a method for automatically generating diverse, coherent prompts that can find unsafe responses in generative language models, outperforming reinforcement learning-based red teaming.</p><hr><h3>Augmenting Replay in World Models for Continual Reinforcement Learning</h3>
<p><a href='http://arxiv.org/abs/2401.16650v1'>http://arxiv.org/abs/2401.16650v1</a></p>
<p><b>Compressor summary</b>: Our method improves continual RL by using less memory, but may sometimes struggle with learning new tasks.</p><hr><h3>Using Motion Forecasting for Behavior-Based Virtual Reality (VR)  Authentication</h3>
<p><a href='http://arxiv.org/abs/2401.16649v1'>http://arxiv.org/abs/2401.16649v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new approach that uses Transformer-based forecasting to predict future user behavior in VR environments, improving task-based behavioral biometric authentication.</p><hr><h3>Incoherent Probability Judgments in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2401.16646v1'>http://arxiv.org/abs/2401.16646v1</a></p>
<p><b>Compressor summary</b>: The text discusses how large language models can make incoherent probability judgments, similar to humans, due to their connection to implicit Bayesian inference.</p><hr><h3>Speeding up and reducing memory usage for scientific machine learning  via mixed precision</h3>
<p><a href='http://arxiv.org/abs/2401.16645v1'>http://arxiv.org/abs/2401.16645v1</a></p>
<p><b>Compressor summary</b>: Mixed precision training improves the efficiency of physics-informed neural networks without sacrificing accuracy in solving complex problems.</p><hr><h3>TeenyTinyLlama: open-source tiny language models trained in Brazilian  Portuguese</h3>
<p><a href='http://arxiv.org/abs/2401.16640v1'>http://arxiv.org/abs/2401.16640v1</a></p>
<p><b>Compressor summary</b>: The study introduces TeenyTinyLlama, two small language models for Brazilian Portuguese text generation, to address limitations of large language models in low-resource settings.</p><hr><h3>Breaking Free Transformer Models: Task-specific Context Attribution  Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs</h3>
<p><a href='http://arxiv.org/abs/2401.16638v1'>http://arxiv.org/abs/2401.16638v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to enhance NLP classification tasks by using task-specific context attribution, which improves generalizability and performance on three datasets.</p><hr><h3>Improving Reinforcement Learning from Human Feedback with Efficient  Reward Model Ensemble</h3>
<p><a href='http://arxiv.org/abs/2401.16635v1'>http://arxiv.org/abs/2401.16635v1</a></p>
<p><b>Compressor summary</b>: RLHF may produce outputs misaligned with human values due to inaccurate reward model predictions, but a reward ensemble method can improve its alignment performance.</p><hr><h3>The Why, When, and How to Use Active Learning in Large-Data-Driven 3D  Object Detection for Safe Autonomous Driving: An Empirical Exploration</h3>
<p><a href='http://arxiv.org/abs/2401.16634v1'>http://arxiv.org/abs/2401.16634v1</a></p>
<p><b>Compressor summary</b>: Entropy querying helps select informative 3D object detection samples, reducing annotation costs and improving model performance in autonomous driving datasets.</p>