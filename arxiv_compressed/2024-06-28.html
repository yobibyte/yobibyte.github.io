
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>arxiv compressed, 2024-06-28</h1>
            <p>This page contains one-sentence summaries of cs.AI/ML/CV/CL papers announced on 2024-06-28 generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Dataset Size Recovery from LoRA Weights</h3>
<p><a href='http://arxiv.org/abs/2406.19395v1'>http://arxiv.org/abs/2406.19395v1</a></p>
<p><b>Compressor summary</b>: The paper introduces dataset size recovery, a method to estimate how many samples were used to train a model, using its weights and LoRA matrices.</p><hr><h3>HUWSOD: Holistic Self-training for Unified Weakly Supervised Object  Detection</h3>
<p><a href='http://arxiv.org/abs/2406.19394v1'>http://arxiv.org/abs/2406.19394v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a self-training framework called HUWSOD for weakly supervised object detection, which uses innovative proposal generators and does not require external modules or additional supervision.</p><hr><h3>Looking 3D: Anomaly Detection with 2D-3D Alignment</h3>
<p><a href='http://arxiv.org/abs/2406.19393v1'>http://arxiv.org/abs/2406.19393v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new anomaly detection problem using 3D shapes and a large dataset of images with diverse anomalies, and proposes a transformer-based approach to solve it.</p><hr><h3>ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos</h3>
<p><a href='http://arxiv.org/abs/2406.19392v1'>http://arxiv.org/abs/2406.19392v1</a></p>
<p><b>Compressor summary</b>: ReXTime is a benchmark to test AI models' ability to reason about cause-and-effect relationships across video segments, and it shows that current models are not yet as good as humans at this task.</p><hr><h3>Fibottention: Inceptive Visual Representation Learning with Diverse  Attention Across Heads</h3>
<p><a href='http://arxiv.org/abs/2406.19391v1'>http://arxiv.org/abs/2406.19391v1</a></p>
<p><b>Compressor summary</b>: Fibottention is a sparse, efficient, and general self-attention architecture based on Fibonacci sequences for visual tasks that captures fine-grained details while reducing computational overhead.</p><hr><h3>SALVe: Semantic Alignment Verification for Floorplan Reconstruction from  Sparse Panoramas</h3>
<p><a href='http://arxiv.org/abs/2406.19390v1'>http://arxiv.org/abs/2406.19390v1</a></p>
<p><b>Compressor summary</b>: Key points:
- New system for automatic 2D floorplan reconstruction using SALVe, a novel learned alignment verifier
- Inputs: sparse 360° panoramas with semantic features of windows, doors, and openings
- Outputs: room poses, layouts, and floorplan
- Outperforms state-of-the-art SfM systems in completeness and accuracy

Summary:
The authors present a new system that uses SALVe, a learned alignment verifier, to reconstruct 2D floorplans from sparse 360° panoramas with semantic features, achieving better results than existing methods.</p><hr><h3>OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and  Understanding</h3>
<p><a href='http://arxiv.org/abs/2406.19389v1'>http://arxiv.org/abs/2406.19389v1</a></p>
<p><b>Compressor summary</b>: OMG-LLaVA is a framework that combines pixel-level image understanding with reasoning abilities, enabling flexible user interaction via visual and text prompts.</p><hr><h3>The Remarkable Robustness of LLMs: Stages of Inference?</h3>
<p><a href='http://arxiv.org/abs/2406.19384v1'>http://arxiv.org/abs/2406.19384v1</a></p>
<p><b>Compressor summary</b>: The text shows that Large Language Models are very robust and can still predict well even after layers are deleted or swapped, and suggests that there are four stages of inference across different models.</p><hr><h3>TabReD: A Benchmark of Tabular Machine Learning in-the-Wild</h3>
<p><a href='http://arxiv.org/abs/2406.19380v1'>http://arxiv.org/abs/2406.19380v1</a></p>
<p><b>Compressor summary</b>: TabReD is a new collection of tabular machine learning benchmarks that reflect real-world scenarios by including time-based splits and feature engineering.</p><hr><h3>Suri: Multi-constraint Instruction Following for Long-form Text  Generation</h3>
<p><a href='http://arxiv.org/abs/2406.19371v1'>http://arxiv.org/abs/2406.19371v1</a></p>
<p><b>Compressor summary</b>: This paper introduces Suri, a dataset for multi-constraint instruction following in long-form text generation, and proposes Instructional ORPO (I-ORPO), an alignment method that uses synthetic negative feedback from LLM-generated corrupted instructions to improve quality.</p><hr><h3>Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept  Space</h3>
<p><a href='http://arxiv.org/abs/2406.19370v1'>http://arxiv.org/abs/2406.19370v1</a></p>
<p><b>Compressor summary</b>: The authors propose analyzing how generative models learn abstract concepts using a concept space framework, finding that hidden capabilities emerge suddenly in the learning process.</p><hr><h3>Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment  Anything Model</h3>
<p><a href='http://arxiv.org/abs/2406.19369v1'>http://arxiv.org/abs/2406.19369v1</a></p>
<p><b>Compressor summary</b>: RWKV-SAM is a fast and accurate segmentation model that combines convolution and radial wave kernel correlation (RWKV) operations in its backbone and an efficient decoder for multiscale tokens.</p><hr><h3>SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text  Cues</h3>
<p><a href='http://arxiv.org/abs/2406.19364v1'>http://arxiv.org/abs/2406.19364v1</a></p>
<p><b>Compressor summary</b>: SimTxtSeg is a novel framework that uses simple text cues to generate pseudo-labels and fuse text and image features for weakly-supervised medical image segmentation.</p><hr><h3>STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via  Collaborating Self-Training and Adversarial Learning</h3>
<p><a href='http://arxiv.org/abs/2406.19362v1'>http://arxiv.org/abs/2406.19362v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new framework, STAL3D, that combines self-training and adversarial learning for unsupervised domain adaptation in 3D object detection, improving performance on cross-domain tasks and addressing issues like background interference and source domain size bias.</p><hr><h3>The Model Arena for Cross-lingual Sentiment Analysis: A Comparative  Study in the Era of Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.19358v1'>http://arxiv.org/abs/2406.19358v1</a></p>
<p><b>Compressor summary</b>: This text compares the cross-lingual sentiment analysis performance of SMLMs and LLMs, finding that SMLMs excel in zero-shot settings while LLMs adapt better in few-shot settings.</p><hr><h3>DiVERT: Distractor Generation with Variational Errors Represented as  Text for Math Multiple-choice Questions</h3>
<p><a href='http://arxiv.org/abs/2406.19356v1'>http://arxiv.org/abs/2406.19356v1</a></p>
<p><b>Compressor summary</b>: DiVERT is a novel method for generating and understanding implausible multiple-choice question distractors in math by using a large language model.</p><hr><h3>Fundamental Problems With Model Editing: How Should Rational Belief  Revision Work in LLMs?</h3>
<p><a href='http://arxiv.org/abs/2406.19354v1'>http://arxiv.org/abs/2406.19354v1</a></p>
<p><b>Compressor summary</b>: The paper discusses challenges and proposes a testbed for model editing, which involves updating knowledge in language models, using a semi-synthetic dataset based on Wikidata.</p><hr><h3>CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative  Object REarrangement</h3>
<p><a href='http://arxiv.org/abs/2406.19353v1'>http://arxiv.org/abs/2406.19353v1</a></p>
<p><b>Compressor summary</b>: The paper introduces CORE4D, a large-scale 4D human-object-human interaction dataset that helps study collaborative object rearrangement and provides new challenges for generating human-object interactions.</p><hr><h3>IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and  Toxicity Types for Indonesian Language</h3>
<p><a href='http://arxiv.org/abs/2406.19349v1'>http://arxiv.org/abs/2406.19349v1</a></p>
<p><b>Compressor summary</b>: The paper introduces IndoToxic2024, a dataset for Indonesian hate speech and toxicity classification, focusing on vulnerable groups during the presidential election, and evaluates its effectiveness with various models.</p><hr><h3>Learning Visual Conditioning Tokens to Correct Domain Shift for Fully  Test-time Adaptation</h3>
<p><a href='http://arxiv.org/abs/2406.19341v1'>http://arxiv.org/abs/2406.19341v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a bi-level learning method for a visual conditioning token that can adapt a deep neural network model to different domains in image classification, improving its performance by up to 1.9%.</p><hr><h3>Efficient World Models with Context-Aware Tokenization</h3>
<p><a href='http://arxiv.org/abs/2406.19320v1'>http://arxiv.org/abs/2406.19320v1</a></p>
<p><b>Compressor summary</b>: $\Delta$-IRIS is a fast and efficient model-based RL agent that uses discrete autoencoders and autoregressive transformers to predict future deltas, achieving state of the art results on the Crafter benchmark.</p><hr><h3>Jump Starting Bandits with LLM-Generated Prior Knowledge</h3>
<p><a href='http://arxiv.org/abs/2406.19317v1'>http://arxiv.org/abs/2406.19317v1</a></p>
<p><b>Compressor summary</b>: The paper shows how Large Language Models can improve Contextual Multi-Armed Bandits by simulating human preferences, reducing online learning regret and data-gathering costs.</p><hr><h3>Enhanced Data Transfer Cooperating with Artificial Triplets for Scene  Graph Generation</h3>
<p><a href='http://arxiv.org/abs/2406.19316v1'>http://arxiv.org/abs/2406.19316v1</a></p>
<p><b>Compressor summary</b>: This paper proposes two methods to improve training data for Scene Graph Generation, which are Feature Space Triplet Augmentation and Soft Transfer, and shows their effectiveness in achieving high Recall scores.</p><hr><h3>LiveBench: A Challenging, Contamination-Free LLM Benchmark</h3>
<p><a href='http://arxiv.org/abs/2406.19314v1'>http://arxiv.org/abs/2406.19314v1</a></p>
<p><b>Compressor summary</b>: LiveBench is a new benchmark for large language models that updates questions from recent sources, scores answers automatically, and covers various challenging tasks to avoid test set contamination and biases.</p><hr><h3>The Odyssey of Commonsense Causality: From Foundational Benchmarks to  Cutting-Edge Reasoning</h3>
<p><a href='http://arxiv.org/abs/2406.19307v1'>http://arxiv.org/abs/2406.19307v1</a></p>
<p><b>Compressor summary</b>: This paper reviews the current state of research on commonsense causality, which is essential for human intelligence and decision-making, but lacks systematic exploration.</p><hr><h3>Mapping Land Naturalness from Sentinel-2 using Deep Contextual and  Geographical Priors</h3>
<p><a href='http://arxiv.org/abs/2406.19302v1'>http://arxiv.org/abs/2406.19302v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Climate change is accelerating due to human actions
- Satellite images help observe and measure effects on natural areas
- A deep learning framework maps land naturalness from Sentinel-2 data using contextual and geographical priors
- Quantifying naturalness aids environmental stewardship

Summary:
The text describes how satellite images and a deep learning framework can map land naturalness, which is affected by human actions and climate change, to help protect the environment.</p><hr><h3>MCNC: Manifold Constrained Network Compression</h3>
<p><a href='http://arxiv.org/abs/2406.19301v1'>http://arxiv.org/abs/2406.19301v1</a></p>
<p><b>Compressor summary</b>: MCNC is a new method to compress large AI models by constraining their parameter space to low-dimensional nonlinear manifolds, achieving high compression rates and performance across various tasks.</p><hr><h3>scTree: Discovering Cellular Hierarchies in the Presence of Batch  Effects in scRNA-seq Data</h3>
<p><a href='http://arxiv.org/abs/2406.19300v1'>http://arxiv.org/abs/2406.19300v1</a></p>
<p><b>Compressor summary</b>: scTree is a new method for single-cell RNA sequencing data that corrects batch effects and learns a tree structure representing clusters and their hierarchies, improving understanding of cellular landscapes.</p><hr><h3>PNeRV: A Polynomial Neural Representation for Videos</h3>
<p><a href='http://arxiv.org/abs/2406.19299v1'>http://arxiv.org/abs/2406.19299v1</a></p>
<p><b>Compressor summary</b>: PNeRV is a patch-wise implicit neural representation for videos that preserves spatiotemporal continuity using polynomial neural networks and hierarchical sampling, achieving better performance in tasks like compression and downstream applications.</p><hr><h3>Compositional Image Decomposition with Diffusion Models</h3>
<p><a href='http://arxiv.org/abs/2406.19298v1'>http://arxiv.org/abs/2406.19298v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Decomp Diffusion, an unsupervised method that decomposes images into compositional components, allowing for flexible scene composition.</p><hr><h3>Enhancing Continual Learning in Visual Question Answering with  Modality-Aware Feature Distillation</h3>
<p><a href='http://arxiv.org/abs/2406.19297v1'>http://arxiv.org/abs/2406.19297v1</a></p>
<p><b>Compressor summary</b>: This paper explores how different modalities (e.g., vision and language) evolve at different rates when training models on a sequence of tasks, proposes a modality-aware feature distillation method to improve performance, and shows its effectiveness in multimodal continual learning settings.</p><hr><h3>From Artificial Needles to Real Haystacks: Improving Retrieval  Capabilities in LLMs by Finetuning on Synthetic Data</h3>
<p><a href='http://arxiv.org/abs/2406.19292v1'>http://arxiv.org/abs/2406.19292v1</a></p>
<p><b>Compressor summary</b>: Finetuning large language models on a synthetic dataset of numerical key-value retrieval tasks enhances their information retrieval and reasoning abilities in long-context settings without causing hallucination or sacrificing general benchmark performance.</p><hr><h3>Human Modelling and Pose Estimation Overview</h3>
<p><a href='http://arxiv.org/abs/2406.19290v1'>http://arxiv.org/abs/2406.19290v1</a></p>
<p><b>Compressor summary</b>: This paper investigates various methods and applications in 2D and 3D human pose estimation, comparing state-of-the-art algorithms and discussing future directions.</p><hr><h3>HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into  Multimodal LLMs at Scale</h3>
<p><a href='http://arxiv.org/abs/2406.19280v1'>http://arxiv.org/abs/2406.19280v1</a></p>
<p><b>Compressor summary</b>: The paper introduces PubMedVision, a large and high-quality dataset for medical image-text pairs created by refining existing data and using GPT-4V to denoise and reformat it, leading to improved medical multimodal capabilities of language models.</p><hr><h3>VERISCORE: Evaluating the factuality of verifiable claims in long-form  text generation</h3>
<p><a href='http://arxiv.org/abs/2406.19276v1'>http://arxiv.org/abs/2406.19276v1</a></p>
<p><b>Compressor summary</b>: VERISCORE is a metric for evaluating factuality in diverse long-form generation tasks that can handle both verifiable and unverifiable claims, and it reveals that different models perform differently across tasks.</p><hr><h3>Stochastic Concept Bottleneck Models</h3>
<p><a href='http://arxiv.org/abs/2406.19272v1'>http://arxiv.org/abs/2406.19272v1</a></p>
<p><b>Compressor summary</b>: SCBMs use distributional parameterization to model concept dependencies and improve intervention effectiveness in interpretable Concept Bottleneck Models.</p><hr><h3>AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning</h3>
<p><a href='http://arxiv.org/abs/2406.19271v1'>http://arxiv.org/abs/2406.19271v1</a></p>
<p><b>Compressor summary</b>: The research proposes a system that collects and filters web data using trusted AI models to ensure pure and reliable training data for Large Language Models.</p><hr><h3>Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens  Grounding</h3>
<p><a href='http://arxiv.org/abs/2406.19263v1'>http://arxiv.org/abs/2406.19263v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a Tree-of-Lens (ToL) agent that uses a Hierarchical Layout Tree to understand and describe screen content and layout based on user-indicated points, outperforming other tools on a new Screen Point-and-Read benchmark.</p><hr><h3>Leveraging Contrastive Learning for Enhanced Node Representations in  Tokenized Graph Transformers</h3>
<p><a href='http://arxiv.org/abs/2406.19258v1'>http://arxiv.org/abs/2406.19258v1</a></p>
<p><b>Compressor summary</b>: GCFormer is a new graph Transformer that creates positive and negative token sequences to better capture diverse graph information and improve node representation quality for node classification tasks.</p><hr><h3>AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data  Readiness for AI</h3>
<p><a href='http://arxiv.org/abs/2406.19256v1'>http://arxiv.org/abs/2406.19256v1</a></p>
<p><b>Compressor summary</b>: AIDRIN is a framework that evaluates the quality and suitability of data for AI using various metrics, visualizations, and reports.</p><hr><h3>Enhancing Video-Language Representations with Structural Spatio-Temporal  Alignment</h3>
<p><a href='http://arxiv.org/abs/2406.19255v1'>http://arxiv.org/abs/2406.19255v1</a></p>
<p><b>Compressor summary</b>: This paper proposes a method called Finsta to improve video-language models by aligning text and video using fine-grained scene graphs, achieving better results on various tasks.</p><hr><h3>Advection Augmented Convolutional Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2406.19253v1'>http://arxiv.org/abs/2406.19253v1</a></p>
<p><b>Compressor summary</b>: The authors propose a new architecture for predicting space-time sequences that combines CNNs with advection and reaction-diffusion components, inspired by physical processes, to improve performance and explainability.</p><hr><h3>AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for  Retrieval-Augmented Generation</h3>
<p><a href='http://arxiv.org/abs/2406.19251v1'>http://arxiv.org/abs/2406.19251v1</a></p>
<p><b>Compressor summary</b>: The AutoRAG-HP framework uses a hierarchical multi-armed bandit method to efficiently tune hyper-parameters for Retrieval-Augmented Generation systems, achieving high recall with less API calls than Grid Search.</p><hr><h3>NTFormer: A Composite Node Tokenized Graph Transformer for Node  Classification</h3>
<p><a href='http://arxiv.org/abs/2406.19249v1'>http://arxiv.org/abs/2406.19249v1</a></p>
<p><b>Compressor summary</b>: NTFormer is a new graph Transformer that uses a novel token generator called Node2Par to express rich graph features, enabling it to learn node representations without requiring graph-specific modifications.</p><hr><h3>Local Manifold Learning for No-Reference Image Quality Assessment</h3>
<p><a href='http://arxiv.org/abs/2406.19247v1'>http://arxiv.org/abs/2406.19247v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for assessing image quality by combining local manifold learning with contrastive learning, which improves differentiation and performance compared to existing methods.</p><hr><h3>Improving the Expressiveness of $K$-hop Message-Passing GNNs by  Injecting Contextualized Substructure Information</h3>
<p><a href='http://arxiv.org/abs/2406.19244v1'>http://arxiv.org/abs/2406.19244v1</a></p>
<p><b>Compressor summary</b>: Key points:
- GNNs are powerful for graph learning but have limited expressiveness
- $K$-hop GNNs aggregate information from neighbors within $K$ hops
- Substructure encoding function enhances expressive power of $K$-hop GNNs
- Method is provably more powerful than previous works and achieves state-of-the-art results

Summary:
The paper proposes a substructure encoding function to improve the expressive power of $K$-hop graph neural networks, which outperforms previous works and matches 3-WL test.</p><hr><h3>Revealing Fine-Grained Values and Opinions in Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.19238v1'>http://arxiv.org/abs/2406.19238v1</a></p>
<p><b>Compressor summary</b>: The study analysed how large language models respond to political statements and found that demographic features and question formats affect their stances, revealing biases and patterns in their justifications.</p><hr><h3>FlowVQA: Mapping Multimodal Logic in Visual Question Answering with  Flowcharts</h3>
<p><a href='http://arxiv.org/abs/2406.19237v1'>http://arxiv.org/abs/2406.19237v1</a></p>
<p><b>Compressor summary</b>: FlowVQA is a new benchmark for testing visual question-answering models on flowcharts with various reasoning tasks.</p><hr><h3>Human-Aware Vision-and-Language Navigation: Bridging Simulation to  Reality with Dynamic Human Interactions</h3>
<p><a href='http://arxiv.org/abs/2406.19236v1'>http://arxiv.org/abs/2406.19236v1</a></p>
<p><b>Compressor summary</b>: HA-VLN is a new approach to navigation that considers dynamic human activities and uses novel datasets and agents to improve real-world applicability and robustness.</p><hr><h3>RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs</h3>
<p><a href='http://arxiv.org/abs/2406.19232v1'>http://arxiv.org/abs/2406.19232v1</a></p>
<p><b>Compressor summary</b>: RuBLiMP is a new benchmark for testing Russian language models' grammatical knowledge by providing diverse minimal pairs covering various linguistic phenomena.</p><hr><h3>Tools Fail: Detecting Silent Errors in Faulty Tools</h3>
<p><a href='http://arxiv.org/abs/2406.19228v1'>http://arxiv.org/abs/2406.19228v1</a></p>
<p><b>Compressor summary</b>: The text discusses a framework for LLMs to detect silent tool errors and plan better, focusing on their use as tools rather than just choosing them.</p><hr><h3>Aligning Teacher with Student Preferences for Tailored Training Data  Generation</h3>
<p><a href='http://arxiv.org/abs/2406.19227v1'>http://arxiv.org/abs/2406.19227v1</a></p>
<p><b>Compressor summary</b>: ARTE is a framework that aligns teacher instructional content with student preferences to generate tailored training examples for Knowledge Distillation on edge devices.</p><hr><h3>Simulating Classroom Education with LLM-Empowered Agents</h3>
<p><a href='http://arxiv.org/abs/2406.19226v1'>http://arxiv.org/abs/2406.19226v1</a></p>
<p><b>Compressor summary</b>: Key points:
- The paper proposes SimClass, a multi-agent framework using LLMs for virtual classroom teaching
- SimClass simulates classroom interaction patterns and enhances user experience
- Agents collaborate to create enlivening interactions and improve learning process

Summary:
SimClass is a novel framework that uses large language models to simulate and enhance classroom interactions in a virtual setting, improving the user's learning experience.</p><hr><h3>ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model  for Semantic Segmentation</h3>
<p><a href='http://arxiv.org/abs/2406.19225v1'>http://arxiv.org/abs/2406.19225v1</a></p>
<p><b>Compressor summary</b>: The ProtoGMM model uses Gaussian mixtures to estimate multi-prototype distributions for semantic segmentation in unlabeled target domains by leveraging supervised models from labeled source domains, improving predictions with contrastive learning.</p><hr><h3>T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for  Memory-Efficient Embeddings</h3>
<p><a href='http://arxiv.org/abs/2406.19223v1'>http://arxiv.org/abs/2406.19223v1</a></p>
<p><b>Compressor summary</b>: T-FREE is a novel tokenizer that improves efficiency and cross-lingual transfer learning by embedding words using sparse activation patterns over character triplets without a reference corpus.</p><hr><h3>Think Step by Step: Chain-of-Gesture Prompting for Error Detection in  Robotic Surgical Videos</h3>
<p><a href='http://arxiv.org/abs/2406.19217v1'>http://arxiv.org/abs/2406.19217v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel error detection method for robot-assisted surgeries using contextual information from videos and reasoning modules inspired by natural language processing.</p><hr><h3>SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented  Generation</h3>
<p><a href='http://arxiv.org/abs/2406.19215v1'>http://arxiv.org/abs/2406.19215v1</a></p>
<p><b>Compressor summary</b>: SeaKR is a new model that uses LLMs' self-aware uncertainty to retrieve and integrate knowledge for better question answering.</p><hr><h3>Estimating Long-term Heterogeneous Dose-response Curve: Generalization  Bound Leveraging Optimal Transport Weights</h3>
<p><a href='http://arxiv.org/abs/2406.19195v1'>http://arxiv.org/abs/2406.19195v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to estimate the long-term heterogeneous dose-response curve by using optimal transport weighting to account for unobserved confounders and providing theoretical guarantees for counterfactual prediction error.</p><hr><h3>BISeizuRe: BERT-Inspired Seizure Data Representation to Improve Epilepsy  Monitoring</h3>
<p><a href='http://arxiv.org/abs/2406.19189v1'>http://arxiv.org/abs/2406.19189v1</a></p>
<p><b>Compressor summary</b>: The study proposes BENDR, a BERT-based model that uses pre-training and fine-tuning on large datasets to improve seizure detection from EEG recordings with low false positives and acceptable sensitivity.</p><hr><h3>Averaging log-likelihoods in direct alignment</h3>
<p><a href='http://arxiv.org/abs/2406.19188v1'>http://arxiv.org/abs/2406.19188v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a method to make direct alignment in large language models more consistent across different completion lengths using a new averaging operator.</p><hr><h3>Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a  supervised-friendly fashion</h3>
<p><a href='http://arxiv.org/abs/2406.19185v1'>http://arxiv.org/abs/2406.19185v1</a></p>
<p><b>Compressor summary</b>: CoPG is an RL algorithm that can finetune LLMs with off-policy data and outperforms direct alignment methods and policy gradient in generalization.</p><hr><h3>Towards Reducing Data Acquisition and Labeling for Defect Detection  using Simulated Data</h3>
<p><a href='http://arxiv.org/abs/2406.19175v1'>http://arxiv.org/abs/2406.19175v1</a></p>
<p><b>Compressor summary</b>: The paper discusses using synthetic and real-world X-ray images to train an object detection model, finding that a mix of synthetic and unlabeled real-world data is more cost-efficient than using only labeled real-world data.</p><hr><h3>Annotation Errors and NER: A Study with OntoNotes 5.0</h3>
<p><a href='http://arxiv.org/abs/2406.19172v1'>http://arxiv.org/abs/2406.19172v1</a></p>
<p><b>Compressor summary</b>: The paper presents three techniques to detect and fix annotation errors in the OntoNotes 5.0 English NER corpus, improving model performance.</p><hr><h3>The Illusion of Competence: Evaluating the Effect of Explanations on  Users' Mental Models of Visual Question Answering Systems</h3>
<p><a href='http://arxiv.org/abs/2406.19170v1'>http://arxiv.org/abs/2406.19170v1</a></p>
<p><b>Compressor summary</b>: The study investigates if providing explanations helps users understand an AI system's limitations when it performs poorly on a visual task involving full-color or grayscale images, but finds that explanations do not improve users' perceptions of the system's capabilities and limitations.</p><hr><h3>Single Image Estimation of Cell Migration Direction by Deep Circular  Regression</h3>
<p><a href='http://arxiv.org/abs/2406.19162v1'>http://arxiv.org/abs/2406.19162v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for estimating the migration direction of cells from a single image using deep circular regression and achieving better accuracy than previous methods.</p><hr><h3>Heterogeneous Causal Metapath Graph Neural Network for  Gene-Microbe-Disease Association Prediction</h3>
<p><a href='http://arxiv.org/abs/2406.19156v1'>http://arxiv.org/abs/2406.19156v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a neural network (HCMGNN) to predict gene-microbe-disease associations using causal metapaths and semantic sharing.</p><hr><h3>Advancing operational PM2.5 forecasting with dual deep neural networks  (D-DNet)</h3>
<p><a href='http://arxiv.org/abs/2406.19154v1'>http://arxiv.org/abs/2406.19154v1</a></p>
<p><b>Compressor summary</b>: The dual deep neural network (D-DNet) is a more efficient and accurate model for predicting PM2.5 and AOD550 levels than traditional models or the CAMS 4D-Var system, using real-time observations to improve forecasting.</p><hr><h3>RAVEN: Multitask Retrieval Augmented Vision-Language Learning</h3>
<p><a href='http://arxiv.org/abs/2406.19150v1'>http://arxiv.org/abs/2406.19150v1</a></p>
<p><b>Compressor summary</b>: RAVEN is a framework that enhances vision-language models with retrieval augmentation for multiple tasks, improving efficiency and performance.</p><hr><h3>BackMix: Mitigating Shortcut Learning in Echocardiography with Minimal  Supervision</h3>
<p><a href='http://arxiv.org/abs/2406.19148v1'>http://arxiv.org/abs/2406.19148v1</a></p>
<p><b>Compressor summary</b>: The authors propose a random background augmentation method called BackMix for neural networks to improve echocardiogram view classification by focusing on the image content and reducing spurious correlations.</p><hr><h3>Resolving Discrepancies in Compute-Optimal Scaling of Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.19146v1'>http://arxiv.org/abs/2406.19146v1</a></p>
<p><b>Compressor summary</b>: The paper compares two scaling laws for optimal model size as a function of compute budget, identifies factors causing discrepancies, and shows how to obtain agreement with one law by correcting these factors.</p><hr><h3>YZS-model: A Predictive Model for Organic Drug Solubility Based on Graph  Convolutional Networks and Transformer-Attention</h3>
<p><a href='http://arxiv.org/abs/2406.19136v1'>http://arxiv.org/abs/2406.19136v1</a></p>
<p><b>Compressor summary</b>: Key points:
- Solubility prediction is crucial for drug effectiveness and safety
- Traditional methods fail to capture complex molecular structures
- Novel deep learning framework combines attention-based transformers, LSTM networks, and GCN
- Outperforms benchmark models and offers insights for drug design and selection

Summary:
The text presents a novel deep learning framework that improves the prediction of drug solubility by capturing complex molecular structures better than traditional methods, with potential applications for drug discovery.</p><hr><h3>CELLO: Causal Evaluation of Large Vision-Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.19131v1'>http://arxiv.org/abs/2406.19131v1</a></p>
<p><b>Compressor summary</b>: The authors introduce CELLO, a novel dataset for causal reasoning tasks involving interactions between humans and objects, and propose CELLO-CoT, a chain-of-thought prompting strategy to improve large vision-language models' performance on these tasks.</p><hr><h3>Evidential Concept Embedding Models: Towards Reliable Concept  Explanations for Skin Disease Diagnosis</h3>
<p><a href='http://arxiv.org/abs/2406.19130v1'>http://arxiv.org/abs/2406.19130v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an evidential Concept Embedding Model that improves interpretable deep learning methods in medical image analysis by modeling concept uncertainty and rectifying misalignments for better clinical diagnosis explanations.</p><hr><h3>Towards Learning Abductive Reasoning using VSA Distributed  Representations</h3>
<p><a href='http://arxiv.org/abs/2406.19121v1'>http://arxiv.org/abs/2406.19121v1</a></p>
<p><b>Compressor summary</b>: ARLC is a model that learns abductive reasoning with context, achieving high accuracy and interpretability on Raven's matrices tasks, surpassing neuro-symbolic and large language models with fewer parameters.</p><hr><h3>CHEW: A Dataset of CHanging Events in Wikipedia</h3>
<p><a href='http://arxiv.org/abs/2406.19116v1'>http://arxiv.org/abs/2406.19116v1</a></p>
<p><b>Compressor summary</b>: CHEW is a dataset of Wikipedia changes in text, used to test LLMs' timeline understanding and identify meaning shifts.</p><hr><h3>A Teacher Is Worth A Million Instructions</h3>
<p><a href='http://arxiv.org/abs/2406.19112v1'>http://arxiv.org/abs/2406.19112v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an improved training method for smaller LLMs by leveraging knowledge from larger models and a novel post-training domain alignment phase, achieving better results than state-of-the-art models with more parameters.</p><hr><h3>FDLite: A Single Stage Lightweight Face Detector Network</h3>
<p><a href='http://arxiv.org/abs/2406.19107v1'>http://arxiv.org/abs/2406.19107v1</a></p>
<p><b>Compressor summary</b>: The text describes a new lightweight face detector with a customized backbone network and two multi-task losses that achieves high accuracy on the WIDER FACE dataset.</p><hr><h3>Statements: Universal Information Extraction from Tables with Large  Language Models for ESG KPIs</h3>
<p><a href='http://arxiv.org/abs/2406.19102v1'>http://arxiv.org/abs/2406.19102v1</a></p>
<p><b>Compressor summary</b>: Statements are a new data structure for extracting quantitative facts from ESG reports using deep learning models like SemTabNet, which can facilitate exploratory data analysis.</p><hr><h3>DocKylin: A Large Multimodal Model for Visual Document Understanding  with Efficient Visual Slimming</h3>
<p><a href='http://arxiv.org/abs/2406.19101v1'>http://arxiv.org/abs/2406.19101v1</a></p>
<p><b>Compressor summary</b>: DocKylin is a document-centric MLLM that uses pixel-level slimming and token-level slimming to improve visual content understanding in high-resolution document images.</p><hr><h3>Fairness and Bias in Multimodal AI: A Survey</h3>
<p><a href='http://arxiv.org/abs/2406.19097v1'>http://arxiv.org/abs/2406.19097v1</a></p>
<p><b>Compressor summary</b>: This survey examines 50 examples of datasets and models for fairness and bias in Large Multimodal Models (LMMs) and proposes a new category of quantifying bias, preuse.</p><hr><h3>Adaptive Stochastic Weight Averaging</h3>
<p><a href='http://arxiv.org/abs/2406.19092v1'>http://arxiv.org/abs/2406.19092v1</a></p>
<p><b>Compressor summary</b>: ASWA is a technique that improves generalization by updating a running average of model parameters only when it helps the validation performance, combining SWA and early stopping.</p><hr><h3>Dimensions underlying the representational alignment of deep neural  networks with humans</h3>
<p><a href='http://arxiv.org/abs/2406.19087v1'>http://arxiv.org/abs/2406.19087v1</a></p>
<p><b>Compressor summary</b>: The text compares how humans and artificial neural networks represent images, finding similarities and differences in their strategies and highlighting the need for better alignment.</p><hr><h3>AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database  Queries</h3>
<p><a href='http://arxiv.org/abs/2406.19073v1'>http://arxiv.org/abs/2406.19073v1</a></p>
<p><b>Compressor summary</b>: AMBROSIA is a new benchmark for testing text-to-SQL parsers' ability to handle ambiguous questions with different types of uncertainty, using controlled databases generated from scratch.</p><hr><h3>EmPO: Theory-Driven Dataset Construction for Empathetic Response  Generation through Preference Optimization</h3>
<p><a href='http://arxiv.org/abs/2406.19071v1'>http://arxiv.org/abs/2406.19071v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to improve empathetic responses from conversational agents using preference datasets and optimization algorithms, evaluating it on two metrics and a benchmark dataset.</p><hr><h3>FAGhead: Fully Animate Gaussian Head from Monocular Videos</h3>
<p><a href='http://arxiv.org/abs/2406.19070v1'>http://arxiv.org/abs/2406.19070v1</a></p>
<p><b>Compressor summary</b>: FAGhead is a method that creates realistic 3D human avatars from monocular videos with controllable expressions and poses, using Point-based Learnable Representation Field (PLRF) and alpha rendering.</p><hr><h3>Dancing in the Shadows: Harnessing Ambiguity for Fairer Classifiers</h3>
<p><a href='http://arxiv.org/abs/2406.19066v1'>http://arxiv.org/abs/2406.19066v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using uncertain identity data to train classifiers for better algorithmic fairness.</p><hr><h3>STBench: Assessing the Ability of Large Language Models in  Spatio-Temporal Analysis</h3>
<p><a href='http://arxiv.org/abs/2406.19065v1'>http://arxiv.org/abs/2406.19065v1</a></p>
<p><b>Compressor summary</b>: The paper introduces STBench, a benchmark dataset for evaluating spatio-temporal understanding in large language models, and assesses 13 LLMs on four dimensions of this capability.</p><hr><h3>Segment Anything Model for automated image data annotation: empirical  studies using text prompts from Grounding DINO</h3>
<p><a href='http://arxiv.org/abs/2406.19057v1'>http://arxiv.org/abs/2406.19057v1</a></p>
<p><b>Compressor summary</b>: REC-based detection uses language descriptions to detect objects not in existing class names, but may make false positive predictions; however, these can be mitigated by filtering detections by size, improving semantic segmentation and data annotation efficiency.</p><hr><h3>SimpleFusion: A Simple Fusion Framework for Infrared and Visible Images</h3>
<p><a href='http://arxiv.org/abs/2406.19055v1'>http://arxiv.org/abs/2406.19055v1</a></p>
<p><b>Compressor summary</b>: SimpleFusion is a simple framework for fusing visible and infrared images using two plain convolutional neural networks without downsampling, preserving complementary information between the modalities.</p><hr><h3>A look under the hood of the Interactive Deep Learning Enterprise  (No-IDLE)</h3>
<p><a href='http://arxiv.org/abs/2406.19054v1'>http://arxiv.org/abs/2406.19054v1</a></p>
<p><b>Compressor summary</b>: The text describes an accessible prototype system for non-experts to interact with machine learning and gain insights into user behavior, using a novel methodology for interactive machine learning and multimodal interaction.</p><hr><h3>Accuracy on the wrong line: On the pitfalls of noisy data for  out-of-distribution generalisation</h3>
<p><a href='http://arxiv.org/abs/2406.19049v1'>http://arxiv.org/abs/2406.19049v1</a></p>
<p><b>Compressor summary</b>: The study explores when the accuracy-on-the-line relationship in machine learning breaks down due to noisy data, nuisance features, and spurious features, leading to "Accuracy-on-the-wrong-line".</p><hr><h3>BiCo-Fusion: Bidirectional Complementary LiDAR-Camera Fusion for  Semantic- and Spatial-Aware 3D Object Detection</h3>
<p><a href='http://arxiv.org/abs/2406.19048v1'>http://arxiv.org/abs/2406.19048v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel bidirectional complementary Lidar-camera fusion framework, called BiCo-Fusion, that can achieve robust semantic- and spatial-aware 3D object detection by mutually enhancing multi-modal features and adaptatively selecting them.</p><hr><h3>On Convex Optimization with Semi-Sensitive Features</h3>
<p><a href='http://arxiv.org/abs/2406.19040v1'>http://arxiv.org/abs/2406.19040v1</a></p>
<p><b>Compressor summary</b>: The paper analyzes differential privacy in empirical risk minimization for semi-sensitive data and provides better bounds on excess risk.</p><hr><h3>Improving Weak-to-Strong Generalization with Reliability-Aware Alignment</h3>
<p><a href='http://arxiv.org/abs/2406.19032v1'>http://arxiv.org/abs/2406.19032v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve large language models' generalization by using the reliability of weak supervision signals in the alignment process, which helps reduce errors from noisy supervision and enhance model accuracy.</p><hr><h3>Using diffusion model as constraint: Empower Image Restoration Network  Training with Diffusion Model</h3>
<p><a href='http://arxiv.org/abs/2406.19030v1'>http://arxiv.org/abs/2406.19030v1</a></p>
<p><b>Compressor summary</b>: The paper introduces DiffLoss, a naturalness-oriented and semantic-aware optimization mechanism for image restoration that leverages diffusion models' distribution coverage and high-level semantic space to improve visual and semantic perception quality.</p><hr><h3>Lithium-Ion Battery System Health Monitoring and Fault Analysis from  Field Data Using Gaussian Processes</h3>
<p><a href='http://arxiv.org/abs/2406.19015v1'>http://arxiv.org/abs/2406.19015v1</a></p>
<p><b>Compressor summary</b>: The authors use Gaussian process models to analyze battery resistance, develop fault detection rules, and understand cell-level failure mechanisms for lithium iron phosphate batteries based on a large dataset of returned batteries.</p><hr><h3>VideoMambaPro: A Leap Forward for Mamba in Video Understanding</h3>
<p><a href='http://arxiv.org/abs/2406.19006v1'>http://arxiv.org/abs/2406.19006v1</a></p>
<p><b>Compressor summary</b>: VideoMambaPro improves video action recognition by addressing limitations in Mamba's token processing with masked backward computation and elemental residual connections, outperforming transformer models while being more efficient.</p><hr><h3>Improving Taxonomic Image-based Out-of-distribution Detection With DNA  Barcodes</h3>
<p><a href='http://arxiv.org/abs/2406.18999v1'>http://arxiv.org/abs/2406.18999v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a re-ordering approach to improve image-based species identification by using DNA barcodes to detect out-of-distribution classes.</p><hr><h3>Zero-shot domain adaptation based on dual-level mix and contrast</h3>
<p><a href='http://arxiv.org/abs/2406.18996v1'>http://arxiv.org/abs/2406.18996v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a zero-shot domain adaptation method that uses data augmentation, domain adversarial learning, and dual-level contrastive learning to learn domain-invariant features with low task bias for the target task of interest.</p><hr><h3>Semi-supervised Concept Bottleneck Models</h3>
<p><a href='http://arxiv.org/abs/2406.18992v1'>http://arxiv.org/abs/2406.18992v1</a></p>
<p><b>Compressor summary</b>: SSCBM is a new framework that improves concept bottleneck models by using semi-supervised learning and aligning unlabeled data with concepts, making them more accurate and efficient even with limited labeled data.</p><hr><h3>A Fast Learning-Based Surrogate of Electrical Machines using a Reduced  Basis</h3>
<p><a href='http://arxiv.org/abs/2406.18990v1'>http://arxiv.org/abs/2406.18990v1</a></p>
<p><b>Compressor summary</b>: The article proposes a real-time surrogate model for parameterized PDEs using a hybrid method of POD and SVR, aimed for interactive analysis in digital twins applications.</p><hr><h3>Structural Attention: Rethinking Transformer for Unpaired Medical Image  Synthesis</h3>
<p><a href='http://arxiv.org/abs/2406.18967v1'>http://arxiv.org/abs/2406.18967v1</a></p>
<p><b>Compressor summary</b>: UNest is a novel unpaired medical image synthesis architecture that uses structural inductive biases and structural attention to improve the synthesis of anatomical regions, achieving significant improvements over recent methods on three modalities.</p><hr><h3>UniGen: A Unified Framework for Textual Dataset Generation Using Large  Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.18966v1'>http://arxiv.org/abs/2406.18966v1</a></p>
<p><b>Compressor summary</b>: UniGen is a framework that uses large language models to generate diverse, accurate, and controllable text datasets for various applications, such as benchmarking and data augmentation.</p><hr><h3>AnyControl: Create Your Artwork with Versatile Control on Text-to-Image  Generation</h3>
<p><a href='http://arxiv.org/abs/2406.18958v1'>http://arxiv.org/abs/2406.18958v1</a></p>
<p><b>Compressor summary</b>: AnyControl is a framework for image synthesis that handles diverse control signals and produces high-quality images faithful to the input text.</p><hr><h3>Alignment For Performance Improvement in Conversation Bots</h3>
<p><a href='http://arxiv.org/abs/2406.18954v1'>http://arxiv.org/abs/2406.18954v1</a></p>
<p><b>Compressor summary</b>: Alignment methods improve bot performance in following rules compared to instruction fine-tuning alone.</p><hr><h3>Investigating and Defending Shortcut Learning in Personalized Diffusion  Models</h3>
<p><a href='http://arxiv.org/abs/2406.18944v1'>http://arxiv.org/abs/2406.18944v1</a></p>
<p><b>Compressor summary</b>: This paper analyzes the vulnerability of personalized diffusion models to adversarial perturbations and proposes a method to align the latent image with its semantic meaning and contrastive learning to prevent performance degradation.</p><hr><h3>CLIP3D-AD: Extending CLIP for 3D Few-Shot Anomaly Detection with  Multi-View Images Generation</h3>
<p><a href='http://arxiv.org/abs/2406.18941v1'>http://arxiv.org/abs/2406.18941v1</a></p>
<p><b>Compressor summary</b>: CLIP3D-AD is a novel 3D few-shot anomaly detection method that adapts CLIP for classification and segmentation using synthesized anomalous images, image and text adapters, and multi-view fusion.</p><hr><h3>Evaluating AI Group Fairness: a Fuzzy Logic Perspective</h3>
<p><a href='http://arxiv.org/abs/2406.18939v1'>http://arxiv.org/abs/2406.18939v1</a></p>
<p><b>Compressor summary</b>: The text proposes a fuzzy logic framework to evaluate and standardize different definitions of group fairness in AI systems, considering uncertain and context-specific beliefs.</p><hr><h3>Semi-adaptive Synergetic Two-way Pseudoinverse Learning System</h3>
<p><a href='http://arxiv.org/abs/2406.18931v1'>http://arxiv.org/abs/2406.18931v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new learning system that simplifies hyperparameter tuning and improves training efficiency using semi-adaptive synergetic two-way pseudoinverse learning subsystems trained without gradient descent.</p><hr><h3>Reasoning About Action and Change</h3>
<p><a href='http://arxiv.org/abs/2406.18930v1'>http://arxiv.org/abs/2406.18930v1</a></p>
<p><b>Compressor summary</b>: The book covers various aspects of AI research, from fundamentals to applications, and targets students and professionals interested in the field.</p><hr><h3>RoFIR: Robust Fisheye Image Rectification Framework Impervious to  Optical Center Deviation</h3>
<p><a href='http://arxiv.org/abs/2406.18927v1'>http://arxiv.org/abs/2406.18927v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for rectifying deviated fisheye images by learning a distortion vector map that captures local distortion features and improves performance with data augmentation.</p><hr><h3>Fine-tuned network relies on generic representation to solve unseen  cognitive task</h3>
<p><a href='http://arxiv.org/abs/2406.18926v1'>http://arxiv.org/abs/2406.18926v1</a></p>
<p><b>Compressor summary</b>: The study compares two GPT-2 models on a novel decision-making task, showing that fine-tuned models rely more on pretrained representations than scratch-trained ones, which develop more specific mechanisms.</p><hr><h3>Selective Vision is the Challenge for Visual Reasoning: A Benchmark for  Visual Argument Understanding</h3>
<p><a href='http://arxiv.org/abs/2406.18925v1'>http://arxiv.org/abs/2406.18925v1</a></p>
<p><b>Compressor summary</b>: The paper introduces VisArgs, a dataset for evaluating AI's ability to understand visual arguments, and shows that current AI models struggle with identifying relevant visual cues and perform better when given them as input.</p><hr><h3>Learning Pareto Set for Multi-Objective Continuous Robot Control</h3>
<p><a href='http://arxiv.org/abs/2406.18924v1'>http://arxiv.org/abs/2406.18924v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an efficient multi-objective reinforcement learning algorithm that learns a continuous representation of the Pareto set using a single hypernet, achieving better performance on robot control problems.</p><hr><h3>Time Matters: Scaling Laws for Any Budget</h3>
<p><a href='http://arxiv.org/abs/2406.18922v1'>http://arxiv.org/abs/2406.18922v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a better way to estimate training time and loss for transformer models based on memory copies, allowing for more efficient architecture design.</p><hr><h3>Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models  with Personality-Indicative Data</h3>
<p><a href='http://arxiv.org/abs/2406.18921v1'>http://arxiv.org/abs/2406.18921v1</a></p>
<p><b>Compressor summary</b>: The paper proposes using psychological questions to enhance small role-playing language models, improving their dialogue generation and character portrayal.</p><hr><h3>TrustUQA: A Trustful Framework for Unified Structured Data Question  Answering</h3>
<p><a href='http://arxiv.org/abs/2406.18916v1'>http://arxiv.org/abs/2406.18916v1</a></p>
<p><b>Compressor summary</b>: UnifiedTQA is a trustful question answering framework that supports multiple types of structured data using a Condition Graph representation and a two-level querying method with dynamic demonstration retrieval.</p><hr><h3>Factor-Conditioned Speaking-Style Captioning</h3>
<p><a href='http://arxiv.org/abs/2406.18910v1'>http://arxiv.org/abs/2406.18910v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a method for generating diverse and accurate speaking-style captions by first predicting speaking-style factors and then sampling from them.</p><hr><h3>A Universal Railway Obstacle Detection System based on Semi-supervised  Segmentation And Optical Flow</h3>
<p><a href='http://arxiv.org/abs/2406.18908v1'>http://arxiv.org/abs/2406.18908v1</a></p>
<p><b>Compressor summary</b>: The authors propose a semi-supervised segmentation method using synthetic images and optical flow clues for detecting obstacles in railway scenarios with varying conditions.</p><hr><h3>Historia Magistra Vitae: Dynamic Topic Modeling of Roman Literature  using Neural Embeddings</h3>
<p><a href='http://arxiv.org/abs/2406.18907v1'>http://arxiv.org/abs/2406.18907v1</a></p>
<p><b>Compressor summary</b>: The authors compare traditional and BERT-based dynamic topic models for Roman literature and find that while statistics favor the former, insights from the latter are better and it's easier to use.</p><hr><h3>Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets</h3>
<p><a href='http://arxiv.org/abs/2406.18906v1'>http://arxiv.org/abs/2406.18906v1</a></p>
<p><b>Compressor summary</b>: The authors develop a task to assess how well large language models can recognize different poetic forms and elements, and discuss the challenges of creating benchmarks for poetry and other creative tasks.</p><hr><h3>Autoencoder based approach for the mitigation of spurious correlations</h3>
<p><a href='http://arxiv.org/abs/2406.18901v1'>http://arxiv.org/abs/2406.18901v1</a></p>
<p><b>Compressor summary</b>: The paper proposes an autoencoder-based method to analyze and reduce spurious correlations, improving out-of-distribution generalization for deep neural networks on the Global Wheat Head Detection dataset.</p><hr><h3>360 in the Wild: Dataset for Depth Prediction and View Synthesis</h3>
<p><a href='http://arxiv.org/abs/2406.18898v1'>http://arxiv.org/abs/2406.18898v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a large-scale 360° videos dataset collected from diverse real-world environments, with pose and depth information, for learning-based tasks such as depth estimation and view synthesis.</p><hr><h3>Can we teach language models to gloss endangered languages?</h3>
<p><a href='http://arxiv.org/abs/2406.18895v1'>http://arxiv.org/abs/2406.18895v1</a></p>
<p><b>Compressor summary</b>: The paper explores using large language models for generating interlinear glossed text without any training and shows that targeted example selection can improve performance.</p><hr><h3>AlignIT: Enhancing Prompt Alignment in Customization of Text-to-Image  Models</h3>
<p><a href='http://arxiv.org/abs/2406.18893v1'>http://arxiv.org/abs/2406.18893v1</a></p>
<p><b>Compressor summary</b>: The paper proposes new methods to improve text-to-image customization by aligning generated images with user-supplied reference images and fixing issues in existing methods.</p><hr><h3>Sequential three-way group decision-making for double hierarchy hesitant  fuzzy linguistic term set</h3>
<p><a href='http://arxiv.org/abs/2406.18884v1'>http://arxiv.org/abs/2406.18884v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel multi-level sequential three-way decision for group decision-making (S3W-GDM) method that considers vagueness, hesitation, and variation in GDM problems using granular computing to improve efficiency.</p><hr><h3>SSP: Self-Supervised Prompting for Cross-Lingual Transfer to  Low-Resource Languages using Large Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.18880v1'>http://arxiv.org/abs/2406.18880v1</a></p>
<p><b>Compressor summary</b>: The paper explores how large language models can be used for low-resource languages tasks without labeled data by using a novel in-context learning approach called Self-Supervised Prompting (SSP).</p><hr><h3>Efficacy of Language Model Self-Play in Non-Zero-Sum Games</h3>
<p><a href='http://arxiv.org/abs/2406.18872v1'>http://arxiv.org/abs/2406.18872v1</a></p>
<p><b>Compressor summary</b>: Self-play improves language models in both cooperative and competitive settings, even when objectives change from cooperation to competition or vice versa.</p><hr><h3>Advancing Cross-domain Discriminability in Continual Learning of  Vison-Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.18868v1'>http://arxiv.org/abs/2406.18868v1</a></p>
<p><b>Compressor summary</b>: RAIL is a method to learn from multiple domains without forgetting or relying on extra data, while preserving VLM's zero-shot ability using recursive ridge regression and feature projection.</p><hr><h3>From Biased Selective Labels to Pseudo-Labels: An  Expectation-Maximization Framework for Learning from Biased Decisions</h3>
<p><a href='http://arxiv.org/abs/2406.18865v1'>http://arxiv.org/abs/2406.18865v1</a></p>
<p><b>Compressor summary</b>: DCEM is an algorithm for learning from selective labels with disparate censorship, which mitigates labeling biases without compromising performance on synthetic and clinical data.</p><hr><h3>Learning Modality Knowledge Alignment for Cross-Modality Transfer</h3>
<p><a href='http://arxiv.org/abs/2406.18864v1'>http://arxiv.org/abs/2406.18864v1</a></p>
<p><b>Compressor summary</b>: The paper studies how modality gap affects cross-modality transfer and proposes MoNA, a meta-learning method that reduces the gap by transforming target data.</p><hr><h3>Predicting the duration of traffic incidents for Sydney greater  metropolitan area using machine learning methods</h3>
<p><a href='http://arxiv.org/abs/2406.18861v1'>http://arxiv.org/abs/2406.18861v1</a></p>
<p><b>Compressor summary</b>: The researchers developed a machine learning model to predict traffic incident durations in Sydney, using data on road characteristics, incidents, and socio-economic factors, and found that XGBoost performed best.</p><hr><h3>Two-Pronged Human Evaluation of ChatGPT Self-Correction in Radiology  Report Simplification</h3>
<p><a href='http://arxiv.org/abs/2406.18859v1'>http://arxiv.org/abs/2406.18859v1</a></p>
<p><b>Compressor summary</b>: The study investigates if large language models can generate patient-friendly versions of radiology reports using self-correction prompts and a new evaluation method with radiologists and laypeople.</p><hr><h3>FFN: a Fine-grained Chinese-English Financial Domain Parallel Corpus</h3>
<p><a href='http://arxiv.org/abs/2406.18856v1'>http://arxiv.org/abs/2406.18856v1</a></p>
<p><b>Compressor summary</b>: The paper introduces a Chinese-English financial news dataset (FFN) and evaluates ChatGPT, ERNIE-bot, and OpenNMT models for financial machine translation, highlighting challenges and opportunities in this domain.</p><hr><h3>What Is Missing In Homophily? Disentangling Graph Homophily For Graph  Neural Networks</h3>
<p><a href='http://arxiv.org/abs/2406.18854v1'>http://arxiv.org/abs/2406.18854v1</a></p>
<p><b>Compressor summary</b>: The paper proposes Tri-Hom, a new composite metric that considers three aspects of graph homophily and shows its effectiveness in understanding the performance of Graph Neural Networks.</p><hr><h3>Decoding-Time Language Model Alignment with Multiple Objectives</h3>
<p><a href='http://arxiv.org/abs/2406.18853v1'>http://arxiv.org/abs/2406.18853v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a multi-objective decoding algorithm that combines multiple language models to better serve diverse user needs and shows its effectiveness in improving various metrics.</p><hr><h3>LICO: Large Language Models for In-Context Molecular Optimization</h3>
<p><a href='http://arxiv.org/abs/2406.18851v1'>http://arxiv.org/abs/2406.18851v1</a></p>
<p><b>Compressor summary</b>: LICO is a model that enhances large language models for black-box optimization in the molecular domain using in-context predictions.</p><hr><h3>Dysca: A Dynamic and Scalable Benchmark for Evaluating Perception  Ability of LVLMs</h3>
<p><a href='http://arxiv.org/abs/2406.18849v1'>http://arxiv.org/abs/2406.18849v1</a></p>
<p><b>Compressor summary</b>: The paper introduces Dysca, a dynamic and scalable benchmark for evaluating large vision-language models on novel images, questions, and answers across different styles, scenarios, and question types.</p><hr><h3>Temporally Multi-Scale Sparse Self-Attention for Physical Activity Data  Imputation</h3>
<p><a href='http://arxiv.org/abs/2406.18848v1'>http://arxiv.org/abs/2406.18848v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a sparse self-attention model using domain knowledge to impute missing hourly step count data from wearable sensors in real-world settings.</p><hr><h3>Learning Retrieval Augmentation for Personalized Dialogue Generation</h3>
<p><a href='http://arxiv.org/abs/2406.18847v1'>http://arxiv.org/abs/2406.18847v1</a></p>
<p><b>Compressor summary</b>: The paper proposes LAPDOG, a method that uses external knowledge and story retrieval to generate personalized dialogues based on persona profiles.</p><hr><h3>Retain, Blend, and Exchange: A Quality-aware Spatial-Stereo Fusion  Approach for Event Stream Recognition</h3>
<p><a href='http://arxiv.org/abs/2406.18845v1'>http://arxiv.org/abs/2406.18845v1</a></p>
<p><b>Compressor summary</b>: The paper proposes a dual-stream framework for event stream-based pattern recognition that uses differentiated fusion, Transformer, GNN, and a hybrid interaction readout mechanism to achieve state-of-the-art performance on multiple datasets.</p><hr><h3>Revisiting Backdoor Attacks against Large Vision-Language Models</h3>
<p><a href='http://arxiv.org/abs/2406.18844v1'>http://arxiv.org/abs/2406.18844v1</a></p>
<p><b>Compressor summary</b>: This paper investigates the generalizability of backdoor attacks on large vision-language models during instruction tuning and proposes modifications to improve attack effectiveness across different domains.</p><hr><h3>Disentangling Knowledge-based and Visual Reasoning by Question  Decomposition in KB-VQA</h3>
<p><a href='http://arxiv.org/abs/2406.18839v1'>http://arxiv.org/abs/2406.18839v1</a></p>
<p><b>Compressor summary</b>: The study proposes using simpler questions before retrieving visual or non-visual information to improve knowledge-based visual question-answering performance on three datasets.</p><hr><h3>Dense Monocular Motion Segmentation Using Optical Flow and Pseudo Depth  Map: A Zero-Shot Approach</h3>
<p><a href='http://arxiv.org/abs/2406.18837v1'>http://arxiv.org/abs/2406.18837v1</a></p>
<p><b>Compressor summary</b>: Our hybrid approach combines deep learning and optical flow to perform motion segmentation without training data, using object proposals, clustering, and depth maps as cues.</p><hr><h3>Zero-shot Composed Image Retrieval Considering Query-target Relationship  Leveraging Masked Image-text Pairs</h3>
<p><a href='http://arxiv.org/abs/2406.18836v1'>http://arxiv.org/abs/2406.18836v1</a></p>
<p><b>Compressor summary</b>: The paper presents a new image retrieval method using masked image-text pairs to learn the relationship between a query image and text, improving accuracy.</p><hr><h3>OutlierTune: Efficient Channel-Wise Quantization for Large Language  Models</h3>
<p><a href='http://arxiv.org/abs/2406.18832v1'>http://arxiv.org/abs/2406.18832v1</a></p>
<p><b>Compressor summary</b>: OutlierTune is an efficient method for quantizing large language models' activations by pre-executing dequantization and symmetrization, achieving better generalization and hardware efficiency.</p><hr><h3>Correspondence-Free Non-Rigid Point Set Registration Using Unsupervised  Clustering Analysis</h3>
<p><a href='http://arxiv.org/abs/2406.18817v1'>http://arxiv.org/abs/2406.18817v1</a></p>
<p><b>Compressor summary</b>: Key points:
- A novel non-rigid point set registration method based on clustering centroids and members
- Tikhonov regularization with an $\ell_1$-induced Laplacian kernel for smooth and robust displacement fields
- Clustering-improved Nystr"om method to reduce computational complexity and storage of the Gram matrix
- High accuracy, low-dimensionality, and ability to handle large deformations

Summary:
The paper proposes a new non-rigid point set registration method that uses clustering analysis, Tikhonov regularization with an $\ell_1$ kernel, and a clustering-improved Nystr"om method to achieve high accuracy, low complexity, and robustness.</p><hr><h3>Divide, Ensemble and Conquer: The Last Mile on Unsupervised Domain  Adaptation for On-Board Semantic Segmentation</h3>
<p><a href='http://arxiv.org/abs/2406.18809v1'>http://arxiv.org/abs/2406.18809v1</a></p>
<p><b>Compressor summary</b>: DEC is a flexible framework for unsupervised domain adaptation in semantic segmentation that uses synthetic multi-source datasets to improve performance on real-world datasets.</p><hr><h3>Online Stackelberg Optimization via Nonlinear Control</h3>
<p><a href='http://arxiv.org/abs/2406.18805v1'>http://arxiv.org/abs/2406.18805v1</a></p>
<p><b>Compressor summary</b>: The text proposes a unified algorithmic framework for minimizing regret in interactive problems with adaptive agents, by casting them as online control problems and analyzing their properties.</p><hr><h3>All Random Features Representations are Equivalent</h3>
<p><a href='http://arxiv.org/abs/2406.18802v1'>http://arxiv.org/abs/2406.18802v1</a></p>
<p><b>Compressor summary</b>: Random feature techniques can approximate positive-definite kernels with infinite-dimensional dot products using optimal sampling policies.</p><hr><h3>Infinite Width Models That Work: Why Feature Learning Doesn't Matter as  Much as You Think</h3>
<p><a href='http://arxiv.org/abs/2406.18800v1'>http://arxiv.org/abs/2406.18800v1</a></p>
<p><b>Compressor summary</b>: Infinite-width NTK models can access richer features than finite models, but their performance is limited by weak optimizers like SGD.</p>