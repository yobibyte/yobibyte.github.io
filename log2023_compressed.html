
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<link rel="stylesheet" href="style.css"/>
		<title>Welcome to yobihome</title>
		<a href="https://yobibyte.github.io/"><img src="./pics/socrat.png" class="center" width=50%></a>
		<h1>Learning on Graphs Conference 2023 compressed</h1>
		<p>This page contains summaries of all LoG 2023 accepted papers generated by the compressor, my personal LLM-based project.</p>
	<hr><h3>Spectral Subgraph Localization</h3>
<p>Ama Bembua Bainson, Judith Hermanns, Petros Petsinis, Niklas Aavad, Casper Dam Larsen, Tiarnan Swayne, Amit Boyarski, Davide Mottin, Alex M. Bronstein, Panagiotis Karras</p>
<p><a href='https://openreview.net/forum?id=zrOMpghV0M'>https://openreview.net/forum?id=zrOMpghV0M</a></p>
<p><b>Keywords</b>: spectral methods; subgraph localization; subgraph isomorphism; optimization
</p><p><b>Compressor summary</b>: The paper proposes a method to find the best match position of a query graph Q in a given graph G by aligning their Laplacian spectra and improves its stability using bagging strategies, while postponing the exact node correspondence task.</p><hr><h3>GwAC: GNNs with Asynchronous Communication</h3>
<p>Lukas Faber, Roger Wattenhofer</p>
<p><a href='https://openreview.net/forum?id=zffXH0sEJP'>https://openreview.net/forum?id=zffXH0sEJP</a></p>
<p><b>Keywords</b>: GNNs, Weisfeiler-Lehman, Oversmoothing, Undereaching
</p><p><b>Compressor summary</b>: The paper introduces an asynchronous communication framework for graph neural networks that preserves their expressiveness and improves performance on several graph learning tasks.</p><hr><h3>GSCAN: Graph Stability Clustering for Applications with Noise using Edge-Aware Excess-of-Mass</h3>
<p>Etzion Harari, Naphtali Abudarham, Roee Litman</p>
<p><a href='https://openreview.net/forum?id=xazYC6pGO5'>https://openreview.net/forum?id=xazYC6pGO5</a></p>
<p><b>Keywords</b>: Graph, Clustering, GNN, Unsupervised, Stability
</p><p><b>Compressor summary</b>: The paper proposes GSCAN, a graph clustering method based on node features and graph structure that maximizes cluster stability, resists outliers, and works well with Graph Neural Networks (GNN).</p><hr><h3>Latent Space Representations of Neural Algorithmic Reasoners</h3>
<p>Vladimir V Mirjanic, Razvan Pascanu, Petar Veličković</p>
<p><a href='https://openreview.net/forum?id=tRP0Ydz5nN'>https://openreview.net/forum?id=tRP0Ydz5nN</a></p>
<p><b>Keywords</b>: machine learning, graph neural networks, neural algorithmic reasoning, latent spaces, algorithms
</p><p><b>Compressor summary</b>: The paper analyzes the latent space structure of Graph Neural Networks (GNNs) for executing classical algorithms and proposes improvements to handle loss of resolution and out-of-range values using a softmax aggregator and decaying the latent space.</p><hr><h3>Topological Graph Signal Compression</h3>
<p>Guillermo Bernardez, Lev Telyatnikov, Eduard Alarcon, Albert Cabellos-Aparicio, Pere Barlet-Ros, Pietro Lio</p>
<p><a href='https://openreview.net/forum?id=rqp8NfM7Tn'>https://openreview.net/forum?id=rqp8NfM7Tn</a></p>
<p><b>Keywords</b>: Topological Deep Learning, Graph Neural Networks, Compression
</p><p><b>Compressor summary</b>: The paper introduces a new Topological Deep Learning method for compressing signals over graphs by inferring higher-order structures and passing messages within them, achieving significant improvements in reconstructing temporal link signals from real-world networks.</p><hr><h3>Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark</h3>
<p>Jan Tönshoff, Martin Ritzert, Eran Rosenbluth, Martin Grohe</p>
<p><a href='https://openreview.net/forum?id=rIUjwxc5lj'>https://openreview.net/forum?id=rIUjwxc5lj</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Message Passing, Graph Transformers, Long-Range Graph Benchmark
</p><p><b>Compressor summary</b>: Graph Transformers and Message Passing GNNs have similar performance on long-range interaction tasks when properly optimized, and some issues were found in LRGB's datasets and metric.</p><hr><h3>Representing Edge Flows on Graphs via Sparse Cell Complexes</h3>
<p>Josef Hoppe, Michael T Schaub</p>
<p><a href='https://openreview.net/forum?id=qix189lq5D'>https://openreview.net/forum?id=qix189lq5D</a></p>
<p><b>Keywords</b>: graph signal processing, topological signal processing, cell complexes, topology inference
</p><p><b>Compressor summary</b>: The paper proposes a method to obtain sparse and interpretable representations of edge flows in graphs using cellular complexes and an efficient approximation algorithm for the resulting flow representation learning problem.</p><hr><h3>Multicoated and Folded Graph Neural Networks with Strong Lottery Tickets</h3>
<p>Jiale Yan, Hiroaki Ito, Ángel López García-Arias, Yasuyuki Okoshi, Hikari Otsuka, Kazushi Kawamura, Thiem Van Chu, Masato Motomura</p>
<p><a href='https://openreview.net/forum?id=oLrNolMbO8'>https://openreview.net/forum?id=oLrNolMbO8</a></p>
<p><b>Keywords</b>: Graph neural networks, Lottery ticket hypothesis, Recurrent neural networks, Pruning
</p><p><b>Compressor summary</b>: This paper explores subnetworks in graph neural networks (GNNs) using pruning methods and shows that sparse GNNs can achieve competitive performance and high memory efficiency.</p><hr><h3>PyTorch Geometric Signed Directed: A Software Package on Graph Neural Networks for Signed and Directed Graphs</h3>
<p>Yixuan He, Xitong Zhang, Junjie Huang, Benedek Rozemberczki, Mihai Cucuringu, Gesine Reinert</p>
<p><a href='https://openreview.net/forum?id=mni7vnYmvY'>https://openreview.net/forum?id=mni7vnYmvY</a></p>
<p><b>Keywords</b>: graph neural networks, open-source library, signed networks, directed networks, machine learning
</p><p><b>Compressor summary</b>: The paper introduces PyTorch Geometric Signed Directed (PyGSD), a software package for graph neural networks on signed and directed networks, with easy-to-use models, data, metrics, and evaluation methods.</p><hr><h3>SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics</h3>
<p>Stefan Künzli, Florian Grötschla, Joël Mathys, Roger Wattenhofer</p>
<p><a href='https://openreview.net/forum?id=lf22LaheVr'>https://openreview.net/forum?id=lf22LaheVr</a></p>
<p><b>Keywords</b>: generalization, fluid dynamics, benchmark, physics simulation
</p><p><b>Compressor summary</b>: SURF is a benchmark to test the generalization of learned graph-based fluid simulators by providing performance and generalization metrics for evaluating different models.</p><hr><h3>Transformers over Directed Acyclic Graphs</h3>
<p>Yuankai Luo, Veronika Thost, Lei Shi</p>
<p><a href='https://openreview.net/forum?id=kkOSWva0Fx'>https://openreview.net/forum?id=kkOSWva0Fx</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Transformers, Graph Classification, Node Classification, Scalability
</p><p><b>Compressor summary</b>: The paper proposes efficient attention and positional encoding adaptations for transformer models on directed acyclic graphs (DAGs) that improve their performance over other methods.</p><hr><h3>Geometric instability of graph neural networks on large graphs</h3>
<p>Borun Shi, Emily Morris, Haotian Shen, Weiling Du, Muhammad Hamza Sajjad</p>
<p><a href='https://openreview.net/forum?id=kQHZfyL2XM'>https://openreview.net/forum?id=kQHZfyL2XM</a></p>
<p><b>Keywords</b>: embedding instability, geometric instability, large graphs, graph neural networks
</p><p><b>Compressor summary</b>: The paragraph discusses a new method (Graph Gram Index) to measure geometric instability in graph neural network embeddings, which is invariant to various transformations and can be used on large graphs.</p><hr><h3>FreshGNN: Reducing Memory Access via Stable Historical Embeddings for Graph Neural Network Training</h3>
<p>Kezhao Huang, Haitian Jiang, Minjie Wang, Guangxuan Xiao, David Wipf, Xiang song, Quan Gan, Zengfeng Huang, Jidong Zhai, Zheng Zhang</p>
<p><a href='https://openreview.net/forum?id=iyJjEkU0Ve'>https://openreview.net/forum?id=iyJjEkU0Ve</a></p>
<p><b>Keywords</b>: GNN; Performance; Data loading;
</p><p><b>Compressor summary</b>: The system is a framework for training GNN models faster and more efficiently by using a historical cache of node embeddings instead of recomputing them from scratch every time.</p><hr><h3>Point-wise Activations and Steerable Convolutional Networks</h3>
<p>Marco Pacini, Xiaowen Dong, Bruno Lepri, Gabriele Santin</p>
<p><a href='https://openreview.net/forum?id=gsJPYzdA0S'>https://openreview.net/forum?id=gsJPYzdA0S</a></p>
<p><b>Keywords</b>: Deep Learning, Equivariant Neural Networks, Steerable Neural Networks
</p><p><b>Compressor summary</b>: The paper investigates when point-wise activations can be used in equivariant neural networks and shows their limitations, highlighting the need for more research on better activation functions.</p><hr><h3>Meta-Path Learning for Multi-relational Graph Neural Networks</h3>
<p>Francesco Ferrini, Antonio Longa, Andrea Passerini, Manfred Jaeger</p>
<p><a href='https://openreview.net/forum?id=gW9ZmT9hAe'>https://openreview.net/forum?id=gW9ZmT9hAe</a></p>
<p><b>Keywords</b>: Graph Neural Network, Meta-path, Knowledge graph
</p><p><b>Compressor summary</b>: The paper proposes a new method for learning informative relations in graph neural networks using a scoring function and a small set of meta-paths, which performs better than existing approaches on various datasets.</p><hr><h3>Three Revisits to Node-Level Graph Anomaly Detection: Outliers, Message Passing and Hyperbolic Neural Networks</h3>
<p>Jing Gu, Dongmian Zou</p>
<p><a href='https://openreview.net/forum?id=fNsU9gi1Fy'>https://openreview.net/forum?id=fNsU9gi1Fy</a></p>
<p><b>Keywords</b>: anomaly detection, graph, message passing, hyperbolic neural networks
</p><p><b>Compressor summary</b>: The paper proposes new methods and comparisons for detecting abnormal instances in complex networks using deep learning techniques and hyperbolic neural networks.</p><hr><h3>HOT: Higher-Order Dynamic Graph Representation Learning with Efficient Transformers</h3>
<p>Maciej Besta, Afonso Claudino Catarino, Lukas Gianinazzi, Nils Blach, Piotr Nyczyk, Hubert Niewiadomski, Torsten Hoefler</p>
<p><a href='https://openreview.net/forum?id=edAX8h5mdA'>https://openreview.net/forum?id=edAX8h5mdA</a></p>
<p><b>Keywords</b>: Dynamic Graph Representation Learning, Higher-Order Graph Representation Learning, Transformer, Block-Recurrent Transformer
</p><p><b>Compressor summary</b>: The paper proposes HOT, a model that uses higher-order graph structures to improve dynamic link prediction accuracy while minimizing memory usage by imposing hierarchy on the attention matrix of a Transformer.</p><hr><h3>Enhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation</h3>
<p>Vishal Dey, Xia Ning</p>
<p><a href='https://openreview.net/forum?id=eR7wBTSF2u'>https://openreview.net/forum?id=eR7wBTSF2u</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Auxiliary Learning, Molecular Property Prediction, Transfer Learning, Adaptation
</p><p><b>Compressor summary</b>: The authors propose methods to adapt pretrained Graph Neural Networks (GNNs) for molecular property prediction tasks by jointly training them with multiple auxiliary tasks, which can improve generalization and suggest future research directions.</p><hr><h3>Explaining Link Predictions in Knowledge Graph Embedding Models with Influential Examples</h3>
<p>Adrianna Janik, Luca Costabello</p>
<p><a href='https://openreview.net/forum?id=eOwYHXDaHn'>https://openreview.net/forum?id=eOwYHXDaHn</a></p>
<p><b>Keywords</b>: explainable ML, link prediction, knowledge graph embeddings
</p><p><b>Compressor summary</b>: The article proposes an example-based method to generate explanations for link predictions in Knowledge Graph Embedding models using the latent space representation of nodes and edges.</p><hr><h3>Generalized Reasoning with Graph Neural Networks by Relational Bayesian Network Encodings</h3>
<p>Raffaele Pojer, Andrea Passerini, Manfred Jaeger</p>
<p><a href='https://openreview.net/forum?id=dxhasYAMQ4'>https://openreview.net/forum?id=dxhasYAMQ4</a></p>
<p><b>Keywords</b>: Graph neural networks, statistical relational learning, relational Bayesian networks, neuro-symbolic integration, explanation
</p><p><b>Compressor summary</b>: The paper proposes embedding graph neural networks in a statistical relational learning framework for generating models that support various queries and provide explanations for graph data.</p><hr><h3>Non-Isotropic Persistent Homology: Leveraging the Metric Dependency of PH</h3>
<p>Vincent Peter Grande, Michael T Schaub</p>
<p><a href='https://openreview.net/forum?id=cewQK9Sjvh'>https://openreview.net/forum?id=cewQK9Sjvh</a></p>
<p><b>Keywords</b>: Topology, Point Clouds, Geometry Processing, Persistent Homology, Metric Spaces, Simplicial Complexes, Optimal Transport
</p><p><b>Compressor summary</b>: The paper proposes using different distance functions for persistent homology analysis to reveal more topological and geometrical information from point clouds.</p><hr><h3>Transferable Hypergraph Neural Networks via Spectral Similarity</h3>
<p>Mikhail Hayhoe, Hans Matthew Riess, Michael M. Zavlanos, VICTOR PRECIADO, Alejandro Ribeiro</p>
<p><a href='https://openreview.net/forum?id=cHuii4NOB9'>https://openreview.net/forum?id=cHuii4NOB9</a></p>
<p><b>Keywords</b>: hypergraphs, graph neural networks, graph signal processing, spectral graph theory, hypergraph Laplacian, graph diffusion
</p><p><b>Compressor summary</b>: The text introduces a new neural network model called HENNs that can process signals on hypergraphs using graph neural networks, and shows its effectiveness in transferring knowledge between multiple graph representations.</p><hr><h3>Asynchronous Algorithmic Alignment with Cocycles</h3>
<p>Andrew Joseph Dudzik, Tamara von Glehn, Razvan Pascanu, Petar Veličković</p>
<p><a href='https://openreview.net/forum?id=ba4bbZ4KoF'>https://openreview.net/forum?id=ba4bbZ4KoF</a></p>
<p><b>Keywords</b>: algorithmic reasoning, graph neural networks, category theory, bellman-ford, commutative monoids, idempotence, cocycles, monoid homomorphisms, dynamic programming
</p><p><b>Compressor summary</b>: The paper proposes a way to improve neural algorithmic reasoners by separating node state update and message function invocation in graph neural networks, enabling asynchronous computation and reducing irrelevant data transmission.</p><hr><h3>Mitigating Over-smoothing and Over-squashing using Augmentations of Forman-Ricci Curvature</h3>
<p>Lukas Fesser, Melanie Weber</p>
<p><a href='https://openreview.net/forum?id=bKTkZMRtfC'>https://openreview.net/forum?id=bKTkZMRtfC</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Discrete Curvature, Over-smoothing, Over-squashing
</p><p><b>Compressor summary</b>: The paper introduces a new rewiring technique for graph neural networks that uses scalable Augmented Forman-Ricci curvature to characterize and mitigate over-smoothing and over-squashing effects, achieving state-of-the-art performance with reduced computational cost.</p><hr><h3>Extending Graph Neural Networks with Global Features</h3>
<p>Andrei Dragos Brasoveanu, Fabian Jogl, Pascal Welke, Maximilian Thiessen</p>
<p><a href='https://openreview.net/forum?id=aisVQy6R2k'>https://openreview.net/forum?id=aisVQy6R2k</a></p>
<p><b>Keywords</b>: graph neural networks, message passing neural networks, expressivity, topological index
</p><p><b>Compressor summary</b>: The authors propose a method to enhance message passing graph neural networks by incorporating global graph features, which they show can improve predictive performance on molecular benchmark datasets.</p><hr><h3>Demystifying Oversmoothing in Attention-Based Graph Neural Networks</h3>
<p>Xinyi Wu, Amir Ajorlou, Zihui Wu, Ali Jadbabaie</p>
<p><a href='https://openreview.net/forum?id=aTw3Mu2VA2'>https://openreview.net/forum?id=aTw3Mu2VA2</a></p>
<p><b>Keywords</b>: graph neural networks, oversmoothing, dynamical systems, representation power, theory
</p><p><b>Compressor summary</b>: This paper studies how the graph attention mechanism in Graph Neural Networks affects oversmoothing and expressive power, using tools from matrix theory and showing that it cannot prevent oversmoothing.</p><hr><h3>Interaction Models and Generalized Score Matching for Compositional Data</h3>
<p>Shiqing Yu, Mathias Drton, Ali Shojaie</p>
<p><a href='https://openreview.net/forum?id=aRUhkrf0W4'>https://openreview.net/forum?id=aRUhkrf0W4</a></p>
<p><b>Keywords</b>: Compositional data, Graphical model, High-dimensional statistics, Interaction, Sparsity
</p><p><b>Compressor summary</b>: The authors propose a class of exponential family models for compositional data with pairwise interactions and develop estimation methods using generalized score matching.</p><hr><h3>Will More Expressive Graph Neural Networks do Better on Generative Tasks?</h3>
<p>Xiandong Zou, Xiangyu Zhao, Pietro Lio, Yiren Zhao</p>
<p><a href='https://openreview.net/forum?id=aBL9SfWVJb'>https://openreview.net/forum?id=aBL9SfWVJb</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Expressiveness, Graph Generative Models, De-novo Molecular Design
</p><p><b>Compressor summary</b>: The authors explore different Graph Neural Networks (GNNs) for improving molecular graph generation tasks and compare their performance with various generative models and metrics.</p><hr><h3>How Faithful are Self-Explainable GNNs?</h3>
<p>Marc Christiansen, Lea Villadsen, Zhiqiang Zhong, Stefano Teso, Davide Mottin</p>
<p><a href='https://openreview.net/forum?id=ZS2t7ZSh8E'>https://openreview.net/forum?id=ZS2t7ZSh8E</a></p>
<p><b>Keywords</b>: deep learning, explainability, graph neural networks, self-explainable models, concepts
</p><p><b>Compressor summary</b>: Self-explainable graph neural networks aim to provide faithful explanations for their reasoning on graph data, but face challenges in fulfilling this goal and improving the evaluation of these models.</p><hr><h3>Inferring dynamic regulatory interaction graphs from time series data with perturbations</h3>
<p>Dhananjay Bhaskar, Daniel Sumner Magruder, Edward De Brouwer, Aarthi Venkat, Frederik Wenkel, Matheo Morales, Guy Wolf, Smita Krishnaswamy</p>
<p><a href='https://openreview.net/forum?id=ZObhwMbBA9'>https://openreview.net/forum?id=ZObhwMbBA9</a></p>
<p><b>Keywords</b>: regulatory network inference, graph ODE, attention, dynamics
</p><p><b>Compressor summary</b>: RiTINI is a novel method for inferring dynamic interaction graphs in complex systems using space-and-time graph attentions and graph neural ODEs, outperforming previous methods on various simulations.</p><hr><h3>Explaining Unfairness in GNN-based Recommendation</h3>
<p>Ludovico Boratto, Francesco Fabbri, Gianni Fenu, Mirko Marras, Giacomo Medda</p>
<p><a href='https://openreview.net/forum?id=YuOwqCnPIc'>https://openreview.net/forum?id=YuOwqCnPIc</a></p>
<p><b>Keywords</b>: Recommender Systems, User Fairness, Explanation, Graph Neural Networks, Counterfactual Reasoning
</p><p><b>Compressor summary</b>: The paper proposes a new algorithm that uses counterfactual methods to explain user unfairness in graph neural network-based recommendation systems by perturbing the graph structure.</p><hr><h3>Intrinsically Motivated Graph Exploration Using Network Theories of Human Curiosity</h3>
<p>Shubhankar Prashant Patankar, mathieu ouellet, Juan Cervino, Alejandro Ribeiro, Kieran A. Murphy, Danielle Bassett</p>
<p><a href='https://openreview.net/forum?id=XJpQnN4JNE'>https://openreview.net/forum?id=XJpQnN4JNE</a></p>
<p><b>Keywords</b>: intrinsic motivations, human curiosity, reinforcement learning, graph neural networks
</p><p><b>Compressor summary</b>: The paper proposes a new method for exploring graph-structured data based on human curiosity theories, which improves reinforcement learning and recommender system performance.</p><hr><h3>RegExplainer: Generating Explanations for Graph Neural Networks in Regression Tasks</h3>
<p>Jiaxing Zhang, Zhuomin Chen, hao mei, Dongsheng Luo, Hua Wei</p>
<p><a href='https://openreview.net/forum?id=WZUH0fMbzb'>https://openreview.net/forum?id=WZUH0fMbzb</a></p>
<p><b>Keywords</b>: Graph Neural Networks, Graph Explanation, Graph Regression
</p><p><b>Compressor summary</b>: The paper proposes XAIG-R, a novel explanation method for graph regression models that addresses distribution shifting, continuous decision boundary issues, and uses information bottleneck theory, mix-up framework, and contrastive learning to support various GNNs.</p><hr><h3>WL meet VC</h3>
<p>Christopher Morris, Floris Geerts, Jan Tönshoff, Martin Grohe</p>
<p><a href='https://openreview.net/forum?id=WYWU9aZmkX'>https://openreview.net/forum?id=WYWU9aZmkX</a></p>
<p><b>Keywords</b>: GNNs, generalization, expressivity, Weisfeiler-Leman, VC dimension
</p><p><b>Compressor summary</b>: The paper explores how graph neural networks' generalization performance can be analyzed using Vapnik-Chervonenkis dimension theory, linking it to the Weisfeiler-Leman algorithm and deriving upper bounds based on the number of colors or distinguishable graphs.</p><hr><h3>EMP: Effective Multidimensional Persistence for Graph Representation Learning</h3>
<p>Yuzhou Chen, Ignacio Segovia-Dominguez, Cuneyt Gurcan Akcora, Zhiwei Zhen, Murat Kantarcioglu, Yulia Gel, Baris Coskunuzer</p>
<p><a href='https://openreview.net/forum?id=WScCJnX4ek'>https://openreview.net/forum?id=WScCJnX4ek</a></p>
<p><b>Keywords</b>: multiparameter persistence, persistent homology, topological data analysis, graph classification, graph representation learning
</p><p><b>Compressor summary</b>: The Effective Multidimensional Persistence framework allows for the simultaneous analysis of data using multiple scale parameters, providing a more comprehensive and expressive summary that improves performance in graph classification tasks.</p><hr><h3>The Self-loop Paradox: Investigating the Impact of Self-Loops on Graph Neural Networks</h3>
<p>Moritz Lampert, Ingo Scholtes</p>
<p><a href='https://openreview.net/forum?id=Urf6G7rk8A'>https://openreview.net/forum?id=Urf6G7rk8A</a></p>
<p><b>Keywords</b>: GNNs, Message Passing, Self-loops, Node Classification, Graph Ensembles
</p><p><b>Compressor summary</b>: The paper studies how the presence of self-loops in graph neural networks affects information flow, especially in odd vs even layers, and calls this effect the "self-loop paradox".</p><hr><h3>Over-squashing in Riemannian Graph Neural Networks</h3>
<p>Julia Balla</p>
<p><a href='https://openreview.net/forum?id=UUnYi0yLcM'>https://openreview.net/forum?id=UUnYi0yLcM</a></p>
<p><b>Keywords</b>: Graph Neural Network, Over-squashing, Riemannian Manifold, Graph Embedding
</p><p><b>Compressor summary</b>: The paper explores using Riemannian manifolds with variable curvature to reduce over-squashing in graph neural networks by preserving the geometry of the graph's topology.</p><hr><h3>Beyond Erdos-Renyi: Generalization in Algorithmic Reasoning on Graphs</h3>
<p>Dobrik Georgiev Georgiev, Pietro Lio, Jakub Bachurski, Junhua Chen, Tunan Shi, Lorenzo Giusti</p>
<p><a href='https://openreview.net/forum?id=TTxQAkg9QG'>https://openreview.net/forum?id=TTxQAkg9QG</a></p>
<p><b>Keywords</b>: graph neural networks, algorithmic reasoning
</p><p><b>Compressor summary</b>: This study explores how well neural algorithmic reasoning generalizes to different graph distributions, finding that selecting source distributions based on Tree Mover's Distance can help.</p><hr><h3>Edge Directionality Improves Learning on Heterophilic Graphs</h3>
<p>Emanuele Rossi, Bertrand Charpentier, Francesco Di Giovanni, Fabrizio Frasca, Stephan Günnemann, Michael M. Bronstein</p>
<p><a href='https://openreview.net/forum?id=T4LRbAMWFn'>https://openreview.net/forum?id=T4LRbAMWFn</a></p>
<p><b>Keywords</b>: graph neural networks, directed graphs, heterophily, node classification, graphs, geometric deep learning
</p><p><b>Compressor summary</b>: The paper introduces Dir-GNN, a framework for deep learning on directed graphs that leverages edge directionality information to improve performance on heterophilic datasets.</p><hr><h3>Dynamic Hyper-graph Regularised Non-negative Matrix Factorisation</h3>
<p>Nasr Ullah Khan, Luke Dickens</p>
<p><a href='https://openreview.net/forum?id=SFFs9AtGSi'>https://openreview.net/forum?id=SFFs9AtGSi</a></p>
<p><b>Keywords</b>: Dynamic link prediction, dynamic graphs, hyper-graphs, graph regularization, non-negative matrix factorization, graph machine learning, time series analysis
</p><p><b>Compressor summary</b>: Dynamic hypergraph methods use recent observations and provide more accurate predictions of relationships between entities than traditional dynamic uni-graph approaches.</p><hr><h3>A Simple Latent Variable Model for Graph Learning and Inference</h3>
<p>Manfred Jaeger, Antonio Longa, Steve Azzolin, Oliver Schulte, Andrea Passerini</p>
<p><a href='https://openreview.net/forum?id=S9jem2KZVr'>https://openreview.net/forum?id=S9jem2KZVr</a></p>
<p><b>Keywords</b>: Stochastic block model, graphon, latent variable model, generative models
</p><p><b>Compressor summary</b>: The histogram AHK model is a simple and versatile probabilistic latent variable model for graphs that can handle complex predictive inference and graph generation, and it generalizes both graphons and stochastic block models.</p><hr><h3>Sampling Networks from Modular Compression of Network Flows</h3>
<p>Christopher Blöcker, Jelena Smiljanić, Martin Rosvall, Ingo Scholtes</p>
<p><a href='https://openreview.net/forum?id=Pz5UCXAoV6'>https://openreview.net/forum?id=Pz5UCXAoV6</a></p>
<p><b>Keywords</b>: flow community, network model, benchmark
</p><p><b>Compressor summary</b>: The authors propose a new method to generate networks based on dynamic processes that captures structural characteristics like degree distribution and community structure.</p><hr><h3>SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning</h3>
<p>Julian Minder, Florian Grötschla, Joël Mathys, Roger Wattenhofer</p>
<p><a href='https://openreview.net/forum?id=PRapGjDGFQ'>https://openreview.net/forum?id=PRapGjDGFQ</a></p>
<p><b>Keywords</b>: algorithmic reasoning, benchmark, generalisation, scalability
</p><p><b>Compressor summary</b>: The paragraph describes an extension to the CLRS algorithmic learning benchmark called SALSA-CLRS that focuses on scalability and sparseness, with adapted and new problems from distributed and randomized algorithms.</p><hr><h3>Geodesic Distributions Reveal How Heterophily and Bottlenecks Limit the Expressive Power of Message Passing Neural Networks</h3>
<p>Jonathan Rubin, Sahil Loomba, Nick S. Jones</p>
<p><a href='https://openreview.net/forum?id=PEVln6psEH'>https://openreview.net/forum?id=PEVln6psEH</a></p>
<p><b>Keywords</b>: message passing neural networks, expressive power, statistical graph ensembles, graph geodesic length distribution, graph bottlenecks, heterophily
</p><p><b>Compressor summary</b>: The paper proposes a statistical approach to analyse how heterophily and bottlenecking influence the expressiveness of MMPNs in node classification, introduces the concept of "homophilic bottlenecking", and derives bounds on it using random graphs.</p><hr><h3>KGEx: Explaining Knowledge Graph Embeddings Via Subgraph Sampling and Knowledge Distillation</h3>
<p>Vasileios Baltatzis, Luca Costabello</p>
<p><a href='https://openreview.net/forum?id=NSXXSyc2DF'>https://openreview.net/forum?id=NSXXSyc2DF</a></p>
<p><b>Keywords</b>: knowledge graph embeddings, explainable AI
</p><p><b>Compressor summary</b>: KGEx is a method for explaining link predictions in knowledge graph embeddings by training surrogate models on different subsets of the target triple's neighborhood and selecting important triples based on their impact on the prediction accuracy.</p><hr><h3>Neural Algorithmic Reasoning for Combinatorial Optimisation</h3>
<p>Dobrik Georgiev Georgiev, Danilo Numeroso, Davide Bacciu, Pietro Lio</p>
<p><a href='https://openreview.net/forum?id=N8awTT5ep7'>https://openreview.net/forum?id=N8awTT5ep7</a></p>
<p><b>Keywords</b>: Neural Algorithmic Reasoning, Neural Combinatorial Optimisation, Graph Neural Networks
</p><p><b>Compressor summary</b>: The paper proposes a new approach to solve NP-hard problems using neural networks pre-trained on relevant algorithms, which outperforms traditional methods and non-algorithmically informed deep learning models.</p><hr><h3>A Latent Diffusion Model for Protein Structure Generation</h3>
<p>Cong Fu, Keqiang Yan, Limei Wang, Wing Yee Au, Michael Curtis McThrow, Tao Komikado, Koji Maruhashi, Kanji Uchino, Xiaoning Qian, Shuiwang Ji</p>
<p><a href='https://openreview.net/forum?id=MBZVrtbi06'>https://openreview.net/forum?id=MBZVrtbi06</a></p>
<p><b>Keywords</b>: protein generation, equivariant, 3D protein autoencoder
</p><p><b>Compressor summary</b>: The proposed latent diffusion model simplifies protein modeling by capturing natural protein structure distributions in a condensed latent space, enabling efficient generation of new protein backbone structures for synthetic biology applications.</p><hr><h3>Accelerating Molecular Graph Neural Networks via Knowledge Distillation</h3>
<p>Filip Ekström Kelvinius, Dimitar Georgiev, Artur Toshev, Johannes Gasteiger</p>
<p><a href='https://openreview.net/forum?id=KWkzecJ4or'>https://openreview.net/forum?id=KWkzecJ4or</a></p>
<p><b>Keywords</b>: GNN, graph neural networks, knowledge distillation, molecules, molecular simulations
</p><p><b>Compressor summary</b>: The paper explores using knowledge distillation to accelerate molecular graph neural networks without sacrificing predictive accuracy or inference speed.</p><hr><h3>United We Stand, Divided We Fall: Networks to Graph (N2G) Abstraction for Robust Graph Classification under Graph Label Corruption</h3>
<p>Zhiwei Zhen, Yuzhou Chen, Murat Kantarcioglu, Yulia Gel, Kangkook Jee</p>
<p><a href='https://openreview.net/forum?id=K5g021Ex14'>https://openreview.net/forum?id=K5g021Ex14</a></p>
<p><b>Keywords</b>: Representation Learning, Classification
</p><p><b>Compressor summary</b>: The paper proposes a new representation method called N2G that improves the robustness and performance of graph neural networks for graph classification tasks with noisy labels.</p><hr><h3>Parallel Algorithms Align with Neural Execution</h3>
<p>Valerie Engelmayer, Dobrik Georgiev Georgiev, Petar Veličković</p>
<p><a href='https://openreview.net/forum?id=IC6kpv87LB'>https://openreview.net/forum?id=IC6kpv87LB</a></p>
<p><b>Keywords</b>: Parallel Algorithms, Neural Algorithmic Reasoning, Graph Neural Networks
</p><p><b>Compressor summary</b>: Parallel algorithms for neural reasoners use fewer layers and train faster with better results than sequential ones.</p><hr><h3>Generative modeling of labeled graphs under data scarcity</h3>
<p>Sahil Manchanda, Shubham Gupta, Sayan Ranu, Srikanta J. Bedathur</p>
<p><a href='https://openreview.net/forum?id=Hy9K2WiVwW'>https://openreview.net/forum?id=Hy9K2WiVwW</a></p>
<p><b>Keywords</b>: Labeled Graph Generative modeling, Data scarcity, Meta-Learning
</p><p><b>Compressor summary</b>: The paper proposes a meta-learning based framework for generating graphs under data scarcity conditions, which transfers knowledge from similar auxiliary datasets and adapts to unseen graphs through self-paced fine-tuning.</p><hr><h3>Kùzu: Graph Learning Applications Need a Modern Graph DBMS</h3>
<p>ziyi Chen, Xiyang Feng, Guodong Jin, Chang Liu, Semih Salihoglu</p>
<p><a href='https://openreview.net/forum?id=Eg3MthXzeT'>https://openreview.net/forum?id=Eg3MthXzeT</a></p>
<p><b>Keywords</b>: graph database, graph database management system, systems for graph learning
</p><p><b>Compressor summary</b>: Building a graph learning application requires performing a series of data processing steps, such as extracting data from tabular sources into a graph, cleaning the graph, extracting node/edge features, moving the data into a graph learning library to generate embeddings, and possibly saving these embeddings in a software for further processing. Many of these steps can be performed in an efficient way by database management systems (DBMSs), which come with high-level data models and query languages, and functionalities to export datasets into other formats. However, no current DBMS is tailored for graph learning pipelines. We present Kùzu, an open-sourced graph DBMS that aims to fill this gap. Kùzu is an embeddable system that runs as part of users' applications, implements the property graph data model and the openCypher query language, a graph-optimized storage structures, and join algorithms. Kùzu can ingest data from several tabular raw file formats and export data to popular graph learning libraries. We present Kùzu's design goals, architecture, our ongoing work, and demonstrate how it can be used to train large GNN models that do not fit into main memory. Kùzu is available under a permissive license.</p><hr><h3>Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond</h3>
<p>Oleg Platonov, Denis Kuznedelev, Artem Babenko, Liudmila Prokhorenkova</p>
<p><a href='https://openreview.net/forum?id=D4GLZkTphJ'>https://openreview.net/forum?id=D4GLZkTphJ</a></p>
<p><b>Keywords</b>: homophily, heterophily, adjusted homophily, label informativeness, constant baseline, GNN
</p><p><b>Compressor summary</b>: The authors propose adjusted homophily as a superior measure of node similarity in graphs, and introduce label informativeness as a new characteristic to distinguish different types of heterophily.</p><hr><h3>MUDiff: Unified Diffusion for Complete Molecule Generation</h3>
<p>Chenqing Hua, Sitao Luan, Minkai Xu, Zhitao Ying, Jie Fu, Stefano Ermon, Doina Precup</p>
<p><a href='https://openreview.net/forum?id=C7Z3yhWUAU'>https://openreview.net/forum?id=C7Z3yhWUAU</a></p>
<p><b>Keywords</b>: Molecule Generation; Graph Neural Network
</p><p><b>Compressor summary</b>: The paper presents a new model that combines discrete and continuous diffusion processes to generate a comprehensive representation of molecules, including atom features, 2D structures, and 3D coordinates, and uses a novel graph transformer to denoise the process and learn invariant representations.</p><hr><h3>On the Robustness of Post-hoc GNN Explainers to Label Noise</h3>
<p>Zhiqiang Zhong, Yangqianzi Jiang, Davide Mottin</p>
<p><a href='https://openreview.net/forum?id=BWZnVy021e'>https://openreview.net/forum?id=BWZnVy021e</a></p>
<p><b>Keywords</b>: Post-hoc Graph Neural Network Explainers, Robustness, Label Noise
</p><p><b>Compressor summary</b>: The text discusses the limitations and susceptibility of post-hoc graph neural network explainers under label noise conditions.</p><hr><h3>HEAL: Unlocking the Potential of Learning on Hypergraphs Enriched with Attributes and Layers</h3>
<p>Naganand Yadati, Tarun Kumar, Deepak Maurya, Balaraman Ravindran, Partha Talukdar</p>
<p><a href='https://openreview.net/forum?id=BUj4BqjGC3'>https://openreview.net/forum?id=BUj4BqjGC3</a></p>
<p><b>Keywords</b>: Hypergraphs, Multi-layer Graphs, Feature Smoothing
</p><p><b>Compressor summary</b>: The paper introduces HEAL, a novel hypergraph learning framework that leverages attribute-rich and multi-layered structures to effectively model complex relationships in real-world systems.</p><hr><h3>Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks</h3>
<p>Andreas Roth, Thomas Liebig</p>
<p><a href='https://openreview.net/forum?id=9aIDdGm7a6'>https://openreview.net/forum?id=9aIDdGm7a6</a></p>
<p><b>Keywords</b>: Graph-based Learning, graph neural networks, over-smoothing, over-correlation, expressivity, rank collapse
</p><p><b>Compressor summary</b>: The study explains why deep graph neural networks can have problems with over-smoothing and feature over-correlation, and suggests using a sum of Kronecker products to avoid these issues.</p><hr><h3>Advection Diffusion Reaction Graph Neural Networks for Spatio-Temporal Data</h3>
<p>Moshe Eliasof, Eldad Haber, Eran Treister</p>
<p><a href='https://openreview.net/forum?id=8jCpJE3ugQ'>https://openreview.net/forum?id=8jCpJE3ugQ</a></p>
<p><b>Keywords</b>: Advection, Diffusion, Reaction, Temporal, PDE, ODE
</p><p><b>Compressor summary</b>: The paper introduces a new GNN model for learning from graph-structured data with advection, diffusion, and reaction processes.</p><hr><h3>Maximally Expressive GNNs for Outerplanar Graphs</h3>
<p>Franka Bause, Fabian Jogl, Pascal Welke, Maximilian Thiessen</p>
<p><a href='https://openreview.net/forum?id=7vyGCFTajk'>https://openreview.net/forum?id=7vyGCFTajk</a></p>
<p><b>Keywords</b>: outerplanar graphs, message passing neural networks, expressivity, Weisfeiler-Leman, graph transformation
</p><p><b>Compressor summary</b>: The authors propose a fast linear-time graph transformation method to enhance expressivity of graph neural networks and distinguish pharmaceutical graphs based on their outerplanar structure.</p><hr><h3>Cycle Invariant Positional Encoding for Graph Representation Learning</h3>
<p>Zuoyu Yan, Tengfei Ma, Liangcai Gao, Zhi Tang, Chao Chen, Yusu Wang</p>
<p><a href='https://openreview.net/forum?id=7BQZyQERuP'>https://openreview.net/forum?id=7BQZyQERuP</a></p>
<p><b>Keywords</b>: permutation invariance, algebraic topology, hodge laplacian, computational topology
</p><p><b>Compressor summary</b>: The paper proposes CycleNet, a structure encoding module for graph neural networks that encodes cycle information using edge structure encoding in a permutation invariant manner and shows its effectiveness in various benchmarks.</p><hr><h3>Triplet Edge Attention for Algorithmic Reasoning</h3>
<p>Yeonjoon Jung, Sungsoo Ahn</p>
<p><a href='https://openreview.net/forum?id=6CCR9gCKGd'>https://openreview.net/forum?id=6CCR9gCKGd</a></p>
<p><b>Keywords</b>: graph neural network, algorithmic reasoning
</p><p><b>Compressor summary</b>: The paper proposes Triplet Edge Attention (TEA), a new graph neural network layer that improves learning from classical algorithms by paying attention to edges and achieving better results on CLRS benchmarks.</p><hr><h3>Semi-Supervised Learning for High-Fidelity Fluid Flow Reconstruction</h3>
<p>Cong Fu, Jacob Helwig, Shuiwang Ji</p>
<p><a href='https://openreview.net/forum?id=695IYJh1Ba'>https://openreview.net/forum?id=695IYJh1Ba</a></p>
<p><b>Keywords</b>: physical simulation, fluid flow reconstruction
</p><p><b>Compressor summary</b>: The proposed cascaded fluid reconstruction framework combines low-resolution and high-resolution simulations to improve accuracy and efficiency in fluid dynamics analysis, using a proposal network and a ModeFormer transformer for refinement.</p><hr><h3>BeMap: Balanced Message Passing for Fair Graph Neural Network</h3>
<p>Xiao Lin, Jian Kang, Weilin Cong, Hanghang Tong</p>
<p><a href='https://openreview.net/forum?id=4RiLDrCbzW'>https://openreview.net/forum?id=4RiLDrCbzW</a></p>
<p><b>Keywords</b>: group fairness, graph neural network, message passing
</p><p><b>Compressor summary</b>: The paper studies how message passing can amplify bias in graph neural networks and proposes a method called BeMap that balances the number of 1-hop neighbors for fairness.</p><hr><h3>Recursive Algorithmic Reasoning</h3>
<p>Jonas Jürß, Dulhan Hansaja Jayalath, Petar Veličković</p>
<p><a href='https://openreview.net/forum?id=43M1bPorxU'>https://openreview.net/forum?id=43M1bPorxU</a></p>
<p><b>Keywords</b>: graph neural networks, algorithmic reasoning
</p><p><b>Compressor summary</b>: The paper presents methods to enhance graph neural networks with a stack and sampling techniques, enabling them to execute recursive algorithms and better generalize to out-of-distribution data.</p><hr><h3>On Performance Discrepancies Across Local Homophily Levels in Graph Neural Networks</h3>
<p>Donald Loveland, Jiong Zhu, Mark Heimann, Benjamin Fish, Michael T Schaub, Danai Koutra</p>
<p><a href='https://openreview.net/forum?id=3sPJt65hzO'>https://openreview.net/forum?id=3sPJt65hzO</a></p>
<p><b>Keywords</b>: graph neural network, heterophily, discrepancy
</p><p><b>Compressor summary</b>: The authors study how local homophily levels affect the performance of graph neural networks (GNNs) in node classification and show that GNNs designed for heterophilous graphs can improve performance across different homophily settings.</p><hr><h3>Rethinking Higher-order Representation Learning with Graph Neural Networks</h3>
<p>Tuo Xu, Lei Zou</p>
<p><a href='https://openreview.net/forum?id=2OyoYw4InI'>https://openreview.net/forum?id=2OyoYw4InI</a></p>
<p><b>Keywords</b>: graph neural networks, higher-order representaton, expressiveness
</p><p><b>Compressor summary</b>: The paper analyzes the expressive power of higher-order representation learning methods for graph machine learning and proposes a simple labeling trick method for link prediction tasks.</p><hr><h3>Propagate & Distill: Towards Effective Graph Learners Using Propagation-Embracing MLPs</h3>
<p>Yong-Min Shin, Won-Yong Shin</p>
<p><a href='https://openreview.net/forum?id=2A14hhZsnA'>https://openreview.net/forum?id=2A14hhZsnA</a></p>
<p><b>Keywords</b>: Graph neural network; Knowledge distillation; Propagation; Multilayer perceptron
</p><p><b>Compressor summary</b>: The authors propose Propagate \& Distill (P\&D), a method to improve semi-supervised node classification on graphs by training a student MLP using knowledge distillation from a teacher GNN, while injecting structural information in an explicit and interpretable manner.</p><hr><h3>Geometric Epitope and Paratope Prediction</h3>
<p>Marco Pegoraro, Clémentine Carla Juliette Dominé, Emanuele Rodolà, Petar Veličković, Andreea Deac</p>
<p><a href='https://openreview.net/forum?id=22NrcBctdI'>https://openreview.net/forum?id=22NrcBctdI</a></p>
<p><b>Keywords</b>: graph learning, learning on surface, drug discovery, paratope-epitope prediciton
</p><p><b>Compressor summary</b>: The paper explores how using geometric information from proteins' surfaces can improve predictions of antibody-antigen binding sites.</p><hr><h3>Inferring Networks from Marginals Using Iterative Proportional Fitting</h3>
<p>Serina Chang, Zhaonan Qu, Jure Leskovec, Johan Ugander</p>
<p><a href='https://openreview.net/forum?id=1HSlaSnKhI'>https://openreview.net/forum?id=1HSlaSnKhI</a></p>
<p><b>Keywords</b>: network inference, dynamic graphs, iterative proportional fitting, Sinkhorn's algorithm
</p><p><b>Compressor summary</b>: The authors explain how they use Sinkhorn's algorithm to infer dynamic networks from 3-dimensional marginals and provide a statistical justification for its minimization principle, as well as demonstrate its effectiveness with real-world mobility data.</p><hr><h3>Interpretable Graph Networks Formulate Universal Algebra Conjectures</h3>
<p>Francesco Giannini, Stefano Fioravanti, Oguzhan Keskin, Alisia Maria Lupidi, Lucie Charlotte Magister, Pietro Lio, Pietro Barbiero</p>
<p><a href='https://openreview.net/forum?id=KhOkUnO04d'>https://openreview.net/forum?id=KhOkUnO04d</a></p>
<p><b>Keywords</b>: explainable AI, universal algebra, concept-based models, graph neural networks, interpretablity
</p><p><b>Compressor summary</b>: The authors propose using AI and interpretable graph networks to analyze and test conjectures in Universal Algebra, a foundational field of mathematics.</p>