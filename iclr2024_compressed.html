
            <meta name="viewport" content="width=device-width, initial-scale=1.0" />
            <link rel="stylesheet" href="../style.css"/>
            <title>Welcome to yobihome</title>
            <a href="https://yobibyte.github.io/"><img src="../pics/socrat.png" class="center"></a>
            <h1>ICLR2024 compressed</h1>
            <p>This page contains one-sentence summaries of ICLR2024 accepted papers, generated by the compressor, my personal LLM-based project.</p>
    <hr><h3>Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages</h3>
<p><a href='https://openreview.net/forum?id=zzqn5G9fjn'>https://openreview.net/forum?id=zzqn5G9fjn</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for fine-tuning multilingual language models that preserves user privacy and improves performance, especially for low-resource languages.</p><hr><h3>On the generalization capacity of neural networks during generic multimodal reasoning</h3>
<p><a href='https://openreview.net/forum?id=zyBJodMrn5'>https://openreview.net/forum?id=zyBJodMrn5</a></p>
<p><b>Compressor summary</b>: The study evaluates how well different neural network architectures handle multimodal generalization tasks and introduces a new benchmark called gCOG.</p><hr><h3>Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach</h3>
<p><a href='https://openreview.net/forum?id=zwU9scoU4A'>https://openreview.net/forum?id=zwU9scoU4A</a></p>
<p><b>Compressor summary</b>: The authors propose Graphex Mean Field Games (GXMFGs), which combine graph theoretical concepts with multi-agent reinforcement learning to handle sparse network topologies, and present a novel learning algorithm for these games.</p><hr><h3>Out-of-Variable Generalisation for Discriminative Models</h3>
<p><a href='https://openreview.net/forum?id=zwMfg9PfPs'>https://openreview.net/forum?id=zwMfg9PfPs</a></p>
<p><b>Compressor summary</b>: The paper explores how agents can generalize to new environments with unobserved variables by reusing past marginal information and using residual distributions to estimate causal relationships.</p><hr><h3>Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting</h3>
<p><a href='https://openreview.net/forum?id=ztpy1gsUpT'>https://openreview.net/forum?id=ztpy1gsUpT</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to boost the performance of small language models in medical tasks while preserving patient privacy by using large language models' knowledge-intensive context.</p><hr><h3>Get What You Want, Not What You Don't: Image Content Suppression for Text-to-Image Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=zpVPhvVKXk'>https://openreview.net/forum?id=zpVPhvVKXk</a></p>
<p><b>Compressor summary</b>: The paper presents two techniques to improve text-to-image diffusion models by removing undesired content from text embeddings, which are tested on different types of diffusion models.</p><hr><h3>Nemesis: Normalizing the soft-prompt vectors of vision-language models</h3>
<p><a href='https://openreview.net/forum?id=zmJDzPh1Dm'>https://openreview.net/forum?id=zmJDzPh1Dm</a></p>
<p><b>Compressor summary</b>: This paper explores how normalizing soft prompt vectors in vision-language models affects their performance and proposes a method to optimize this process.</p><hr><h3>Fast and unified path gradient estimators for normalizing flows</h3>
<p><a href='https://openreview.net/forum?id=zlkXLb3wpF'>https://openreview.net/forum?id=zlkXLb3wpF</a></p>
<p><b>Compressor summary</b>: This paper introduces a fast path gradient estimator for normalizing flows that works with any relevant architecture, improves training, and allows maximum likelihood estimation for various scientific problems.</p><hr><h3>Long-Term Typhoon Trajectory Prediction: A Physics-Conditioned Approach Without Reanalysis Data</h3>
<p><a href='https://openreview.net/forum?id=ziDFH8TPPK'>https://openreview.net/forum?id=ziDFH8TPPK</a></p>
<p><b>Compressor summary</b>: The paper proposes a new approach to predict typhoon trajectories using real-time data and outperforms existing methods in providing accurate forecasts up to 72 hours in advance.</p><hr><h3>Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction</h3>
<p><a href='https://openreview.net/forum?id=zgQ0PHeGnL'>https://openreview.net/forum?id=zgQ0PHeGnL</a></p>
<p><b>Compressor summary</b>: ElliDock is a novel learning-based method that predicts elliptic paraboloid interfaces to enable fast and accurate protein-protein docking, especially for antibody-antigen pairs.</p><hr><h3>On the Joint Interaction of Models, Data, and Features</h3>
<p><a href='https://openreview.net/forum?id=ze7DOLi394'>https://openreview.net/forum?id=ze7DOLi394</a></p>
<p><b>Compressor summary</b>: The interaction tensor is a tool to analyze how features in deep learning models are influenced by data and random seeds, leading to a better understanding of feature learning and generalization error estimation.</p><hr><h3>A ROBUST DIFFERENTIAL NEURAL ODE OPTIMIZER</h3>
<p><a href='https://openreview.net/forum?id=zbOSJ3CATY'>https://openreview.net/forum?id=zbOSJ3CATY</a></p>
<p><b>Compressor summary</b>: The paper proposes GTSONO, a game theoretic optimizer for neural ODEs that is more robust to adversarial attacks and has computational benefits.</p><hr><h3>The Dark Side of the Hyperbolic Moon</h3>
<p><a href='https://openreview.net/forum?id=zbKcFZ6Dbp'>https://openreview.net/forum?id=zbKcFZ6Dbp</a></p>
<p><b>Compressor summary</b>: Shadow cones use shadows formed by a light source in hyperbolic space to model hierarchical relations in data better than existing entailment cone constructions.</p><hr><h3>How do Language Models Bind Entities in Context?</h3>
<p><a href='https://openreview.net/forum?id=zb3b6oKO77'>https://openreview.net/forum?id=zb3b6oKO77</a></p>
<p><b>Compressor summary</b>: LMs represent facts about entities and their attributes using binding ID vectors, which are interpretable and transferable across tasks.</p><hr><h3>On the Limitations of Temperature Scaling for Distributions with Overlaps</h3>
<p><a href='https://openreview.net/forum?id=zavLQJ1XjB'>https://openreview.net/forum?id=zavLQJ1XjB</a></p>
<p><b>Compressor summary</b>: This paper compares temperature scaling and Mixup, two techniques for calibrating deep neural networks' confidence, and shows that Mixup performs better when there are overlapping classes or noisy labels.</p><hr><h3>FROSTER: Frozen CLIP is A Strong Teacher for Open-Vocabulary Action Recognition</h3>
<p><a href='https://openreview.net/forum?id=zYXFMeHRtO'>https://openreview.net/forum?id=zYXFMeHRtO</a></p>
<p><b>Compressor summary</b>: FROSTER is a framework that adapts CLIP for open-vocabulary action recognition by distilling features from a frozen CLIP model and adding video-specific features.</p><hr><h3>Detecting Pretraining Data from Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=zWqr3MQuNs'>https://openreview.net/forum?id=zWqr3MQuNs</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method called MIN-K PROB to detect if an LLM was trained on a given text without knowing the pretraining data, using WIKIMIA as a dynamic benchmark and outperforming previous methods.</p><hr><h3>Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=zSxpnKh1yS'>https://openreview.net/forum?id=zSxpnKh1yS</a></p>
<p><b>Compressor summary</b>: The paper proposes a new metric (WSEP) to improve unsupervised reinforcement learning by enhancing skill diversity and separatability, and shows that using Wasserstein distance instead of KL divergence leads to better task adaptation.</p><hr><h3>Neural Processing of Tri-Plane Hybrid Neural Fields</h3>
<p><a href='https://openreview.net/forum?id=zRkM6UcA22'>https://openreview.net/forum?id=zRkM6UcA22</a></p>
<p><b>Compressor summary</b>: The paper explores how tri-plane discrete data structures can effectively process neural fields for tasks like classification and part segmentation without sacrificing reconstruction quality.</p><hr><h3>Error Norm Truncation: Robust Training in the Presence of Data Noise for Text Generation Models</h3>
<p><a href='https://openreview.net/forum?id=zMvMwNvs4R'>https://openreview.net/forum?id=zMvMwNvs4R</a></p>
<p><b>Compressor summary</b>: ENT improves text generation quality and robustness against noise by truncating noisy data based on its distribution of non-target tokens.</p><hr><h3>CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling</h3>
<p><a href='https://openreview.net/forum?id=zMoNrajk2X'>https://openreview.net/forum?id=zMoNrajk2X</a></p>
<p><b>Compressor summary</b>: Condition-Annealed Diffusion Sampler (CADS) improves diversity and quality of diffusion models by adjusting the conditioning signal during inference.</p><hr><h3>Protein Discovery with Discrete Walk-Jump Sampling</h3>
<p><a href='https://openreview.net/forum?id=zMPHKOmQNb'>https://openreview.net/forum?id=zMPHKOmQNb</a></p>
<p><b>Compressor summary</b>: The paper proposes a discrete sampling method for generative models that improves sample quality and robustness, and applies it to generate functional antibody proteins with high success rate.</p><hr><h3>SetCSE: Set Operations using Contrastive Learning of Sentence Embeddings</h3>
<p><a href='https://openreview.net/forum?id=zEHGSN8Hy8'>https://openreview.net/forum?id=zEHGSN8Hy8</a></p>
<p><b>Compressor summary</b>: SetCSE is an innovative information retrieval framework that uses sets to represent complex semantics and improve language model comprehension for structured information querying.</p><hr><h3>AgentBench: Evaluating LLMs as Agents</h3>
<p><a href='https://openreview.net/forum?id=zAdUB0aCTQ'>https://openreview.net/forum?id=zAdUB0aCTQ</a></p>
<p><b>Compressor summary</b>: AgentBench is a benchmark to evaluate large language models as agents in complex interactive tasks, revealing their strengths and weaknesses in reasoning and decision-making.</p><hr><h3>MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning</h3>
<p><a href='https://openreview.net/forum?id=z8TW0ttBPp'>https://openreview.net/forum?id=z8TW0ttBPp</a></p>
<p><b>Compressor summary</b>: The paper introduces MathCodeInstruct, a method to fine-tune language models using code for solving math problems, and presents MathCoder models that achieve state-of-the-art results on MATH and GSM8K datasets.</p><hr><h3>Perceptual Measurements, Distances and Metrics</h3>
<p><a href='https://openreview.net/forum?id=z7K2faBrDG'>https://openreview.net/forum?id=z7K2faBrDG</a></p>
<p><b>Compressor summary</b>: The text discusses how measuring the perceptual scale of physical variables can improve probabilistic modeling of perception and lead to a better understanding of perceptual distances.</p><hr><h3>An Efficient Tester-Learner for Halfspaces</h3>
<p><a href='https://openreview.net/forum?id=z6n1fKMMC1'>https://openreview.net/forum?id=z6n1fKMMC1</a></p>
<p><b>Compressor summary</b>: The paper presents an efficient algorithm for learning halfspaces in the testable learning model with different types of noise and target distributions, using new tests that leverage the labels and moment-matching techniques.</p><hr><h3>Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game</h3>
<p><a href='https://openreview.net/forum?id=z6KS9D1dxt'>https://openreview.net/forum?id=z6KS9D1dxt</a></p>
<p><b>Compressor summary</b>: The study proposes a Bayesian framework for cooperative multi-agent reinforcement learning that robustly handles Byzantine failures and adversarial attacks by learning policies based on beliefs about other agents' types.</p><hr><h3>Pre-training with Random Orthogonal Projection Image Modeling</h3>
<p><a href='https://openreview.net/forum?id=z4Hcegjzph'>https://openreview.net/forum?id=z4Hcegjzph</a></p>
<p><b>Compressor summary</b>: ROPIM is a self-supervised method for visual pre-training without labels that uses random orthogonal projection instead of binary masking and achieves better performance than MIM.</p><hr><h3>Revisit and Outstrip Entity Alignment: A Perspective of Generative Models</h3>
<p><a href='https://openreview.net/forum?id=z3dfuRcGAK'>https://openreview.net/forum?id=z3dfuRcGAK</a></p>
<p><b>Compressor summary</b>: The paper explores embedding-based entity alignment as a generative model problem, proposes a new framework called generative entity alignment (GEEA) that uses mutual variational autoencoder (M-VAE) for better conversion between knowledge graphs and entity synthesis from noise vectors.</p><hr><h3>Massively Scalable Inverse Reinforcement Learning for Route Optimization</h3>
<p><a href='https://openreview.net/forum?id=z3L59iGALM'>https://openreview.net/forum?id=z3L59iGALM</a></p>
<p><b>Compressor summary</b>: The paper presents RHIP, a novel IRL method for route recommendation that leverages scaling techniques, eigenvector algorithms, and planning horizons to achieve significant improvements in route quality.</p><hr><h3>Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching</h3>
<p><a href='https://openreview.net/forum?id=yzRXdhk2he'>https://openreview.net/forum?id=yzRXdhk2he</a></p>
<p><b>Compressor summary</b>: The paper introduces Matcher, a perception paradigm that leverages off-the-shelf vision foundation models for various segmentation tasks without training, achieving impressive performance on COCO-20 and LVIS-92 benchmarks.</p><hr><h3>Generating Pragmatic Examples to Train Neural Program Synthesizers</h3>
<p><a href='https://openreview.net/forum?id=yxKZGQLzOP'>https://openreview.net/forum?id=yxKZGQLzOP</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method to improve program synthesis from user-provided examples by sampling pairs of programs and examples and using pragmatic inference to choose informative training examples for the synthesizer.</p><hr><h3>Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation</h3>
<p><a href='https://openreview.net/forum?id=yuy6cGt3KL'>https://openreview.net/forum?id=yuy6cGt3KL</a></p>
<p><b>Compressor summary</b>: The authors study how to select the best model for estimating the treatment effect in causal inference using various metrics and real datasets, suggesting new strategies based on hyperparameter tuning and causal ensembling.</p><hr><h3>A Quadratic Synchronization Rule for Distributed Deep Learning</h3>
<p><a href='https://openreview.net/forum?id=yroyhkhWS6'>https://openreview.net/forum?id=yroyhkhWS6</a></p>
<p><b>Compressor summary</b>: The Quadratic Synchronization Rule (QSR) method improves test accuracy in local gradient methods for distributed deep learning by dynamically adjusting the local computation steps $H$ based on the learning rate $\eta$.</p><hr><h3>Equivariant Matrix Function Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=yrgQdA5NkI'>https://openreview.net/forum?id=yrgQdA5NkI</a></p>
<p><b>Compressor summary</b>: MFNs are a new architecture for learning on graphs that captures non-local interactions using matrix equivariant functions, achieving high performance and scalability.</p><hr><h3>In defense of parameter sharing for model-compression</h3>
<p><a href='https://openreview.net/forum?id=ypAT2ixD4X'>https://openreview.net/forum?id=ypAT2ixD4X</a></p>
<p><b>Compressor summary</b>: This paper shows that randomized parameter-sharing (RPS) outperforms other methods for compressing neural network models and achieves better trade-offs between memory and accuracy, especially in high compression settings. It also introduces a modified RPS technique called STABLE-RPS to address stability and Pareto-continuity issues.</p><hr><h3>Image Translation as Diffusion Visual Programmers</h3>
<p><a href='https://openreview.net/forum?id=yozwqhIHXj'>https://openreview.net/forum?id=yozwqhIHXj</a></p>
<p><b>Compressor summary</b>: DVP is a neuro-symbolic image translation framework that uses computer vision models for pro-symbolic steps and achieves condition-flexible, in-context reasoning, and systemic controllability and explainability.</p><hr><h3>Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=yoVq2BGQdP'>https://openreview.net/forum?id=yoVq2BGQdP</a></p>
<p><b>Compressor summary</b>: The paper presents an RL method that ensures fairness in multi-agent MDPs with unknown environments using a fairness function, online convex optimization, confidence bounds, and policy-gradient method.</p><hr><h3>Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion</h3>
<p><a href='https://openreview.net/forum?id=ymjI8feDTD'>https://openreview.net/forum?id=ymjI8feDTD</a></p>
<p><b>Compressor summary</b>: CTM is a new diffusion model sampling method that balances speed and quality by combining adversarial training and denoising score matching, and enables various sampling schemes and exact likelihood computation.</p><hr><h3>Image Background Serves as Good Proxy for Out-of-distribution Data</h3>
<p><a href='https://openreview.net/forum?id=ym0ubZrsmm'>https://openreview.net/forum?id=ym0ubZrsmm</a></p>
<p><b>Compressor summary</b>: The paper proposes a new model (SSOD) that improves out-of-distribution detection without using explicit out-of-distribution data by exploiting natural cues from in-distribution data.</p><hr><h3>Pre-Training and Fine-Tuning Generative Flow Networks</h3>
<p><a href='https://openreview.net/forum?id=ylhiMfpqkm'>https://openreview.net/forum?id=ylhiMfpqkm</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel reward-free pre-training method for Generative Flow Networks, which can adapt to downstream tasks and discover modes more efficiently.</p><hr><h3>Incentivized Truthful Communication for Federated Bandits</h3>
<p><a href='https://openreview.net/forum?id=ykEixGIJYb'>https://openreview.net/forum?id=ykEixGIJYb</a></p>
<p><b>Compressor summary</b>: Truth-FedBan is a communication protocol that motivates truthful reporting in federated bandit learning by providing independent incentives unrelated to self-reported costs, leading to near-optimal performance and efficiency.</p><hr><h3>Chain of Log-Concave Markov Chains</h3>
<p><a href='https://openreview.net/forum?id=yiMB2DOjsR'>https://openreview.net/forum?id=yiMB2DOjsR</a></p>
<p><b>Compressor summary</b>: The paper proposes a new non-Markovian sampling method based on Gaussian kernels, log-concave conditional densities, and empirical Bayes that can tunnel between modes and is compared with existing Langevin MCMC algorithms.</p><hr><h3>VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency</h3>
<p><a href='https://openreview.net/forum?id=ygxTuVz9eU'>https://openreview.net/forum?id=ygxTuVz9eU</a></p>
<p><b>Compressor summary</b>: VDC uses multimodal language models to detect and clean dirty data in AI systems by aligning and reasoning across images and labels.</p><hr><h3>Variance-enlarged Poisson Learning for Graph-based Semi-Supervised Learning with Extremely Sparse Labeled Data</h3>
<p><a href='https://openreview.net/forum?id=yeeVBMDAwy'>https://openreview.net/forum?id=yeeVBMDAwy</a></p>
<p><b>Compressor summary</b>: VPL is a framework that reduces label degeneracy by using variance-enlarged regularization and improves unsupervised learning on graphs.</p><hr><h3>EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision</h3>
<p><a href='https://openreview.net/forum?id=ycv2z8TYur'>https://openreview.net/forum?id=ycv2z8TYur</a></p>
<p><b>Compressor summary</b>: EmerNeRF is a method that learns spatial-temporal representations of dynamic scenes using neural fields, achieving state-of-the-art performance in sensor simulation and addressing positional bias in Transformers.</p><hr><h3>Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation</h3>
<p><a href='https://openreview.net/forum?id=ycF7mKfVGO'>https://openreview.net/forum?id=ycF7mKfVGO</a></p>
<p><b>Compressor summary</b>: SharpeRatio@k is a new metric that measures the risk-return tradeoff and efficiency of policy portfolios formed by an OPE estimator under varying online evaluation budgets, aiming to improve the selection of policies for online deployment.</p><hr><h3>Safe Collaborative Filtering</h3>
<p><a href='https://openreview.net/forum?id=yarUvgEXq3'>https://openreview.net/forum?id=yarUvgEXq3</a></p>
<p><b>Compressor summary</b>: The study develops a scalable and effective recommender system that ensures high quality for less-satisfied users by minimizing the risk over their loss.</p><hr><h3>Hybrid Sharing for Multi-Label Image Classification</h3>
<p><a href='https://openreview.net/forum?id=yVJd8lKyVX'>https://openreview.net/forum?id=yVJd8lKyVX</a></p>
<p><b>Compressor summary</b>: The Hybrid Sharing Query (HSQ) is a transformer-based model that uses a mixture-of-experts architecture to improve image multi-label classification by leveraging label correlations and mitigating heterogeneity.</p><hr><h3>Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information</h3>
<p><a href='https://openreview.net/forum?id=yV6wwEbtkR'>https://openreview.net/forum?id=yV6wwEbtkR</a></p>
<p><b>Compressor summary</b>: The paper proposes MCMI, a new method to estimate BCPD for KD that enhances the teacher's ability to capture contextual information and boosts the student's performance in zero-shot and few-shot scenarios.</p><hr><h3>ValUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation</h3>
<p><a href='https://openreview.net/forum?id=yV6fD7LYkF'>https://openreview.net/forum?id=yV6fD7LYkF</a></p>
<p><b>Compressor summary</b>: The paper proposes an evaluation framework to systematically assess uncertainty methods for semantic segmentation applications, addressing key research questions and revealing insights on data and model uncertainties.</p><hr><h3>Provable Reward-Agnostic Preference-Based Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=yTBXeXdbMf'>https://openreview.net/forum?id=yTBXeXdbMf</a></p>
<p><b>Compressor summary</b>: This study develops a new theoretical framework for preference-based reinforcement learning that reduces human feedback and can handle linear and low-rank MDPs, as well as action-based comparison feedback.</p><hr><h3>Sparse MoE with Language Guided Routing for Multilingual Machine Translation</h3>
<p><a href='https://openreview.net/forum?id=ySS7hH1smL'>https://openreview.net/forum?id=ySS7hH1smL</a></p>
<p><b>Compressor summary</b>: The authors propose Lingual-SMoE, a multilingual machine translation framework that adapts routing policies based on linguistic features and difficulty levels to improve performance over existing methods.</p><hr><h3>Conversational Drug Editing Using Retrieval and Domain Feedback</h3>
<p><a href='https://openreview.net/forum?id=yRrPfKyJQ2'>https://openreview.net/forum?id=yRrPfKyJQ2</a></p>
<p><b>Compressor summary</b>: ChatDrug is a framework using LLMs to systematically investigate and improve drug editing tasks, achieving the best performance on all 39 tasks and providing diverse, valid suggestions and explanations.</p><hr><h3>Bidirectional Temporal Diffusion Model for Temporally Consistent Human Animation</h3>
<p><a href='https://openreview.net/forum?id=yQDFsuG9HP'>https://openreview.net/forum?id=yQDFsuG9HP</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to generate realistic human animation from limited input using bidirectional temporal modeling and denoising diffusion models.</p><hr><h3>Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining</h3>
<p><a href='https://openreview.net/forum?id=yN4Wv17ss3'>https://openreview.net/forum?id=yN4Wv17ss3</a></p>
<p><b>Compressor summary</b>: This paper analyzes how supervised pretraining helps transformer models perform in-context reinforcement learning, and shows they can approximate near-optimal algorithms for certain tasks.</p><hr><h3>MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning</h3>
<p><a href='https://openreview.net/forum?id=yLClGs770I'>https://openreview.net/forum?id=yLClGs770I</a></p>
<p><b>Compressor summary</b>: MAmmoTH is a new open-source language model that excels at solving various math problems using a unique combination of chain-of-thought and program-of-thought rationales, achieving significant accuracy gains over existing models.</p><hr><h3>Faithful and Efficient Explanations for Neural Networks via Neural Tangent Kernel Surrogate Models</h3>
<p><a href='https://openreview.net/forum?id=yKksu38BpM'>https://openreview.net/forum?id=yKksu38BpM</a></p>
<p><b>Compressor summary</b>: This paper explores how to use simpler ML algorithms to analyze and explain neural networks, focusing on new methods for approximating the empirical neural tangent kernel (eNTK) that balance time, memory, and accuracy.</p><hr><h3>Inverse Approximation Theory for Nonlinear Recurrent Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=yC2waD70Vj'>https://openreview.net/forum?id=yC2waD70Vj</a></p>
<p><b>Compressor summary</b>: The paper proves that nonlinear RNNs can approximate certain sequences well if they have an exponential decaying memory structure, and proposes a reparameterization method to address the limitations of RNNs for long-term memory.</p><hr><h3>Bandits with Replenishable Knapsacks: the Best of both Worlds</h3>
<p><a href='https://openreview.net/forum?id=yBIJRIYTqa'>https://openreview.net/forum?id=yBIJRIYTqa</a></p>
<p><b>Compressor summary</b>: The paper proposes a generalization of the bandits with knapsacks framework that allows resource replenishment and shows its effectiveness in online learning and economic applications.</p><hr><h3>Adapting Large Language Models via Reading Comprehension</h3>
<p><a href='https://openreview.net/forum?id=y886UXPEZ0'>https://openreview.net/forum?id=y886UXPEZ0</a></p>
<p><b>Compressor summary</b>: The authors propose a method to transform raw corpora into reading comprehension texts, which improves large language models' performance on various tasks and domains without requiring domain-specific training.</p><hr><h3>AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models</h3>
<p><a href='https://openreview.net/forum?id=y33lDRBgWI'>https://openreview.net/forum?id=y33lDRBgWI</a></p>
<p><b>Compressor summary</b>: The paper proposes AdjointDPM, a method to optimize diffusion models with less memory by using probability-flow ODEs and the adjoint sensitivity method for efficient gradient backpropagation.</p><hr><h3>PolyGCL: GRAPH CONTRASTIVE LEARNING via Learnable Spectral Polynomial Filters</h3>
<p><a href='https://openreview.net/forum?id=y21ZO6M86t'>https://openreview.net/forum?id=y21ZO6M86t</a></p>
<p><b>Compressor summary</b>: PolyGCL is a graph contrastive learning technique that uses polynomial filters to capture high-pass information for heterophily, improving performance on both homophilic and heterophilic graphs.</p><hr><h3>Is Self-Repair a Silver Bullet for Code Generation?</h3>
<p><a href='https://openreview.net/forum?id=y0GJXRungR'>https://openreview.net/forum?id=y0GJXRungR</a></p>
<p><b>Compressor summary</b>: This paper analyzes how well large language models like Code Llama, GPT-3.5, and GPT-4 can fix their own mistakes in code generation tasks and finds that self-repair is often limited by the model's feedback ability.</p><hr><h3>DreamLLM: Synergistic Multimodal Comprehension and Creation</h3>
<p><a href='https://openreview.net/forum?id=y01KGvd9Bw'>https://openreview.net/forum?id=y01KGvd9Bw</a></p>
<p><b>Compressor summary</b>: DreamLLM is a framework that uses direct sampling and interleaved document generation to create versatile multimodal models with better comprehension and creation capabilities.</p><hr><h3>Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation</h3>
<p><a href='https://openreview.net/forum?id=xyxU99Nutg'>https://openreview.net/forum?id=xyxU99Nutg</a></p>
<p><b>Compressor summary</b>: UnMix-TNS is a novel strategy to adapt batch normalization parameters online using mixed statistics components, improving the robustness of test-time adaptation methods under non-i.i.d. conditions.</p><hr><h3>Revisiting the Last-Iterative Convergence of Stochastic Gradient Methods</h3>
<p><a href='https://openreview.net/forum?id=xxaEhwC1I4'>https://openreview.net/forum?id=xxaEhwC1I4</a></p>
<p><b>Compressor summary</b>: This paper investigates the optimal convergence rate of SGD's last iterate for various optimization scenarios without restrictive assumptions, providing a unified proof method.</p><hr><h3>Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging</h3>
<p><a href='https://openreview.net/forum?id=xx0ITyHp3u'>https://openreview.net/forum?id=xx0ITyHp3u</a></p>
<p><b>Compressor summary</b>: Sparse Model Soups improves neural network compression by combining sparse models with shared connectivity, enhancing generalization and out-of-distribution performance.</p><hr><h3>Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks</h3>
<p><a href='https://openreview.net/forum?id=xwKt6bUkXj'>https://openreview.net/forum?id=xwKt6bUkXj</a></p>
<p><b>Compressor summary</b>: The text discusses how different mechanisms in recurrent neural networks help them solve memory-dependent tasks with varying memory requirements, and how a multi-head curriculum can improve their performance and stability.</p><hr><h3>Retrieval meets Long Context Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=xw5nxFWMlo'>https://openreview.net/forum?id=xw5nxFWMlo</a></p>
<p><b>Compressor summary</b>: The paper compares two methods to enhance large language models (LLMs) for tasks like question answering and summarization: extending their context window or adding retrieval, and shows that retrieval can improve performance regardless of the context window size.</p><hr><h3>LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning</h3>
<p><a href='https://openreview.net/forum?id=xw29VvOMmU'>https://openreview.net/forum?id=xw29VvOMmU</a></p>
<p><b>Compressor summary</b>: Our method adapts pretrained language models efficiently by decomposing matrices into high-precision and memory-efficient parts, updating the latter during finetuning.</p><hr><h3>Threaten Spiking Neural Networks through Combining Rate and Temporal Information</h3>
<p><a href='https://openreview.net/forum?id=xv8iGxENyI'>https://openreview.net/forum?id=xv8iGxENyI</a></p>
<p><b>Compressor summary</b>: This paper proposes a hybrid adversarial attack (HART) on spiking neural networks (SNNs) using rate and temporal information, which outperforms previous methods in various scenarios.</p><hr><h3>ClimODE: Climate Forecasting With Physics-informed Neural ODEs</h3>
<p><a href='https://openreview.net/forum?id=xuY33XhEGR'>https://openreview.net/forum?id=xuY33XhEGR</a></p>
<p><b>Compressor summary</b>: ClimODE is a novel deep learning model that incorporates physics-based advection and uncertainty estimation to improve climate prediction.</p><hr><h3>TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series</h3>
<p><a href='https://openreview.net/forum?id=xtOydkE1Ku'>https://openreview.net/forum?id=xtOydkE1Ku</a></p>
<p><b>Compressor summary</b>: The paper presents a new model for multivariate time series prediction using transformer-based attentional copulas, which improves performance and training dynamics while handling various tasks and data characteristics.</p><hr><h3>Dual RL: Unification and New Methods for Reinforcement and Imitation Learning</h3>
<p><a href='https://openreview.net/forum?id=xt9Bu66rqv'>https://openreview.net/forum?id=xt9Bu66rqv</a></p>
<p><b>Compressor summary</b>: The paper proposes dual RL framework to unify offline imitation learning and reinforcement learning, and improves their performance with new methods ReCOIL and $f$-DVL.</p><hr><h3>GLD: Generative Latent Dynamics for Structured Motion Representation and Learning</h3>
<p><a href='https://openreview.net/forum?id=xsd2llWYSA'>https://openreview.net/forum?id=xsd2llWYSA</a></p>
<p><b>Compressor summary</b>: The text introduces a method that learns motion dynamics from sparse data using self-supervised representation and generation, enabling online tracking of various motions with safe action execution.</p><hr><h3>Inherently Interpretable Time Series Classification via Multiple Instance Learning</h3>
<p><a href='https://openreview.net/forum?id=xriGRsoAza'>https://openreview.net/forum?id=xriGRsoAza</a></p>
<p><b>Compressor summary</b>: MILLET is a framework that uses Multiple Instance Learning to make Time Series Classification models more interpretable without sacrificing performance.</p><hr><h3>Teaching Language Models to Hallucinate Less with Synthetic Tasks</h3>
<p><a href='https://openreview.net/forum?id=xpw7V0P136'>https://openreview.net/forum?id=xpw7V0P136</a></p>
<p><b>Compressor summary</b>: SynTra reduces hallucination in large language models by optimizing their system messages on a synthetic task before applying them to real-world summarization tasks.</p><hr><h3>On the Fairness ROAD: Robust Optimization for Adversarial Debiasing</h3>
<p><a href='https://openreview.net/forum?id=xnhvVtZtLD'>https://openreview.net/forum?id=xnhvVtZtLD</a></p>
<p><b>Compressor summary</b>: ROAD is a new algorithm that improves local fairness in machine learning by using distributionally robust optimization and adversarial learning.</p><hr><h3>Exploring Target Representations for Masked Autoencoders</h3>
<p><a href='https://openreview.net/forum?id=xmQMz9OPF5'>https://openreview.net/forum?id=xmQMz9OPF5</a></p>
<p><b>Compressor summary</b>: The paper proposes a new way to train masked autoencoders without carefully choosing target representations, using a randomly initialized model as the teacher and achieving better results on downstream tasks.</p><hr><h3>Federated Recommendation with Additive Personalization</h3>
<p><a href='https://openreview.net/forum?id=xkXdE81mOK'>https://openreview.net/forum?id=xkXdE81mOK</a></p>
<p><b>Compressor summary</b>: FedRAP is a new federated learning model for recommendations that learns global and personalized item embeddings, improving performance and reducing communication costs.</p><hr><h3>Robust Model-Based Optimization for Challenging Fitness Landscapes</h3>
<p><a href='https://openreview.net/forum?id=xhEN0kJh4q'>https://openreview.net/forum?id=xhEN0kJh4q</a></p>
<p><b>Compressor summary</b>: The authors propose a new method using a novel VAE to overcome the problem of separation in protein design optimization, which leads to improved sampling and generalization across discrete and continuous spaces.</p><hr><h3>Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion</h3>
<p><a href='https://openreview.net/forum?id=xhCZD9hiiA'>https://openreview.net/forum?id=xhCZD9hiiA</a></p>
<p><b>Compressor summary</b>: This study proposes a batch-normalized MLP with linear activations and batch normalization that has bounded gradients and optimal signal propagation.</p><hr><h3>Learning to design protein-protein interactions with enhanced generalization</h3>
<p><a href='https://openreview.net/forum?id=xcMmebCT7s'>https://openreview.net/forum?id=xcMmebCT7s</a></p>
<p><b>Compressor summary</b>: The work introduces PPIRef, a large 3D protein-protein interaction dataset, and PPIformer, a new SE(3)-equivariant model that predicts mutation effects on protein-protein interactions and generalizes well across diverse scenarios.</p><hr><h3>Statistical Rejection Sampling Improves Preference Optimization</h3>
<p><a href='https://openreview.net/forum?id=xbjSwwrQOe'>https://openreview.net/forum?id=xbjSwwrQOe</a></p>
<p><b>Compressor summary</b>: RLHF alternatives like SLiC and DPO have limitations in using preference data; RSO overcomes these by sourcing data from the optimal policy, and a unified framework improves loss functions for both methods.</p><hr><h3>Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs</h3>
<p><a href='https://openreview.net/forum?id=xZDWO0oejD'>https://openreview.net/forum?id=xZDWO0oejD</a></p>
<p><b>Compressor summary</b>: PASTA is a method that allows large language models to read text with user-specified emphasis marks, enhancing their ability to follow instructions or integrate new knowledge.</p><hr><h3>Privacy Amplification for Matrix Mechanisms</h3>
<p><a href='https://openreview.net/forum?id=xUzWmFdglP'>https://openreview.net/forum?id=xUzWmFdglP</a></p>
<p><b>Compressor summary</b>: The paper proposes MMCC, a new algorithm that analyzes privacy amplification via sampling for any matrix mechanism, and shows its utility in improving privacy/utility trade-offs for DP-FTRL style algorithms.</p><hr><h3>Negative Label Guided OOD Detection with Pretrained Vision-Language Models</h3>
<p><a href='https://openreview.net/forum?id=xUO1HXz4an'>https://openreview.net/forum?id=xUO1HXz4an</a></p>
<p><b>Compressor summary</b>: NegLabel is a novel post hoc out-of-distribution (OOD) detection method for vision-language models that uses negative labels from corpus databases and achieves state-of-the-art performance with good generalization and robustness.</p><hr><h3>SALMON: Self-Alignment with Principle-Following Reward Models</h3>
<p><a href='https://openreview.net/forum?id=xJbsmB8UMx'>https://openreview.net/forum?id=xJbsmB8UMx</a></p>
<p><b>Compressor summary</b>: The paper proposes SALMON, a novel alignment paradigm for large language models that uses human-defined principles to control the behavior of an AI assistant without relying on extensive human feedback.</p><hr><h3>Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control</h3>
<p><a href='https://openreview.net/forum?id=xJEd8PkdNz'>https://openreview.net/forum?id=xJEd8PkdNz</a></p>
<p><b>Compressor summary</b>: The choice of the quadrature rule in integral reinforcement learning affects control performance due to computational errors, which can be reduced by using Bayesian quadrature with an appropriate kernel function.</p><hr><h3>Constrained Bi-Level Optimization: Proximal Lagrangian Value function Approach and Hessian-free Algorithm</h3>
<p><a href='https://openreview.net/forum?id=xJ5N8qrEPl'>https://openreview.net/forum?id=xJ5N8qrEPl</a></p>
<p><b>Compressor summary</b>: The paper proposes a new Hessian-free algorithm for solving constrained Bi-Level Optimization problems in machine learning, which uses a smooth proximal Lagrangian value function to handle constraints and performs well in practice.</p><hr><h3>Subtractive Mixture Models via Squaring: Representation and Learning</h3>
<p><a href='https://openreview.net/forum?id=xIHi5nxu9P'>https://openreview.net/forum?id=xIHi5nxu9P</a></p>
<p><b>Compressor summary</b>: Squaring subtractive mixtures in probabilistic circuits allows for better modeling of complex distributions with fewer components.</p><hr><h3>Towards 3D Molecule-Text Interpretation in Language Models</h3>
<p><a href='https://openreview.net/forum?id=xI4yNlkaqh'>https://openreview.net/forum?id=xI4yNlkaqh</a></p>
<p><b>Compressor summary</b>: 3D-MoLM is a language model that can comprehend 3D molecular structures by integrating a 3D molecular encoder, improving its performance in biomolecular tasks.</p><hr><h3>Correlated Noise Provably Beats Independent Noise for Differentially Private Learning</h3>
<p><a href='https://openreview.net/forum?id=xHmCdSArUC'>https://openreview.net/forum?id=xHmCdSArUC</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve privacy in machine learning by introducing correlations in the noise, providing analytical bounds for its utility, and demonstrating its effectiveness in experiments on deep learning.</p><hr><h3>How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization</h3>
<p><a href='https://openreview.net/forum?id=xGvPKAiOhq'>https://openreview.net/forum?id=xGvPKAiOhq</a></p>
<p><b>Compressor summary</b>: This paper shows how over-parameterization affects gradient descent for matrix sensing problems, leading to faster convergence rates with an asymmetric parameterization and a novel method to remove dependency on initialization scale.</p><hr><h3>Elucidating the Exposure Bias in Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=xEJMoj1SpX'>https://openreview.net/forum?id=xEJMoj1SpX</a></p>
<p><b>Compressor summary</b>: The paper investigates the exposure bias problem in diffusion models and proposes Epsilon Scaling, a training-free method that improves generative performance across various frameworks and settings.</p><hr><h3>Score Regularized Policy Optimization through Diffusion Behavior</h3>
<p><a href='https://openreview.net/forum?id=xCRr9DrolJ'>https://openreview.net/forum?id=xCRr9DrolJ</a></p>
<p><b>Compressor summary</b>: The paper proposes a faster way to learn policies from diffusion models using deterministic inference, improving both speed and performance on offline reinforcement learning tasks.</p><hr><h3>Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on HuggingFace</h3>
<p><a href='https://openreview.net/forum?id=xC8xh2RSs2'>https://openreview.net/forum?id=xC8xh2RSs2</a></p>
<p><b>Compressor summary</b>: Key findings from analyzing 7,433 dataset documentation cards on Hugging Face reveal the importance of completing all sections, prioritizing description and structure, and improving accessibility and reproducibility.</p><hr><h3>GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation</h3>
<p><a href='https://openreview.net/forum?id=xBfQZWeDRH'>https://openreview.net/forum?id=xBfQZWeDRH</a></p>
<p><b>Compressor summary</b>: GeoDiffusion is a framework that uses text prompts to generate high-quality detection data from pre-trained text-to-image diffusion models, encoding geometric conditions like bounding boxes and camera views.</p><hr><h3>On the Stability of Expressive Positional Encodings for Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=xAqcJ9XoTf'>https://openreview.net/forum?id=xAqcJ9XoTf</a></p>
<p><b>Compressor summary</b>: SPE is a new method for graph neural networks that uses eigenvalues to softly partition eigenvectors, improving stability and expressivity over existing Laplacian-based positional encodings.</p><hr><h3>Are Bert Family Good Instruction Followers?  A Study on Their Potential And Limitations</h3>
<p><a href='https://openreview.net/forum?id=x8VNtpCu1I'>https://openreview.net/forum?id=x8VNtpCu1I</a></p>
<p><b>Compressor summary</b>: This paper explores how encoder-only BERT models can perform instruction following and zero-shot learning using XML-R and shows that they achieve competitive results while being faster than autoregressive models.</p><hr><h3>A Restoration Network as an Implicit Prior</h3>
<p><a href='https://openreview.net/forum?id=x7d1qXEn1e'>https://openreview.net/forum?id=x7d1qXEn1e</a></p>
<p><b>Compressor summary</b>: The authors propose a method to improve image restoration by using pre-trained neural networks as implicit priors, achieving state-of-the-art performance.</p><hr><h3>Tag2Text: Guiding Vision-Language Model via Image Tagging</h3>
<p><a href='https://openreview.net/forum?id=x6u2BQ7xcq'>https://openreview.net/forum?id=x6u2BQ7xcq</a></p>
<p><b>Compressor summary</b>: Tag2Text is a framework that uses image tags from paired text to improve vision-language models' performance on various tasks without extensive annotations.</p><hr><h3>Synaptic Weight Distributions Depend on the Geometry of Plasticity</h3>
<p><a href='https://openreview.net/forum?id=x5txICnnjC'>https://openreview.net/forum?id=x5txICnnjC</a></p>
<p><b>Compressor summary</b>: The assumption of Euclidean geometry in studying synaptic plasticity may be incorrect, and experiments could reveal the true geometry by comparing weight distributions before and after learning.</p><hr><h3>PROGRAM: PROtotype GRAph Model based Pseudo-Label Learning for Test-Time Adaptation</h3>
<p><a href='https://openreview.net/forum?id=x5LvBK43wg'>https://openreview.net/forum?id=x5LvBK43wg</a></p>
<p><b>Compressor summary</b>: PROGRAM is a novel test-time adaptation method that uses a prototype graph model for reliable pseudo-label generation and robust self-training for target domain adaptation with noisy pseudo-labels.</p><hr><h3>Privacy-Preserving In-Context Learning for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=x4OPJ7lHVU'>https://openreview.net/forum?id=x4OPJ7lHVU</a></p>
<p><b>Compressor summary</b>: DP-ICL is a method to protect sensitive information in large language models by generating noisy responses from multiple models.</p><hr><h3>ADoPD: A Large-Scale Document Page Decomposition Dataset</h3>
<p><a href='https://openreview.net/forum?id=x1ptaXpOYa'>https://openreview.net/forum?id=x1ptaXpOYa</a></p>
<p><b>Compressor summary</b>: ADoPD is a new dataset for analyzing documents that combines large-scale data, human expertise, and diversity to improve document understanding.</p><hr><h3>Urial: Aligning Untuned LLMs with Just the 'Write' Amount of In-Context Learning</h3>
<p><a href='https://openreview.net/forum?id=wxJ0eXwwda'>https://openreview.net/forum?id=wxJ0eXwwda</a></p>
<p><b>Compressor summary</b>: Alignment tuning mostly changes the language style of AI assistants, so an alternative method called Urial can achieve effective alignment with only a few stylistic examples.</p><hr><h3>Toward Student-oriented Teacher Network Training for Knowledge Distillation</h3>
<p><a href='https://openreview.net/forum?id=wsWGcw6qKD'>https://openreview.net/forum?id=wsWGcw6qKD</a></p>
<p><b>Compressor summary</b>: The paper explores how to train teachers for knowledge distillation using empirical risk minimization (ERM) and proposes a method called SoTeacher that improves student accuracy.</p><hr><h3>Consistency-guided Prompt Learning for Vision-Language Models</h3>
<p><a href='https://openreview.net/forum?id=wsRXwlwx4w'>https://openreview.net/forum?id=wsRXwlwx4w</a></p>
<p><b>Compressor summary</b>: CoPrompt is a new fine-tuning method for vision-language models that uses consistency constraints, perturbed inputs, and adapters/prompts to improve generalization on few-shot tasks.</p><hr><h3>On the Effect of Batch Size in Byzantine-Robust Distributed Learning</h3>
<p><a href='https://openreview.net/forum?id=wriKDQqiOQ'>https://openreview.net/forum?id=wriKDQqiOQ</a></p>
<p><b>Compressor summary</b>: This paper studies the effect of batch size on Byzantine-robust distributed learning and proposes a new method (ByzSGDnm) that increases model accuracy under attacks by using larger batch sizes and normalized momentum.</p><hr><h3>Benchmarking Algorithms for Federated Domain Generalization</h3>
<p><a href='https://openreview.net/forum?id=wprSv7ichW'>https://openreview.net/forum?id=wprSv7ichW</a></p>
<p><b>Compressor summary</b>: The paper proposes a new Federated Domain Generalization benchmark and evaluates 13 methods on it, finding significant performance gaps in this challenging setting.</p><hr><h3>Adaptive deep spiking neural network with global-local learning via balanced excitatory and inhibitory mechanism</h3>
<p><a href='https://openreview.net/forum?id=wpnlc2ONu0'>https://openreview.net/forum?id=wpnlc2ONu0</a></p>
<p><b>Compressor summary</b>: The paper proposes EIHL, an algorithm that adjusts network connectivity using excitation-inhibition and switches between local and global learning for better spiking neural network training performance and sparsity.</p><hr><h3>Sparsistency for inverse optimal transport</h3>
<p><a href='https://openreview.net/forum?id=wpXGPCBOTX'>https://openreview.net/forum?id=wpXGPCBOTX</a></p>
<p><b>Compressor summary</b>: This paper studies inverse optimal transport, which infers the ground cost from samples, and derives a sufficient condition for sparse recovery of the cost using $\ell_1$ regularization.</p><hr><h3>Compositional Generative Inverse Design</h3>
<p><a href='https://openreview.net/forum?id=wmX0CqFSd7'>https://openreview.net/forum?id=wmX0CqFSd7</a></p>
<p><b>Compressor summary</b>: Inverse design can be improved by optimizing over the learned energy function of a diffusion model, enabling compositional system design and avoiding adversarial examples.</p><hr><h3>Scalable Diffusion for Materials Generation</h3>
<p><a href='https://openreview.net/forum?id=wm4WlHoXpC'>https://openreview.net/forum?id=wm4WlHoXpC</a></p>
<p><b>Compressor summary</b>: The authors propose a novel unified representation for generating high-fidelity crystal structures from complex chemical systems using a diffusion probabilistic model and introduce new metrics to evaluate the quality of generated materials for downstream applications like discovering novel stable materials.</p><hr><h3>LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition</h3>
<p><a href='https://openreview.net/forum?id=wkbeqr5XhC'>https://openreview.net/forum?id=wkbeqr5XhC</a></p>
<p><b>Compressor summary</b>: The authors propose a novel approach using a Vision Transformer variant with a learnable under-sampling mask to reduce hyperspectral data acquisition volume and enhance real-time detection applications.</p><hr><h3>$\texttt{NAISR}$: A 3D Neural Additive Model for Interpretable Shape Representation</h3>
<p><a href='https://openreview.net/forum?id=wg8NPfeMF9'>https://openreview.net/forum?id=wg8NPfeMF9</a></p>
<p><b>Compressor summary</b>: The proposed NAISR method represents 3D shapes by deforming an atlas using disentangled covariates for interpretable shape discovery and transfer.</p><hr><h3>Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation</h3>
<p><a href='https://openreview.net/forum?id=wfzXa8e783'>https://openreview.net/forum?id=wfzXa8e783</a></p>
<p><b>Compressor summary</b>: LyCORIS is an open-source library that helps fine-tune Stable Diffusion, a text-to-image generator, by offering various methodologies and assessing their effects with different metrics and prompts.</p><hr><h3>Robust Classification via Regression-Based Loss Reweighting and Label Correction</h3>
<p><a href='https://openreview.net/forum?id=wfgZc3IMqo'>https://openreview.net/forum?id=wfgZc3IMqo</a></p>
<p><b>Compressor summary</b>: The authors propose a unified method that combines loss reweighting and label correction to improve robustness against label noise in classification tasks using a shifted Gaussian model.</p><hr><h3>Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND</h3>
<p><a href='https://openreview.net/forum?id=wcka3bd7P4'>https://openreview.net/forum?id=wcka3bd7P4</a></p>
<p><b>Compressor summary</b>: FROND is a graph neural network model that uses fractional calculus to capture long-term memories and mitigate over-smoothing, improving graph representation learning.</p><hr><h3>Uni3D: Exploring Unified 3D Representation at Scale</h3>
<p><a href='https://openreview.net/forum?id=wcaE4Dfgt8'>https://openreview.net/forum?id=wcaE4Dfgt8</a></p>
<p><b>Compressor summary</b>: Uni3D is a 3D foundation model that leverages 2D pretrained models and image-text aligned models to create scalable 3D representations, achieving state-of-the-art results on various 3D tasks.</p><hr><h3>Counterfactual Density Estimation using Kernel Stein Discrepancies</h3>
<p><a href='https://openreview.net/forum?id=wZXlEFO3tZ'>https://openreview.net/forum?id=wZXlEFO3tZ</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to estimate counterfactual distributions for causal effects using kernel Stein discrepancies, which is consistent and asymptotically normal under certain conditions.</p><hr><h3>Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations</h3>
<p><a href='https://openreview.net/forum?id=wZWTHU7AsQ'>https://openreview.net/forum?id=wZWTHU7AsQ</a></p>
<p><b>Compressor summary</b>: The text introduces GRAD, a game-theoretic method for robust reinforcement learning that handles temporally-coupled perturbations, outperforming prior methods in experiments on continuous control tasks.</p><hr><h3>Mixture of Weak and Strong Experts on Graphs</h3>
<p><a href='https://openreview.net/forum?id=wYvuY60SdD'>https://openreview.net/forum?id=wYvuY60SdD</a></p>
<p><b>Compressor summary</b>: Mowst is a method that combines a lightweight MLP and a GNN to classify nodes on graphs, with a confidence mechanism that controls when to use the GNN for better performance.</p><hr><h3>Benign Oscillation of Stochastic Gradient Descent with Large Learning Rate</h3>
<p><a href='https://openreview.net/forum?id=wYmvN3sQpG'>https://openreview.net/forum?id=wYmvN3sQpG</a></p>
<p><b>Compressor summary</b>: The study shows that neural networks trained with large learning rates and oscillations can learn weak features better than those trained with small learning rates, leading to improved generalization.</p><hr><h3>CLEX: Continuous  Length Extrapolation for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=wXpSidPpc5'>https://openreview.net/forum?id=wXpSidPpc5</a></p>
<p><b>Compressor summary</b>: CLEX is a novel method that extends the context window of LLMs by using ordinary differential equations to model position embedding scaling, enabling better performance in long-context applications without sacrificing accuracy or increasing latency.</p><hr><h3>MVSFormer++: Revealing the Devil in the Transformer's Details for Multi-View Stereo</h3>
<p><a href='https://openreview.net/forum?id=wXWfvSpYHh'>https://openreview.net/forum?id=wXWfvSpYHh</a></p>
<p><b>Compressor summary</b>: MVSFormer++ is a transformer-based model with attention mechanisms that enhances different components of the MVS pipeline, achieving state-of-the-art results on challenging benchmarks.</p><hr><h3>AutoVP: An Automated Visual Prompting Framework and Benchmark</h3>
<p><a href='https://openreview.net/forum?id=wR9qVlPh0P'>https://openreview.net/forum?id=wR9qVlPh0P</a></p>
<p><b>Compressor summary</b>: AutoVP is an end-to-end framework for automating visual prompting design choices and a benchmark for evaluating its performance on 12 image-classification tasks, significantly improving accuracy over existing methods.</p><hr><h3>STARC: A General Framework For Quantifying Differences Between Reward Functions</h3>
<p><a href='https://openreview.net/forum?id=wPhbtwlCDa'>https://openreview.net/forum?id=wPhbtwlCDa</a></p>
<p><b>Compressor summary</b>: The paper proposes a new class of pseudometrics called STARC metrics for comparing reward functions in reinforcement learning, which can help improve theoretical and empirical evaluation of reward learning algorithms.</p><hr><h3>Batch normalization is sufficient for universal function approximation in CNNs</h3>
<p><a href='https://openreview.net/forum?id=wOSYMHfENq'>https://openreview.net/forum?id=wOSYMHfENq</a></p>
<p><b>Compressor summary</b>: The text explains why normalization layers like batch normalization are important for deep learning and shows how using enough random features can achieve universal function approximation in various network architectures.</p><hr><h3>Learning No-Regret Sparse Generalized Linear Models with Varying Observation(s)</h3>
<p><a href='https://openreview.net/forum?id=wISvONp3Kq'>https://openreview.net/forum?id=wISvONp3Kq</a></p>
<p><b>Compressor summary</b>: The paper proposes an algorithm for training sparse generalized linear models online with varying observations, using a bilevel optimization framework and adaptive regularization parameters.</p><hr><h3>BarLeRIa: An Efficient Tuning Framework for Referring Image Segmentation</h3>
<p><a href='https://openreview.net/forum?id=wHLDHRkmEu'>https://openreview.net/forum?id=wHLDHRkmEu</a></p>
<p><b>Compressor summary</b>: BarLeRIa is a novel Parameter-Efficient Tuning framework for dense prediction tasks in computer vision that uses bi-directional intertwined vision language adapters and outperforms prior methods by 5.6%.</p><hr><h3>YaRN: Efficient Context Window Extension of Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=wHBfxhZu1u'>https://openreview.net/forum?id=wHBfxhZu1u</a></p>
<p><b>Compressor summary</b>: YaRN extends the context window of language models using RoPE embeddings, improving generalization and extrapolation beyond the original pre-training.</p><hr><h3>Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian distributions</h3>
<p><a href='https://openreview.net/forum?id=wG12xUSqrI'>https://openreview.net/forum?id=wG12xUSqrI</a></p>
<p><b>Compressor summary</b>: The paper analyzes how score-based generative models learn sub-Gaussian distributions and proves their approximation accuracy with respect to the target distribution.</p><hr><h3>Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder</h3>
<p><a href='https://openreview.net/forum?id=wFf9m4v7oC'>https://openreview.net/forum?id=wFf9m4v7oC</a></p>
<p><b>Compressor summary</b>: The paper presents a novel method for estimating causal effects from observational data with unobserved confounding variables, using conditional front-door adjustment and deep generative models.</p><hr><h3>On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=wFWuX1Fhtj'>https://openreview.net/forum?id=wFWuX1Fhtj</a></p>
<p><b>Compressor summary</b>: The paper analyzes the limitations of primal-dual algorithms for constrained cooperative multi-agent reinforcement learning due to nonconvex constraints and proposes a decentralized primal approach with its own challenges.</p><hr><h3>Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment</h3>
<p><a href='https://openreview.net/forum?id=w9tc699w3Z'>https://openreview.net/forum?id=w9tc699w3Z</a></p>
<p><b>Compressor summary</b>: The authors propose a method to train VLMs for remote-sensing images without textual annotations by using co-located internet imagery as an intermediary and show that their approach performs better than supervised methods on various tasks.</p><hr><h3>Linearity of Relation Decoding in Transformer Language Models</h3>
<p><a href='https://openreview.net/forum?id=w7LU2s14kE'>https://openreview.net/forum?id=w7LU2s14kE</a></p>
<p><b>Compressor summary</b>: Transformer language models use linear relations to approximate some of their knowledge, which can be extracted by approximating the model with a single prompt.</p><hr><h3>Geometry-Aware Projective Mapping for Unbounded Neural Radiance Fields</h3>
<p><a href='https://openreview.net/forum?id=w7BwaDHppp'>https://openreview.net/forum?id=w7BwaDHppp</a></p>
<p><b>Compressor summary</b>: The paper proposes a new mapping function for neural radiance fields (NeRFs) to generate novel views of unbounded scenes by adaptively sampling rays based on scene geometry.</p><hr><h3>Independent-Set Design of Experiments for Estimating Treatment and Spillover Effects under Network Interference</h3>
<p><a href='https://openreview.net/forum?id=w50MQ9Vfty'>https://openreview.net/forum?id=w50MQ9Vfty</a></p>
<p><b>Compressor summary</b>: The article proposes a new experimental design for causal inference on networks with interference, which improves performance by controlling interference exposure.</p><hr><h3>Reverse Forward Curriculum Learning for Extreme Sample and Demo Efficiency</h3>
<p><a href='https://openreview.net/forum?id=w4rODxXsmM'>https://openreview.net/forum?id=w4rODxXsmM</a></p>
<p><b>Compressor summary</b>: The paper proposes RFCL, a method to improve reinforcement learning with offline data by using a reverse curriculum (generated via state resets) followed by a forward curriculum to efficiently leverage multiple demonstrations and solve complex tasks from sparse rewards.</p><hr><h3>Batched Low-Rank Adaptation of Foundation Models</h3>
<p><a href='https://openreview.net/forum?id=w4abltTZ2f'>https://openreview.net/forum?id=w4abltTZ2f</a></p>
<p><b>Compressor summary</b>: FLORA is a framework that improves LoRA's ability to handle multiple task-specific adaptations efficiently, enabling fine-tuning of foundation models for diverse requests.</p><hr><h3>SuRe: Improving Open-domain Question Answering of LLMs via Summarized Retrieval</h3>
<p><a href='https://openreview.net/forum?id=w4DW6qkRmt'>https://openreview.net/forum?id=w4DW6qkRmt</a></p>
<p><b>Compressor summary</b>: SuRe is a framework that enhances open-domain question answering with large language models by generating summaries of retrieved passages and using them to select the most plausible answer.</p><hr><h3>MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training</h3>
<p><a href='https://openreview.net/forum?id=w3YZ9MSlBu'>https://openreview.net/forum?id=w3YZ9MSlBu</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-supervised learning framework for music audio, using teacher models to provide pseudo labels in acoustic pre-training, achieving state-of-the-art results on 14 music understanding tasks.</p><hr><h3>Are Models Biased on Text without Gender-related Language?</h3>
<p><a href='https://openreview.net/forum?id=w1JanwReU6'>https://openreview.net/forum?id=w1JanwReU6</a></p>
<p><b>Compressor summary</b>: The study examines how large language models show gender biases in neutral sentences without obvious word-gender associations, and finds high levels of bias across different models.</p><hr><h3>Off-Policy Primal-Dual Safe Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=vy42bYs1Wo'>https://openreview.net/forum?id=vy42bYs1Wo</a></p>
<p><b>Compressor summary</b>: Our method improves safety and constraint satisfaction in primal-dual safe RL by reducing cost estimation uncertainty with conservative policy optimization and local policy convexification.</p><hr><h3>GRAPH-CONSTRAINED DIFFUSION FOR END-TO-END PATH PLANNING</h3>
<p><a href='https://openreview.net/forum?id=vuK8MhVtuu'>https://openreview.net/forum?id=vuK8MhVtuu</a></p>
<p><b>Compressor summary</b>: The paper proposes a new data-driven path planning model called GDP, which learns path patterns from road networks and outperforms traditional search-based methods.</p><hr><h3>CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs</h3>
<p><a href='https://openreview.net/forum?id=vtyasLn4RM'>https://openreview.net/forum?id=vtyasLn4RM</a></p>
<p><b>Compressor summary</b>: The paper presents a scalable Graph Neural Network (GNN) based Graph Drawing framework that learns to optimize stress using coarsening and positional rewiring techniques, achieving state-of-the-art performance.</p><hr><h3>Grokking as the transition from lazy to rich training dynamics</h3>
<p><a href='https://openreview.net/forum?id=vt5mnLVIVo'>https://openreview.net/forum?id=vt5mnLVIVo</a></p>
<p><b>Compressor summary</b>: Grokking occurs when a neural network initially tries to fit a simple solution with its initial features before transitioning to feature learning and eventually generalizing.</p><hr><h3>Fixed-Budget Differentially Private Best Arm Identification</h3>
<p><a href='https://openreview.net/forum?id=vrE2fqAInO'>https://openreview.net/forum?id=vrE2fqAInO</a></p>
<p><b>Compressor summary</b>: The paper studies how to find the best arm in a bandit problem with a limited budget and privacy constraints, using a policy based on maximum absolute determinants and deriving upper and lower bounds on the error probability.</p><hr><h3>Q-TAPE: A Task-Agnostic Pre-Trained Approach for Quantum Properties Estimation</h3>
<p><a href='https://openreview.net/forum?id=vrBVFXwAmi'>https://openreview.net/forum?id=vrBVFXwAmi</a></p>
<p><b>Compressor summary</b>: Q-TAPE is a pre-trained deep learning model that learns from diverse quantum systems in an unsupervised way, improving performance and reducing costs for quantum many-body problems.</p><hr><h3>Can Large Language Models Infer Causation from Correlation?</h3>
<p><a href='https://openreview.net/forum?id=vqIH0ObdqL'>https://openreview.net/forum?id=vqIH0ObdqL</a></p>
<p><b>Compressor summary</b>: The paper introduces Corr2Cause, a new task to test causal inference skills of large language models, and shows their poor performance and lack of generalization on it.</p><hr><h3>Decision Transformer is a Robust Contender for Offline Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=vpV7fOFQy4'>https://openreview.net/forum?id=vpV7fOFQy4</a></p>
<p><b>Compressor summary</b>: The paper compares three offline RL algorithms (CQL, BC, and DT) across various data settings and tasks to determine their strengths and weaknesses.</p><hr><h3>ModernTCN: A Modern Pure Convolution Structure for General Time Series Analysis</h3>
<p><a href='https://openreview.net/forum?id=vpJMJerXHU'>https://openreview.net/forum?id=vpJMJerXHU</a></p>
<p><b>Compressor summary</b>: The paper proposes ModernTCN, a modified version of traditional TCN that improves performance and efficiency in time series analysis by increasing effective receptive fields.</p><hr><h3>From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication</h3>
<p><a href='https://openreview.net/forum?id=vngVydDWft'>https://openreview.net/forum?id=vngVydDWft</a></p>
<p><b>Compressor summary</b>: The paper introduces a method to incorporate invariances into neural representations, improving latent similarity and downstream performance across different tasks and modalities.</p><hr><h3>Alice Benchmarks: Connecting Real World Object Re-Identification with the Synthetic</h3>
<p><a href='https://openreview.net/forum?id=vkkHqoerLV'>https://openreview.net/forum?id=vkkHqoerLV</a></p>
<p><b>Compressor summary</b>: The Alice benchmarks provide large-scale datasets and evaluation protocols for object re-ID tasks using synthetic data, aiming to train effective models for real-world applications.</p><hr><h3>Universal Graph Random Features</h3>
<p><a href='https://openreview.net/forum?id=viftsX50Rt'>https://openreview.net/forum?id=viftsX50Rt</a></p>
<p><b>Compressor summary</b>: The paper introduces a new algorithm for fast and scalable estimation of graph kernel functions using random walks and neural networks.</p><hr><h3>CODE REPRESENTATION LEARNING AT SCALE</h3>
<p><a href='https://openreview.net/forum?id=vfzRRjumpX'>https://openreview.net/forum?id=vfzRRjumpX</a></p>
<p><b>Compressor summary</b>: This paper proposes a two-stage pretraining scheme for code representation learning using a large amount of code data, which improves performance on various downstream tasks.</p><hr><h3>Towards Robust and Efficient Cloud-Edge Model Adaptation via Selective Entropy Distillation</h3>
<p><a href='https://openreview.net/forum?id=vePdNU3u6n'>https://openreview.net/forum?id=vePdNU3u6n</a></p>
<p><b>Compressor summary</b>: CEMA is a paradigm that enables online adaptation of cloud models using limited computation and data transmission by excluding unreliable and low-informative samples and distilling normalization layers.</p><hr><h3>SAS: Structured Activation Sparsification</h3>
<p><a href='https://openreview.net/forum?id=vZfi5to2Xl'>https://openreview.net/forum?id=vZfi5to2Xl</a></p>
<p><b>Compressor summary</b>: SAS improves accuracy in neural networks by using sparse activation with a specific structure, reducing computation and leveraging NVIDIA's SparseTensorCore.</p><hr><h3>Learning Multi-Agent Communication with Contrastive Learning</h3>
<p><a href='https://openreview.net/forum?id=vZZ4hhniJU'>https://openreview.net/forum?id=vZZ4hhniJU</a></p>
<p><b>Compressor summary</b>: The paper proposes a contrastive learning approach to improve communication in multi-agent RL by using messages as encodings that capture global state information.</p><hr><h3>Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks</h3>
<p><a href='https://openreview.net/forum?id=vZ6r9GMT1n'>https://openreview.net/forum?id=vZ6r9GMT1n</a></p>
<p><b>Compressor summary</b>: The paper proposes a simple and lightweight defense against black-box attacks on deep neural networks by adding random noise to intermediate layers, enhancing resilience without adversarial training or accuracy loss.</p><hr><h3>CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules</h3>
<p><a href='https://openreview.net/forum?id=vYhglxSj8j'>https://openreview.net/forum?id=vYhglxSj8j</a></p>
<p><b>Compressor summary</b>: CodeChain is a framework that improves the modularity and correctness of code generated by large language models by guiding them to reuse previously developed sub-modules through a chain of self-revisions.</p><hr><h3>Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis</h3>
<p><a href='https://openreview.net/forum?id=vY9nzQmQBw'>https://openreview.net/forum?id=vY9nzQmQBw</a></p>
<p><b>Compressor summary</b>: Vocos is a new neural vocoding model that generates speech from spectral coefficients using fast Fourier-based time-frequency representations, achieving high audio quality and significant computational efficiency.</p><hr><h3>SliceGPT: Compress Large Language Models by Deleting Rows and Columns</h3>
<p><a href='https://openreview.net/forum?id=vXxardq6db'>https://openreview.net/forum?id=vXxardq6db</a></p>
<p><b>Compressor summary</b>: SliceGPT is a post-training sparsification technique that reduces the embedding dimension of large language models, enabling faster inference with fewer GPUs and minimal accuracy loss.</p><hr><h3>Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation and Human Feedback</h3>
<p><a href='https://openreview.net/forum?id=vW1SkPl4kp'>https://openreview.net/forum?id=vW1SkPl4kp</a></p>
<p><b>Compressor summary</b>: The paper introduces a risk-sensitive reinforcement learning framework that uses Iterated CVaR objective, human feedback, and provides sample-efficient algorithms with theoretical analysis and lower bound proof.</p><hr><h3>Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling</h3>
<p><a href='https://openreview.net/forum?id=vSwu81S33z'>https://openreview.net/forum?id=vSwu81S33z</a></p>
<p><b>Compressor summary</b>: Nonparametric transfer learning (NPTL) is a flexible posterior sampling method for deep neural networks that handles distribution shifts between upstream and downstream data better than existing methods.</p><hr><h3>How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?</h3>
<p><a href='https://openreview.net/forum?id=vSh5ePa0ph'>https://openreview.net/forum?id=vSh5ePa0ph</a></p>
<p><b>Compressor summary</b>: The paper studies how pretraining a simple linear attention model for linear regression with a Gaussian prior enables it to solve unseen tasks using in-context learning, showing that effective pretraining requires only a few tasks and that the model closely matches the Bayes optimal algorithm.</p><hr><h3>Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios</h3>
<p><a href='https://openreview.net/forum?id=vRyp2dhEQp'>https://openreview.net/forum?id=vRyp2dhEQp</a></p>
<p><b>Compressor summary</b>: The paper proposes realistic data-constrained backdoor attacks on DNNs, where attackers can't access all training data, and introduces CLIP-based techniques to improve efficiency in injecting malicious features.</p><hr><h3>Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=vNiI3aGcE6'>https://openreview.net/forum?id=vNiI3aGcE6</a></p>
<p><b>Compressor summary</b>: The paper proposes a memory-efficient model-free self-play algorithm for two-player zero-sum Markov games in multi-agent reinforcement learning, achieving low space and computational complexity, and improving sample and burn-in cost compared to existing methods.</p><hr><h3>Fine-Tuned Language Models Generate Stable Inorganic Materials as Text</h3>
<p><a href='https://openreview.net/forum?id=vN9fpfqoP1'>https://openreview.net/forum?id=vN9fpfqoP1</a></p>
<p><b>Compressor summary</b>: The authors propose using fine-tuned large language models to generate novel materials with favorable properties for engineering by sampling from text-encoded atomistic data, which is simple, reliable, and captures crystal symmetries better than diffusion models.</p><hr><h3>SYMBOL: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning</h3>
<p><a href='https://openreview.net/forum?id=vLJcd43U7a'>https://openreview.net/forum?id=vLJcd43U7a</a></p>
<p><b>Compressor summary</b>: SYMBOL is a novel meta-learning framework that uses symbolic equation learning to automate the discovery of black-box optimizers, surpassing state-of-the-art baselines and showing exceptional zero-shot generalization.</p><hr><h3>Intriguing Properties of Data Attribution on Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=vKViCoKGcB'>https://openreview.net/forum?id=vKViCoKGcB</a></p>
<p><b>Compressor summary</b>: The paper explores data attribution methods for diffusion models and finds that theoretically unjustified design choices perform better than previous baselines in attributing diffusion models' outputs to training data.</p><hr><h3>Skip-Attention: Improving Vision Transformers by Paying Less Attention</h3>
<p><a href='https://openreview.net/forum?id=vI95kcLAoU'>https://openreview.net/forum?id=vI95kcLAoU</a></p>
<p><b>Compressor summary</b>: The paper proposes SkipAT, a method to reuse self-attention computations across layers in vision transformers, improving efficiency and performance.</p><hr><h3>Learning in reverse causal strategic environments with ramifications on two sided markets</h3>
<p><a href='https://openreview.net/forum?id=vEfmVS5ywF'>https://openreview.net/forum?id=vEfmVS5ywF</a></p>
<p><b>Compressor summary</b>: The text discusses how employers can strategically manipulate hiring policies to improve outcomes, but also shows that these policies may have unintended negative consequences in certain situations.</p><hr><h3>Multilingual Jailbreak Challenges in Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=vESNKdEMGp'>https://openreview.net/forum?id=vESNKdEMGp</a></p>
<p><b>Compressor summary</b>: The study shows that large language models can produce harmful content when prompted with non-English or jailbreak instructions, especially for low-resource languages, and proposes a self-defense framework to mitigate these risks.</p><hr><h3>Threshold-Consistent Margin Loss for Open-World Deep Metric Learning</h3>
<p><a href='https://openreview.net/forum?id=vE5MyzpP92'>https://openreview.net/forum?id=vE5MyzpP92</a></p>
<p><b>Compressor summary</b>: The paper proposes a new metric and loss function to improve image retrieval performance and consistency across different classes and data distributions.</p><hr><h3>The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.</h3>
<p><a href='https://openreview.net/forum?id=vE1e1mLJ0U'>https://openreview.net/forum?id=vE1e1mLJ0U</a></p>
<p><b>Compressor summary</b>: The Expressive Leaky Memory (ELM) neuron model, which uses memory-like hidden states and nonlinear integration of synaptic input, accurately mimics cortical neuron computations with under 10,000 parameters and performs well on temporal tasks.</p><hr><h3>Encoding Unitig-level Assembly Graphs with Heterophilous Constraints for Metagenomic Contigs Binning</h3>
<p><a href='https://openreview.net/forum?id=vBw8JGBJWj'>https://openreview.net/forum?id=vBw8JGBJWj</a></p>
<p><b>Compressor summary</b>: UnitigBin is a novel metagenomic binning tool that uses representation learning and single-copy marker genes to improve the accuracy of clustering genomic sequences from microbial communities.</p><hr><h3>Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms</h3>
<p><a href='https://openreview.net/forum?id=vBo7544jZx'>https://openreview.net/forum?id=vBo7544jZx</a></p>
<p><b>Compressor summary</b>: The PMI framework mimics human brain's memory system to enhance question-answering, relation reasoning, and image classification tasks using Transformers and CNN models.</p><hr><h3>Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=v8jdwkUNXb'>https://openreview.net/forum?id=v8jdwkUNXb</a></p>
<p><b>Compressor summary</b>: The paper proposes a fast and effective policy representation called consistency policy for different reinforcement learning settings, using an actor-critic algorithm with a generative model as a policy.</p><hr><h3>Let's Verify Step by Step</h3>
<p><a href='https://openreview.net/forum?id=v8L0pN6EOi'>https://openreview.net/forum?id=v8L0pN6EOi</a></p>
<p><b>Compressor summary</b>: The text compares outcome and process supervision methods for training reliable large language models and shows that process supervision with active learning performs better on MATH problems.</p><hr><h3>Statistically Optimal $K$-means Clustering via Nonnegative Low-rank Semidefinite Programming</h3>
<p><a href='https://openreview.net/forum?id=v7ZPwoHU1j'>https://openreview.net/forum?id=v7ZPwoHU1j</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel clustering method that solves a nonnegative low-rank $K$-means problem using NMF, and shows its superior performance and guarantees.</p><hr><h3>Faster Sampling from Log-Concave Densities over Polytopes via Efficient Linear Solvers</h3>
<p><a href='https://openreview.net/forum?id=v63GWletn8'>https://openreview.net/forum?id=v63GWletn8</a></p>
<p><b>Compressor summary</b>: The paper proposes an improved algorithm for sampling from logconcave distributions on a polytope with constraints, using efficient linear solvers, determinantal estimators, and randomized Taylor series.</p><hr><h3>RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment</h3>
<p><a href='https://openreview.net/forum?id=v3XXtxWKi6'>https://openreview.net/forum?id=v3XXtxWKi6</a></p>
<p><b>Compressor summary</b>: RLCD is a method that uses contrasting prompts to create preference pairs without human feedback, then trains a language model with reinforcement learning to follow alignment principles like harmlessness.</p><hr><h3>AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ</h3>
<p><a href='https://openreview.net/forum?id=v3K5TVP8kZ'>https://openreview.net/forum?id=v3K5TVP8kZ</a></p>
<p><b>Compressor summary</b>: The authors propose using TikZ as an intermediate representation for scientific figures, introduce a large dataset (DaTikZ) of TikZ drawings with captions, and fine-tune LLaMA and CLiMA models on it, which outperform GPT-4 and Claude 2 in generating realistic and aligned figures.</p><hr><h3>Towards Characterizing Domain Counterfactuals for Invertible Latent Causal Models</h3>
<p><a href='https://openreview.net/forum?id=v1VvCWJAL8'>https://openreview.net/forum?id=v1VvCWJAL8</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to estimate domain counterfactuals, which are what a sample would have looked like in a different environment, by making assumptions on invertibility and sparse interventions, and shows that this approach improves counterfactual estimation over existing methods.</p><hr><h3>How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation</h3>
<p><a href='https://openreview.net/forum?id=v0zNCwwkaV'>https://openreview.net/forum?id=v0zNCwwkaV</a></p>
<p><b>Compressor summary</b>: The text introduces a generalized attention scheme that captures triple-wise correlations using tensors and shows its advantages and limitations in terms of efficiency and expressivity.</p><hr><h3>On Stationary Point Convergence of PPO-Clip</h3>
<p><a href='https://openreview.net/forum?id=uznKlCpWjV'>https://openreview.net/forum?id=uznKlCpWjV</a></p>
<p><b>Compressor summary</b>: This paper analyzes the theoretical convergence properties of PPO-Clip, a popular reinforcement learning algorithm that uses a clipped surrogate objective function.</p><hr><h3>Bayesian Coreset Optimization for Personalized Federated Learning</h3>
<p><a href='https://openreview.net/forum?id=uz7d2N2zul'>https://openreview.net/forum?id=uz7d2N2zul</a></p>
<p><b>Compressor summary</b>: The paper proposes a personalized coreset weighted federated learning method that reduces training time and improves accuracy by using representative data points instead of the entire client data.</p><hr><h3>Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation</h3>
<p><a href='https://openreview.net/forum?id=uwO71a8wET'>https://openreview.net/forum?id=uwO71a8wET</a></p>
<p><b>Compressor summary</b>: The paper proposes a new Bayesian method that can estimate both the treatment effect and its uncertainty in continuous time using neural equations, which can help doctors make better decisions in personalized medicine.</p><hr><h3>DEEP NEURAL NETWORK INITIALIZATION WITH SPARSITY INDUCING ACTIVATIONS</h3>
<p><a href='https://openreview.net/forum?id=uvXK8Xk9Jk'>https://openreview.net/forum?id=uvXK8Xk9Jk</a></p>
<p><b>Compressor summary</b>: The text discusses how using sparse activations in deep networks can improve efficiency, but shows that some natural sparsifiers have training instability issues and suggests clipping their magnitude to fix this.</p><hr><h3>Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs</h3>
<p><a href='https://openreview.net/forum?id=uvFhCUPjtI'>https://openreview.net/forum?id=uvFhCUPjtI</a></p>
<p><b>Compressor summary</b>: The Evolving Graph Fourier Transform (EFT) is a new method for capturing the evolving spectra of graphs over time, which is efficient, invertible, and effective for downstream tasks.</p><hr><h3>LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors</h3>
<p><a href='https://openreview.net/forum?id=usrChqw6yK'>https://openreview.net/forum?id=usrChqw6yK</a></p>
<p><b>Compressor summary</b>: DVDet is a new open-vocabulary detector that uses conditional context prompts and hierarchical textual descriptors to align visual embeddings with fine-grained text descriptions of object parts, achieving state-of-the-art performance.</p><hr><h3>Bridging Neural and Symbolic Representations with Transitional Dictionary Learning</h3>
<p><a href='https://openreview.net/forum?id=uqxBTcWRnj'>https://openreview.net/forum?id=uqxBTcWRnj</a></p>
<p><b>Compressor summary</b>: The paper presents a new framework for learning symbolic knowledge from images using a game-theoretic diffusion model and proposes two evaluation metrics that align with human judgments.</p><hr><h3>Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=up6hr4hIQH'>https://openreview.net/forum?id=up6hr4hIQH</a></p>
<p><b>Compressor summary</b>: This paper proposes new fidelity measures for GNN explainability that resist distribution shifts and align better with the information-theoretic definition of explainability.</p><hr><h3>GROOT: Learning to Follow Instructions by Watching Gameplay Videos</h3>
<p><a href='https://openreview.net/forum?id=uleDLeiaT3'>https://openreview.net/forum?id=uleDLeiaT3</a></p>
<p><b>Compressor summary</b>: The paper presents GROOT, an AI agent that learns to follow reference videos as instructions in open-world environments, achieving competitive performance against human players and generalist agents in Minecraft SkillForge.</p><hr><h3>Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs</h3>
<p><a href='https://openreview.net/forum?id=ulaUJFd96G'>https://openreview.net/forum?id=ulaUJFd96G</a></p>
<p><b>Compressor summary</b>: HOMER is a training-free scheme that splits long inputs into smaller units, fuses them at Transformer layers, and reduces token count to improve performance and memory efficiency of large language models.</p><hr><h3>Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping</h3>
<p><a href='https://openreview.net/forum?id=ukidfml68f'>https://openreview.net/forum?id=ukidfml68f</a></p>
<p><b>Compressor summary</b>: The text proposes a new technique called Pixel-wise Gradient Clipping (PGC) that improves the quality of 3D object generation by controlling the gradients used in latent representation-based models.</p><hr><h3>A Discretization Framework for Robust Contextual Stochastic Optimization</h3>
<p><a href='https://openreview.net/forum?id=ueTdErd5Ib'>https://openreview.net/forum?id=ueTdErd5Ib</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for solving uncertain optimization problems that combines learning with downstream optimization, discretizes the feasible region into subsets, and integrates individual approximations to prescribe decisions that minimize expected cost and protect against worst case scenarios.</p><hr><h3>Mathematical Justification of Hard Negative Mining via Isometric Approximation Theorem</h3>
<p><a href='https://openreview.net/forum?id=udO3k28bEw'>https://openreview.net/forum?id=udO3k28bEw</a></p>
<p><b>Compressor summary</b>: The paper explains how hard negative mining works in deep metric learning and shows its equivalence to an optimization problem, providing theoretical justification for its effectiveness in preventing network collapse.</p><hr><h3>Implicit Neural Representations and the Algebra of Complex Wavelets</h3>
<p><a href='https://openreview.net/forum?id=uZfjFyPAvn'>https://openreview.net/forum?id=uZfjFyPAvn</a></p>
<p><b>Compressor summary</b>: The paper explores how implicit neural representations using wavelets can better capture high-frequency features of signals in image processing.</p><hr><h3>Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches</h3>
<p><a href='https://openreview.net/forum?id=uXjfOmTiDt'>https://openreview.net/forum?id=uXjfOmTiDt</a></p>
<p><b>Compressor summary</b>: Embodied Active Defense (EAD) is a proactive strategy that uses recurrent feedback mechanisms and active vision to counter adversarial patches in 3D real-world settings, enhancing robustness without compromising accuracy.</p><hr><h3>MoLE: Mixture of LoRA Experts</h3>
<p><a href='https://openreview.net/forum?id=uWvKBCYh4S'>https://openreview.net/forum?id=uWvKBCYh4S</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method called Mixture of LoRA Experts (MoLE) to effectively combine multiple LoRAs for fine-tuning pre-trained models, overcoming challenges in existing approaches and achieving better results in NLP and V\&L tasks.</p><hr><h3>Unbiased Watermark for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=uWVC5FVidc'>https://openreview.net/forum?id=uWVC5FVidc</a></p>
<p><b>Compressor summary</b>: This study shows that it is possible to integrate watermarks in large language models without affecting their output quality or performance, enabling responsible AI development.</p><hr><h3>In-context Autoencoder for Context Compression in a Large Language Model</h3>
<p><a href='https://openreview.net/forum?id=uREj4ZuGJE'>https://openreview.net/forum?id=uREj4ZuGJE</a></p>
<p><b>Compressor summary</b>: The In-context Autoencoder (ICAE) uses a large language model to compress long texts into compact memory slots for efficient response generation.</p><hr><h3>Unpaired Image-to-Image Translation via Neural Schrdinger Bridge</h3>
<p><a href='https://openreview.net/forum?id=uQBW7ELXfO'>https://openreview.net/forum?id=uQBW7ELXfO</a></p>
<p><b>Compressor summary</b>: UNSB is a new method for unpaired image-to-image translation using Schrdinger Bridge with improved discriminators and regularization.</p><hr><h3>Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs</h3>
<p><a href='https://openreview.net/forum?id=uNrFpDPMyo'>https://openreview.net/forum?id=uNrFpDPMyo</a></p>
<p><b>Compressor summary</b>: The study presents a method to reduce memory usage in generative inference for large language models by adaptively compressing the key-value cache based on attention module profiles.</p><hr><h3>Learning with Mixture of Prototypes for Out-of-Distribution Detection</h3>
<p><a href='https://openreview.net/forum?id=uNkKaD3MCs'>https://openreview.net/forum?id=uNkKaD3MCs</a></p>
<p><b>Compressor summary</b>: PALM is a distance-based OOD detection method that models each class with multiple prototypes to capture sample diversities and enhance OOD detection performance.</p><hr><h3>Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment</h3>
<p><a href='https://openreview.net/forum?id=uMAujpVi9m'>https://openreview.net/forum?id=uMAujpVi9m</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel pocket pretraining method that leverages knowledge from atomic protein structures and pretrained small molecule representations to improve biomedical applications involving pockets and ligands.</p><hr><h3>BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks</h3>
<p><a href='https://openreview.net/forum?id=uKB4cFNQFg'>https://openreview.net/forum?id=uKB4cFNQFg</a></p>
<p><b>Compressor summary</b>: The study presents BEND, a benchmark for evaluating DNA language models on realistic and biologically meaningful tasks related to genome annotation.</p><hr><h3>GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers</h3>
<p><a href='https://openreview.net/forum?id=uJVHygNeSZ'>https://openreview.net/forum?id=uJVHygNeSZ</a></p>
<p><b>Compressor summary</b>: The paper proposes a geometry-aware attention mechanism for 3D vision tasks that encodes the geometric structure of tokens, leading to improved learning efficiency and performance of transformer-based neural networks.</p><hr><h3>In-context Exploration-Exploitation for Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=uIKZSStON3'>https://openreview.net/forum?id=uIKZSStON3</a></p>
<p><b>Compressor summary</b>: The ICEE algorithm improves the efficiency and speed of in-context policy learning by optimizing exploration-exploitation trade-offs within a Transformer model without Bayesian inference.</p><hr><h3>Expected flow networks in stochastic environments and two-player zero-sum games</h3>
<p><a href='https://openreview.net/forum?id=uH0FGECSEI'>https://openreview.net/forum?id=uH0FGECSEI</a></p>
<p><b>Compressor summary</b>: EFlowNets improve GFlowNets for stochastic tasks, while AFlowNets enable strong performance in adversarial environments like two-player games.</p><hr><h3>Boosting Selective Rationalization with Shortcuts Discovery</h3>
<p><a href='https://openreview.net/forum?id=uGtfk2OphU'>https://openreview.net/forum?id=uGtfk2OphU</a></p>
<p><b>Compressor summary</b>: The paper proposes a Selective Rationalization method that uses shortcuts to improve neural network explanations and provides data augmentation techniques to increase annotated rationales.</p><hr><h3>Differentially Private SGD Without Clipping Bias: An Error-Feedback Approach</h3>
<p><a href='https://openreview.net/forum?id=uFbWHyTlPn'>https://openreview.net/forum?id=uFbWHyTlPn</a></p>
<p><b>Compressor summary</b>: The paper proposes a new differentially private algorithm for training deep learning models with sensitive data, which avoids constant bias from gradient clipping and achieves higher accuracy.</p><hr><h3>Enhanced Face Recognition using Intra-class Incoherence Constraint</h3>
<p><a href='https://openreview.net/forum?id=uELjxVbrqG'>https://openreview.net/forum?id=uELjxVbrqG</a></p>
<p><b>Compressor summary</b>: The paper proposes a feature recombination technique and a IIC constraint to improve FR performance by exploiting the feature representation space of different models.</p><hr><h3>Entity-Centric Reinforcement Learning for Object Manipulation from Pixels</h3>
<p><a href='https://openreview.net/forum?id=uDxeSZ1wdI'>https://openreview.net/forum?id=uDxeSZ1wdI</a></p>
<p><b>Compressor summary</b>: The paper proposes a structured method for visual reinforcement learning that can handle multiple object manipulation and goal dependencies, enabling agents to generalize from 3 objects to over 10 objects tasks.</p><hr><h3>Augmenting transformers with recursively composed multi-grained representations</h3>
<p><a href='https://openreview.net/forum?id=u859gX7ADC'>https://openreview.net/forum?id=u859gX7ADC</a></p>
<p><b>Compressor summary</b>: ReCAT is a Transformer model that can learn hierarchical syntax without gold trees and use a novel CIO layer for better contextualization and inter-span communication.</p><hr><h3>Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization</h3>
<p><a href='https://openreview.net/forum?id=u7559ZMvwY'>https://openreview.net/forum?id=u7559ZMvwY</a></p>
<p><b>Compressor summary</b>: AToP is a novel framework that combines random transforms and purifier model fine-tuning to enhance the robustness and generalization of deep neural networks against adversarial attacks.</p><hr><h3>Large Language Models as Generalizable Policies for Embodied Tasks</h3>
<p><a href='https://openreview.net/forum?id=u6imHU4Ebu'>https://openreview.net/forum?id=u6imHU4Ebu</a></p>
<p><b>Compressor summary</b>: The paper presents LLaRP, a method to adapt large language models as generalizable policies for embodied visual tasks, achieving higher success rates than common baselines on unseen tasks and introducing a new benchmark, Language Rearrangement.</p><hr><h3>ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=u48tHG5f66'>https://openreview.net/forum?id=u48tHG5f66</a></p>
<p><b>Compressor summary</b>: The authors propose a method to generate high-resolution images from pre-trained diffusion models without training, addressing object repetition and unreasonable structures by adjusting the convolutional perception field with re-dilation and using dispersed convolution and noise-damped guidance.</p><hr><h3>Scaling physics-informed hard constraints with mixture-of-experts</h3>
<p><a href='https://openreview.net/forum?id=u3dX2CEIZb'>https://openreview.net/forum?id=u3dX2CEIZb</a></p>
<p><b>Compressor summary</b>: The paper proposes a scalable method to enforce hard physical constraints in neural networks using Mixture-of-Experts (MoE), which improves accuracy, convergence, and efficiency for complex dynamical systems.</p><hr><h3>The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting - An Analytical Model</h3>
<p><a href='https://openreview.net/forum?id=u3dHl287oB'>https://openreview.net/forum?id=u3dHl287oB</a></p>
<p><b>Compressor summary</b>: Our paper studies how overparameterization and task similarity in continual learning affect forgetting and presents an analytical expression for it.</p><hr><h3>Learning invariant representations of time-homogeneous stochastic dynamical systems</h3>
<p><a href='https://openreview.net/forum?id=twSnZwiOIm'>https://openreview.net/forum?id=twSnZwiOIm</a></p>
<p><b>Compressor summary</b>: The paper proposes an optimization problem over neural networks to learn a state representation for stochastic dynamical systems, which can be used for forecasting and interpreting system dynamics.</p><hr><h3>Towards Understanding Sycophancy in Language Models</h3>
<p><a href='https://openreview.net/forum?id=tvhaxkMKAn'>https://openreview.net/forum?id=tvhaxkMKAn</a></p>
<p><b>Compressor summary</b>: RLHF-trained AI assistants often prioritize matching user beliefs over truthfulness, and this behavior is partly influenced by human preferences for sycophancy.</p><hr><h3>SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS</h3>
<p><a href='https://openreview.net/forum?id=tveiUXU2aa'>https://openreview.net/forum?id=tveiUXU2aa</a></p>
<p><b>Compressor summary</b>: SWAP-Score is a novel training-free metric that measures network expressivity based on sample-wise activation patterns, outperforming existing metrics in Neural Architecture Search.</p><hr><h3>Zero Bubble Pipeline Parallelism</h3>
<p><a href='https://openreview.net/forum?id=tuzTN0eIO5'>https://openreview.net/forum?id=tuzTN0eIO5</a></p>
<p><b>Compressor summary</b>: The paper proposes a scheduling strategy to eliminate pipeline bubbles in distributed training, improving throughput and memory efficiency.</p><hr><h3>Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal Data</h3>
<p><a href='https://openreview.net/forum?id=ttXg3SKAg5'>https://openreview.net/forum?id=ttXg3SKAg5</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve cross-modal learning from uni-modal data by bridging the modality gap in multi-modal contrastive space using Connect, Collapse, Corrupt steps, achieving state-of-the-art results on various tasks.</p><hr><h3>SafeDreamer: Safe Reinforcement Learning with World Models</h3>
<p><a href='https://openreview.net/forum?id=tsE5HLYtYg'>https://openreview.net/forum?id=tsE5HLYtYg</a></p>
<p><b>Compressor summary</b>: SafeDreamer is a new algorithm that uses world models to improve safe reinforcement learning, achieving nearly zero-cost performance in various tasks while maintaining safety criteria.</p><hr><h3>Evaluating Large Language Models at Evaluating Instruction Following</h3>
<p><a href='https://openreview.net/forum?id=tr0KidwPLc'>https://openreview.net/forum?id=tr0KidwPLc</a></p>
<p><b>Compressor summary</b>: This paper introduces LLMBAR, a benchmark to test how well large language models can evaluate instructions followed by generated text, revealing differences among evaluators and suggesting prompting strategies for improvement.</p><hr><h3>Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How</h3>
<p><a href='https://openreview.net/forum?id=tqh1zdXIra'>https://openreview.net/forum?id=tqh1zdXIra</a></p>
<p><b>Compressor summary</b>: The paper presents a fast and accurate method to select and finetune pretrained models using a meta-dataset of performance data and a meta-learned hyperparameter predictor.</p><hr><h3>Diffusion Posterior Sampling for Linear Inverse Problem Solving: A Filtering Perspective</h3>
<p><a href='https://openreview.net/forum?id=tplXNcHZs1'>https://openreview.net/forum?id=tplXNcHZs1</a></p>
<p><b>Compressor summary</b>: The paper introduces a new sampling algorithm for diffusion models that solves linear inverse problems without approximations and shows improved performance on tasks like image inpainting, super-resolution, and deblurring.</p><hr><h3>Does CLIPs generalization performance mainly stem from high train-test similarity?</h3>
<p><a href='https://openreview.net/forum?id=tnBaiidobu'>https://openreview.net/forum?id=tnBaiidobu</a></p>
<p><b>Compressor summary</b>: The study examines how CLIP's out-of-distribution generalization is affected by training data similarity and pruning, finding that other factors besides data similarity drive CLIP's performance.</p><hr><h3>Dissecting learning and forgetting in language model finetuning</h3>
<p><a href='https://openreview.net/forum?id=tmsqb6WpLz'>https://openreview.net/forum?id=tmsqb6WpLz</a></p>
<p><b>Compressor summary</b>: Finetuning language models on domain-specific data affects their topic and style preferences more than their factual knowledge, and this study uses instruction-following LLMs to generate controlled text examples to analyze these effects.</p><hr><h3>Maximum Entropy Heterogeneous-Agent Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=tmqOhBC4a5'>https://openreview.net/forum?id=tmqOhBC4a5</a></p>
<p><b>Compressor summary</b>: The paper proposes a unified framework for learning stochastic policies in cooperative MARL using probabilistic graphical models and maximum entropy objectives, leading to HASAC algorithm that improves sample efficiency, robustness, and exploration.</p><hr><h3>Motif: Intrinsic Motivation from Artificial Intelligence Feedback</h3>
<p><a href='https://openreview.net/forum?id=tmBKIecDE9'>https://openreview.net/forum?id=tmBKIecDE9</a></p>
<p><b>Compressor summary</b>: Motif is a method to use a large language model's knowledge to improve an agent's decision-making and performance in complex environments like NetHack game.</p><hr><h3>Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting</h3>
<p><a href='https://openreview.net/forum?id=tm8s3696Ox'>https://openreview.net/forum?id=tm8s3696Ox</a></p>
<p><b>Compressor summary</b>: Co-Boosting is a novel framework that improves the quality of synthesized data and ensemble models in One-shot Federated Learning through an adversarial process.</p><hr><h3>Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community</h3>
<p><a href='https://openreview.net/forum?id=tjn2YZSHUv'>https://openreview.net/forum?id=tjn2YZSHUv</a></p>
<p><b>Compressor summary</b>: The paper introduces Social Reward, a reward modeling framework that uses implicit feedback from social network users to evaluate and improve text-to-image models, making AI-generated art more aligned with community preferences.</p><hr><h3>Compositional Preference Models for Aligning LMs</h3>
<p><a href='https://openreview.net/forum?id=tiiAzqi6Ol'>https://openreview.net/forum?id=tiiAzqi6Ol</a></p>
<p><b>Compressor summary</b>: CPMs are a new framework for training Preference Models that decompose preference assessments into interpretable features, use a LM to score these features, and aggregate them with logistic regression, improving generalization and robustness over standard PMs.</p><hr><h3>$\mathbb{D}^2$ Pruning: Message Passing for Balancing Diversity & Difficulty in Data Pruning</h3>
<p><a href='https://openreview.net/forum?id=thbtoAkCe9'>https://openreview.net/forum?id=thbtoAkCe9</a></p>
<p><b>Compressor summary</b>: The paper proposes a new coreset selection method, $\mathbb{D}^2$ Pruning, that uses graph-based message passing to balance data diversity and difficulty scores, improving model performance on various vision and NLP tasks.</p><hr><h3>A Benchmark for Learning to Translate a New Language from One Grammar Book</h3>
<p><a href='https://openreview.net/forum?id=tbVWug9f2h'>https://openreview.net/forum?id=tbVWug9f2h</a></p>
<p><b>Compressor summary</b>: The paper introduces MTOB, a benchmark for testing large language models' ability to learn low-resource languages from a single book of grammar explanations, finding that current models perform similarly to humans but not as well as desired.</p><hr><h3>Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization</h3>
<p><a href='https://openreview.net/forum?id=tbFBh3LMKi'>https://openreview.net/forum?id=tbFBh3LMKi</a></p>
<p><b>Compressor summary</b>: Uni-O4 is a reinforcement learning method that combines offline and online learning seamlessly for efficient and safe learning in real-world environments.</p><hr><h3>Learning to Reject for Balanced Error and Beyond</h3>
<p><a href='https://openreview.net/forum?id=ta26LtNq2r'>https://openreview.net/forum?id=ta26LtNq2r</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel approach for learning to reject classifiers that perform well under imbalanced label distributions and show its effectiveness through experiments on image classification tasks.</p><hr><h3>GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs</h3>
<p><a href='https://openreview.net/forum?id=tVTN7Zs0ml'>https://openreview.net/forum?id=tVTN7Zs0ml</a></p>
<p><b>Compressor summary</b>: GraphCare uses external knowledge graphs and language models to generate patient-specific graphs and improve predictions from electronic health records.</p><hr><h3>Provable Offline Preference-Based Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=tVMPfEGT2w'>https://openreview.net/forum?id=tVMPfEGT2w</a></p>
<p><b>Compressor summary</b>: The paper proposes an algorithm for offline preference-based reinforcement learning using human feedback and shows a novel guarantee that allows learning any target policy with general function approximation, as long as it's covered by the offline data.</p><hr><h3>Generative Modeling with Phase Stochastic Bridge</h3>
<p><a href='https://openreview.net/forum?id=tUtGjQEDd4'>https://openreview.net/forum?id=tUtGjQEDd4</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel phase space-based generative model that leverages velocity information for efficient and realistic data generation, outperforming or rivaling existing diffusion models on image generation benchmarks.</p><hr><h3>Learning Conditional Invariances through Non-Commutativity</h3>
<p><a href='https://openreview.net/forum?id=tUVG9nGzgE'>https://openreview.net/forum?id=tUVG9nGzgE</a></p>
<p><b>Compressor summary</b>: Non-commutative invariance learning improves domain adaptation by using source domain samples more effectively.</p><hr><h3>Text2Reward: Dense Reward Generation with Language Models for Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=tUM39YTRxH'>https://openreview.net/forum?id=tUM39YTRxH</a></p>
<p><b>Compressor summary</b>: Text2Reward is a framework that uses large language models to generate dense reward functions for reinforcement learning tasks, achieving good performance on robotic manipulation and locomotion benchmarks.</p><hr><h3>Towards Robust Out-of-Distribution Generalization Bounds via Sharpness</h3>
<p><a href='https://openreview.net/forum?id=tPEwSYPtAC'>https://openreview.net/forum?id=tPEwSYPtAC</a></p>
<p><b>Compressor summary</b>: This paper connects sharpness and robustness to improve out-of-distribution generalization bounds for models.</p><hr><h3>Matryoshka Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=tOzCcDdH9O'>https://openreview.net/forum?id=tOzCcDdH9O</a></p>
<p><b>Compressor summary</b>: Matryoshka Diffusion Model (MDM) is a new framework for generating high-quality images and videos that denoises inputs at multiple resolutions jointly and trains end-to-end using a progressive training schedule, achieving strong zero-shot generalization.</p><hr><h3>Generalization of Deep ResNets in the Mean-Field Regime</h3>
<p><a href='https://openreview.net/forum?id=tMzPZTvz2H'>https://openreview.net/forum?id=tMzPZTvz2H</a></p>
<p><b>Compressor summary</b>: The paper explores how ResNet generalizes in the limit of infinitely wide and deep neural networks, using a time-variant Gram matrix and analyzing KL divergence.</p><hr><h3>Reasoning with Latent Diffusion in Offline Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=tGQirjzddO'>https://openreview.net/forum?id=tGQirjzddO</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel approach that uses latent diffusion to model trajectory sequences as compressed latent skills for offline reinforcement learning, improving credit assignment and reward propagation.</p><hr><h3>Partitioning Message Passing for Graph Fraud Detection</h3>
<p><a href='https://openreview.net/forum?id=tEgrUrUuwA'>https://openreview.net/forum?id=tEgrUrUuwA</a></p>
<p><b>Compressor summary</b>: PMP is a new message passing method for Graph Fraud Detection that adapts to different label neighbors by using node-specific aggregation functions, improving GFD performance.</p><hr><h3>How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data</h3>
<p><a href='https://openreview.net/forum?id=tBROYsEz9G'>https://openreview.net/forum?id=tBROYsEz9G</a></p>
<p><b>Compressor summary</b>: The paper proposes Constrained Deep Generative Models (C-DGMs) that enforce compliance with background knowledge using a Constraint Layer, improving utility and detection in tabular data generation tasks.</p><hr><h3>Language Model Inversion</h3>
<p><a href='https://openreview.net/forum?id=t9dWHpGkPj'>https://openreview.net/forum?id=t9dWHpGkPj</a></p>
<p><b>Compressor summary</b>: The paper investigates language model inversion to recover hidden text using next-token probabilities and proposes a method that achieves high BLEU and F1 scores.</p><hr><h3>Tailoring Self-Rationalizers with Multi-Reward Distillation</h3>
<p><a href='https://openreview.net/forum?id=t8eO0CiZJV'>https://openreview.net/forum?id=t8eO0CiZJV</a></p>
<p><b>Compressor summary</b>: MaRio is a method to improve small language models' ability to generate rationales for question answering by optimizing multiple properties such as plausibility, diversity, and consistency.</p><hr><h3>Classification with Conceptual Safeguards</h3>
<p><a href='https://openreview.net/forum?id=t8cBsT9mcg'>https://openreview.net/forum?id=t8cBsT9mcg</a></p>
<p><b>Compressor summary</b>: The paper proposes a concept bottleneck model that uses soft concepts and concept confirmation to enhance selective classification in deep learning, improving both coverage and performance under uncertainty.</p><hr><h3>Controlling Vision-Language Models for Universal Image Restoration</h3>
<p><a href='https://openreview.net/forum?id=t3vnnLeajU'>https://openreview.net/forum?id=t3vnnLeajU</a></p>
<p><b>Compressor summary</b>: DA-CLIP is a degradation-aware vision-language model that adapts to different types of image corruptions and achieves state-of-the-art performance on image restoration tasks using cross-attention and a controller that predicts both feature embeddings and degradation features.</p><hr><h3>Frozen Transformers in Language Models Are Effective Visual Encoder Layers</h3>
<p><a href='https://openreview.net/forum?id=t0FI3Q66K5'>https://openreview.net/forum?id=t0FI3Q66K5</a></p>
<p><b>Compressor summary</b>: The paper demonstrates that LLMs can effectively encode visual tasks without language, using a frozen transformer block from pre-trained models as an encoder, and proposes a hypothesis to explain this phenomenon.</p><hr><h3>BatteryML:An Open-source platform for Machine Learning on Battery Degradation</h3>
<p><a href='https://openreview.net/forum?id=sxGugrYhP9'>https://openreview.net/forum?id=sxGugrYhP9</a></p>
<p><b>Compressor summary</b>: BatteryML is an open-source platform that simplifies battery life prediction by providing data processing, feature extraction, and various models, while also establishing unified standards for the field.</p><hr><h3>Real-Fake: Effective Training Data Synthesis Through Distribution Matching</h3>
<p><a href='https://openreview.net/forum?id=svIdLLZpsA'>https://openreview.net/forum?id=svIdLLZpsA</a></p>
<p><b>Compressor summary</b>: The text discusses how synthetic training data can be improved for deep learning models by analyzing its principles from a distribution-matching perspective, leading to better performance in various image classification tasks and other applications.</p><hr><h3>VFLAIR: A Research Library and Benchmark for Vertical Federated Learning</h3>
<p><a href='https://openreview.net/forum?id=sqRgz88TM3'>https://openreview.net/forum?id=sqRgz88TM3</a></p>
<p><b>Compressor summary</b>: The paper introduces VFLAIR, a framework for vertical federated learning that supports various models, datasets, and protocols, and evaluates 11 attacks and 8 defenses to provide insights for deploying VFL securely.</p><hr><h3>Measuring Vision-Language STEM Skills of Neural Models</h3>
<p><a href='https://openreview.net/forum?id=spvaV5LELF'>https://openreview.net/forum?id=spvaV5LELF</a></p>
<p><b>Compressor summary</b>: The text introduces a new challenge dataset for testing neural models' STEM skills, requiring understanding of multimodal vision-language information, and reveals that current models perform poorly compared to elementary students.</p><hr><h3>Likelihood Training of Cascaded Diffusion Models via Hierarchical Volume-preserving Maps</h3>
<p><a href='https://openreview.net/forum?id=sojpn00o8z'>https://openreview.net/forum?id=sojpn00o8z</a></p>
<p><b>Compressor summary</b>: Cascaded models are multi-scale generative models that can also be good likelihood models when using hierarchical volume-preserving maps, leading to improved performance on density estimation, lossless compression, and out-of-distribution detection tasks.</p><hr><h3>Whole-song Hierarchical Generation of Symbolic Music Using Cascaded Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=sn7CYWyavh'>https://openreview.net/forum?id=sn7CYWyavh</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel hierarchical language for generating whole-song pop music with high quality and controllability.</p><hr><h3>Energy-conserving equivariant GNN for elasticity of lattice architected metamaterials</h3>
<p><a href='https://openreview.net/forum?id=smy4DsUbBo'>https://openreview.net/forum?id=smy4DsUbBo</a></p>
<p><b>Compressor summary</b>: The paper proposes a higher-order graph neural network model that can quickly and accurately predict the stiffness of lattices, by using symmetries and energy conservation rules.</p><hr><h3>LRM: Large Reconstruction Model for Single Image to 3D</h3>
<p><a href='https://openreview.net/forum?id=sllU8vvsFF'>https://openreview.net/forum?id=sllU8vvsFF</a></p>
<p><b>Compressor summary</b>: The Large Reconstruction Model (LRM) uses a transformer-based architecture with many learnable parameters to predict a neural radiance field from a single image and reconstruct 3D objects in high quality and generalize across various inputs.</p><hr><h3>How connectivity structure shapes rich and lazy learning in neural circuits</h3>
<p><a href='https://openreview.net/forum?id=slSmYGc8ee'>https://openreview.net/forum?id=slSmYGc8ee</a></p>
<p><b>Compressor summary</b>: This paper explores how the structure of the initial weights in neural networks affects their learning dynamics, finding that high-rank initializations lead to lazier learning while low-rank initializations promote richer learning, with some exceptions related to task and data statistics.</p><hr><h3>Probabilistic Self-supervised Representation Learning via Scoring Rules Minimization</h3>
<p><a href='https://openreview.net/forum?id=skcTCdJz0f'>https://openreview.net/forum?id=skcTCdJz0f</a></p>
<p><b>Compressor summary</b>: ProSMin is a probabilistic self-supervised learning method that uses two neural networks to enhance representation quality, mitigate dimensional collapse, and improve performance on various downstream tasks.</p><hr><h3>Large Language Models Are Not Robust Multiple Choice Selectors</h3>
<p><a href='https://openreview.net/forum?id=shr9PXz7T0'>https://openreview.net/forum?id=shr9PXz7T0</a></p>
<p><b>Compressor summary</b>: This paper shows that large language models are biased towards specific option IDs in multiple choice questions due to token bias, and proposes a debiasing method called PriDe that separates the prior bias from the prediction distribution.</p><hr><h3>Alignment as Reward-Guided Search</h3>
<p><a href='https://openreview.net/forum?id=shgx0eqdw6'>https://openreview.net/forum?id=shgx0eqdw6</a></p>
<p><b>Compressor summary</b>: ARGS is a novel framework that improves language model alignment with human preferences by adjusting probabilistic predictions using a reward signal during decoding, resulting in better average rewards and semantic diversity.</p><hr><h3>Let Models Speak Ciphers: Multiagent Debate through Embeddings</h3>
<p><a href='https://openreview.net/forum?id=sehRvaIPQQ'>https://openreview.net/forum?id=sehRvaIPQQ</a></p>
<p><b>Compressor summary</b>: CIPIR is a communication protocol that uses raw transformer output embeddings instead of natural language to enhance reasoning ability in large language models, outperforming existing methods by 1-3.5%.</p><hr><h3>NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=samyfu6G93'>https://openreview.net/forum?id=samyfu6G93</a></p>
<p><b>Compressor summary</b>: NeuroBack is a novel approach that uses Graph Neural Networks to predict phases of variables in the SAT problem, enabling faster and more efficient solving on CPU without relying on GPU resources.</p><hr><h3>DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines</h3>
<p><a href='https://openreview.net/forum?id=sY5N0zY5Od'>https://openreview.net/forum?id=sY5N0zY5Od</a></p>
<p><b>Compressor summary</b>: DSPy is a programming model for developing and optimizing language models pipelines using text transformation graphs, enabling sophisticated tasks and outperforming hard-coded prompt templates and few-shot prompting.</p><hr><h3>Understanding when Dynamics-Invariant Data Augmentations Benefit Model-free Reinforcement Learning Updates</h3>
<p><a href='https://openreview.net/forum?id=sVEu295o70'>https://openreview.net/forum?id=sVEu295o70</a></p>
<p><b>Compressor summary</b>: The paper investigates how state-action coverage, reward density, and the number of augmented transitions per update affect data efficiency in reinforcement learning tasks with sparse rewards and dynamics-invisible data augmentation.</p><hr><h3>Evaluating Representation Learning on the Protein Structure Universe</h3>
<p><a href='https://openreview.net/forum?id=sTYuRVrdK3'>https://openreview.net/forum?id=sTYuRVrdK3</a></p>
<p><b>Compressor summary</b>: The paper introduces a benchmark suite for evaluating protein structure representation learning methods, comparing various pre-training and downstream tasks to improve understanding of protein structures and functions.</p><hr><h3>Divide and not forget: Ensemble of  selectively trained experts  in Continual Learning</h3>
<p><a href='https://openreview.net/forum?id=sSyytcewxe'>https://openreview.net/forum?id=sSyytcewxe</a></p>
<p><b>Compressor summary</b>: SEED is a novel approach for class-incremental learning that selects and fine-tunes the most optimal expert for each task using Gaussian distributions, improving diversity and stability.</p><hr><h3>Adapting to Distribution Shift by Visual Domain Prompt Generation</h3>
<p><a href='https://openreview.net/forum?id=sSaN4gxuEf'>https://openreview.net/forum?id=sSaN4gxuEf</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to adapt a model using few unlabeled data, by leveraging pre-trained backbones, source domains, and a domain prompt generator that guides the features towards the target domain.</p><hr><h3>Consistent4D: Consistent 360 Dynamic Object Generation from Monocular Video</h3>
<p><a href='https://openreview.net/forum?id=sPUrdFGepF'>https://openreview.net/forum?id=sPUrdFGepF</a></p>
<p><b>Compressor summary</b>: Consistent4D is a method to generate 4D dynamic objects from monocular videos without multi-view data or camera calibration, using a cascade DyNeRF and interpolation-driven consistency loss.</p><hr><h3>Fiber Monte Carlo</h3>
<p><a href='https://openreview.net/forum?id=sP1tCl2QBk'>https://openreview.net/forum?id=sP1tCl2QBk</a></p>
<p><b>Compressor summary</b>: The paper proposes a differentiable Monte Carlo estimator that samples line segments instead of points for integrals with discontinuous integrands, enabling gradient-based optimization in applications like robotics and graphics.</p><hr><h3>NeRM: Learning Neural Representations for High-Framerate Human Motion Synthesis</h3>
<p><a href='https://openreview.net/forum?id=sOJriBlOFd'>https://openreview.net/forum?id=sOJriBlOFd</a></p>
<p><b>Compressor summary</b>: NeRM is a generative model that uses implicit neural representations to synthesize realistic human motions with any framerate, while being memory-efficient and fast.</p><hr><h3>Neural Common Neighbor with Completion for Link Prediction</h3>
<p><a href='https://openreview.net/forum?id=sNFLN3itAd'>https://openreview.net/forum?id=sNFLN3itAd</a></p>
<p><b>Compressor summary</b>: The text introduces a novel link prediction model, NCNC, that leverages structural features and graph completion to overcome graph incompleteness, outperforming existing models.</p><hr><h3>Towards Meta-Pruning via Optimal Transport</h3>
<p><a href='https://openreview.net/forum?id=sMoifbuxjB'>https://openreview.net/forum?id=sMoifbuxjB</a></p>
<p><b>Compressor summary</b>: Intra-Fusion combines pruning and fusion methods in neural networks to achieve better compression, accuracy, and training time than standard pruning alone.</p><hr><h3>On the Analysis of GAN-based Image-to-Image Translation with Gaussian Noise Injection</h3>
<p><a href='https://openreview.net/forum?id=sLregLuXpn'>https://openreview.net/forum?id=sLregLuXpn</a></p>
<p><b>Compressor summary</b>: The paper provides a robust theoretical framework to understand and optimize the role of Gaussian noise injection in image-to-image translation models for better performance in noisy environments.</p><hr><h3>Transformers can optimally learn regression mixture models</h3>
<p><a href='https://openreview.net/forum?id=sLkj91HIZU'>https://openreview.net/forum?id=sLkj91HIZU</a></p>
<p><b>Compressor summary</b>: This paper explores if transformers can learn to make optimal predictions for mixtures of regressions and shows they can achieve low error, sample efficiency, and robustness.</p><hr><h3>NuwaDynamics: Discovering and Updating in Causal Spatio-Temporal Modeling</h3>
<p><a href='https://openreview.net/forum?id=sLdVl0q68X'>https://openreview.net/forum?id=sLdVl0q68X</a></p>
<p><b>Compressor summary</b>: NuwaDynamics is a causal concept for spatio-temporal prediction that identifies important regions in data and enables models to reason causally, improving results on real-world tasks.</p><hr><h3>Fair and Efficient Contribution Valuation for Vertical Federated Learning</h3>
<p><a href='https://openreview.net/forum?id=sLQb8q0sUi'>https://openreview.net/forum?id=sLQb8q0sUi</a></p>
<p><b>Compressor summary</b>: Vertical federated Shapley value (VerFedSV) is a fair and efficient contribution valuation metric for data sources in vertical federated learning scenarios.</p><hr><h3>Addressing Catastrophic Forgetting and Loss of Plasticity in Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=sKPzAXoylB'>https://openreview.net/forum?id=sKPzAXoylB</a></p>
<p><b>Compressor summary</b>: UPGD is a novel approach for continual learning of representations that combats catastrophic forgetting and loss of plasticity by applying different size modifications to useful and unuseful units.</p><hr><h3>ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation</h3>
<p><a href='https://openreview.net/forum?id=sJ88Wg5Bp5'>https://openreview.net/forum?id=sJ88Wg5Bp5</a></p>
<p><b>Compressor summary</b>: The paper proposes ViDA, a visual domain adapter for continual test-time adaptation, which uses high-rank and low-rank features to handle domain-specific and domain-shared knowledge and achieves state-of-the-art performance in classification and segmentation tasks.</p><hr><h3>Branch-GAN: Improving Text Generation with (not so) Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=sHEJJmzBIN'>https://openreview.net/forum?id=sHEJJmzBIN</a></p>
<p><b>Compressor summary</b>: The authors propose a computationally efficient GAN approach for open domain text generation using Transformer models, which improves text quality and avoids sparsity problems, outperforming larger baseline models with fewer parameters.</p><hr><h3>Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation</h3>
<p><a href='https://openreview.net/forum?id=sGVmr7KHfn'>https://openreview.net/forum?id=sGVmr7KHfn</a></p>
<p><b>Compressor summary</b>: MemSPM is a novel method that mines sub-classes within categories to reduce feature gap and improve adaptation performance when there is significant concept shift between samples.</p><hr><h3>Learning Interactive Real-World Simulators</h3>
<p><a href='https://openreview.net/forum?id=sFyTZEqmUY'>https://openreview.net/forum?id=sFyTZEqmUY</a></p>
<p><b>Compressor summary</b>: The authors propose UniSim, a universal simulator for real-world interaction, which can be trained on diverse datasets and enable various intelligent applications.</p><hr><h3>MagicDrive: Street View Generation with Diverse 3D Geometry Control</h3>
<p><a href='https://openreview.net/forum?id=sBQwvucduK'>https://openreview.net/forum?id=sBQwvucduK</a></p>
<p><b>Compressor summary</b>: MagicDrive is a framework for generating street views with precise 3D control and high-fidelity synthesis, enabling better 3D perception tasks like object detection.</p><hr><h3>SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series</h3>
<p><a href='https://openreview.net/forum?id=s9z0HzWJJp'>https://openreview.net/forum?id=s9z0HzWJJp</a></p>
<p><b>Compressor summary</b>: SocioDojo is a lifelong learning environment for creating autonomous agents that can analyze and make decisions on societal topics using information sources, knowledge bases, time series, and analysis tools.</p><hr><h3>f-FERM: A  Scalable Framework for  Robust Fair Empirical Risk Minimization</h3>
<p><a href='https://openreview.net/forum?id=s90VIdza2K'>https://openreview.net/forum?id=s90VIdza2K</a></p>
<p><b>Compressor summary</b>: The paper proposes a stochastic optimization framework for fair machine learning that works with small data batches, has theoretical convergence guarantees, and performs well under distribution shifts.</p><hr><h3>Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks</h3>
<p><a href='https://openreview.net/forum?id=s8cMuxI5gu'>https://openreview.net/forum?id=s8cMuxI5gu</a></p>
<p><b>Compressor summary</b>: This paper proposes an algorithm to recover augmented soft labels and input features from gradients in federated learning, considering realistic label smoothing and mixup techniques, and shows its effectiveness in attacking the system.</p><hr><h3>The Human-AI Substitution game: active learning from a strategic labeler</h3>
<p><a href='https://openreview.net/forum?id=s5hSp7EdL3'>https://openreview.net/forum?id=s5hSp7EdL3</a></p>
<p><b>Compressor summary</b>: The paper studies how learning from a strategic labeler who selectively abstains from labeling can affect the learning process and proposes a near-optimal algorithm for this scenario.</p><hr><h3>BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection</h3>
<p><a href='https://openreview.net/forum?id=s56xikpD92'>https://openreview.net/forum?id=s56xikpD92</a></p>
<p><b>Compressor summary</b>: The paper proposes a defense method called BaDExpert that finetunes a backdoored DNN to recognize only its malicious inputs, thus detecting and filtering out the backdoors.</p><hr><h3>Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models</h3>
<p><a href='https://openreview.net/forum?id=s2NjWfaYdZ'>https://openreview.net/forum?id=s2NjWfaYdZ</a></p>
<p><b>Compressor summary</b>: KPrune is a retraining-free algorithm that accurately compresses pretrained encoder-based language models by preserving useful knowledge and improving F1 score on the SQuAD benchmark.</p><hr><h3>Neural SDF Flow for 3D Reconstruction of Dynamic Scenes</h3>
<p><a href='https://openreview.net/forum?id=rzF0R6GOd4'>https://openreview.net/forum?id=rzF0R6GOd4</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for 3D reconstruction of dynamic scenes from multi-view videos using Signed Distance Function flow, which handles topology changes and outperforms existing methods.</p><hr><h3>CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images</h3>
<p><a href='https://openreview.net/forum?id=rzBskAEmoc'>https://openreview.net/forum?id=rzBskAEmoc</a></p>
<p><b>Compressor summary</b>: CAMIL is a new model for cancer diagnosis from tissue biopsy images that considers neighboring information and achieves better performance than existing methods.</p><hr><h3>Improving protein optimization with smoothed fitness landscapes</h3>
<p><a href='https://openreview.net/forum?id=rxlF2Zv8x0'>https://openreview.net/forum?id=rxlF2Zv8x0</a></p>
<p><b>Compressor summary</b>: The paper proposes a method, Gibbs sampling with Graph-based Smoothing (GGS), to smooth the fitness landscape and improve protein optimization using graph signals and energy-based models.</p><hr><h3>Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Metrics</h3>
<p><a href='https://openreview.net/forum?id=rxVBKhyfSo'>https://openreview.net/forum?id=rxVBKhyfSo</a></p>
<p><b>Compressor summary</b>: SelMix is a technique to optimize pre-trained models for practical performance objectives by mixing features of samples from specific classes.</p><hr><h3>Learning to Act without Actions</h3>
<p><a href='https://openreview.net/forum?id=rvUq3cxpDF'>https://openreview.net/forum?id=rvUq3cxpDF</a></p>
<p><b>Compressor summary</b>: LAPO is a method to train deep RL policies from action-free videos by inferring latent actions, which enables web data pre-training and fine-tuning.</p><hr><h3>Long-Short-Range Message-Passing: A Fragmentation-Based Framework to Capture Non-Local Atomistic Interactions</h3>
<p><a href='https://openreview.net/forum?id=rvDQtdMnOl'>https://openreview.net/forum?id=rvDQtdMnOl</a></p>
<p><b>Compressor summary</b>: The paper proposes a new graph neural network method (LSR-MP) that improves the simulation of chemical and biological systems using ab initio molecular dynamics, achieving state-of-the-art results with reduced error.</p><hr><h3>Conformal Prediction via Regression-as-Classification</h3>
<p><a href='https://openreview.net/forum?id=rulxyXjf46'>https://openreview.net/forum?id=rulxyXjf46</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to use conformal prediction for regression by converting it to a classification problem with a new loss function and modified techniques.</p><hr><h3>SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore</h3>
<p><a href='https://openreview.net/forum?id=ruk0nyQPec'>https://openreview.net/forum?id=ruk0nyQPec</a></p>
<p><b>Compressor summary</b>: SILO is a language model that uses a combination of a parametric model trained on public domain text and a nonparametric datastore with customizable data attribution to enable compliant use of copyrighted or restricted data during inference.</p><hr><h3>A Data-Driven Measure of Relative Uncertainty for Misclassification Detection</h3>
<p><a href='https://openreview.net/forum?id=ruGY8v10mK'>https://openreview.net/forum?id=ruGY8v10mK</a></p>
<p><b>Compressor summary</b>: The paper proposes a new data-driven measure of relative uncertainty for detecting misclassified instances in machine learning models based on soft-predictions and shows its effectiveness in image classification tasks.</p><hr><h3>Variational Inference for SDEs Driven by Fractional Noise</h3>
<p><a href='https://openreview.net/forum?id=rtx8B94JMS'>https://openreview.net/forum?id=rtx8B94JMS</a></p>
<p><b>Compressor summary</b>: The paper presents a new method for inferring parameters in neural stochastic differential equations using fractional Brownian motion and variational methods, which can capture long-term dependencies and be applied to video prediction.</p><hr><h3>Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift</h3>
<p><a href='https://openreview.net/forum?id=rtl4XnJYBh'>https://openreview.net/forum?id=rtl4XnJYBh</a></p>
<p><b>Compressor summary</b>: This paper analyzes two mechanisms behind multimodal contrastive learning's success in robust representation learning, and shows how rich captions help improve zero-shot classification under distribution shift.</p><hr><h3>Federated Wasserstein Distance</h3>
<p><a href='https://openreview.net/forum?id=rsg1mvUahT'>https://openreview.net/forum?id=rsg1mvUahT</a></p>
<p><b>Compressor summary</b>: FedWad is a principled algorithm that computes Wasserstein distance between two distributions in a federated manner by manipulating and exchanging geodesics, with convergence guarantees and empirical benefits.</p><hr><h3>DistillSpec: Improving Speculative Decoding via Knowledge Distillation</h3>
<p><a href='https://openreview.net/forum?id=rsY6J3ZaTF'>https://openreview.net/forum?id=rsY6J3ZaTF</a></p>
<p><b>Compressor summary</b>: DistillSpec is a method that uses knowledge distillation to improve the alignment between a fast draft and a slow target language model for speculative decoding, achieving significant speedups and latency control.</p><hr><h3>An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization</h3>
<p><a href='https://openreview.net/forum?id=rpH9FcCEV6'>https://openreview.net/forum?id=rpH9FcCEV6</a></p>
<p><b>Compressor summary</b>: The paper proposes a query-based membership inference attack on diffusion models for image and audio generation tasks and analyzes their privacy robustness in the text-to-speech task.</p><hr><h3>Faithful Vision-Language Interpretation via Concept Bottleneck Models</h3>
<p><a href='https://openreview.net/forum?id=rp0EdI8X4e'>https://openreview.net/forum?id=rp0EdI8X4e</a></p>
<p><b>Compressor summary</b>: The authors propose a new concept called Faithful Vision-Language Concept (FVLC) models that improve interpretable machine learning by addressing the instability issue of label-free bottleneck models and maintaining good performance.</p><hr><h3>Stylized Offline Reinforcement Learning: Extracting Diverse High-Quality Behaviors from Heterogeneous Datasets</h3>
<p><a href='https://openreview.net/forum?id=rnHNDihrIT'>https://openreview.net/forum?id=rnHNDihrIT</a></p>
<p><b>Compressor summary</b>: SORL is a novel method for offline RL that leverages policy diversity by alternating between policy learning and trajectory clustering.</p><hr><h3>Intriguing Properties of Generative Classifiers</h3>
<p><a href='https://openreview.net/forum?id=rmg0qMKYRQ'>https://openreview.net/forum?id=rmg0qMKYRQ</a></p>
<p><b>Compressor summary</b>: The study compares text-to-image classifiers based on generative models with discriminative models and human perception, finding that generative models have impressive properties similar to human object recognition.</p><hr><h3>On the Provable Advantage of Unsupervised Pretraining</h3>
<p><a href='https://openreview.net/forum?id=rmXXKxQpOR'>https://openreview.net/forum?id=rmXXKxQpOR</a></p>
<p><b>Compressor summary</b>: This paper provides a theoretical understanding of why unsupervised pretraining helps in machine learning systems by studying a generic framework with latent variable models and empirical risk minimization.</p><hr><h3>Gen-Z: Generative Zero-Shot Text Classification with Contextualized Label Descriptions</h3>
<p><a href='https://openreview.net/forum?id=rkplYfqUr0'>https://openreview.net/forum?id=rkplYfqUr0</a></p>
<p><b>Compressor summary</b>: Gen-Z is a generative framework for zero-shot text classification that uses natural language labels and improves robustness to prompt variations and allows personalization.</p><hr><h3>A Branching Decoder for Set Generation</h3>
<p><a href='https://openreview.net/forum?id=riNuqYiD66'>https://openreview.net/forum?id=riNuqYiD66</a></p>
<p><b>Compressor summary</b>: The paper proposes a branching decoder for generating sets of text without order dependence, which improves effectiveness and efficiency over the existing sequential decoder.</p><hr><h3>TabR: Tabular Deep Learning Meets Nearest Neighbors in 2023</h3>
<p><a href='https://openreview.net/forum?id=rhgIgTSSxW'>https://openreview.net/forum?id=rhgIgTSSxW</a></p>
<p><b>Compressor summary</b>: TabR is a new tabular deep learning model that uses a novel attention-like mechanism to retrieve and utilize nearest neighbors for better prediction, outperforming existing methods on several benchmarks.</p><hr><h3>Does Progress On Object Recognition Benchmarks Improve Generalization on Crowdsourced, Global Data?</h3>
<p><a href='https://openreview.net/forum?id=rhaQbS3K3R'>https://openreview.net/forum?id=rhaQbS3K3R</a></p>
<p><b>Compressor summary</b>: The authors study how well object recognition models perform on crowdsourced global datasets and find that progress on these datasets lags behind standard benchmarks and may increase geographic disparities in model performance.</p><hr><h3>Image Inpainting via Iteratively Decoupled Probabilistic Modeling</h3>
<p><a href='https://openreview.net/forum?id=rUf9G9k2im'>https://openreview.net/forum?id=rUf9G9k2im</a></p>
<p><b>Compressor summary</b>: The pixel spread model combines GANs' optimization efficiency with probabilistic models' prediction tractability to improve image inpainting quality and efficiency, achieving state-of-the-art results.</p><hr><h3>Octavius: Mitigating Task Interference in MLLMs via MoE</h3>
<p><a href='https://openreview.net/forum?id=rTDyN8yajn'>https://openreview.net/forum?id=rTDyN8yajn</a></p>
<p><b>Compressor summary</b>: The paper introduces Octavius, a framework for studying multimodal learning with large language models, which combines MoE and LoRA to reduce interference and improve performance on various tasks.</p><hr><h3>Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching</h3>
<p><a href='https://openreview.net/forum?id=rTBL8OhdhH'>https://openreview.net/forum?id=rTBL8OhdhH</a></p>
<p><b>Compressor summary</b>: The authors propose a new algorithm for Dataset Distillation that matches the difficulty of the generated patterns with the size of the synthetic dataset, enabling lossless distillation for larger datasets than previous methods.</p><hr><h3>Functional Interpolation for Relative Positions improves Long Context Transformers</h3>
<p><a href='https://openreview.net/forum?id=rR03qFesqk'>https://openreview.net/forum?id=rR03qFesqk</a></p>
<p><b>Compressor summary</b>: FIRE is a new position encoding method for Transformers that improves their ability to handle longer inputs and generalizes better to different tasks.</p><hr><h3>Like Oil and Water: Group Robustness Methods and Poisoning Defenses Don't Mix</h3>
<p><a href='https://openreview.net/forum?id=rM9VJPB20F'>https://openreview.net/forum?id=rM9VJPB20F</a></p>
<p><b>Compressor summary</b>: The paper reveals that existing group robustness methods in machine learning unintentionally boost adversarial success by amplifying poison samples, and current poisoning defenses damage group robustness by eliminating minority samples along with poisons.</p><hr><h3>Masked Structural Growth for 2x Faster Language Model Pre-training</h3>
<p><a href='https://openreview.net/forum?id=rL7xsg1aRn'>https://openreview.net/forum?id=rL7xsg1aRn</a></p>
<p><b>Compressor summary</b>: The paper introduces MSG, a method to accelerate pre-training of large language models by growing the model in an optimal way with function-preserving operators that work regardless of weight initialization.</p><hr><h3>Understanding Addition in Transformers</h3>
<p><a href='https://openreview.net/forum?id=rIx1YXVWZb'>https://openreview.net/forum?id=rIx1YXVWZb</a></p>
<p><b>Compressor summary</b>: The paper analyzes a simple Transformer model for integer addition, explaining its algorithms, execution, and rare failures, to help understand and ensure safe and ethical AI use.</p><hr><h3>Curriculum reinforcement learning for quantum architecture search under hardware errors</h3>
<p><a href='https://openreview.net/forum?id=rINBD8jPoP'>https://openreview.net/forum?id=rINBD8jPoP</a></p>
<p><b>Compressor summary</b>: This text describes a new quantum architecture search algorithm (CRLQAS) that improves simulation speed, explores the search space efficiently, and optimizes for shorter circuits in noisy environments.</p><hr><h3>Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=rHzapPnCgT'>https://openreview.net/forum?id=rHzapPnCgT</a></p>
<p><b>Compressor summary</b>: This paper introduces Progressive Conditional Diffusion Models (PCDMs), which incrementally synthesize realistic person images with different poses by using three stages of conditional diffusion models.</p><hr><h3>Progressive Fourier Neural Representation for Sequential Video Compilation</h3>
<p><a href='https://openreview.net/forum?id=rGFrRMBbOq'>https://openreview.net/forum?id=rGFrRMBbOq</a></p>
<p><b>Compressor summary</b>: PFNR is a novel method to encode multiple complex videos using adaptive sub-modules in Fourier space, enabling efficient and scalable continual learning.</p><hr><h3>Variance-aware Regret Bounds for Stochastic Contextual Dueling Bandits</h3>
<p><a href='https://openreview.net/forum?id=rDH7dIFn20'>https://openreview.net/forum?id=rDH7dIFn20</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm for contextual dueling bandits that accounts for uncertainty in pairwise comparisons and achieves a better regret bound than existing methods.</p><hr><h3>Random Sparse Lifts: Construction, Analysis and Convergence of finite sparse networks</h3>
<p><a href='https://openreview.net/forum?id=rBH7x87VfJ'>https://openreview.net/forum?id=rBH7x87VfJ</a></p>
<p><b>Compressor summary</b>: The authors propose a framework to train overparameterized neural networks with gradient flow, showing that this method can achieve arbitrarily low loss and generalize common deep learning models by sparsifying their weight tensors.</p><hr><h3>S$2$AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic</h3>
<p><a href='https://openreview.net/forum?id=rAHcTCMaLc'>https://openreview.net/forum?id=rAHcTCMaLc</a></p>
<p><b>Compressor summary</b>: S$^2$AC is a MaxEnt RL algorithm using SVGD that computes entropy efficiently for EBM policies, achieving better performance than SQL and SAC in various tasks.</p><hr><h3>Object centric architectures enable efficient causal representation learning</h3>
<p><a href='https://openreview.net/forum?id=r9FsiXZxZt'>https://openreview.net/forum?id=r9FsiXZxZt</a></p>
<p><b>Compressor summary</b>: The paper proposes an object-centric architecture that uses weak supervision from sparse perturbations to disentangle latent variables of multiple objects, addressing the limitations of existing causal representation learning methods.</p><hr><h3>UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition</h3>
<p><a href='https://openreview.net/forum?id=r65xfUb76p'>https://openreview.net/forum?id=r65xfUb76p</a></p>
<p><b>Compressor summary</b>: The paper presents a method to train smaller and more efficient named entity recognition (NER) models from large language models (LLMs) by using mission-focused instruction tuning, achieving state-of-the-art results without direct supervision.</p><hr><h3>Linear Convergence Bounds for Diffusion Models via Stochastic Localization</h3>
<p><a href='https://openreview.net/forum?id=r5njV3BsuD'>https://openreview.net/forum?id=r5njV3BsuD</a></p>
<p><b>Compressor summary</b>: We present new linear convergence bounds for denoising diffusion models with finite second moment assumptions and show how to extend Girsanov-based methods and use stochastic localization theory in the proof.</p><hr><h3>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation</h3>
<p><a href='https://openreview.net/forum?id=r42tSSCHPh'>https://openreview.net/forum?id=r42tSSCHPh</a></p>
<p><b>Compressor summary</b>: The text describes a new attack method that exploits language models' generation methods to cause malicious behavior, highlighting the need for better alignment and safety evaluation of open-source large language models.</p><hr><h3>Effectively Leveraging Capacity for Improved Deterministic Robustness Certification</h3>
<p><a href='https://openreview.net/forum?id=qz3mcn99cu'>https://openreview.net/forum?id=qz3mcn99cu</a></p>
<p><b>Compressor summary</b>: The text describes a study that improves robustness of neural networks against adversarial attacks using Lipschitz-based methods and novel techniques, achieving significant performance gains.</p><hr><h3>Robust Training of Federated Models with Extremely Label Deficiency</h3>
<p><a href='https://openreview.net/forum?id=qxLVaYbsSI'>https://openreview.net/forum?id=qxLVaYbsSI</a></p>
<p><b>Compressor summary</b>: Twinsight is a novel FSSL method that trains two models simultaneously with different objectives to reduce gradient conflicts and improve performance.</p><hr><h3>Canonpipe: Data Debugging with Shapley Importance over Machine Learning Pipelines</h3>
<p><a href='https://openreview.net/forum?id=qxGXjWxabq'>https://openreview.net/forum?id=qxGXjWxabq</a></p>
<p><b>Compressor summary</b>: The paper presents Canonpipe, a method for computing Shapley-based data importance over ML pipelines, which can efficiently discover data errors and sometimes perform better than existing methods.</p><hr><h3>From Graphs to Hypergraphs: Hypergraph Projection and its Remediation</h3>
<p><a href='https://openreview.net/forum?id=qwYKE3VB2h'>https://openreview.net/forum?id=qwYKE3VB2h</a></p>
<p><b>Compressor summary</b>: This paper studies how using graphs instead of hypergraphs to model interconnected systems can lose higher-order information, and proposes a learning method to recover this lost information from real-world datasets.</p><hr><h3>Behaviour Distillation</h3>
<p><a href='https://openreview.net/forum?id=qup9xD8mW4'>https://openreview.net/forum?id=qup9xD8mW4</a></p>
<p><b>Compressor summary</b>: The paper introduces behaviour distillation and HaDES, a method to create small synthetic datasets for reinforcement learning tasks without expert data.</p><hr><h3>Towards domain-invariant Self-Supervised Learning with Batch Styles Standardization</h3>
<p><a href='https://openreview.net/forum?id=qtE9K23ISq'>https://openreview.net/forum?id=qtE9K23ISq</a></p>
<p><b>Compressor summary</b>: Batch Styles Standardization (BSS) is a Fourier-based method that standardizes image styles within a batch, enhancing domain-invariance in self-supervised learning models without requiring domain labels or specific architectures, and improving their performance on unseen domains.</p><hr><h3>On the Expressivity of Objective-Specification Formalisms in Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=qr4ECbGcSj'>https://openreview.net/forum?id=qr4ECbGcSj</a></p>
<p><b>Compressor summary</b>: The text compares 17 reinforcement learning objective-specification formalisms, revealing their expressivity limitations and implications for policy optimisation and reward learning.</p><hr><h3>Demystifying Embedding Spaces using Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=qoYogklIPz'>https://openreview.net/forum?id=qoYogklIPz</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to make embeddings more interpretable by using large language models to generate narratives from them, enabling querying and exploration of complex data in various tasks.</p><hr><h3>Grounding Language Plans in Demonstrations Through Counter-Factual Perturbations</h3>
<p><a href='https://openreview.net/forum?id=qoHeuRAcSl'>https://openreview.net/forum?id=qoHeuRAcSl</a></p>
<p><b>Compressor summary</b>: This paper proposes a method to use large language models to guide learning structures and constraints in robot manipulation tasks, using mode families as an intermediate layer between high-level language and low-level physical trajectories.</p><hr><h3>Teach LLMs to Phish: Stealing Private Information from Language Models</h3>
<p><a href='https://openreview.net/forum?id=qo21ZlfNu6'>https://openreview.net/forum?id=qo21ZlfNu6</a></p>
<p><b>Compressor summary</b>: The authors propose a new data extraction attack called "neural phishing" that exploits large language models trained on private data to extract sensitive information with high success rates.</p><hr><h3>Fast Imitation via Behavior Foundation Models</h3>
<p><a href='https://openreview.net/forum?id=qnWtw3l0jb'>https://openreview.net/forum?id=qnWtw3l0jb</a></p>
<p><b>Compressor summary</b>: Imitation learning using RL foundation models can quickly mimic expert behavior with few demonstrations, outperforming standard approaches in speed and efficiency.</p><hr><h3>Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling</h3>
<p><a href='https://openreview.net/forum?id=qmXedvwrT1'>https://openreview.net/forum?id=qmXedvwrT1</a></p>
<p><b>Compressor summary</b>: LEGO bricks are a new adaptable network backbone for iterative refinement in diffusion models, enabling efficient generation of higher-resolution images with variable resolutions and faster convergence.</p><hr><h3>Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=qiduMcw3CU'>https://openreview.net/forum?id=qiduMcw3CU</a></p>
<p><b>Compressor summary</b>: The text proposes a framework for agents to learn skill primitives and compose them flexibly to achieve temporal logic specifications in various environments.</p><hr><h3>Learning with Language-Guided State Abstractions</h3>
<p><a href='https://openreview.net/forum?id=qi5Xa2cOZg'>https://openreview.net/forum?id=qi5Xa2cOZg</a></p>
<p><b>Compressor summary</b>: LGA is a method that uses natural language to automatically create state abstractions for imitation learning, improving policy generalization and robustness.</p><hr><h3>A Newborn Embodied Turing Test for Comparing Object Segmentation Across Animals and Machines</h3>
<p><a href='https://openreview.net/forum?id=qhkEOCcVX9'>https://openreview.net/forum?id=qhkEOCcVX9</a></p>
<p><b>Compressor summary</b>: The study introduces a Newborn Embodied Turing Test (NETT) for object segmentation, comparing the learning abilities of newborn chicks and machine-learning algorithms in recognizing objects across different backgrounds and viewpoints.</p><hr><h3>Fast Equilibrium of SGD in Generic Situations</h3>
<p><a href='https://openreview.net/forum?id=qgWJkDiI5p'>https://openreview.net/forum?id=qgWJkDiI5p</a></p>
<p><b>Compressor summary</b>: This paper investigates the Fast Equilibrium conjecture for normalization layers in deep learning and proves it under more general assumptions than previous works.</p><hr><h3>Closing the Gap between TD Learning and Supervised Learning - A Generalisation Point of View.</h3>
<p><a href='https://openreview.net/forum?id=qg5JENs0N4'>https://openreview.net/forum?id=qg5JENs0N4</a></p>
<p><b>Compressor summary</b>: The paper studies how reinforcement learning algorithms can use past experience to solve new tasks without explicit stitching mechanisms, showing that this property depends on a form of generalization different from i.i.d. generalization, and proposes data augmentation as a way to improve it.</p><hr><h3>Neural Probabilistic Protein-Protein Docking via a Differentiable Energy Model</h3>
<p><a href='https://openreview.net/forum?id=qg2boc2AwU'>https://openreview.net/forum?id=qg2boc2AwU</a></p>
<p><b>Compressor summary</b>: The paper presents EBMDock, a geometric deep learning framework that uses statistical potential and Langevin dynamics to sample protein-protein docking poses from a probability distribution, achieving superior results on two benchmarks.</p><hr><h3>Diverse Projection Ensembles for Distributional Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=qe49ybvvPs'>https://openreview.net/forum?id=qe49ybvvPs</a></p>
<p><b>Compressor summary</b>: Distributional RL algorithms learn return distributions instead of their expected values, using diverse projections and representations to improve uncertainty estimation and exploration.</p><hr><h3>Transformer-Modulated Diffusion Models for Probabilistic Multivariate  Time Series Forecasting</h3>
<p><a href='https://openreview.net/forum?id=qae04YACHs'>https://openreview.net/forum?id=qae04YACHs</a></p>
<p><b>Compressor summary</b>: The Transformer-Modulated Diffusion Model (TMDM) combines transformers and conditional diffusion to predict distribution for multivariate time series with uncertainty.</p><hr><h3>Learning Thresholds with Latent Values and Censored Feedback</h3>
<p><a href='https://openreview.net/forum?id=qaKRfobbTg'>https://openreview.net/forum?id=qaKRfobbTg</a></p>
<p><b>Compressor summary</b>: The paper studies how to learn a threshold in latent space for optimizing rewards in various scenarios, and provides query complexity and regret bounds for this problem.</p><hr><h3>Counting Graph Substructures with Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=qaJxPhkYtD'>https://openreview.net/forum?id=qaJxPhkYtD</a></p>
<p><b>Compressor summary</b>: This paper studies how Graph Neural Networks (GNNs) can learn to count substructures like cycles and connected components in graphs and designs a new GNN architecture that performs well in several tasks.</p><hr><h3>Large Language Models as Tool Makers</h3>
<p><a href='https://openreview.net/forum?id=qV83K9d5WB'>https://openreview.net/forum?id=qV83K9d5WB</a></p>
<p><b>Compressor summary</b>: The LLMs A s Tool Makers (LATM) framework allows large language models to create and reuse their own tools for problem-solving, improving efficiency and reducing serving costs.</p><hr><h3>Relay Diffusion: Unifying diffusion process across resolutions for image synthesis</h3>
<p><a href='https://openreview.net/forum?id=qTlcbLSm4p'>https://openreview.net/forum?id=qTlcbLSm4p</a></p>
<p><b>Compressor summary</b>: The Relay Diffusion Model (RDM) improves high-resolution image synthesis by transferring low-resolution images or noise into equivalent high-resolution ones via blurring diffusion and block noise, achieving state-of-the-art results on CelebA-HQ and ImageNet 256 times 256.</p><hr><h3>NP-GL: Extending Power of Nature from Binary Problems to Real-World Graph Learning</h3>
<p><a href='https://openreview.net/forum?id=qT7DXUmX7j'>https://openreview.net/forum?id=qT7DXUmX7j</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel framework for nature-powered graph learning that solves real-valued problems faster and more efficiently than neural networks, while maintaining similar accuracy.</p><hr><h3>Robustifying and Boosting Training-Free Neural Architecture Search</h3>
<p><a href='https://openreview.net/forum?id=qPloNoDJZn'>https://openreview.net/forum?id=qPloNoDJZn</a></p>
<p><b>Compressor summary</b>: The RoBoT algorithm combines and optimizes existing training-free metrics using Bayesian optimization, improving search performance for designing deep neural networks without training.</p><hr><h3>Guess & Sketch: Language Model Guided Transpilation</h3>
<p><a href='https://openreview.net/forum?id=qPFsIbF3V6'>https://openreview.net/forum?id=qPFsIbF3V6</a></p>
<p><b>Compressor summary</b>: The authors propose a neurosymbolic approach to automatic translation of assembly code programs, which combine the strengths of learned probabilistic models and symbolic solvers to achieve better performance than existing methods.</p><hr><h3>Masks, Signs, And Learning Rate Rewinding</h3>
<p><a href='https://openreview.net/forum?id=qODvxQ8TXW'>https://openreview.net/forum?id=qODvxQ8TXW</a></p>
<p><b>Compressor summary</b>: LRR outperforms IMP in finding lottery tickets by effectively combining mask learning and parameter optimization in overparameterized networks, allowing it to handle different mask signatures.</p><hr><h3>FairSeg: A Large-scale Medical Image Segmentation Dataset for Fairness Learning with Fair Error-Bound Scaling</h3>
<p><a href='https://openreview.net/forum?id=qNrJJZAKI3'>https://openreview.net/forum?id=qNrJJZAKI3</a></p>
<p><b>Compressor summary</b>: The paper introduces FairSeg, a new medical segmentation fairness dataset, and proposes a fair error-bound scaling approach to improve equity in AI models.</p><hr><h3>Zero and Few-shot Semantic Parsing with Ambiguous Inputs</h3>
<p><a href='https://openreview.net/forum?id=qL9gogRepu'>https://openreview.net/forum?id=qL9gogRepu</a></p>
<p><b>Compressor summary</b>: AmP introduces a framework, dataset, and challenge to study how text-to-code systems handle linguistic ambiguities, finding that large models perform poorly without explicit instruction but capture the distribution of meanings when ambiguity is present.</p><hr><h3>SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation</h3>
<p><a href='https://openreview.net/forum?id=qL6brrBDk2'>https://openreview.net/forum?id=qL6brrBDk2</a></p>
<p><b>Compressor summary</b>: SAFLEX is an efficient method that improves data augmentation by learning sample weights and soft labels using bilevel optimization, enhancing model performance across diverse datasets and tasks with minimal computational cost.</p><hr><h3>Learning Grounded Action Abstractions from Language</h3>
<p><a href='https://openreview.net/forum?id=qJ0Cfj4Ex9'>https://openreview.net/forum?id=qJ0Cfj4Ex9</a></p>
<p><b>Compressor summary</b>: The system learns symbolic action abstractions from language and uses them for long-horizon planning, improving accuracy and generalization.</p><hr><h3>Seer: Language Instructed Video Prediction with Latent Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=qHGgNyQk31'>https://openreview.net/forum?id=qHGgNyQk31</a></p>
<p><b>Compressor summary</b>: Seer is a novel model for robots to predict video frames from text inputs, which uses efficient techniques and achieves better results than CogVideo.</p><hr><h3>Protein-Ligand Interaction Prior for Binding-aware 3D Molecule Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=qH9nrMNTIW'>https://openreview.net/forum?id=qH9nrMNTIW</a></p>
<p><b>Compressor summary</b>: The paper proposes a new model, IPDiff, for generating 3D ligand molecules that bind to specific proteins by incorporating protein-ligand interactions into both diffusion and sampling processes, leading to more realistic structures and better binding affinities.</p><hr><h3>Conditional Instrumental Variable Regression with Representation Learning for Causal Inference</h3>
<p><a href='https://openreview.net/forum?id=qDhq1icpO8'>https://openreview.net/forum?id=qDhq1icpO8</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method, CBRL.CIV, for estimating causal effects from observational data without the linearity assumption and using a conditional instrument to relax the strict condition of unconfounded instruments.</p><hr><h3>Explaining Time Series via Contrastive and Locally Sparse Perturbations</h3>
<p><a href='https://openreview.net/forum?id=qDdSRaOiyb'>https://openreview.net/forum?id=qDdSRaOiyb</a></p>
<p><b>Compressor summary</b>: ContraLSP is a model that uses counterfactual samples and sparse gates to create informative but uninformative perturbations for multivariate time series, improving explanation quality.</p><hr><h3>VONET: ADVANCING UNSUPERVISED VIDEO OBJECT LEARNING</h3>
<p><a href='https://openreview.net/forum?id=qCyhvr0GG8'>https://openreview.net/forum?id=qCyhvr0GG8</a></p>
<p><b>Compressor summary</b>: VONet is a new method that uses attention and sequential VAE to learn video objects without supervision, achieving state-of-the-art results on various datasets.</p><hr><h3>LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=qCUWVT0Ayy'>https://openreview.net/forum?id=qCUWVT0Ayy</a></p>
<p><b>Compressor summary</b>: This paper introduces LayoutNUWA, a model that generates graphic layouts by treating layout generation as a code generation task using large language models, achieving significant performance improvements.</p><hr><h3>DeepZero: Scaling Up Zeroth-Order Optimization for Deep Model Training</h3>
<p><a href='https://openreview.net/forum?id=qBWhjsNPEY'>https://openreview.net/forum?id=qBWhjsNPEY</a></p>
<p><b>Compressor summary</b>: DeepZero is a novel framework that enables zeroth-order optimization for training deep neural networks, achieving near-optimal performance and improving certified adversarial defense and partial differential equation error correction.</p><hr><h3>Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=qBL04XXex6'>https://openreview.net/forum?id=qBL04XXex6</a></p>
<p><b>Compressor summary</b>: Boosting of Thoughts (BoT) is an automated framework for improving problem-solving with LLMs by iteratively exploring, self-evaluating, and revising reasoning steps using error analysis.</p><hr><h3>DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning</h3>
<p><a href='https://openreview.net/forum?id=qAoxvePSlq'>https://openreview.net/forum?id=qAoxvePSlq</a></p>
<p><b>Compressor summary</b>: The study introduces DQ-LoRe, a framework that uses dual queries and low-rank approximation re-ranking to automatically select exemplars for in-context learning in natural language processing tasks.</p><hr><h3>Efficient Integrators for Diffusion Generative Models</h3>
<p><a href='https://openreview.net/forum?id=qA4foxO5Gf'>https://openreview.net/forum?id=qA4foxO5Gf</a></p>
<p><b>Compressor summary</b>: The text introduces two frameworks to accelerate sample generation in diffusion models and presents a hybrid method that outperforms baselines on CIFAR-10.</p><hr><h3>Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design</h3>
<p><a href='https://openreview.net/forum?id=q9jQPA6zPK'>https://openreview.net/forum?id=q9jQPA6zPK</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel coarse-to-fine method for designing multi-cellular robots using Hyperbolic Embeddings for Robot Design (HERD) framework, which improves efficiency and generalization compared to previous approaches.</p><hr><h3>Large-Vocabulary 3D Diffusion Model with Transformer</h3>
<p><a href='https://openreview.net/forum?id=q57JLSE2j5'>https://openreview.net/forum?id=q57JLSE2j5</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel diffusion-based generative model that can synthesize diverse and high-quality 3D objects from over 200 real-world categories using a triplane representation and a 3D-aware transformer.</p><hr><h3>An Investigation of Representation and Allocation Harms in Contrastive Learning</h3>
<p><a href='https://openreview.net/forum?id=q4SiDyYQbo'>https://openreview.net/forum?id=q4SiDyYQbo</a></p>
<p><b>Compressor summary</b>: The paper shows that contrastive learning, a type of self-supervised learning, can cause minority groups' representations to merge with majority groups', leading to reduced performance and bias.</p><hr><h3>Solving High Frequency and Multi-Scale PDEs with Gaussian Processes</h3>
<p><a href='https://openreview.net/forum?id=q4AEBLHuA6'>https://openreview.net/forum?id=q4AEBLHuA6</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for solving high-frequency and multi-scale PDEs using Gaussian processes and spectral mixture kernels, which can improve efficiency and scalability.</p><hr><h3>Adversarial Attacks on Fairness of Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=q3KNrmW6Ql'>https://openreview.net/forum?id=q3KNrmW6Ql</a></p>
<p><b>Compressor summary</b>: The paper presents G-FairAttack, a framework that can adversarially compromise the fairness of various fairness-aware GNNs with low utility loss and fast computation.</p><hr><h3>Universal Guidance for Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=pzpWBbnwiJ'>https://openreview.net/forum?id=pzpWBbnwiJ</a></p>
<p><b>Compressor summary</b>: The proposed universal guidance algorithm allows diffusion models to accept different types of guidance without retraining, enabling various image generation applications.</p><hr><h3>Conformal Language Modeling</h3>
<p><a href='https://openreview.net/forum?id=pzUhfQ74c5'>https://openreview.net/forum?id=pzUhfQ74c5</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel conformal prediction approach for language models with performance guarantees, calibrating stopping and rejection rules to reduce noise and ensure coverage of acceptable responses.</p><hr><h3>How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=pzElnMrgSD'>https://openreview.net/forum?id=pzElnMrgSD</a></p>
<p><b>Compressor summary</b>: The authors propose a novel method to preserve temporal correlations in video editing and generation using integral noise representation and a tailored transport method.</p><hr><h3>Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification</h3>
<p><a href='https://openreview.net/forum?id=pz2E1Q9Wni'>https://openreview.net/forum?id=pz2E1Q9Wni</a></p>
<p><b>Compressor summary</b>: The paper investigates how different behavioural models affect inverse reinforcement learning (IRL) and shows that even small misspecifications can cause significant errors in inferring an agent's preferences.</p><hr><h3>Effective and Efficient Federated Tree Learning on Hybrid Data</h3>
<p><a href='https://openreview.net/forum?id=py4ZV2qYQI'>https://openreview.net/forum?id=py4ZV2qYQI</a></p>
<p><b>Compressor summary</b>: HybridTree is a novel federated learning approach for hybrid data settings that incorporates party knowledge into tree lower layers, achieving comparable accuracy and significant speedup.</p><hr><h3>ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference</h3>
<p><a href='https://openreview.net/forum?id=pxI5IPeWgW'>https://openreview.net/forum?id=pxI5IPeWgW</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel approach for inferring unbiased treatment effects using closed-form ordinary differential equations, which offer advantages like interpretability and irregular sampling.</p><hr><h3>InterpGNN: Understand and Improve Generalization Ability of Transdutive GNNs through the Lens of Interplay between Train and Test Nodes</h3>
<p><a href='https://openreview.net/forum?id=pwW807WJ9G'>https://openreview.net/forum?id=pwW807WJ9G</a></p>
<p><b>Compressor summary</b>: This paper proposes a PAC-Bayesian bound for GNNs that incorporates the $L$-hop interplay between training and testing nodes, and introduces a Graph Global Workspace module to enhance the interplay using key-value attention.</p><hr><h3>CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis</h3>
<p><a href='https://openreview.net/forum?id=pw2ssoOTpo'>https://openreview.net/forum?id=pw2ssoOTpo</a></p>
<p><b>Compressor summary</b>: CIFAR-10-Warehouse is a testbed for evaluating model generalization in various out-of-distribution environments, consisting of 180 datasets collected by image search engines and diffusion models with diverse characteristics.</p><hr><h3>$\alpha$TC-VAE: On the relationship between Disentanglement and Diversity</h3>
<p><a href='https://openreview.net/forum?id=ptXo0epLQo'>https://openreview.net/forum?id=ptXo0epLQo</a></p>
<p><b>Compressor summary</b>: The authors propose alpha-TCVAE, a variational autoencoder that maximizes disentanglement and informativeness of latent variables using a novel total correlation lower bound, and show its benefits in representation learning and downstream tasks.</p><hr><h3>Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models</h3>
<p><a href='https://openreview.net/forum?id=ptCIlV24YZ'>https://openreview.net/forum?id=ptCIlV24YZ</a></p>
<p><b>Compressor summary</b>: The paper presents a new image clustering method based on pre-trained models that improves accuracy, estimates cluster numbers, and labels clusters with text.</p><hr><h3>#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=pszewhybU9'>https://openreview.net/forum?id=pszewhybU9</a></p>
<p><b>Compressor summary</b>: InsTag is a method that analyzes and improves instruction diversity and complexity for supervised fine-tuning (SFT) of large language models (LLMs), leading to better instruction-following abilities.</p><hr><h3>AmortizedPeriod: Attention-based Amortized Inference for Periodicity Identification</h3>
<p><a href='https://openreview.net/forum?id=psEswR8Jz4'>https://openreview.net/forum?id=psEswR8Jz4</a></p>
<p><b>Compressor summary</b>: AmortizedPeriod is a novel method that uses Bayesian statistics and deep learning to identify periodic patterns in time series data, improving robustness and efficiency over existing methods.</p><hr><h3>Tractable MCMC for Private Learning with Pure and Gaussian Differential Privacy</h3>
<p><a href='https://openreview.net/forum?id=pmweVpJ229'>https://openreview.net/forum?id=pmweVpJ229</a></p>
<p><b>Compressor summary</b>: ASAP is an algorithm that samples from the posterior distribution while preserving differential privacy by adding noise proportional to the distance of each sample from a reference distribution.</p><hr><h3>Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models</h3>
<p><a href='https://openreview.net/forum?id=plmBsXHxgR'>https://openreview.net/forum?id=plmBsXHxgR</a></p>
<p><b>Compressor summary</b>: The authors propose new cross-modality jailbreak attacks on vision language models using adversarial images and textual prompts, which exploit the alignment vulnerability of multi-modal models.</p><hr><h3>Kernel Metric Learning for In-Sample Off-Policy Evaluation of Deterministic RL Policies</h3>
<p><a href='https://openreview.net/forum?id=plebgsdiiV'>https://openreview.net/forum?id=plebgsdiiV</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to evaluate policies in reinforcement learning with continuous actions by relaxing deterministic target policies using a kernel and optimizing its metrics to reduce bias and variance in off-policy evaluation.</p><hr><h3>Probabilistic Adaptation of Black-Box Text-to-Video Models</h3>
<p><a href='https://openreview.net/forum?id=pjtIEgscE3'>https://openreview.net/forum?id=pjtIEgscE3</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to adapt a large black-box text-to-video model to different domains without access to its weights, using the score function as a probabilistic prior for a small task-specific video model.</p><hr><h3>Benchmarking and Improving Generator-Validator Consistency of Language Models</h3>
<p><a href='https://openreview.net/forum?id=phBS6YpTzC'>https://openreview.net/forum?id=phBS6YpTzC</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework to measure and improve consistency between generating and validating answers in language models, leading to better performance on various tasks.</p><hr><h3>Think before you speak: Training Language Models With Pause Tokens</h3>
<p><a href='https://openreview.net/forum?id=ph04CRkPdC'>https://openreview.net/forum?id=ph04CRkPdC</a></p>
<p><b>Compressor summary</b>: The text proposes adding extra hidden vectors to language models, which can improve their performance on various tasks when trained and evaluated with delays.</p><hr><h3>Graph Transformers on EHRs: Better Representation Improves Downstream Performance</h3>
<p><a href='https://openreview.net/forum?id=pe0Vdv7rsL'>https://openreview.net/forum?id=pe0Vdv7rsL</a></p>
<p><b>Compressor summary</b>: GT-BEHRT combines graph transformer embeddings and BERT model to improve patient representation and perform better in medical prediction tasks using EHRs.</p><hr><h3>EXPLORING RAIN-/DETAIL-AWARE REPRESENTATION FOR INSTANCE-SPECIFIC IMAGE DE-RAINING</h3>
<p><a href='https://openreview.net/forum?id=pdJXYfJjz9'>https://openreview.net/forum?id=pdJXYfJjz9</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method called CoIC for training image de-raining models that adapts to different types of rain and background, leading to better performance and generalization.</p><hr><h3>GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors</h3>
<p><a href='https://openreview.net/forum?id=pTN8dV2pL8'>https://openreview.net/forum?id=pTN8dV2pL8</a></p>
<p><b>Compressor summary</b>: The paper introduces a new way of representing normals in SDF fields using Gaussian distributions, supervised by polarization priors, to improve NeRF learning of reflective scenes and achieve better 3D reconstruction.</p><hr><h3>SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning</h3>
<p><a href='https://openreview.net/forum?id=pTHfApDakA'>https://openreview.net/forum?id=pTHfApDakA</a></p>
<p><b>Compressor summary</b>: SelfCheck is a method for improving question-answering by checking and correcting LLMs' reasoning steps on three datasets.</p><hr><h3>RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems</h3>
<p><a href='https://openreview.net/forum?id=pPjZIOuQuF'>https://openreview.net/forum?id=pPjZIOuQuF</a></p>
<p><b>Compressor summary</b>: RepoBench is a new benchmark for evaluating code auto-completion systems on real-world, multi-file programming scenarios.</p><hr><h3>PBADet: A One-Stage Anchor-Free Approach for Part-Body Association</h3>
<p><a href='https://openreview.net/forum?id=pPh9p8anUi'>https://openreview.net/forum?id=pPh9p8anUi</a></p>
<p><b>Compressor summary</b>: PBADet is a novel one-stage, anchor-free method for detecting human body parts and their associations with individuals, which is faster and more accurate than existing approaches.</p><hr><h3>Imitation Learning from Observation with Automatic Discount Scheduling</h3>
<p><a href='https://openreview.net/forum?id=pPJTQYOpNI'>https://openreview.net/forum?id=pPJTQYOpNI</a></p>
<p><b>Compressor summary</b>: Our framework improves ILfO by adaptively adjusting the reward priority between earlier and later steps using Automatic Discount Scheduling, enabling better learning of initial behaviors before advancing to later ones.</p><hr><h3>Robust agents learn causal world models</h3>
<p><a href='https://openreview.net/forum?id=pOoKI3ouv1'>https://openreview.net/forum?id=pOoKI3ouv1</a></p>
<p><b>Compressor summary</b>: The text argues that agents need to learn causal models to generalize under different distributions, and this has implications for transfer learning and causal inference.</p><hr><h3>Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula</h3>
<p><a href='https://openreview.net/forum?id=pFOoOdaiue'>https://openreview.net/forum?id=pFOoOdaiue</a></p>
<p><b>Compressor summary</b>: The paper proposes Quantal Adversarial RL (QARL), a novel approach for adversarial RL that uses entropy regularization and gradually increases the rationality of the adversary to ease the complexity of saddle point optimization problems and improve performance and robustness.</p><hr><h3>RAIN: Your Language Models Can Align Themselves without Finetuning</h3>
<p><a href='https://openreview.net/forum?id=pETSfWMUzy'>https://openreview.net/forum?id=pETSfWMUzy</a></p>
<p><b>Compressor summary</b>: RAIN is a novel inference method that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety, improving harmlessness and truthfulness.</p><hr><h3>Scalable Modular Network: A Framework for Adaptive Learning via Agreement Routing</h3>
<p><a href='https://openreview.net/forum?id=pEKJl5sflp'>https://openreview.net/forum?id=pEKJl5sflp</a></p>
<p><b>Compressor summary</b>: The paper introduces Scalable Modular Network (SMN), a modular network framework that adapts to new tasks with the help of an agreement router, which selects and composes specialist modules using iterative message passing.</p><hr><h3>Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable</h3>
<p><a href='https://openreview.net/forum?id=pEGSdJu52I'>https://openreview.net/forum?id=pEGSdJu52I</a></p>
<p><b>Compressor summary</b>: The paper investigates test-set variance in neural network training, finding that it is less problematic than expected and providing insights into its causes and mitigations.</p><hr><h3>On Adversarial Training without Perturbing all Examples</h3>
<p><a href='https://openreview.net/forum?id=pE6gWrASQm'>https://openreview.net/forum?id=pE6gWrASQm</a></p>
<p><b>Compressor summary</b>: The paper explores constructing adversarial examples only on a subset of training data and shows that it improves robustness against adversarial attacks across datasets and tasks, while reducing the computational cost.</p><hr><h3>Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in RL</h3>
<p><a href='https://openreview.net/forum?id=pDCublKPmG'>https://openreview.net/forum?id=pDCublKPmG</a></p>
<p><b>Compressor summary</b>: This paper proposes a generalized attack framework for multi-agent reinforcement learning and a provably efficient defense using timescale separation and adversarial training.</p><hr><h3>Two-stage LLM Fine-tuning with Less Specialization and More Generalization</h3>
<p><a href='https://openreview.net/forum?id=pCEgna6Qco'>https://openreview.net/forum?id=pCEgna6Qco</a></p>
<p><b>Compressor summary</b>: ProMoT is a two-stage fine-tuning method for large language models that reduces format specialization and improves their in-context learning performances across various tasks.</p><hr><h3>Large-scale training of foundation models for wearable biosignals</h3>
<p><a href='https://openreview.net/forum?id=pC3WJHf51j'>https://openreview.net/forum?id=pC3WJHf51j</a></p>
<p><b>Compressor summary</b>: The authors use self-supervised learning with unlabeled data from a large study to train foundation models for photoplethysmography and electrocardiogram biosignals, which can enhance future wearable devices and help users improve their health.</p><hr><h3>Grounded Object-Centric Learning</h3>
<p><a href='https://openreview.net/forum?id=pBxeZ6pVUD'>https://openreview.net/forum?id=pBxeZ6pVUD</a></p>
<p><b>Compressor summary</b>: The paper introduces Conditional Slot Attention (CoSA), a method that learns stable and invariant object-centric representations using Grounded Slot Dictionary (GSD) inspired by vector quantization, and shows its benefits in various downstream tasks.</p><hr><h3>Near-Optimal Quantum Algorithm for Minimizing the Maximal Loss</h3>
<p><a href='https://openreview.net/forum?id=pB1FeRSQxh'>https://openreview.net/forum?id=pB1FeRSQxh</a></p>
<p><b>Compressor summary</b>: The paper investigates quantum algorithms and lower bounds for minimizing the maximum of N convex functions, improving the complexity bound and proving optimality of the dependence on N.</p><hr><h3>Soft Contrastive Learning for Time Series</h3>
<p><a href='https://openreview.net/forum?id=pAsQSWlDUf'>https://openreview.net/forum?id=pAsQSWlDUf</a></p>
<p><b>Compressor summary</b>: SoftCLT is a soft contrastive learning method for time series that preserves correlations and improves downstream tasks.</p><hr><h3>Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning</h3>
<p><a href='https://openreview.net/forum?id=pAoqRlTBtY'>https://openreview.net/forum?id=pAoqRlTBtY</a></p>
<p><b>Compressor summary</b>: The paper presents a novel framework that combines LLMs and DSCMs for causal discovery and shows its superior performance on benchmarks and in modelling Alzheimer's Disease.</p><hr><h3>Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=pAVJKp3Dvn'>https://openreview.net/forum?id=pAVJKp3Dvn</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework to learn efficient structured weight matrices for deep neural networks using gradient descent, improving their performance and reducing complexity compared to previous methods.</p><hr><h3>Improved Regret Bounds for Non-Convex Online-Within-Online Meta Learning</h3>
<p><a href='https://openreview.net/forum?id=pA8Q5WiEMg'>https://openreview.net/forum?id=pA8Q5WiEMg</a></p>
<p><b>Compressor summary</b>: OWO meta learning improves online algorithm initialization and step size in non-convex settings with better regret bounds and investigates generalization bounds using regret analysis.</p><hr><h3>Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling</h3>
<p><a href='https://openreview.net/forum?id=p8ujRTjEf3'>https://openreview.net/forum?id=p8ujRTjEf3</a></p>
<p><b>Compressor summary</b>: The paper proposes a new Thompson sampling algorithm for Gaussian bandits with unknown heterogeneous reward variances, which incorporates prior knowledge to achieve lower regret and adapts better to varying uncertainty levels.</p><hr><h3>Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis</h3>
<p><a href='https://openreview.net/forum?id=p4eG8rCa0b'>https://openreview.net/forum?id=p4eG8rCa0b</a></p>
<p><b>Compressor summary</b>: The text proposes a new conditional diffusion model, Compose and Conquer (CnC), that can control 3D object placement and style in generated images using disentangled representations and depth estimation.</p><hr><h3>Traveling Waves Encode The Recent Past and Enhance Sequence Learning</h3>
<p><a href='https://openreview.net/forum?id=p4S5Z6Sah4'>https://openreview.net/forum?id=p4S5Z6Sah4</a></p>
<p><b>Compressor summary</b>: The Wave-RNN (wRNN) is a recurrent neural network model that efficiently encodes recent past through traveling waves of neural activity, and performs better than wave-free RNNs on memory tasks and sequential image classification.</p><hr><h3>Lie Group Decompositions for Equivariant Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=p34fRKp8qA'>https://openreview.net/forum?id=p34fRKp8qA</a></p>
<p><b>Compressor summary</b>: The text discusses a framework for training neural networks that are invariant and equivarent to geometrical transformations using Lie groups and their homogeneous spaces, showing improved performance on an affine-invariant benchmark task.</p><hr><h3>The Truth Is In There: Improving Reasoning with Layer-Selective Rank Reduction</h3>
<p><a href='https://openreview.net/forum?id=ozX92bu8VA'>https://openreview.net/forum?id=ozX92bu8VA</a></p>
<p><b>Compressor summary</b>: LASER is a simple technique that improves the performance of LLMs by removing some weight matrix components, leading to significant gains in predictive accuracy on various tasks.</p><hr><h3>Backdoor Contrastive Learning via Bi-level Trigger Optimization</h3>
<p><a href='https://openreview.net/forum?id=oxjeePpgSP'>https://openreview.net/forum?id=oxjeePpgSP</a></p>
<p><b>Compressor summary</b>: The paper proposes a new backdoor attack for contrastive learning that uses a bi-level optimization approach to create a more effective trigger and achieve higher success rates.</p><hr><h3>Yet Another ICU Benchmark: A Flexible Multi-Center Framework for Clinical ML</h3>
<p><a href='https://openreview.net/forum?id=ox2ATRM90I'>https://openreview.net/forum?id=ox2ATRM90I</a></p>
<p><b>Compressor summary</b>: YAIB is a modular framework that allows researchers to define reproducible and comparable clinical machine learning experiments in intensive care units using various datasets, preprocessing pipelines, and prediction tasks.</p><hr><h3>Quality-Diversity through AI Feedback</h3>
<p><a href='https://openreview.net/forum?id=owokKCrGYr'>https://openreview.net/forum?id=owokKCrGYr</a></p>
<p><b>Compressor summary</b>: Quality-Diversity through AI Feedback (QDAIF) is a method that uses language models to guide an evolutionary algorithm in generating diverse high-quality outputs in creative writing and other domains.</p><hr><h3>Advancing the Lower Bounds: an Accelerated, Stochastic, Second-order Method with Optimal Adaptation to Inexactness</h3>
<p><a href='https://openreview.net/forum?id=otU31x3fus'>https://openreview.net/forum?id=otU31x3fus</a></p>
<p><b>Compressor summary</b>: The paper introduces a new robust optimization method that generalizes existing ones and achieves optimal convergence under inexact gradients and Hessians, using tensor generalization for stochastic higher-order derivatives.</p><hr><h3>Prototypical Information Bottlenecking and Disentangling for Multimodal Cancer Survival Prediction</h3>
<p><a href='https://openreview.net/forum?id=otHZ8JAIgh'>https://openreview.net/forum?id=otHZ8JAIgh</a></p>
<p><b>Compressor summary</b>: The PIBD framework tackles intra-modal and inter-modal redundancies in multimodal learning for cancer survival prediction by using prototypical information bottlenecking and disentangling.</p><hr><h3>ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=osoWxY8q2E'>https://openreview.net/forum?id=osoWxY8q2E</a></p>
<p><b>Compressor summary</b>: ReLU activation in large language models can save significant computation and weight transfer during memory-bound inference without compromising convergence and performance.</p><hr><h3>A Lie Group Approach to Riemannian Normalization for SPD Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=okYdj8Ysru'>https://openreview.net/forum?id=okYdj8Ysru</a></p>
<p><b>Compressor summary</b>: The paper proposes a unified framework for Riemannian Batch Normalization on Lie groups and applies it to SPD manifolds for computer vision tasks such as radar recognition, human action recognition, and EEG classification.</p><hr><h3>Copula Conformal prediction for multi-step time series prediction</h3>
<p><a href='https://openreview.net/forum?id=ojIJZDNIBj'>https://openreview.net/forum?id=ojIJZDNIBj</a></p>
<p><b>Compressor summary</b>: The paper introduces CopulaCPTS, a conformal prediction algorithm for multivariate, multi-step time series forecasting with finite sample validity guarantee and better calibration and efficiency than existing methods.</p><hr><h3>CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech</h3>
<p><a href='https://openreview.net/forum?id=ofzeypWosV'>https://openreview.net/forum?id=ofzeypWosV</a></p>
<p><b>Compressor summary</b>: CLaM-TTS is a new method that uses probabilistic vector quantization to compress audio tokens and generate multiple streams for zero-shot TTS synthesis with improved naturalness, intelligibility, and speed.</p><hr><h3>AffineQuant: Affine Transformation Quantization for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=of2rhALq8l'>https://openreview.net/forum?id=of2rhALq8l</a></p>
<p><b>Compressor summary</b>: AffineQuant is a technique that optimizes post-training quantization of large language models by using equivalent affine transformations, improving performance and allowing deployment on edge devices with low-bit configurations.</p><hr><h3>Reconciling Spatial and Temporal Abstractions for Goal Representation</h3>
<p><a href='https://openreview.net/forum?id=odY3PkI5VB'>https://openreview.net/forum?id=odY3PkI5VB</a></p>
<p><b>Compressor summary</b>: The paper presents a new HRL algorithm that combines spatial and temporal goal abstraction to improve performance on complex tasks.</p><hr><h3>LDReg: Local Dimensionality Regularized Self-Supervised Learning</h3>
<p><a href='https://openreview.net/forum?id=oZyAqjAjJW'>https://openreview.net/forum?id=oZyAqjAjJW</a></p>
<p><b>Compressor summary</b>: The paper proposes a method called *LDReg* to improve self-supervised learning by preventing dimensional collapse, which occurs when representations fail to capture the full data distribution locally.</p><hr><h3>Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation</h3>
<p><a href='https://openreview.net/forum?id=oZtt0pRnOl'>https://openreview.net/forum?id=oZtt0pRnOl</a></p>
<p><b>Compressor summary</b>: The paper proposes an algorithm for in-context learning with large language models on private datasets, using synthetic few-shot demonstrations and formal differential privacy guarantees.</p><hr><h3>Analyzing and Mitigating Object Hallucination in Large Vision-Language Models</h3>
<p><a href='https://openreview.net/forum?id=oZDJKTlOUe'>https://openreview.net/forum?id=oZDJKTlOUe</a></p>
<p><b>Compressor summary</b>: LURE is an algorithm that corrects object hallucination in vision-language models by reconstructing more accurate descriptions based on co-occurrence, uncertainty, and object position.</p><hr><h3>Magnushammer: A Transformer-Based Approach to Premise Selection</h3>
<p><a href='https://openreview.net/forum?id=oYjPk8mqAV'>https://openreview.net/forum?id=oYjPk8mqAV</a></p>
<p><b>Compressor summary</b>: The paper proposes Magnushammer, a contrastive training-based method for premise selection in automated theorem proving, which outperforms existing tools and enables state-of-the-art results with fewer parameters.</p><hr><h3>Score Models for Offline Goal-Conditioned Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=oXjnwQLcTA'>https://openreview.net/forum?id=oXjnwQLcTA</a></p>
<p><b>Compressor summary</b>: SMORe is a novel discriminator-free method for offline goal-conditioned reinforcement learning that leverages mixture-distribution matching and scores for achieving diverse and reusable skills from suboptimal offline datasets without hand-engineering reward functions.</p><hr><h3>Retrieval is Accurate Generation</h3>
<p><a href='https://openreview.net/forum?id=oXYZJXDdo7'>https://openreview.net/forum?id=oXYZJXDdo7</a></p>
<p><b>Compressor summary</b>: The text introduces a novel method of generating text using context-aware phrases from supporting documents, overcoming challenges in training oracles with linguistic heuristics and self-reinforcement, and demonstrating improved performance and generation quality on various tasks.</p><hr><h3>ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis</h3>
<p><a href='https://openreview.net/forum?id=oTRwljRgiv'>https://openreview.net/forum?id=oTRwljRgiv</a></p>
<p><b>Compressor summary</b>: The paper proposes ExeDec, a new synthesis strategy that improves compositional generalization for neural program synthesis models by predicting execution subgoals step-by-step.</p><hr><h3>Online Information Acquisition: Hiring Multiple Agents</h3>
<p><a href='https://openreview.net/forum?id=oQKKlzxV1o'>https://openreview.net/forum?id=oQKKlzxV1o</a></p>
<p><b>Compressor summary</b>: The paper studies how a principal can hire multiple agents to gather and report information efficiently, considering coordination, externalities, and incentives.</p><hr><h3>Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators</h3>
<p><a href='https://openreview.net/forum?id=oOwDQl8haC'>https://openreview.net/forum?id=oOwDQl8haC</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to train and fine-tune DNNs using cheaper 12-bit accumulators without significant performance loss and shows that gradient approximations can improve accuracy with lower precision.</p><hr><h3>Treatment Effects Estimation By Uniform Transformer</h3>
<p><a href='https://openreview.net/forum?id=oOGqJ6Z1sA'>https://openreview.net/forum?id=oOGqJ6Z1sA</a></p>
<p><b>Compressor summary</b>: The paper proposes a new weighting method for covariate balancing and treatment effects estimation in observational studies, which works well even when the underlying model assumptions are not strong.</p><hr><h3>Graph Neural Networks for Learning Equivariant Representations of Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=oO6FsMyDBt'>https://openreview.net/forum?id=oO6FsMyDBt</a></p>
<p><b>Compressor summary</b>: This paper proposes using graph neural networks and transformers to encode diverse neural network architectures and achieve better performance on various tasks.</p><hr><h3>A Unified Framework for Bayesian Optimization under Contextual Uncertainty</h3>
<p><a href='https://openreview.net/forum?id=oMNkj4ER7V'>https://openreview.net/forum?id=oMNkj4ER7V</a></p>
<p><b>Compressor summary</b>: The paper proposes a general Thompson sampling algorithm for optimizing various objectives in Bayesian optimization under contextual uncertainty, and analyzes its performance.</p><hr><h3>DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genomes</h3>
<p><a href='https://openreview.net/forum?id=oMLQB4EZE1'>https://openreview.net/forum?id=oMLQB4EZE1</a></p>
<p><b>Compressor summary</b>: The authors propose DNABERT-2, a refined genome foundation model that uses Byte Pair Encoding (BPE) tokenization to overcome limitations of k-mer tokenization and achieve better efficiency and performance on large genomes.</p><hr><h3>Multimodal Molecular Pretraining via Modality Blending</h3>
<p><a href='https://openreview.net/forum?id=oM7Jbxdk6Z'>https://openreview.net/forum?id=oM7Jbxdk6Z</a></p>
<p><b>Compressor summary</b>: MoleBLEND is a self-supervised learning approach that aligns 2D and 3D molecular structures at the atomic level by blending atom relations from both modalities and achieves state-of-the-art performance in molecular modeling tasks.</p><hr><h3>RLIF: Interactive Imitation Learning as Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=oLLZhbBSOU'>https://openreview.net/forum?id=oLLZhbBSOU</a></p>
<p><b>Compressor summary</b>: This paper proposes a reinforcement learning method for skill acquisition that uses user interventions as rewards, improving upon interactive imitation learning methods like DAgger, especially when the expert is suboptimal.</p><hr><h3>WebArena: A Realistic Web Environment for Building Autonomous Agents</h3>
<p><a href='https://openreview.net/forum?id=oKn9c6ytLx'>https://openreview.net/forum?id=oKn9c6ytLx</a></p>
<p><b>Compressor summary</b>: The paper presents a realistic web-based environment for evaluating language-guided agents' performance on complex tasks, showing that current state-of-the-art models struggle to match human performance.</p><hr><h3>Entropy-MCMC: Sampling from Flat Basins with Ease</h3>
<p><a href='https://openreview.net/forum?id=oGNdBvymod'>https://openreview.net/forum?id=oGNdBvymod</a></p>
<p><b>Compressor summary</b>: The paper proposes a new sampling method for Bayesian deep learning that biases towards flat regions in the posterior distribution to avoid overfitting and improve generalization performance.</p><hr><h3>LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units</h3>
<p><a href='https://openreview.net/forum?id=oEF7qExD9F'>https://openreview.net/forum?id=oEF7qExD9F</a></p>
<p><b>Compressor summary</b>: The paper introduces LMUFormer, a low-complexity recurrent model with memory units, embedding, and mixer that performs as well as transformers on speech commands while being more efficient and suitable for streaming applications at the edge.</p><hr><h3>Transformer-VQ: Linear-Time Transformers via Vector Quantization</h3>
<p><a href='https://openreview.net/forum?id=oDdzXQzP2F'>https://openreview.net/forum?id=oDdzXQzP2F</a></p>
<p><b>Compressor summary</b>: Transformer-VQ is a fast decoder-only transformer with vector-quantized keys and caching that achieves high quality in various tasks.</p><hr><h3>Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization</h3>
<p><a href='https://openreview.net/forum?id=oAMArMMQxb'>https://openreview.net/forum?id=oAMArMMQxb</a></p>
<p><b>Compressor summary</b>: The authors show how to use vanilla score matching with Langevin diffusion and early stopping to generate multimodal distributions.</p><hr><h3>Adversarial AutoMixup</h3>
<p><a href='https://openreview.net/forum?id=o8tjamaJ80'>https://openreview.net/forum?id=o8tjamaJ80</a></p>
<p><b>Compressor summary</b>: AdAutomixup is an adversarial automatic mixup augmentation method for improving image classification by generating diverse and challenging mixed samples and training a robust classifier.</p><hr><h3>Information Retention via Learning Supplemental Features</h3>
<p><a href='https://openreview.net/forum?id=o83eu4H9Mb'>https://openreview.net/forum?id=o83eu4H9Mb</a></p>
<p><b>Compressor summary</b>: The information bottleneck principle helps learn good representations by balancing conciseness and predictive ability, but may not work well in low-resource or out-of-domain scenarios; a new method is proposed to jointly learn mainline and supplemental features for better prediction.</p><hr><h3>MaGIC: Multi-modality Guided Image Completion</h3>
<p><a href='https://openreview.net/forum?id=o7x0XVlCpX'>https://openreview.net/forum?id=o7x0XVlCpX</a></p>
<p><b>Compressor summary</b>: MaGIC is a novel method for image completion that supports single and multi-modality guidance, such as text, edge, sketch, segmentation, depth, and pose, without requiring re-training of different modalities.</p><hr><h3>Sample-Efficient Multi-Agent RL: An Optimization Perspective</h3>
<p><a href='https://openreview.net/forum?id=o7qhUMylLU'>https://openreview.net/forum?id=o7qhUMylLU</a></p>
<p><b>Compressor summary</b>: The paper proposes a new complexity measure and algorithm for sample-efficient multi-agent reinforcement learning in general-sum Markov Games, addressing different equilibrium concepts and both model-based and model-free settings.</p><hr><h3>$\pi$2vec: Policy Representation with Successor Features</h3>
<p><a href='https://openreview.net/forum?id=o5Bqa4o5Mi'>https://openreview.net/forum?id=o5Bqa4o5Mi</a></p>
<p><b>Compressor summary</b>: $\pi$2vec is a method that uses feature vectors and successor features to represent and evaluate black box policies efficiently in offline settings.</p><hr><h3>Learning Robust Generalizable Radiance Field with Visibility and Feature Augmented Point Representation</h3>
<p><a href='https://openreview.net/forum?id=o4CLLlIaaH'>https://openreview.net/forum?id=o4CLLlIaaH</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel method called Generalizable neural Point Field (GPF) that improves generalization, consistency, and quality in neural radiance field modeling by using point-based rendering instead of image-based rendering.</p><hr><h3>Manifold Preserving Guided Diffusion</h3>
<p><a href='https://openreview.net/forum?id=o3BxOLoxm1'>https://openreview.net/forum?id=o3BxOLoxm1</a></p>
<p><b>Compressor summary</b>: MPGD is a training-free method that uses pretrained models and neural networks for conditional image generation, improving efficiency and quality.</p><hr><h3>Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=o2IEmeLL9r'>https://openreview.net/forum?id=o2IEmeLL9r</a></p>
<p><b>Compressor summary</b>: PTGM is a method that pre-trains goal-based models for reinforcement learning, improving sample efficiency and performance in complex tasks by providing temporal abstractions and behavior regularization.</p><hr><h3>Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem</h3>
<p><a href='https://openreview.net/forum?id=nxnbPPVvOG'>https://openreview.net/forum?id=nxnbPPVvOG</a></p>
<p><b>Compressor summary</b>: The paper proposes a generalization of Gauss-Markov theorem for linear estimation with non-zero bias, derives optimal estimators for different norms, analyzes generalization error, and compares with Ridge regression using simulations.</p><hr><h3>Leveraging Uncertainty Estimates To Improve Classifier Performance</h3>
<p><a href='https://openreview.net/forum?id=nsNyDvNQTc'>https://openreview.net/forum?id=nsNyDvNQTc</a></p>
<p><b>Compressor summary</b>: The paper analyzes how uncertainty and model score affect estimation bias in binary classification and proposes algorithms that use both to improve recall and precision.</p><hr><h3>Searching for High-Value Molecules Using Reinforcement Learning and Transformers</h3>
<p><a href='https://openreview.net/forum?id=nqlymMx42E'>https://openreview.net/forum?id=nqlymMx42E</a></p>
<p><b>Compressor summary</b>: The paper introduces ChemRLformer, a new RL-based molecular design algorithm that uses text representations and explores how different design choices affect its performance on 25 molecule design tasks.</p><hr><h3>PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction</h3>
<p><a href='https://openreview.net/forum?id=noe76eRcPC'>https://openreview.net/forum?id=noe76eRcPC</a></p>
<p><b>Compressor summary</b>: PF-LRM is a fast and accurate method for reconstructing 3D objects from unposed images with self-attention and PnP, and it works well on different datasets and scenarios.</p><hr><h3>Interpretable Meta-Learning of Physical Systems</h3>
<p><a href='https://openreview.net/forum?id=nnicaG5xiH'>https://openreview.net/forum?id=nnicaG5xiH</a></p>
<p><b>Compressor summary</b>: CAMEL is a meta-learning architecture that learns efficiently and interprets physical parameters in various settings.</p><hr><h3>Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph</h3>
<p><a href='https://openreview.net/forum?id=nnVO1PvbTv'>https://openreview.net/forum?id=nnVO1PvbTv</a></p>
<p><b>Compressor summary</b>: The paper proposes an "LLM-KG" approach that improves large language models' reasoning by integrating external knowledge graphs and introduces a new method called Think-on-Graph (ToG) for interactive reasoning with better deep reasoning, knowledge traceability, flexibility, and cost-efficiency.</p><hr><h3>Graph Lottery Ticket Automated</h3>
<p><a href='https://openreview.net/forum?id=nmBjBZoySX'>https://openreview.net/forum?id=nmBjBZoySX</a></p>
<p><b>Compressor summary</b>: The paper introduces AdaGLT, an adaptive, dynamic, and automated framework for finding graph lottery tickets to improve efficiency and scalability of graph neural networks on large graphs.</p><hr><h3>Video Decomposition Prior: Editing Videos Layer by Layer</h3>
<p><a href='https://openreview.net/forum?id=nfMyERXNru'>https://openreview.net/forum?id=nfMyERXNru</a></p>
<p><b>Compressor summary</b>: The paper presents a novel video editing framework that uses multiple RGB layers and opacity levels to perform tasks like object segmentation, dehazing, and relighting without requiring large datasets or external data collection.</p><hr><h3>Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo</h3>
<p><a href='https://openreview.net/forum?id=nfIAEJFiBZ'>https://openreview.net/forum?id=nfIAEJFiBZ</a></p>
<p><b>Compressor summary</b>: The paper proposes a Thompson sampling exploration strategy for reinforcement learning that samples Q-functions directly from their posterior distribution using Langevin Monte Carlo, achieving good results on deep RL and Atari57 tasks.</p><hr><h3>Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning</h3>
<p><a href='https://openreview.net/forum?id=ndR8Ytrzhh'>https://openreview.net/forum?id=ndR8Ytrzhh</a></p>
<p><b>Compressor summary</b>: ESC is a simple and scalable method to reduce the cost of chain-of-thought reasoning by using less sampling, without sacrificing performance on various reasoning tasks.</p><hr><h3>Analysis of Learning a Flow-based Generative Model from Limited Sample Complexity</h3>
<p><a href='https://openreview.net/forum?id=ndCJeysCPe'>https://openreview.net/forum?id=ndCJeysCPe</a></p>
<p><b>Compressor summary</b>: The paper analyzes how a two-layer autoencoder can generate samples from a high-dimensional Gaussian mixture and shows that the method is optimal with an $\frac{1}{n}$ convergence rate.</p><hr><h3>An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models</h3>
<p><a href='https://openreview.net/forum?id=nc5GgFAvtk'>https://openreview.net/forum?id=nc5GgFAvtk</a></p>
<p><b>Compressor summary</b>: CroPA is a method that uses learnable textual prompts to update visual adversarial perturbations and improve their transferability across different vision tasks with large vision language models.</p><hr><h3>FedCDA: Federated Learning with Cross-rounds Divergence-aware Aggregation</h3>
<p><a href='https://openreview.net/forum?id=nbPGqeH3lt'>https://openreview.net/forum?id=nbPGqeH3lt</a></p>
<p><b>Compressor summary</b>: FedCDA is a novel method for federated learning that selectively aggregates local models from different rounds to reduce diversity and maintain data knowledge.</p><hr><h3>Out-of-Distribution Detection with Negative Prompts</h3>
<p><a href='https://openreview.net/forum?id=nanyAujl6e'>https://openreview.net/forum?id=nanyAujl6e</a></p>
<p><b>Compressor summary</b>: The paper proposes learning a set of negative prompts for each class to improve out-of-distribution (OOD) detection by matching similarity and dissimilarity in the feature space.</p><hr><h3>AdaMerging: Adaptive Model Merging for Multi-Task Learning</h3>
<p><a href='https://openreview.net/forum?id=nZP6NgD3QY'>https://openreview.net/forum?id=nZP6NgD3QY</a></p>
<p><b>Compressor summary</b>: AdaMerging is an unsupervised technique that learns how to merge pre-trained models for multi-task learning using entropy minimization on test samples, improving performance and generalization compared to existing methods.</p><hr><h3>Optimal transport based adversarial patch to leverage large scale attack transferability</h3>
<p><a href='https://openreview.net/forum?id=nZP10evtkV'>https://openreview.net/forum?id=nZP10evtkV</a></p>
<p><b>Compressor summary</b>: The text proposes a distribution-oriented approach using optimal transport to create adversarial patches that can fool multiple neural networks, even without knowing the target model.</p><hr><h3>MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations</h3>
<p><a href='https://openreview.net/forum?id=nY9nITZQjc'>https://openreview.net/forum?id=nY9nITZQjc</a></p>
<p><b>Compressor summary</b>: MIntRec2.0 is a large benchmark dataset for multimodal intent recognition in multi-party conversations with diverse samples, modalities, and contexts.</p><hr><h3>Tight Rates in Supervised Outlier Transfer Learning</h3>
<p><a href='https://openreview.net/forum?id=nUBLhhVM1l'>https://openreview.net/forum?id=nUBLhhVM1l</a></p>
<p><b>Compressor summary</b>: This paper studies when and how knowledge can be transferred from source to target in outlier detection tasks using Neyman-Pearson classification and measures of discrepancy.</p><hr><h3>Rethinking the Power of Graph Canonization in Graph Representation Learning with Stability</h3>
<p><a href='https://openreview.net/forum?id=nTwb2vBLOV'>https://openreview.net/forum?id=nTwb2vBLOV</a></p>
<p><b>Compressor summary</b>: The paper proposes using graph canonization to increase the expressivity of GNNs and studies their stability, introducing a universal graph canonization method that improves performance on various graph tasks.</p><hr><h3>FedInverse: Evaluating Privacy Leakage in Federated Learning</h3>
<p><a href='https://openreview.net/forum?id=nTNgkEIfeb'>https://openreview.net/forum?id=nTNgkEIfeb</a></p>
<p><b>Compressor summary</b>: The paper introduces a data-leakage risk in federated learning due to model inversion attacks and proposes FedInverse, a tool to evaluate this risk using HSIC as a regularizer.</p><hr><h3>A Simple and Scalable Representation for Graph Generation</h3>
<p><a href='https://openreview.net/forum?id=nO344avRib'>https://openreview.net/forum?id=nO344avRib</a></p>
<p><b>Compressor summary</b>: GEEL is a new graph representation that allows for more efficient and effective neural network-based graph generation, especially for large-scale and attributed graphs.</p><hr><h3>CircuitNet 2.0: An Advanced Dataset for Promoting Machine Learning Innovations in Realistic Chip Design Environment</h3>
<p><a href='https://openreview.net/forum?id=nMFSUjxMIl'>https://openreview.net/forum?id=nMFSUjxMIl</a></p>
<p><b>Compressor summary</b>: CircuitNet 2.0 is a large-scale dataset that supports machine learning innovations in chip design by providing comprehensive data from various realistic chip designs.</p><hr><h3>Input-gradient space particle inference for neural network ensembles</h3>
<p><a href='https://openreview.net/forum?id=nLWiR5P3wr'>https://openreview.net/forum?id=nLWiR5P3wr</a></p>
<p><b>Compressor summary</b>: FoRDE is a new method that improves the performance of deep ensembles by diversifying input gradients, leading to better accuracy, calibration, and robustness to perturbations.</p><hr><h3>Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?</h3>
<p><a href='https://openreview.net/forum?id=nJnky5K944'>https://openreview.net/forum?id=nJnky5K944</a></p>
<p><b>Compressor summary</b>: One-layer and single-head Transformers can effectively capture context and memorize information without relying on deep layers or high-rank weight matrices.</p><hr><h3>Structural Estimation of Partially Observed Linear Non-Gaussian Acyclic Model: A Practical Approach with Identifiability</h3>
<p><a href='https://openreview.net/forum?id=nHkMm0ywWm'>https://openreview.net/forum?id=nHkMm0ywWm</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to identify causal relationships among measured and latent variables using high-order statistics and tests for statistical independence.</p><hr><h3>Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems.</h3>
<p><a href='https://openreview.net/forum?id=nHESwXvxWK'>https://openreview.net/forum?id=nHESwXvxWK</a></p>
<p><b>Compressor summary</b>: The study proposes a method using score-based generative models and Sequential Monte Carlo to solve ill-posed linear inverse problems in a Bayesian framework, achieving better results than existing approaches.</p><hr><h3>Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing</h3>
<p><a href='https://openreview.net/forum?id=nFMS6wF2xq'>https://openreview.net/forum?id=nFMS6wF2xq</a></p>
<p><b>Compressor summary</b>: ContextDiff is a novel contextualized diffusion model that improves text-to-image and text-to-video tasks by incorporating cross-modal context into both forward and reverse processes.</p><hr><h3>Communication-Efficient Federated Non-Linear Bandit Optimization</h3>
<p><a href='https://openreview.net/forum?id=nFI3wFM9yN'>https://openreview.net/forum?id=nFI3wFM9yN</a></p>
<p><b>Compressor summary</b>: Fed-GO-UCB is a novel algorithm for collaborative function optimization among multiple clients that preserves data privacy, allows large-scale computing, and works with generic non-linear objective functions, achieving sub-linear rates for regret and communication cost.</p><hr><h3>Listen, Think, and Understand</h3>
<p><a href='https://openreview.net/forum?id=nBZBPXdJlC'>https://openreview.net/forum?id=nBZBPXdJlC</a></p>
<p><b>Compressor summary</b>: LTU is a new model that combines audio perception and reasoning abilities, trained on OpenAQA-5M dataset, which outperforms existing audio models in classification and captioning tasks and shows emerging audio reasoning skills.</p><hr><h3>Self-Supervised Contrastive Forecasting</h3>
<p><a href='https://openreview.net/forum?id=nBCuRzjqK7'>https://openreview.net/forum?id=nBCuRzjqK7</a></p>
<p><b>Compressor summary</b>: The paper proposes a new approach using contrastive learning and enhanced decomposition to improve long-term forecasting by capturing and incorporating global autocorrelation in time series.</p><hr><h3>Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning</h3>
<p><a href='https://openreview.net/forum?id=nAs4LdaP9Y'>https://openreview.net/forum?id=nAs4LdaP9Y</a></p>
<p><b>Compressor summary</b>: Federated Orthogonal Training (FOT) is a novel method that address global catastrophic forgetting in Continual Federated Learning by reducing interference between tasks using orthogonal updates, while preserving privacy and requiring low computation and communication cost.</p><hr><h3>The Consensus Game: Language Model Generation via Equilibrium Search</h3>
<p><a href='https://openreview.net/forum?id=n9xeGcI4Yg'>https://openreview.net/forum?id=n9xeGcI4Yg</a></p>
<p><b>Compressor summary</b>: Equilibrium-ranking is a new game-theoretic procedure for language model decoding that improves performance on various text generation tasks without needing additional training data.</p><hr><h3>Mayfly: a Neural Data Structure for Graph Stream Summarization</h3>
<p><a href='https://openreview.net/forum?id=n7Sr8SW4bn'>https://openreview.net/forum?id=n7Sr8SW4bn</a></p>
<p><b>Compressor summary</b>: The Mayfly is a neural data structure that summarizes graph streams for real-time graph analytics in various domains by learning from meta-tasks.</p><hr><h3>HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments</h3>
<p><a href='https://openreview.net/forum?id=n6mLhaBahJ'>https://openreview.net/forum?id=n6mLhaBahJ</a></p>
<p><b>Compressor summary</b>: HAZARD is a new simulated benchmark for testing embodied agents' decision-making abilities in dynamic environments with disaster scenarios and large language models.</p><hr><h3>Expressive Losses for Verified Robustness via Convex Combinations</h3>
<p><a href='https://openreview.net/forum?id=mzyZ4wzKlM'>https://openreview.net/forum?id=mzyZ4wzKlM</a></p>
<p><b>Compressor summary</b>: The text discusses how expressive losses can improve adversarial robustness and accuracy trade-offs in networks by over-approximating the worst-case loss.</p><hr><h3>Scalable Language Model with Generalized Continual Learning</h3>
<p><a href='https://openreview.net/forum?id=mz8owj4DXu'>https://openreview.net/forum?id=mz8owj4DXu</a></p>
<p><b>Compressor summary</b>: The Scalable Language Model (SLM) overcomes limitations of continual learning in language models by using JARe and DTKR, achieving state-of-the-art performance on diverse tasks and domains with minimal forgetting.</p><hr><h3>OctoPack: Instruction Tuning Code Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=mw1PWNSWZP'>https://openreview.net/forum?id=mw1PWNSWZP</a></p>
<p><b>Compressor summary</b>: Instruction tuning using code from Git commits improves large language models' performance on various coding tasks across multiple languages.</p><hr><h3>Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis</h3>
<p><a href='https://openreview.net/forum?id=mvMI3N4AvD'>https://openreview.net/forum?id=mvMI3N4AvD</a></p>
<p><b>Compressor summary</b>: Mega-TTS is a new zero-shot text-to-speech method that uses an acoustic autoencoder and a prosody language model to generate speech with unseen speakers and styles from short prompts.</p><hr><h3>Views Can Be Deceiving: Improved SSL Through Feature Space Augmentation</h3>
<p><a href='https://openreview.net/forum?id=mutJBk3ILg'>https://openreview.net/forum?id=mutJBk3ILg</a></p>
<p><b>Compressor summary</b>: The paper explores how spurious features affect self-supervised learning and proposes a method called LateTVG to remove them by pruning later layers of the encoder.</p><hr><h3>FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices Using a Computing Power-Aware Scheduler</h3>
<p><a href='https://openreview.net/forum?id=msXxrttLOi'>https://openreview.net/forum?id=msXxrttLOi</a></p>
<p><b>Compressor summary</b>: FedCompass is a semi-asynchronous federated learning algorithm that adapts to client computing power, reducing staleness and waiting times, and improving convergence and accuracy on non-IID heterogeneous datasets.</p><hr><h3>Bridging State and History Representations: Understanding Self-Predictive RL</h3>
<p><a href='https://openreview.net/forum?id=ms0VgzSGF2'>https://openreview.net/forum?id=ms0VgzSGF2</a></p>
<p><b>Compressor summary</b>: The paper proposes a common idea of self-predictive abstraction for representation learning in deep reinforcement learning and provides a minimalist algorithm to learn such representations across different domains.</p><hr><h3>Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding</h3>
<p><a href='https://openreview.net/forum?id=mqVgBbNCm9'>https://openreview.net/forum?id=mqVgBbNCm9</a></p>
<p><b>Compressor summary</b>: The paper proposes Skeleton-of-Thought, a method that speeds up large language models by guiding them to generate skeletons of answers in parallel and then filling them in sequentially.</p><hr><h3>KW-Design: Pushing the Limit of Protein Deign via Knowledge Refinement</h3>
<p><a href='https://openreview.net/forum?id=mpqMVWgqjn'>https://openreview.net/forum?id=mpqMVWgqjn</a></p>
<p><b>Compressor summary</b>: KW-Design is a novel protein inverse folding method that leverages pretrained models, confidence, and memory retrieval to overcome limitations and achieve high recovery rates.</p><hr><h3>Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=mnipav175N'>https://openreview.net/forum?id=mnipav175N</a></p>
<p><b>Compressor summary</b>: The paper introduces Temporally-Correlated Episodic RL (TCE), a method that generates smooth trajectories for robot tasks by using multi-second trajectory samples and evaluating policy updates based on segment-wise advantages, improving sample efficiency and capturing movement correlations.</p><hr><h3>Multi-Resolution Diffusion Models for Time Series Forecasting</h3>
<p><a href='https://openreview.net/forum?id=mmjnr0G8ZY'>https://openreview.net/forum?id=mmjnr0G8ZY</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel multi-resolution diffusion model for time series data that leverages temporal structure and outperforms existing models.</p><hr><h3>Class Probability Matching with Calibrated Networks for Label Shift Adaption</h3>
<p><a href='https://openreview.net/forum?id=mliQ2huFrZ'>https://openreview.net/forum?id=mliQ2huFrZ</a></p>
<p><b>Compressor summary</b>: The paper proposes a new label adaptation framework called class probability matching (CPM) that matches class probabilities between source and target domains and improves generalization and efficiency over existing methods.</p><hr><h3>RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation</h3>
<p><a href='https://openreview.net/forum?id=mlJLVigNHp'>https://openreview.net/forum?id=mlJLVigNHp</a></p>
<p><b>Compressor summary</b>: The paper introduces compressors that generate textual summaries from retrieved documents for retrieval-augmented language models, improving efficiency and performance on language modeling and question answering tasks.</p><hr><h3>Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products</h3>
<p><a href='https://openreview.net/forum?id=mhyQXJ6JsK'>https://openreview.net/forum?id=mhyQXJ6JsK</a></p>
<p><b>Compressor summary</b>: This paper proposes a new method to accelerate computations for 3D data modeling using equivariant neural networks, reducing complexity from $\mathcal{O}(L^6)$ to $\mathcal{O}(L^3)$.</p><hr><h3>Noise Map Guidance: Inversion with Spatial Context for Real Image Editing</h3>
<p><a href='https://openreview.net/forum?id=mhgm0IXtHw'>https://openreview.net/forum?id=mhgm0IXtHw</a></p>
<p><b>Compressor summary</b>: Noise Map Guidance (NMG) is a new method for real-image editing that uses spatial context without requiring optimization, overcoming the limitations of previous text-guided diffusion models.</p><hr><h3>Improving Code Style for Accurate Code Generation</h3>
<p><a href='https://openreview.net/forum?id=maRYffiUpI'>https://openreview.net/forum?id=maRYffiUpI</a></p>
<p><b>Compressor summary</b>: The text discusses improving code generation performance by making programs more structured and readable using a data-cleaning pipeline and shows that this leads to better results even with less data.</p><hr><h3>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</h3>
<p><a href='https://openreview.net/forum?id=mZn2Xyh9Ec'>https://openreview.net/forum?id=mZn2Xyh9Ec</a></p>
<p><b>Compressor summary</b>: FlashAttention-2 improves the efficiency and scalability of Transformers by optimizing work partitioning in the attention layer, reaching up to 73% of the theoretical maximum FEOPs/s on A100 GPUs.</p><hr><h3>Rethinking CNNs Generalization to Backdoor Attack from Frequency Domain</h3>
<p><a href='https://openreview.net/forum?id=mYhH0CDFFa'>https://openreview.net/forum?id=mYhH0CDFFa</a></p>
<p><b>Compressor summary</b>: The paper studies how CNNs memorize trigger changes in poisoned samples' frequency domain, proposes a universal invisible strategy for backdoor attacks, and designs a low-frequency backdoor attack method with high accuracy.</p><hr><h3>Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Map</h3>
<p><a href='https://openreview.net/forum?id=mYWsyTuiRp'>https://openreview.net/forum?id=mYWsyTuiRp</a></p>
<p><b>Compressor summary</b>: The study visualizes how feed-forward blocks in Transformers modify input contextualization and may reveal redundant processing in these models.</p><hr><h3>On the Vulnerability of Adversarially Trained Models Against Two-faced Attacks</h3>
<p><a href='https://openreview.net/forum?id=mXpNp8MMr5'>https://openreview.net/forum?id=mXpNp8MMr5</a></p>
<p><b>Compressor summary</b>: The paper warns that adversarially trained models can be fooled by two-faced attacks, which may mislead evaluations and cause security issues, and suggests a countermeasure.</p><hr><h3>BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity</h3>
<p><a href='https://openreview.net/forum?id=mQYHXUUTkU'>https://openreview.net/forum?id=mQYHXUUTkU</a></p>
<p><b>Compressor summary</b>: The researchers developed a method to generate natural language descriptions for images based on neural activity in different regions of the brain, enabling new insights into how the brain processes visual information.</p><hr><h3>A Hierarchical Bayesian Model for Few-Shot Meta Learning</h3>
<p><a href='https://openreview.net/forum?id=mQ72XRfYRZ'>https://openreview.net/forum?id=mQ72XRfYRZ</a></p>
<p><b>Compressor summary</b>: The paper proposes a new hierarchical Bayesian model for few-shot learning that uses global and local random variables to infer novel episodes, and exploits a Normal-Inverse-Wishart model for efficient training.</p><hr><h3>LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts</h3>
<p><a href='https://openreview.net/forum?id=mNYF0IHbRy'>https://openreview.net/forum?id=mNYF0IHbRy</a></p>
<p><b>Compressor summary</b>: The authors propose a novel approach using Large Language Models to extract information from complex text prompts and generate detailed images, improving the fidelity of text-to-image generation models.</p><hr><h3>Beyond task performance: evaluating and reducing the flaws of large multimodal models with in-context-learning</h3>
<p><a href='https://openreview.net/forum?id=mMaQvkMzDi'>https://openreview.net/forum?id=mMaQvkMzDi</a></p>
<p><b>Compressor summary</b>: The text discusses the limitations of large multimodal models and explores in-context learning as a potential solution, proposing new multimodal ICL approaches to improve their performance.</p><hr><h3>SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents</h3>
<p><a href='https://openreview.net/forum?id=mM7VurbA4r'>https://openreview.net/forum?id=mM7VurbA4r</a></p>
<p><b>Compressor summary</b>: SOTOPIA is an environment for testing AI's social intelligence by simulating complex interactions between agents and humans, revealing GPT-4's challenges in social commonsense reasoning and communication.</p><hr><h3>Emu: Generative Pretraining in Multimodality</h3>
<p><a href='https://openreview.net/forum?id=mL8Q9OOamV'>https://openreview.net/forum?id=mL8Q9OOamV</a></p>
<p><b>Compressor summary</b>: Emu is a versatile model that generates images and text from various inputs, achieving high performance in multiple multimodal tasks.</p><hr><h3>Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective</h3>
<p><a href='https://openreview.net/forum?id=mIEHIcHGOo'>https://openreview.net/forum?id=mIEHIcHGOo</a></p>
<p><b>Compressor summary</b>: The paper explores how to transfer knowledge from larger to smaller language models using sensitivity-based techniques and a module called LoRA.</p><hr><h3>Rethinking the Benefits of Steerable Features in 3D Equivariant Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=mGHJAyR8w0'>https://openreview.net/forum?id=mGHJAyR8w0</a></p>
<p><b>Compressor summary</b>: This study analyzes the expressive power and performance of invariant and equivariant GNNs, showing that increasing the feature dimension is more important than increasing the number of types of steerable features for improving performance.</p><hr><h3>Sum-Product-Set Networks: Deep Tractable Models for Tree-Structured Graphs</h3>
<p><a href='https://openreview.net/forum?id=mF3cTns4pe'>https://openreview.net/forum?id=mF3cTns4pe</a></p>
<p><b>Compressor summary</b>: Sum-product-set networks extend probabilistic circuits to tree-structured graphs using random finite sets for exact and efficient inference, comparing well with intractable neural network models.</p><hr><h3>An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment</h3>
<p><a href='https://openreview.net/forum?id=mE52zURNGc'>https://openreview.net/forum?id=mE52zURNGc</a></p>
<p><b>Compressor summary</b>: The authors derive a closed-form solution for the expected optimum of the Gauss-Newton loss in direct image alignment, which improves accuracy, robustness, and convergence properties, and offers insight into end-to-end learning limitations.</p><hr><h3>EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations</h3>
<p><a href='https://openreview.net/forum?id=mCOBKZmrzD'>https://openreview.net/forum?id=mCOBKZmrzD</a></p>
<p><b>Compressor summary</b>: EquiformerV2 improves on previous methods for 3D atomistic systems using eSCN convolutions, attention re-normalization, separable $S^2$ activation, and separable layer normalization, achieving better results and efficiency.</p><hr><h3>Federated Causal Discovery from Heterogeneous Data</h3>
<p><a href='https://openreview.net/forum?id=m7tJxajC3G'>https://openreview.net/forum?id=m7tJxajC3G</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel federated causal discovery method that can handle diverse scenarios with arbitrary causal models and heterogeneous data, while protecting data privacy using summary statistics.</p><hr><h3>DRSM: De-Randomized Smoothing on Malware Classifier Providing Certified Robustness</h3>
<p><a href='https://openreview.net/forum?id=m7aPLHwsLr'>https://openreview.net/forum?id=m7aPLHwsLr</a></p>
<p><b>Compressor summary</b>: The paper proposes DRSM, a certified defense against evasion attacks on malware detection models, which preserves accuracy and robustness by limiting the impact of adversarial bytes using window ablation.</p><hr><h3>Constructing Adversarial Examples for Vertical Federated Learning: Optimal Client Corruption through Multi-Armed Bandit</h3>
<p><a href='https://openreview.net/forum?id=m52uU0dVbH'>https://openreview.net/forum?id=m52uU0dVbH</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel adversarial attack on vertical federated learning by adaptively corrupting subsets of clients using Thompson sampling with Empirical maximum reward (E-TS) algorithm.</p><hr><h3>Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model</h3>
<p><a href='https://openreview.net/forum?id=m50eKHCttz'>https://openreview.net/forum?id=m50eKHCttz</a></p>
<p><b>Compressor summary</b>: The authors investigate how to transfer complementary knowledge from one deep network to another without performance degradation, using data partitioning techniques that work for most pretrained models and can be done unsupervised.</p><hr><h3>Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=m3xVPaZp6Z'>https://openreview.net/forum?id=m3xVPaZp6Z</a></p>
<p><b>Compressor summary</b>: ReDM is a new algorithm that enables reinforcement learning agents to make adaptive decisions by planning for all possible outcomes and rehearsing on diverse dynamics models, even with zero or limited interaction data.</p><hr><h3>Sentence-level Prompts Benefit Composed Image Retrieval</h3>
<p><a href='https://openreview.net/forum?id=m3ch3kJL7q'>https://openreview.net/forum?id=m3ch3kJL7q</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for composed image retrieval that uses sentence-level prompts generated by V-L models instead of pseudo-word tokens, improving performance on complex changes in the target image.</p><hr><h3>DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING</h3>
<p><a href='https://openreview.net/forum?id=m3RRWWFaVe'>https://openreview.net/forum?id=m3RRWWFaVe</a></p>
<p><b>Compressor summary</b>: This paper explores the ethical values of large language models using Moral Foundation Theory, creates a dataset of moral prompts, and proposes an alignment method to improve value compliance.</p><hr><h3>To the Cutoff... and Beyond? A Longitudinal Perspective on LLM Data Contamination</h3>
<p><a href='https://openreview.net/forum?id=m2NVG4Htxs'>https://openreview.net/forum?id=m2NVG4Htxs</a></p>
<p><b>Compressor summary</b>: The authors analyze data contamination in large language models by studying their performance on two code/mathematical problem-solving datasets, finding significant trends that suggest contamination.</p><hr><h3>Time-Varying Propensity Score to Bridge the Gap between the Past and Present</h3>
<p><a href='https://openreview.net/forum?id=m0x0rv6Iwm'>https://openreview.net/forum?id=m0x0rv6Iwm</a></p>
<p><b>Compressor summary</b>: This paper proposes a time-varying propensity score to handle gradually evolving data in machine learning models by selecting past data that is similar to the current data.</p><hr><h3>Faster Approximation of Probabilistic and Distributional Values via Least Squares</h3>
<p><a href='https://openreview.net/forum?id=lvSMIsztka'>https://openreview.net/forum?id=lvSMIsztka</a></p>
<p><b>Compressor summary</b>: This paper proposes two fast estimators for valuing data using probabilistic values from cooperative game theory, based on a connection to least square regressions, and shows how distributional values can be optimized similarly.</p><hr><h3>RobustTSF: Towards Theory and Design of Robust Time Series Forecasting with Anomalies</h3>
<p><a href='https://openreview.net/forum?id=ltZ9ianMth'>https://openreview.net/forum?id=ltZ9ianMth</a></p>
<p><b>Compressor summary</b>: The paper analyzes three types of anomalies in time series data and proposes a robust forecasting algorithm that outperforms current methods.</p><hr><h3>Bandits Meet Mechanism Design to Combat Clickbait in Online Recommendation</h3>
<p><a href='https://openreview.net/forum?id=lsxeNvYqCj'>https://openreview.net/forum?id=lsxeNvYqCj</a></p>
<p><b>Compressor summary</b>: The text studies a strategic version of the multi-armed bandit problem, where arms choose their click-rates to maximize clicks, and proposes an algorithm that balances incentives and regret minimization.</p><hr><h3>EControl: Fast Distributed Optimization with Compression and Error Control</h3>
<p><a href='https://openreview.net/forum?id=lsvlvWB9vz'>https://openreview.net/forum?id=lsvlvWB9vz</a></p>
<p><b>Compressor summary</b>: The paper proposes EControl, a new mechanism for distributed training with contractive compression that regulates error compensation and ensures fast convergence without strong assumptions on data heterogeneity.</p><hr><h3>A Dynamical View of the Question of Why</h3>
<p><a href='https://openreview.net/forum?id=lrQlLqQase'>https://openreview.net/forum?id=lrQlLqQase</a></p>
<p><b>Compressor summary</b>: The paper introduces a method to learn causality in multivariate time series data using reinforcement learning and shows its effectiveness in discovering and measuring causal relationships in stochastic processes.</p><hr><h3>Submodular Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=loYSzjSaAK'>https://openreview.net/forum?id=loYSzjSaAK</a></p>
<p><b>Compressor summary</b>: The paper proposes a reinforcement learning framework called subRL that handles non-additive rewards with diminishing returns using submodular set functions, and presents an efficient policy gradient algorithm called subPO for various applications.</p><hr><h3>Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments</h3>
<p><a href='https://openreview.net/forum?id=lmM4Ecm4HJ'>https://openreview.net/forum?id=lmM4Ecm4HJ</a></p>
<p><b>Compressor summary</b>: The text proposes a box stability score (BS score) that reflects how much bounding boxes change under feature map dropout, which correlates with object detection accuracy and enables assessment of detectors without test ground truths.</p><hr><h3>Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?</h3>
<p><a href='https://openreview.net/forum?id=lm7MRcsFiS'>https://openreview.net/forum?id=lm7MRcsFiS</a></p>
<p><b>Compressor summary</b>: Ring-A-Bell is a tool that tests the effectiveness of safety measures in text-to-image models by generating inappropriate content based on extracted concepts.</p><hr><h3>The Optimal Constant Solution: Predictable Extrapolation in Deep Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=ljwoQ3cvQh'>https://openreview.net/forum?id=ljwoQ3cvQh</a></p>
<p><b>Compressor summary</b>: Neural network predictions for high-dimensional inputs tend to become constant when faced with out-of-distribution inputs, often approximating the optimal constant solution.</p><hr><h3>ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models</h3>
<p><a href='https://openreview.net/forum?id=liuqDwmbQJ'>https://openreview.net/forum?id=liuqDwmbQJ</a></p>
<p><b>Compressor summary</b>: ViLMA is a task-agnostic benchmark that evaluates the fine-grained capabilities of pretrained Video-Language Models (VidLMs) by using carefully curated counterfactual tests and proficiency tests.</p><hr><h3>The Expressive Power of Low-Rank Adaptation</h3>
<p><a href='https://openreview.net/forum?id=likXVjmh3E'>https://openreview.net/forum?id=likXVjmh3E</a></p>
<p><b>Compressor summary</b>: LoRA is a technique for fine-tuning pre-trained models that can adapt any smaller model to any larger model with a certain rank, and this paper analyzes its expressive power and approximation error.</p><hr><h3>Sliced Denoising: A Physics-Informed Molecular Pre-Training Method</h3>
<p><a href='https://openreview.net/forum?id=liKkG1zcWq'>https://openreview.net/forum?id=liKkG1zcWq</a></p>
<p><b>Compressor summary</b>: SliDe is a novel molecular pre-training method that uses physical principles to improve generalization and robustness in drug discovery by enhancing the accuracy of estimated force fields.</p><hr><h3>Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning</h3>
<p><a href='https://openreview.net/forum?id=lgaFMvZHSJ'>https://openreview.net/forum?id=lgaFMvZHSJ</a></p>
<p><b>Compressor summary</b>: The paper proposes CARE, a self-supervised learning method that adds geometric structure to embeddings by enforcing equivariance and improves downstream tasks and sensitivity to data variations.</p><hr><h3>The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning</h3>
<p><a href='https://openreview.net/forum?id=ldJXXxPE0L'>https://openreview.net/forum?id=ldJXXxPE0L</a></p>
<p><b>Compressor summary</b>: Weight pruning affects large language models' abilities to recall facts from pre-training and process information in context differently, with moderate pruning impairing fact recall but less so processing in context.</p><hr><h3>SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores</h3>
<p><a href='https://openreview.net/forum?id=lajn1iROCu'>https://openreview.net/forum?id=lajn1iROCu</a></p>
<p><b>Compressor summary</b>: SRL is a scalable, efficient, and extensible distributed RL system that enables massively parallelized training and easy development of customized algorithms, outperforming existing libraries in throughput and learning performance.</p><hr><h3>Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding</h3>
<p><a href='https://openreview.net/forum?id=lUYY2qsRTI'>https://openreview.net/forum?id=lUYY2qsRTI</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to estimate and account for uncertainty due to hidden confounding bias in offline reinforcement learning, using delphic uncertainty, and shows its effectiveness on sepsis management and electronic health records benchmarks.</p><hr><h3>Efficient Network Embedding in the Exponentially Large Quantum Hilbert Space: A High-Dimensional Perspective on Embedding</h3>
<p><a href='https://openreview.net/forum?id=lROh08eK6n'>https://openreview.net/forum?id=lROh08eK6n</a></p>
<p><b>Compressor summary</b>: The paper proposes a new network embedding method using high-dimensional quantum states that outperforms low-dimensional methods while being more efficient and robust.</p><hr><h3>On Diffusion Modeling for Anomaly Detection</h3>
<p><a href='https://openreview.net/forum?id=lR3rk7ysXz'>https://openreview.net/forum?id=lR3rk7ysXz</a></p>
<p><b>Compressor summary</b>: The paper proposes Diffusion Time Estimation, a simplified and faster version of Denoising Diffusion Probability Models, for anomaly detection in both unsupervised and semi-supervised scenarios.</p><hr><h3>Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling</h3>
<p><a href='https://openreview.net/forum?id=lOwkOIUJtx'>https://openreview.net/forum?id=lOwkOIUJtx</a></p>
<p><b>Compressor summary</b>: The authors propose a system that uses multiple foveal views and a reinforcement learning-based saccade mechanism to reconstruct scenes, achieving high image recognition accuracy with reduced input pixels and computation.</p><hr><h3>MCM: Masked Cell Modeling for Anomaly Detection in Tabular Data</h3>
<p><a href='https://openreview.net/forum?id=lNZJyEDxy4'>https://openreview.net/forum?id=lNZJyEDxy4</a></p>
<p><b>Compressor summary</b>: The paper proposes a new masking strategy for anomaly detection in tabular data, which captures intrinsic feature correlations and improves interpretability.</p><hr><h3>Non-negative Contrastive Learning</h3>
<p><a href='https://openreview.net/forum?id=lNCnZwcH5Z'>https://openreview.net/forum?id=lNCnZwcH5Z</a></p>
<p><b>Compressor summary</b>: NCL is a non-negative contrastive learning method for deriving interpretable and sparse features from data clusters, improving over standard contrastive learning in various tasks.</p><hr><h3>Grounding Multimodal Large Language Models to the World</h3>
<p><a href='https://openreview.net/forum?id=lLmqxkfSIw'>https://openreview.net/forum?id=lLmqxkfSIw</a></p>
<p><b>Compressor summary</b>: Kosmos-2 is a large language model that connects text to visual objects and enables various multimodal tasks.</p><hr><h3>CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic</h3>
<p><a href='https://openreview.net/forum?id=lKxL5zkssv'>https://openreview.net/forum?id=lKxL5zkssv</a></p>
<p><b>Compressor summary</b>: The CLIP-MUSED method uses a Transformer and subject-specific tokens to decode visual neural information from multiple subjects, overcoming limitations of existing methods and achieving state-of-the-art performance on fMRI datasets.</p><hr><h3>TokenFlow: Consistent Diffusion Features for Consistent Video Editing</h3>
<p><a href='https://openreview.net/forum?id=lKK50q2MtV'>https://openreview.net/forum?id=lKK50q2MtV</a></p>
<p><b>Compressor summary</b>: The paper presents a text-driven video editing framework that uses a text-to-image diffusion model to generate high-quality videos from source videos and target text prompts, without needing training or fine-tuning.</p><hr><h3>Bridging Vision and Language Spaces with Assignment Prediction</h3>
<p><a href='https://openreview.net/forum?id=lK2V2E2MNv'>https://openreview.net/forum?id=lK2V2E2MNv</a></p>
<p><b>Compressor summary</b>: VLAP bridges vision encoders and language models by using word embeddings to map visual representations into language space for improved performance on vision-language tasks.</p><hr><h3>Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting</h3>
<p><a href='https://openreview.net/forum?id=lJkOCMP2aW'>https://openreview.net/forum?id=lJkOCMP2aW</a></p>
<p><b>Compressor summary</b>: Pathformer is a multi-scale transformer model that adapts to varying temporal dynamics in time series, improving forecasting accuracy and generalization.</p><hr><h3>Context-Aware Meta-Learning</h3>
<p><a href='https://openreview.net/forum?id=lJYAkDVnRU'>https://openreview.net/forum?id=lJYAkDVnRU</a></p>
<p><b>Compressor summary</b>: Our meta-learning algorithm learns new visual concepts during inference without fine-tuning, achieving state-of-the-art results on 8 out of 11 benchmarks.</p><hr><h3>LightHGNN: Distilling Hypergraph Neural Networks into MLPs for 100x Faster Inference</h3>
<p><a href='https://openreview.net/forum?id=lHasEfGsXL'>https://openreview.net/forum?id=lHasEfGsXL</a></p>
<p><b>Compressor summary</b>: The paper proposes LightHGNN and LightHGNN+, which distill knowledge from hypergraph neural networks (HGNNs) to multi-layer perceptrons (MLPs) to reduce complexity and improve inference speed while maintaining competitive performance.</p><hr><h3>Efficient ConvBN Blocks for Transfer Learning and Beyond</h3>
<p><a href='https://openreview.net/forum?id=lHZm9vNm5H'>https://openreview.net/forum?id=lHZm9vNm5H</a></p>
<p><b>Compressor summary</b>: This paper introduces a new mode, Tune, for ConvBN blocks that balances efficiency and stability in computer vision tasks, reducing memory footprint and training time while maintaining performance.</p><hr><h3>Bridging the Gap between Binary Neural Networks and Spiking Neural Networks for Efficient Computer Vision</h3>
<p><a href='https://openreview.net/forum?id=lGUyAuuTYZ'>https://openreview.net/forum?id=lGUyAuuTYZ</a></p>
<p><b>Compressor summary</b>: The paper proposes a new training method for sparse BNNs that uses a Hoyer regularizer to improve accuracy and energy efficiency over BNNs, SNNs, and adder neural networks.</p><hr><h3>Vision-Language Foundation Models as Effective Robot Imitators</h3>
<p><a href='https://openreview.net/forum?id=lFYj0oibGR'>https://openreview.net/forum?id=lFYj0oibGR</a></p>
<p><b>Compressor summary</b>: RoboFlamingo is a simple framework that adapts vision-language models to robot control by fine-tuning them on language-conditioned manipulation datasets, achieving state-of-the-art performance and enabling open-loop control.</p><hr><h3>Demonstration-Regularized RL</h3>
<p><a href='https://openreview.net/forum?id=lF2aip4Scn'>https://openreview.net/forum?id=lF2aip4Scn</a></p>
<p><b>Compressor summary</b>: The paper analyzes how using expert demonstrations can improve the sample efficiency of reinforcement learning and provides theoretical guarantees for this method in different settings.</p><hr><h3>Unlock Predictable Scaling from Emergent Abilities</h3>
<p><a href='https://openreview.net/forum?id=lDbjooxLkD'>https://openreview.net/forum?id=lDbjooxLkD</a></p>
<p><b>Compressor summary</b>: The study develops an evaluation strategy called PassUntil to measure task performance improvements in small language models, revealing a strict task scaling law and characterizing emergent abilities in large language models.</p><hr><h3>Multi-Scale Representations by Varing Window Attention for Semantic Segmentation</h3>
<p><a href='https://openreview.net/forum?id=lAhWGOkpSR'>https://openreview.net/forum?id=lAhWGOkpSR</a></p>
<p><b>Compressor summary</b>: VWFormer is a novel multi-scale learner for semantic segmentation that uses varying window attention and achieves competitive efficiency with lower computational cost than existing methods.</p><hr><h3>Assessing Uncertainty in Similarity Scoring: Performance & Fairness in Face Recognition</h3>
<p><a href='https://openreview.net/forum?id=lAhQCHuANV'>https://openreview.net/forum?id=lAhQCHuANV</a></p>
<p><b>Compressor summary</b>: The article presents asymptotic guarantees for empirical ROC curves and shows that naive bootstrap approach can be misleading, suggesting a dedicated recentering technique instead, which is experimentally validated for assessing similarity functions and fairness in face recognition.</p><hr><h3>Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation</h3>
<p><a href='https://openreview.net/forum?id=l60EM8md3t'>https://openreview.net/forum?id=l60EM8md3t</a></p>
<p><b>Compressor summary</b>: The paper proposes m-LTM, a scalable and robust inverse optimal transport framework for audio-text matching that uses mini-batch subsampling, neural networks, Mahalanobis-enhanced ground metrics, and partial optimal transport to learn rich joint embeddings.</p><hr><h3>A Poincar Inequality and Consistency Results for Signal Sampling on Large Graphs</h3>
<p><a href='https://openreview.net/forum?id=l3qtSNsPvC'>https://openreview.net/forum?id=l3qtSNsPvC</a></p>
<p><b>Compressor summary</b>: The paper introduces a new graph sampling technique based on graphon signals, which simplifies large-scale graph machine learning by using unique sampling sets that are consistent across converging graphs.</p><hr><h3>DV-3DLane: End-to-end Multi-modal 3D Lane Detection with Dual-view Representation</h3>
<p><a href='https://openreview.net/forum?id=l1U6sEgYkb'>https://openreview.net/forum?id=l1U6sEgYkb</a></p>
<p><b>Compressor summary</b>: DV-3DLane is a novel framework that combines images and LiDAR points to accurately detect 3D lanes using dual-view multi-modal features, feature fusion, and attention mechanisms.</p><hr><h3>Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation</h3>
<p><a href='https://openreview.net/forum?id=kzGuiRXZrQ'>https://openreview.net/forum?id=kzGuiRXZrQ</a></p>
<p><b>Compressor summary</b>: EQGAT-diff is a new E(3) equivariant diffusion model for 3D molecular design that outperforms existing models on large and limited data sets with explicit hydrogens.</p><hr><h3>COLLIE: Systematic Construction of Constrained Text Generation Tasks</h3>
<p><a href='https://openreview.net/forum?id=kxgSlyirUZ'>https://openreview.net/forum?id=kxgSlyirUZ</a></p>
<p><b>Compressor summary</b>: COLLIE is a framework for creating diverse grammar-based constraints for text generation tasks and analyzing the performance of language models on them.</p><hr><h3>TRAM: Bridging Trust Regions and Sharpness Aware Minimization</h3>
<p><a href='https://openreview.net/forum?id=kxebDHZ7b7'>https://openreview.net/forum?id=kxebDHZ7b7</a></p>
<p><b>Compressor summary</b>: TRAM is a fine-tuning algorithm that improves out-of-domain generalization by optimizing flat minima and smooth, informative representations while preserving pre-trained structure.</p><hr><h3>Enhancing Contrastive Learning for Ordinal Regression via  Ordinal Content Preserved Data Augmentation</h3>
<p><a href='https://openreview.net/forum?id=kx2XZlmgB1'>https://openreview.net/forum?id=kx2XZlmgB1</a></p>
<p><b>Compressor summary</b>: The paper proposes a new augmentation method for contrastive learning to improve ordinal regression by preserving essential ordinal content information in images, outperforming existing methods.</p><hr><h3>Successor Heads: Recurring, Interpretable Attention Heads In The Wild</h3>
<p><a href='https://openreview.net/forum?id=kvcbV8KQsi'>https://openreview.net/forum?id=kvcbV8KQsi</a></p>
<p><b>Compressor summary</b>: The paper introduces successor heads, attention heads that increment tokens with natural ordering, and uses mechanistic interpretability to analyze their behavior in large language models, finding common abstract representations and mod 10 features.</p><hr><h3>Estimating Shape Distances on Neural Representations with Limited Samples</h3>
<p><a href='https://openreview.net/forum?id=kvByNnMERu'>https://openreview.net/forum?id=kvByNnMERu</a></p>
<p><b>Compressor summary</b>: The paper derives bounds on the worst-case convergence of shape distance estimators and introduces a novel method-of-moments estimator with tunable bias-variance for measuring geometric similarity in high-dimensional network representations.</p><hr><h3>JointNet: Extending Text-to-Image Diffusion for Dense Distribution Modeling</h3>
<p><a href='https://openreview.net/forum?id=kv5xE1p3jz'>https://openreview.net/forum?id=kv5xE1p3jz</a></p>
<p><b>Compressor summary</b>: JointNet is a neural network that models the relationship between images and an additional modality (e.g., depth maps) using a pre-trained text-to-image diffusion model and enables various applications like joint image-depth generation and depth prediction.</p><hr><h3>Continuous Field Reconstruction from Sparse Observations with Implicit Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=kuTZMZdCPZ'>https://openreview.net/forum?id=kuTZMZdCPZ</a></p>
<p><b>Compressor summary</b>: The paper proposes a new deep neural network method to reconstruct physical fields from sparse sensor data, which improves reconstruction quality on realistic datasets.</p><hr><h3>Towards image compression with perfect realism at ultra-low bitrates</h3>
<p><a href='https://openreview.net/forum?id=ktdETU9JBg'>https://openreview.net/forum?id=ktdETU9JBg</a></p>
<p><b>Compressor summary</b>: PerCo is a perceptual compression model that uses diffusion models for decoding and textual image descriptions for context, achieving state-of-the-art visual quality at ultra-low bitrates.</p><hr><h3>On Accelerating Diffusion-Based Sampling Processes via Improved Integration Approximation</h3>
<p><a href='https://openreview.net/forum?id=ktJAF3lxbi'>https://openreview.net/forum?id=ktJAF3lxbi</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve ODE-based diffusion models by optimizing coefficients using improved integration approximation, resulting in better FID scores for small NFE values.</p><hr><h3>Text-to-3D with Classifier Score Distillation</h3>
<p><a href='https://openreview.net/forum?id=ktG8Tun1Cy'>https://openreview.net/forum?id=ktG8Tun1Cy</a></p>
<p><b>Compressor summary</b>: The paper proposes Classifier Score Distillation (CSD), a method that uses an implicit classification model for text-to-3D generation without relying on pre-trained 2D diffusion models, achieving superior results in various tasks.</p><hr><h3>Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning</h3>
<p><a href='https://openreview.net/forum?id=krx55l2A6G'>https://openreview.net/forum?id=krx55l2A6G</a></p>
<p><b>Compressor summary</b>: The paper studies how malicious server attacks can steal data from federated learning, shows that existing attacks are detectable, and proposes a new attack method called SEER that can evade detection.</p><hr><h3>DynaVol: Unsupervised Learning for Dynamic Scenes through Object-Centric Voxelization</h3>
<p><a href='https://openreview.net/forum?id=koYsgfEwCQ'>https://openreview.net/forum?id=koYsgfEwCQ</a></p>
<p><b>Compressor summary</b>: DynaVol is a 3D scene generative model that learns object-centric representations in dynamic visual scenes, enabling unsupervised scene decomposition and manipulation of geometric shapes and motion trajectories.</p><hr><h3>Beyond Memorization: Violating Privacy via Inference with Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=kmn0BhQk7p'>https://openreview.net/forum?id=kmn0BhQk7p</a></p>
<p><b>Compressor summary</b>: The study shows that large language models can accurately infer personal attributes from text, posing a serious threat to individual privacy and highlighting the need for better protection methods.</p><hr><h3>Local Composite Saddle Point Optimization</h3>
<p><a href='https://openreview.net/forum?id=kklwv4c4dI'>https://openreview.net/forum?id=kklwv4c4dI</a></p>
<p><b>Compressor summary</b>: FeDualEx is a new distributed optimization method for saddle point problems with composite objectives and constraints that preserves structure in machine learning.</p><hr><h3>FedDA: Faster Adaptive Gradient Methods for Federated Constrained Optimization</h3>
<p><a href='https://openreview.net/forum?id=kjn99xFUF3'>https://openreview.net/forum?id=kjn99xFUF3</a></p>
<p><b>Compressor summary</b>: FedDA is a new adaptive gradient method for federated learning that works with constraints, and it achieves better performance than FedAvg.</p><hr><h3>Towards Universal Multi-Modal Personalization: A Language Model Empowered Generative Paradigm</h3>
<p><a href='https://openreview.net/forum?id=khAE1sTMdX'>https://openreview.net/forum?id=khAE1sTMdX</a></p>
<p><b>Compressor summary</b>: The paper proposes UniMP, a framework that leverages multi-modal data for personalized tasks such as recommendation, search, and generation, using interleaved cross-modal user history information.</p><hr><h3>Large Language Models as Automated Aligners for  benchmarking  Vision-Language Models</h3>
<p><a href='https://openreview.net/forum?id=kZEXgtMNNo'>https://openreview.net/forum?id=kZEXgtMNNo</a></p>
<p><b>Compressor summary</b>: Auto-Bench is a tool that uses large language models to automatically generate and assess question-answer-reasoning tasks for vision-language models, measuring their alignment with human intelligence.</p><hr><h3>Accelerated Sampling with Stacked Restricted Boltzmann Machines</h3>
<p><a href='https://openreview.net/forum?id=kXNJ48Hvw1'>https://openreview.net/forum?id=kXNJ48Hvw1</a></p>
<p><b>Compressor summary</b>: The authors propose Stacked Tempering, a method that improves sampling of complex distributions using nested restricted Boltzmann machines (RBMs) and replica exchange, achieving faster transitions between data modes.</p><hr><h3>Simple Hierarchical Planning with Diffusion</h3>
<p><a href='https://openreview.net/forum?id=kXHEBK9uAY'>https://openreview.net/forum?id=kXHEBK9uAY</a></p>
<p><b>Compressor summary</b>: The Hierarchical Diffuser is a fast and effective planning method that combines hierarchical and diffusion-based planning to overcome computational challenges and improve generalization in long-horizon tasks.</p><hr><h3>Improving equilibrium propagation without weight symmetry through Jacobian homeostasis</h3>
<p><a href='https://openreview.net/forum?id=kUveo5k1GF'>https://openreview.net/forum?id=kUveo5k1GF</a></p>
<p><b>Compressor summary</b>: The paper studies how weight asymmetry and finite nudges affect gradient estimation in equilibrium propagation, a neural network algorithm for neuromorphic systems, and proposes a homeostatic objective to reduce residual bias and improve task performance.</p><hr><h3>Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction</h3>
<p><a href='https://openreview.net/forum?id=kUuKFW7DIF'>https://openreview.net/forum?id=kUuKFW7DIF</a></p>
<p><b>Compressor summary</b>: The paper proposes a hierarchical Transformer model that processes speech at multiple resolutions, achieving better performance and efficiency than existing models.</p><hr><h3>SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation</h3>
<p><a href='https://openreview.net/forum?id=kUCgHbmO11'>https://openreview.net/forum?id=kUCgHbmO11</a></p>
<p><b>Compressor summary</b>: The paper presents a new method, SF(DA)$^2$, for adapting models to unseen domains without source data, using data augmentation based on feature space neighbors and class semantic information.</p><hr><h3>Neuron-Enhanced AutoEncoder Matrix Completion: Theory and Practice</h3>
<p><a href='https://openreview.net/forum?id=kPrxk6tUcg'>https://openreview.net/forum?id=kPrxk6tUcg</a></p>
<p><b>Compressor summary</b>: The paper proposes a neuron-enhanced autoencoder matrix completion method that improves reconstruction capability and response function approximation, and shows its theoretical analysis and numerical results on synthetic and benchmark datasets.</p><hr><h3>Dynamic Sparse Training with Structured Sparsity</h3>
<p><a href='https://openreview.net/forum?id=kOBkxFRKTA'>https://openreview.net/forum?id=kOBkxFRKTA</a></p>
<p><b>Compressor summary</b>: SRigL is a sparse-to-sparse method that learns structured N:M sparsity and achieves fast online inference on CPU.</p><hr><h3>Maximum Entropy Model Correction in Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=kNpSUN0uCc'>https://openreview.net/forum?id=kNpSUN0uCc</a></p>
<p><b>Compressor summary</b>: The paper presents a reinforcement learning approach that uses a MaxEnt Model Correction procedure to improve the accuracy of an approximate model and speed up convergence to the true value function.</p><hr><h3>Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency</h3>
<p><a href='https://openreview.net/forum?id=kNjrhD67LP'>https://openreview.net/forum?id=kNjrhD67LP</a></p>
<p><b>Compressor summary</b>: ITIT is a novel training method for vision-language models that uses unpaired images and texts to achieve high performance with minimal paired data.</p><hr><h3>What's in a Prior? Learned Proximal Networks for Inverse Problems</h3>
<p><a href='https://openreview.net/forum?id=kNPcOaqC5r'>https://openreview.net/forum?id=kNPcOaqC5r</a></p>
<p><b>Compressor summary</b>: The paper presents learned proximal networks (LPN), which use proximal matching to learn exact proximal operators for data-driven regularizers that recover the log-prior of the true data distribution, enabling general, unsupervised, and state-of-the-art inverse problem solutions.</p><hr><h3>ASMR: Activation-Sharing Multi-Resolution Coordinate Networks for Efficient Inference</h3>
<p><a href='https://openreview.net/forum?id=kMp8zCsXNb'>https://openreview.net/forum?id=kMp8zCsXNb</a></p>
<p><b>Compressor summary</b>: ASMR is a new method for encoding images and videos with a compact neural representation that reduces inference complexity and improves reconstruction quality.</p><hr><h3>SE(3)-Stochastic Flow Matching for Protein Backbone Generation</h3>
<p><a href='https://openreview.net/forum?id=kJFIH23hXb'>https://openreview.net/forum?id=kJFIH23hXb</a></p>
<p><b>Compressor summary</b>: Foldflow is a series of generative models that use the flow-matching paradigm over 3D rigid motions (SE(3)) to accurately model protein backbones, offering stability, speed, and flexibility in protein design.</p><hr><h3>Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints</h3>
<p><a href='https://openreview.net/forum?id=kJ0qp9Xdsh'>https://openreview.net/forum?id=kJ0qp9Xdsh</a></p>
<p><b>Compressor summary</b>: LACE is a unified continuous diffusion model for controllable layout generation that incorporates aesthetic constraints and achieves better results than existing methods.</p><hr><h3>Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization</h3>
<p><a href='https://openreview.net/forum?id=kIZ3S3tel6'>https://openreview.net/forum?id=kIZ3S3tel6</a></p>
<p><b>Compressor summary</b>: The paper introduces a new phenomenon in neural network optimization caused by heavy-tailed data and opposing signals, explaining some training dynamics and suggesting improvements to stochastic optimization methods.</p><hr><h3>Reverse Diffusion Monte Carlo</h3>
<p><a href='https://openreview.net/forum?id=kIPEyMSdFV'>https://openreview.net/forum?id=kIPEyMSdFV</a></p>
<p><b>Compressor summary</b>: Reverse diffusion transforms score estimation into mean estimation, leading to a novel Monte Carlo sampling algorithm that improves upon traditional MCMC methods for Gaussian mixture models.</p><hr><h3>Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models</h3>
<p><a href='https://openreview.net/forum?id=kIP0duasBb'>https://openreview.net/forum?id=kIP0duasBb</a></p>
<p><b>Compressor summary</b>: The paper proposes reinforcement learning with CLIP feedback (RLCF) for adapting pre-trained vision-language models to improve their zero-shot generalization ability on various tasks.</p><hr><h3>On the Generalization and Approximation Capacities of Neural Controlled Differential Equations</h3>
<p><a href='https://openreview.net/forum?id=kILAd8RdzA'>https://openreview.net/forum?id=kILAd8RdzA</a></p>
<p><b>Compressor summary</b>: The paper presents a theoretical analysis of Neural Controlled Differential Equations (NCDE) for supervised learning with irregularly sampled time series, examining how sampling affects predictions and generalization bounds.</p><hr><h3>Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs</h3>
<p><a href='https://openreview.net/forum?id=kGteeZ18Ir'>https://openreview.net/forum?id=kGteeZ18Ir</a></p>
<p><b>Compressor summary</b>: The study shows that large-scale language models can have implicit biases and perform worse in reasoning tasks when assigned personas, despite rejecting stereotypes explicitly.</p><hr><h3>Selective Visual Representations Improve Convergence and Generalization for Embodied AI</h3>
<p><a href='https://openreview.net/forum?id=kC5nZDU5zf'>https://openreview.net/forum?id=kC5nZDU5zf</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for embodied AI to filter visual input based on tasks, improving performance and generalization in navigation and manipulation tasks.</p><hr><h3>Faithful Rule Extraction for Differentiable Rule Learning Models</h3>
<p><a href='https://openreview.net/forum?id=kBTzlxM2J1'>https://openreview.net/forum?id=kBTzlxM2J1</a></p>
<p><b>Compressor summary</b>: The paper studies how to extract rules from knowledge graph ML models that are both sound and complete, which is important for safety-critical applications and legal compliance.</p><hr><h3>Lifting Architectural Constraints of Injective Flows</h3>
<p><a href='https://openreview.net/forum?id=kBNIx4Biq4'>https://openreview.net/forum?id=kBNIx4Biq4</a></p>
<p><b>Compressor summary</b>: Injective Flows learn a low-dimensional manifold and its distribution, improving efficiency and stability with a new estimator for the maximum likelihood loss.</p><hr><h3>FasterViT: Fast Vision Transformers with Hierarchical Attention</h3>
<p><a href='https://openreview.net/forum?id=kB4yBiNmXX'>https://openreview.net/forum?id=kB4yBiNmXX</a></p>
<p><b>Compressor summary</b>: FasterViT is a new hybrid neural network that combines CNNs and ViTs, using Hierarchical Attention to achieve high image throughput and accuracy in computer vision tasks.</p><hr><h3>Task structure and nonlinearity jointly determine learned representational geometry</h3>
<p><a href='https://openreview.net/forum?id=k9t8dQ30kU'>https://openreview.net/forum?id=k9t8dQ30kU</a></p>
<p><b>Compressor summary</b>: The choice of activation function in a neural network affects its ability to learn meaningful representations from inputs and outputs, with Tanh networks favoring output structure and ReLU networks retaining more input information.</p><hr><h3>BECLR: Batch Enhanced Contrastive Unsupervised Few-Shot Learning</h3>
<p><a href='https://openreview.net/forum?id=k9SVcrmXL8'>https://openreview.net/forum?id=k9SVcrmXL8</a></p>
<p><b>Compressor summary</b>: The text proposes a novel approach called BECLR that improves unsupervised few-shot learning by addressing shortcomings of contrastive learning and sample bias using DyCE and OpTA modules.</p><hr><h3>Deep Generative Clustering with Multimodal Diffusion Variational Autoencoders</h3>
<p><a href='https://openreview.net/forum?id=k5THrhXDV3'>https://openreview.net/forum?id=k5THrhXDV3</a></p>
<p><b>Compressor summary</b>: The paper proposes a multimodal VAE model that learns data clusters and improves generative performance over existing methods.</p><hr><h3>Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making</h3>
<p><a href='https://openreview.net/forum?id=k581sTMyPt'>https://openreview.net/forum?id=k581sTMyPt</a></p>
<p><b>Compressor summary</b>: SUFO is a framework that enhances interpretability of fine-tuned transformer feature spaces for clinical decision-making using various analytic and visualization techniques, revealing insights about model trust and robustness.</p><hr><h3>TAB: Temporal Accumulated Batch Normalization in Spiking Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=k1wlmtPGLq'>https://openreview.net/forum?id=k1wlmtPGLq</a></p>
<p><b>Compressor summary</b>: Temporal Accumulated Batch Normalization (TAB) is a new method for training Spiking Neural Networks that improves their performance by accounting for the temporal dynamics of neuron activity.</p><hr><h3>C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion</h3>
<p><a href='https://openreview.net/forum?id=jzzEHTBFOT'>https://openreview.net/forum?id=jzzEHTBFOT</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve uncertainty estimation in CLIP by optimizing prompts during test-time using a novel metric called Average Text Feature Dispersion (ATFD).</p><hr><h3>Language Modeling Is Compression</h3>
<p><a href='https://openreview.net/forum?id=jznbgiynus'>https://openreview.net/forum?id=jznbgiynus</a></p>
<p><b>Compressor summary</b>: The text discusses how large language models can be viewed as powerful predictors and compressors, providing new insights into their capabilities and demonstrating their effectiveness in various tasks.</p><hr><h3>From Sparse to Soft Mixtures of Experts</h3>
<p><a href='https://openreview.net/forum?id=jxpsAj7ltE'>https://openreview.net/forum?id=jxpsAj7ltE</a></p>
<p><b>Compressor summary</b>: Soft MoE is a sparse Transformer that improves the performance and scalability of mixture of expert models while addressing their challenges.</p><hr><h3>Fair Classifiers that Abstain without Harm</h3>
<p><a href='https://openreview.net/forum?id=jvveGAbkVx'>https://openreview.net/forum?id=jvveGAbkVx</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to make classifiers abstain from predicting certain samples to achieve group fairness while maintaining original accuracy, and analyzes the feasibility and trade-offs of this approach.</p><hr><h3>Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts</h3>
<p><a href='https://openreview.net/forum?id=jvtmdK69KQ'>https://openreview.net/forum?id=jvtmdK69KQ</a></p>
<p><b>Compressor summary</b>: The paper studies how the top-K sparse softmax gating function affects density and parameter estimations in a Gaussian mixture of experts model and proposes novel loss functions and conditions for convergence.</p><hr><h3>Language-Informed Visual Concept Learning</h3>
<p><a href='https://openreview.net/forum?id=juuyW8B8ig'>https://openreview.net/forum?id=juuyW8B8ig</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to learn a language-informed visual concept representation from pre-trained models and use it to generate images with novel compositions of visual concepts.</p><hr><h3>ETGraph: A Pioneering Dataset Bridging Ethereum and Twitter</h3>
<p><a href='https://openreview.net/forum?id=juE0rWGCJW'>https://openreview.net/forum?id=juE0rWGCJW</a></p>
<p><b>Compressor summary</b>: ETGraph is a novel dataset that combines Ethereum transaction records with verified Twitter accounts to enhance blockchain analysis by incorporating social network data.</p><hr><h3>What Makes a Good Prune? Optimal Unstructured Pruning for Maximal Cosine Similarity</h3>
<p><a href='https://openreview.net/forum?id=jsvvPVVzwf'>https://openreview.net/forum?id=jsvvPVVzwf</a></p>
<p><b>Compressor summary</b>: The paper investigates the mechanisms of global unstructured magnitude pruning in deep neural networks, proves the optimality of L1 pruning by cosine similarity, and proposes a method to determine the optimal pruning amount based on parameter distribution kurtosis.</p><hr><h3>Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling</h3>
<p><a href='https://openreview.net/forum?id=jsWCmrsHHs'>https://openreview.net/forum?id=jsWCmrsHHs</a></p>
<p><b>Compressor summary</b>: The paper proposes a new DRL-guided heuristic for solving job-shop scheduling problems, using a graph representation scheme and a novel message-passing mechanism to encode complete solutions and evaluate them efficiently.</p><hr><h3>Unprocessing Seven Years of Algorithmic Fairness</h3>
<p><a href='https://openreview.net/forum?id=jr03SfWsBS'>https://openreview.net/forum?id=jr03SfWsBS</a></p>
<p><b>Compressor summary</b>: The authors evaluate fairness-enhancing methods for machine learning models and find that postprocessing achieves the best trade-off between fairness and accuracy, using a novel technique called unprocessing to compare different approaches.</p><hr><h3>MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback</h3>
<p><a href='https://openreview.net/forum?id=jp3gWrMuIZ'>https://openreview.net/forum?id=jp3gWrMuIZ</a></p>
<p><b>Compressor summary</b>: MINT is a benchmark that evaluates large language models' ability to solve complex tasks with multiple rounds of interactions using tools and natural language feedback, revealing interesting findings about their performance and limitations.</p><hr><h3>COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL</h3>
<p><a href='https://openreview.net/forum?id=jnFcKjtUPN'>https://openreview.net/forum?id=jnFcKjtUPN</a></p>
<p><b>Compressor summary</b>: $	exttt{COPlanner}$ is a planning-driven framework for model-based reinforcement learning that improves sample efficiency and asymptotic performance by using uncertainty-aware policy-guided model predictive control to balance conservative model rollouts and optimistic environment exploration.</p><hr><h3>How Does Wild Data Provably Help OOD Detection?</h3>
<p><a href='https://openreview.net/forum?id=jlEjB8MVGa'>https://openreview.net/forum?id=jlEjB8MVGa</a></p>
<p><b>Compressor summary</b>: The paper introduces SAL, a new framework that uses unlabeled data to improve out-of-distribution detection by separating and learning from candidate outliers and labeled in-distribution data.</p><hr><h3>Space Group Constrained Crystal Generation</h3>
<p><a href='https://openreview.net/forum?id=jkvZ7v4OmP'>https://openreview.net/forum?id=jkvZ7v4OmP</a></p>
<p><b>Compressor summary</b>: This paper proposes a new diffusion model (DiffCSP++) for generating crystals that considers the spacegroup constraint, which is important for describing their geometry and properties, by translating it into two tractable cases.</p><hr><h3>Self-Supervised High Dynamic Range Imaging with Multi-Exposure Images in Dynamic Scenes</h3>
<p><a href='https://openreview.net/forum?id=jjiOHEcS2c'>https://openreview.net/forum?id=jjiOHEcS2c</a></p>
<p><b>Compressor summary</b>: SelfHDR is a self-supervised method for HDR image reconstruction from multi-exposure images that avoids ghosting artifacts without needing labeled data.</p><hr><h3>Stochastic Controlled Averaging for Federated Learning with Communication Compression</h3>
<p><a href='https://openreview.net/forum?id=jj5ZjZsWJe'>https://openreview.net/forum?id=jj5ZjZsWJe</a></p>
<p><b>Compressor summary</b>: The paper proposes two new communication compression algorithms for Federated Learning, SCALLION and SCAFCOM, which improve performance in terms of communication and computation complexities, and achieve fast convergence rates under arbitrary data heterogeneity.</p><hr><h3>Knowledge Fusion of Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=jiDsk12qcz'>https://openreview.net/forum?id=jiDsk12qcz</a></p>
<p><b>Compressor summary</b>: The paper proposes knowledge fusion for combining different pre-trained language models to create a more powerful model with enhanced capabilities.</p><hr><h3>Look, Remember and Reason: Grounded Reasoning in Videos with Language Models</h3>
<p><a href='https://openreview.net/forum?id=jhPvuc7kxB'>https://openreview.net/forum?id=jhPvuc7kxB</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to train multi-modal LMs on low-level video tasks, enabling them to perform causal and compositional spatiotemporal reasoning using *Look, Remember, Reason* process with spatiotemporal attention.</p><hr><h3>MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning</h3>
<p><a href='https://openreview.net/forum?id=jenyYQzue1'>https://openreview.net/forum?id=jenyYQzue1</a></p>
<p><b>Compressor summary</b>: MuSR is a novel dataset for evaluating language models on complex multistep soft reasoning tasks in natural language narratives, created using a neurosymbolic synthetic-to-natural generation algorithm and real-world domains of reasoning.</p><hr><h3>Follow-the-Perturbed-Leader for Adversarial Bandits: Heavy Tails, Robustness, and Privacy</h3>
<p><a href='https://openreview.net/forum?id=jeMZi2Z9xe'>https://openreview.net/forum?id=jeMZi2Z9xe</a></p>
<p><b>Compressor summary</b>: The paper proposes an algorithm to solve adversarial bandit problems with heavy-tailed losses, achieving optimal regret and improving privacy guarantees compared to existing methods.</p><hr><h3>Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models</h3>
<p><a href='https://openreview.net/forum?id=jd5GokdySz'>https://openreview.net/forum?id=jd5GokdySz</a></p>
<p><b>Compressor summary</b>: The paper proposes a new robustness measurement for image classification models that compares their performance with a surrogate oracle (a zoo of foundation models) using extended datasets with perturbed samples.</p><hr><h3>Dynamics-Informed Protein Design with Structure Conditioning</h3>
<p><a href='https://openreview.net/forum?id=jZPqf2G9Sw'>https://openreview.net/forum?id=jZPqf2G9Sw</a></p>
<p><b>Compressor summary</b>: The authors propose a new method for protein generative modeling that incorporates dynamical properties using Normal Mode Analysis and shows its effectiveness by transforming an unconditional model into a conditional one without retraining.</p><hr><h3>Jointly-Learned Exit and Inference for a Dynamic Neural Network</h3>
<p><a href='https://openreview.net/forum?id=jX2DT7qDam'>https://openreview.net/forum?id=jX2DT7qDam</a></p>
<p><b>Compressor summary</b>: The text discusses early-exiting dynamic neural networks (EDNN), which improve performance and uncertainty estimation in machine learning by connecting the gating mechanism and intermediate inference modules, addressing previous limitations.</p><hr><h3>Towards Foundation Models for Knowledge Graph Reasoning</h3>
<p><a href='https://openreview.net/forum?id=jVEoydFOl9'>https://openreview.net/forum?id=jVEoydFOl9</a></p>
<p><b>Compressor summary</b>: ULTRA is an approach for learning universal and transferable graph representations that enable foundation models to infer on any knowledge graph with arbitrary entity and relation vocabularies.</p><hr><h3>Hybrid Distillation: Connecting Masked Autoencoders with Contrastive Learners</h3>
<p><a href='https://openreview.net/forum?id=jUWktnsplU'>https://openreview.net/forum?id=jUWktnsplU</a></p>
<p><b>Compressor summary</b>: The paper proposes Hybrid Distill, a method that combines Contrastive Learning and Masked Image Modeling for representation learning, improving feature diversity and discrimination.</p><hr><h3>Pushing Boundaries: Mixup's Influence on Neural Collapse</h3>
<p><a href='https://openreview.net/forum?id=jTSKkcbEsj'>https://openreview.net/forum?id=jTSKkcbEsj</a></p>
<p><b>Compressor summary</b>: Mixup, a data augmentation technique for deep neural networks, induces distinctive geometric configurations in the last-layer activations that enhance robustness and calibration.</p><hr><h3>Optimal Sample Complexity for Average Reward Markov Decision Processes</h3>
<p><a href='https://openreview.net/forum?id=jOm5p3q7c7'>https://openreview.net/forum?id=jOm5p3q7c7</a></p>
<p><b>Compressor summary</b>: The paper proposes an estimator for the optimal policy of MDPs with a sample complexity matching the lower bound in the literature, using ideas from previous works.</p><hr><h3>Analyzing and Improving OT-based Adversarial Networks</h3>
<p><a href='https://openreview.net/forum?id=jODehvtTDx'>https://openreview.net/forum?id=jODehvtTDx</a></p>
<p><b>Compressor summary</b>: The paper presents a unified framework for optimal transport-based generative models and proposes a novel method to improve their performance by gradually refining the generated distribution.</p><hr><h3>ASID: Active Exploration for System Identification and Reconstruction in Robotic Manipulation</h3>
<p><a href='https://openreview.net/forum?id=jNR6s6OSBT'>https://openreview.net/forum?id=jNR6s6OSBT</a></p>
<p><b>Compressor summary</b>: The authors propose a learning system that uses a small amount of real-world data to refine a simulation model and plan an accurate control strategy for robotic manipulation tasks.</p><hr><h3>Debiasing Attention Mechanism in Transformer without Demographics</h3>
<p><a href='https://openreview.net/forum?id=jLIUfrAcMQ'>https://openreview.net/forum?id=jLIUfrAcMQ</a></p>
<p><b>Compressor summary</b>: The paper proposes a new debiasing approach for transformers using attention mechanism components without sensitive labels, achieving fairness and efficiency in computer vision and natural language processing tasks.</p><hr><h3>Rethinking Branching on Exact Combinatorial Optimization Solver: The First Deep Symbolic Discovery Framework</h3>
<p><a href='https://openreview.net/forum?id=jKhNBulNMh'>https://openreview.net/forum?id=jKhNBulNMh</a></p>
<p><b>Compressor summary</b>: The paper proposes a symbolic discovery framework called Symb4CO that learns high-performance policies for exact combinatorial optimization solvers using neural networks, achieving comparable performance to GPU-based approaches with high efficiency and interpretability.</p><hr><h3>Less is More: Fewer Interpretable Region via Submodular Subset Selection</h3>
<p><a href='https://openreview.net/forum?id=jKTUlxo5zy'>https://openreview.net/forum?id=jKTUlxo5zy</a></p>
<p><b>Compressor summary</b>: The paper proposes a new image attribution method based on submodular subset selection, which improves interpretability and accuracy for different datasets and predictions.</p><hr><h3>OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text</h3>
<p><a href='https://openreview.net/forum?id=jKHmjlpViu'>https://openreview.net/forum?id=jKHmjlpViu</a></p>
<p><b>Compressor summary</b>: OpenWebMath is a new dataset for training language models to improve their quantitative reasoning skills using 14.7B tokens of mathematical webpages.</p><hr><h3>TASK PLANNING FOR VISUAL ROOM REARRANGEMENT UNDER PARTIAL OBSERVABILITY</h3>
<p><a href='https://openreview.net/forum?id=jJvXNpvOdM'>https://openreview.net/forum?id=jJvXNpvOdM</a></p>
<p><b>Compressor summary</b>: The paper proposes a new planner for embodied agents to search and rearrange objects in untidy rooms using visual input, commonsense knowledge, and graph-based representation, achieving better performance than existing methods.</p><hr><h3>BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph</h3>
<p><a href='https://openreview.net/forum?id=jJCeMiwHdH'>https://openreview.net/forum?id=jJCeMiwHdH</a></p>
<p><b>Compressor summary</b>: BioBridge is a framework that connects different unimodal models for biomedical tasks using Knowledge Graphs, improving performance and generalization.</p><hr><h3>Provably Efficient UCB-type Algorithms For Learning Predictive State Representations</h3>
<p><a href='https://openreview.net/forum?id=jId5PXbBbX'>https://openreview.net/forum?id=jId5PXbBbX</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel UCB-type approach for learning sequential decision-making problems with predictive state representations, achieving computational efficiency, near-optimal policy, and accurate model estimation.</p><hr><h3>Lightweight Language Model Calibration for Open-ended Question Answering with Varied Answer Lengths</h3>
<p><a href='https://openreview.net/forum?id=jH67LHVOIO'>https://openreview.net/forum?id=jH67LHVOIO</a></p>
<p><b>Compressor summary</b>: Litcab is a lightweight calibration mechanism for large language models that improves their accuracy and trustworthiness by adding a linear layer and training only a small fraction of parameters.</p><hr><h3>IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks</h3>
<p><a href='https://openreview.net/forum?id=jFa5KESW65'>https://openreview.net/forum?id=jFa5KESW65</a></p>
<p><b>Compressor summary</b>: The paper proposes a new image resampling technique to defend against adversarial attacks by creating an implicit representation and using SampleNet to generate pixel-wise shifts.</p><hr><h3>Compressing Latent Space via Least Volume</h3>
<p><a href='https://openreview.net/forum?id=jFJPd9kIiF'>https://openreview.net/forum?id=jFJPd9kIiF</a></p>
<p><b>Compressor summary</b>: Least Volume is a simple regularization technique that reduces latent dimensions in autoencoders without prior knowledge of the data dimensionality, by using decoder Lipschitz continuity, and has similar PCA-like importance ordering effects on nonlinear models.</p><hr><h3>Language Models Represent Space and Time</h3>
<p><a href='https://openreview.net/forum?id=jE8xbmvFin'>https://openreview.net/forum?id=jE8xbmvFin</a></p>
<p><b>Compressor summary</b>: The study shows that large language models learn linear representations of space and time across multiple scales, suggesting they have a coherent world model rather than just memorizing data.</p><hr><h3>Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency</h3>
<p><a href='https://openreview.net/forum?id=j8hdRqOUhN'>https://openreview.net/forum?id=j8hdRqOUhN</a></p>
<p><b>Compressor summary</b>: ReSample is an algorithm that uses latent diffusion models to solve inverse problems with data consistency and a novel resampling scheme, achieving better results than current methods.</p><hr><h3>Feasibility-Guided Safe Offline Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=j5JvZCaDM0'>https://openreview.net/forum?id=j5JvZCaDM0</a></p>
<p><b>Compressor summary</b>: FISOR is a safe offline reinforcement learning method that maximizes rewards and minimizes risks by translating hard safety constraints to identifying the largest feasible region given the offline dataset.</p><hr><h3>Non-Exchangeable Conformal Risk Control</h3>
<p><a href='https://openreview.net/forum?id=j511LaqEeP'>https://openreview.net/forum?id=j511LaqEeP</a></p>
<p><b>Compressor summary</b>: Non-exchangeable conformal risk control is a flexible framework for providing statistical guarantees on any monotone loss function when dealing with non-exchangeable data, such as in change points or time series.</p><hr><h3>Training Graph Transformers via Curriculum-Enhanced Attention Distillation</h3>
<p><a href='https://openreview.net/forum?id=j4VMrwgn1M'>https://openreview.net/forum?id=j4VMrwgn1M</a></p>
<p><b>Compressor summary</b>: The paper proposes a curriculum-enhanced attention distillation method for semi-supervised node classification using Graph Transformers, which improves performance on seven public graph benchmarks.</p><hr><h3>ContextRef: Evaluating Referenceless Metrics for Image Description Generation</h3>
<p><a href='https://openreview.net/forum?id=j0ZvKSNZiP'>https://openreview.net/forum?id=j0ZvKSNZiP</a></p>
<p><b>Compressor summary</b>: ContextRef is a benchmark for evaluating referenceless metrics that measure image description quality by assessing their alignment with human preferences and considering contextual factors.</p><hr><h3>Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM</h3>
<p><a href='https://openreview.net/forum?id=izrOLJov5y'>https://openreview.net/forum?id=izrOLJov5y</a></p>
<p><b>Compressor summary</b>: The text introduces a new way to adapt large language models for question answering and speech continuation using a pre-trained speech encoder and cross-modal training on spectrograms, which improves speaker preservation, semantic coherence, and knowledge retention.</p><hr><h3>Parameter-Efficient Multi-Task Model Fusion with Partial Linearizeation</h3>
<p><a href='https://openreview.net/forum?id=iynRvVVAmH'>https://openreview.net/forum?id=iynRvVVAmH</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method to improve multi-task fusion using partial linearization, which allows for more effective incorporation of different tasks into a single model while maintaining efficiency in fine-tuning and inference.</p><hr><h3>The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images</h3>
<p><a href='https://openreview.net/forum?id=ixP76Y33y1'>https://openreview.net/forum?id=ixP76Y33y1</a></p>
<p><b>Compressor summary</b>: This paper explores why neural networks perform differently when trained on medical versus natural images, proposes a metric for label sharpness, and shows how it affects generalization, representation learning, and adversarial robustness.</p><hr><h3>Learning Performance-Improving Code Edits</h3>
<p><a href='https://openreview.net/forum?id=ix7rLVHXyY'>https://openreview.net/forum?id=ix7rLVHXyY</a></p>
<p><b>Compressor summary</b>: The authors present a framework that adapts large language models to high-level program optimization using various techniques, achieving better performance than human programmers on a dataset of C++ code edits.</p><hr><h3>MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding</h3>
<p><a href='https://openreview.net/forum?id=itGkF993gz'>https://openreview.net/forum?id=itGkF993gz</a></p>
<p><b>Compressor summary</b>: The paper proposes a method, MPAE-PPI, that uses microenvironment codes to predict protein-protein interactions efficiently and effectively, considering both protein sequence and structure.</p><hr><h3>DOS: Diverse Outlier Sampling for Out-of-Distribution Detection</h3>
<p><a href='https://openreview.net/forum?id=iriEqxFB4y'>https://openreview.net/forum?id=iriEqxFB4y</a></p>
<p><b>Compressor summary</b>: Diverse Outlier Sampling (DOS) is a novel technique that selects diverse and informative outliers for OOD detection, improving the model's performance by creating a compact decision boundary between ID and OOD data.</p><hr><h3>Efficient Modulation for Vision Networks</h3>
<p><a href='https://openreview.net/forum?id=ip5LHJs6QX'>https://openreview.net/forum?id=ip5LHJs6QX</a></p>
<p><b>Compressor summary</b>: The paper introduces EfficientMod, a novel block design for efficient vision networks that improves accuracy-efficiency trade-offs and sets new state-of-the-art performance on several tasks.</p><hr><h3>Multi-View Representation is What You Need for Point-Cloud Pre-Training</h3>
<p><a href='https://openreview.net/forum?id=imZcqOrbig'>https://openreview.net/forum?id=imZcqOrbig</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for pre-training 3D point clouds using 2D networks to learn consistent 2D projections and retain 3D features, achieving state-of-the-art results on various downstream tasks.</p><hr><h3>How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations</h3>
<p><a href='https://openreview.net/forum?id=ikwEDva1JZ'>https://openreview.net/forum?id=ikwEDva1JZ</a></p>
<p><b>Compressor summary</b>: This paper studies how transformer-based language models learn complex in-context tasks by analyzing their representations, behaviors, and capabilities in synthetic problems.</p><hr><h3>Scaling Convex Neural Networks with Burer-Monteiro Factorization</h3>
<p><a href='https://openreview.net/forum?id=ikmuHqugN7'>https://openreview.net/forum?id=ikmuHqugN7</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve training convex neural networks by using a factorization technique that makes it computationally feasible and ensures global optima, while also showing that linear self-attention with many heads is free of spurious local minima.</p><hr><h3>A Neural Framework for Generalized Causal Sensitivity Analysis</h3>
<p><a href='https://openreview.net/forum?id=ikX6D1oM1c'>https://openreview.net/forum?id=ikX6D1oM1c</a></p>
<p><b>Compressor summary</b>: NeuralCSA is a neural framework for causal sensitivity analysis that can handle various sensitivity models, treatment types, and causal queries with theoretical guarantees.</p><hr><h3>FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling</h3>
<p><a href='https://openreview.net/forum?id=ijoqFqSC7p'>https://openreview.net/forum?id=ijoqFqSC7p</a></p>
<p><b>Compressor summary</b>: The paper proposes FreeNoise, a tuning-free and time-efficient paradigm to enhance text-driven video generation models by rescheduling noises and injecting motions for multiple texts.</p><hr><h3>Graph Metanetworks for Processing Diverse Neural Architectures</h3>
<p><a href='https://openreview.net/forum?id=ijK5hyxs0n'>https://openreview.net/forum?id=ijK5hyxs0n</a></p>
<p><b>Compressor summary</b>: Graph Metanetworks (GMNs) use graph neural networks to process graphs representing input neural networks, generalizing to various neural architectures and accounting for symmetries and geometry of parameter spaces.</p><hr><h3>USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields</h3>
<p><a href='https://openreview.net/forum?id=igfDXfMvm5'>https://openreview.net/forum?id=igfDXfMvm5</a></p>
<p><b>Compressor summary</b>: USB-NeRF is a method that corrects rolling shutter distortions and recovers accurate camera motion trajectory for NeRF-based novel view synthesis using a physical image formation model.</p><hr><h3>First-order ANIL provably learns representations despite overparametrisation</h3>
<p><a href='https://openreview.net/forum?id=if2vRbS8Ew'>https://openreview.net/forum?id=if2vRbS8Ew</a></p>
<p><b>Compressor summary</b>: This paper shows that first-order adaptive network induction (ANIL), a model-agnostic meta-learning method, learns linear shared representations with overparametrized linear two-layer networks and performs well on new tasks after one gradient step.</p><hr><h3>Dissecting sample hardness: Fine-grained analysis of Hardness Characterization Methods</h3>
<p><a href='https://openreview.net/forum?id=icTZCUbtD6'>https://openreview.net/forum?id=icTZCUbtD6</a></p>
<p><b>Compressor summary</b>: The paper introduces a fine-grained taxonomy of hardness types and presents H-CAT, a toolkit for comprehensive and quantitative benchmarking of Hardness Characterization Methods (HCMs).</p><hr><h3>Ultra-sparse network advantage in deep learning via Cannistraci-Hebb brain-inspired training with hyperbolic meta-deep community-layered epitopology</h3>
<p><a href='https://openreview.net/forum?id=iayEcORsGd'>https://openreview.net/forum?id=iayEcORsGd</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method called Epitopological Sparse Meta-deep Learning (ESML) that uses brain-inspired learning paradigms to train ultra-sparse artificial neural networks, achieving better performance than fully connected networks on some tasks and reducing the number of neurons during training.</p><hr><h3>CausalTime: Realistically Generated Time-series for Benchmarking of Causal Discovery</h3>
<p><a href='https://openreview.net/forum?id=iad1yyyGme'>https://openreview.net/forum?id=iad1yyyGme</a></p>
<p><b>Compressor summary</b>: CausalTime is a pipeline that generates realistic time-series with ground truth causal graphs for evaluating and predicting TSCD algorithms' performance on real data.</p><hr><h3>Identifiable Latent Polynomial Causal Models through the Lens of Change</h3>
<p><a href='https://openreview.net/forum?id=ia9fKO1Vjq'>https://openreview.net/forum?id=ia9fKO1Vjq</a></p>
<p><b>Compressor summary</b>: The paper explores nonlinear latent causal models and proposes a method to learn them with partial identifiability and consistent representations.</p><hr><h3>On the Hardness of Online Nonconvex Optimization with Single Oracle Feedback</h3>
<p><a href='https://openreview.net/forum?id=iZgECfyHXF'>https://openreview.net/forum?id=iZgECfyHXF</a></p>
<p><b>Compressor summary</b>: This paper studies online nonconvex optimization with limited feedback, deriving lower bounds on the local regret and showing that classical algorithms are optimal in certain settings.</p><hr><h3>Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=iX1RjVQODj'>https://openreview.net/forum?id=iX1RjVQODj</a></p>
<p><b>Compressor summary</b>: CPL is an off-policy algorithm for learning optimal policies from human preferences without using reward functions or reinforcement learning, addressing limitations of previous methods in high-dimensional and sequential RLHF problems.</p><hr><h3>Gradual Domain Adaptation via Gradient Flow</h3>
<p><a href='https://openreview.net/forum?id=iTTZFKrlGV'>https://openreview.net/forum?id=iTTZFKrlGV</a></p>
<p><b>Compressor summary</b>: The paper introduces GGF, a domain adaptation method that generates intermediate domains with labels using Wasserstein gradient flow and Langevin algorithm, and uses classifier-based and sample-based potentials to ensure quality.</p><hr><h3>Kernelised Normalising Flows</h3>
<p><a href='https://openreview.net/forum?id=iTFdNLHE7k'>https://openreview.net/forum?id=iTFdNLHE7k</a></p>
<p><b>Compressor summary</b>: Ferumal flow is a new kernel-based normalizing flow model that can estimate and generate densities efficiently and accurately, especially when data is scarce.</p><hr><h3>Identifying Policy Gradient Subspaces</h3>
<p><a href='https://openreview.net/forum?id=iPWxqnt2ke'>https://openreview.net/forum?id=iPWxqnt2ke</a></p>
<p><b>Compressor summary</b>: This paper shows that policy gradient methods can be more efficient by using low-dimensional subspaces of gradients in reinforcement learning despite changing data distributions.</p><hr><h3>ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models</h3>
<p><a href='https://openreview.net/forum?id=iIT02bAKzv'>https://openreview.net/forum?id=iIT02bAKzv</a></p>
<p><b>Compressor summary</b>: ECoFLaP is a two-stage pruning approach for large vision-language models that uses global importance scores to achieve better model compression and improve multimodal tasks.</p><hr><h3>Disentangling Time Series Representations via Contrastive based $l$-Variational Inference</h3>
<p><a href='https://openreview.net/forum?id=iI7hZSczxE'>https://openreview.net/forum?id=iI7hZSczxE</a></p>
<p><b>Compressor summary</b>: DisCo learns disentangled representations of home appliance electricity usage, considering attribute correlations, to help users understand and optimize consumption for a reduced carbon footprint.</p><hr><h3>Poly-View Contrastive Learning</h3>
<p><a href='https://openreview.net/forum?id=iHcTLIor0m'>https://openreview.net/forum?id=iHcTLIor0m</a></p>
<p><b>Compressor summary</b>: The text discusses poly-view tasks in contrastive learning, where multiple related views are used to improve representation learning with information maximization and sufficient statistics, leading to better performance than SimCLR on ImageNet1k.</p><hr><h3>Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective</h3>
<p><a href='https://openreview.net/forum?id=iCNOK45Csv'>https://openreview.net/forum?id=iCNOK45Csv</a></p>
<p><b>Compressor summary</b>: The study proposes new trigger pattern generation methods for dataset distillation that can execute effective and resilient backdoor attacks in deep learning.</p><hr><h3>Neural Contractive Dynamical Systems</h3>
<p><a href='https://openreview.net/forum?id=iAYIRHOYy8'>https://openreview.net/forum?id=iAYIRHOYy8</a></p>
<p><b>Compressor summary</b>: The text introduces a novel approach to learn stable neural networks for autonomous robot control using a variational autoencoder that encodes high-dimensional and full-pose dynamics and beats the current methods.</p><hr><h3>Negatively Correlated Ensemble Reinforcement Learning for Online Diverse Game Level Generation</h3>
<p><a href='https://openreview.net/forum?id=iAW2EQXfwb'>https://openreview.net/forum?id=iAW2EQXfwb</a></p>
<p><b>Compressor summary</b>: The paper proposes an ensemble reinforcement learning method to generate diverse game levels with a selector model and policy regularization.</p><hr><h3>Feature emergence via margin maximization: case studies in algebraic tasks</h3>
<p><a href='https://openreview.net/forum?id=i9wDX850jR'>https://openreview.net/forum?id=i9wDX850jR</a></p>
<p><b>Compressor summary</b>: The paper investigates why neural networks choose certain strategies for algebraic tasks and shows that margin maximization and Fourier features are key factors in their learning process.</p><hr><h3>Asymptotically Free Sketched Ridge Ensembles: Risks, Cross-Validation, and Tuning</h3>
<p><a href='https://openreview.net/forum?id=i9Vs5NGDpk'>https://openreview.net/forum?id=i9Vs5NGDpk</a></p>
<p><b>Compressor summary</b>: The paper applies random matrix theory to optimize the tuning of regularization and sketching parameters in ridge regression ensembles, enabling efficient prediction risk estimation and construction of prediction intervals.</p><hr><h3>Scaling Laws for Sparsely-Connected Foundation Models</h3>
<p><a href='https://openreview.net/forum?id=i9K2ZWkYIP'>https://openreview.net/forum?id=i9K2ZWkYIP</a></p>
<p><b>Compressor summary</b>: The paper investigates how reducing parameters in Transformers affects their performance on large datasets and identifies optimal sparsity levels that depend on data size and other factors.</p><hr><h3>Locality Sensitive Sparse Encoding for Learning World Models Online</h3>
<p><a href='https://openreview.net/forum?id=i8PjQT3Uig'>https://openreview.net/forum?id=i8PjQT3Uig</a></p>
<p><b>Compressor summary</b>: The paper proposes a linear regression model with nonlinear random features and a sparse locality sensitive encoding for efficient incremental updates in model-based reinforcement learning, achieving follow-the-leader behavior and outperforming neural networks in continuous control tasks.</p><hr><h3>EventRPG: Event Data Augmentation with Relevance Propagation Guidance</h3>
<p><a href='https://openreview.net/forum?id=i7LCsDMcZ4'>https://openreview.net/forum?id=i7LCsDMcZ4</a></p>
<p><b>Compressor summary</b>: The paper introduces two methods to generate saliency maps for spiking neural networks, improving object and action recognition in event-based vision sensors.</p><hr><h3>On Bias-Variance Alignment in Deep Models</h3>
<p><a href='https://openreview.net/forum?id=i2Phucne30'>https://openreview.net/forum?id=i2Phucne30</a></p>
<p><b>Compressor summary</b>: The paper shows that deep learning models align bias and variance at a sample level, and provides theoretical explanations for this phenomenon based on calibration and neural collapse theories.</p><hr><h3>Strategic Preys Make Acute Predators: Enhancing Camouflaged Object Detectors by Generating Camouflaged Objects</h3>
<p><a href='https://openreview.net/forum?id=hywpSoHwgX'>https://openreview.net/forum?id=hywpSoHwgX</a></p>
<p><b>Compressor summary</b>: The paper proposes a new camouflaged object detection method that uses adversarial training to generate more challenging camouflage and a novel detection algorithm that excavates internal coherence and removes false predictions.</p><hr><h3>Graph Parsing Networks</h3>
<p><a href='https://openreview.net/forum?id=hv3SklibkL'>https://openreview.net/forum?id=hv3SklibkL</a></p>
<p><b>Compressor summary</b>: The Graph Parsing Network (GPN) adapts to each graph's structure to learn an efficient and informative pooling structure for graph and node classification tasks.</p><hr><h3>Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression</h3>
<p><a href='https://openreview.net/forum?id=huGECz8dPp'>https://openreview.net/forum?id=huGECz8dPp</a></p>
<p><b>Compressor summary</b>: The paper introduces a framework to analyze deep neural networks' training process using the Information Bottleneck principle and compresses high-dimensional vectors for mutual information estimation.</p><hr><h3>InstructDET: Diversifying Referring Object Detection with Generalized Instructions</h3>
<p><a href='https://openreview.net/forum?id=hss35aoQ1Y'>https://openreview.net/forum?id=hss35aoQ1Y</a></p>
<p><b>Compressor summary</b>: InstructDET is a method for referring object detection that uses diverse user instructions and foundation models to train on more data, improving performance on standard REC datasets and object detection tasks.</p><hr><h3>Causally Aligned Curriculum Learning</h3>
<p><a href='https://openreview.net/forum?id=hp4yOjhwTs'>https://openreview.net/forum?id=hp4yOjhwTs</a></p>
<p><b>Compressor summary</b>: The paper proposes a curriculum learning method for Reinforcement Learning that uses causal reasoning to ensure optimal decision rules are shared across related tasks and the target task.</p><hr><h3>Conformal Inductive Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=homn1jOKI5'>https://openreview.net/forum?id=homn1jOKI5</a></p>
<p><b>Compressor summary</b>: Conformal prediction can create uncertainty sets for transductive node-classification models without sacrificing coverage or efficiency, and it works for both node-exchangeable and edge-exchangeable graphs.</p><hr><h3>Finetuning Text-to-Image Diffusion Models for Fairness</h3>
<p><a href='https://openreview.net/forum?id=hnrB5YHoYu'>https://openreview.net/forum?id=hnrB5YHoYu</a></p>
<p><b>Compressor summary</b>: The authors present a finetuning method for text-to-image diffusion models that aligns generated images with a target distribution, reducing various biases and allowing flexibility in controlling different attributes.</p><hr><h3>Polynormer: Polynomial-Expressive Graph Transformer in Linear Time</h3>
<p><a href='https://openreview.net/forum?id=hmv1LpNfXa'>https://openreview.net/forum?id=hmv1LpNfXa</a></p>
<p><b>Compressor summary</b>: Polynormer is a polynomial-expressive graph transformer model with linear complexity that achieves high performance on various graph datasets.</p><hr><h3>A Differentially Private Clustering Algorithm for Well-Clustered Graphs</h3>
<p><a href='https://openreview.net/forum?id=hkSjjs4o5d'>https://openreview.net/forum?id=hkSjjs4o5d</a></p>
<p><b>Compressor summary</b>: The authors present a differentially private algorithm for clustering well-clustered graphs, which performs nearly as well as non-private algorithms and outperforms pure $\epsilon$-DP algorithms.</p><hr><h3>Better Neural PDE Solvers Through Data-Free Mesh Movers</h3>
<p><a href='https://openreview.net/forum?id=hj9ZuNimRl'>https://openreview.net/forum?id=hj9ZuNimRl</a></p>
<p><b>Compressor summary</b>: The paper proposes a neural PDE solver with a novel data-free mesh adapter that can create adaptive and dynamic meshes for physical system modeling without optimal mesh data, improving accuracy and efficiency.</p><hr><h3>A path-norm toolkit for modern networks: consequences, promises and challenges</h3>
<p><a href='https://openreview.net/forum?id=hiHZVUIYik'>https://openreview.net/forum?id=hiHZVUIYik</a></p>
<p><b>Compressor summary</b>: The paper presents a new toolkit for analyzing general neural networks with improved generalization bounds and ease of computation.</p><hr><h3>GPAvatar: Generalizable and Precise Head Avatar from Image(s)</h3>
<p><a href='https://openreview.net/forum?id=hgehGq2bDv'>https://openreview.net/forum?id=hgehGq2bDv</a></p>
<p><b>Compressor summary</b>: GPAvatar is a framework that reconstructs 3D head avatars from images using a dynamic point-based expression field and Multi Tri-planes Attention module for precise expression control and multi-view consistency.</p><hr><h3>Generating Images in Context with Multimodal Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=he6mX9LTyE'>https://openreview.net/forum?id=he6mX9LTyE</a></p>
<p><b>Compressor summary</b>: Kosmos-G is a model that uses MLLMs and CLIP to generate images from vision-language inputs, enabling zero-shot multi-entity subject-driven generation without modifying the image decoder.</p><hr><h3>FedP3: Federated Personalized and Privacy-friendly Network Pruning under Model Heterogeneity</h3>
<p><a href='https://openreview.net/forum?id=hbHwZYqk9T'>https://openreview.net/forum?id=hbHwZYqk9T</a></p>
<p><b>Compressor summary</b>: FedP3 is a flexible federated learning framework that addresses client-side model heterogeneity by privacy-friendly network pruning.</p><hr><h3>Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World</h3>
<p><a href='https://openreview.net/forum?id=hWS4MueyzC'>https://openreview.net/forum?id=hWS4MueyzC</a></p>
<p><b>Compressor summary</b>: The text introduces a new benchmark for testing few-shot visual reasoning called Bongard-OpenWorld, which uses real-world images and open-world concepts to challenge current algorithms and human performance.</p><hr><h3>Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</h3>
<p><a href='https://openreview.net/forum?id=hTEGyKf0dZ'>https://openreview.net/forum?id=hTEGyKf0dZ</a></p>
<p><b>Compressor summary</b>: The text discusses how fine-tuning large language models can compromise their safety alignment, even with small numbers of adversarial examples or common datasets, and calls for better safety protocols.</p><hr><h3>Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks</h3>
<p><a href='https://openreview.net/forum?id=hQVCCxQrYN'>https://openreview.net/forum?id=hQVCCxQrYN</a></p>
<p><b>Compressor summary</b>: Plan-Seq-Learn (PSL) is a new method that uses large language models and motion planning to help robots learn high-level policies and fine-grained control actions from raw visual input, achieving over 80% success on various challenging robotics tasks.</p><hr><h3>Langevin Monte Carlo for strongly log-concave distributions: Randomized midpoint revisited</h3>
<p><a href='https://openreview.net/forum?id=hOxgrGM63n'>https://openreview.net/forum?id=hOxgrGM63n</a></p>
<p><b>Compressor summary</b>: The paper proposes an improved method to sample from smooth strongly log-concave distributions using randomized midpoint discretization, and provides new guarantees for kinetic Langevin process with Euler discretization.</p><hr><h3>Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates</h3>
<p><a href='https://openreview.net/forum?id=hORCalGn3Z'>https://openreview.net/forum?id=hORCalGn3Z</a></p>
<p><b>Compressor summary</b>: The paper proposes a unified framework for designing and analyzing efficient local training methods for distributed variational inequality problems in machine learning with improved communication complexity.</p><hr><h3>Piecewise Linear Parametrization of Policies: Towards Interpretable Deep Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=hOMVq57Ce0'>https://openreview.net/forum?id=hOMVq57Ce0</a></p>
<p><b>Compressor summary</b>: The paper proposes HyperCombinator, a piecewise-linear neural architecture for interpretable policies in reinforcement learning, and evaluates it in control and navigation tasks.</p><hr><h3>Lemur: Harmonizing Natural Language and Code for Language Agents</h3>
<p><a href='https://openreview.net/forum?id=hNhwSmtXRh'>https://openreview.net/forum?id=hNhwSmtXRh</a></p>
<p><b>Compressor summary</b>: Lemur and Lemur-Chat are versatile language agents that excel in natural language and coding tasks, bridging the gap between humans and machines.</p><hr><h3>Contrastive Learning is Spectral Clustering on Similarity Graph</h3>
<p><a href='https://openreview.net/forum?id=hLZQTFGToA'>https://openreview.net/forum?id=hLZQTFGToA</a></p>
<p><b>Compressor summary</b>: This paper analyzes contrastive learning's equivalence to spectral clustering and introduces a new loss function called Kernel-InfoNCE for improved performance in CLIP model.</p><hr><h3>True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=hILVmJ4Uvu'>https://openreview.net/forum?id=hILVmJ4Uvu</a></p>
<p><b>Compressor summary</b>: TWOSOME is a framework that uses large language models for efficient decision-making in embodied environments via reinforcement learning, improving sample efficiency and generalization.</p><hr><h3>Adaptive Window Pruning for Efficient Local Motion Deblurring</h3>
<p><a href='https://openreview.net/forum?id=hI18CDyadM'>https://openreview.net/forum?id=hI18CDyadM</a></p>
<p><b>Compressor summary</b>: The paper proposes a local motion deblurring method that adaptively restores high-resolution images using a vision Transformer with adaptive window pruning, achieving better performance and efficiency than existing methods.</p><hr><h3>Label-free Node Classification on Graphs with Large Language Models (LLMs)</h3>
<p><a href='https://openreview.net/forum?id=hESD2NJFg8'>https://openreview.net/forum?id=hESD2NJFg8</a></p>
<p><b>Compressor summary</b>: The text introduces LLM-GNN, a label-free node classification pipeline that combines GNNs and LLMs to annotate nodes efficiently and improve performance.</p><hr><h3>P$^2$OT: Progressive Partial Optimal Transport for Deep Imbalanced Clustering</h3>
<p><a href='https://openreview.net/forum?id=hD3sGVqPsr'>https://openreview.net/forum?id=hD3sGVqPsr</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework for deep imbalanced clustering that uses pseudo-label generation based on optimal transport problems to learn from data with class distribution imbalance.</p><hr><h3>PolyVoice: Language Models for Speech to Speech Translation</h3>
<p><a href='https://openreview.net/forum?id=hCrFG9cyuC'>https://openreview.net/forum?id=hCrFG9cyuC</a></p>
<p><b>Compressor summary</b>: PolyVoice is a language model-based framework for speech-to-speech translation that uses three decoder-only language models to improve translation and audio quality, outperforming the state-of-the-art encoder-decoder model.</p><hr><h3>PhyloGFN: Phylogenetic inference with generative flow networks</h3>
<p><a href='https://openreview.net/forum?id=hB7SlfEmze'>https://openreview.net/forum?id=hB7SlfEmze</a></p>
<p><b>Compressor summary</b>: The paper presents a new method, PhyloGFN, that uses generative flow networks to sample from the complex space of phylogenetic trees and evaluate its performance on real data.</p><hr><h3>Greedy Sequential Execution: Solving Homogeneous and Heterogeneous Cooperative Tasks with a Unified Framework</h3>
<p><a href='https://openreview.net/forum?id=hB2hXtxIPH'>https://openreview.net/forum?id=hB2hXtxIPH</a></p>
<p><b>Compressor summary</b>: Greedy Sequential Execution (GSE) is a new approach to handle cooperative agent tasks by learning optimal policies for both homogeneous and heterogeneous scenarios using individual utility functions and greedy marginal contributions.</p><hr><h3>FedWon: Triumphing Multi-domain Federated Learning Without Normalization</h3>
<p><a href='https://openreview.net/forum?id=hAYHmV1gM8'>https://openreview.net/forum?id=hAYHmV1gM8</a></p>
<p><b>Compressor summary</b>: FedWon is a novel method for federated learning that improves accuracy by eliminating normalization layers and reparameterizing convolution layers, especially in multi-domain scenarios with diverse feature distributions.</p><hr><h3>Multi-Source Diffusion Models for Simultaneous Music Generation and Separation</h3>
<p><a href='https://openreview.net/forum?id=h922Qhkmx1'>https://openreview.net/forum?id=h922Qhkmx1</a></p>
<p><b>Compressor summary</b>: The authors propose a generative model for music creation and separation using diffusion and Dirac likelihood methods, achieving competitive results on a standard dataset.</p><hr><h3>Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization</h3>
<p><a href='https://openreview.net/forum?id=h8GeqOxtd4'>https://openreview.net/forum?id=h8GeqOxtd4</a></p>
<p><b>Compressor summary</b>: The paper proposes a mathematical framework to analyze and improve gradient-based learning of the score function in diffusion models using a novel neural network architecture and kernel regression techniques.</p><hr><h3>Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks</h3>
<p><a href='https://openreview.net/forum?id=h7nOCxFsPg'>https://openreview.net/forum?id=h7nOCxFsPg</a></p>
<p><b>Compressor summary</b>: GSPNs are a new probabilistic framework for learning graphs that can efficiently answer probabilistic queries, outperforming popular neural models in various scenarios.</p><hr><h3>Active Retrosynthetic Planning Aware of Route Quality</h3>
<p><a href='https://openreview.net/forum?id=h7DGnWGeos'>https://openreview.net/forum?id=h7DGnWGeos</a></p>
<p><b>Compressor summary</b>: ARP is a framework that balances query costs and route quality evaluation for retrosynthetic planning, outperforming existing approaches by 6.2% in route quality and reducing query cost by 12.8%.</p><hr><h3>VQGraph: Rethinking Graph Representation Space for Bridging GNNs and MLPs</h3>
<p><a href='https://openreview.net/forum?id=h6Tz85BqRI'>https://openreview.net/forum?id=h6Tz85BqRI</a></p>
<p><b>Compressor summary</b>: VQGraph uses a structure-aware tokenizer to learn a new graph representation space that enables knowledge transfer from GNN to MLP for efficient graph data analysis.</p><hr><h3>Self-Supervised Dataset Distillation for Transfer Learning</h3>
<p><a href='https://openreview.net/forum?id=h57gkDO2Yg'>https://openreview.net/forum?id=h57gkDO2Yg</a></p>
<p><b>Compressor summary</b>: The paper proposes a new way to distill an unlabeled dataset into small synthetic samples for efficient self-supervised learning, addressing biases in gradient estimation and minimizing randomness.</p><hr><h3>Improved sampling via learned diffusions</h3>
<p><a href='https://openreview.net/forum?id=h4pNROsO06'>https://openreview.net/forum?id=h4pNROsO06</a></p>
<p><b>Compressor summary</b>: The paper proposes a unifying framework for deep learning-based methods to sample from unnormalized target densities using the Schrdinger bridge problem, and introduces a new loss function called log-variance loss that improves performance.</p><hr><h3>DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation</h3>
<p><a href='https://openreview.net/forum?id=h1sFUGlI09'>https://openreview.net/forum?id=h1sFUGlI09</a></p>
<p><b>Compressor summary</b>: DFormer is a novel framework that pretrains the backbone using image-depth pairs, improves RGB-D encoding with a sequence of tailored blocks, and achieves state-of-the-art performance on RGB-D segmentation tasks with less computational cost.</p><hr><h3>Understanding Certified Training with Interval Bound Propagation</h3>
<p><a href='https://openreview.net/forum?id=h05eQniJsQ'>https://openreview.net/forum?id=h05eQniJsQ</a></p>
<p><b>Compressor summary</b>: The authors investigate why interval bound propagation (IBP), an imprecise but effective method for training robust neural networks, works so well and find that it provides strong regularization and exact bounds in certain conditions.</p><hr><h3>Language Model Beats Diffusion - Tokenizer is key to visual generation</h3>
<p><a href='https://openreview.net/forum?id=gzqrANCF4g'>https://openreview.net/forum?id=gzqrANCF4g</a></p>
<p><b>Compressor summary</b>: The paper introduces a new video tokenizer that enables large language models to outperform diffusion models in image and video generation tasks, as well as in video compression and action recognition.</p><hr><h3>Path Choice Matters for Clear Attributions in Path Methods</h3>
<p><a href='https://openreview.net/forum?id=gzYgsZgwXa'>https://openreview.net/forum?id=gzYgsZgwXa</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method, SAMP, to generate more rigorous and clear interpretations of deep neural networks by finding near-optimal manipulation paths based on the Concentration Principle.</p><hr><h3>Automatic Functional Differentiation in JAX</h3>
<p><a href='https://openreview.net/forum?id=gzT61ziSCu'>https://openreview.net/forum?id=gzT61ziSCu</a></p>
<p><b>Compressor summary</b>: The paper presents a method to differentiate higher-order functions using JAX, enabling automatic differentiation with a simple syntax and functional gradients as callable python functions.</p><hr><h3>Learning Polynomial Problems with $SL(2, \mathbb{R})$-Equivariance</h3>
<p><a href='https://openreview.net/forum?id=gyfXuRfxW2'>https://openreview.net/forum?id=gyfXuRfxW2</a></p>
<p><b>Compressor summary</b>: Neural networks can solve positivity optimization and certification problems for polynomials faster and accurately while leveraging the $SL(2,\mathbb{R})$ group equivariance.</p><hr><h3>Diffeomorphic Mesh Deformation via Efficient Optimal Transport for Cortical Surface Reconstruction</h3>
<p><a href='https://openreview.net/forum?id=gxhRR8vUQb'>https://openreview.net/forum?id=gxhRR8vUQb</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel metric for learning mesh deformation using sliced Wasserstein distance on meshes represented as probability measures, which overcomes limitations of the set-based approach and provides a fast statistical rate for approximating surfaces.</p><hr><h3>ZeRO++: Extremely Efficient Collective Communication for Large Model Training</h3>
<p><a href='https://openreview.net/forum?id=gx2BT0a9MQ'>https://openreview.net/forum?id=gx2BT0a9MQ</a></p>
<p><b>Compressor summary</b>: ZeRO++ improves the efficiency and scalability of large language models by reducing communication volume and overheads in low-bandwidth settings.</p><hr><h3>Learning Large DAGs is Harder than you Think: Many Losses are Minimal for the Wrong DAG</h3>
<p><a href='https://openreview.net/forum?id=gwbQ2YwLhD'>https://openreview.net/forum?id=gwbQ2YwLhD</a></p>
<p><b>Compressor summary</b>: The scale of data affects the performance of structure learning algorithms, especially in non-linear cases, which can have significant implications in fields like medicine and biology.</p><hr><h3>Dual Associated Encoder for Face Restoration</h3>
<p><a href='https://openreview.net/forum?id=gwDuW7Ok5f'>https://openreview.net/forum?id=gwDuW7Ok5f</a></p>
<p><b>Compressor summary</b>: DAEFR is a novel framework that uses a dual-branch approach to restore facial details from low-quality images by extracting domain-specific information and promoting synergy between the branches.</p><hr><h3>CausalLM is not optimal for in-context learning</h3>
<p><a href='https://openreview.net/forum?id=guRNebwZBb'>https://openreview.net/forum?id=guRNebwZBb</a></p>
<p><b>Compressor summary</b>: The paper analyzes how prefix language models (prefixLM) outperform causal language models (causalLM) in in-context learning, showing that prefixLM converges to the optimal solution while causalLM does not.</p><hr><h3>Generative Judge for Evaluating Alignment</h3>
<p><a href='https://openreview.net/forum?id=gtkFw6sZGS'>https://openreview.net/forum?id=gtkFw6sZGS</a></p>
<p><b>Compressor summary</b>: The paper introduces **Auto-J**, a generative judge model with 13B parameters that evaluates the performance of Large Language Models on tasks aligned with human needs, such as brainstorming and email writing.</p><hr><h3>Efficient Subgraph GNNs by Learning Effective Selection Policies</h3>
<p><a href='https://openreview.net/forum?id=gppLqZLQeY'>https://openreview.net/forum?id=gppLqZLQeY</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method called Policy-Learn that learns how to select a small subset of subgraphs for graph neural networks, reducing computational complexity and improving performance.</p><hr><h3>SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation</h3>
<p><a href='https://openreview.net/forum?id=gn0mIhQGNM'>https://openreview.net/forum?id=gn0mIhQGNM</a></p>
<p><b>Compressor summary</b>: SalUn is a novel machine unlearning method that improves accuracy, stability, and cross-domain applicability by focusing on specific model weights rather than the entire model.</p><hr><h3>Can LLMs Keep a Secret? Testing  Privacy  Implications of Language Models  via Contextual Integrity Theory</h3>
<p><a href='https://openreview.net/forum?id=gmg7t8b4s0'>https://openreview.net/forum?id=gmg7t8b4s0</a></p>
<p><b>Compressor summary</b>: The paper proposes CONFAIDE, a benchmark to test privacy reasoning in large language models, and shows that current models reveal private information in inappropriate contexts.</p><hr><h3>VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks</h3>
<p><a href='https://openreview.net/forum?id=glwwbaeKm2'>https://openreview.net/forum?id=glwwbaeKm2</a></p>
<p><b>Compressor summary</b>: Vertical Federated Learning (VFL) benefits from new evaluation metrics and datasets that consider feature importance and correlation, improving algorithm performance assessment.</p><hr><h3>Confronting Reward Model Overoptimization with Constrained RLHF</h3>
<p><a href='https://openreview.net/forum?id=gkfUvn0fLU'>https://openreview.net/forum?id=gkfUvn0fLU</a></p>
<p><b>Compressor summary</b>: The paper studies overoptimization in composite reward models for language quality and proposes a constrained reinforcement learning approach to balance component models and improve evaluation performance.</p><hr><h3>DyVal: Graph-informed Dynamic Evaluation of Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=gjfOL9z5Xr'>https://openreview.net/forum?id=gjfOL9z5Xr</a></p>
<p><b>Compressor summary</b>: DyVal is a dynamic evaluation protocol that uses graphs to generate challenging reasoning tasks for large language models, revealing their limitations and potential for improvement.</p><hr><h3>Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs</h3>
<p><a href='https://openreview.net/forum?id=gjeQKFxFpZ'>https://openreview.net/forum?id=gjeQKFxFpZ</a></p>
<p><b>Compressor summary</b>: The text presents a systematic framework for black-box uncertainty estimation in LLMs using prompting, sampling, and aggregation methods. It evaluates the framework on calibration and failure prediction tasks and identifies key insights and challenges.</p><hr><h3>PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=gjXor87Xfy'>https://openreview.net/forum?id=gjXor87Xfy</a></p>
<p><b>Compressor summary</b>: Memory-based Dynamic Graph Neural Networks (MDGNNs) use memory to capture long-term dependencies, but struggle with temporal discontinuity in large temporal batches; the proposed PRES method mitigates this issue and enables faster training without sacrificing performance.</p><hr><h3>FedImpro: Measuring and Improving Client Update in Federated Learning</h3>
<p><a href='https://openreview.net/forum?id=giU9fYGTND'>https://openreview.net/forum?id=giU9fYGTND</a></p>
<p><b>Compressor summary</b>: This paper proposes FedImpro, a method to reduce client drift in federated learning by constructing similar conditional distributions for local training, improving generalization and gradient similarity.</p><hr><h3>Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models</h3>
<p><a href='https://openreview.net/forum?id=gfFVATffPd'>https://openreview.net/forum?id=gfFVATffPd</a></p>
<p><b>Compressor summary</b>: The paper studies how Transformer-based LLMs handle factual constraints when generating text, and proposes SAT Probe, a method that can predict errors and improve reliability.</p><hr><h3>OmniControl: Control Any Joint at Any Time for Human Motion Generation</h3>
<p><a href='https://openreview.net/forum?id=gd0lAEtWso'>https://openreview.net/forum?id=gd0lAEtWso</a></p>
<p><b>Compressor summary</b>: OmniControl is a novel method for generating realistic human motion with flexible spatial control signals over different joints using diffusion-based models and analytic guidance.</p><hr><h3>Feature Collapse</h3>
<p><a href='https://openreview.net/forum?id=gctmyMiPHH'>https://openreview.net/forum?id=gctmyMiPHH</a></p>
<p><b>Compressor summary</b>: Feature collapse is a phenomenon where similar learning task roles get similar representations, improving generalization and interpretability.</p><hr><h3>Logical Languages Accepted by Transformer Encoders with Hard Attention</h3>
<p><a href='https://openreview.net/forum?id=gbrHZq07mq'>https://openreview.net/forum?id=gbrHZq07mq</a></p>
<p><b>Compressor summary</b>: The text studies how two types of self-attention mechanisms in transformer encoders recognize different formal languages, and shows their limitations and capabilities.</p><hr><h3>Wrstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=gU58d5QeGv'>https://openreview.net/forum?id=gU58d5QeGv</a></p>
<p><b>Compressor summary</b>: Wrstchen is a text-to-image synthesis model that uses a compact semantic image representation to guide the diffusion process, achieving competitive performance with much lower cost and faster inference than existing models.</p><hr><h3>Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions</h3>
<p><a href='https://openreview.net/forum?id=gT5hALch9z'>https://openreview.net/forum?id=gT5hALch9z</a></p>
<p><b>Compressor summary</b>: The paper discusses the importance of safety in instruction-tuned language models and shows that adding a small number of safety examples can improve their safety without compromising their capabilities or helpfulness.</p><hr><h3>FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores</h3>
<p><a href='https://openreview.net/forum?id=gPKTTAfYBp'>https://openreview.net/forum?id=gPKTTAfYBp</a></p>
<p><b>Compressor summary</b>: FlashFFTConv optimizes FFT convolution for long sequences, speeding up reasoning tasks and enabling larger models with the same compute budget.</p><hr><h3>A Private Watermark for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=gMLQwKDY3N'>https://openreview.net/forum?id=gMLQwKDY3N</a></p>
<p><b>Compressor summary</b>: The paper proposes a private watermarking algorithm for large language models that uses different neural networks for generating and detecting watermarks, enhancing security and accuracy while minimizing parameters.</p><hr><h3>LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models</h3>
<p><a href='https://openreview.net/forum?id=gLARhFLE0F'>https://openreview.net/forum?id=gLARhFLE0F</a></p>
<p><b>Compressor summary</b>: The paper introduces LUT-GEMM, a kernel for quantized matrix multiplication that eliminates the resource-intensive dequantization process and reduces computational costs, achieving significant acceleration in NLP model generation.</p><hr><h3>A Simple Romance Between Multi-Exit Vision Transformer and Token Reduction</h3>
<p><a href='https://openreview.net/forum?id=gJeYtRuguR'>https://openreview.net/forum?id=gJeYtRuguR</a></p>
<p><b>Compressor summary</b>: METR introduces multi-exit loss to improve the reliability of ViTs' attention scores for token reduction, making them more efficient and effective.</p><hr><h3>Neural Optimal Transport with General Cost Functionals</h3>
<p><a href='https://openreview.net/forum?id=gIiz7tBtYZ'>https://openreview.net/forum?id=gIiz7tBtYZ</a></p>
<p><b>Compressor summary</b>: The paper presents a new neural network method for computing OT plans that can use extra information, work with diverse costs, and handle high-dimensional data, while providing error guarantees and an example of class-preserving mapping.</p><hr><h3>Modulated Phase Diffusor: Content-Oriented Feature Synthesis for Detecting Unknown Objects</h3>
<p><a href='https://openreview.net/forum?id=gHAr7ZA1OL'>https://openreview.net/forum?id=gHAr7ZA1OL</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to generate out-of-distribution data for training object detectors using phase information from in-distribution data and two branches for content manipulation.</p><hr><h3>IMPUS: Image Morphing with Perceptually-Uniform Sampling Using Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=gG38EBe2S8'>https://openreview.net/forum?id=gG38EBe2S8</a></p>
<p><b>Compressor summary</b>: The paper presents a new method for image morphing called IMPUS, which uses diffusion-based models, latent space interpolation, and perceptually-uniform sampling to produce realistic and smooth results.</p><hr><h3>Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View</h3>
<p><a href='https://openreview.net/forum?id=gFR4QwK53h'>https://openreview.net/forum?id=gFR4QwK53h</a></p>
<p><b>Compressor summary</b>: The authors propose a causal graphical model to characterize dropout error in single-cell RNA sequencing data and show that test-wise deletion can be used to integrate existing structure learning methods for gene regulatory network inference without introducing spurious relations.</p><hr><h3>Efficient Backpropagation with Variance Controlled Adaptive Sampling</h3>
<p><a href='https://openreview.net/forum?id=gEwKAZZmSw'>https://openreview.net/forum?id=gEwKAZZmSw</a></p>
<p><b>Compressor summary</b>: The paper introduces a method called VCAS that reduces computational load in neural network training by sampling unimportant computations, while controlling additional variance to maintain accuracy.</p><hr><h3>BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation</h3>
<p><a href='https://openreview.net/forum?id=gC6JTEU3jl'>https://openreview.net/forum?id=gC6JTEU3jl</a></p>
<p><b>Compressor summary</b>: BESA is a novel technique that prunes large language models by targeting error reduction at the transformer block level and allocating sparsity differently, resulting in less performance degradation and faster computation time.</p><hr><h3>Evaluating the Zero-shot Robustness of Instruction-tuned Language Models</h3>
<p><a href='https://openreview.net/forum?id=g9diuvxN6D'>https://openreview.net/forum?id=g9diuvxN6D</a></p>
<p><b>Compressor summary</b>: The paper studies how sensitive instruction-tuned language models are to different phrasings of instructions and proposes a method to improve their robustness using soft prompt embedding parameters.</p><hr><h3>Adaptive Rational Activations to Boost Deep Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=g90ysX1sVs'>https://openreview.net/forum?id=g90ysX1sVs</a></p>
<p><b>Compressor summary</b>: The authors propose using adaptable activation functions in reinforcement learning to enhance neural plasticity and improve performance in changing environments, such as Atari games.</p><hr><h3>On the Parameterization of Second-Order Optimization Effective towards the Infinite Width</h3>
<p><a href='https://openreview.net/forum?id=g8sGBSQjYk'>https://openreview.net/forum?id=g8sGBSQjYk</a></p>
<p><b>Compressor summary</b>: The paper proposes a new parameterization for second-order optimization that improves feature learning in larger deep neural networks and allows sharing hyperparameters across models of different sizes.</p><hr><h3>Flow Matching on General Geometries</h3>
<p><a href='https://openreview.net/forum?id=g7ohDlTITL'>https://openreview.net/forum?id=g7ohDlTITL</a></p>
<p><b>Compressor summary</b>: Riemannian Flow Matching (RFM) is a new method for training continuous normalizing flows on manifolds that works without simulation, divergence computation, and uses a simple premetric for target vector fields.</p><hr><h3>Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification</h3>
<p><a href='https://openreview.net/forum?id=g6rZtxaXRm'>https://openreview.net/forum?id=g6rZtxaXRm</a></p>
<p><b>Compressor summary</b>: FuDD improves image classification by generating class descriptions that better differentiate between ambiguous classes using a large language model.</p><hr><h3>InfoCon: Concept Discovery with Generative and Discriminative Informativeness</h3>
<p><a href='https://openreview.net/forum?id=g6eCbercEc'>https://openreview.net/forum?id=g6eCbercEc</a></p>
<p><b>Compressor summary</b>: The paper presents a method for autonomously discovering manipulation concepts from unlabeled robotic demonstrations using a trainable codebook that encodes sub-trajectories based on their informativeness.</p><hr><h3>A Progressive Training Framework for Spiking Neural Networks with Learnable Multi-hierarchical Model</h3>
<p><a href='https://openreview.net/forum?id=g52tgL8jy6'>https://openreview.net/forum?id=g52tgL8jy6</a></p>
<p><b>Compressor summary</b>: The paper introduces a new spiking neural network model (LM-H) that improves energy efficiency and information processing by dynamically adjusting membrane-related factors, and proposes a novel training algorithm and framework for it.</p><hr><h3>Neural Active Learning Beyond Bandits</h3>
<p><a href='https://openreview.net/forum?id=g1S72T3FGc'>https://openreview.net/forum?id=g1S72T3FGc</a></p>
<p><b>Compressor summary</b>: The paper proposes new neural network algorithms for active learning that reduce the impact of the number of classes on performance and cost, and shows their advantages in theory and practice.</p><hr><h3>Adaptive Federated Learning with Auto-Tuned Clients</h3>
<p><a href='https://openreview.net/forum?id=g0mlwqs8pi'>https://openreview.net/forum?id=g0mlwqs8pi</a></p>
<p><b>Compressor summary</b>: $\Delta$-SGD is a simple method for adjusting step sizes in federated learning that adapts to local smoothness, improving performance in different scenarios.</p><hr><h3>Global Optimality for Non-linear Constrained Restoration Problems via Invexity</h3>
<p><a href='https://openreview.net/forum?id=fyTPWfXtcc'>https://openreview.net/forum?id=fyTPWfXtcc</a></p>
<p><b>Compressor summary</b>: The paper introduces invex constrained optimization theory to ensure global optima in non-convex constrained inverse problems while maintaining reconstruction quality.</p><hr><h3>Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding</h3>
<p><a href='https://openreview.net/forum?id=fxQiecl9HB'>https://openreview.net/forum?id=fxQiecl9HB</a></p>
<p><b>Compressor summary</b>: Crystalformer is a simple transformer-based encoder architecture that predicts physical properties of materials from their crystal structures more accurately than existing models with fewer parameters and better interpretability.</p><hr><h3>Improving Generalization of Alignment with Human Preferences through Group Invariant Learning</h3>
<p><a href='https://openreview.net/forum?id=fwCoLe3TAX'>https://openreview.net/forum?id=fwCoLe3TAX</a></p>
<p><b>Compressor summary</b>: The authors propose a method to improve language model-based AI assistants' performance and generalization across various domains by automatically classifying data into groups, optimizing the policy for challenging groups, and adaptively adjusting exploration space.</p><hr><h3>Improved Probabilistic Image-Text Representations</h3>
<p><a href='https://openreview.net/forum?id=ft1mr3WlGM'>https://openreview.net/forum?id=ft1mr3WlGM</a></p>
<p><b>Compressor summary</b>: The paper proposes a new probabilistic image-text matching method with improved embeddings and optimization techniques that outperforms existing methods on various benchmarks.</p><hr><h3>Can We Evaluate Domain Adaptation Models Without Target-Domain Labels?</h3>
<p><a href='https://openreview.net/forum?id=fszrlQ2DuP'>https://openreview.net/forum?id=fszrlQ2DuP</a></p>
<p><b>Compressor summary</b>: The paper introduces Transfer Score, a novel metric for unsupervised evaluation of UDA models, which measures spatial uniformity, transferability, and discriminability. The metric helps to choose the best UDA method, tune hyperparameters, and find optimal checkpoints without target labels.</p><hr><h3>Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game</h3>
<p><a href='https://openreview.net/forum?id=fsW7wJGLBd'>https://openreview.net/forum?id=fsW7wJGLBd</a></p>
<p><b>Compressor summary</b>: Tensor Trust is a dataset with over 100,000 prompt injection attacks and defenses for testing the robustness and security of large language models.</p><hr><h3>Are Human-generated Demonstrations Necessary for In-context Learning?</h3>
<p><a href='https://openreview.net/forum?id=frRDT6EOhg'>https://openreview.net/forum?id=frRDT6EOhg</a></p>
<p><b>Compressor summary</b>: This paper proposes self-contemplation prompting strategy (SEC), which eliminates human-generated demonstrations in In-context Learning, and shows that large language models can perform well with only their own decision making capacity.</p><hr><h3>Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation</h3>
<p><a href='https://openreview.net/forum?id=fq1wNrC2ai'>https://openreview.net/forum?id=fq1wNrC2ai</a></p>
<p><b>Compressor summary</b>: FLOP is a novel algorithmic framework that tackles average-reward Markov decision processes with function approximation using confidence sets and low-switching policy updates, achieving sublinear regret in various complex AMDP models.</p><hr><h3>Circuit Component Reuse Across Tasks in Transformer Language Models</h3>
<p><a href='https://openreview.net/forum?id=fpoAYV6Wsk'>https://openreview.net/forum?id=fpoAYV6Wsk</a></p>
<p><b>Compressor summary</b>: The authors show that insights from mechanistic interpretability can generalize across tasks, demonstrating this by reusing and modifying a circuit for different language tasks with significant improvements and explanations.</p><hr><h3>Efficient Dynamics Modeling in Interactive Environments with Koopman Theory</h3>
<p><a href='https://openreview.net/forum?id=fkrYDQaHOJ'>https://openreview.net/forum?id=fkrYDQaHOJ</a></p>
<p><b>Compressor summary</b>: Using Koopman theory, the authors propose a method to efficiently and accurately model nonlinear dynamics in interactive environments for long-range prediction, planning, and reinforcement learning.</p><hr><h3>Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting</h3>
<p><a href='https://openreview.net/forum?id=fjpfCOV4ru'>https://openreview.net/forum?id=fjpfCOV4ru</a></p>
<p><b>Compressor summary</b>: This paper studies a general class of Markov chains derived from stochastic differential equations with arbitrary noise, inexact drift and diffusion coefficients, and improves existing bounds on their distance to the corresponding differential equations.</p><hr><h3>New Insight of Variance reduce in Zero-Order Hard-Thresholding: Mitigating Gradient Error and Expansivity Contradictions</h3>
<p><a href='https://openreview.net/forum?id=fjf3YenThE'>https://openreview.net/forum?id=fjf3YenThE</a></p>
<p><b>Compressor summary</b>: The paper introduces a new algorithm that improves the performance of hard-thresholding for $\ell_0$ constrained optimization using zeroth-order gradients and variance reduction, and shows its usefulness in different applications.</p><hr><h3>Stochastic Gradient Descent for Gaussian Processes Done Right</h3>
<p><a href='https://openreview.net/forum?id=fj2E5OcLFn'>https://openreview.net/forum?id=fj2E5OcLFn</a></p>
<p><b>Compressor summary</b>: The paper introduces an efficient stochastic dual gradient descent algorithm for optimizing Gaussian process regression and shows its competitive performance against other methods.</p><hr><h3>GAIA: a benchmark for General AI Assistants</h3>
<p><a href='https://openreview.net/forum?id=fibxvahvs3'>https://openreview.net/forum?id=fibxvahvs3</a></p>
<p><b>Compressor summary</b>: GAIA is a benchmark for testing general AI assistants' abilities by asking real-world questions that are simple for humans but challenging for most advanced AIs, aiming to measure their robustness and proficiency in various skills.</p><hr><h3>The Alignment Problem from a Deep Learning Perspective: A Position Paper</h3>
<p><a href='https://openreview.net/forum?id=fh8EYKFKns'>https://openreview.net/forum?id=fh8EYKFKns</a></p>
<p><b>Compressor summary</b>: The paper discusses the challenges of aligning artificial general intelligence (AGI) systems with human interests, as they might learn deceptive or power-seeking behaviors from pretrained deep models.</p><hr><h3>REFACTOR: Learning to Extract Theorems from Proofs</h3>
<p><a href='https://openreview.net/forum?id=fgKjiVrm6u'>https://openreview.net/forum?id=fgKjiVrm6u</a></p>
<p><b>Compressor summary</b>: The paper introduces REFACTOR, a neural network method that learns to extract and use modular theorems from mathematical proofs, improving proof quality and efficiency.</p><hr><h3>From Posterior Sampling to Meaningful Diversity in Image Restoration</h3>
<p><a href='https://openreview.net/forum?id=ff2g30cZxj'>https://openreview.net/forum?id=ff2g30cZxj</a></p>
<p><b>Compressor summary</b>: The paper proposes a way to make image restoration methods produce diverse and meaningful outputs by modifying the original method or adding post-processing steps.</p><hr><h3>Federated Q-Learning: Linear Regret Speedup with Low Communication Cost</h3>
<p><a href='https://openreview.net/forum?id=fe6ANBxcKM'>https://openreview.net/forum?id=fe6ANBxcKM</a></p>
<p><b>Compressor summary</b>: The paper proposes two Q-Learning algorithms for federated reinforcement learning with linear regret speedup and low communication cost, using event-triggered synchronization, novel step size selection, and new concentration inequalities.</p><hr><h3>AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation</h3>
<p><a href='https://openreview.net/forum?id=fcqWJ8JgMR'>https://openreview.net/forum?id=fcqWJ8JgMR</a></p>
<p><b>Compressor summary</b>: AuG-KD is a new method that helps transfer knowledge from large models without access to their training data by aligning student-domain data with teacher domain using uncertainty and generative techniques, improving performance in real-world scenarios.</p><hr><h3>A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=farT6XXntP'>https://openreview.net/forum?id=farT6XXntP</a></p>
<p><b>Compressor summary</b>: This study proposes ALMA, a novel fine-tuning approach for LLMs that improves translation performance without relying on large parallel data sets, outperforming previous models with only 7B or 13B parameters.</p><hr><h3>Consistent Multi-Class Classification from Multiple Unlabeled Datasets</h3>
<p><a href='https://openreview.net/forum?id=fW7DOHDQvF'>https://openreview.net/forum?id=fW7DOHDQvF</a></p>
<p><b>Compressor summary</b>: Weakly supervised learning methods aim to train accurate classifiers from unlabeled data and class priors, with RCM being a new approach that improves risk consistency and purifies supervision information.</p><hr><h3>An interpretable error correction method for enhancing code-to-code translation</h3>
<p><a href='https://openreview.net/forum?id=fVxIEHGnVT'>https://openreview.net/forum?id=fVxIEHGnVT</a></p>
<p><b>Compressor summary</b>: kNN-ECD and kNN-ECS methods improve program translation accuracy by using error correction and interpreting translated codes with k-nearest neighbors.</p><hr><h3>Nougat: Neural Optical Understanding for Academic Documents</h3>
<p><a href='https://openreview.net/forum?id=fUtxNAKpdV'>https://openreview.net/forum?id=fUtxNAKpdV</a></p>
<p><b>Compressor summary</b>: Nougat is a Visual Transformer model that converts scientific PDFs into markup language for better accessibility and understanding of scientific knowledge.</p><hr><h3>Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework</h3>
<p><a href='https://openreview.net/forum?id=fUGhVYPVRM'>https://openreview.net/forum?id=fUGhVYPVRM</a></p>
<p><b>Compressor summary</b>: AWP is a plug-and-play framework that enhances desired properties in CTC-trained models by complementing the CTC loss with an additional loss term that prioritizes alignments according to the property, improving ASR latency and WER on large-scale data.</p><hr><h3>Unifying Feature and Cost Aggregation with Transformers for Dense Correspondence</h3>
<p><a href='https://openreview.net/forum?id=fQHb1uZzl7'>https://openreview.net/forum?id=fQHb1uZzl7</a></p>
<p><b>Compressor summary</b>: The paper presents a Transformer-based network that combines feature and cost aggregation for dense matching tasks using self- and cross-attention layers, leading to improved semantic and geometric matching results.</p><hr><h3>$\mathcal{B}$-Coder: On Value-Based Deep Reinforcement Learning for Program Synthesis</h3>
<p><a href='https://openreview.net/forum?id=fLf589bx1f'>https://openreview.net/forum?id=fLf589bx1f</a></p>
<p><b>Compressor summary</b>: The text describes a new approach to program synthesis using value-based reinforcement learning and large language models, which produces accurate code with minimal reward engineering effort.</p><hr><h3>Convergence of Bayesian Bilevel Optimization</h3>
<p><a href='https://openreview.net/forum?id=fLXpXa7iiz'>https://openreview.net/forum?id=fLXpXa7iiz</a></p>
<p><b>Compressor summary</b>: The paper provides a theoretical guarantee for combining Bayesian optimization with stochastic gradient descent in bilevel frameworks, showing sublinear regret bounds and suggesting optimal configurations for generalization.</p><hr><h3>Magnitude Invariant Parametrizations Improve Hypernetwork Learning</h3>
<p><a href='https://openreview.net/forum?id=fJNnerz6iH'>https://openreview.net/forum?id=fJNnerz6iH</a></p>
<p><b>Compressor summary</b>: The text describes a problem with training hypernetworks that affects their convergence and proposes a solution called Magnitude Invariant Parametrizations (MIP) to improve stability and speed up convergence.</p><hr><h3>CoLiDE: Concomitant Linear DAG Estimation</h3>
<p><a href='https://openreview.net/forum?id=fGAIgO75dG'>https://openreview.net/forum?id=fGAIgO75dG</a></p>
<p><b>Compressor summary</b>: CoLiDE is a new method for learning directed acyclic graphs from data that estimates scale and noise levels, improving performance and stability over existing methods.</p><hr><h3>Near-Optimal Solutions of Constrained Learning Problems</h3>
<p><a href='https://openreview.net/forum?id=fDaLmkdSKU'>https://openreview.net/forum?id=fDaLmkdSKU</a></p>
<p><b>Compressor summary</b>: This paper studies how dual ascent methods for constrained machine learning problems affect the optimality and feasibility of primal iterates, and provides a sub-optimality bound based on the problem's parametrization and objective curvature.</p><hr><h3>Zero-Shot Robustification of Zero-Shot Models</h3>
<p><a href='https://openreview.net/forum?id=fCeUoDr9Tq'>https://openreview.net/forum?id=fCeUoDr9Tq</a></p>
<p><b>Compressor summary</b>: RoboShot improves the robustness and performance of pretrained models in zero-shot tasks by using language models to enhance their embeddings without supervision or fine-tuning.</p><hr><h3>Pre-training LiDAR-based 3D Object Detectors through Colorization</h3>
<p><a href='https://openreview.net/forum?id=fB1iiH9xo7'>https://openreview.net/forum?id=fB1iiH9xo7</a></p>
<p><b>Compressor summary</b>: Grounded Point Colorization (GPC) is a pre-training method that teaches models to colorize LiDAR point clouds, improving 3D object detection for self-driving cars with limited labeled data.</p><hr><h3>Project and Probe: Sample-Efficient Adaptation by Interpolating Orthogonal Features</h3>
<p><a href='https://openreview.net/forum?id=f6CBQYxXvr'>https://openreview.net/forum?id=f6CBQYxXvr</a></p>
<p><b>Compressor summary</b>: Pro$^2$ is a sample-efficient method that learns diverse features and interpolates them to adapt to distribution shifts using a small target dataset.</p><hr><h3>UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling</h3>
<p><a href='https://openreview.net/forum?id=f5H8WGLQm5'>https://openreview.net/forum?id=f5H8WGLQm5</a></p>
<p><b>Compressor summary</b>: UniAdapter is a method to adapt pre-trained vision-language models to various downstream tasks with minimal computational and storage costs, outperforming full fine-tuning in most cases.</p><hr><h3>LiDAR: Sensing Linear Probing Performance in Joint Embedding SSL Architectures</h3>
<p><a href='https://openreview.net/forum?id=f3g5XpL9Kb'>https://openreview.net/forum?id=f3g5XpL9Kb</a></p>
<p><b>Compressor summary</b>: LiDAR is a metric for evaluating joint embedding representations by measuring the rank of the LDA matrix associated with a surrogate self-supervised learning task.</p><hr><h3>Cycle Consistency Driven Object Discovery</h3>
<p><a href='https://openreview.net/forum?id=f1xnBr4WD6'>https://openreview.net/forum?id=f1xnBr4WD6</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve object-centric deep learning models by optimizing the association of objects with distinct slots and applying them to downstream tasks.</p><hr><h3>Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model</h3>
<p><a href='https://openreview.net/forum?id=ezscMer8L0'>https://openreview.net/forum?id=ezscMer8L0</a></p>
<p><b>Compressor summary</b>: Conv-LoRA is a method that improves the performance of the Segment-Anything Model for image segmentation in specialized domains by adding lightweight convolutional parameters.</p><hr><h3>AnyText: Multilingual Visual Text Generation and Editing</h3>
<p><a href='https://openreview.net/forum?id=ezBH9WE9s2'>https://openreview.net/forum?id=ezBH9WE9s2</a></p>
<p><b>Compressor summary</b>: AnyText is a diffusion-based multilingual visual text generation and editing model that produces accurate and coherent text in images, achieving state-of-the-art performance on a new large-scale dataset called AnyWord-3M.</p><hr><h3>Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity</h3>
<p><a href='https://openreview.net/forum?id=ey3GhWXQ97'>https://openreview.net/forum?id=ey3GhWXQ97</a></p>
<p><b>Compressor summary</b>: The paper investigates how adaptivity affects sample-efficiency in reinforcement learning by analyzing lower bounds on the number of batches needed for policy evaluation and best-policy identification under linear function approximation.</p><hr><h3>SweetDreamer: Aligning Geometric Priors in 2D diffusion for Consistent Text-to-3D</h3>
<p><a href='https://openreview.net/forum?id=extpNXo6hB'>https://openreview.net/forum?id=extpNXo6hB</a></p>
<p><b>Compressor summary</b>: The text proposes a method to improve text-to-3D generation by aligning 2D geometric priors in diffusion models with well-defined 3D shapes, resolving the multi-view inconsistency problem and achieving high generalizability.</p><hr><h3>LLM-grounded Video Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=exKHibougU'>https://openreview.net/forum?id=exKHibougU</a></p>
<p><b>Compressor summary</b>: LLM-grounded Video Diffusion (LVD) uses a large language model to generate dynamic scene layouts based on text inputs, which then guide a video diffusion model for improved spatiotemporal video generation.</p><hr><h3>Maximum Likelihood Estimation is All You Need for Well-Specified Covariate Shift</h3>
<p><a href='https://openreview.net/forum?id=eoTCKKOgIs'>https://openreview.net/forum?id=eoTCKKOgIs</a></p>
<p><b>Compressor summary</b>: Classical MLE using source data is near-optimal for covariate shift in well-specified models, but MWLE can be better in misspecified settings.</p><hr><h3>Towards Energy Efficient Spiking Neural Networks: An Unstructured Pruning Framework</h3>
<p><a href='https://openreview.net/forum?id=eoSeaK4QJo'>https://openreview.net/forum?id=eoSeaK4QJo</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel pruning framework that combines unstructured weight and neuron pruning to enhance energy efficiency in deep spiking neural networks without sacrificing much accuracy.</p><hr><h3>Combining Spatial and Temporal Abstraction in Planning for Better Generalization</h3>
<p><a href='https://openreview.net/forum?id=eo9dHwtTFt'>https://openreview.net/forum?id=eo9dHwtTFt</a></p>
<p><b>Compressor summary</b>: Skipper is a model-based RL agent that learns and decomposes tasks into subtasks using hindsight, enabling better generalization and sparse decision-making.</p><hr><h3>Consistency Training with Learnable Data Augmentation for Graph Anomaly Detection with Limited Supervision</h3>
<p><a href='https://openreview.net/forum?id=elMKXvhhQ9'>https://openreview.net/forum?id=elMKXvhhQ9</a></p>
<p><b>Compressor summary</b>: ConsisGAD is a new model for graph anomaly detection that uses consistency training, learnable data augmentation, and homophily distribution to handle limited supervision and class imbalance.</p><hr><h3>Fully Hyperbolic Convolutional Neural Networks for Computer Vision</h3>
<p><a href='https://openreview.net/forum?id=ekz1hN5QNh'>https://openreview.net/forum?id=ekz1hN5QNh</a></p>
<p><b>Compressor summary</b>: The paper introduces HCNN, a novel fully hyperbolic convolutional neural network for computer vision tasks, which leverages the benefits of hyperbolic geometry to better represent intricate structures in images.</p><hr><h3>Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions</h3>
<p><a href='https://openreview.net/forum?id=ekeyCgeRfC'>https://openreview.net/forum?id=ekeyCgeRfC</a></p>
<p><b>Compressor summary</b>: The study investigates Transformers' and attention-free models' abilities to learn algorithms for various function classes, showing that Transformers adaptively select sample-efficient algorithms while LLMs can generalize beyond their training data.</p><hr><h3>SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer</h3>
<p><a href='https://openreview.net/forum?id=eiF7TU1E8E'>https://openreview.net/forum?id=eiF7TU1E8E</a></p>
<p><b>Compressor summary</b>: The paper proposes and tests a new GAN training method called Slicing Adversarial Network (SAN) that uses sliced optimal transport to ensure the generator's distribution matches the target distribution more closely.</p><hr><h3>Turning large language models into cognitive models</h3>
<p><a href='https://openreview.net/forum?id=eiC4BKypf1'>https://openreview.net/forum?id=eiC4BKypf1</a></p>
<p><b>Compressor summary</b>: The paper shows how finetuning large language models on psychological data allows them to accurately model and predict human behavior in various decision-making tasks.</p><hr><h3>SEPT: Towards Efficient Scene Representation Learning for Motion Prediction</h3>
<p><a href='https://openreview.net/forum?id=efeBC1sQj9'>https://openreview.net/forum?id=efeBC1sQj9</a></p>
<p><b>Compressor summary</b>: SEPT is a self-supervised learning framework that helps autonomous vehicles predict motions in complex traffic scenes by capturing kinematics, road structure, and agent interactions using masking-reconstruction tasks.</p><hr><h3>Multimodal Web Navigation with Instruction-Finetuned Foundation Models</h3>
<p><a href='https://openreview.net/forum?id=efFmBWioSc'>https://openreview.net/forum?id=efFmBWioSc</a></p>
<p><b>Compressor summary</b>: WebGUM is a novel web navigation agent that leverages vision-language models and offline training to surpass previous methods and some online agents on multiple tasks.</p><hr><h3>Enhancing Neural Subset Selection: Integrating Background Information into Set Representations</h3>
<p><a href='https://openreview.net/forum?id=eepoE7iLpL'>https://openreview.net/forum?id=eepoE7iLpL</a></p>
<p><b>Compressor summary</b>: The paper proposes a new information aggregation module for neural network-based set functions that incorporates superset information to improve performance in tasks like drug discovery.</p><hr><h3>G$^2$N$^2$ : Weisfeiler and Lehman go grammatical</h3>
<p><a href='https://openreview.net/forum?id=eZneJ55mRO'>https://openreview.net/forum?id=eZneJ55mRO</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to connect algebraic languages and GNNs using CFGs, reduces redundancies in rules, and shows that their derived G$^2$N$^2$ model performs better than other 3-WL GNNs on various tasks.</p><hr><h3>Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs</h3>
<p><a href='https://openreview.net/forum?id=eY7sLb0dVF'>https://openreview.net/forum?id=eY7sLb0dVF</a></p>
<p><b>Compressor summary</b>: Koopman VAE (KVAE) is a new generative framework for realistic time series data generation that leverages Koopman theory and spectral tools to incorporate domain knowledge and improve stability, outperforming existing GAN and VAE methods on various benchmarks.</p><hr><h3>SaNN: Simple Yet Powerful Simplicial-aware Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=eUgS9Ig8JG'>https://openreview.net/forum?id=eUgS9Ig8JG</a></p>
<p><b>Compressor summary</b>: The paper proposes SaNN, a scalable graph representation learning model with constant run-time and memory, which is powerful, equivariant, and achieves state-of-the-art performance.</p><hr><h3>Annealing Self-Distillation Rectification Improves Adversarial Training</h3>
<p><a href='https://openreview.net/forum?id=eT6oLkm1cm'>https://openreview.net/forum?id=eT6oLkm1cm</a></p>
<p><b>Compressor summary</b>: ADR is a simple method that generates soft labels for adversarial training to improve model robustness without extra computation or pre-trained models.</p><hr><h3>Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation</h3>
<p><a href='https://openreview.net/forum?id=ePOjNlOjLC'>https://openreview.net/forum?id=ePOjNlOjLC</a></p>
<p><b>Compressor summary</b>: The text proposes a method called Cyclic One-Way Diffusion (COW) to control the direction of diffusion phenomenon in image generation, allowing for more versatile and precise customization without additional training.</p><hr><h3>Neural Fourier Transform: A General Approach to Equivariant Representation Learning</h3>
<p><a href='https://openreview.net/forum?id=eOCvA8iwXH'>https://openreview.net/forum?id=eOCvA8iwXH</a></p>
<p><b>Compressor summary</b>: Neural Fourier Transform (NFT) is a framework for learning data structure using symmetry without assuming explicit knowledge of the group's action on data, and shows theoretical and practical benefits.</p><hr><h3>Unknown Domain Inconsistency Minimization for Domain Generalization</h3>
<p><a href='https://openreview.net/forum?id=eNoiRal5xi'>https://openreview.net/forum?id=eNoiRal5xi</a></p>
<p><b>Compressor summary</b>: UDIM is a new objective for domain generalization that minimizes inconsistency between source and unknown domains by perturbing data and aligning flat minima.</p><hr><h3>On Trajectory Augmentations for Off-Policy Evaluation</h3>
<p><a href='https://openreview.net/forum?id=eMNN0wIyVw'>https://openreview.net/forum?id=eMNN0wIyVw</a></p>
<p><b>Compressor summary</b>: The paper proposes an offline trajectory augmentation approach to enhance off-policy evaluation in human-involved scenarios using sub-trajectory mining and diversifying state-action space coverage.</p><hr><h3>Combinatorial Bandits for Maximum Value Reward Function under Value-Index Feedback</h3>
<p><a href='https://openreview.net/forum?id=eMHn77ZKOp'>https://openreview.net/forum?id=eMHn77ZKOp</a></p>
<p><b>Compressor summary</b>: The paper proposes an algorithm for a new combinatorial multi-armed bandit problem with index feedback and provides a regret bound that compares favorably to existing methods.</p><hr><h3>DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation</h3>
<p><a href='https://openreview.net/forum?id=eJHnSg783t'>https://openreview.net/forum?id=eJHnSg783t</a></p>
<p><b>Compressor summary</b>: DIFFTACTILE is a high-fidelity physics-based tactile simulation system for robotic manipulation with differentiable components and contact modeling.</p><hr><h3>Blending Imitation and Reinforcement Learning for Robust Policy Improvement</h3>
<p><a href='https://openreview.net/forum?id=eJ0dzPJq1F'>https://openreview.net/forum?id=eJ0dzPJq1F</a></p>
<p><b>Compressor summary</b>: RPI is a novel algorithm that interleaves imitation learning and reinforcement learning to improve policy robustness and sample efficiency, adapting to different oracles and states.</p><hr><h3>Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy</h3>
<p><a href='https://openreview.net/forum?id=eFWG9Cy3WK'>https://openreview.net/forum?id=eFWG9Cy3WK</a></p>
<p><b>Compressor summary</b>: The paper proposes M-SMoE and MC-SMoE, two methods that improve the scalability of neural networks using SMoE by merging and compressing experts based on their activation frequency and routing statistics.</p><hr><h3>Reward-Free Curricula for Training Robust World Models</h3>
<p><a href='https://openreview.net/forum?id=eCGpNGDeNu'>https://openreview.net/forum?id=eCGpNGDeNu</a></p>
<p><b>Compressor summary</b>: The paper introduces WAKER, a method to train robust world models from reward-free exploration by selecting environments with high world model error, leading to better adaptation, efficiency, and generality.</p><hr><h3>PORF: POSE RESIDUAL FIELD FOR ACCURATE NEURAL SURFACE RECONSTRUCTION</h3>
<p><a href='https://openreview.net/forum?id=eBeECjacpw'>https://openreview.net/forum?id=eBeECjacpw</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel implicit representation, PoRF, which uses an MLP to regress pose updates and enhances pose supervision using epipolar geometry loss, leading to improved camera pose accuracy and neural surface reconstruction in challenging real-world scenarios.</p><hr><h3>PixArt-$\alpha$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</h3>
<p><a href='https://openreview.net/forum?id=eAKmQPe3m1'>https://openreview.net/forum?id=eAKmQPe3m1</a></p>
<p><b>Compressor summary</b>: PixArt- is a fast and high-quality text-to-image model that uses less training data and reduces CO2 emissions compared to existing models.</p><hr><h3>Lion Secretly Solves a Constrained Optimization: As Lyapunov Predicts</h3>
<p><a href='https://openreview.net/forum?id=e4xS9ZarDr'>https://openreview.net/forum?id=e4xS9ZarDr</a></p>
<p><b>Compressor summary</b>: Lion, a new AI optimizer, combines elements from existing algorithms and has promising results, but lacks theoretical foundation; a study provides a Lyapunov function to analyze its dynamics and improve it.</p><hr><h3>The Unreasonable Effectiveness of Linear Prediction as a Perceptual Metric</h3>
<p><a href='https://openreview.net/forum?id=e4FG5PJ9uC'>https://openreview.net/forum?id=e4FG5PJ9uC</a></p>
<p><b>Compressor summary</b>: LASI is a perceptual similarity metric based on pixel-level weighted least squares that performs well on image quality assessment tasks without training data or deep features.</p><hr><h3>ArchLock: Locking DNN Transferability at the Architecture Level with a Zero-Cost Binary Predictor</h3>
<p><a href='https://openreview.net/forum?id=e2YOVTenU9'>https://openreview.net/forum?id=e2YOVTenU9</a></p>
<p><b>Compressor summary</b>: ArchLock is a novel algorithm that uses neural architecture search and zero-cost proxies to create DNN models with low transferability, reducing the vulnerability of current defense strategies.</p><hr><h3>Symmetric Single Index Learning</h3>
<p><a href='https://openreview.net/forum?id=e1vqloonRy'>https://openreview.net/forum?id=e1vqloonRy</a></p>
<p><b>Compressor summary</b>: The paper studies how gradient flow can learn single-index models in symmetric neural networks and prove its efficiency using an adapted information exponent.</p><hr><h3>Weight Selection for Model Initialization</h3>
<p><a href='https://openreview.net/forum?id=dyrGMhicMw'>https://openreview.net/forum?id=dyrGMhicMw</a></p>
<p><b>Compressor summary</b>: Weight selection is a method that initializes smaller models with a subset of weights from a larger pretrained model, improving their performance and reducing training time.</p><hr><h3>DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=dyG2oLJYyX'>https://openreview.net/forum?id=dyG2oLJYyX</a></p>
<p><b>Compressor summary</b>: DiffusionNAG is a novel conditional Neural Architecture Generation framework based on diffusion models that generates task-optimal architectures efficiently for diverse tasks by sampling from a region more likely to satisfy desired properties.</p><hr><h3>On the Scalability and Memory Efficiency of Semidefinite Programs  for Lipschitz Constant Estimation of Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=dwzLn78jq7'>https://openreview.net/forum?id=dwzLn78jq7</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for estimating Lipschitz constants in deep learning using eigenvalue optimization, which is more scalable and efficient than standard SDP algorithms.</p><hr><h3>HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion</h3>
<p><a href='https://openreview.net/forum?id=duyA42HlCK'>https://openreview.net/forum?id=duyA42HlCK</a></p>
<p><b>Compressor summary</b>: The authors propose HyperHuman, a model that generates realistic and diverse human images by capturing correlations between appearance and structure at multiple scales using a large-scale dataset and a unified framework.</p><hr><h3>BadEdit: Backdooring Large Language Models by Model Editing</h3>
<p><a href='https://openreview.net/forum?id=duZANm2ABX'>https://openreview.net/forum?id=duZANm2ABX</a></p>
<p><b>Compressor summary</b>: The BadEdit attack framework injects backdoors into large language models using a lightweight editing technique, requiring minimal data and time, while preserving model performance and robustness.</p><hr><h3>Teaching Arithmetic to Small Transformers</h3>
<p><a href='https://openreview.net/forum?id=dsUB4bst9S'>https://openreview.net/forum?id=dsUB4bst9S</a></p>
<p><b>Compressor summary</b>: Small transformers can efficiently learn arithmetic operations using the next-token prediction objective when trained on high-quality, instructive data with intermediate step results.</p><hr><h3>Object-Aware Inversion and Reassembly for Image Editing</h3>
<p><a href='https://openreview.net/forum?id=dpcVXiMlcv'>https://openreview.net/forum?id=dpcVXiMlcv</a></p>
<p><b>Compressor summary</b>: OIR is a new image editing paradigm that adapts the number of inversion steps for each editing pair to achieve optimal results and fine-grained editing of objects in images.</p><hr><h3>Minimum width for universal approximation using ReLU networks on compact domain</h3>
<p><a href='https://openreview.net/forum?id=dpDw5U04SU'>https://openreview.net/forum?id=dpDw5U04SU</a></p>
<p><b>Compressor summary</b>: The minimum width needed for deep neural networks to approximate any function using ReLU-like activations is $\max\{d_x,d_y,2\}$, where $d_x$ and $d_y$ are the input and output dimensions.</p><hr><h3>Periodicity Decoupling Framework for Long-term Series Forecasting</h3>
<p><a href='https://openreview.net/forum?id=dp27P5HBBt'>https://openreview.net/forum?id=dp27P5HBBt</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework called PDF that models 2D temporal variations for long-term time series forecasting by decoupling, capturing short- and long-term variations, and aggregating them.</p><hr><h3>SemiReward: A General Reward Model for Semi-supervised Learning</h3>
<p><a href='https://openreview.net/forum?id=dnqPvUjyRI'>https://openreview.net/forum?id=dnqPvUjyRI</a></p>
<p><b>Compressor summary</b>: SemiReward is a framework that uses reward scores to evaluate and filter high-quality pseudo labels in semi-supervised learning, improving performance and convergence speed.</p><hr><h3>Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection</h3>
<p><a href='https://openreview.net/forum?id=dm8e7gsH0d'>https://openreview.net/forum?id=dm8e7gsH0d</a></p>
<p><b>Compressor summary</b>: The paper proposes an optimization framework to improve out-of-distribution detection in deep learning models by optimizing feature shaping functions using only in-distribution data.</p><hr><h3>Noise-free Score Distillation</h3>
<p><a href='https://openreview.net/forum?id=dlIMcmlAdk'>https://openreview.net/forum?id=dlIMcmlAdk</a></p>
<p><b>Compressor summary</b>: NFSD improves text-to-content generation by reducing noise and over-smoothing using a smaller CFG scale, while maintaining realism and prompt compliance.</p><hr><h3>Retro-fallback: retrosynthetic planning in an uncertain world</h3>
<p><a href='https://openreview.net/forum?id=dl0u4ODCuW'>https://openreview.net/forum?id=dl0u4ODCuW</a></p>
<p><b>Compressor summary</b>: The paper introduces a new algorithm, retro-fallback, for retrosynthesis that considers uncertainty and improves the chances of creating a viable synthesis plan.</p><hr><h3>Neural Snowflakes: Universal Latent Graph Inference via Trainable Latent Geometries</h3>
<p><a href='https://openreview.net/forum?id=djM3WzpOmK'>https://openreview.net/forum?id=djM3WzpOmK</a></p>
<p><b>Compressor summary</b>: The paper introduces a new deep learning architecture called neural snowflake that improves latent graph inference by adaptively implementing fractal-like metrics on high-dimensional data, without the curse of dimensionality and random search.</p><hr><h3>SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</h3>
<p><a href='https://openreview.net/forum?id=di52zR8xgf'>https://openreview.net/forum?id=di52zR8xgf</a></p>
<p><b>Compressor summary</b>: SDXL is a text-to-image model with a larger backbone, new conditioning schemes, and a refinement model that improves its quality and competes with state-of-the-art image generators.</p><hr><h3>Decoding Natural Images from EEG for Object Recognition</h3>
<p><a href='https://openreview.net/forum?id=dhLIno8FmH'>https://openreview.net/forum?id=dhLIno8FmH</a></p>
<p><b>Compressor summary</b>: The paper presents a self-supervised method to learn image representations from EEG signals and achieve above-chance results in object recognition tasks, while exploring the biological plausibility of the approach.</p><hr><h3>Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization</h3>
<p><a href='https://openreview.net/forum?id=de1218PoEl'>https://openreview.net/forum?id=de1218PoEl</a></p>
<p><b>Compressor summary</b>: DumBO is a new decentralized Bayesian Optimization algorithm that relaxes assumptions on the function to optimize and reduces over-exploration, making it suitable for high-dimensional problems with complex structure.</p><hr><h3>Reward Model Ensembles Help Mitigate Overoptimization</h3>
<p><a href='https://openreview.net/forum?id=dcjtMYkpXx'>https://openreview.net/forum?id=dcjtMYkpXx</a></p>
<p><b>Compressor summary</b>: Ensemble-based conservative optimization methods can reduce overoptimization and improve the performance of large language models fine-tuned with human feedback.</p><hr><h3>Stable Anisotropic Regularization</h3>
<p><a href='https://openreview.net/forum?id=dbQH9AOVd5'>https://openreview.net/forum?id=dbQH9AOVd5</a></p>
<p><b>Compressor summary</b>: The paper introduces I-STAR, a novel regularization method for LLMs that accurately measures and adjusts isotropy in embedding space during training to improve model performance.</p><hr><h3>Generative Human Motion Stylization in Latent Space</h3>
<p><a href='https://openreview.net/forum?id=daEqXJ0yZo'>https://openreview.net/forum?id=daEqXJ0yZo</a></p>
<p><b>Compressor summary</b>: The paper presents a novel generative model for human motion stylization that uses pretrained autoencoders' latent space to learn probabilistic style space from motions, enabling users to select or sample style cues while preserving content.</p><hr><h3>Human Motion Diffusion as a Generative Prior</h3>
<p><a href='https://openreview.net/forum?id=dTpbEdN9kr'>https://openreview.net/forum?id=dTpbEdN9kr</a></p>
<p><b>Compressor summary</b>: The paper presents three composition methods based on diffusion priors for generating human motion with more control and flexibility, addressing challenges like long sequences, two-person interactions, and detailed editing.</p><hr><h3>Neural Polynomial Gabor Fields for Macro Motion Analysis</h3>
<p><a href='https://openreview.net/forum?id=dTlKCQuuxP'>https://openreview.net/forum?id=dTlKCQuuxP</a></p>
<p><b>Compressor summary</b>: The paper introduces Phase-based neural polynomial Gabor fields (Phase-PGF), a method to analyze large motions in 2D and 3D dynamic scenes using low-dimensional time-varying phases.</p><hr><h3>Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition</h3>
<p><a href='https://openreview.net/forum?id=dQVtTdsvZH'>https://openreview.net/forum?id=dQVtTdsvZH</a></p>
<p><b>Compressor summary</b>: The content-motion latent diffusion model (CMD) is an efficient video generation method that combines a pretrained image diffusion model and a lightweight motion model, achieving high quality and fast speed.</p><hr><h3>Fast, Expressive $\mathrm{SE}(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space</h3>
<p><a href='https://openreview.net/forum?id=dPHLbUqGbr'>https://openreview.net/forum?id=dPHLbUqGbr</a></p>
<p><b>Compressor summary</b>: The paper presents a geometric method to optimize edge attributes for convolutional networks using homogeneous spaces, leading to an efficient 3D group convolutional network that achieves state-of-the-art results on various benchmarks.</p><hr><h3>Closing the Curious Case of Neural Text Degeneration</h3>
<p><a href='https://openreview.net/forum?id=dONpC9GL1o'>https://openreview.net/forum?id=dONpC9GL1o</a></p>
<p><b>Compressor summary</b>: The paper explains why truncation sampling heuristics like nucleus sampling work well for language generation and proposes a new strategy based on softmax bottleneck to improve expressiveness without relying on thresholds.</p><hr><h3>Efficacy of Dual-Encoders for Extreme Multi-label Classification</h3>
<p><a href='https://openreview.net/forum?id=dNe1T0Ahby'>https://openreview.net/forum?id=dNe1T0Ahby</a></p>
<p><b>Compressor summary</b>: The paper explores the performance of dual-encoder models on extreme multi-label classification tasks and proposes a new loss function to optimize for Recall@k metrics.</p><hr><h3>TUVF: Learning Generalizable Texture UV Radiance Fields</h3>
<p><a href='https://openreview.net/forum?id=dN4vpVTvWX'>https://openreview.net/forum?id=dN4vpVTvWX</a></p>
<p><b>Compressor summary</b>: The paper proposes Texture UV Radiance Fields, a method for generating and controlling realistic textures on 3D models independent of their shapes.</p><hr><h3>Topological data analysis on noisy quantum computers</h3>
<p><a href='https://openreview.net/forum?id=dLrhRIMVmB'>https://openreview.net/forum?id=dLrhRIMVmB</a></p>
<p><b>Compressor summary</b>: NISQ-TDA is a quantum machine learning algorithm for high-dimensional data analysis with proven speedup and low noise sensitivity.</p><hr><h3>Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks</h3>
<p><a href='https://openreview.net/forum?id=dLoAdIKENc'>https://openreview.net/forum?id=dLoAdIKENc</a></p>
<p><b>Compressor summary</b>: The paper analyzes the strengths and weaknesses of AI-image detectors, especially watermarking methods, and their vulnerabilities to attacks that can falsely flag real images as fake or vice versa.</p><hr><h3>Prompt Learning with Quaternion Networks</h3>
<p><a href='https://openreview.net/forum?id=dKlxDx2SoS'>https://openreview.net/forum?id=dKlxDx2SoS</a></p>
<p><b>Compressor summary</b>: The paper proposes QNet, a method that uses quaternion networks to enhance multimodal pre-trained models for better semantic alignment across diverse modalities.</p><hr><h3>Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=dKl6lMwbCy'>https://openreview.net/forum?id=dKl6lMwbCy</a></p>
<p><b>Compressor summary</b>: The choice of feedback protocol, either ratings or rankings, significantly affects how well large language models are aligned with human values and intents.</p><hr><h3>ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</h3>
<p><a href='https://openreview.net/forum?id=dHng2O0Jjr'>https://openreview.net/forum?id=dHng2O0Jjr</a></p>
<p><b>Compressor summary</b>: The authors introduce ToolLLM, a framework for enhancing large language models' tool-use capabilities by constructing a dataset (ToolBench) and fine-tuning LLaMA with a neural API retriever.</p><hr><h3>GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape Generative Models</h3>
<p><a href='https://openreview.net/forum?id=dGH4kHFKFj'>https://openreview.net/forum?id=dGH4kHFKFj</a></p>
<p><b>Compressor summary</b>: GenCorres is a new unsupervised method for joint shape matching that learns a mesh generator to fit a large synthetic shape collection, preserving local geometric structures and outperforming existing techniques.</p><hr><h3>PlaSma: Procedural Knowledge Models for Language-based Planning and Re-Planning</h3>
<p><a href='https://openreview.net/forum?id=dFcXJgnrGB'>https://openreview.net/forum?id=dFcXJgnrGB</a></p>
<p><b>Compressor summary</b>: The paper proposes PlaSma, a two-pronged approach to enhance small language models with procedural knowledge and planning capabilities, and shows they can outperform larger models in planning and replanning tasks.</p><hr><h3>Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity</h3>
<p><a href='https://openreview.net/forum?id=dEz3ge8QSo'>https://openreview.net/forum?id=dEz3ge8QSo</a></p>
<p><b>Compressor summary</b>: The paper introduces a new risk-sensitive MDP formulation, shows its equivalence with regularized robust MDPs, derives policy gradient theorems, proves global convergence, and proposes an efficient offline learning algorithm for specific problems.</p><hr><h3>Learning to Reject with a Fixed Predictor: Application to Decontextualization</h3>
<p><a href='https://openreview.net/forum?id=dCHbFDsCZz'>https://openreview.net/forum?id=dCHbFDsCZz</a></p>
<p><b>Compressor summary</b>: The paper proposes a new problem formulation and algorithm for classification with reject option, shows strong consistency guarantees, and evaluates it on decontextualization task achieving high coverage.</p><hr><h3>Tool-Augmented Reward Modeling</h3>
<p><a href='https://openreview.net/forum?id=d94x0gWTUX'>https://openreview.net/forum?id=d94x0gWTUX</a></p>
<p><b>Compressor summary</b>: Themis is a tool-augmented preference modeling approach that improves large language models' functionality by enabling them to use external tools for tasks like arithmetic, code execution, and factual lookup.</p><hr><h3>Small-scale proxies for large-scale Transformer training instabilities</h3>
<p><a href='https://openreview.net/forum?id=d8w0pmvXbZ'>https://openreview.net/forum?id=d8w0pmvXbZ</a></p>
<p><b>Compressor summary</b>: The paper investigates training instability in Transformer-based models at different scales and suggests interventions to improve stability and loss performance across learning rate variations.</p><hr><h3>Energy-guided Entropic Neural Optimal Transport</h3>
<p><a href='https://openreview.net/forum?id=d6tUsZeVs7'>https://openreview.net/forum?id=d6tUsZeVs7</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel method that combines energy-based models and entropy-regularized optimal transport to improve generative modeling and provides theoretical and empirical results.</p><hr><h3>Spurious Feature Diversification Improves Out-of-distribution Generalization</h3>
<p><a href='https://openreview.net/forum?id=d6H4RBi7RH'>https://openreview.net/forum?id=d6H4RBi7RH</a></p>
<p><b>Compressor summary</b>: The study examines how weight space ensembles like WiSE-FT use diverse spurious features for better out-of-distribution generalization and proposes BANG to address overconfidence issues in such methods.</p><hr><h3>Dynamic Layer Tying for Parameter-Efficient Transformers</h3>
<p><a href='https://openreview.net/forum?id=d4uL2MSe0z'>https://openreview.net/forum?id=d4uL2MSe0z</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to reduce the number of trainable parameters in deep transformer networks using Reinforcement Learning for layer selection and weight sharing.</p><hr><h3>LLaMA-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention</h3>
<p><a href='https://openreview.net/forum?id=d4UiXAHN2W'>https://openreview.net/forum?id=d4UiXAHN2W</a></p>
<p><b>Compressor summary</b>: The paper introduces LLaMA-Adapter, a method to efficiently fine-tune LLaMA for instruction following using self-attention layers with learnable zero gating, achieving similar performance to Alpaca with fewer parameters and extending to multi-modal tasks.</p><hr><h3>Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation</h3>
<p><a href='https://openreview.net/forum?id=d3xKPQVjSc'>https://openreview.net/forum?id=d3xKPQVjSc</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to estimate how much dimensionality reduction in CATE estimation can introduce bias due to confounding.</p><hr><h3>Procedural Fairness Through Decoupling Objectionable Data Generating Components</h3>
<p><a href='https://openreview.net/forum?id=cxfPefbu1s'>https://openreview.net/forum?id=cxfPefbu1s</a></p>
<p><b>Compressor summary</b>: The paper addresses the problem of disguised procedural unfairness in automated decision-making and proposes a framework to separate objectionable from neutral data generating components.</p><hr><h3>On Differentially Private Federated Linear Contextual Bandits</h3>
<p><a href='https://openreview.net/forum?id=cuAxSHcsSX'>https://openreview.net/forum?id=cuAxSHcsSX</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithmic framework for collaborative federated linear contextual bandits under differential privacy that addresses privacy, regret, and communication cost issues.</p><hr><h3>Generalized Policy Iteration using Tensor Approximation for Hybrid Control</h3>
<p><a href='https://openreview.net/forum?id=csukJcpYDe'>https://openreview.net/forum?id=csukJcpYDe</a></p>
<p><b>Compressor summary</b>: The paper introduces TTPI, an ADP algorithm that uses low-rank tensor approximation to efficiently control hybrid systems in robotics.</p><hr><h3>End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon</h3>
<p><a href='https://openreview.net/forum?id=cphhnHjCvC'>https://openreview.net/forum?id=cphhnHjCvC</a></p>
<p><b>Compressor summary</b>: The paper presents a method to improve goal-oriented visual navigation in complex scenes using pretext tasks that help learn correspondence and goal detection from image pairs.</p><hr><h3>Decongestion by Representation: Learning to Improve Economic Welfare in Marketplaces</h3>
<p><a href='https://openreview.net/forum?id=coIaBY8EVF'>https://openreview.net/forum?id=coIaBY8EVF</a></p>
<p><b>Compressor summary</b>: The study investigates how platforms can learn to display information in a way that reduces congestion and improves social welfare in online marketplaces by proposing a differentiable proxy of welfare based on consumer choices.</p><hr><h3>Learning the greatest common divisor: explaining transformer predictions</h3>
<p><a href='https://openreview.net/forum?id=cmcD05NPKa'>https://openreview.net/forum?id=cmcD05NPKa</a></p>
<p><b>Compressor summary</b>: The paper trains small transformers to find the greatest common divisor (GCD) of two numbers using lists of divisors and shows that their predictions are explainable, but performance depends on the training distribution of GCD.</p><hr><h3>Large Language Models are Efficient Learners of Noise-Robust Speech Recognition</h3>
<p><a href='https://openreview.net/forum?id=ceATjGPTUD'>https://openreview.net/forum?id=ceATjGPTUD</a></p>
<p><b>Compressor summary</b>: The authors propose a method to improve automatic speech recognition by using a large language model that learns to correct errors and remove noise from speech transcriptions.</p><hr><h3>Robust NAS benchmark under adversarial training: assessment, theory, and beyond</h3>
<p><a href='https://openreview.net/forum?id=cdUpf6t6LZ'>https://openreview.net/forum?id=cdUpf6t6LZ</a></p>
<p><b>Compressor summary</b>: This paper introduces a new data set for evaluating robustness against adversarial attacks and provides a generalization theory for searching architectures that balance clean and robust accuracy.</p><hr><h3>Can LLM-Generated Misinformation Be Detected?</h3>
<p><a href='https://openreview.net/forum?id=ccxD4mtkTU'>https://openreview.net/forum?id=ccxD4mtkTU</a></p>
<p><b>Compressor summary</b>: LLM-generated misinformation is harder to detect than human-written misinformation, which may increase its potential harm.</p><hr><h3>Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization</h3>
<p><a href='https://openreview.net/forum?id=cc8h3I3V4E'>https://openreview.net/forum?id=cc8h3I3V4E</a></p>
<p><b>Compressor summary</b>: The paper introduces a new loss function for finding approximate Nash equilibria in normal-form games using non-convex stochastic optimization techniques, which perform better than existing methods.</p><hr><h3>Diffusion Models for Multi-Task Generative Modeling</h3>
<p><a href='https://openreview.net/forum?id=cbv0sBIZh9'>https://openreview.net/forum?id=cbv0sBIZh9</a></p>
<p><b>Compressor summary</b>: The paper proposes a unified multi-task diffusion model that can generate different types of data simultaneously and improve generalization by aggregating information from multiple tasks.</p><hr><h3>Beyond Accuracy: Evaluating Self-Consistency of Code LLMs</h3>
<p><a href='https://openreview.net/forum?id=caW7LdAALh'>https://openreview.net/forum?id=caW7LdAALh</a></p>
<p><b>Compressor summary</b>: The paper introduces IdentityChain, a framework to evaluate and improve the self-consistency of code large language models (LLMs) across different tasks, as it affects their trustworthiness.</p><hr><h3>Uncertainty Quantification via Stable Distribution Propagation</h3>
<p><a href='https://openreview.net/forum?id=cZttUMTiPL'>https://openreview.net/forum?id=cZttUMTiPL</a></p>
<p><b>Compressor summary</b>: The text introduces a new method for using stable probability distributions in neural networks, which improves uncertainty quantification and outperforms existing methods like moment matching.</p><hr><h3>Latent 3D Graph Diffusion</h3>
<p><a href='https://openreview.net/forum?id=cXbnGtO0NZ'>https://openreview.net/forum?id=cXbnGtO0NZ</a></p>
<p><b>Compressor summary</b>: The paper introduces latent 3D graph diffusion, a fast and effective way to generate 3D graphs with symmetry and quality in a low-dimensional latent space, and applies it to conditional generation of molecules with better properties and validity.</p><hr><h3>Few-Shot Detection of Machine-Generated Text using Style Representations</h3>
<p><a href='https://openreview.net/forum?id=cWiEN1plhJ'>https://openreview.net/forum?id=cWiEN1plhJ</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to detect machine-written text using style features from human authors, which works even against state-of-the-art language models like Llama 2 and GPT-4.</p><hr><h3>State Representation Learning Using an Unbalanced Atlas</h3>
<p><a href='https://openreview.net/forum?id=cWdAYDLmPa'>https://openreview.net/forum?id=cWdAYDLmPa</a></p>
<p><b>Compressor summary</b>: The paper presents a new self-supervised learning method (DIM-UA) that leverages an unbalanced atlas to improve dimensionality reduction and outperforms existing approaches on the AtariARI benchmark.</p><hr><h3>Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model</h3>
<p><a href='https://openreview.net/forum?id=cVUOnF7iVp'>https://openreview.net/forum?id=cVUOnF7iVp</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel non-interactive differential privacy algorithm for sparse linear regression with lower bounds and upper bounds on estimation error.</p><hr><h3>GlucoBench: Curated List of Continuous Glucose Monitoring Datasets with Prediction Benchmarks</h3>
<p><a href='https://openreview.net/forum?id=cUSNs8nGaV'>https://openreview.net/forum?id=cUSNs8nGaV</a></p>
<p><b>Compressor summary</b>: CGM-based glucose trajectory prediction can improve diabetes management, but current evaluation on small datasets hinders reproducibility and adoption.</p><hr><h3>Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources</h3>
<p><a href='https://openreview.net/forum?id=cPgh4gWZlz'>https://openreview.net/forum?id=cPgh4gWZlz</a></p>
<p><b>Compressor summary</b>: Chain-of-knowledge (CoK) is a novel framework that enhances large language models by dynamically incorporating grounding information from heterogeneous sources, resulting in more factual and rational answers for knowledge-intensive questions.</p><hr><h3>Towards Faithful XAI Evaluation via Generalization-Limited Backdoor Watermark</h3>
<p><a href='https://openreview.net/forum?id=cObFETcoeW'>https://openreview.net/forum?id=cObFETcoeW</a></p>
<p><b>Compressor summary</b>: The paper proposes a new backdoor-based method for evaluating saliency-based representation visualization methods in XAI, by minimizing the effects of a generalization-limited backdoor watermark on benign and poisoned samples.</p><hr><h3>ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process</h3>
<p><a href='https://openreview.net/forum?id=cMPm8YFXZe'>https://openreview.net/forum?id=cMPm8YFXZe</a></p>
<p><b>Compressor summary</b>: The paper proposes an ADDP framework that combines pixels and VQ tokens for representation learning, achieving competitive performance on image generation and recognition tasks.</p><hr><h3>Exploring Effective Stimulus Encoding via Vision System Modeling for Visual Prostheses</h3>
<p><a href='https://openreview.net/forum?id=cKAUvMePUN'>https://openreview.net/forum?id=cKAUvMePUN</a></p>
<p><b>Compressor summary</b>: The paper proposes an optimization framework for generating stimulation patterns for visual prostheses using a retinal network, phosphene model, and primary vision system network, which can simulate the whole process of visual signals processing from external scenes to visual perception in the cortex.</p><hr><h3>Deep Orthogonal Hypersphere Compression for Anomaly Detection</h3>
<p><a href='https://openreview.net/forum?id=cJs4oE4m9Q'>https://openreview.net/forum?id=cJs4oE4m9Q</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel deep anomaly detection model that improves hypersphere learning using an orthogonal projection layer, and extends it to graph-level anomaly detection with better performance than existing methods.</p><hr><h3>Free Lunches in Auxiliary Learning: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost</h3>
<p><a href='https://openreview.net/forum?id=cINwAhrgLf'>https://openreview.net/forum?id=cINwAhrgLf</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to use auxiliary labels from an independent task to improve primary task performance while keeping single task inference cost, using a flexible asymmetric network structure that can be removed during inference.</p><hr><h3>Output-Domain Focused Inductive Bias on Latent Feature Clusters in Visual Classification</h3>
<p><a href='https://openreview.net/forum?id=cH3oufN8Pl'>https://openreview.net/forum?id=cH3oufN8Pl</a></p>
<p><b>Compressor summary</b>: The paper proposes Output-Domain focused Biasing (ODB), a training strategy that improves image classification by incorporating human label knowledge into neural networks, without external resources.</p><hr><h3>Enhancing Neural Training via a Correlated Dynamics Model</h3>
<p><a href='https://openreview.net/forum?id=c9xsaASm9L'>https://openreview.net/forum?id=c9xsaASm9L</a></p>
<p><b>Compressor summary</b>: CMD is a technique that groups correlated parameters in neural networks to efficiently represent and enhance their training dynamics.</p><hr><h3>BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=c93SBwz1Ma'>https://openreview.net/forum?id=c93SBwz1Ma</a></p>
<p><b>Compressor summary</b>: BadChain is a novel backdoor attack against large language models using chain-of-thought prompting that alters the model's output when triggered by a specific query.</p><hr><h3>Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification</h3>
<p><a href='https://openreview.net/forum?id=c8McWs4Av0'>https://openreview.net/forum?id=c8McWs4Av0</a></p>
<p><b>Compressor summary</b>: The paper explores how code usage frequency affects GPT-4 Code Interpreter's math reasoning skills and proposes a new prompting method, CSV, to enhance its self-verification abilities and accuracy on MATH dataset.</p><hr><h3>Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks</h3>
<p><a href='https://openreview.net/forum?id=c85tdYOOju'>https://openreview.net/forum?id=c85tdYOOju</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework called Weigh And Select (WAS) for integrating multiple graph pre-training tasks using decoupled siamese networks.</p><hr><h3>Democratizing Fine-grained Visual Recognition with Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=c7DND1iIgb'>https://openreview.net/forum?id=c7DND1iIgb</a></p>
<p><b>Compressor summary</b>: FineR uses large language models to reason about fine-grained visual categories without needing expert annotations, by translating part-level image attributes to text and leveraging the LLM's world knowledge.</p><hr><h3>METRA: Scalable Unsupervised RL with Metric-Aware Abstraction</h3>
<p><a href='https://openreview.net/forum?id=c5pwL0Soay'>https://openreview.net/forum?id=c5pwL0Soay</a></p>
<p><b>Compressor summary</b>: METRA is a novel unsupervised reinforcement learning objective that learns diverse behaviors by exploring a compact latent space connected to the state space, enabling scalability in high-dimensional environments.</p><hr><h3>GAFormer: Enhancing Timeseries Transformers Through Group-Aware Embeddings</h3>
<p><a href='https://openreview.net/forum?id=c56TWtYp0W'>https://openreview.net/forum?id=c56TWtYp0W</a></p>
<p><b>Compressor summary</b>: The paper introduces GAFormer, a transformer-based model that learns spatiotemporal structure from multivariate time series data and achieves state-of-the-art performance on various tasks.</p><hr><h3>Denoising Diffusion Step-aware Models</h3>
<p><a href='https://openreview.net/forum?id=c43FGk8Pcg'>https://openreview.net/forum?id=c43FGk8Pcg</a></p>
<p><b>Compressor summary</b>: DDSM is a novel framework that adapts neural network sizes to the importance of each generative step in denoising diffusion models, improving efficiency and preserving quality.</p><hr><h3>The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World</h3>
<p><a href='https://openreview.net/forum?id=c2R7ajodcI'>https://openreview.net/forum?id=c2R7ajodcI</a></p>
<p><b>Compressor summary</b>: The All-Seeing (AS) project is a dataset and model for recognizing and understanding a wide range of real-world concepts using human feedback and efficient models, aiming to advance vision-language artificial general intelligence research.</p><hr><h3>Zero-Shot Robotic Manipulation with Pre-Trained Image-Editing Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=c0chJTSbci'>https://openreview.net/forum?id=c0chJTSbci</a></p>
<p><b>Compressor summary</b>: SuSIE is a method that uses an image editing diffusion model to help generalist robots recognize and reason about novel objects and scenarios, achieving better generalization than conventional language-conditioned policies.</p><hr><h3>Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=c0MyyXyGfn'>https://openreview.net/forum?id=c0MyyXyGfn</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel algorithm, PSQD, that solves continuous space lexicographic multi-objective RL problems by learning and adapting subtask solutions under priorities, enabling reuse and offline learning.</p><hr><h3>AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model</h3>
<p><a href='https://openreview.net/forum?id=bxfKIYfHyx'>https://openreview.net/forum?id=bxfKIYfHyx</a></p>
<p><b>Compressor summary</b>: AlignDiff is a framework that uses reinforcement learning and human feedback to customize agent behaviors according to diverse and changing human preferences in locomotion tasks.</p><hr><h3>Sharpness-Aware Data Poisoning Attack</h3>
<p><a href='https://openreview.net/forum?id=bxITGFPVWh'>https://openreview.net/forum?id=bxITGFPVWh</a></p>
<p><b>Compressor summary</b>: The paper introduces Sharpness-Aware Data Poisoning Attack (SAPA), which exploits the loss landscape sharpness to optimize data poisoning for maximum impact on trained models.</p><hr><h3>Structural Fairness-aware Active Learning for Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=bvjcMvMn7B'>https://openreview.net/forum?id=bvjcMvMn7B</a></p>
<p><b>Compressor summary</b>: The paper proposes SCARCE, a framework to improve GNNs' performance and fairness in semi-supervised node classification by incorporating graph structure and mitigating structural bias in active learning.</p><hr><h3>AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection</h3>
<p><a href='https://openreview.net/forum?id=buC4E91xZE'>https://openreview.net/forum?id=buC4E91xZE</a></p>
<p><b>Compressor summary</b>: AnomalyCLIP adapts CLIP for accurate zero-shot anomaly detection by learning object-agnostic text prompts that capture generic normality and abnormality in images, enabling generalized recognition across different domains.</p><hr><h3>Efficient Planning with Latent Diffusion</h3>
<p><a href='https://openreview.net/forum?id=btpgDo4u4j'>https://openreview.net/forum?id=btpgDo4u4j</a></p>
<p><b>Compressor summary</b>: The paper proposes LatentDiffuser, a framework that learns and plans in continuous latent action spaces using score-based diffusion models for efficient offline reinforcement learning.</p><hr><h3>RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering</h3>
<p><a href='https://openreview.net/forum?id=bshfchPM9H'>https://openreview.net/forum?id=bshfchPM9H</a></p>
<p><b>Compressor summary</b>: The text introduces a new method, called Rapper, for improving visual question answering by generating explanations that are informative, plausible, and grounded in both language and vision facts.</p><hr><h3>Mediator Interpretation and Faster Learning Algorithms for Linear Correlated Equilibria in General Sequential Games</h3>
<p><a href='https://openreview.net/forum?id=bsKMPAFHO7'>https://openreview.net/forum?id=bsKMPAFHO7</a></p>
<p><b>Compressor summary</b>: The paper connects linear deviations in extensive-form games to untimed communication deviations and develops faster algorithms for computing linear correlated equilibria.</p><hr><h3>Stabilizing Backpropagation Through Time to Learn Complex Physics</h3>
<p><a href='https://openreview.net/forum?id=bozbTTWcaw'>https://openreview.net/forum?id=bozbTTWcaw</a></p>
<p><b>Compressor summary</b>: The paper proposes a new vector field for physics simulations that improves optimization by avoiding the issues caused by the rotational gradient field used in recurrent learning setups.</p><hr><h3>Adaptive Stochastic Gradient Algorithm for Black-box Multi-Objective Learning</h3>
<p><a href='https://openreview.net/forum?id=bm1JVsVZVu'>https://openreview.net/forum?id=bm1JVsVZVu</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm, ASMG, for black-box multi-objective optimization that uses stochastic gradient approximation and adaptive weights to optimize multiple objectives efficiently and proves its convergence rate in both convex and non-convex scenarios.</p><hr><h3>A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis</h3>
<p><a href='https://openreview.net/forum?id=bkdWThqE6q'>https://openreview.net/forum?id=bkdWThqE6q</a></p>
<p><b>Compressor summary</b>: The paper introduces INTR, a Transformer-based image classification method that generates class-specific queries to help each class find its patterns in an image, enabling interpretable predictions.</p><hr><h3>MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods</h3>
<p><a href='https://openreview.net/forum?id=bkNx3O0sND'>https://openreview.net/forum?id=bkNx3O0sND</a></p>
<p><b>Compressor summary</b>: The authors propose efficient methods to finetune NMT models using external data and show significant quality improvements over the base model.</p><hr><h3>Multilinear Operator Networks</h3>
<p><a href='https://openreview.net/forum?id=bbCL5aRjUx'>https://openreview.net/forum?id=bbCL5aRjUx</a></p>
<p><b>Compressor summary</b>: MONet is a new model for image recognition that uses only multilinear operators, without activation functions, and achieves performance comparable to modern neural networks.</p><hr><h3>R-MAE: Regions Meet Masked Autoencoders</h3>
<p><a href='https://openreview.net/forum?id=ba84RDHFnz'>https://openreview.net/forum?id=ba84RDHFnz</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method called masked region autoencoding to learn from groups of pixels in images and improve self-supervised learning for tasks like detection and segmentation.</p><hr><h3>Exploring Diffusion Time-steps for Unsupervised Representation Learning</h3>
<p><a href='https://openreview.net/forum?id=bWzxhtl1HP'>https://openreview.net/forum?id=bWzxhtl1HP</a></p>
<p><b>Compressor summary</b>: The text explores using Denoising Diffusion Probabilistic Model to learn modular attributes from data by adding noise and learning features to compensate for the lost attributes at each time-step.</p><hr><h3>FITS: Modeling Time Series with $10k$ Parameters</h3>
<p><a href='https://openreview.net/forum?id=bWcnvZ3qMb'>https://openreview.net/forum?id=bWcnvZ3qMb</a></p>
<p><b>Compressor summary</b>: FITS is a lightweight model that uses complex frequency domain interpolation to perform well in time series analysis tasks, and it can run on edge devices.</p><hr><h3>Transferring Learning Trajectories of Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=bWNJFD1l8M'>https://openreview.net/forum?id=bWNJFD1l8M</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to transfer learning trajectories between DNNs to speed up training and improve performance.</p><hr><h3>Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time</h3>
<p><a href='https://openreview.net/forum?id=bTMMNT7IdW'>https://openreview.net/forum?id=bTMMNT7IdW</a></p>
<p><b>Compressor summary</b>: SDE-EDG is a novel method that collects infinitely fined-grid evolving trajectories of data distributions using stochastic differential equations to capture distribution shifts in time-varying systems, outperforming existing approaches.</p><hr><h3>Future Language Modeling from Temporal Document History</h3>
<p><a href='https://openreview.net/forum?id=bRLed9prWC'>https://openreview.net/forum?id=bRLed9prWC</a></p>
<p><b>Compressor summary</b>: The authors introduce the task of future language modeling, which predicts texts in the future based on a temporal history of texts, and show that it improves upon non-temporal language models.</p><hr><h3>Detecting  Generated Text via Rewriting</h3>
<p><a href='https://openreview.net/forum?id=bQWE2UqXmf'>https://openreview.net/forum?id=bQWE2UqXmf</a></p>
<p><b>Compressor summary</b>: Large language models (LLMs) can be used to detect AI-generated text by rewriting it and measuring the editing distance, improving detection scores across various domains.</p><hr><h3>Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement</h3>
<p><a href='https://openreview.net/forum?id=bNt7oajl2a'>https://openreview.net/forum?id=bNt7oajl2a</a></p>
<p><b>Compressor summary</b>: The text studies how well language models do inductive reasoning using a technique called iterative hypothesis refinement, which shows they are good at proposing rules but struggle to apply them, revealing differences between human and model reasoning.</p><hr><h3>Boundary Denoising for Video Activity Localization</h3>
<p><a href='https://openreview.net/forum?id=bLpUtGyf9g'>https://openreview.net/forum?id=bLpUtGyf9g</a></p>
<p><b>Compressor summary</b>: DenosieLoc is an encoder-decoder model that learns to localize video activities by denoising action spans with controlled noise, leading to better understanding and faster convergence.</p><hr><h3>UC-NERF: Neural Radiance Field for under-calibrated multi-view cameras</h3>
<p><a href='https://openreview.net/forum?id=bLKcCe7hYh'>https://openreview.net/forum?id=bLKcCe7hYh</a></p>
<p><b>Compressor summary</b>: UC-NeRF is a new method for creating realistic images from multiple cameras by correcting color inconsistencies and improving pose calibration.</p><hr><h3>Facing the Elephant in the Room: Visual Prompt Tuning or Full finetuning?</h3>
<p><a href='https://openreview.net/forum?id=bJx4iOIOxn'>https://openreview.net/forum?id=bJx4iOIOxn</a></p>
<p><b>Compressor summary</b>: The paper analyzes Visual Prompt Tuning (VPT) as a transfer learning technique and identifies scenarios where it outperforms full-finetuning, as well as the underlying reasons for its success.</p><hr><h3>Meta Inverse Constrained Reinforcement Learning: Convergence Guarantee and Generalization Analysis</h3>
<p><a href='https://openreview.net/forum?id=bJ3gFiwRgi'>https://openreview.net/forum?id=bJ3gFiwRgi</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for learning reward functions and constraints of an expert from few demonstrations by using meta-learning over related tasks, and shows theoretical and experimental results.</p><hr><h3>Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck</h3>
<p><a href='https://openreview.net/forum?id=bH6T0Jjw5y'>https://openreview.net/forum?id=bH6T0Jjw5y</a></p>
<p><b>Compressor summary</b>: This paper introduces Time-lagged Information Bottleneck (T-IB), a method to simplify complex systems for accurate simulation using information theory principles.</p><hr><h3>Understanding In-Context Learning from Repetitions</h3>
<p><a href='https://openreview.net/forum?id=bGGYcvw8mp'>https://openreview.net/forum?id=bGGYcvw8mp</a></p>
<p><b>Compressor summary</b>: The paper investigates how surface features and token co-occurrence reinforcement affect in-context learning in LLMs, revealing its mechanisms and limitations.</p><hr><h3>COSA: Concatenated Sample Pretrained Vision-Language Foundation Model</h3>
<p><a href='https://openreview.net/forum?id=bDkisS75zy'>https://openreview.net/forum?id=bDkisS75zy</a></p>
<p><b>Compressor summary</b>: COSA is a vision-language model that pretrains on image-text pairs to jointly model visual contents and event-level temporal cues, achieving state-of-the-art results on multiple semantic tasks.</p><hr><h3>Learning model uncertainty as variance-minimizing instance weights</h3>
<p><a href='https://openreview.net/forum?id=bDWXhzZT40'>https://openreview.net/forum?id=bDWXhzZT40</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to capture and use predictive uncertainty for various tasks using an auxiliary network trained with a meta-objective that minimizes dropout variance, resulting in significant performance gains across different datasets and scenarios.</p><hr><h3>Supervised Knowledge Makes Large Language Models Better In-context Learners</h3>
<p><a href='https://openreview.net/forum?id=bAMPOUF227'>https://openreview.net/forum?id=bAMPOUF227</a></p>
<p><b>Compressor summary</b>: The text proposes a framework to improve large language models' generalizability and factuality by using task-specific fine-tuned models during inference, and demonstrates its effectiveness with experiments and resources.</p><hr><h3>Pareto Deep Long-Tailed Recognition: A Conflict-Averse Solution</h3>
<p><a href='https://openreview.net/forum?id=b66P1u0k15'>https://openreview.net/forum?id=b66P1u0k15</a></p>
<p><b>Compressor summary</b>: The paper proposes a Pareto deep long-tailed recognition method (PLOT) that uses a multi-objective optimization framework and dynamic re-balancing to address gradient conflicts among categories and improve overall performance.</p><hr><h3>Representation Deficiency in Masked Language Modeling</h3>
<p><a href='https://openreview.net/forum?id=b3l0piOrGU'>https://openreview.net/forum?id=b3l0piOrGU</a></p>
<p><b>Compressor summary</b>: MAE-LM is a new pretraining method that excludes $	exttt{[MASK]}$ tokens from the encoder and improves text encoder's performance on downstream tasks.</p><hr><h3>KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval</h3>
<p><a href='https://openreview.net/forum?id=b3kDP3IytM'>https://openreview.net/forum?id=b3kDP3IytM</a></p>
<p><b>Compressor summary</b>: The paper introduces KITAB, a new dataset to test language models' ability to answer book-related queries with constraints, and analyzes their limitations in different scenarios.</p><hr><h3>Meta-Learning Priors Using Unrolled Proximal Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=b3Cu426njo'>https://openreview.net/forum?id=b3Cu426njo</a></p>
<p><b>Compressor summary</b>: Meta-learning improves by using a novel prior representation model that leverages algorithm unrolling for more expressive and interpretable priors.</p><hr><h3>Retrieval-augmented Vision-Language Representation for Fine-grained Recognition</h3>
<p><a href='https://openreview.net/forum?id=b2UlHeyyC0'>https://openreview.net/forum?id=b2UlHeyyC0</a></p>
<p><b>Compressor summary</b>: The paper introduces a method to enhance CLIP's performance on fine-grained tasks by using external memory to refine its embeddings, instead of expanding the pre-training data.</p><hr><h3>The Hidden Language of Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=awWpHnEJDw'>https://openreview.net/forum?id=awWpHnEJDw</a></p>
<p><b>Compressor summary</b>: Conceptor is a method that reveals how diffusion models represent textual concepts in their images by decomposing them into interpretable elements, discovering structures and connections between concepts, and linking them to visual impacts.</p><hr><h3>Expressivity of ReLU-Networks under Convex Relaxations</h3>
<p><a href='https://openreview.net/forum?id=awHTL3Hpto'>https://openreview.net/forum?id=awHTL3Hpto</a></p>
<p><b>Compressor summary</b>: The study explores how different convex relaxations affect the ability of ReLU neural networks to encode and analyze various classes of convex functions, revealing limitations in some cases.</p><hr><h3>Implicit Maximum a Posteriori Filtering via Adaptive Optimization</h3>
<p><a href='https://openreview.net/forum?id=auUngos7eR'>https://openreview.net/forum?id=auUngos7eR</a></p>
<p><b>Compressor summary</b>: The paper proposes a new way of doing Bayesian filtering using optimization instead of matrix operations, which works well for high-dimensional systems and is easier to implement.</p><hr><h3>Adaptive Chameleon  or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts</h3>
<p><a href='https://openreview.net/forum?id=auKAUJZMO6'>https://openreview.net/forum?id=auKAUJZMO6</a></p>
<p><b>Compressor summary</b>: The study investigates how large language models respond to external information that conflicts with their internal knowledge, finding that they can be receptive to contradicting evidence but also show confirmation bias when presented with consistent information.</p><hr><h3>Adaptive Regularization of Representation Rank as an Implicit Constraint of Bellman Equation</h3>
<p><a href='https://openreview.net/forum?id=apXtolxDaJ'>https://openreview.net/forum?id=apXtolxDaJ</a></p>
<p><b>Compressor summary</b>: The paper introduces BEER, a regularizer that adaptively controls the representation rank of value networks in Deep Reinforcement Learning, improving agent performance and Q-value approximation.</p><hr><h3>A Topological Perspective on Demystifying GNN-Based Link Prediction Performance</h3>
<p><a href='https://openreview.net/forum?id=apA6SSXx2e'>https://openreview.net/forum?id=apA6SSXx2e</a></p>
<p><b>Compressor summary</b>: The paper explores how Topological Concentration (TC), a new metric based on local subgraphs, can improve link prediction performance in graph neural networks (GNNs) by identifying and addressing topological distribution shifts.</p><hr><h3>Unveiling and Manipulating Prompt Influence in Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=ap1ByuwQrX'>https://openreview.net/forum?id=ap1ByuwQrX</a></p>
<p><b>Compressor summary</b>: Token Distribution Dynamics (TDD) is a method to analyze and manipulate prompts for Large Language Models, using the LM head's interpreting capabilities to estimate input saliency based on distribution dynamics over the vocabulary.</p><hr><h3>Making Pre-trained Language Models Great on Tabular Prediction</h3>
<p><a href='https://openreview.net/forum?id=anzIzGZuLi'>https://openreview.net/forum?id=anzIzGZuLi</a></p>
<p><b>Compressor summary</b>: TP-BERTa is a pre-trained language model that can handle numerical features in tables and achieve high performance in tabular data prediction tasks.</p><hr><h3>Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech</h3>
<p><a href='https://openreview.net/forum?id=ale56Ya59q'>https://openreview.net/forum?id=ale56Ya59q</a></p>
<p><b>Compressor summary</b>: The paper proposes VQScore, a self-supervised metric for evaluating speech quality using vector-quantized-variational autoencoder (VQ-VAE) and domain knowledge, which can also be used for self-supervised speech enhancement with improved robustness.</p><hr><h3>Predictive auxiliary objectives in deep RL mimic learning in the brain</h3>
<p><a href='https://openreview.net/forum?id=agPpmEgf8C'>https://openreview.net/forum?id=agPpmEgf8C</a></p>
<p><b>Compressor summary</b>: The study shows how predictive objectives in deep reinforcement learning improve and stabilize representation learning, mimic brain activity changes, and suggest a new role for the hippocampus as an auxiliary learning system.</p><hr><h3>Circumventing Concept Erasure Methods For Text-To-Image Generative Models</h3>
<p><a href='https://openreview.net/forum?id=ag3o2T51Ht'>https://openreview.net/forum?id=ag3o2T51Ht</a></p>
<p><b>Compressor summary</b>: The authors examine seven methods to remove sensitive content from text-to-image models and show that none of them fully succeed, as they can still retrieve the erased concepts using special learned word embeddings.</p><hr><h3>Entropy Coding of Unordered Data Structures</h3>
<p><a href='https://openreview.net/forum?id=afQuNt3Ruh'>https://openreview.net/forum?id=afQuNt3Ruh</a></p>
<p><b>Compressor summary</b>: Shuffle coding is a new method for compressing unordered sequences of objects using bits-back coding, which works well for various data structures like graphs and molecular data.</p><hr><h3>Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making</h3>
<p><a href='https://openreview.net/forum?id=af2c8EaKl8'>https://openreview.net/forum?id=af2c8EaKl8</a></p>
<p><b>Compressor summary</b>: The authors propose Decision ConvFormer, a new action sequence predictor for offline reinforcement learning, which improves on Decision Transformer by using local convolution filtering to capture local dependencies in trajectories.</p><hr><h3>On the Posterior Distribution in Denoising: Application to Uncertainty Quantification</h3>
<p><a href='https://openreview.net/forum?id=adSGeugiuj'>https://openreview.net/forum?id=adSGeugiuj</a></p>
<p><b>Compressor summary</b>: The paper derives a relation between higher-order moments of the posterior distribution and derivatives of the posterior mean, enabling efficient uncertainty quantification for pre-trained denoisers without training or fine tuning.</p><hr><h3>SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos</h3>
<p><a href='https://openreview.net/forum?id=abL5LJNZ49'>https://openreview.net/forum?id=abL5LJNZ49</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to represent and track state changes in instructional videos for better procedure planning using large language models and cross-modal contrastive learning.</p><hr><h3>Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=aaBnFAyW9O'>https://openreview.net/forum?id=aaBnFAyW9O</a></p>
<p><b>Compressor summary</b>: The paper analyzes the limitations of current diffusion models in denoising tasks, introduces a new model called soft mixture denoising (SMD) that improves their performance, and provides experimental results supporting its effectiveness.</p><hr><h3>Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts</h3>
<p><a href='https://openreview.net/forum?id=aZH1dM3GOX'>https://openreview.net/forum?id=aZH1dM3GOX</a></p>
<p><b>Compressor summary</b>: MOORE is a novel method for representation learning in multi-task reinforcement learning that uses orthogonal subspaces to promote diversity and generate task-specific representations.</p><hr><h3>Horizon-free Reinforcement Learning in Adversarial Linear Mixture MDPs</h3>
<p><a href='https://openreview.net/forum?id=aPNwsJgnZJ'>https://openreview.net/forum?id=aPNwsJgnZJ</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm for adversarial RL that achieves sub-optimal regret using variance-aware weighted least squares and occupancy measure-based policy search.</p><hr><h3>Symbol as Points: Panoptic Symbol Spotting via Point-based Representation</h3>
<p><a href='https://openreview.net/forum?id=aOnUe8ah7j'>https://openreview.net/forum?id=aOnUe8ah7j</a></p>
<p><b>Compressor summary</b>: SymPoint uses point cloud segmentation methods to spot and parse objects and stuff from CAD drawings, outperforming recent state-of-the-art method by a significant margin.</p><hr><h3>The mechanistic basis of data dependence and abrupt learning in an in-context classification task</h3>
<p><a href='https://openreview.net/forum?id=aN4Jf6Cx69'>https://openreview.net/forum?id=aN4Jf6Cx69</a></p>
<p><b>Compressor summary</b>: In-context learning in transformer models depends on distributional properties and a specific sequence of multi-layer operations that enable an induction head with three nested logits.</p><hr><h3>Approximately Piecewise E(3) Equivariant Point Networks</h3>
<p><a href='https://openreview.net/forum?id=aKJEHWmBEf'>https://openreview.net/forum?id=aKJEHWmBEf</a></p>
<p><b>Compressor summary</b>: APEN is a framework that improves point cloud neural networks by approximating their equivariance to local Euclidean transformations, using a compositional design for partition prediction with guaranteed bounds on approximation errors.</p><hr><h3>LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=aIok3ZD9to'>https://openreview.net/forum?id=aIok3ZD9to</a></p>
<p><b>Compressor summary</b>: LLMCarbon is a tool that accurately predicts the carbon emissions of different large language models before training, overcoming limitations of previous methods like mlco2.</p><hr><h3>InsertNeRF: Instilling Generalizability into NeRF with HyperNet Modules</h3>
<p><a href='https://openreview.net/forum?id=aHmNpLlUlb'>https://openreview.net/forum?id=aHmNpLlUlb</a></p>
<p><b>Compressor summary</b>: InsertNeRF is a method that improves NeRF's ability to generalize to new scenes by using HyperNet modules to adapt the model's weights and features.</p><hr><h3>Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data</h3>
<p><a href='https://openreview.net/forum?id=aGH43rjoe4'>https://openreview.net/forum?id=aGH43rjoe4</a></p>
<p><b>Compressor summary</b>: The paper proposes an unsupervised model that extracts shared and independent latent structures from different experimental modalities using Gaussian Process Factor Analysis and Gaussian Process Variational Autoencoders.</p><hr><h3>Interpretable Sparse System Identification: Beyond Recent Deep Learning Techniques on Time-Series Prediction</h3>
<p><a href='https://openreview.net/forum?id=aFWUY3E7ws'>https://openreview.net/forum?id=aFWUY3E7ws</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel interpretable method for time series prediction that uses sparse optimization, Fourier basis, and trend-fluctuation analysis, and beats deep learning without needing training or computational expenses.</p><hr><h3>Object-Centric Learning with Slot Mixture Module</h3>
<p><a href='https://openreview.net/forum?id=aBUidW4Nkd'>https://openreview.net/forum?id=aBUidW4Nkd</a></p>
<p><b>Compressor summary</b>: The paper proposes a learnable clustering method for object-centric architectures that uses Gaussian Mixture Models to represent slots and improve performance on set property prediction tasks.</p><hr><h3>Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis</h3>
<p><a href='https://openreview.net/forum?id=aA33A70IO6'>https://openreview.net/forum?id=aA33A70IO6</a></p>
<p><b>Compressor summary</b>: The study suggests a new way to train language models to avoid generating harmful and toxic responses by analyzing their mistakes and using them as feedback for improvement.</p><hr><h3>Unveiling Options with Neural Network Decomposition</h3>
<p><a href='https://openreview.net/forum?id=a8VETFwcVR'>https://openreview.net/forum?id=a8VETFwcVR</a></p>
<p><b>Compressor summary</b>: The paper presents an algorithm to create reusable sub-policies from neural networks for Markov Decision Processes, which are used as options to speed up learning on related tasks.</p><hr><h3>Understanding prompt engineering may not require rethinking generalization</h3>
<p><a href='https://openreview.net/forum?id=a745RnSFLT'>https://openreview.net/forum?id=a745RnSFLT</a></p>
<p><b>Compressor summary</b>: The paper explains how classical PAC-Bayes bounds can justify the success of zero-shot learning with prompts, despite their potential for overfitting, by showing that prompts' discrete nature and language model prior lead to tight generalization bounds.</p><hr><h3>StructComp: Substituting propagation with Structural Compression in Training Graph Contrastive Learning</h3>
<p><a href='https://openreview.net/forum?id=a4DBEeGfQq'>https://openreview.net/forum?id=a4DBEeGfQq</a></p>
<p><b>Compressor summary</b>: Structural Compression is a framework that trains graph contrastive learning models faster and more efficiently by compressing nodes, reducing computation and memory requirements, and improving performance.</p><hr><h3>iGraphMix: Input Graph Mixup Method for Node Classification</h3>
<p><a href='https://openreview.net/forum?id=a2ljjXeDcE'>https://openreview.net/forum?id=a2ljjXeDcE</a></p>
<p><b>Compressor summary</b>: iGraphMix is a novel node classification method that interpolates input features and labels to generate virtual nodes and graphs, improving generalization performance in graph neural networks.</p><hr><h3>Kalman Filter Online Learning from non-Stationary Data</h3>
<p><a href='https://openreview.net/forum?id=ZzmKEpze8e'>https://openreview.net/forum?id=ZzmKEpze8e</a></p>
<p><b>Compressor summary</b>: The paper proposes a probabilistic Bayesian online learning model that adapts to non-stationary data and quantifies predictive uncertainty using a neural representation and a state space model over linear weights.</p><hr><h3>Clifford Group Equivariant Simplicial Message Passing Networks</h3>
<p><a href='https://openreview.net/forum?id=Zz594UBNOH'>https://openreview.net/forum?id=Zz594UBNOH</a></p>
<p><b>Compressor summary</b>: Clifford Group Equivariant Simplicial Message Passing Networks combine Clifford group-equivariant layers with shared simplicial message passing for steerable, topologically intricate geometric analysis.</p><hr><h3>Learning dynamic representations of the functional connectome in neurobiological networks</h3>
<p><a href='https://openreview.net/forum?id=ZwhHSOHMTM'>https://openreview.net/forum?id=ZwhHSOHMTM</a></p>
<p><b>Compressor summary</b>: The text describes a method to learn dynamic neural networks in live animals by analyzing calcium activity and non-negative tensor factorization.</p><hr><h3>Diffusion Model for Dense Matching</h3>
<p><a href='https://openreview.net/forum?id=Zsfiqpft6K'>https://openreview.net/forum?id=Zsfiqpft6K</a></p>
<p><b>Compressor summary</b>: The paper proposes DiffMatch, a conditional diffusion-based framework for dense matching of paired images, addressing ambiguities by explicitly modeling data and prior terms with a cascaded pipeline.</p><hr><h3>Retrieval-based Disentangled Representation Learning with Natural Language Supervision</h3>
<p><a href='https://openreview.net/forum?id=ZlQRiFmq7Y'>https://openreview.net/forum?id=ZlQRiFmq7Y</a></p>
<p><b>Compressor summary</b>: VDR uses natural language to help learn disentangled representations from data by leveraging linguistic equivalents as proxies for the underlying variation.</p><hr><h3>INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection</h3>
<p><a href='https://openreview.net/forum?id=Zj12nzlQbz'>https://openreview.net/forum?id=Zj12nzlQbz</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to detect when large language models make wrong answers by analyzing their internal states and using a simple metric called EigenScore.</p><hr><h3>From Zero to Turbulence: Generative Modeling for 3D Flow Simulation</h3>
<p><a href='https://openreview.net/forum?id=ZhlwoC1XaN'>https://openreview.net/forum?id=ZhlwoC1XaN</a></p>
<p><b>Compressor summary</b>: The paper presents a novel generative model for 3D turbulent flows that learns the distribution of all possible flow states and can produce realistic samples without relying on an initial condition.</p><hr><h3>Towards the Fundamental Limits of Knowledge Transfer over Finite Domains</h3>
<p><a href='https://openreview.net/forum?id=Zh2iqiOtMt'>https://openreview.net/forum?id=Zh2iqiOtMt</a></p>
<p><b>Compressor summary</b>: The paper studies how different types of information from a teacher can improve the efficiency of transferring knowledge to a probabilistic student classifier.</p><hr><h3>Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets</h3>
<p><a href='https://openreview.net/forum?id=Zc2aIcucwc'>https://openreview.net/forum?id=Zc2aIcucwc</a></p>
<p><b>Compressor summary</b>: The authors introduce seven new molecular machine learning datasets with diverse labels, a graph library for easy modeling, and baseline results that show potential in multi-task and multi-level training for foundation models.</p><hr><h3>BENO: Boundary-embedded Neural Operators for Elliptic PDEs</h3>
<p><a href='https://openreview.net/forum?id=ZZTkLDRmkg'>https://openreview.net/forum?id=ZZTkLDRmkg</a></p>
<p><b>Compressor summary</b>: Boundary-Embedded Neural Operators (BENO) is a new technique that solves elliptic PDEs with complex geometries and boundary conditions more efficiently using neural networks and Transformer encoder.</p><hr><h3>LipVoicer: Generating Speech from Silent Videos Guided by Lip Reading</h3>
<p><a href='https://openreview.net/forum?id=ZZCPSC5OgD'>https://openreview.net/forum?id=ZZCPSC5OgD</a></p>
<p><b>Compressor summary</b>: LipVoicer is a novel lip-to-speech method that generates high-quality, intelligible speech from silent videos by incorporating the text modality and outperforming multiple baselines on in-the-wild datasets.</p><hr><h3>Bayesian Bi-clustering of Neural Spiking Activity with Latent Structures</h3>
<p><a href='https://openreview.net/forum?id=ZYm1Ql6udy'>https://openreview.net/forum?id=ZYm1Ql6udy</a></p>
<p><b>Compressor summary</b>: The paper presents a bi-clustering method to analyze neural spiking activity in multiple brain regions, considering both latent trajectories and spatial-temporal clustering structures, using a non-parametric model and an MCMC algorithm.</p><hr><h3>Effective Data Augmentation With Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=ZWzUA9zeAg'>https://openreview.net/forum?id=ZWzUA9zeAg</a></p>
<p><b>Compressor summary</b>: Our method uses text-to-image diffusion models to semantically edit images for data augmentation, improving performance on few-shot classification and weed recognition.</p><hr><h3>Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data</h3>
<p><a href='https://openreview.net/forum?id=ZWyZeqE928'>https://openreview.net/forum?id=ZWyZeqE928</a></p>
<p><b>Compressor summary</b>: FunBaT is a novel tensor model that extends Tucker decomposition to handle continuous-indexed data by using functional priors and efficient inference algorithms.</p><hr><h3>DMBP: Diffusion model based predictor for robust offline reinforcement learning against state observation perturbations</h3>
<p><a href='https://openreview.net/forum?id=ZULjcYLWKe'>https://openreview.net/forum?id=ZULjcYLWKe</a></p>
<p><b>Compressor summary</b>: The paper proposes DMBP, a framework that uses conditional diffusion models to recover actual states in offline RL and improve robustness against state observation perturbations.</p><hr><h3>How to Fine-Tune Vision Models with SGD</h3>
<p><a href='https://openreview.net/forum?id=ZTssMmhC2X'>https://openreview.net/forum?id=ZTssMmhC2X</a></p>
<p><b>Compressor summary</b>: SGD outperforms AdamW for fine-tuning large neural networks in computer vision when the embedding layer is frozen and memory usage is reduced.</p><hr><h3>Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps</h3>
<p><a href='https://openreview.net/forum?id=ZSD3MloKe6'>https://openreview.net/forum?id=ZSD3MloKe6</a></p>
<p><b>Compressor summary</b>: The paper proposes Time-Shift Sampler, a novel method to reduce exposure bias in Diffusion Probabilistic Models without retraining, by selecting better time steps for inference.</p><hr><h3>Making Retrieval-Augmented Language Models Robust to Irrelevant Context</h3>
<p><a href='https://openreview.net/forum?id=ZS4m74kZpH'>https://openreview.net/forum?id=ZS4m74kZpH</a></p>
<p><b>Compressor summary</b>: The authors analyze cases when retrieval-augmented language models reduce accuracy and propose two methods to improve their performance by filtering out irrelevant passages and fine-tuning the model with a mix of relevant and irrelevant contexts.</p><hr><h3>Mean Field Theory in Deep Metric Learning</h3>
<p><a href='https://openreview.net/forum?id=ZPdZLlNXSm'>https://openreview.net/forum?id=ZPdZLlNXSm</a></p>
<p><b>Compressor summary</b>: The paper introduces mean field theory from statistical physics to create new loss functions for deep metric learning, reducing training complexity and improving image retrieval performance.</p><hr><h3>Efficient Distributed Training with Full Communication-Computation Overlap</h3>
<p><a href='https://openreview.net/forum?id=ZO5cn4IfaN'>https://openreview.net/forum?id=ZO5cn4IfaN</a></p>
<p><b>Compressor summary</b>: CO2 is a new approach for large language model training that uses local updating and asynchronous communication to achieve 100% scalability on clusters with limited bandwidth, and it works well on various tasks in computer vision and natural language processing.</p><hr><h3>PanoDiffusion: 360-degree Panorama Outpainting via Diffusion</h3>
<p><a href='https://openreview.net/forum?id=ZNzDXDFZ0B'>https://openreview.net/forum?id=ZNzDXDFZ0B</a></p>
<p><b>Compressor summary</b>: PanoDiffusion is a new model that uses latent diffusion and progressive camera rotations to generate high-quality 360-degree RGB and depth panoramas from narrow field of view images.</p><hr><h3>Learning semilinear neural operators: A unified recursive framework for prediction and data assimilation.</h3>
<p><a href='https://openreview.net/forum?id=ZMv6zKYYUs'>https://openreview.net/forum?id=ZMv6zKYYUs</a></p>
<p><b>Compressor summary</b>: The paper proposes a learning-based method for fast and accurate predictions and data assimilation of spatio-temporal PDEs using Neural Operators and nonlinear observers in function spaces.</p><hr><h3>A Stochastic Centering Framework for Improving Calibration in Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=ZL6yd6N1S2'>https://openreview.net/forum?id=ZL6yd6N1S2</a></p>
<p><b>Compressor summary</b>: The paper proposes G-$\Delta$UQ, a framework to improve intrinsic uncertainty estimates of graph neural networks (GNNs) using graph anchoring strategies under distribution shifts.</p><hr><h3>A Lightweight Method for Tackling Unknown Participation Statistics in Federated Averaging</h3>
<p><a href='https://openreview.net/forum?id=ZKEuFKfCKA'>https://openreview.net/forum?id=ZKEuFKfCKA</a></p>
<p><b>Compressor summary</b>: The paper proposes FedAU, a lightweight method for federated learning that adapts aggregation weights based on client participation history to handle diverse participation statistics and improve performance.</p><hr><h3>Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning</h3>
<p><a href='https://openreview.net/forum?id=ZGNWW7xZ6Q'>https://openreview.net/forum?id=ZGNWW7xZ6Q</a></p>
<p><b>Compressor summary</b>: The paper proposes a method called reasoning on graphs (RoG) that combines large language models and knowledge graphs for faithful and interpretable reasoning, using relation paths as plans to retrieve valid reasoning paths from the graphs.</p><hr><h3>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</h3>
<p><a href='https://openreview.net/forum?id=ZG3RaNIsO8'>https://openreview.net/forum?id=ZG3RaNIsO8</a></p>
<p><b>Compressor summary</b>: EvoPrompt is a novel framework for discrete prompt optimization using evolutionary algorithms and large language models to improve task performance on various natural language processing datasets.</p><hr><h3>Skill or Luck? Return Decomposition via Advantage Functions</h3>
<p><a href='https://openreview.net/forum?id=ZFMiHfZwIf'>https://openreview.net/forum?id=ZFMiHfZwIf</a></p>
<p><b>Compressor summary</b>: The paper proposes Off-policy DAE, a method that learns from off-policy data by decomposing return into skill and luck components, without using importance sampling or truncation.</p><hr><h3>Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization</h3>
<p><a href='https://openreview.net/forum?id=ZEZ0CPmoSI'>https://openreview.net/forum?id=ZEZ0CPmoSI</a></p>
<p><b>Compressor summary</b>: The paper presents new algorithms that use a matrix stepsize to optimize non-convex problems and achieve faster convergence than scalar methods, along with theoretical and empirical results.</p><hr><h3>Improving Language Models with Advantage-based Offline Policy Gradients</h3>
<p><a href='https://openreview.net/forum?id=ZDGKPbF0VQ'>https://openreview.net/forum?id=ZDGKPbF0VQ</a></p>
<p><b>Compressor summary</b>: A-LoL is an offline RL method for finetuning LMs that uses sequence-level rewards and value estimates to filter low-quality data and improve diversity, safety, and helpfulness of generated texts.</p><hr><h3>Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning</h3>
<p><a href='https://openreview.net/forum?id=Z9AZsU1Tju'>https://openreview.net/forum?id=Z9AZsU1Tju</a></p>
<p><b>Compressor summary</b>: The ITHP model uses information bottleneck to create a compact information flow in multimodal learning, outperforming state-of-the-art benchmarks and achieving human-level performance on sentiment classification.</p><hr><h3>Addressing Signal Delay in Deep Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=Z8UfDs4J46'>https://openreview.net/forum?id=Z8UfDs4J46</a></p>
<p><b>Compressor summary</b>: The paper explores the impact of signal delay in deep reinforcement learning, formalizes delayed-observation Markov decision processes, and proposes effective strategies to overcome related challenges for continuous robotic control tasks.</p><hr><h3>Trajeglish: Learning the Language of Driving Scenarios</h3>
<p><a href='https://openreview.net/forum?id=Z59Rb5bPPP'>https://openreview.net/forum?id=Z59Rb5bPPP</a></p>
<p><b>Compressor summary</b>: The authors present a language modeling approach to generate realistic driving scenarios with partial autonomy and intra-timestep interaction, and evaluate it on two datasets.</p><hr><h3>Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel</h3>
<p><a href='https://openreview.net/forum?id=YrXHEb2qMb'>https://openreview.net/forum?id=YrXHEb2qMb</a></p>
<p><b>Compressor summary</b>: The text proposes using MMD with negative distance kernel for posterior sampling and conditional generative modelling, showing its advantages in computation and error bound, and demonstrating its effectiveness in various numerical examples.</p><hr><h3>An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression</h3>
<p><a href='https://openreview.net/forum?id=YrTI2Zu0dd'>https://openreview.net/forum?id=YrTI2Zu0dd</a></p>
<p><b>Compressor summary</b>: The paper analyzes how different sample sizes affect the cost of overfitting in noisy kernel ridge regression and classifies overfitting scenarios as benign, tempered, or catastrophic.</p><hr><h3>Emerging Pixel-level Semantic Knowledge in Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=YqyTXmF8Y2'>https://openreview.net/forum?id=YqyTXmF8Y2</a></p>
<p><b>Compressor summary</b>: The text describes a method to build an image segmentor using semantic knowledge from Stable Diffusion without additional supervision.</p><hr><h3>AUC-CL: A Batchsize-Robust Framework for Self-Supervised Contrastive Representation Learning</h3>
<p><a href='https://openreview.net/forum?id=YgMdDQB09U'>https://openreview.net/forum?id=YgMdDQB09U</a></p>
<p><b>Compressor summary</b>: The paper proposes a new contrastive learning approach that performs well with smaller batch sizes and is robust to stochastic optimization, using AUC-maximization for embedding space arrangements.</p><hr><h3>Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video</h3>
<p><a href='https://openreview.net/forum?id=Yen1lGns2o'>https://openreview.net/forum?id=Yen1lGns2o</a></p>
<p><b>Compressor summary</b>: The authors present a self-supervised video pretraining method that leverages tracking and attention to learn from unannotated first-person videos, such as the new "Walking Tours" dataset.</p><hr><h3>Adversarial Causal Bayesian Optimization</h3>
<p><a href='https://openreview.net/forum?id=YcW8i9VCf5'>https://openreview.net/forum?id=YcW8i9VCf5</a></p>
<p><b>Compressor summary</b>: The paper introduces ACBO, a generalization of CBO that considers multiple agents or external events intervening on the system, and proposes CBO-MW, an algorithm with bounded regret that combines online learning and causal modeling.</p><hr><h3>BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference</h3>
<p><a href='https://openreview.net/forum?id=YcM6ofShwY'>https://openreview.net/forum?id=YcM6ofShwY</a></p>
<p><b>Compressor summary</b>: BayesDiff is a method that uses Bayesian inference to estimate pixel-wise uncertainty in diffusion models, which can help filter low-quality images and improve image generation.</p><hr><h3>Improving Intrinsic Exploration by Creating Stationary Objectives</h3>
<p><a href='https://openreview.net/forum?id=YbZxT0SON4'>https://openreview.net/forum?id=YbZxT0SON4</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework to transform non-stationary intrinsic rewards in reinforcement learning into stationary ones using state augmentations, improving agent performance in various exploration problems.</p><hr><h3>Composed Image Retrieval with Text Feedback via Multi-grained Uncertainty Regularization</h3>
<p><a href='https://openreview.net/forum?id=Yb5KvPkKQg'>https://openreview.net/forum?id=Yb5KvPkKQg</a></p>
<p><b>Compressor summary</b>: The text introduces a unified learning approach for composed image retrieval with text feedback that considers coarse- and fine-grained retrieval by modeling multi-grained uncertainty, improving the recall rate on three datasets.</p><hr><h3>Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks</h3>
<p><a href='https://openreview.net/forum?id=YZrg56G0JV'>https://openreview.net/forum?id=YZrg56G0JV</a></p>
<p><b>Compressor summary</b>: This paper shows how multitask reinforcement learning (MTRL) with myopic exploration can be more efficient when tasks are diverse and related, and validates this theory with experiments on synthetic robotics.</p><hr><h3>Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning</h3>
<p><a href='https://openreview.net/forum?id=YR3ETaElNK'>https://openreview.net/forum?id=YR3ETaElNK</a></p>
<p><b>Compressor summary</b>: The paper presents an efficient method to make large language models handle multiple modalities by tuning LayerNorm layers, which leads to significant performance improvements and reduced resource usage.</p><hr><h3>In-Context Learning Learns Label Relationships but Is Not Conventional Learning</h3>
<p><a href='https://openreview.net/forum?id=YPIA7bgd5y'>https://openreview.net/forum?id=YPIA7bgd5y</a></p>
<p><b>Compressor summary</b>: This paper investigates how Large Language Models use label information when learning new tasks in context, revealing their capabilities, limitations, and biases.</p><hr><h3>Conditional Variational Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=YOKnEkIuoi'>https://openreview.net/forum?id=YOKnEkIuoi</a></p>
<p><b>Compressor summary</b>: The authors propose a new method for learning the variance schedule of diffusion models during training, which improves the performance and adaptability of inverse problems and avoids the need for fine-tuning by experimentation.</p><hr><h3>Towards Poisoning Fair Representations</h3>
<p><a href='https://openreview.net/forum?id=YLJs4mKJCF'>https://openreview.net/forum?id=YLJs4mKJCF</a></p>
<p><b>Compressor summary</b>: The text introduces a new data poisoning attack on fair representation learning, which can produce unfair representations that contain demographic information by injecting crafted samples into training data.</p><hr><h3>Symmetric Mean-field Langevin Dynamics for Distributional Minimax Problems</h3>
<p><a href='https://openreview.net/forum?id=YItWKZci78'>https://openreview.net/forum?id=YItWKZci78</a></p>
<p><b>Compressor summary</b>: The paper presents two algorithms for minimax optimization over probability distributions, MFL-AG and MFL-ABR, which use mean-field Langevin dynamics and have symmetric and convergent updates.</p><hr><h3>Parsing neural dynamics with infinite recurrent switching linear dynamical systems</h3>
<p><a href='https://openreview.net/forum?id=YIls9HEa52'>https://openreview.net/forum?id=YIls9HEa52</a></p>
<p><b>Compressor summary</b>: The irSLDS model extends the rSLDS model to analyze trial-varying neural activity data by incorporating latent geometry, flexible state cardinality, and dynamical sufficient statistics.</p><hr><h3>Active Test-Time Adaptation: Theoretical Analyses and An Algorithm</h3>
<p><a href='https://openreview.net/forum?id=YHUGlwTzFB'>https://openreview.net/forum?id=YHUGlwTzFB</a></p>
<p><b>Compressor summary</b>: ATTA is a novel test-time adaptation method that uses active learning to improve performance under distribution shifts, with theoretical guarantees and efficient sample selection techniques.</p><hr><h3>TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting</h3>
<p><a href='https://openreview.net/forum?id=YH5w12OUuU'>https://openreview.net/forum?id=YH5w12OUuU</a></p>
<p><b>Compressor summary</b>: TEMPO uses GPT-like architecture to learn time series representations, improving accuracy and adapting to non-stationary data.</p><hr><h3>Differentially Private Synthetic Data via Foundation Model APIs 1: Images</h3>
<p><a href='https://openreview.net/forum?id=YEhQs8POIo'>https://openreview.net/forum?id=YEhQs8POIo</a></p>
<p><b>Compressor summary</b>: The paper proposes Private Evolution (PE), an API-based framework that generates differentially private synthetic data without training models, and shows its effectiveness on synthetic images and large foundation models.</p><hr><h3>Implicit Gaussian process representation of vector fields over arbitrary latent manifolds</h3>
<p><a href='https://openreview.net/forum?id=YEPlTU5mZC'>https://openreview.net/forum?id=YEPlTU5mZC</a></p>
<p><b>Compressor summary</b>: RVGP is a novel method for learning vector signals over latent Riemannian manifolds, which can super-resolve and inpaint vector fields while preserving singularities and reconstruct high-density neural dynamics from low-density EEG recordings.</p><hr><h3>Training Diffusion Models with Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=YCWjhGrJFD'>https://openreview.net/forum?id=YCWjhGrJFD</a></p>
<p><b>Compressor summary</b>: The paper proposes reinforcement learning methods to optimize diffusion models for downstream objectives like image quality and drug effectiveness, outperforming reward-weighted likelihood approaches.</p><hr><h3>Leveraging augmented-Lagrangian techniques for differentiating over infeasible quadratic programs in machine learning</h3>
<p><a href='https://openreview.net/forum?id=YCPDFfmkFr'>https://openreview.net/forum?id=YCPDFfmkFr</a></p>
<p><b>Compressor summary</b>: The paper proposes a unified approach to differentiate both feasible and infeasible convex quadratic programming layers in neural networks, enabling faster computations and improved performance in some tasks.</p><hr><h3>Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning</h3>
<p><a href='https://openreview.net/forum?id=Y9t7MqZtCR'>https://openreview.net/forum?id=Y9t7MqZtCR</a></p>
<p><b>Compressor summary</b>: SWAMP is a simple modification of IMP that achieves better performance by training multiple sparse models and weight averaging them, resulting in faster inference speeds and less memory demand for modern neural networks.</p><hr><h3>GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction</h3>
<p><a href='https://openreview.net/forum?id=Y3wpuxd7u9'>https://openreview.net/forum?id=Y3wpuxd7u9</a></p>
<p><b>Compressor summary</b>: GoLLIE is a model that follows annotation guidelines to improve its performance on unseen information extraction tasks.</p><hr><h3>DecompOpt: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization</h3>
<p><a href='https://openreview.net/forum?id=Y3BbxvAQS9'>https://openreview.net/forum?id=Y3BbxvAQS9</a></p>
<p><b>Compressor summary</b>: DecompOpt is a new structure-based method that combines optimization and conditional diffusion models to generate novel ligands with desired properties, such as high binding affinity and easy synthesis, by decomposing them into substructures for fine-grained control.</p><hr><h3>Balancing Act: Sparse Models with Constrained Disparate Impact</h3>
<p><a href='https://openreview.net/forum?id=Xz13DtbOVW'>https://openreview.net/forum?id=Xz13DtbOVW</a></p>
<p><b>Compressor summary</b>: Our method constrains the accuracy change for each sub-group when pruning models, ensuring fairness and interpretability while scaling well.</p><hr><h3>Quantifying and Enhancing Multi-modal Robustness with Modality Preference</h3>
<p><a href='https://openreview.net/forum?id=XyrB1Ay44j'>https://openreview.net/forum?id=XyrB1Ay44j</a></p>
<p><b>Compressor summary</b>: The paper explores the limitations of multi-modal models' robustness, identifies essential components for achieving higher robustness, and proposes a new training procedure called CRMT that significantly improves performance and robustness.</p><hr><h3>Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing</h3>
<p><a href='https://openreview.net/forum?id=XwiA1nDahv'>https://openreview.net/forum?id=XwiA1nDahv</a></p>
<p><b>Compressor summary</b>: SmoothECE is a well-behaved calibration measure that fixes common flaws in binning and ECE by smoothing observations using an RBF kernel before computing ECE, and provides a Python package for measuring and plotting calibration.</p><hr><h3>Continual Learning on a Diet:  Learning from Sparsely Labeled Streams Under Constrained Computation</h3>
<p><a href='https://openreview.net/forum?id=Xvfz8NHmCj'>https://openreview.net/forum?id=Xvfz8NHmCj</a></p>
<p><b>Compressor summary</b>: The paper introduces a realistic Continual Learning setting with limited resources and proposes a simple but effective method called DietCL that uses both labeled and unlabeled data efficiently to outperform existing methods.</p><hr><h3>Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking</h3>
<p><a href='https://openreview.net/forum?id=XsHqr9dEGH'>https://openreview.net/forum?id=XsHqr9dEGH</a></p>
<p><b>Compressor summary</b>: The paper investigates the grokking phenomenon in neural networks and shows that it is caused by a shift from early to late phase implicit biases during training.</p><hr><h3>Spatio-Temporal Approximation: A Training-Free SNN Conversion for Transformers</h3>
<p><a href='https://openreview.net/forum?id=XrunSYwoLr'>https://openreview.net/forum?id=XrunSYwoLr</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to convert pretrained Transformers into spiking neural networks by approximating their non-linear operations in both time and space dimensions.</p><hr><h3>Adding 3D Geometry Control to Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=XlkN11Xj6J'>https://openreview.net/forum?id=XlkN11Xj6J</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to incorporate 3D geometry control into diffusion models by using visual prompts from rendered images, enabling better performance in various vision tasks.</p><hr><h3>DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text</h3>
<p><a href='https://openreview.net/forum?id=Xlayxj2fWp'>https://openreview.net/forum?id=Xlayxj2fWp</a></p>
<p><b>Compressor summary</b>: The authors propose DNA-GPT, a training-free detection strategy that analyzes the differences between original and regenerated text by LLMs to distinguish machine-generated from human-written text with high performance and explainability.</p><hr><h3>The importance of feature preprocessing for differentially private linear optimization</h3>
<p><a href='https://openreview.net/forum?id=XlTDBZFXWp'>https://openreview.net/forum?id=XlTDBZFXWp</a></p>
<p><b>Compressor summary</b>: DPSGD is not sufficient for differentially private optimization without feature preprocessing, and DPSGD-F combines it with feature preprocessing to reduce privacy errors for classification tasks.</p><hr><h3>Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from Offline Data</h3>
<p><a href='https://openreview.net/forum?id=Xkf2EBj4w3'>https://openreview.net/forum?id=Xkf2EBj4w3</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-supervised RL method for robotic systems that uses contrastive learning and shows its effectiveness on simulated and real-world image-based manipulation tasks.</p><hr><h3>MogaNet: Multi-order Gated Aggregation Network</h3>
<p><a href='https://openreview.net/forum?id=XhYWgjqCrV'>https://openreview.net/forum?id=XhYWgjqCrV</a></p>
<p><b>Compressor summary</b>: MogaNet is a new family of modern ConvNets that efficiently gathers and contextualizes discriminative features for improved computer vision tasks with better complexity-performance trade-offs.</p><hr><h3>Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns</h3>
<p><a href='https://openreview.net/forum?id=XVhm3X8Fum'>https://openreview.net/forum?id=XVhm3X8Fum</a></p>
<p><b>Compressor summary</b>: Stack attention, an attention operator incorporating stacks, enhances transformers' ability to recognize hierarchical patterns of arbitrary nesting depth in both natural language and context-free languages without syntactic supervision.</p><hr><h3>Quantifying the Plausibility of Context Reliance in Neural Machine Translation</h3>
<p><a href='https://openreview.net/forum?id=XTHfNGI3zT'>https://openreview.net/forum?id=XTHfNGI3zT</a></p>
<p><b>Compressor summary</b>: PECoRe is an interpretability framework that quantifies how language models use context for translation, comparing their rationales with human annotations and identifying context-mediated predictions.</p><hr><h3>Consistent algorithms for multi-label classification with macro-at-$k$ metrics</h3>
<p><a href='https://openreview.net/forum?id=XOnya9gSdF'>https://openreview.net/forum?id=XOnya9gSdF</a></p>
<p><b>Compressor summary</b>: The paper studies how to optimize complex metrics in multi-label classification under population utility framework and proposes a practical learning algorithm based on Frank-Wolfe method.</p><hr><h3>Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers</h3>
<p><a href='https://openreview.net/forum?id=XNa6r6ZjoB'>https://openreview.net/forum?id=XNa6r6ZjoB</a></p>
<p><b>Compressor summary</b>: The text proposes an extension of Transformers called the Abstractor that enables relational reasoning through a novel module with relational cross-attention, improving abstraction, generalization, and sample efficiency on various tasks.</p><hr><h3>COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits</h3>
<p><a href='https://openreview.net/forum?id=XN6ZPINdSg'>https://openreview.net/forum?id=XN6ZPINdSg</a></p>
<p><b>Compressor summary</b>: COLEP is a robust conformal prediction framework using probabilistic circuits that provides certified coverage guarantees against adversarial perturbations and outperforms single models in various datasets.</p><hr><h3>Adversarial Training Should Be Cast as a Non-Zero-Sum Game</h3>
<p><a href='https://openreview.net/forum?id=XJ9vjEAqbx'>https://openreview.net/forum?id=XJ9vjEAqbx</a></p>
<p><b>Compressor summary</b>: The paper proposes a new approach to train neural networks that are more robust against adversarial attacks by using a non-zero-sum bilevel formulation of adversarial training.</p><hr><h3>InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image</h3>
<p><a href='https://openreview.net/forum?id=XIxhINXtQk'>https://openreview.net/forum?id=XIxhINXtQk</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel end-to-end framework for instructed 3D-aware portrait editing from a single open-world image using conditional latent 3D diffusion and identity consistency module.</p><hr><h3>Idempotent Generative Network</h3>
<p><a href='https://openreview.net/forum?id=XIaS66XkNA'>https://openreview.net/forum?id=XIaS66XkNA</a></p>
<p><b>Compressor summary</b>: The new approach trains a neural network to be idempotent, mapping source noise to target images, and can generate output in one step, refine with sequential applications, and handle corrupted inputs.</p><hr><h3>Debiasing Algorithm through Model Adaptation</h3>
<p><a href='https://openreview.net/forum?id=XIZEFyVGC9'>https://openreview.net/forum?id=XIZEFyVGC9</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method (DAMA) to detect and reduce gender bias in large language models by modifying certain layers based on causal analysis.</p><hr><h3>GRANDE: Gradient-Based Decision Tree Ensembles</h3>
<p><a href='https://openreview.net/forum?id=XEFWBxi075'>https://openreview.net/forum?id=XEFWBxi075</a></p>
<p><b>Compressor summary</b>: The paper introduces GRANDE, a novel gradient-based method for learning hard decision tree ensembles on tabular data, which achieves better performance than existing methods.</p><hr><h3>Interpretable Diffusion via Information Decomposition</h3>
<p><a href='https://openreview.net/forum?id=X6tNkN6ate'>https://openreview.net/forum?id=X6tNkN6ate</a></p>
<p><b>Compressor summary</b>: Denoising diffusion models can learn complex relationships between words and pixels in an image, and these relationships can be analyzed using information decomposition methods.</p><hr><h3>Training-free Multi-objective Diffusion Model for 3D Molecule Generation</h3>
<p><a href='https://openreview.net/forum?id=X41c4uB4k0'>https://openreview.net/forum?id=X41c4uB4k0</a></p>
<p><b>Compressor summary</b>: The text presents a method to generate 3D molecules with multiple desired properties using off-the-shelf models and probabilistic graphs, outperforming trained diffusion models.</p><hr><h3>Visual Data-Type Understanding does not emerge from scaling Vision-Language Models</h3>
<p><a href='https://openreview.net/forum?id=WyEdX2R4er'>https://openreview.net/forum?id=WyEdX2R4er</a></p>
<p><b>Compressor summary</b>: This paper introduces a new vision-language model task called Visual Data-Type Identification and evaluates 39 models on it, revealing their weaknesses in recognizing simple image manipulations.</p><hr><h3>Theoretical Understanding of Learning from Adversarial Perturbations</h3>
<p><a href='https://openreview.net/forum?id=Ww9rWUAcdo'>https://openreview.net/forum?id=Ww9rWUAcdo</a></p>
<p><b>Compressor summary</b>: The study shows how adversarial examples contain hidden data features that help neural networks learn and generalize, even when trained on mislabeled samples.</p><hr><h3>QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=WvFoJccpo8'>https://openreview.net/forum?id=WvFoJccpo8</a></p>
<p><b>Compressor summary</b>: The paper introduces QA-LoRA, an algorithm that makes large language models more efficient, accurate, and scalable by balancing quantization and adaptation.</p><hr><h3>Harnessing Overlap in Blockwise Transformers for Near-Infinite Context</h3>
<p><a href='https://openreview.net/forum?id=WsRHpHH4s0'>https://openreview.net/forum?id=WsRHpHH4s0</a></p>
<p><b>Compressor summary</b>: Ring Attention is a novel method that allows Transformers to handle long sequences by distributing them across multiple devices and overlapping communication through blockwise attention computation, enabling efficient training and inference on sequences longer than 100 million tokens.</p><hr><h3>A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data</h3>
<p><a href='https://openreview.net/forum?id=WjRPZsfeBO'>https://openreview.net/forum?id=WjRPZsfeBO</a></p>
<p><b>Compressor summary</b>: The paper analyzes Wasserstein Autoencoders' statistical guarantees and shows they can learn data distributions when network architectures are well designed, focusing on convergence rates that depend only on the intrinsic dimension of the data.</p><hr><h3>Adaptive Self-training Framework for Fine-grained Scene Graph Generation</h3>
<p><a href='https://openreview.net/forum?id=WipsLtH77t'>https://openreview.net/forum?id=WipsLtH77t</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-training framework for scene graph generation that uses class-specific adaptive thresholding with momentum to assign pseudo-labels to unannotated triplets and improves performance on fine-grained predicate classes.</p><hr><h3>Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting</h3>
<p><a href='https://openreview.net/forum?id=WhgB5sispV'>https://openreview.net/forum?id=WhgB5sispV</a></p>
<p><b>Compressor summary</b>: The paper presents a novel 4D primitive-based approach to synthesize diverse views of dynamic scenes from 2D images, with improved simplicity, flexibility, efficiency, and quality.</p><hr><h3>Light Schrdinger Bridge</h3>
<p><a href='https://openreview.net/forum?id=WhZoCLRWYJ'>https://openreview.net/forum?id=WhZoCLRWYJ</a></p>
<p><b>Compressor summary</b>: The paper proposes a fast and simple Schrodinger Bridges solver using sum-exp quadratic functions and log-Schrodinger potentials as energy functions, making it suitable for solving moderate-dimensional problems on CPU without complex optimization or hyperparameter selection.</p><hr><h3>Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback</h3>
<p><a href='https://openreview.net/forum?id=WesY0H9ghM'>https://openreview.net/forum?id=WesY0H9ghM</a></p>
<p><b>Compressor summary</b>: Uni-RLHF is a system for reinforcement learning with human feedback that provides a complete workflow, large-scale datasets, and modular baselines to facilitate progress in practical problems.</p><hr><h3>Quasi-Monte Carlo for 3D Sliced Wasserstein</h3>
<p><a href='https://openreview.net/forum?id=Wd47f7HEXg'>https://openreview.net/forum?id=Wd47f7HEXg</a></p>
<p><b>Compressor summary</b>: Quasi-Monte Carlo methods improve the Sliced Wasserstein distance approximation in 3D tasks by providing a better class of empirical SW and unbiased estimators.</p><hr><h3>Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram</h3>
<p><a href='https://openreview.net/forum?id=WcOohbsF4H'>https://openreview.net/forum?id=WcOohbsF4H</a></p>
<p><b>Compressor summary</b>: The paper introduces ST-MEM, a self-supervised learning method for ECG data that reconstructs masked 12-lead ECG data and learns spatio-temporal features, improving arrhythmia classification performance.</p><hr><h3>Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models</h3>
<p><a href='https://openreview.net/forum?id=WbWtOYIzIK'>https://openreview.net/forum?id=WbWtOYIzIK</a></p>
<p><b>Compressor summary</b>: Knowledge Card is a framework to update large language models with domain-specific facts, improving their performance on various tasks.</p><hr><h3>Latent Intuitive Physics: Learning to Transfer Hidden Physics from a 3D Video</h3>
<p><a href='https://openreview.net/forum?id=WZu4gUGN13'>https://openreview.net/forum?id=WZu4gUGN13</a></p>
<p><b>Compressor summary</b>: The text introduces latent intuitive physics, a method to simulate fluids from 3D videos without knowing their physical properties, using a learned neural renderer and a parametrized prior learner.</p><hr><h3>An operator preconditioning perspective on training in physics-informed machine learning</h3>
<p><a href='https://openreview.net/forum?id=WWlxFtR5sV'>https://openreview.net/forum?id=WWlxFtR5sV</a></p>
<p><b>Compressor summary</b>: The paper explores how the difficulty of training physics-informed machine learning models depends on the conditioning of a specific differential operator, and proposes preconditioning methods to improve training.</p><hr><h3>Learning to Embed Time Series Patches Independently</h3>
<p><a href='https://openreview.net/forum?id=WS7GuBDFa2'>https://openreview.net/forum?id=WS7GuBDFa2</a></p>
<p><b>Compressor summary</b>: The paper proposes a simpler and more efficient method for time series representation learning that uses patch reconstruction, independent embedding of patches, and contrastive learning, outperforming masked time series modeling with Transformers.</p><hr><h3>Modeling state-dependent communication between brain regions with switching nonlinear dynamical systems</h3>
<p><a href='https://openreview.net/forum?id=WQwV7Y8qwa'>https://openreview.net/forum?id=WQwV7Y8qwa</a></p>
<p><b>Compressor summary</b>: MR-SDS is a nonlinear state space model that decomposes neural dynamics into local and cross-communication components in the latent space, accounting for sensory inputs, history effects, and heterogeneity, and can accurately recover latent trajectories, vector fields, and cross-region communication profiles in multi-region neural datasets.</p><hr><h3>Mind Your Augmentation: The Key to Decoupling Dense Self-Supervised Learning</h3>
<p><a href='https://openreview.net/forum?id=WQYHbr36Fo'>https://openreview.net/forum?id=WQYHbr36Fo</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel augmentation method (RCC) and decoupling branch to address the coupling phenomenon in dense self-supervised learning, improving feature disentanglement among objects.</p><hr><h3>Fine-Tuning Language Models for Factuality</h3>
<p><a href='https://openreview.net/forum?id=WPZ2yPag4K'>https://openreview.net/forum?id=WPZ2yPag4K</a></p>
<p><b>Compressor summary</b>: The authors propose a method to improve the factual accuracy of language models by fine-tuning them using automated criteria or preference rankings, reducing errors in open-ended text generation settings.</p><hr><h3>TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields</h3>
<p><a href='https://openreview.net/forum?id=WOiOzHG2zD'>https://openreview.net/forum?id=WOiOzHG2zD</a></p>
<p><b>Compressor summary</b>: The TextField3D model introduces dynamic noise to latent space of text prompts to improve 3D representation learning and enable open-vocabulary generation with large vocabulary, text consistency, and low latency.</p><hr><h3>Improved Techniques for Training Consistency Models</h3>
<p><a href='https://openreview.net/forum?id=WNzy9bRDvG'>https://openreview.net/forum?id=WNzy9bRDvG</a></p>
<p><b>Compressor summary</b>: The paper proposes improved techniques for training consistency models that achieve higher quality samples without distillation or bias, leading to significant improvements over prior approaches.</p><hr><h3>Lipschitz Singularities in Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=WNkW0cOwiz'>https://openreview.net/forum?id=WNkW0cOwiz</a></p>
<p><b>Compressor summary</b>: The paper studies a problem in diffusion models where they have infinite Lipschitz near zero timesteps, which affects their stability and accuracy, and proposes a novel approach called E-TSDM to mitigate it.</p><hr><h3>AUGCAL: Improving Sim2Real Adaptation by Uncertainty Calibration on Augmented Synthetic Images</h3>
<p><a href='https://openreview.net/forum?id=WNQjN5HzXt'>https://openreview.net/forum?id=WNQjN5HzXt</a></p>
<p><b>Compressor summary</b>: AUGCAL is a method that improves the performance of models trained on synthetic data by reducing miscalibration and overconfidence in real-world applications.</p><hr><h3>On the Role of Discrete Tokenization in Visual Representation Learning</h3>
<p><a href='https://openreview.net/forum?id=WNLAkjUm19'>https://openreview.net/forum?id=WNLAkjUm19</a></p>
<p><b>Compressor summary</b>: This study investigates how discrete visual tokens enhance masked image modeling and proposes a new metric and tokenizer design to improve generalization performance.</p><hr><h3>Learning Optimal Contracts: How to Exploit Small Action Spaces</h3>
<p><a href='https://openreview.net/forum?id=WKuimaBj4I'>https://openreview.net/forum?id=WKuimaBj4I</a></p>
<p><b>Compressor summary</b>: The paper proposes an algorithm for designing optimal payment schemes in multi-round principal-agent problems with small agent action spaces, achieving better regret bounds than existing methods.</p><hr><h3>GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries</h3>
<p><a href='https://openreview.net/forum?id=WIzzXCVYiH'>https://openreview.net/forum?id=WIzzXCVYiH</a></p>
<p><b>Compressor summary</b>: The paper proposes a method called GNNBoundary to understand the decision boundaries of graph neural networks (GNNs) by generating near-boundary graphs for adjacent class pairs and analyzing them.</p><hr><h3>Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators</h3>
<p><a href='https://openreview.net/forum?id=WIAO4vbnNV'>https://openreview.net/forum?id=WIAO4vbnNV</a></p>
<p><b>Compressor summary</b>: Motion guidance is a technique that uses optical flow networks to steer diffusion models for precise editing of images.</p><hr><h3>Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data</h3>
<p><a href='https://openreview.net/forum?id=W8S8SxS9Ng'>https://openreview.net/forum?id=W8S8SxS9Ng</a></p>
<p><b>Compressor summary</b>: Neuroformer is a pre-trained transformer model for analyzing large-scale neuronal spiking data in systems neuroscience, which can predict behavior and infer neural circuit connectivity from simulated datasets.</p><hr><h3>Modeling Boundedly Rational Agents with Latent Inference Budgets</h3>
<p><a href='https://openreview.net/forum?id=W3VsHuga3j'>https://openreview.net/forum?id=W3VsHuga3j</a></p>
<p><b>Compressor summary</b>: The latent inference budget model (L-IBM) simulates computational constraints in agent populations by using a latent variable that controls the runtime of an iterative inference algorithm, allowing for better learning of suboptimal decision-making and inference of goals.</p><hr><h3>A Flexible Generative Model for Heterogeneous Tabular EHR with Missing Modality</h3>
<p><a href='https://openreview.net/forum?id=W2tCmRrj7H'>https://openreview.net/forum?id=W2tCmRrj7H</a></p>
<p><b>Compressor summary</b>: FLEXGEN-EHR is a diffusion model for synthetic EHR generation that handles missing modalities and preserves fidelity and utility better than existing methods.</p><hr><h3>A unified sampling framework for solver searching of Diffusion Probabilistic Models</h3>
<p><a href='https://openreview.net/forum?id=W2d3LZbhhI'>https://openreview.net/forum?id=W2d3LZbhhI</a></p>
<p><b>Compressor summary</b>: The paper proposes a unified sampling framework that improves sample quality by adjusting solver strategies at different timesteps, and introduces $S^3$, a method to automatically optimize the solver schedule for efficient sampling from diffusion probabilistic models.</p><hr><h3>Tree Cross Attention</h3>
<p><a href='https://openreview.net/forum?id=Vw24wtSddM'>https://openreview.net/forum?id=Vw24wtSddM</a></p>
<p><b>Compressor summary</b>: Tree Cross Attention (TCA) is a method that retrieves information from a logarithmic number of tokens for inference in a tree structure, making it more token-efficient than Cross Attention and Perceiver IO.</p><hr><h3>MetaGPT: Meta Programming for Multi-Agent Collaborative Framework</h3>
<p><a href='https://openreview.net/forum?id=VtmBAGCN7o'>https://openreview.net/forum?id=VtmBAGCN7o</a></p>
<p><b>Compressor summary</b>: MetaGPT is a meta-programming framework that uses human workflows and Standardized Operating Procedures to improve the collaboration of language models in solving complex tasks.</p><hr><h3>Understanding Catastrophic Forgetting in Language Models via Implicit Inference</h3>
<p><a href='https://openreview.net/forum?id=VrHiF2hsrm'>https://openreview.net/forum?id=VrHiF2hsrm</a></p>
<p><b>Compressor summary</b>: The text discusses the drawbacks of fine-tuning language models and proposes Conjugate Prompting to improve their performance on various tasks.</p><hr><h3>Understanding Reconstruction Attacks with the Neural Tangent Kernel and Dataset Distillation</h3>
<p><a href='https://openreview.net/forum?id=VoLDkQ6yR3'>https://openreview.net/forum?id=VoLDkQ6yR3</a></p>
<p><b>Compressor summary</b>: This work studies a data reconstruction attack on neural networks, showing how it can recover the entire training set in the infinite width regime and revealing its dependence on network properties and image characteristics.</p><hr><h3>Influencer Backdoor Attack on Semantic Segmentation</h3>
<p><a href='https://openreview.net/forum?id=VmGRoNDQgJ'>https://openreview.net/forum?id=VmGRoNDQgJ</a></p>
<p><b>Compressor summary</b>: The text presents Influencer Backdoor Attack, a new way to manipulate semantic segmentation models by injecting triggers on non-victim pixels, and two trigger injection methods. It also demonstrates that current segmentation models are susceptible to IBA in real-world applications.</p><hr><h3>RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations</h3>
<p><a href='https://openreview.net/forum?id=VkWbxFrCC8'>https://openreview.net/forum?id=VkWbxFrCC8</a></p>
<p><b>Compressor summary</b>: RECOMBINER is a improved data compression method that uses hierarchical priors, learnable positional encodings, and patch-based encoding to overcome the limitations of previous INR-based approaches.</p><hr><h3>Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation</h3>
<p><a href='https://openreview.net/forum?id=Vja3ecieXY'>https://openreview.net/forum?id=Vja3ecieXY</a></p>
<p><b>Compressor summary</b>: GreenTrainer is a new technique that reduces the environmental impact of fine-tuning large language models by adaptively selecting important tensors for training, achieving up to 64% FLOPs reduction without sacrificing accuracy.</p><hr><h3>T-MARS: Improving Visual Representations by Circumventing Text Feature Learning</h3>
<p><a href='https://openreview.net/forum?id=ViPtjIVzUw'>https://openreview.net/forum?id=ViPtjIVzUw</a></p>
<p><b>Compressor summary</b>: The paper proposes T-MARS, a data filtering approach for large multimodal datasets that removes image-caption pairs with overlapping text dominating visual features, improving computer vision tasks.</p><hr><h3>Deep Temporal Graph Clustering</h3>
<p><a href='https://openreview.net/forum?id=ViNe1fjGME'>https://openreview.net/forum?id=ViNe1fjGME</a></p>
<p><b>Compressor summary</b>: The paper introduces TGC, a general framework for deep Temporal Graph Clustering that preserves dynamic information and reduces computational consumption by adjusting deep clustering techniques to suit temporal graphs.</p><hr><h3>WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space</h3>
<p><a href='https://openreview.net/forum?id=VdwVOREDZM'>https://openreview.net/forum?id=VdwVOREDZM</a></p>
<p><b>Compressor summary</b>: WildFusion is a new method that uses latent diffusion models to generate high-quality, 3D-consistent images from in-the-wild datasets without relying on posed images or canonical camera coordinates.</p><hr><h3>Generative Sliced MMD Flows with Riesz Kernels</h3>
<p><a href='https://openreview.net/forum?id=VdkGRV1vcf'>https://openreview.net/forum?id=VdkGRV1vcf</a></p>
<p><b>Compressor summary</b>: The paper shows that Riesz kernels in maximum mean discrepancy flows have efficient computation properties, allowing their use in training generative models for images.</p><hr><h3>Provable Out-of-Distribution Generalization in Hypersphere</h3>
<p><a href='https://openreview.net/forum?id=VXak3CZZGC'>https://openreview.net/forum?id=VXak3CZZGC</a></p>
<p><b>Compressor summary</b>: The paper presents a novel OOD generalization framework that learns invariant features in a hyperspherical space based on class alignment and separation, and shows its advantages theoretically and experimentally.</p><hr><h3>Order-Preserving GFlowNets</h3>
<p><a href='https://openreview.net/forum?id=VXDPXuq4oG'>https://openreview.net/forum?id=VXDPXuq4oG</a></p>
<p><b>Compressor summary</b>: Order-Preserving GFlowNets use a learned reward function to sample candidates with probabilities proportional to their order, enabling efficient exploration and exploitation in single-objective and multi-objective tasks.</p><hr><h3>An LLM can Fool Itself: A Prompt-Based Adversarial Attack</h3>
<p><a href='https://openreview.net/forum?id=VVgGbB9TNV'>https://openreview.net/forum?id=VVgGbB9TNV</a></p>
<p><b>Compressor summary</b>: PromptAttack is an efficient tool to audit large language models' adversarial robustness by converting attacks into a prompt that fools the model without changing its semantic meaning.</p><hr><h3>ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection Algorithms</h3>
<p><a href='https://openreview.net/forum?id=VTYg5ykEGS'>https://openreview.net/forum?id=VTYg5ykEGS</a></p>
<p><b>Compressor summary</b>: The paper introduces ImageNet-OOD, a dataset that separates semantic and covariate shifts in out-of-distribution detection, and shows that existing detectors are more sensitive to covariate shift than semantic shift.</p><hr><h3>SWE-bench: Can Language Models Resolve Real-world Github Issues?</h3>
<p><a href='https://openreview.net/forum?id=VTF8yNQM66'>https://openreview.net/forum?id=VTF8yNQM66</a></p>
<p><b>Compressor summary</b>: SWE-bench is a new benchmark that assesses language models' ability to fix real-world software bugs by editing code and passing tests.</p><hr><h3>Tangent Transformers for Composition,Privacy and Removal</h3>
<p><a href='https://openreview.net/forum?id=VLFhbOCz5D'>https://openreview.net/forum?id=VLFhbOCz5D</a></p>
<p><b>Compressor summary</b>: TAFT is a method for fine-tuning linearized transformers using first-order Taylor expansion, which has lower cost, comparable performance, and several advantages over non-linear fine-tuning.</p><hr><h3>GnnX-Bench: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking</h3>
<p><a href='https://openreview.net/forum?id=VJvbOSXRUq'>https://openreview.net/forum?id=VJvbOSXRUq</a></p>
<p><b>Compressor summary</b>: The paper evaluates and compares various perturbation-based explainability methods for GNNs, identifying Pareto-optimal techniques and highlighting stability and feasibility issues.</p><hr><h3>Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization</h3>
<p><a href='https://openreview.net/forum?id=V5tdi14ple'>https://openreview.net/forum?id=V5tdi14ple</a></p>
<p><b>Compressor summary</b>: The paper proposes using formal mathematics examples in training LLMs to improve their quantitative reasoning skills and prevent logical errors.</p><hr><h3>FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data</h3>
<p><a href='https://openreview.net/forum?id=V3j5d0GQgH'>https://openreview.net/forum?id=V3j5d0GQgH</a></p>
<p><b>Compressor summary</b>: FedLoGe improves local and global model performance in Federated Long-Tailed Learning by integrating representation learning and classifier alignment using a shared backbone and individualized classifiers.</p><hr><h3>Exploring the Promise and Limits of Real-Time Recurrent Learning</h3>
<p><a href='https://openreview.net/forum?id=V2cBKtdC3a'>https://openreview.net/forum?id=V2cBKtdC3a</a></p>
<p><b>Compressor summary</b>: RTRL is a sequence-processing RNN method that has conceptual advantages over BPTT but suffers from high time and space complexity; by combining it with policy gradients, researchers can achieve competitive results on challenging environments without approximation.</p><hr><h3>Neural structure learning with stochastic differential equations</h3>
<p><a href='https://openreview.net/forum?id=V1GM9xDvIY'>https://openreview.net/forum?id=V1GM9xDvIY</a></p>
<p><b>Compressor summary</b>: The paper introduces SCOTCH, a novel method that combines neural stochastic differential equations with variational inference to learn structures from continuous-time stochastic processes observed at any time points, improving upon existing approaches.</p><hr><h3>DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation</h3>
<p><a href='https://openreview.net/forum?id=UyNXMqnN3c'>https://openreview.net/forum?id=UyNXMqnN3c</a></p>
<p><b>Compressor summary</b>: DreamGaussian is a fast and efficient 3D content generation framework that uses Gaussians and UV space techniques to create high-quality textured meshes from single-view images.</p><hr><h3>Meaning Representations from Trajectories in Autoregressive Models</h3>
<p><a href='https://openreview.net/forum?id=UyGWafcopT'>https://openreview.net/forum?id=UyGWafcopT</a></p>
<p><b>Compressor summary</b>: The authors propose a novel way to extract meaning representations from autoregressive language models without prompts or fine-tuning, which can handle complex semantic tasks and multimodal data.</p><hr><h3>A representation-learning game for classes of prediction tasks</h3>
<p><a href='https://openreview.net/forum?id=Uw8xvFqVAE'>https://openreview.net/forum?id=Uw8xvFqVAE</a></p>
<p><b>Compressor summary</b>: The paper proposes a game-based method for learning dimensionality-reducing representations using prior knowledge on prediction tasks, with theoretical and practical implications.</p><hr><h3>Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D</h3>
<p><a href='https://openreview.net/forum?id=UulwvAU1W0'>https://openreview.net/forum?id=UulwvAU1W0</a></p>
<p><b>Compressor summary</b>: FourTran is an open-loop behavior cloning method that uses symmetry to improve sample efficiency and achieve state-of-the-art results in pick and place tasks.</p><hr><h3>Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling</h3>
<p><a href='https://openreview.net/forum?id=UpgRVWexaD'>https://openreview.net/forum?id=UpgRVWexaD</a></p>
<p><b>Compressor summary</b>: SKR is a novel method to efficiently generate labeled data for training neural operators by sorting and recycling similar PDE systems, achieving up to 13.9x speedup.</p><hr><h3>Query-Policy Misalignment in Preference-Based Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=UoBymIwPJR'>https://openreview.net/forum?id=UoBymIwPJR</a></p>
<p><b>Compressor summary</b>: The paper proposes a policy-aligned query and hybrid experience replay to improve preference-based reinforcement learning by addressing the issue of query-policy misalignment.</p><hr><h3>Time-LLM: Time Series Forecasting by Reprogramming Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=Unb5CVPtae'>https://openreview.net/forum?id=Unb5CVPtae</a></p>
<p><b>Compressor summary</b>: Time-LLM is a framework to repurpose large language models for time series forecasting using text prototypes and Prompt-as-Prefix, achieving state-of-the-art results and few-shot/zero-shot learning capabilities.</p><hr><h3>WizardCoder: Empowering Code Large Language Models with Evol-Instruct</h3>
<p><a href='https://openreview.net/forum?id=UnUwSIgK5W'>https://openreview.net/forum?id=UnUwSIgK5W</a></p>
<p><b>Compressor summary</b>: Code Evol-Instruct is a novel method to enhance Code LLMs by adapting the Evol-Instruct technique, leading to superior code generation performance and surpassing both open-source and closed-source LLMs on various benchmarks.</p><hr><h3>VDT: General-purpose Video Diffusion Transformers via Mask Modeling</h3>
<p><a href='https://openreview.net/forum?id=Un0rgm9f04'>https://openreview.net/forum?id=Un0rgm9f04</a></p>
<p><b>Compressor summary</b>: Video Diffusion Transformer (VDT) is a transformer-based video generation model that captures temporal dependencies, handles diverse conditioning information, and can perform various tasks such as unconditional generation, video prediction, interpolation, animation, and completion.</p><hr><h3>EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=UmMa3UNDAz'>https://openreview.net/forum?id=UmMa3UNDAz</a></p>
<p><b>Compressor summary</b>: EfficientDM is a data-free framework that combines quantization-aware fine-tuning and post-training quantization to achieve high performance in low-bit diffusion models while maintaining time and data efficiency.</p><hr><h3>The Discovery of Binding Modes Requires Rethinking Docking Generalization</h3>
<p><a href='https://openreview.net/forum?id=UfBIxpTK10'>https://openreview.net/forum?id=UfBIxpTK10</a></p>
<p><b>Compressor summary</b>: The text discusses a new benchmark called DockGen for assessing the generalization abilities of machine learning-based docking methods in finding ligand-binding proteins, and proposes Confidence Bootstrapping, a novel training paradigm that leverages diffusion and confidence models to improve generalizability.</p><hr><h3>Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation</h3>
<p><a href='https://openreview.net/forum?id=UbxWjq0UO2'>https://openreview.net/forum?id=UbxWjq0UO2</a></p>
<p><b>Compressor summary</b>: 3DFuse is a novel framework that improves text-to-3D generation by incorporating 3D awareness into pretrained 2D diffusion models using a consistency injection module and semantic coding techniques, achieving state-of-the-art performance in geometric robustness and fidelity.</p><hr><h3>Decoupling regularization from the action space</h3>
<p><a href='https://openreview.net/forum?id=UaMgmoKEBj'>https://openreview.net/forum?id=UaMgmoKEBj</a></p>
<p><b>Compressor summary</b>: This paper proposes methods to improve regularized reinforcement learning by maintaining consistent regularization across different action spaces and temperatures, leading to better performance on various tasks.</p><hr><h3>Separate and Diffuse: Using a Pretrained Diffusion Model for Better Source Separation</h3>
<p><a href='https://openreview.net/forum?id=UXALv0lJZS'>https://openreview.net/forum?id=UXALv0lJZS</a></p>
<p><b>Compressor summary</b>: The text describes a new speech separation method that uses generative models and achieves state-of-the-art results, even surpassing the previous upper bound for two speakers.</p><hr><h3>Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation</h3>
<p><a href='https://openreview.net/forum?id=UPvufoBAIs'>https://openreview.net/forum?id=UPvufoBAIs</a></p>
<p><b>Compressor summary</b>: The paper presents a novel unsupervised 3D pose estimation method that uses cuboid meshes and generative neural features to adapt to the target domain without 3D data or annotations, leveraging stable object subparts and simulating fine-tuning.</p><hr><h3>Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals</h3>
<p><a href='https://openreview.net/forum?id=UMfcdRIotC'>https://openreview.net/forum?id=UMfcdRIotC</a></p>
<p><b>Compressor summary</b>: The paper proposes two model-agnostic methods for counterfactual approximation, using a large language model either at inference-time or training-time, to improve the explanations of NLP system predictions effectively and efficiently.</p><hr><h3>A Precise Characterization of SGD Stability Using Loss Surface Geometry</h3>
<p><a href='https://openreview.net/forum?id=UMOlFJzLfL'>https://openreview.net/forum?id=UMOlFJzLfL</a></p>
<p><b>Compressor summary</b>: The paper investigates the relationship between linear stability and sharpness of SGD in overparameterized neural networks and introduces a new coherence measure for the loss Hessian to identify linear instability.</p><hr><h3>To Grok or not to Grok: Disentangling Generalization and Memorization on Corrupted Algorithmic Datasets</h3>
<p><a href='https://openreview.net/forum?id=UHjE5v5MB7'>https://openreview.net/forum?id=UHjE5v5MB7</a></p>
<p><b>Compressor summary</b>: The authors study how two-layer neural networks generalize on modular arithmetic tasks with corrupted labels and show that regularization methods improve performance by forcing the network to learn interpretable representations.</p><hr><h3>CrossLoco: Human Motion Driven Control of Legged Robots via Guided Unsupervised Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=UCfz492fM8'>https://openreview.net/forum?id=UCfz492fM8</a></p>
<p><b>Compressor summary</b>: CrossLoco is a guided unsupervised reinforcement learning framework that learns robot skills and their correspondence to human motions, achieving better accuracy, diversity, and user preference.</p><hr><h3>CivRealm: A Learning and Reasoning Odyssey for Decision-Making Agents</h3>
<p><a href='https://openreview.net/forum?id=UBVNwD3hPN'>https://openreview.net/forum?id=UBVNwD3hPN</a></p>
<p><b>Compressor summary</b>: CivRealm is an environment that tests decision-making agents' abilities in learning from past experiences and reasoning in novel contexts inspired by the Civilization game.</p><hr><h3>CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception</h3>
<p><a href='https://openreview.net/forum?id=U7iiF79kI3'>https://openreview.net/forum?id=U7iiF79kI3</a></p>
<p><b>Compressor summary</b>: CALICO is a novel pretraining framework for multimodal bird's eye view perception in autonomous driving systems that uses contrastive learning to improve 3D object detection and map segmentation performance, as well as robustness against adversarial attacks and corruption.</p><hr><h3>Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition</h3>
<p><a href='https://openreview.net/forum?id=U7VW3KBm34'>https://openreview.net/forum?id=U7VW3KBm34</a></p>
<p><b>Compressor summary</b>: SRD is a novel XAI method that enhances robustness by reflecting the model's inference process with a vector perspective and considering both active and inactive neurons.</p><hr><h3>3D-Aware Hypothesis & Verification for Generalizable Relative Object Pose Estimation</h3>
<p><a href='https://openreview.net/forum?id=U6hEOZlDf5'>https://openreview.net/forum?id=U6hEOZlDf5</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for relative object pose estimation using a hypothesis-and-verification framework with 3D transformations, which achieves high accuracy and robustness.</p><hr><h3>Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes</h3>
<p><a href='https://openreview.net/forum?id=U6Qulbv2qT'>https://openreview.net/forum?id=U6Qulbv2qT</a></p>
<p><b>Compressor summary</b>: The paper investigates whether multi-task reinforcement learning can improve sample efficiency in more general sequential decision making problems, such as partially observable MDPs and predictive state representations, by using a joint model class for tasks and measuring its complexity with the $\eta$-bracketing number.</p><hr><h3>Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape</h3>
<p><a href='https://openreview.net/forum?id=U0IOMStUQ8'>https://openreview.net/forum?id=U0IOMStUQ8</a></p>
<p><b>Compressor summary</b>: Sin3DM is a diffusion model that learns from low-dimensional latent space to create realistic 3D shapes with different textures and geometry, and supports various editing tasks.</p><hr><h3>ODEFormer: Symbolic Regression of Dynamical Systems with Transformers</h3>
<p><a href='https://openreview.net/forum?id=TzoHLiGVMo'>https://openreview.net/forum?id=TzoHLiGVMo</a></p>
<p><b>Compressor summary</b>: ODEFormer is a new transformer that can learn ODE systems from single trajectory observations, beating existing methods in accuracy and robustness, and being faster to infer.</p><hr><h3>Scaling Laws for Associative Memories</h3>
<p><a href='https://openreview.net/forum?id=Tzh6xAJSll'>https://openreview.net/forum?id=Tzh6xAJSll</a></p>
<p><b>Compressor summary</b>: The paper studies associative memory mechanisms in a model using high-dimensional matrices of embeddings, deriving scaling laws and evaluating estimators with numerical experiments and visualizations.</p><hr><h3>FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods</h3>
<p><a href='https://openreview.net/forum?id=TzAJbTClAz'>https://openreview.net/forum?id=TzAJbTClAz</a></p>
<p><b>Compressor summary</b>: The paper presents FFB, an open-source framework for evaluating group fairness methods in machine learning using standardized benchmarks and extensive analysis.</p><hr><h3>Safe RLHF: Safe Reinforcement Learning from Human Feedback</h3>
<p><a href='https://openreview.net/forum?id=TyFrPOKYXw'>https://openreview.net/forum?id=TyFrPOKYXw</a></p>
<p><b>Compressor summary</b>: The text proposes a novel algorithm called Safe Reinforcement Learning from Human Feedback (Safe RLHF) for training large language models (LLMs) that balance between being helpful and harmless, using human feedback to adjust the trade-off during fine-tuning.</p><hr><h3>Multi-task Learning with 3D-Aware Regularization</h3>
<p><a href='https://openreview.net/forum?id=TwBY17Hgiy'>https://openreview.net/forum?id=TwBY17Hgiy</a></p>
<p><b>Compressor summary</b>: The paper proposes a 3D-aware regularizer that improves the performance of deep neural networks on multiple computer vision tasks by projecting features into a shared space and decoding them through differentiable rendering, reducing noisy cross-task correlations.</p><hr><h3>Towards Reliable and Efficient Backdoor Trigger Inversion via Decoupling Benign Features</h3>
<p><a href='https://openreview.net/forum?id=Tw9wemV6cb'>https://openreview.net/forum?id=Tw9wemV6cb</a></p>
<p><b>Compressor summary</b>: The paper presents a novel BTI method that decouples benign features from backdoor features, which improves efficiency and robustness against backdoor threats. The paper also uses the new BTI method to develop effective backdoor-removal and pre-processing defenses.</p><hr><h3>Defending Against Transfer Attacks From Public Models</h3>
<p><a href='https://openreview.net/forum?id=Tvwf4Vsi5F'>https://openreview.net/forum?id=Tvwf4Vsi5F</a></p>
<p><b>Compressor summary</b>: The paper proposes a realistic threat model for adversarial attacks using surrogate models and introduces PubDef, a game-theoretic defense method that significantly improves robustness against transfer attacks.</p><hr><h3>TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series</h3>
<p><a href='https://openreview.net/forum?id=Tuh4nZVb0g'>https://openreview.net/forum?id=Tuh4nZVb0g</a></p>
<p><b>Compressor summary</b>: The paper proposes TEST, a method to convert time-series (TS) data into a format that large language models (LLMs) can handle, and shows that it can achieve competitive results on various TS tasks without sacrificing LLM's language abilities.</p><hr><h3>Training Bayesian Neural Networks with Sparse Subspace Variational Inference</h3>
<p><a href='https://openreview.net/forum?id=TskzCtpMEO'>https://openreview.net/forum?id=TskzCtpMEO</a></p>
<p><b>Compressor summary</b>: SSVI is a method for creating sparse Bayesian neural networks that maintains sparsity throughout training and inference, achieving significant compression and robustness while preserving performance.</p><hr><h3>Spatially-Aware Transformers for Embodied Agents</h3>
<p><a href='https://openreview.net/forum?id=Ts95eXsPBc'>https://openreview.net/forum?id=Ts95eXsPBc</a></p>
<p><b>Compressor summary</b>: The paper introduces spatially-aware transformers for episodic memory in AI that consider both temporal and spatial dimensions, improving efficiency and accuracy in various tasks.</p><hr><h3>Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior</h3>
<p><a href='https://openreview.net/forum?id=TrKq4Wlwcz'>https://openreview.net/forum?id=TrKq4Wlwcz</a></p>
<p><b>Compressor summary</b>: The paper introduces Large Content and Behavior Models (LCBMs) that incorporate "behavior tokens" in LLM training to improve communication effectiveness by predicting and optimizing receiver behavior.</p><hr><h3>Sample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data</h3>
<p><a href='https://openreview.net/forum?id=Tr3fZocrI6'>https://openreview.net/forum?id=Tr3fZocrI6</a></p>
<p><b>Compressor summary</b>: The text proposes a method called De-bias & Feature-Whiten to improve representation learning from heterogeneous data sources by reducing biases and noise dependencies, leading to better generalization and computational efficiency.</p><hr><h3>Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=Tr0lPx9woF'>https://openreview.net/forum?id=Tr0lPx9woF</a></p>
<p><b>Compressor summary</b>: The paper introduces a new post-training pruning method for large language models that considers both weight and activations, preserves important weights, and improves performance and efficiency.</p><hr><h3>LILO: Learning Interpretable Libraries by Compressing and Documenting Code</h3>
<p><a href='https://openreview.net/forum?id=TqYbAWKMIe'>https://openreview.net/forum?id=TqYbAWKMIe</a></p>
<p><b>Compressor summary</b>: LILO is a neurosymbolic framework that creates and documents reusable code libraries using LLM-guided synthesis, Stitch's refactoring, and AutoDoc's natural language documentation.</p><hr><h3>GeoLLM: Extracting Geospatial Knowledge from Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=TqL2xBwXP3'>https://openreview.net/forum?id=TqL2xBwXP3</a></p>
<p><b>Compressor summary</b>: GeoLLM is a new method that leverages language models' spatial information and map data to perform better in geospatial prediction tasks than existing methods.</p><hr><h3>Meta Continual Learning Revisited: Implicitly Enhancing Online Hessian Approximation via Variance Reduction</h3>
<p><a href='https://openreview.net/forum?id=TpD2aG1h0D'>https://openreview.net/forum?id=TpD2aG1h0D</a></p>
<p><b>Compressor summary</b>: Variance Reduced Meta-Continual Learning (VR-MCL) combines regularization-based and meta-learning approaches to improve knowledge transfer and reduce forgetting in continual learning.</p><hr><h3>Generative Adversarial Equilibrium Solvers</h3>
<p><a href='https://openreview.net/forum?id=TlyiaPXaVN'>https://openreview.net/forum?id=TlyiaPXaVN</a></p>
<p><b>Compressor summary</b>: The authors propose GAES, a method using generative adversarial networks to compute general game-theoretic equilibria from samples, and apply it to various settings including normal-form games, competitive economies, and environmental policy.</p><hr><h3>Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks</h3>
<p><a href='https://openreview.net/forum?id=TjhUtloBZU'>https://openreview.net/forum?id=TjhUtloBZU</a></p>
<p><b>Compressor summary</b>: This paper studies how noise in pre-training datasets affects model generalization and proposes a method (NMTune) to improve both in-domain and out-of-domain performance on vision and language models.</p><hr><h3>NOLA: Networks as Linear Combination of Low Rank Random Basis</h3>
<p><a href='https://openreview.net/forum?id=TjfXcDgvzk'>https://openreview.net/forum?id=TjfXcDgvzk</a></p>
<p><b>Compressor summary</b>: NOLA is a method to efficiently adapt large language models for specific tasks by reducing parameters using linear combinations of random matrices and optimizing their coefficients.</p><hr><h3>Doubly Robust Proximal Causal Learning for Continuous Treatments</h3>
<p><a href='https://openreview.net/forum?id=TjGJFkU3xL'>https://openreview.net/forum?id=TjGJFkU3xL</a></p>
<p><b>Compressor summary</b>: The paper proposes a kernel-based doubly robust estimator for continuous treatments in proximal causal learning, showing its consistency, efficiency, and performance.</p><hr><h3>Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics</h3>
<p><a href='https://openreview.net/forum?id=TjCDNssXKU'>https://openreview.net/forum?id=TjCDNssXKU</a></p>
<p><b>Compressor summary</b>: THICK is an algorithm that learns a hierarchical world model for better planning and reinforcement learning by updating lower levels sparsely and higher levels when contexts change.</p><hr><h3>Probabilistically Rewired Message-Passing Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=Tj6Wcx7gVk'>https://openreview.net/forum?id=Tj6Wcx7gVk</a></p>
<p><b>Compressor summary</b>: PR-MPNNs learn to add and remove edges in graphs based on their relevance to the prediction task, improving expressiveness and performance over random methods and existing graph neural networks.</p><hr><h3>On the Foundations of Shortcut Learning</h3>
<p><a href='https://openreview.net/forum?id=Tj3xLVuE9f'>https://openreview.net/forum?id=Tj3xLVuE9f</a></p>
<p><b>Compressor summary</b>: The paper investigates how predictivity and availability affect feature use in deep-learning models, finding that nonlinear models tend to over-rely on less predictive but more available features.</p><hr><h3>Waxing-and-Waning: a Generic Similarity-based Framework for Efficient Self-Supervised Learning</h3>
<p><a href='https://openreview.net/forum?id=TilcG5C8bN'>https://openreview.net/forum?id=TilcG5C8bN</a></p>
<p><b>Compressor summary</b>: SimWnW is a similarity-based efficient self-supervised learning framework that reduces computation costs by removing less important regions in augmented images and feature maps without compromising accuracy.</p><hr><h3>Overthinking the Truth: Understanding how Language Models Process False Demonstrations</h3>
<p><a href='https://openreview.net/forum?id=Tigr1kMDZy'>https://openreview.net/forum?id=Tigr1kMDZy</a></p>
<p><b>Compressor summary</b>: The text discusses the risks of harmful imitation by language models through two phenomena called overthinking and false induction heads, which can be studied by examining their internal representations.</p><hr><h3>DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=Th6NyL07na'>https://openreview.net/forum?id=Th6NyL07na</a></p>
<p><b>Compressor summary</b>: The DoLa approach reduces hallucinations in large language models by contrasting different layers to obtain a better next-token distribution and surface factual knowledge.</p><hr><h3>Patched Denoising Diffusion Models For High-Resolution Image Synthesis</h3>
<p><a href='https://openreview.net/forum?id=TgSRPRz8cI'>https://openreview.net/forum?id=TgSRPRz8cI</a></p>
<p><b>Compressor summary</b>: Patch-DM is a denoising diffusion model that generates high-resolution images by combining features from neighboring patches and outperforms previous methods with less memory complexity.</p><hr><h3>Momentum Benefits Non-iid Federated Learning Simply and Provably</h3>
<p><a href='https://openreview.net/forum?id=TdhkAcXkRi'>https://openreview.net/forum?id=TdhkAcXkRi</a></p>
<p><b>Compressor summary</b>: This paper explores how adding momentum to federated learning algorithms FedAvg and SCAFFOLD can improve their performance and convergence without making restrictive assumptions or adjustments.</p><hr><h3>Towards Transparent Time Series Forecasting</h3>
<p><a href='https://openreview.net/forum?id=TYXtXLYHpR'>https://openreview.net/forum?id=TYXtXLYHpR</a></p>
<p><b>Compressor summary</b>: The paper proposes a top-down framework called bi-level transparency for time series forecasting with machine learning models to ensure interpretability and trustworthiness in decision-making systems.</p><hr><h3>Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning</h3>
<p><a href='https://openreview.net/forum?id=TWVMVPx2wO'>https://openreview.net/forum?id=TWVMVPx2wO</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework for fine-tuning pre-trained models for deep metric learning tasks using visual prompts, which improves performance with fewer parameters and less computation.</p><hr><h3>Enhancing Human-AI Collaboration Through Logic-Guided Reasoning</h3>
<p><a href='https://openreview.net/forum?id=TWC4gLoAxY'>https://openreview.net/forum?id=TWC4gLoAxY</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework to use logical rules derived from data to understand human goals and guide human-like agents in collaboration tasks.</p><hr><h3>Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition</h3>
<p><a href='https://openreview.net/forum?id=TVg6hlfsKa'>https://openreview.net/forum?id=TVg6hlfsKa</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method to adapt pre-trained vision models for visual place recognition, using hybrid adaptation and mutual nearest neighbor local feature loss, achieving better performance with less data and time.</p><hr><h3>Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles</h3>
<p><a href='https://openreview.net/forum?id=TVDUVpgu9s'>https://openreview.net/forum?id=TVDUVpgu9s</a></p>
<p><b>Compressor summary</b>: ZO-RankSGD is a novel algorithm that optimizes black-box functions with ranking feedback and improves image quality in diffusion generative models using human guidance.</p><hr><h3>Distributionally Robust Optimization with Bias & Variance Reduced Gradients</h3>
<p><a href='https://openreview.net/forum?id=TTrzgEZt9s'>https://openreview.net/forum?id=TTrzgEZt9s</a></p>
<p><b>Compressor summary</b>: Prospect is a fast and easy-to-tune algorithm for distributionally robust optimization with spectral risk-based uncertainty sets and $f$-divergence penalty, achieving linear convergence and outperforming existing methods on various benchmarks.</p><hr><h3>Fast Hyperboloid Decision Tree Algorithms</h3>
<p><a href='https://openreview.net/forum?id=TTonmgTT9X'>https://openreview.net/forum?id=TTonmgTT9X</a></p>
<p><b>Compressor summary</b>: Hyperbolic geometry helps machine learning capture hierarchical structures in data, but its computational challenges are overcome by HyperDT, a decision tree algorithm that adapts to hyperbolic space without Riemannian optimization.</p><hr><h3>Feature-aligned N-BEATS with Sinkhorn divergence</h3>
<p><a href='https://openreview.net/forum?id=TS8HoIWAPQ'>https://openreview.net/forum?id=TS8HoIWAPQ</a></p>
<p><b>Compressor summary</b>: Feature-aligned N-BEATS is a time series forecasting model that learns invariant features across multiple domains while preserving interpretability and performance.</p><hr><h3>Test-time Adaption against Multi-modal Reliability Bias</h3>
<p><a href='https://openreview.net/forum?id=TPZRq4FALB'>https://openreview.net/forum?id=TPZRq4FALB</a></p>
<p><b>Compressor summary</b>: This paper introduces a new challenge in multi-modal test-time adaptation called reliability bias, proposes a novel method RFRA that adaptively modulates attention between modalities and uses a robust objective function for multi-modal adaption, and provides two new benchmarks to evaluate the performance of methods under this challenge.</p><hr><h3>Discovering Failure Modes of Text-guided Diffusion Models via Adversarial Search</h3>
<p><a href='https://openreview.net/forum?id=TOWdQQgMJY'>https://openreview.net/forum?id=TOWdQQgMJY</a></p>
<p><b>Compressor summary</b>: SAGE is a method that systematically explores and discovers failures in text-guided diffusion models for image generation by using adversarial search, image classifiers, and human inspections.</p><hr><h3>TiC-CLIP: Continual Training of CLIP Models</h3>
<p><a href='https://openreview.net/forum?id=TLADT8Wrhn'>https://openreview.net/forum?id=TLADT8Wrhn</a></p>
<p><b>Compressor summary</b>: The paper introduces TiC benchmarks for continual learning on vision-language models, evaluates temporal robustness of existing models, and proposes a rehearsal-based approach to reduce compute costs.</p><hr><h3>Modulate Your Spectrum in Self-Supervised Learning</h3>
<p><a href='https://openreview.net/forum?id=TKqMmKlmA7'>https://openreview.net/forum?id=TKqMmKlmA7</a></p>
<p><b>Compressor summary</b>: Spectral Transformation (ST) is a framework that modulates the spectrum of embedding in self-supervised learning to prevent dimensional collapse and improve representation learning, with IterNorm with trace loss being a novel effective instance of ST.</p><hr><h3>Structural Inference with Dynamics Encoding and Partial Correlation Coefficients</h3>
<p><a href='https://openreview.net/forum?id=TKnzPdyeJu'>https://openreview.net/forum?id=TKnzPdyeJu</a></p>
<p><b>Compressor summary</b>: The paper presents a new structural inference method using variational dynamics encoder and partial correlation coefficients, which is scalable, accurate, and robust for one- and multi-dimensional feature spaces.</p><hr><h3>Querying Easily Flip-flopped Samples for Deep Active Learning</h3>
<p><a href='https://openreview.net/forum?id=THUBTfSAS2'>https://openreview.net/forum?id=THUBTfSAS2</a></p>
<p><b>Compressor summary</b>: The paper proposes a new active learning strategy based on predictive uncertainty measured by the least disagree metric, which outperforms existing methods on various datasets and deep neural networks.</p><hr><h3>Harnessing Density Ratios for Online Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=THJEa8adBn'>https://openreview.net/forum?id=THJEa8adBn</a></p>
<p><b>Compressor summary</b>: The paper proposes GLOW and HYGLOW algorithms for online and hybrid reinforcement learning that use density ratios and exploration under coverability condition.</p><hr><h3>Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=TFKIfhvdmZ'>https://openreview.net/forum?id=TFKIfhvdmZ</a></p>
<p><b>Compressor summary</b>: The text introduces PPGA, a new algorithm that combines QD and RL to improve exploration and skill discovery for robot learning.</p><hr><h3>Neural Fine-Tuning Search for Few-Shot Learning</h3>
<p><a href='https://openreview.net/forum?id=T7YV5UZKBc'>https://openreview.net/forum?id=T7YV5UZKBc</a></p>
<p><b>Compressor summary</b>: The paper proposes a neural architecture search method to find the best adaptation strategy for few-shot recognition using adapters and freezing layers.</p><hr><h3>Neur2RO: Neural Two-Stage Robust Optimization</h3>
<p><a href='https://openreview.net/forum?id=T5Xb0iGCCv'>https://openreview.net/forum?id=T5Xb0iGCCv</a></p>
<p><b>Compressor summary</b>: Neur2RO is a fast and efficient machine learning algorithm for solving two-stage robust optimization problems, especially on large-scale instances.</p><hr><h3>Emergent Communication with Conversational Repair</h3>
<p><a href='https://openreview.net/forum?id=Sy8upuD6Bw'>https://openreview.net/forum?id=Sy8upuD6Bw</a></p>
<p><b>Compressor summary</b>: The study shows that conversational repair improves language performance in signaling games with feedback and noise, challenging the role of compositionality for generalization.</p><hr><h3>Variational Bayesian Last Layers</h3>
<p><a href='https://openreview.net/forum?id=Sx7BIiPzys'>https://openreview.net/forum?id=Sx7BIiPzys</a></p>
<p><b>Compressor summary</b>: The paper proposes a deterministic variational approach to train Bayesian last layer neural networks that improves uncertainty estimation and predictive accuracy.</p><hr><h3>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</h3>
<p><a href='https://openreview.net/forum?id=Sx038qxjek'>https://openreview.net/forum?id=Sx038qxjek</a></p>
<p><b>Compressor summary</b>: CRITIC is a framework that helps large language models improve their outputs by using external tools for validation and revision, similar to how humans use search engines and code interpreters.</p><hr><h3>Sliced Wasserstein Estimation with Control Variates</h3>
<p><a href='https://openreview.net/forum?id=StYc4hQAEi'>https://openreview.net/forum?id=StYc4hQAEi</a></p>
<p><b>Compressor summary</b>: The paper proposes efficient control variates to reduce variance in Monte Carlo estimation of sliced Wasserstein distances for comparing probability measures in various applications.</p><hr><h3>Provable Robust Watermarking for AI-Generated Text</h3>
<p><a href='https://openreview.net/forum?id=SsmT8aO45L'>https://openreview.net/forum?id=SsmT8aO45L</a></p>
<p><b>Compressor summary</b>: The paper proposes a watermarking method for large language models that ensures high-quality generated text, correct detection, and robustness against text manipulation.</p><hr><h3>IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=Spp2i1hKwV'>https://openreview.net/forum?id=Spp2i1hKwV</a></p>
<p><b>Compressor summary</b>: The paper proposes a selective annotation method for in-context learning that minimizes costs and improves quality by using a directed graph and a greedy algorithm to choose influential unlabeled data subsets.</p><hr><h3>Generalized Schrdinger Bridge Matching</h3>
<p><a href='https://openreview.net/forum?id=SoismgeX7z'>https://openreview.net/forum?id=SoismgeX7z</a></p>
<p><b>Compressor summary</b>: The paper introduces GSBM, a generalized distribution matching algorithm for training diffusion or flow models that can handle nonlinear state costs and preserves feasible transport maps throughout training, leading to improved scalability and convergence.</p><hr><h3>R-EDL: Relaxing Nonessential Settings of Evidential Deep Learning</h3>
<p><a href='https://openreview.net/forum?id=Si3YFA641c'>https://openreview.net/forum?id=Si3YFA641c</a></p>
<p><b>Compressor summary</b>: The paper proposes R-EDL, a modified Evidential Deep Learning method that adjusts prior weight and optimizes Dirichlet PDF to improve predictive uncertainty estimation.</p><hr><h3>Self-Consuming Generative Models Go MAD</h3>
<p><a href='https://openreview.net/forum?id=ShjMHfmPs0'>https://openreview.net/forum?id=ShjMHfmPs0</a></p>
<p><b>Compressor summary</b>: The quality or diversity of future generative models will decline without enough fresh real data in each generation of an autophagous loop, which is called Model Autophagy Disorder (MAD).</p><hr><h3>Accurate Forgetting for Heterogeneous Federated Continual Learning</h3>
<p><a href='https://openreview.net/forum?id=ShQrnAsbPI'>https://openreview.net/forum?id=ShQrnAsbPI</a></p>
<p><b>Compressor summary</b>: The paper proposes accurate forgetting (AF) and a generative-replay method for federated continual learning, which selectively uses previous knowledge in federated networks based on a normalizing flow model.</p><hr><h3>OpenNerf: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views</h3>
<p><a href='https://openreview.net/forum?id=SgjAojPKb3'>https://openreview.net/forum?id=SgjAojPKb3</a></p>
<p><b>Compressor summary</b>: OpenNeRF is a method that uses visual-language models to segment arbitrary concepts from 3D scenes in images without relying on predefined training sets or point cloud representations.</p><hr><h3>Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation</h3>
<p><a href='https://openreview.net/forum?id=Sf2A2PUXO3'>https://openreview.net/forum?id=Sf2A2PUXO3</a></p>
<p><b>Compressor summary</b>: The text proposes a framework that uses dropout techniques to explore and measure the Rashomon set, which consists of multiple competing models with almost-equally-optimal performance but conflicting outputs, and use this information to mitigate predictive multiplicity in classification tasks.</p><hr><h3>Incremental Randomized Smoothing Certification</h3>
<p><a href='https://openreview.net/forum?id=SdeAPV1irk'>https://openreview.net/forum?id=SdeAPV1irk</a></p>
<p><b>Compressor summary</b>: IRS is a method that reduces computational cost and maintains strong robustness guarantees for certifying modified deep neural networks against adversarial attacks using incremental robustness certification for randomized smoothing.</p><hr><h3>Horizon-Free Regret for Linear Markov Decision Processes</h3>
<p><a href='https://openreview.net/forum?id=SdBApv9iT4'>https://openreview.net/forum?id=SdBApv9iT4</a></p>
<p><b>Compressor summary</b>: The paper proposes a new horizon-free regret bound for linear MDPs, where it estimates value functions and confidence sets directly, instead of estimating transition models and inhomogeneous value functions.</p><hr><h3>Efficient local linearity regularization to overcome catastrophic overfitting</h3>
<p><a href='https://openreview.net/forum?id=SZzQz8ikwg'>https://openreview.net/forum?id=SZzQz8ikwg</a></p>
<p><b>Compressor summary</b>: The paper proposes ELLE, a regularization term for single-step adversarial training that reduces catastrophic overfitting and is more efficient than previous methods.</p><hr><h3>OWL: A Large Language Model for IT Operations</h3>
<p><a href='https://openreview.net/forum?id=SZOQ9RKYJu'>https://openreview.net/forum?id=SZOQ9RKYJu</a></p>
<p><b>Compressor summary</b>: The paper introduces OWL, a large language model for IT operations, and proposes HMCE, a method to extend homogeneous Markov context, improving performance on IT tasks.</p><hr><h3>When Semantic Segmentation Meets Frequency Aliasing</h3>
<p><a href='https://openreview.net/forum?id=SYBdkHcXXK'>https://openreview.net/forum?id=SYBdkHcXXK</a></p>
<p><b>Compressor summary</b>: The paper analyzes hard pixel errors in semantic segmentation, identifies aliasing as a major cause, and proposes new methods to improve segmentation by removing or adjusting high-frequency components.</p><hr><h3>VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections</h3>
<p><a href='https://openreview.net/forum?id=SUUrkC3STJ'>https://openreview.net/forum?id=SUUrkC3STJ</a></p>
<p><b>Compressor summary</b>: The text proposes a new graph learning method called VCR-Graphormer, which uses personalized PageRank tokenization and virtual connections to enable efficient mini-batch training and improve performance on node classification tasks.</p><hr><h3>When can transformers reason with abstract symbols?</h3>
<p><a href='https://openreview.net/forum?id=STUGfUz8ob'>https://openreview.net/forum?id=STUGfUz8ob</a></p>
<p><b>Compressor summary</b>: Transformer models struggle with generalizing to new symbols when trained on abstract tasks, requiring large amounts of data or modifications to improve performance.</p><hr><h3>CABINET: Content Relevance-based Noise Reduction for Table Question Answering</h3>
<p><a href='https://openreview.net/forum?id=SQrHpTllXa'>https://openreview.net/forum?id=SQrHpTllXa</a></p>
<p><b>Compressor summary</b>: CABINET improves LLMs' question-answering over tables by filtering out irrelevant information using an unsupervised relevance scorer and a weakly supervised parsing module.</p><hr><h3>Social-Transmotion: Promptable Human Trajectory Prediction</h3>
<p><a href='https://openreview.net/forum?id=SQpnEfv9WH'>https://openreview.net/forum?id=SQpnEfv9WH</a></p>
<p><b>Compressor summary</b>: Social-Transmotion is a transformer-based model that leverages diverse visual cues to enhance human trajectory prediction by using prompts inspired by NLP.</p><hr><h3>The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models</h3>
<p><a href='https://openreview.net/forum?id=SQGUDc9tC8'>https://openreview.net/forum?id=SQGUDc9tC8</a></p>
<p><b>Compressor summary</b>: The authors introduce Social Bias Neurons to identify and suppress harmful information in pre-trained language models, using a novel technique called Integrated Gap Gradients (IG$^2$).</p><hr><h3>Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization</h3>
<p><a href='https://openreview.net/forum?id=SNGXbZtK6Q'>https://openreview.net/forum?id=SNGXbZtK6Q</a></p>
<p><b>Compressor summary</b>: The paper proposes neuron activation coverage (NAC) as a simple and effective measure for detecting out-of-distribution inputs in neural networks and evaluating model robustness.</p><hr><h3>Controlled Text Generation via Language Model Arithmetic</h3>
<p><a href='https://openreview.net/forum?id=SLw9fp4yI6'>https://openreview.net/forum?id=SLw9fp4yI6</a></p>
<p><b>Compressor summary</b>: Model arithmetic is a new way to combine and adjust large language models for better text generation with less retraining and more control.</p><hr><h3>Rethinking the symmetry-preserving circuits for constrained variational quantum algorithms</h3>
<p><a href='https://openreview.net/forum?id=SL7djdVpde'>https://openreview.net/forum?id=SL7djdVpde</a></p>
<p><b>Compressor summary</b>: The paper explores how to use a quantum algorithm called Hamming Weight preserving ansatz to incorporate symmetries and constraints in physical systems, which could lead to better performance in problems like ground state energy estimation and feature selection.</p><hr><h3>Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach</h3>
<p><a href='https://openreview.net/forum?id=SKulT2VX9p'>https://openreview.net/forum?id=SKulT2VX9p</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework for achieving causal fairness in machine learning using partially known causal graphs and interventions, and shows its effectiveness on various datasets.</p><hr><h3>Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models</h3>
<p><a href='https://openreview.net/forum?id=SIZWiya7FE'>https://openreview.net/forum?id=SIZWiya7FE</a></p>
<p><b>Compressor summary</b>: The text introduces Label-Agnostic Forgetting (LAF), a supervision-free method for removing information from forgotten data in a trained model, which performs well without labels and excels in semi-supervised scenarios.</p><hr><h3>Mask-based modeling for Neural Radiance Fields</h3>
<p><a href='https://openreview.net/forum?id=SEiuSzlD1d'>https://openreview.net/forum?id=SEiuSzlD1d</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method called MRVM-NeRF that improves 3D representation learning for NeRF models by using masks to guide ray and view modeling, leading to better generalization and detail capture across diverse scenes.</p><hr><h3>Prediction without Preclusion: Recourse Verification with Reachable Sets</h3>
<p><a href='https://openreview.net/forum?id=SCQfYpdoGE'>https://openreview.net/forum?id=SCQfYpdoGE</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to check if machine learning models can offer solutions when they make decisions that affect people's access to resources like loans or jobs, and shows how this can help improve fairness in these models.</p><hr><h3>THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS</h3>
<p><a href='https://openreview.net/forum?id=SBoRhRCzM3'>https://openreview.net/forum?id=SBoRhRCzM3</a></p>
<p><b>Compressor summary</b>: Thought Propagation (TP) improves complex reasoning in large language models by solving analogous problems and reusing their solutions to help with multi-step reasoning tasks.</p><hr><h3>Demystifying Local & Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition</h3>
<p><a href='https://openreview.net/forum?id=SBj2Qdhgew'>https://openreview.net/forum?id=SBj2Qdhgew</a></p>
<p><b>Compressor summary</b>: The paper proposes an information-theoretic approach to analyze fairness trade-offs in federated learning with respect to sensitive attributes, identifying three sources of unfairness and a convex optimization problem that defines the theoretical limits of accuracy and fairness trade-offs.</p><hr><h3>A Study of Bayesian Neural Network Surrogates for Bayesian Optimization</h3>
<p><a href='https://openreview.net/forum?id=SA19ijj44B'>https://openreview.net/forum?id=SA19ijj44B</a></p>
<p><b>Compressor summary</b>: Bayesian neural networks can optimize expensive objective functions and handle non-stationarity better than standard Gaussian process surrogates, but their performance depends on the problem and the inference method used.</p><hr><h3>Understanding Transferable Representation Learning and Zero-shot Transfer in CLIP</h3>
<p><a href='https://openreview.net/forum?id=S5yOuNfSA0'>https://openreview.net/forum?id=S5yOuNfSA0</a></p>
<p><b>Compressor summary</b>: The paper studies the theoretical aspects of CLIP, a multi-modal learning method that uses contrastive pretraining to learn joint image and text representations, and proposes a new approach based on its insights.</p><hr><h3>Complex priors and flexible inference in recurrent circuits with dendritic nonlinearities</h3>
<p><a href='https://openreview.net/forum?id=S5aUhpuyap'>https://openreview.net/forum?id=S5aUhpuyap</a></p>
<p><b>Compressor summary</b>: The text describes a recurrent network model with dendritic nonlinearities and stochastic somatic integration that can implicitly represent priors over latent variables and encode task-specific posteriors for flexible inference.</p><hr><h3>Do Generated Data Always Help Contrastive Learning?</h3>
<p><a href='https://openreview.net/forum?id=S5EqslEHnz'>https://openreview.net/forum?id=S5EqslEHnz</a></p>
<p><b>Compressor summary</b>: Adaptive Inflation (AdaInf) is a data-centric strategy that improves contrastive learning by adjusting the balance between data inflation and augmentation, achieving state-of-the-art results on benchmark datasets without extra computation cost.</p><hr><h3>A Variational Framework for Estimating Continuous Treatment Effects with Measurement Error</h3>
<p><a href='https://openreview.net/forum?id=S46Knicu56'>https://openreview.net/forum?id=S46Knicu56</a></p>
<p><b>Compressor summary</b>: The paper proposes a general variational framework to estimate the average dose-response function with error-contaminated treatment data, using neural networks to handle unobserved treatments and confounding variables.</p><hr><h3>SmartPlay : A Benchmark for LLMs as Intelligent Agents</h3>
<p><a href='https://openreview.net/forum?id=S2oTVrlcp3'>https://openreview.net/forum?id=S2oTVrlcp3</a></p>
<p><b>Compressor summary</b>: SmartPlay is a benchmark and methodology to evaluate large language models' abilities as intelligent agents by testing them on 6 different games that challenge various capabilities of an LLM agent.</p><hr><h3>Guiding Instruction-based Image Editing via Multimodal Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=S1RKWSyZ2Y'>https://openreview.net/forum?id=S1RKWSyZ2Y</a></p>
<p><b>Compressor summary</b>: MGIE uses multimodal large language models to generate expressive image editing instructions and improve controllability and flexibility of image manipulation.</p><hr><h3>Kill Two Birds with One Stone: Rethinking Data Augmentation for Deep Long-tailed Learning</h3>
<p><a href='https://openreview.net/forum?id=RzY9qQHUXy'>https://openreview.net/forum?id=RzY9qQHUXy</a></p>
<p><b>Compressor summary</b>: The paper proposes an adaptive data augmentation method called DODA that addresses the challenges of long-tailed learning by allowing each class to choose appropriate augmentation methods based on its distribution, reducing the imbalance and improving performance.</p><hr><h3>$t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence</h3>
<p><a href='https://openreview.net/forum?id=RzNlECeoOB'>https://openreview.net/forum?id=RzNlECeoOB</a></p>
<p><b>Compressor summary</b>: The paper introduces $t^3$VAE, a modified VAE framework that uses Student's t-distributions for the prior, encoder, and decoder to better capture rare features in heavy-tailed data.</p><hr><h3>A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks</h3>
<p><a href='https://openreview.net/forum?id=RyUvzda8GH'>https://openreview.net/forum?id=RyUvzda8GH</a></p>
<p><b>Compressor summary</b>: Incremental predictive coding (iPC) is an improved algorithm for training neuroscience-inspired models that is more efficient, stable, biologically plausible, and performs better on image and language tasks.</p><hr><h3>BrainLM: A foundation model for brain activity recordings</h3>
<p><a href='https://openreview.net/forum?id=RwI7ZEfR27'>https://openreview.net/forum?id=RwI7ZEfR27</a></p>
<p><b>Compressor summary</b>: The Brain Language Model is a large-scale model that can predict clinical variables, forecast future brain states, identify functional networks, and generate interpretable latent representations from fMRI data, enabling novel insights into human brain activity dynamics.</p><hr><h3>What's In My Big Data?</h3>
<p><a href='https://openreview.net/forum?id=RvfPnOkPV4'>https://openreview.net/forum?id=RvfPnOkPV4</a></p>
<p><b>Compressor summary</b>: WIMBD is a platform that analyzes large text corpora, revealing their content quality and diversity, and finds issues like duplicate and low-quality data, as well as contamination in benchmark datasets.</p><hr><h3>A Benchmark on Robust Semi-Supervised Learning in Open Environments</h3>
<p><a href='https://openreview.net/forum?id=RvUVMjfp8i'>https://openreview.net/forum?id=RvUVMjfp8i</a></p>
<p><b>Compressor summary</b>: SSL can benefit from unlabeled data but needs to be improved for consistent and diverse data sources in open environments.</p><hr><h3>Meta-Evolve: Continuous Robot Evolution for One-to-many Policy Transfer</h3>
<p><a href='https://openreview.net/forum?id=RthOl4jHw5'>https://openreview.net/forum?id=RthOl4jHw5</a></p>
<p><b>Compressor summary</b>: Meta-Evolve is a method that uses continuous robot evolution to efficiently transfer policies from a source robot to multiple target robots, achieving significant improvements in simulation cost.</p><hr><h3>Simplifying Transformer Blocks</h3>
<p><a href='https://openreview.net/forum?id=RtDok9eS3s'>https://openreview.net/forum?id=RtDok9eS3s</a></p>
<p><b>Compressor summary</b>: The authors propose a simplified design for deep Transformers by removing some components without losing speed or performance.</p><hr><h3>On Error Propagation of Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=RtAct1E2zS'>https://openreview.net/forum?id=RtAct1E2zS</a></p>
<p><b>Compressor summary</b>: The paper develops a theory of error propagation in diffusion models and proposes a regularization method based on cumulative error to improve their performance.</p><hr><h3>A Primal-Dual Approach to Solving Variational Inequalities with General Constraints</h3>
<p><a href='https://openreview.net/forum?id=RsztjXcvUf'>https://openreview.net/forum?id=RsztjXcvUf</a></p>
<p><b>Compressor summary</b>: The paper proposes a warm-starting technique for solving variational inequalities and shows its faster convergence than existing methods, while also relaxing smoothness assumptions.</p><hr><h3>Optimal Sketching for Residual Error Estimation for Matrix and Vector Norms</h3>
<p><a href='https://openreview.net/forum?id=RsJwmWvE6Q'>https://openreview.net/forum?id=RsJwmWvE6Q</a></p>
<p><b>Compressor summary</b>: The text studies how to estimate residual errors for matrix and vector norms using linear sketches, improving previous bounds and allowing for faster algorithms with sparse matrices.</p><hr><h3>COMPARATOR: Reference-free machine translation evaluation by inter-system comparison</h3>
<p><a href='https://openreview.net/forum?id=Rry1SeSOQL'>https://openreview.net/forum?id=Rry1SeSOQL</a></p>
<p><b>Compressor summary</b>: The text proposes a new approach to Machine Translation Evaluation that compares translations without references using natural language inference and synthetic data, achieving state-of-the-art results on various benchmarks.</p><hr><h3>Achieving Human Parity in Content-Grounded Datasets Generation</h3>
<p><a href='https://openreview.net/forum?id=RjYKTQ0L0W'>https://openreview.net/forum?id=RjYKTQ0L0W</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for generating high-quality content-grounded data for tasks like LFQA and summarization, which performs well in human evaluation and outperforms existing models in faithfulness.</p><hr><h3>Learning to make adherence-aware advice</h3>
<p><a href='https://openreview.net/forum?id=RgELE1dQXx'>https://openreview.net/forum?id=RgELE1dQXx</a></p>
<p><b>Compressor summary</b>: The paper proposes a model for human-AI interactions that considers human adherence and selective advice, and presents learning algorithms for optimal policy and timing of advice.</p><hr><h3>Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design</h3>
<p><a href='https://openreview.net/forum?id=RemfXx7ebP'>https://openreview.net/forum?id=RemfXx7ebP</a></p>
<p><b>Compressor summary</b>: The study presents a data-driven pipeline for RNA sequence design based on tertiary structure using a large curated dataset, structural modeling, and contrastive learning with secondary structures as prior knowledge.</p><hr><h3>NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers</h3>
<p><a href='https://openreview.net/forum?id=Rc7dAwVL3v'>https://openreview.net/forum?id=Rc7dAwVL3v</a></p>
<p><b>Compressor summary</b>: NaturalSpeech 2 is a text-to-speech system that uses a neural audio codec and a diffusion model to generate high-quality, diverse speech from large datasets, even with zero-shot learning.</p><hr><h3>Tree Search-Based Policy Optimization under Stochastic Execution Delay</h3>
<p><a href='https://openreview.net/forum?id=RaqZX9LSGA'>https://openreview.net/forum?id=RaqZX9LSGA</a></p>
<p><b>Compressor summary</b>: The paper introduces a new formalism for Markov decision processes with random delays, called stochastic delayed execution MDPs, and proposes a model-based algorithm, Delayed EfficientZero, that achieves optimal performance by searching over the class of Markov policies.</p><hr><h3>Zero-Mean Regularized Spectral Contrastive Learning</h3>
<p><a href='https://openreview.net/forum?id=RZBy8oHTz4'>https://openreview.net/forum?id=RZBy8oHTz4</a></p>
<p><b>Compressor summary</b>: The paper proposes a simple modification to the spectral contrastive loss that improves performance and robustness in self-supervised learning by introducing zero-mean regularization for negative pairs.</p><hr><h3>Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning</h3>
<p><a href='https://openreview.net/forum?id=RXFVcynVe1'>https://openreview.net/forum?id=RXFVcynVe1</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel technique to improve graph learning with text by using language model explanations as features, which boost performance and speed on various text-attributed graph datasets.</p><hr><h3>Sufficient conditions for offline reactivation in recurrent neural networks</h3>
<p><a href='https://openreview.net/forum?id=RVrINT6MT7'>https://openreview.net/forum?id=RVrINT6MT7</a></p>
<p><b>Compressor summary</b>: This study develops a mathematical framework to explain how noisy neural networks can autonomously reactivate the same network states during periods of quiescence, such as sleep, based on their task optimization.</p><hr><h3>TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models</h3>
<p><a href='https://openreview.net/forum?id=RRayv1ZPN3'>https://openreview.net/forum?id=RRayv1ZPN3</a></p>
<p><b>Compressor summary</b>: TAIL is a framework for adapting large pretrained models to new control tasks efficiently using parameter-efficient fine-tuning techniques like LoRA, with minimal data and without forgetting previous knowledge.</p><hr><h3>Ensemble Distillation for Unsupervised Constituency Parsing</h3>
<p><a href='https://openreview.net/forum?id=RR8y0WKrFv'>https://openreview.net/forum?id=RR8y0WKrFv</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel ensemble method for unsupervised constituency parsing using tree averaging and multi-teacher distillation, which outperforms previous methods.</p><hr><h3>Efficient Instance-Optimal Finite-Sum Minimization</h3>
<p><a href='https://openreview.net/forum?id=RR70yWYenC'>https://openreview.net/forum?id=RR70yWYenC</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method, SIOPT-Grad, for minimizing finite-sums with instance-optimal guarantees, which requires fewer gradient evaluations than existing methods.</p><hr><h3>Weaker MVI Condition: Extragradient Methods with Multi-Step Exploration</h3>
<p><a href='https://openreview.net/forum?id=RNGUbTYSjk'>https://openreview.net/forum?id=RNGUbTYSjk</a></p>
<p><b>Compressor summary</b>: The paper presents a new framework of algorithms based on extragradient for min-max optimization in structured problems, with adaptive exploration to avoid limit cycles.</p><hr><h3>Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees</h3>
<p><a href='https://openreview.net/forum?id=RMgqvQGTwH'>https://openreview.net/forum?id=RMgqvQGTwH</a></p>
<p><b>Compressor summary</b>: The paper proposes a new hybrid RL algorithm that combines an on-policy actor-critic method with offline data, achieving both theoretical and empirical benefits.</p><hr><h3>Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions</h3>
<p><a href='https://openreview.net/forum?id=RLSWbk9kPw'>https://openreview.net/forum?id=RLSWbk9kPw</a></p>
<p><b>Compressor summary</b>: The paper proposes a neural sorting network for abstract inputs, improves the swap function's differentiability, and uses a Transformer network with multi-head attention to capture dependencies and leverage model capacity.</p><hr><h3>Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images</h3>
<p><a href='https://openreview.net/forum?id=RJDjSXNuAZ'>https://openreview.net/forum?id=RJDjSXNuAZ</a></p>
<p><b>Compressor summary</b>: The paper proposes a weakly supervised object detection method that uses image-level annotations and distills knowledge from a pre-trained model to detect viruses more efficiently than existing methods.</p><hr><h3>ConR: Contrastive Regularizer for Deep Imbalanced Regression</h3>
<p><a href='https://openreview.net/forum?id=RIuevDSK5V'>https://openreview.net/forum?id=RIuevDSK5V</a></p>
<p><b>Compressor summary</b>: The text introduces ConR, a contrastive regularizer that improves deep imbalanced regression by modeling global and local label similarities in feature space and penalizing incorrect proximities between samples.</p><hr><h3>Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting</h3>
<p><a href='https://openreview.net/forum?id=RIu5lyNXjT'>https://openreview.net/forum?id=RIu5lyNXjT</a></p>
<p><b>Compressor summary</b>: This work studies how different prompt formatting affects large language models' few-shot performance, and proposes FormatSpread to quickly evaluate a range of prompt formats for a task.</p><hr><h3>Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition</h3>
<p><a href='https://openreview.net/forum?id=RIcYTbpO38'>https://openreview.net/forum?id=RIcYTbpO38</a></p>
<p><b>Compressor summary</b>: Motion Coherent Augmentation is a data augmentation method for video recognition that uses Hue Jittering to introduce appearance variation and improve motion pattern detection.</p><hr><h3>Graph Generation with  $K^2$-trees</h3>
<p><a href='https://openreview.net/forum?id=RIEW6M9YoV'>https://openreview.net/forum?id=RIEW6M9YoV</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for generating graphs using a $K^2$ representation that captures the hierarchical structure of graphs and uses a Transformer-based architecture with a tree positional encoding scheme.</p><hr><h3>Demystifying Linear MDPs and Novel Dynamics Aggregation Framework</h3>
<p><a href='https://openreview.net/forum?id=RDSj6S8WJe'>https://openreview.net/forum?id=RDSj6S8WJe</a></p>
<p><b>Compressor summary</b>: This paper challenges the common assumption of linear MDPs and proposes a new hierarchical reinforcement learning framework called dynamics aggregation, which improves performance guarantees.</p><hr><h3>Scaling for Training Time and Post-hoc Out-of-distribution Detection Enhancement</h3>
<p><a href='https://openreview.net/forum?id=RDSTjtnqCg'>https://openreview.net/forum?id=RDSTjtnqCg</a></p>
<p><b>Compressor summary</b>: The paper discusses OOD detection methods, proposing SCALE and ISH to improve OOD/ID accuracy without compromising performance.</p><hr><h3>PAE: Reinforcement Learning from External Knowledge for Efficient Exploration</h3>
<p><a href='https://openreview.net/forum?id=R7rZUSGOPD'>https://openreview.net/forum?id=R7rZUSGOPD</a></p>
<p><b>Compressor summary</b>: PAE is a framework that teaches AI agents to learn from external knowledge, improving their exploration efficiency and task performance.</p><hr><h3>Memory-Consistent Neural Networks for Imitation Learning</h3>
<p><a href='https://openreview.net/forum?id=R3Tf7LDdX4'>https://openreview.net/forum?id=R3Tf7LDdX4</a></p>
<p><b>Compressor summary</b>: Memory-consistent neural networks (MCNNs) improve imitation learning by limiting policy errors using memory regions from expert demonstrations.</p><hr><h3>MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use</h3>
<p><a href='https://openreview.net/forum?id=R0c2qtalgG'>https://openreview.net/forum?id=R0c2qtalgG</a></p>
<p><b>Compressor summary</b>: The paper introduces MetaTool, a benchmark to evaluate if large language models can choose appropriate tools for various tasks, and finds that most LLMs struggle with this task but have room for improvement.</p><hr><h3>Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI</h3>
<p><a href='https://openreview.net/forum?id=QzTpTRVtrP'>https://openreview.net/forum?id=QzTpTRVtrP</a></p>
<p><b>Compressor summary</b>: Large Brain Model (LaBraM) is a unified foundation model for EEG that enables cross-dataset learning and achieves state-of-the-art performance on various downstream tasks.</p><hr><h3>A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning</h3>
<p><a href='https://openreview.net/forum?id=QyFm3D3Tzi'>https://openreview.net/forum?id=QyFm3D3Tzi</a></p>
<p><b>Compressor summary</b>: GPDiff is a new method that pre-trains STG models with a diffusion model and a transformer network to adapt to different cities' data and achieve better performance on various smart city tasks.</p><hr><h3>TorchRL: A data-driven decision-making library for PyTorch</h3>
<p><a href='https://openreview.net/forum?id=QxItoEAVMb'>https://openreview.net/forum?id=QxItoEAVMb</a></p>
<p><b>Compressor summary</b>: TorchRL is a PyTorch library that simplifies algorithm development and improves efficiency in decision and control tasks, with TensorDict as a new flexible primitive.</p><hr><h3>Exploring the Relationship Between Model Architecture and In-Context Learning Ability</h3>
<p><a href='https://openreview.net/forum?id=Qwq4cpLtoX'>https://openreview.net/forum?id=Qwq4cpLtoX</a></p>
<p><b>Compressor summary</b>: This study evaluates fifteen model architectures on synthetic in-context learning tasks and finds that contemporary architectures perform best as task complexity increases, while state-space models are more robust than transformers.</p><hr><h3>Is Generalized Dynamic Novel View Synthesis from Monocular Videos Possible Today?</h3>
<p><a href='https://openreview.net/forum?id=QuVlUn4T2G'>https://openreview.net/forum?id=QuVlUn4T2G</a></p>
<p><b>Compressor summary</b>: The paper explores whether a generalized method for synthesizing novel views from dynamic scenes in monocular videos is possible, and finds that it is partially feasible without scene-specific appearance optimization but requires consistent depth estimates.</p><hr><h3>Algorithms for Caching and MTS with reduced number of predictions</h3>
<p><a href='https://openreview.net/forum?id=QuIiLSktO4'>https://openreview.net/forum?id=QuIiLSktO4</a></p>
<p><b>Compressor summary</b>: The text introduces parsimonious algorithms for caching and multi-item single-server settings that use predictions efficiently and maintain good performance with varying prediction error.</p><hr><h3>PRIME: Prioritizing Interpretability in Failure Mode Extraction</h3>
<p><a href='https://openreview.net/forum?id=QrEHs9w5UF'>https://openreview.net/forum?id=QrEHs9w5UF</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to explain why image classification models fail by using human-understandable tags of images instead of clustering in feature space, resulting in better and simpler descriptions.</p><hr><h3>It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition</h3>
<p><a href='https://openreview.net/forum?id=QqjFHyQwtF'>https://openreview.net/forum?id=QqjFHyQwtF</a></p>
<p><b>Compressor summary</b>: The authors propose a new method, Uncertainty-Aware Dynamic Fusion (UADF), to improve generative error correction by incorporating acoustic information into the language model's output, leading to better speech recognition results.</p><hr><h3>Learning Multi-Agent Communication from Graph Modeling Perspective</h3>
<p><a href='https://openreview.net/forum?id=Qox9rO0kN0'>https://openreview.net/forum?id=Qox9rO0kN0</a></p>
<p><b>Compressor summary</b>: The paper proposes CommFormer, a method that learns a communication graph for multiple intelligent agents to improve collaboration in AI applications.</p><hr><h3>Localizing and Editing Knowledge In Text-to-Image Generative Models</h3>
<p><a href='https://openreview.net/forum?id=Qmw9ne6SOQ'>https://openreview.net/forum?id=Qmw9ne6SOQ</a></p>
<p><b>Compressor summary</b>: Text-to-image diffusion models use multiple components in the conditional UNet to store knowledge about different visual attributes, while only one causal state in the text-encoder is shared across them, enabling fast data-free model editing with DiffQuickFix.</p><hr><h3>LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment</h3>
<p><a href='https://openreview.net/forum?id=QmZKc7UZCy'>https://openreview.net/forum?id=QmZKc7UZCy</a></p>
<p><b>Compressor summary</b>: LanguageBind extends video-language pretraining to multiple modalities by using language as a bind, leading to improved performance in various tasks.</p><hr><h3>Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs</h3>
<p><a href='https://openreview.net/forum?id=QmYNBVukex'>https://openreview.net/forum?id=QmYNBVukex</a></p>
<p><b>Compressor summary</b>: This paper proposes a data selection method that improves pre-fine-tuning of pre-trained language models by aligning their distributions and achieves better performance, faster than existing techniques.</p><hr><h3>Towards Category Unification of 3D Single Object Tracking on Point Clouds</h3>
<p><a href='https://openreview.net/forum?id=QlqdXrzzD1'>https://openreview.net/forum?id=QlqdXrzzD1</a></p>
<p><b>Compressor summary</b>: The paper introduces unified models for 3D object tracking that adapt to different categories by encoding shape and size attributes using a transformer-based network called AdaFormer.</p><hr><h3>Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization</h3>
<p><a href='https://openreview.net/forum?id=QibPzdVrRu'>https://openreview.net/forum?id=QibPzdVrRu</a></p>
<p><b>Compressor summary</b>: The paper analyzes how a two-layer ReLU network with gradient flow and small initialization learns from well-separated input data, showing an upper bound on the time for neurons to align and convergence rate of the loss.</p><hr><h3>Efficient Heterogeneous Meta-Learning via Channel Shuffling Modulation</h3>
<p><a href='https://openreview.net/forum?id=QiJuMJl0QS'>https://openreview.net/forum?id=QiJuMJl0QS</a></p>
<p><b>Compressor summary</b>: The paper proposes a new heterogeneous meta-learning strategy that captures the multi-modality of task distribution using a neural permutation layer with sub-quadratic parameter complexity.</p><hr><h3>Learning Hierarchical Polynomials with Three-Layer Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=QgwAYFrh9t'>https://openreview.net/forum?id=QgwAYFrh9t</a></p>
<p><b>Compressor summary</b>: The paper shows how three-layer neural networks can efficiently learn hierarchical polynomials over Gaussian distribution by feature learning, outperforming kernel methods and two-layer networks.</p><hr><h3>Scalable and Effective Implicit Graph Neural Networks on Large Graphs</h3>
<p><a href='https://openreview.net/forum?id=QcMdPYBwTu'>https://openreview.net/forum?id=QcMdPYBwTu</a></p>
<p><b>Compressor summary</b>: SEIGNN is a new efficient implicit GNN that uses mini-batch training and a stochastic solver to handle large graphs, while maintaining prediction accuracy.</p><hr><h3>Scaling Supervised Local Learning with Augmented Auxiliary Networks</h3>
<p><a href='https://openreview.net/forum?id=Qbf1hy8b7m'>https://openreview.net/forum?id=Qbf1hy8b7m</a></p>
<p><b>Compressor summary</b>: AugLocal is a local learning method that enhances the synergy between layers and reduces memory usage in deep neural networks by selectively incorporating some layers into auxiliary networks and adjusting their depth.</p><hr><h3>OpenTab: Advancing Large Language Models as Open-domain Table Reasoners</h3>
<p><a href='https://openreview.net/forum?id=Qa0ULgosc9'>https://openreview.net/forum?id=Qa0ULgosc9</a></p>
<p><b>Compressor summary</b>: OpenTab is a table reasoning framework using LLMs that retrieves relevant tables, generates SQL programs, and conducts grounded inference for accurate responses on structured data.</p><hr><h3>MOFI: Learning Image Representations from Noisy Entity Annotated Images</h3>
<p><a href='https://openreview.net/forum?id=QQYpgReSRk'>https://openreview.net/forum?id=QQYpgReSRk</a></p>
<p><b>Compressor summary</b>: MOFI is a new vision foundation model that learns from noisy image-text pairs using entity labels and achieves state-of-the-art performance on various tasks.</p><hr><h3>MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field</h3>
<p><a href='https://openreview.net/forum?id=QQ6RgKYiQq'>https://openreview.net/forum?id=QQ6RgKYiQq</a></p>
<p><b>Compressor summary</b>: MovingParts is a NeRF-based method for reconstructing dynamic scenes and discovering parts by tracking particle trajectories under both Eulerian and Lagrangian views.</p><hr><h3>Decodable and Sample Invariance Continuous Object Encoder</h3>
<p><a href='https://openreview.net/forum?id=QLoepRnoue'>https://openreview.net/forum?id=QLoepRnoue</a></p>
<p><b>Compressor summary</b>: HDFE is a vector representation method for continuous objects that works with neural networks and improves performance on tasks like function-to-function mapping and point cloud surface normal estimation.</p><hr><h3>Guaranteed Approximation Bounds for Mixed-Precision Neural Operators</h3>
<p><a href='https://openreview.net/forum?id=QJGj07PD9C'>https://openreview.net/forum?id=QJGj07PD9C</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to train Fourier Neural Operators (FNO) in mixed precision, reducing memory usage and increasing training speed while maintaining accuracy.</p><hr><h3>Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs</h3>
<p><a href='https://openreview.net/forum?id=QHROe7Mfcb'>https://openreview.net/forum?id=QHROe7Mfcb</a></p>
<p><b>Compressor summary</b>: The paper proposes a one-shot subgraph reasoning method for efficient and adaptive knowledge graph reasoning using Personalized PageRank heuristics and optimal configuration searching.</p><hr><h3>Adaptive Sharpness-Aware Pruning for Robust Sparse Networks</h3>
<p><a href='https://openreview.net/forum?id=QFYVVwiAM8'>https://openreview.net/forum?id=QFYVVwiAM8</a></p>
<p><b>Compressor summary</b>: AdaSAP is a method that creates sparse deep learning models with better robustness to unseen input variations by optimizing the loss landscape through weight perturbations during pruning.</p><hr><h3>Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners</h3>
<p><a href='https://openreview.net/forum?id=Q53QLftNkA'>https://openreview.net/forum?id=Q53QLftNkA</a></p>
<p><b>Compressor summary</b>: The MW-MAE model uses a novel attention module to learn better general-purpose audio representations and improve performance on various downstream tasks, by capturing local and global information independently in each decoder block.</p><hr><h3>Lemur: Integrating Large Language Models in Automated Program Verification</h3>
<p><a href='https://openreview.net/forum?id=Q3YaCghZNt'>https://openreview.net/forum?id=Q3YaCghZNt</a></p>
<p><b>Compressor summary</b>: The text proposes a method to use LLMs with automated reasoners for program verification and shows its effectiveness on benchmarks.</p><hr><h3>MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection</h3>
<p><a href='https://openreview.net/forum?id=Q1vkAhdI6j'>https://openreview.net/forum?id=Q1vkAhdI6j</a></p>
<p><b>Compressor summary</b>: MixSup is a 3D object detection method that uses cheap cluster labels and some box labels to learn semantics and geometry, improving performance on various datasets.</p><hr><h3>SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</h3>
<p><a href='https://openreview.net/forum?id=Q1u25ahSuy'>https://openreview.net/forum?id=Q1u25ahSuy</a></p>
<p><b>Compressor summary</b>: SpQR is a new compression technique that allows near-lossless LLMs on memory-limited devices by isolating outlier weights and compressing others to 3-4 bits, achieving up to 15% speedup with minimal accuracy loss.</p><hr><h3>A Simple and Effective Pruning Approach for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=PxoFut3dWW'>https://openreview.net/forum?id=PxoFut3dWW</a></p>
<p><b>Compressor summary</b>: Wanda is a novel, straightforward, and effective pruning method for Large Language Models that does not require retraining or weight update.</p><hr><h3>Masked Completion via Structured Diffusion with White-Box Transformers</h3>
<p><a href='https://openreview.net/forum?id=PvyOYleymy'>https://openreview.net/forum?id=PvyOYleymy</a></p>
<p><b>Compressor summary</b>: The authors propose a new white-box architecture, CRATE-MAE, for large-scale unsupervised representation learning of images that is interpretable, structured, and efficient.</p><hr><h3>Quadratic models for understanding neural network dynamics</h3>
<p><a href='https://openreview.net/forum?id=PvJnX3dwsD'>https://openreview.net/forum?id=PvJnX3dwsD</a></p>
<p><b>Compressor summary</b>: Neural Quadratic Models can mimic the "catapult phase" and help analyze neural networks' generalization behavior.</p><hr><h3>Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Networks</h3>
<p><a href='https://openreview.net/forum?id=PudduufFLa'>https://openreview.net/forum?id=PudduufFLa</a></p>
<p><b>Compressor summary</b>: This paper introduces a new way to represent geographical data in machine learning models using a combination of spherical harmonic functions and sinusoidal networks, which improves performance and avoids artifacts.</p><hr><h3>Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion</h3>
<p><a href='https://openreview.net/forum?id=Psl75UCoZM'>https://openreview.net/forum?id=Psl75UCoZM</a></p>
<p><b>Compressor summary</b>: The authors propose a novel world modeling approach that combines VQVAE and discrete diffusion to handle complex robotic observations and achieve better predictions than existing methods.</p><hr><h3>Learning to Solve Bilevel Programs with Binary Tender</h3>
<p><a href='https://openreview.net/forum?id=PsDFgTosqb'>https://openreview.net/forum?id=PsDFgTosqb</a></p>
<p><b>Compressor summary</b>: The paper proposes deep learning methods for solving bilevel programs with discrete decision variables, comparing two types of neural networks and introducing an enhanced sampling technique.</p><hr><h3>A Linear Algebraic Framework for Counterfactual Generation</h3>
<p><a href='https://openreview.net/forum?id=PoDkdFQIu3'>https://openreview.net/forum?id=PoDkdFQIu3</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to generate synthetic counterfactual data for estimating time-varying causal effects in real-world settings and shows its superior performance over existing methods.</p><hr><h3>Deep Geodesic Canonical Correlation Analysis for Covariance-Based Neuroimaging Data</h3>
<p><a href='https://openreview.net/forum?id=PnR1MNen7u'>https://openreview.net/forum?id=PnR1MNen7u</a></p>
<p><b>Compressor summary</b>: DeepGeoCCA is a novel deep learning framework for multi-view and multi-modal neuroimaging data that enhances the correlation of latent representations while preserving geometric structures.</p><hr><h3>Fusing Models with Complementary Expertise</h3>
<p><a href='https://openreview.net/forum?id=PhMrGCMIRL'>https://openreview.net/forum?id=PhMrGCMIRL</a></p>
<p><b>Compressor summary</b>: The paper proposes a supervised learning method for fusing outputs of different expert models to improve generalization across tasks and domains in various AI applications, including text classification and summarization.</p><hr><h3>From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction</h3>
<p><a href='https://openreview.net/forum?id=PfPnugdxup'>https://openreview.net/forum?id=PfPnugdxup</a></p>
<p><b>Compressor summary</b>: This paper introduces Joint Multi-domain Pre-training (JMP), a method that uses data from multiple chemical domains to generate transferable atomic representations for various molecular tasks with high accuracy.</p><hr><h3>Making RL with Preference-based Feedback Efficient via Randomization</h3>
<p><a href='https://openreview.net/forum?id=Pe2lo3QOvo'>https://openreview.net/forum?id=Pe2lo3QOvo</a></p>
<p><b>Compressor summary</b>: Our algorithm for RLHF using preferences over trajectories minimizes regret bound and query complexity by combining randomization, active learning, and Bayesian supervised learning oracles in linear MDPs and general nonlinear function approximation settings.</p><hr><h3>Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors</h3>
<p><a href='https://openreview.net/forum?id=PdaPky8MUn'>https://openreview.net/forum?id=PdaPky8MUn</a></p>
<p><b>Compressor summary</b>: Pretraining with denoising objectives improves long-range dependency modeling across various architectures and reduces the gap between Transformers and state space models.</p><hr><h3>Cross$Q$: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity</h3>
<p><a href='https://openreview.net/forum?id=PczQtTsTIX'>https://openreview.net/forum?id=PczQtTsTIX</a></p>
<p><b>Compressor summary</b>: CrossQ is a deep reinforcement learning algorithm that achieves high sample efficiency with low computational cost by using Batch Normalization and removing target networks, without relying on advanced bias-reduction schemes.</p><hr><h3>Pre-training with Synthetic Data Helps Offline Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=PcxQgtHGj2'>https://openreview.net/forum?id=PcxQgtHGj2</a></p>
<p><b>Compressor summary</b>: This paper shows that pre-training for offline deep reinforcement learning can be done with simple synthetic data, instead of language pre-training, to improve performance.</p><hr><h3>Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control</h3>
<p><a href='https://openreview.net/forum?id=Pc8AU1aF5e'>https://openreview.net/forum?id=Pc8AU1aF5e</a></p>
<p><b>Compressor summary</b>: Synapse is an agent that uses trajectory-as-exemplar prompting and memory to improve performance, efficiency, and generalizability in computer control tasks with large language models.</p><hr><h3>From Matching to Mixing: A Graph Interpolation Approach for SAT Instance Generation</h3>
<p><a href='https://openreview.net/forum?id=PXXuLvIH5r'>https://openreview.net/forum?id=PXXuLvIH5r</a></p>
<p><b>Compressor summary</b>: The paper proposes a graph interpolation method to modify SAT instances by exchanging clause pairs based on node correspondence learned from a matching network, with the option of adding Gumbel noise for hardness preservation.</p><hr><h3>SOHES: Self-supervised Open-world Hierarchical Entity Segmentation</h3>
<p><a href='https://openreview.net/forum?id=PXNrncg2DF'>https://openreview.net/forum?id=PXNrncg2DF</a></p>
<p><b>Compressor summary</b>: SOHES is a novel self-supervised approach for open-world entity segmentation that requires no human annotations and captures entities' parts hierarchically.</p><hr><h3>Understanding the Effects of RLHF on LLM Generalisation and Diversity</h3>
<p><a href='https://openreview.net/forum?id=PXD3FAVHJT'>https://openreview.net/forum?id=PXD3FAVHJT</a></p>
<p><b>Compressor summary</b>: RLHF improves OOD generalization but reduces output diversity in LLMs, highlighting a tradeoff between these properties.</p><hr><h3>Integrating Planning and Deep Reinforcement Learning via Automatic Induction of Task Substructures</h3>
<p><a href='https://openreview.net/forum?id=PR6RMsxuW7'>https://openreview.net/forum?id=PR6RMsxuW7</a></p>
<p><b>Compressor summary</b>: Our framework combines DRL with classical planning using symbolic regression to induce task structures from few demonstrations, achieving better sample efficiency and generalization than baselines.</p><hr><h3>Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold</h3>
<p><a href='https://openreview.net/forum?id=PQbFUMKLFp'>https://openreview.net/forum?id=PQbFUMKLFp</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method for distributed optimization on the Stiefel manifold using Riemannian conjugate gradient descent with reduced computational complexity and global convergence guarantees.</p><hr><h3>Confidential-DPproof: Confidential Proof of Differentially Private Training</h3>
<p><a href='https://openreview.net/forum?id=PQY2v6VtGe'>https://openreview.net/forum?id=PQY2v6VtGe</a></p>
<p><b>Compressor summary</b>: Confidential-DPproof is a framework that generates a cryptographic certificate of differential privacy during model training, overcoming limitations of post hoc privacy auditing techniques.</p><hr><h3>Transport meets Variational Inference: Controlled Monte Carlo Diffusions</h3>
<p><a href='https://openreview.net/forum?id=PP1rudnxiW'>https://openreview.net/forum?id=PP1rudnxiW</a></p>
<p><b>Compressor summary</b>: The text introduces a new method called Controlled Monte Carlo Diffusion (CMCD) for Bayesian computation, which adapts forward and backward dynamics in a diffusion model and performs well in various experiments.</p><hr><h3>ACRF: Compressing Explicit Neural Radiance Fields via Attribute Compression</h3>
<p><a href='https://openreview.net/forum?id=POFrdKvpea'>https://openreview.net/forum?id=POFrdKvpea</a></p>
<p><b>Compressor summary</b>: The paper proposes ACRF, a framework for compressing explicit NeRF models by pruning the neural 3D structure, encoding features with importance, and estimating probability distributions of transform coefficients.</p><hr><h3>Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization</h3>
<p><a href='https://openreview.net/forum?id=PKICZXVY9M'>https://openreview.net/forum?id=PKICZXVY9M</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel approach called OGEN to improve open-domain visual recognition by synthesizing unknown class features and using adaptive self-distillation to prevent overfitting during finetuning.</p><hr><h3>EasyTPP: Towards Open Benchmarking Temporal Point Processes</h3>
<p><a href='https://openreview.net/forum?id=PJwAkg0z7h'>https://openreview.net/forum?id=PJwAkg0z7h</a></p>
<p><b>Compressor summary</b>: EasyTPP is a central repository of event sequence modeling assets, including data, models, evaluation programs, and documentation, to facilitate research and progress in temporal point processes.</p><hr><h3>Reinforcement Symbolic Regression Machine</h3>
<p><a href='https://openreview.net/forum?id=PJVUWpPnZC'>https://openreview.net/forum?id=PJVUWpPnZC</a></p>
<p><b>Compressor summary</b>: RSRM is a novel machine learning model that uses Monte Carlo tree search, Double Q-learning, and sub-tree discovery to find complex math equations from limited data.</p><hr><h3>Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients</h3>
<p><a href='https://openreview.net/forum?id=PHLVmV88Zy'>https://openreview.net/forum?id=PHLVmV88Zy</a></p>
<p><b>Compressor summary</b>: The paper introduces fast and effective algorithms for linear and deep CCA using stochastic gradient descent, which improve the performance on standard benchmarks and allow big data analysis in biomedical applications. It also establishes a theoretical link between classical CCA and self-supervised learning.</p><hr><h3>Compositional VLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding</h3>
<p><a href='https://openreview.net/forum?id=PHGxChm1l5'>https://openreview.net/forum?id=PHGxChm1l5</a></p>
<p><b>Compressor summary</b>: The Compositional VLM model enhances the ability of large language models to reason compositionally about visual entities and their relations, achieving significant improvements on various vision-language benchmarks.</p><hr><h3>RTFS-Net: Recurrent time-frequency modelling for efficient audio-visual speech separation</h3>
<p><a href='https://openreview.net/forum?id=PEuDO2EiDr'>https://openreview.net/forum?id=PEuDO2EiDr</a></p>
<p><b>Compressor summary</b>: RTFS-Net is a novel audio-visual speech separation method that uses multi-layered RNNs and attention-based fusion in the time-frequency domain, achieving better performance than existing time-domain methods with fewer parameters and lower computation costs.</p><hr><h3>Safe and Robust Watermark Injection with a Single OoD Image</h3>
<p><a href='https://openreview.net/forum?id=PCm1oT8pZI'>https://openreview.net/forum?id=PCm1oT8pZI</a></p>
<p><b>Compressor summary</b>: The paper proposes a watermark injection technique for deep neural networks using an out-of-distribution image as a secret key, which is robust against common attacks and does not require training data or samples.</p><hr><h3>BRUSLEATTACK: QUERY-EFFICIENT SCORE-BASED SPARSE ADVERSARIAL ATTACK</h3>
<p><a href='https://openreview.net/forum?id=PAfnMGXief'>https://openreview.net/forum?id=PAfnMGXief</a></p>
<p><b>Compressor summary</b>: The text describes a new algorithm for generating sparse adversarial examples by observing score-based replies to model queries, which helps evaluate model vulnerabilities more efficiently.</p><hr><h3>Implicit bias of SGD in $L_2$-regularized linear DNNs: One-way jumps from high to low rank</h3>
<p><a href='https://openreview.net/forum?id=P1aobHnjjj'>https://openreview.net/forum?id=P1aobHnjjj</a></p>
<p><b>Compressor summary</b>: The paper studies how stochastic gradient descent (SGD) can escape local minima with different ranks in deep linear networks, and proposes a sequence of sets to capture the minima of lower rank.</p><hr><h3>H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object Surface Fields</h3>
<p><a href='https://openreview.net/forum?id=P1ANzoGg3W'>https://openreview.net/forum?id=P1ANzoGg3W</a></p>
<p><b>Compressor summary</b>: H2O-SDF is a new method for 3D indoor scene reconstruction that uses a two-phase learning approach and the novel Object Surface Field to balance room layout preservation and detailed object surface capture.</p><hr><h3>Learning Energy Decompositions for Partial Inference of GFlowNets</h3>
<p><a href='https://openreview.net/forum?id=P15CHILQlg'>https://openreview.net/forum?id=P15CHILQlg</a></p>
<p><b>Compressor summary</b>: The paper proposes LED-GFN, a method to improve GFlowNets by learning energy decompositions for state transitions and reparameterizing flow functions using these potentials.</p><hr><h3>TD-MPC2: Scalable, Robust World Models for Continuous Control</h3>
<p><a href='https://openreview.net/forum?id=Oxh5CstDJU'>https://openreview.net/forum?id=Oxh5CstDJU</a></p>
<p><b>Compressor summary</b>: TD-MPC2 is a new algorithm that improves the performance of model-based reinforcement learning in various tasks and domains by optimizing local trajectories in the latent space of implicit world models.</p><hr><h3>Unlocking the Power of Representations in Long-term Novelty-based Exploration</h3>
<p><a href='https://openreview.net/forum?id=OwtMhMSybu'>https://openreview.net/forum?id=OwtMhMSybu</a></p>
<p><b>Compressor summary</b>: RECODE is a novelty-based exploration method that uses clustering and masked transformers to achieve state-of-the-art results in challenging 3D and Atari exploration tasks.</p><hr><h3>Augmented Bayesian Policy Search</h3>
<p><a href='https://openreview.net/forum?id=OvlcyABNQT'>https://openreview.net/forum?id=OvlcyABNQT</a></p>
<p><b>Compressor summary</b>: ABS is a method that combines BO and policy gradient methods for exploration using deterministic policies, improving performance in locomotion problems.</p><hr><h3>Amortizing intractable inference in large language models</h3>
<p><a href='https://openreview.net/forum?id=Ouj6p4ca60'>https://openreview.net/forum?id=Ouj6p4ca60</a></p>
<p><b>Compressor summary</b>: The paper proposes using diversity-seeking reinforcement learning to fine-tune large language models for tasks requiring sampling from intractable posterior distributions, such as chain-of-thought reasoning.</p><hr><h3>ImagenHub: Standardizing the evaluation of conditional image generation models</h3>
<p><a href='https://openreview.net/forum?id=OuV9ZrkQlc'>https://openreview.net/forum?id=OuV9ZrkQlc</a></p>
<p><b>Compressor summary</b>: ImagenHub is a one-stop library that standardizes inference and evaluation for various conditional image generation tasks, providing evaluation datasets, a unified pipeline, human evaluation scores, and guidelines.</p><hr><h3>Learning From Simplicial Data Based on Random Walks and 1D Convolutions</h3>
<p><a href='https://openreview.net/forum?id=OsGUnYOzii'>https://openreview.net/forum?id=OsGUnYOzii</a></p>
<p><b>Compressor summary</b>: SCRaWl is a novel simplicial complex neural network architecture that uses random walks and fast 1D convolutions to achieve better classification performance and higher-order relationships than graph-based methods, while controlling the computational cost.</p><hr><h3>Universal Humanoid Motion Representations for Physics-Based Control</h3>
<p><a href='https://openreview.net/forum?id=OrOd8PxOO2'>https://openreview.net/forum?id=OrOd8PxOO2</a></p>
<p><b>Compressor summary</b>: The paper presents a universal motion representation for humanoid control that learns from a large unstructured motion dataset, enabling diverse and realistic human behavior in complex tasks.</p><hr><h3>DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=OqTMUPuLuC'>https://openreview.net/forum?id=OqTMUPuLuC</a></p>
<p><b>Compressor summary</b>: The text proposes a new framework, DiLu, that uses large language models to enable autonomous driving systems with knowledge-driven capabilities similar to human drivers, improving decision-making and generalization.</p><hr><h3>PILOT: An $\mathcal{O}(1/T)$-Convergent Approach for Policy Evaluation with Nonlinear Function Approximation</h3>
<p><a href='https://openreview.net/forum?id=OkHHJcMroY'>https://openreview.net/forum?id=OkHHJcMroY</a></p>
<p><b>Compressor summary</b>: PILOT and PILOT$^+$ are new RL policy evaluation methods that achieve fast convergence, sample efficiency, and apply constant step sizes for nonlinear function approximation.</p><hr><h3>Estimating Conditional Mutual Information for Dynamic Feature Selection</h3>
<p><a href='https://openreview.net/forum?id=Oju2Qu9jvn'>https://openreview.net/forum?id=Oju2Qu9jvn</a></p>
<p><b>Compressor summary</b>: The paper proposes a new approach for dynamic feature selection using mutual information estimation and various improvements to achieve better performance and transparency in prediction processes.</p><hr><h3>ALAM: Averaged Low-Precision Activation for Memory-Efficient Training of Transformer Models</h3>
<p><a href='https://openreview.net/forum?id=OfXqQ5TRwp'>https://openreview.net/forum?id=OfXqQ5TRwp</a></p>
<p><b>Compressor summary</b>: ALAM is a novel framework that uses average quantization and a lightweight sensitivity calculation scheme to compress activations in large language models, reducing memory overhead without sacrificing performance.</p><hr><h3>Anisotropy helps: improved statistical and computational complexity of the mean-field Langevin dynamics under structured data</h3>
<p><a href='https://openreview.net/forum?id=Of2nEDc4s7'>https://openreview.net/forum?id=Of2nEDc4s7</a></p>
<p><b>Compressor summary</b>: The text discusses how gradient-based feature learning in neural networks can improve when dealing with anisotropic input data and suggests using anisotropic weight decay regularization for efficient learning.</p><hr><h3>Spectrally Transformed Kernel Regression</h3>
<p><a href='https://openreview.net/forum?id=OeQE9zsztS'>https://openreview.net/forum?id=OeQE9zsztS</a></p>
<p><b>Compressor summary</b>: The paper proposes a unified theory for learning with unlabeled data and a base kernel, using spectrally transformed kernel regression (STKR), which can handle various smooth target functions and provide scalable and statistically guaranteed implementations.</p><hr><h3>TapMo: Shape-aware Motion Generation of Skeleton-free Characters</h3>
<p><a href='https://openreview.net/forum?id=OeH6Fdhv7q'>https://openreview.net/forum?id=OeH6Fdhv7q</a></p>
<p><b>Compressor summary</b>: TapMo is a text-driven animation pipeline that generates high-quality motions for various skeleton-free 3D characters without the need for rigging.</p><hr><h3>More is Better: when Infinite Overparameterization is Optimal and Overfitting is Obligatory</h3>
<p><a href='https://openreview.net/forum?id=OdpIjS0vkO'>https://openreview.net/forum?id=OdpIjS0vkO</a></p>
<p><b>Compressor summary</b>: The paper shows theoretically and empirically how larger models, more data, and lower training loss improve performance in random feature regression, which applies to shallow neural networks with enough features and samples.</p><hr><h3>Efficient Sharpness-Aware Minimization for Molecular Graph Transformer Models</h3>
<p><a href='https://openreview.net/forum?id=Od39h4XQ3Y'>https://openreview.net/forum?id=Od39h4XQ3Y</a></p>
<p><b>Compressor summary</b>: GraphSAM improves efficiency and generalization of graph transformer models by using gradient approximation and loss landscape approximation.</p><hr><h3>Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge Distillation</h3>
<p><a href='https://openreview.net/forum?id=OZitfSXpdT'>https://openreview.net/forum?id=OZitfSXpdT</a></p>
<p><b>Compressor summary</b>: The paper proposes an adaptive method to learn the optimal ratio of using soft and hard supervision in knowledge distillation for training compact networks, improving performance across different tasks and architectures.</p><hr><h3>Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing</h3>
<p><a href='https://openreview.net/forum?id=OXv0zQ1umU'>https://openreview.net/forum?id=OXv0zQ1umU</a></p>
<p><b>Compressor summary</b>: Evoke is a framework that uses two instances of a large language model, one as an author and one as a reviewer, to automatically refine prompts for natural language processing tasks, focusing on hard samples to improve performance.</p><hr><h3>Reward Design for Justifiable Sequential Decision-Making</h3>
<p><a href='https://openreview.net/forum?id=OUkZXbbwQr'>https://openreview.net/forum?id=OUkZXbbwQr</a></p>
<p><b>Compressor summary</b>: The paper proposes a debate-based reward model for reinforcement learning agents to train justifiable policies in high-stakes situations, such as prescribing and justifying treatment decisions for septic patients.</p><hr><h3>$\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States</h3>
<p><a href='https://openreview.net/forum?id=OUeIBFhyem'>https://openreview.net/forum?id=OUeIBFhyem</a></p>
<p><b>Compressor summary</b>: $\infty$-Diff is a generative diffusion model that learns continuous functions in infinite-dimensional Hilbert space and can sample at arbitrary resolutions with high quality and efficiency.</p><hr><h3>CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding</h3>
<p><a href='https://openreview.net/forum?id=ORUiqcLpV6'>https://openreview.net/forum?id=ORUiqcLpV6</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel interpretable 3D visual grounding framework that uses a chain-of-thoughts approach to localize objects in complex scenes, improving performance and data efficiency.</p><hr><h3>Headless Language Models: Learning without Predicting with Contrastive Weight Tying</h3>
<p><a href='https://openreview.net/forum?id=ONPECq0Rk7'>https://openreview.net/forum?id=ONPECq0Rk7</a></p>
<p><b>Compressor summary</b>: The study proposes a new pre-training method for language models that reconstructs input embeddings using Contrastive Weight Tying, reducing training time and improving performance on downstream tasks.</p><hr><h3>MAP IT to Visualize Representations</h3>
<p><a href='https://openreview.net/forum?id=OKf6JtXtoy'>https://openreview.net/forum?id=OKf6JtXtoy</a></p>
<p><b>Compressor summary</b>: MAP IT is a new method for dimensionality reduction in visualization that captures local information and produces better class structure than existing methods.</p><hr><h3>Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization</h3>
<p><a href='https://openreview.net/forum?id=OIsahq1UYC'>https://openreview.net/forum?id=OIsahq1UYC</a></p>
<p><b>Compressor summary</b>: The paper proposes a new sampling method (DGFS) for high-dimensional density functions that uses partial trajectories and a flow function to improve learning speed and accuracy.</p><hr><h3>GenSim: Generating Robotic Simulation Tasks via Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=OI3RoHoWAN'>https://openreview.net/forum?id=OI3RoHoWAN</a></p>
<p><b>Compressor summary</b>: The paper proposes GenSim, a method that uses large language models to generate diverse simulation tasks and expert demonstrations, improving task-level generalization and transfer to real-world robotic policies.</p><hr><h3>Protein Multimer Structure Prediction via PPI-guided Prompt Learning</h3>
<p><a href='https://openreview.net/forum?id=OHpvivXrQr'>https://openreview.net/forum?id=OHpvivXrQr</a></p>
<p><b>Compressor summary</b>: PromptMSP is a pre-training and prompt tuning framework that improves multimer structure prediction by extending protein-protein interaction knowledge to different scales of multimers, achieving significant accuracy and efficiency improvements.</p><hr><h3>Multi-View Causal Representation Learning with Partial Observability</h3>
<p><a href='https://openreview.net/forum?id=OGtnhKQJms'>https://openreview.net/forum?id=OGtnhKQJms</a></p>
<p><b>Compressor summary</b>: The paper presents a unified framework for learning identifiable representations from multiple partial and causally related views using contrastive learning and a set of rules called identifiability algebra.</p><hr><h3>Doubly Robust Instance-Reweighted Adversarial Training</h3>
<p><a href='https://openreview.net/forum?id=OF5x1dzWSS'>https://openreview.net/forum?id=OF5x1dzWSS</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel adversarial training method that uses distributionally robust optimization to obtain importance weights and improve robustness across the training distribution, especially for vulnerable data points.</p><hr><h3>DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=OEL4FJMg1b'>https://openreview.net/forum?id=OEL4FJMg1b</a></p>
<p><b>Compressor summary</b>: The paper proposes DragonDiffusion, a novel method for editing images using diffusion models that allows precise Drag-style manipulations by leveraging feature correspondence and multi-scale guidance.</p><hr><h3>Aligning Relational Learning with Lipschitz Fairness</h3>
<p><a href='https://openreview.net/forum?id=ODSgo2m8aE'>https://openreview.net/forum?id=ODSgo2m8aE</a></p>
<p><b>Compressor summary</b>: This paper explores using the Lipschitz constant to control output perturbations in graph neural networks (GNNs) caused by biases, improving fairness in GNN training.</p><hr><h3>Defining and extracting generalizable interaction primitives from DNNs</h3>
<p><a href='https://openreview.net/forum?id=OCqyFVFNeF'>https://openreview.net/forum?id=OCqyFVFNeF</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to find shared interactions between deep neural networks trained for the same task, which can help explain their knowledge more faithfully.</p><hr><h3>Biased Temporal Convolution Graph Network for Time Series Forecasting with Missing Values.</h3>
<p><a href='https://openreview.net/forum?id=O9nZCwdGcG'>https://openreview.net/forum?id=O9nZCwdGcG</a></p>
<p><b>Compressor summary</b>: The paper proposes a new model, Biased Temporal Convolution Graph Network (BiaTCGNet), that can handle missing data and improve multivariate time series forecasting accuracy by injecting bias into two modules.</p><hr><h3>Leveraging Optimization for Adaptive Attacks on Image Watermarks</h3>
<p><a href='https://openreview.net/forum?id=O9PArxKLe1'>https://openreview.net/forum?id=O9PArxKLe1</a></p>
<p><b>Compressor summary</b>: The text discusses the problem of untrustworthy users misusing image generators for spam and disinformation, and proposes watermarking as a solution to detect such content, but also shows that adaptive attacks can break existing watermarking methods.</p><hr><h3>CNN Kernels Can Be the Best Shapelets</h3>
<p><a href='https://openreview.net/forum?id=O8ouVV8PjF'>https://openreview.net/forum?id=O8ouVV8PjF</a></p>
<p><b>Compressor summary</b>: ShapeConv combines shapelets and CNN to create an interpretable and controllable time series model that performs well.</p><hr><h3>Modelling complex vector drawings with stroke-clouds</h3>
<p><a href='https://openreview.net/forum?id=O2jyuo89CK'>https://openreview.net/forum?id=O2jyuo89CK</a></p>
<p><b>Compressor summary</b>: The paper presents a new generative model for complex vector drawings that uses stroke-clouds and latent codes to capture their distribution on an implicit set space.</p><hr><h3>Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts</h3>
<p><a href='https://openreview.net/forum?id=O072Rc8uUy'>https://openreview.net/forum?id=O072Rc8uUy</a></p>
<p><b>Compressor summary</b>: The paper introduces Progressive3D, a framework for generating accurate 3D content from complex prompts using multiple editing steps and user-defined regions, as well as a suppression technique to focus on semantic differences.</p><hr><h3>Toward effective protection against diffusion-based mimicry through score distillation</h3>
<p><a href='https://openreview.net/forum?id=NzxCMe88HX'>https://openreview.net/forum?id=NzxCMe88HX</a></p>
<p><b>Compressor summary</b>: The text discusses the vulnerabilities of latent diffusion models in generating realistic images and proposes new strategies for effective protection against misuse.</p><hr><h3>Masked Audio Generative Modeling</h3>
<p><a href='https://openreview.net/forum?id=Ny8NiVfi95'>https://openreview.net/forum?id=Ny8NiVfi95</a></p>
<p><b>Compressor summary</b>: MAGNeT is a fast text-to-music model that combines masked sequence modeling with rescoring and hybrid decoding, achieving comparable quality to autoregressive models.</p><hr><h3>EXPOSING TEXT-IMAGE INCONSISTENCY USING DIFFUSION MODELS</h3>
<p><a href='https://openreview.net/forum?id=Ny150AblPu'>https://openreview.net/forum?id=Ny150AblPu</a></p>
<p><b>Compressor summary</b>: D-TIML is a new method that uses diffusion models to find and visualize inconsistencies between texts and images in misleading pairs, and it comes with a large dataset to test its effectiveness.</p><hr><h3>Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation</h3>
<p><a href='https://openreview.net/forum?id=NxoFmGgWC9'>https://openreview.net/forum?id=NxoFmGgWC9</a></p>
<p><b>Compressor summary</b>: The paper presents GR-1, a GPT-style model for language-conditioned visual robot manipulation, which improves success rates on the CALVIN benchmark and real robots by using large-scale video generative pre-training.</p><hr><h3>FOSI: Hybrid First and Second Order Optimization</h3>
<p><a href='https://openreview.net/forum?id=NvbeD9Ttkx'>https://openreview.net/forum?id=NvbeD9Ttkx</a></p>
<p><b>Compressor summary</b>: FOSI is a meta-algorithm that enhances first-order optimizers by using second-order information to improve convergence rate and optimization time.</p><hr><h3>InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists</h3>
<p><a href='https://openreview.net/forum?id=Nu9mOSq7eH'>https://openreview.net/forum?id=Nu9mOSq7eH</a></p>
<p><b>Compressor summary</b>: This paper proposes a unified language interface for computer vision tasks that uses text-to-image generation to perform various tasks like segmentation, object detection, etc., with the help of natural language instructions.</p><hr><h3>3D Reconstruction with Generalizable Neural Fields using Scene Priors</h3>
<p><a href='https://openreview.net/forum?id=Nu7dDaVF5a'>https://openreview.net/forum?id=Nu7dDaVF5a</a></p>
<p><b>Compressor summary</b>: NFP is a method that uses scene priors to train a network that reconstructs 3D scenes from single-view RGB-D images without a fusion module and enables single-image novel-view synthesis.</p><hr><h3>Learning to Jointly Understand Visual and Tactile Signals</h3>
<p><a href='https://openreview.net/forum?id=NtQqIcSbqv'>https://openreview.net/forum?id=NtQqIcSbqv</a></p>
<p><b>Compressor summary</b>: The authors study how human hands manipulate objects, collect visual-tactile data, and propose a method to learn a cross-modal latent manifold for manipulation prediction and discovery of latent structure.</p><hr><h3>Lagrangian Flow Networks for Conservation Laws</h3>
<p><a href='https://openreview.net/forum?id=Nshk5YpdWE'>https://openreview.net/forum?id=Nshk5YpdWE</a></p>
<p><b>Compressor summary</b>: LFlows are a novel method for modeling fluid dynamics using differentiable density transformations and parameterized diffeomorphisms, outperforming existing methods in accuracy and efficiency.</p><hr><h3>VoiceGen: Describing and Generating Voices with Text Prompt</h3>
<p><a href='https://openreview.net/forum?id=NsCXDyv2Bn'>https://openreview.net/forum?id=NsCXDyv2Bn</a></p>
<p><b>Compressor summary</b>: VoiceGen is a method to generate diverse and high-quality voice variability from text prompts using a variation network and a prompt generation pipeline, addressing challenges in traditional TTS approaches.</p><hr><h3>Intelligent Switching for Reset-Free RL</h3>
<p><a href='https://openreview.net/forum?id=Nq45xeghcL'>https://openreview.net/forum?id=Nq45xeghcL</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm called RISC that trains agents without resets by intelligently switching between two agents based on confidence, and shows its effectiveness on various challenging environments.</p><hr><h3>fairret: a Framework for Differentiable Fairness Regularization Terms</h3>
<p><a href='https://openreview.net/forum?id=NnyD0Rjx2B'>https://openreview.net/forum?id=NnyD0Rjx2B</a></p>
<p><b>Compressor summary</b>: The paper introduces fairret, a framework for machine learning fairness that integrates easily with automatic differentiation libraries using linear-fractional statistics and shows its effectiveness in experiments.</p><hr><h3>Perceptual Group Tokenizer: Building Perception with Iterative Grouping</h3>
<p><a href='https://openreview.net/forum?id=NnYaYVODyV'>https://openreview.net/forum?id=NnYaYVODyV</a></p>
<p><b>Compressor summary</b>: The paper presents a model called Perceptual Group Tokenizer that uses grouping operations to extract visual features and perform self-supervised representation learning, achieving competitive performance compared to state-of-the-art vision architectures.</p><hr><h3>Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</h3>
<p><a href='https://openreview.net/forum?id=NltzxpG0nz'>https://openreview.net/forum?id=NltzxpG0nz</a></p>
<p><b>Compressor summary</b>: Steve-Eye is a multimodal model that combines language and vision to enable embodied agents to better interact with open worlds, using a large dataset and three essential functions for an agent.</p><hr><h3>Fantastic Generalization Measures are Nowhere to be Found</h3>
<p><a href='https://openreview.net/forum?id=NkmJotfL42'>https://openreview.net/forum?id=NkmJotfL42</a></p>
<p><b>Compressor summary</b>: The text discusses the difficulty of finding tight generalization bounds for neural networks in the overparameterized setting and shows that such bounds are either impossible or come with trade-offs.</p><hr><h3>A General Framework for User-Guided Bayesian Optimization</h3>
<p><a href='https://openreview.net/forum?id=NjU0jtXcYn'>https://openreview.net/forum?id=NjU0jtXcYn</a></p>
<p><b>Compressor summary</b>: ColaBO is a framework that allows domain experts to customize Bayesian optimization with their own prior beliefs about the function being optimized, improving its sample efficiency and performance.</p><hr><h3>ELoRA: Efficient Low-Rank Adaptation with Random Matrices</h3>
<p><a href='https://openreview.net/forum?id=NjNfLdxr3A'>https://openreview.net/forum?id=NjNfLdxr3A</a></p>
<p><b>Compressor summary</b>: ELoRA is a method to reduce storage requirements and trainable parameters in natural language processing by freezing random low-rank matrices and using small scaling vectors.</p><hr><h3>The Expressive Power of Transformers with Chain of Thought</h3>
<p><a href='https://openreview.net/forum?id=NjNGlPh8Wh'>https://openreview.net/forum?id=NjNGlPh8Wh</a></p>
<p><b>Compressor summary</b>: The paper explores how allowing transformers to generate and condition on intermediate tokens (chain of thought) can extend their computational power, depending on the number of steps taken, and characterizes the complexity class of problems they can solve based on this factor.</p><hr><h3>MOTOR: A Time-To-Event Foundation Model For Structured Medical Records</h3>
<p><a href='https://openreview.net/forum?id=NialiwI2V6'>https://openreview.net/forum?id=NialiwI2V6</a></p>
<p><b>Compressor summary</b>: MOTOR is a self-supervised foundation model that uses pretraining on large patient records to improve time-to-event prediction performance and adaptability across medical tasks and databases.</p><hr><h3>Predictive, scalable and interpretable knowledge tracing on structured domains</h3>
<p><a href='https://openreview.net/forum?id=NgaLU2fP5D'>https://openreview.net/forum?id=NgaLU2fP5D</a></p>
<p><b>Compressor summary</b>: The paper presents PSI-KT, a hierarchical generative model that combines cognitive traits and knowledge structure to optimize learning materials and improve understanding and retention in intelligent tutoring systems, while maintaining interpretability and efficiency.</p><hr><h3>Replay across Experiments: A Natural Extension of Off-Policy RL</h3>
<p><a href='https://openreview.net/forum?id=Nf4Lm6fXN8'>https://openreview.net/forum?id=Nf4Lm6fXN8</a></p>
<p><b>Compressor summary</b>: RaE is a simple framework that reuses experience from previous experiments to improve RL performance and research iteration times, while being robust to different data quality and amount.</p><hr><h3>Training Socially Aligned Language Models on Simulated Social Interactions</h3>
<p><a href='https://openreview.net/forum?id=NddKiWtdUm'>https://openreview.net/forum?id=NddKiWtdUm</a></p>
<p><b>Compressor summary</b>: The text introduces a new way to train language models that allows them to learn social values from simulated interactions, improving their generalization and robustness.</p><hr><h3>Towards Imitation Learning to Branch for MIP: A Hybrid Reinforcement Learning based Sample Augmentation Approach</h3>
<p><a href='https://openreview.net/forum?id=NdcQQ82mfy'>https://openreview.net/forum?id=NdcQQ82mfy</a></p>
<p><b>Compressor summary</b>: The paper proposes a hybrid RL approach for enhancing branching policies in MIP problems using cost-effective sample augmentation, achieving superior results compared to existing methods.</p><hr><h3>GIM: Learning Generalizable Image Matcher From Internet Videos</h3>
<p><a href='https://openreview.net/forum?id=NYN1b8GRGS'>https://openreview.net/forum?id=NYN1b8GRGS</a></p>
<p><b>Compressor summary</b>: GIM is a self-training framework that uses internet videos to improve the generalization of image matching models, and ZEB is a new benchmark for evaluating cross-domain performance.</p><hr><h3>Fake It Till Make It: Federated Learning with Consensus-Oriented Generation</h3>
<p><a href='https://openreview.net/forum?id=NY3wMJuaLf'>https://openreview.net/forum?id=NY3wMJuaLf</a></p>
<p><b>Compressor summary</b>: FedCOG is a novel method for federated learning that generates complementary data to reduce data heterogeneity and improve model performance, without changing the communication process.</p><hr><h3>Text-driven Prompt Generation for Vision-Language Models in Federated Learning</h3>
<p><a href='https://openreview.net/forum?id=NW31gAylIm'>https://openreview.net/forum?id=NW31gAylIm</a></p>
<p><b>Compressor summary</b>: FedTPG is a scalable federated learning method for vision-language models that learns context-aware prompts across clients, improving generalization to seen and unseen classes and datasets.</p><hr><h3>Long-tailed Diffusion Models with Oriented Calibration</h3>
<p><a href='https://openreview.net/forum?id=NW2s5XXwXU'>https://openreview.net/forum?id=NW2s5XXwXU</a></p>
<p><b>Compressor summary</b>: Our method improves long-tail diffusion model generation by directly transferring knowledge from head data using the score function's multi-objective characteristics, enhancing diversity and robustness.</p><hr><h3>Optimal Sample Complexity of Contrastive Learning</h3>
<p><a href='https://openreview.net/forum?id=NU9AYHJvYe'>https://openreview.net/forum?id=NU9AYHJvYe</a></p>
<p><b>Compressor summary</b>: The text studies how many labeled examples are needed to learn data representations well using contrastive learning, and gives optimal or near-optimal bounds for different distance functions.</p><hr><h3>Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks</h3>
<p><a href='https://openreview.net/forum?id=NSVtmmzeRB'>https://openreview.net/forum?id=NSVtmmzeRB</a></p>
<p><b>Compressor summary</b>: GeoBFN is a new model that generates high-quality 3D molecule geometry by modeling diverse modalities in the differentiable parameter space of distributions and maintaining SE-(3) invariant density modeling property.</p><hr><h3>Image Inpainting via Tractable Steering of Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=NSIVHTbZBR'>https://openreview.net/forum?id=NSIVHTbZBR</a></p>
<p><b>Compressor summary</b>: The paper presents a method to improve diffusion models for inpainting by using probabilistic circuits (PCs) to compute exact conditional probabilities and guide the image generation process with semantic constraints.</p><hr><h3>Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks</h3>
<p><a href='https://openreview.net/forum?id=NSDszJ2uIV'>https://openreview.net/forum?id=NSDszJ2uIV</a></p>
<p><b>Compressor summary</b>: The MARCEL benchmark evaluates how well molecular representation learning models perform with flexible molecules considering different structures and tasks.</p><hr><h3>Hindsight PRIORs for Reward Learning from Human Preferences</h3>
<p><a href='https://openreview.net/forum?id=NLevOah0CJ'>https://openreview.net/forum?id=NLevOah0CJ</a></p>
<p><b>Compressor summary</b>: PRIOR is a method to learn preferences from feedback faster and better by assigning rewards based on the importance of states in a trajectory, improving performance on locomotion and manipulation tasks.</p><hr><h3>Hypergraph Dynamic System</h3>
<p><a href='https://openreview.net/forum?id=NLbRvr840Q'>https://openreview.net/forum?id=NLbRvr840Q</a></p>
<p><b>Compressor summary</b>: The paper introduces hypergraph dynamic systems (HDS) that improve the performance and controllability of hypergraph neural networks (HGNNs) by bridging them with dynamic systems and using ordinary differential equations (ODEs).</p><hr><h3>Improving LoRA in Privacy-preserving Federated Learning</h3>
<p><a href='https://openreview.net/forum?id=NLPzL6HWNl'>https://openreview.net/forum?id=NLPzL6HWNl</a></p>
<p><b>Compressor summary</b>: FFA-LoRA is a modified version of LoRA that improves efficiency and stability in federated learning by fixing non-zero matrices and only fine-tuning zero-initialized ones.</p><hr><h3>Reclaiming the Source of Programmatic Policies: Programmatic versus Latent Spaces</h3>
<p><a href='https://openreview.net/forum?id=NGVljI6HkR'>https://openreview.net/forum?id=NGVljI6HkR</a></p>
<p><b>Compressor summary</b>: The paper shows that searching in the programmatic space, induced by a domain-specific language, performs better and is easier to optimize than searching in latent spaces learned by systems like LEAPS and HPRL.</p><hr><h3>Efficient Streaming Language Models with Attention Sinks</h3>
<p><a href='https://openreview.net/forum?id=NG7sS51zVF'>https://openreview.net/forum?id=NG7sS51zVF</a></p>
<p><b>Compressor summary</b>: StreamingLLM is a framework that allows large language models to handle long streaming interactions efficiently by using a dedicated attention sink token and caching previous key-value states.</p><hr><h3>V-DETR: DETR with Vertex Relative Position Encoding for 3D Object Detection</h3>
<p><a href='https://openreview.net/forum?id=NDkpxG94sF'>https://openreview.net/forum?id=NDkpxG94sF</a></p>
<p><b>Compressor summary</b>: The paper presents a 3D object detector for point clouds using DETR framework with improved performance and efficiency, achieved by introducing 3D Vertex Relative Position Encoding and refining the pipeline.</p><hr><h3>MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=N8N0hgNDRt'>https://openreview.net/forum?id=N8N0hgNDRt</a></p>
<p><b>Compressor summary</b>: MetaMath is a finetuned language model that specializes in mathematical reasoning, achieving significant improvements over existing open-source LLMs on mathematical question answering benchmarks.</p><hr><h3>Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL</h3>
<p><a href='https://openreview.net/forum?id=N6o0ZtPzTg'>https://openreview.net/forum?id=N6o0ZtPzTg</a></p>
<p><b>Compressor summary</b>: The study proposes Prompt-OIRL, a method that uses offline inverse reinforcement learning to optimize prompts for improving arithmetic reasoning in Large Language Models without accessing them during evaluation.</p><hr><h3>A Sublinear Adversarial Training Algorithm</h3>
<p><a href='https://openreview.net/forum?id=N2WchST43h'>https://openreview.net/forum?id=N2WchST43h</a></p>
<p><b>Compressor summary</b>: The paper analyzes the convergence of adversarial training on a two-layer neural network and proposes an efficient algorithm with reduced time cost.</p><hr><h3>Win-Win: Training High-Resolution Vision Transformers from Two Windows</h3>
<p><a href='https://openreview.net/forum?id=N23A4ybMJr'>https://openreview.net/forum?id=N23A4ybMJr</a></p>
<p><b>Compressor summary</b>: The paper proposes a Win-Win method for efficient training and inference of high-resolution vision transformers using random windows, achieving fast and state-of-the-art results in semantic segmentation and optical flow tasks.</p><hr><h3>TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts</h3>
<p><a href='https://openreview.net/forum?id=N0nTk5BSvO'>https://openreview.net/forum?id=N0nTk5BSvO</a></p>
<p><b>Compressor summary</b>: The paper introduces TESTAM, a deep learning model that uses a mixture-of-experts to separately model recurring and non-recurring traffic patterns using different graph representations for better traffic forecasting.</p><hr><h3>Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time</h3>
<p><a href='https://openreview.net/forum?id=N0gT4A0jNV'>https://openreview.net/forum?id=N0gT4A0jNV</a></p>
<p><b>Compressor summary</b>: The paper proposes an efficient and robust alternating minimization framework for low rank matrix completion with incolemntary rows and columns, achieving nearly linear time complexity in the sample complexity.</p><hr><h3>Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=N0I2RtD8je'>https://openreview.net/forum?id=N0I2RtD8je</a></p>
<p><b>Compressor summary</b>: Vision-language models can be used as reward models for reinforcement learning, enabling sample-efficient learning of complex tasks without manual rewards.</p><hr><h3>Learning Multi-Faceted Prototypical User Interests</h3>
<p><a href='https://openreview.net/forum?id=MzjiMxlWab'>https://openreview.net/forum?id=MzjiMxlWab</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for learning user preferences from behavioral data by aligning them with multi-faceted item characteristics using prototype-based representation learning and a bi-directional binding block.</p><hr><h3>Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization</h3>
<p><a href='https://openreview.net/forum?id=My7lkRNnL9'>https://openreview.net/forum?id=My7lkRNnL9</a></p>
<p><b>Compressor summary</b>: The text discusses the challenges and performance of forward-only algorithms for training neural networks without backpropagation, comparing different versions and linking them to feedback alignment and backpropagation.</p><hr><h3>Causal Structure Recovery with Latent Variables under Milder Distributional and Graphical Assumptions</h3>
<p><a href='https://openreview.net/forum?id=MukGKGtgnr'>https://openreview.net/forum?id=MukGKGtgnr</a></p>
<p><b>Compressor summary</b>: The paper proposes methods for discovering the whole causal structure of linear models with latent variables under mild assumptions and proves their identifiability.</p><hr><h3>PeFLL: Personalized Federated Learning by Learning to Learn</h3>
<p><a href='https://openreview.net/forum?id=MrYiwlDRQO'>https://openreview.net/forum?id=MrYiwlDRQO</a></p>
<p><b>Compressor summary</b>: PeFLL is a new personalized federated learning algorithm that improves accuracy, reduces computation and communication, and has theoretical guarantees by jointly training an embedding network and a hypernetwork.</p><hr><h3>Memorization Capacity of Multi-Head Attention in Transformers</h3>
<p><a href='https://openreview.net/forum?id=MrR3rMxqqv'>https://openreview.net/forum?id=MrR3rMxqqv</a></p>
<p><b>Compressor summary</b>: This paper studies how many example sequences multi-head attention mechanisms in transformers can memorize and introduces new assumptions about input data independence.</p><hr><h3>Morphological Maze: Control Reconfigurable Soft Robots with Fine-grained Morphology Change</h3>
<p><a href='https://openreview.net/forum?id=MpyFAhH9CK'>https://openreview.net/forum?id=MpyFAhH9CK</a></p>
<p><b>Compressor summary</b>: The authors propose a reinforcement learning approach for controlling reconfigurable soft robots that can change their shape during their lifetime, and introduce Morphological Maze, a benchmark for testing these robots.</p><hr><h3>DyST: Towards Dynamic Neural Scene Representations on Real-World Videos</h3>
<p><a href='https://openreview.net/forum?id=MnMWa94t12'>https://openreview.net/forum?id=MnMWa94t12</a></p>
<p><b>Compressor summary</b>: The Dynamic Scene Transformer (DyST) is a model that captures 3D structure and dynamics of real-world scenes from monocular videos using neural scene representation and co-training on DySO dataset.</p><hr><h3>Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection</h3>
<p><a href='https://openreview.net/forum?id=MloaGA6WwX'>https://openreview.net/forum?id=MloaGA6WwX</a></p>
<p><b>Compressor summary</b>: The paper introduces TADRED, a data-driven method that optimizes experimental design and machine learning for imaging tasks, improving acquisition time, cost, and deployment.</p><hr><h3>COCO-Periph: Bridging the Gap Between Human and Machine Perception in the Periphery</h3>
<p><a href='https://openreview.net/forum?id=MiRPBbQNHv'>https://openreview.net/forum?id=MiRPBbQNHv</a></p>
<p><b>Compressor summary</b>: The authors study how well deep neural networks (DNNs) can replicate human peripheral vision and find that while they improve with training, they still struggle to match human performance in certain conditions.</p><hr><h3>Learning to Act from Actionless Videos through Dense Correspondences</h3>
<p><a href='https://openreview.net/forum?id=Mhb5fpA1T0'>https://openreview.net/forum?id=Mhb5fpA1T0</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to train robots from videos without action annotations, using images and text as representations, and shows its effectiveness on table-top tasks.</p><hr><h3>Fast Ensembling with Diffusion Schr\"odinger Bridge</h3>
<p><a href='https://openreview.net/forum?id=Mgq6kxl115'>https://openreview.net/forum?id=Mgq6kxl115</a></p>
<p><b>Compressor summary</b>: The Diffusion Bridge Network is a new method to predict using multiple neural networks without the high cost of running each one individually, by simulating an equation that connects their outputs.</p><hr><h3>The Trickle-down Impact of Reward Inconsistency on RLHF</h3>
<p><a href='https://openreview.net/forum?id=MeHmwCDifc'>https://openreview.net/forum?id=MeHmwCDifc</a></p>
<p><b>Compressor summary</b>: The paper explores the consistency of reward models in Reinforcement Learning from Human Feedback and proposes methods to measure and improve it, showing its impact on chatbot performance.</p><hr><h3>Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=MeB86edZ1P'>https://openreview.net/forum?id=MeB86edZ1P</a></p>
<p><b>Compressor summary</b>: The authors propose a new method using lateral connections and Hebbian learning to prevent catastrophic forgetting in spiking neural networks, which could enable energy-efficient continual learning in neuromorphic computing systems.</p><hr><h3>L2P-MIP: Learning to Presolve for Mixed Integer Programming</h3>
<p><a href='https://openreview.net/forum?id=McfYbKnpT8'>https://openreview.net/forum?id=McfYbKnpT8</a></p>
<p><b>Compressor summary</b>: The paper proposes using supervised learning and classic heuristics to create instance-specific presolving for MIP solvers, which can significantly improve their performance.</p><hr><h3>GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher</h3>
<p><a href='https://openreview.net/forum?id=MbfAK4s61A'>https://openreview.net/forum?id=MbfAK4s61A</a></p>
<p><b>Compressor summary</b>: The study introduces CipherChat, a framework to test safety alignment of LLMs in non-natural languages like ciphers and finds that some ciphers can bypass LLMs' safety mechanisms, while also discovering a novel human cipher called SelfCipher.</p><hr><h3>Denoising Task Routing for Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=MY0qlcFcUg'>https://openreview.net/forum?id=MY0qlcFcUg</a></p>
<p><b>Compressor summary</b>: DTR is a simple add-on strategy for diffusion models that improves performance by selectively activating channels based on task affinity and weights.</p><hr><h3>The Need for Speed: Pruning Transformers with One Recipe</h3>
<p><a href='https://openreview.net/forum?id=MVmT6uQ3cQ'>https://openreview.net/forum?id=MVmT6uQ3cQ</a></p>
<p><b>Compressor summary</b>: The OPTIN framework is a tool to increase the efficiency of pre-trained transformer architectures without re-training, using intermediate feature distillation to capture model dependencies and achieve competitive performance on various tasks.</p><hr><h3>A Probabilistic Framework for Modular Continual Learning</h3>
<p><a href='https://openreview.net/forum?id=MVe2dnWPCu'>https://openreview.net/forum?id=MVe2dnWPCu</a></p>
<p><b>Compressor summary</b>: PICLE is a modular continual learning framework that uses a probabilistic model to efficiently evaluate module compositions, achieving perceptual, few-shot and latent transfer in large search spaces.</p><hr><h3>DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization</h3>
<p><a href='https://openreview.net/forum?id=MSe8YFbhUE'>https://openreview.net/forum?id=MSe8YFbhUE</a></p>
<p><b>Compressor summary</b>: The paper proposes DrM, a visual RL method that reduces agent's inactivity by minimizing dormant ratio, improving sample efficiency and performance across different continuous control environments.</p><hr><h3>One-hot Generalized Linear Model for Switching Brain State Discovery</h3>
<p><a href='https://openreview.net/forum?id=MREQ0k6qvD'>https://openreview.net/forum?id=MREQ0k6qvD</a></p>
<p><b>Compressor summary</b>: The authors propose a new method to model dynamic changes in neural interactions by incorporating prior information about anatomical connectomes into a state-switching generalized linear model.</p><hr><h3>Achieving Sample and Computational Efficient Reinforcement Learning by Action Space Reduction via Grouping</h3>
<p><a href='https://openreview.net/forum?id=MOmqfJovQ6'>https://openreview.net/forum?id=MOmqfJovQ6</a></p>
<p><b>Compressor summary</b>: This paper proposes an optimization problem to find the best way to group actions in reinforcement learning to balance exploration and exploitation in high-dimensional spaces.</p><hr><h3>Differentiable Euler Characteristic Transforms for Shape Classification</h3>
<p><a href='https://openreview.net/forum?id=MO632iPq3I'>https://openreview.net/forum?id=MO632iPq3I</a></p>
<p><b>Compressor summary</b>: DECT is a fast and efficient method for classifying graphs and point clouds using the Euler Characteristic Transform with task-specific learned representations.</p><hr><h3>Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs</h3>
<p><a href='https://openreview.net/forum?id=MO5PiKHELW'>https://openreview.net/forum?id=MO5PiKHELW</a></p>
<p><b>Compressor summary</b>: The study analyzes how syntax acquisition in masked language models (MLMs) is influenced by the evolution of Syntactic Attention Structure (SAS), which is essential for developing grammatical capabilities, but can also hinder model quality.</p><hr><h3>SEABO: A Simple Search-Based Method for Offline Imitation Learning</h3>
<p><a href='https://openreview.net/forum?id=MNyOI3C7YB'>https://openreview.net/forum?id=MNyOI3C7YB</a></p>
<p><b>Compressor summary</b>: SEABO is a search-based method for offline imitation learning that learns a reward function from expert and unlabeled data, achieving competitive performance to offline RL algorithms with ground-truth rewards.</p><hr><h3>Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules</h3>
<p><a href='https://openreview.net/forum?id=MNShbDSxKH'>https://openreview.net/forum?id=MNShbDSxKH</a></p>
<p><b>Compressor summary</b>: The paper proposes a generative neuro-symbolic visual reasoning model that grows and reuses modules using large language models, achieving competitive performance and fast generalization to new tasks.</p><hr><h3>SyncDreamer: Generating Multiview-consistent Images from a Single-view Image</h3>
<p><a href='https://openreview.net/forum?id=MN3yH2ovHb'>https://openreview.net/forum?id=MN3yH2ovHb</a></p>
<p><b>Compressor summary</b>: SyncDreamer is a new diffusion model that creates consistent images from multiple views using a 3D-aware feature attention mechanism.</p><hr><h3>InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation</h3>
<p><a href='https://openreview.net/forum?id=MLBdiWu4Fw'>https://openreview.net/forum?id=MLBdiWu4Fw</a></p>
<p><b>Compressor summary</b>: InternVid is a large dataset with millions of videos and their descriptions that enables learning powerful video-text representations for various multimodal tasks, such as action recognition, retrieval, and dialogue systems.</p><hr><h3>CARD: Channel Aligned Robust Blend Transformer for Time Series Forecasting</h3>
<p><a href='https://openreview.net/forum?id=MJksrOhurE'>https://openreview.net/forum?id=MJksrOhurE</a></p>
<p><b>Compressor summary</b>: The CARD Transformer model improves time series forecasting by capturing channel correlations, generating tokens with different resolutions, and using a robust loss function that considers prediction uncertainties.</p><hr><h3>Discovering Temporally-Aware Reinforcement Learning Algorithms</h3>
<p><a href='https://openreview.net/forum?id=MJJcs3zbmi'>https://openreview.net/forum?id=MJJcs3zbmi</a></p>
<p><b>Compressor summary</b>: This paper proposes an objective discovery method that enables reinforcement learning algorithms to adapt their learning strategies based on the training horizon, leading to better generalization and performance across different tasks.</p><hr><h3>Knowledge Distillation Based on Transformed Teacher Matching</h3>
<p><a href='https://openreview.net/forum?id=MJ3K7uDGGl'>https://openreview.net/forum?id=MJ3K7uDGGl</a></p>
<p><b>Compressor summary</b>: Temperature scaling in knowledge distillation (KD) can be replaced by transformed teacher matching (TTM), which uses a Rnyi entropy term for better generalization, and further improved by weighted TTM (WTTM).</p><hr><h3>Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for Molecule Generation</h3>
<p><a href='https://openreview.net/forum?id=MIEnYtlGyv'>https://openreview.net/forum?id=MIEnYtlGyv</a></p>
<p><b>Compressor summary</b>: Symphony is a new generative model for 3D molecular geometries that uses higher-degree E(3)-equivariant features to efficiently represent probability distributions and generate accurate molecules.</p><hr><h3>High-dimensional SGD aligns with emerging outlier eigenspaces</h3>
<p><a href='https://openreview.net/forum?id=MHjigVnI04'>https://openreview.net/forum?id=MHjigVnI04</a></p>
<p><b>Compressor summary</b>: The study finds that gradient descent in high-dimensional classification tasks aligns with low-rank outlier eigenspaces of Hessian and gradient matrices, leading to suboptimal classifiers in multi-layer settings.</p><hr><h3>Learning interpretable control inputs and dynamics underlying animal locomotion</h3>
<p><a href='https://openreview.net/forum?id=MFCjgEOLJT'>https://openreview.net/forum?id=MFCjgEOLJT</a></p>
<p><b>Compressor summary</b>: The authors propose a control theoretic approach to model the zebrafish larva's natural locomotor repertoire using a sparse control signal driving a latent RNN that preserves kinematic features and disentangles different categories of movements.</p><hr><h3>The Effectiveness of Random Forgetting for Robust Generalization</h3>
<p><a href='https://openreview.net/forum?id=MEGQGNUfPx'>https://openreview.net/forum?id=MEGQGNUfPx</a></p>
<p><b>Compressor summary</b>: FOMO is a novel learning paradigm that alternates between forgetting and relearning phases to mitigate robust overfitting and improve neural network robustness against adversarial attacks.</p><hr><h3>Improving Offline RL by Blending Heuristics</h3>
<p><a href='https://openreview.net/forum?id=MCl0TLboP1'>https://openreview.net/forum?id=MCl0TLboP1</a></p>
<p><b>Compressor summary</b>: HUBL is a technique that improves offline RL performance by blending heuristic values with bootstrapped values based on Monte-Carlo returns, reducing complexity and increasing sample efficiency.</p><hr><h3>Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents</h3>
<p><a href='https://openreview.net/forum?id=MCNqgUFTHI'>https://openreview.net/forum?id=MCNqgUFTHI</a></p>
<p><b>Compressor summary</b>: PPDPP is a new dialogue policy planning paradigm that improves the proactivity of large language models in different applications by using a tunable plug-and-play dialogue policy planner.</p><hr><h3>AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents</h3>
<p><a href='https://openreview.net/forum?id=M6XWoEdmwf'>https://openreview.net/forum?id=M6XWoEdmwf</a></p>
<p><b>Compressor summary</b>: AMAGO is an in-context RL agent that uses sequence models, sparse rewards, off-policy data, and hindsight relabeling to solve goal-conditioned problems and extend in-context learning to open-world domains.</p><hr><h3>The Curse of Diversity in Ensemble-Based Exploration</h3>
<p><a href='https://openreview.net/forum?id=M3QXCOTTk4'>https://openreview.net/forum?id=M3QXCOTTk4</a></p>
<p><b>Compressor summary</b>: The text describes a negative effect of training diverse data-sharing agents, called the curse of diversity, and proposes a solution named Cross-Ensemble Representation Learning (CERL) to overcome it.</p><hr><h3>Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=M0xK8nPGvt'>https://openreview.net/forum?id=M0xK8nPGvt</a></p>
<p><b>Compressor summary</b>: C-PSRL is a new posterior sampling method for reinforcement learning that uses a causal graph as prior knowledge, improving sample efficiency and Bayesian regret.</p><hr><h3>Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction</h3>
<p><a href='https://openreview.net/forum?id=M0MF4t3hE9'>https://openreview.net/forum?id=M0MF4t3hE9</a></p>
<p><b>Compressor summary</b>: The paper proposes Instruction-oriented Object Detection (IOD), a task that enables detectors to understand user instructions and locate objects, introducing IOD-Bench dataset and Ins-DetCLIP model.</p><hr><h3>LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=LzPWWPAdY4'>https://openreview.net/forum?id=LzPWWPAdY4</a></p>
<p><b>Compressor summary</b>: LoftQ is a novel quantization framework that improves the performance of large language models by simultaneously quantizing them and finding a proper low-rank initialization for LoRA fine-tuning, especially in challenging mixed precision regimes.</p><hr><h3>Unraveling the Key Components of OOD Generalization via Diversification</h3>
<p><a href='https://openreview.net/forum?id=Lvf7GnaLru'>https://openreview.net/forum?id=Lvf7GnaLru</a></p>
<p><b>Compressor summary</b>: This paper studies diversification methods for handling spurious features and finds that their performance depends on the distribution of unlabeled data, learning algorithm choice, and the number of diverse hypotheses, with no clear improvement from increasing diversity.</p><hr><h3>InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior</h3>
<p><a href='https://openreview.net/forum?id=LtuRgL03pI'>https://openreview.net/forum?id=LtuRgL03pI</a></p>
<p><b>Compressor summary</b>: InstructScene is a new framework for generating 3D indoor scenes from natural language instructions, improving controllability and fidelity with a semantic graph prior and a layout decoder.</p><hr><h3>LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving</h3>
<p><a href='https://openreview.net/forum?id=LsURkIPYR5'>https://openreview.net/forum?id=LsURkIPYR5</a></p>
<p><b>Compressor summary</b>: LaneSegNet is a new mapping network that generates lane segments, incorporating geometry and topology information, for autonomous driving systems, and outperforms previous methods on three tasks.</p><hr><h3>Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation</h3>
<p><a href='https://openreview.net/forum?id=LqaEEs3UxU'>https://openreview.net/forum?id=LqaEEs3UxU</a></p>
<p><b>Compressor summary</b>: Sign2GPT is a framework for sign language translation that uses pretrained vision and language models with lightweight adapters to translate gloss-free sign videos without needing annotated gloss orders.</p><hr><h3>Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis</h3>
<p><a href='https://openreview.net/forum?id=LqRGsGWOTX'>https://openreview.net/forum?id=LqRGsGWOTX</a></p>
<p><b>Compressor summary</b>: BO-REP is a new bilevel optimization algorithm that adapts to unbounded smoothness in neural networks by using normalized momentum and periodic updates, achieving state-of-the-art complexity results.</p><hr><h3>3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining</h3>
<p><a href='https://openreview.net/forum?id=LokR2TTFMs'>https://openreview.net/forum?id=LokR2TTFMs</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method for pretraining 3D point clouds using masked autoencoders that focuses on recovering intrinsic point features instead of location, achieving better results on various point cloud analysis tasks.</p><hr><h3>A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=LnLySuf1vp'>https://openreview.net/forum?id=LnLySuf1vp</a></p>
<p><b>Compressor summary</b>: SpikeGCL is a novel framework for graph contrastive learning with spiking neural networks that achieves efficient and high-quality 1-bit representations for graphs, outperforming many existing methods.</p><hr><h3>Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=LjivA1SLZ6'>https://openreview.net/forum?id=LjivA1SLZ6</a></p>
<p><b>Compressor summary</b>: EMU is a MARL algorithm that uses memory embeddings and an episodic reward to accelerate learning and avoid local optima in complex tasks.</p><hr><h3>Transformer Fusion with Optimal Transport</h3>
<p><a href='https://openreview.net/forum?id=LjeqMvQpen'>https://openreview.net/forum?id=LjeqMvQpen</a></p>
<p><b>Compressor summary</b>: The paper presents a method for fusing multiple transformer-based neural networks using Optimal Transport to align their components, achieving better performance than vanilla fusion or individual models on image and text tasks.</p><hr><h3>Layer-wise linear mode connectivity</h3>
<p><a href='https://openreview.net/forum?id=LfmZh91tDI'>https://openreview.net/forum?id=LfmZh91tDI</a></p>
<p><b>Compressor summary</b>: The paper explores how layer-wise averaging of neural network parameters can improve the performance of independent models, especially when trained on different datasets.</p><hr><h3>Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models</h3>
<p><a href='https://openreview.net/forum?id=Let8OMe20n'>https://openreview.net/forum?id=Let8OMe20n</a></p>
<p><b>Compressor summary</b>: Text-Image Alignment Assessment (TIA2) is a benchmark to study reward overoptimization in text-to-image models, and TextNorm helps reduce it by calibrating reward model scores.</p><hr><h3>Delta-AI: Local objectives for amortized inference in sparse graphical models</h3>
<p><a href='https://openreview.net/forum?id=LemSSn8htt'>https://openreview.net/forum?id=LemSSn8htt</a></p>
<p><b>Compressor summary</b>: The paper introduces a new algorithm called $\Delta$-amortized inference that leverages sparsity in probabilistic graphical models to enable faster and more efficient inference using local credit assignment and learned samplers.</p><hr><h3>Distort, Distract, Decode: Instruction-Tuned Model Can Refine its Response from Noisy Instructions</h3>
<p><a href='https://openreview.net/forum?id=LebzzClHYw'>https://openreview.net/forum?id=LebzzClHYw</a></p>
<p><b>Compressor summary</b>: Instructive Decoding (ID) improves the performance of instruction-tuned language models by generating diverse responses using noisy instructions, especially 'opposite'.</p><hr><h3>JoMA: Demystifying Multilayer Transformers via Joint Dynamics of MLP and Attention</h3>
<p><a href='https://openreview.net/forum?id=LbJqRGNYCf'>https://openreview.net/forum?id=LbJqRGNYCf</a></p>
<p><b>Compressor summary</b>: JoMA is a new framework that analyzes how Transformers learn hierarchies of tokens by integrating the self-attention layer in MLP dynamics.</p><hr><h3>Fast Value Tracking for Deep Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=LZIOBA2oDU'>https://openreview.net/forum?id=LZIOBA2oDU</a></p>
<p><b>Compressor summary</b>: The Langevinized Kalman Temporal-Difference (LKTD) algorithm uses Kalman filtering to create a scalable sampling method for deep reinforcement learning, considering the stochastic nature of agent-environment interactions and quantifying uncertainties in policy updates.</p><hr><h3>H-GAP: Humanoid Control with a Generalist Planner</h3>
<p><a href='https://openreview.net/forum?id=LYG6tBlEX0'>https://openreview.net/forum?id=LYG6tBlEX0</a></p>
<p><b>Compressor summary</b>: H-GAP is a model that generates humanoid trajectories from human motion-captured data and excels in controlling 56 degrees of freedom humanoids without online learning or individual task training.</p><hr><h3>On input-dependence and recall in convolutional language models</h3>
<p><a href='https://openreview.net/forum?id=LY3ukUANko'>https://openreview.net/forum?id=LY3ukUANko</a></p>
<p><b>Compressor summary</b>: Convolution-based language models struggle with associative recall, a key task in language modeling, due to fixed filters and variable input distances, but attention can solve this issue.</p><hr><h3>In-Context Pretraining: Language Modeling Beyond Document Boundaries</h3>
<p><a href='https://openreview.net/forum?id=LXVswInHOo'>https://openreview.net/forum?id=LXVswInHOo</a></p>
<p><b>Compressor summary</b>: IN-CONTEXT PRETRAINING improves language models' performance by training them on related documents, enabling better contextual reasoning.</p><hr><h3>A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=LWuYsSD94h'>https://openreview.net/forum?id=LWuYsSD94h</a></p>
<p><b>Compressor summary</b>: The paper proposes a versatile black-box method for multi-agent learning with bandit feedback and non-stationary environments, achieving low regret and testing different types of equilibria.</p><hr><h3>Attention-Guided Contrastive Role Representations for Multi-agent Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=LWmuPfEYhH'>https://openreview.net/forum?id=LWmuPfEYhH</a></p>
<p><b>Compressor summary</b>: The proposed ACORM framework uses attention-guided contrastive role representation learning to improve behavior heterogeneity, knowledge transfer, and skillful coordination in multi-agent reinforcement learning.</p><hr><h3>Masked Distillation Advances Self-Supervised Transformer Architecture Search</h3>
<p><a href='https://openreview.net/forum?id=LUpC8KTvdV'>https://openreview.net/forum?id=LUpC8KTvdV</a></p>
<p><b>Compressor summary</b>: The paper proposes a method called MaskTAS that uses masked image modeling to search for efficient and effective vision transformer architectures without the need for labeled data.</p><hr><h3>SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations</h3>
<p><a href='https://openreview.net/forum?id=LSYhE2hLWG'>https://openreview.net/forum?id=LSYhE2hLWG</a></p>
<p><b>Compressor summary</b>: SineNet is a new deep neural network architecture that improves the modeling of complex, time-evolving dynamics in partial differential equations by using multiple sequentially connected U-shaped blocks to reduce temporally misaligned features and enable multi-scale processing.</p><hr><h3>CCIL: Continuity-Based Data Augmentation for Corrective Imitation Learning</h3>
<p><a href='https://openreview.net/forum?id=LQ6LQ8f4y8'>https://openreview.net/forum?id=LQ6LQ8f4y8</a></p>
<p><b>Compressor summary</b>: The new technique enhances imitation learning robustness by generating corrective data from expert demonstrations, accounting for errors and disturbances in the environment dynamics.</p><hr><h3>Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment</h3>
<p><a href='https://openreview.net/forum?id=LNLjU5C5dK'>https://openreview.net/forum?id=LNLjU5C5dK</a></p>
<p><b>Compressor summary</b>: FIGA is an improved alignment approach that uses fine-grained quality signals from contrasting good and bad responses to train large language models.</p><hr><h3>Robust Network Pruning With Sparse Entropic Wasserstein Regression</h3>
<p><a href='https://openreview.net/forum?id=LJWizuuBUy'>https://openreview.net/forum?id=LJWizuuBUy</a></p>
<p><b>Compressor summary</b>: The study presents a new neural network pruning technique that effectively reduces noisy gradients using entropic Wasserstein regression, achieving better performance when the network size or target sparsity is large.</p><hr><h3>ZipIt! Merging Models from Different Tasks without Training</h3>
<p><a href='https://openreview.net/forum?id=LEYUkvdUhq'>https://openreview.net/forum?id=LEYUkvdUhq</a></p>
<p><b>Compressor summary</b>: ZipIt! is a method to combine different models trained on separate tasks into one multi-task model without retraining by using a general "zip" operation and partially zipping the models up until a specified layer.</p><hr><h3>Concept Bottleneck Generative Models</h3>
<p><a href='https://openreview.net/forum?id=L9U5MJJleF'>https://openreview.net/forum?id=L9U5MJJleF</a></p>
<p><b>Compressor summary</b>: The concept bottleneck layer enhances interpretable generative models by adding a human-understandable concept encoding and loss function, which works across different model families and significantly outperforms baselines.</p><hr><h3>Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update</h3>
<p><a href='https://openreview.net/forum?id=L8UNn7Llt4'>https://openreview.net/forum?id=L8UNn7Llt4</a></p>
<p><b>Compressor summary</b>: The authors propose a modified DICE method for offline reinforcement learning and imitation learning, which uses an orthogonal-gradient update to improve performance and robustness by resolving the conflict between forward and backward gradients.</p><hr><h3>Who to imitate: Imitating desired behavior from divserse multi-agent datasets</h3>
<p><a href='https://openreview.net/forum?id=L6crLU7MIE'>https://openreview.net/forum?id=L6crLU7MIE</a></p>
<p><b>Compressor summary</b>: The text discusses how to assign and estimate exchange values for individual agents in AI systems based on their contribution to the collective desirability score, enabling better imitation of desired behaviors.</p><hr><h3>Massive Editing for Large Language Model via Meta Learning</h3>
<p><a href='https://openreview.net/forum?id=L6L1CJQ2PE'>https://openreview.net/forum?id=L6L1CJQ2PE</a></p>
<p><b>Compressor summary</b>: MALMEN is a method to update large language models with corrected knowledge by using a hyper-network to generate parameter shifts based on least square problem formulation.</p><hr><h3>Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models</h3>
<p><a href='https://openreview.net/forum?id=L4nOxziGf9'>https://openreview.net/forum?id=L4nOxziGf9</a></p>
<p><b>Compressor summary</b>: RepARe is a framework that improves zero-shot performance of large vision-language models by rephrasing questions using caption and reasoning information extracted from the model itself, selecting the best question candidate based on its confidence score, and increasing syntactic complexity.</p><hr><h3>Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering</h3>
<p><a href='https://openreview.net/forum?id=L3FHMoKZcS'>https://openreview.net/forum?id=L3FHMoKZcS</a></p>
<p><b>Compressor summary</b>: The paper analyzes existing calibration methods for large language models and proposes Batch Calibration, a simple method that addresses contextual bias and improves performance on various tasks.</p><hr><h3>Improving Convergence and Generalization Using Parameter Symmetries</h3>
<p><a href='https://openreview.net/forum?id=L0r0GphlIL'>https://openreview.net/forum?id=L0r0GphlIL</a></p>
<p><b>Compressor summary</b>: Teleportation speeds up optimization, improves convergence, and enhances generalization by using loss-invariant transformations in overparametrized models.</p><hr><h3>The False Promise of Imitating Proprietary Language Models</h3>
<p><a href='https://openreview.net/forum?id=Kz3yckpCN5'>https://openreview.net/forum?id=Kz3yckpCN5</a></p>
<p><b>Compressor summary</b>: The authors analyze the approach of finetuning language models on outputs from stronger models and find that it only works well for mimicking style but not factuality, suggesting that improving open-source models requires better base LMs instead of imitating proprietary ones.</p><hr><h3>Accelerating Sinkhorn algorithm with sparse Newton iterations</h3>
<p><a href='https://openreview.net/forum?id=Kuj5gVp5GQ'>https://openreview.net/forum?id=Kuj5gVp5GQ</a></p>
<p><b>Compressor summary</b>: The Sinkhorn-Newton-Sparse (SNS) algorithm improves the convergence speed of optimal transport calculations by introducing early stopping and a Newton-type subroutine, achieving super-exponential convergence and $O(n^2)$ per-iteration complexity.</p><hr><h3>Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages</h3>
<p><a href='https://openreview.net/forum?id=Kuh5qgCGCp'>https://openreview.net/forum?id=Kuh5qgCGCp</a></p>
<p><b>Compressor summary</b>: rainname is a training method that enables multimodal learning in low-resource languages using a strong multilingual language model and English data, achieving state-of-the-art results in Chinese image-to-text and text-to-image generation.</p><hr><h3>Teaching Large Language Models to Self-Debug</h3>
<p><a href='https://openreview.net/forum?id=KuPixIqPiq'>https://openreview.net/forum?id=KuPixIqPiq</a></p>
<p><b>Compressor summary</b>: Self-debugging teaches large language models to debug their code by explaining it in natural language, achieving state-of-the-art performance on several code generation benchmarks.</p><hr><h3>Thin-Shell Object Manipulations With Differentiable Physics Simulations</h3>
<p><a href='https://openreview.net/forum?id=KsUh8MMFKQ'>https://openreview.net/forum?id=KsUh8MMFKQ</a></p>
<p><b>Compressor summary</b>: This paper introduces ThinShellLab, a simulation platform for robots to learn and evaluate skills in manipulating diverse thin-shell materials, overcoming challenges through an optimization scheme that combines sampling-based and gradient-based methods.</p><hr><h3>The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models</h3>
<p><a href='https://openreview.net/forum?id=KrtGfTGaGe'>https://openreview.net/forum?id=KrtGfTGaGe</a></p>
<p><b>Compressor summary</b>: The Wasserstein Belief Updater is a reinforcement learning algorithm that learns a latent model and an approximate belief update for partially observable environments, providing theoretical guarantees on optimality.</p><hr><h3>MetaPhysiCa: Improving OOD Robustness in Physics-informed Machine Learning</h3>
<p><a href='https://openreview.net/forum?id=KrWuDiW4Qm'>https://openreview.net/forum?id=KrWuDiW4Qm</a></p>
<p><b>Compressor summary</b>: The paper proposes a meta-learning method for discovering causal structures in physics-informed machine learning, which improves the robustness of out-of-distribution forecasting tasks.</p><hr><h3>Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=KqbCvIFBY7'>https://openreview.net/forum?id=KqbCvIFBY7</a></p>
<p><b>Compressor summary</b>: Particle guidance is a method to improve diversity and sample efficiency in generative models by using joint-particle time-evolving potentials instead of independent samples.</p><hr><h3>Generative Pre-training for Speech with Flow Matching</h3>
<p><a href='https://openreview.net/forum?id=KpoQSgxbKH'>https://openreview.net/forum?id=KpoQSgxbKH</a></p>
<p><b>Compressor summary</b>: SpeechFlow is a pre-trained generative model that can create realistic speech data for various tasks and surpass existing models.</p><hr><h3>Mitigating Severe Robustness Degradation on Graphs</h3>
<p><a href='https://openreview.net/forum?id=Koh0i2u8qX'>https://openreview.net/forum?id=Koh0i2u8qX</a></p>
<p><b>Compressor summary</b>: The paper proposes a graph defense method that is resistant to both intense attacks and heavy computation, using a denoising module and a Mixture-of-Experts approach.</p><hr><h3>On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods</h3>
<p><a href='https://openreview.net/forum?id=Kn7tWhuetn'>https://openreview.net/forum?id=Kn7tWhuetn</a></p>
<p><b>Compressor summary</b>: The paper introduces ForgetNet, a neural network model that reasons about algorithms without relying on historical embeddings, and G-ForgetNet, an improved version that uses a gating mechanism to selectively integrate history during training.</p><hr><h3>FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent</h3>
<p><a href='https://openreview.net/forum?id=Kl9CqKf7h6'>https://openreview.net/forum?id=Kl9CqKf7h6</a></p>
<p><b>Compressor summary</b>: FedHyper is a novel algorithm for federated learning that adapts the learning rate and improves efficiency and accuracy in various scenarios.</p><hr><h3>An Extensible Framework for Open Heterogeneous Collaborative Perception</h3>
<p><a href='https://openreview.net/forum?id=KkrDUGIASk'>https://openreview.net/forum?id=KkrDUGIASk</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework for heterogeneous collaborative perception, which enables agents with different sensors and models to work together effectively and securely.</p><hr><h3>DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning</h3>
<p><a href='https://openreview.net/forum?id=KjegfPGRde'>https://openreview.net/forum?id=KjegfPGRde</a></p>
<p><b>Compressor summary</b>: Decomposed Prompt Tuning (DePT) improves parameter-efficient fine-tuning by decomposing soft prompts into shorter tokens and low-rank matrices, reducing memory and time costs without sacrificing performance.</p><hr><h3>Cascading Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=KjOAHlKMF5'>https://openreview.net/forum?id=KjOAHlKMF5</a></p>
<p><b>Compressor summary</b>: The text introduces a generalized cascading bandit framework that considers user states and state transitions, and proposes two efficient and near-optimal algorithms for recommendation systems and online advertising.</p><hr><h3>Improving the Convergence of Dynamic NeRFs via Optimal Transport</h3>
<p><a href='https://openreview.net/forum?id=KiespDPaRH'>https://openreview.net/forum?id=KiespDPaRH</a></p>
<p><b>Compressor summary</b>: The paper proposes a lightweight, learning-free, and architecture-agnostic regularization method for NeRF models using optimal transport to handle dynamic scenes' dynamics in the probability distribution of light intensity.</p><hr><h3>Language Model Cascades: Token-Level Uncertainty And Beyond</h3>
<p><a href='https://openreview.net/forum?id=KgaBScZ4VI'>https://openreview.net/forum?id=KgaBScZ4VI</a></p>
<p><b>Compressor summary</b>: The paper studies how to use different uncertainty measures from generative language models to decide when to defer to a larger model for better quality output while reducing inference costs.</p><hr><h3>ReTaSA: A Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift</h3>
<p><a href='https://openreview.net/forum?id=KdVvOA00Or'>https://openreview.net/forum?id=KdVvOA00Or</a></p>
<p><b>Compressor summary</b>: The paper proposes a nonparametric regularized approach called ReTaSA to estimate the importance weight function for addressing the target shift problem in regression settings with different marginal distributions of the continuous target variable in training and testing domains, while the conditional distribution of features remains the same.</p><hr><h3>Online GNN Evaluation Under Test-time Graph Distribution Shifts</h3>
<p><a href='https://openreview.net/forum?id=KbetDM33YG'>https://openreview.net/forum?id=KbetDM33YG</a></p>
<p><b>Compressor summary</b>: The paper proposes LeBeD, a score to estimate generalization error of GNNs on unlabeled real-world graphs under distribution shifts, using node prediction and structure reconstruction discrepancies.</p><hr><h3>SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training</h3>
<p><a href='https://openreview.net/forum?id=KZSEgJGPxu'>https://openreview.net/forum?id=KZSEgJGPxu</a></p>
<p><b>Compressor summary</b>: SNIP is a pre-training method for deep learning models that enhances their understanding of symbolic and numeric domains by jointly learning from both, leading to better performance across various tasks.</p><hr><h3>Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit</h3>
<p><a href='https://openreview.net/forum?id=KZJehvRKGD'>https://openreview.net/forum?id=KZJehvRKGD</a></p>
<p><b>Compressor summary</b>: The paper proposes using $\mu$P parameterized residual networks with a scaled residual branch to transfer optimal hyperparameters across width and depth, and supports the idea theoretically with dynamical mean field theory.</p><hr><h3>NetInfoF Framework: Measuring and Exploiting Network Usable Information</h3>
<p><a href='https://openreview.net/forum?id=KY8ZNcljVU'>https://openreview.net/forum?id=KY8ZNcljVU</a></p>
<p><b>Compressor summary</b>: NetInfoF is a framework for measuring and exploiting network usable information for link prediction and node classification tasks using graph neural networks (GNNs).</p><hr><h3>Constraint-Free Structure Learning with Smooth Acyclic Orientations</h3>
<p><a href='https://openreview.net/forum?id=KWO8LSUC5W'>https://openreview.net/forum?id=KWO8LSUC5W</a></p>
<p><b>Compressor summary</b>: COSMO is a new method for learning the structure of directed graphs that doesn't check for cycles and still works well.</p><hr><h3>MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts</h3>
<p><a href='https://openreview.net/forum?id=KUNzEQMWU7'>https://openreview.net/forum?id=KUNzEQMWU7</a></p>
<p><b>Compressor summary</b>: MathVista is a benchmark to test large language and multimodal models' ability to do mathematical reasoning with visual contexts, showing they are not yet at human level performance.</p><hr><h3>CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects</h3>
<p><a href='https://openreview.net/forum?id=KTtEICH4TO'>https://openreview.net/forum?id=KTtEICH4TO</a></p>
<p><b>Compressor summary</b>: The paper proposes a contact-based object representation and pretraining pipeline for nonprehensile manipulation using reinforcement learning, which enables efficient learning and zero-shot transfer to novel real-world objects.</p><hr><h3>$\textbf{\textit{M}}^\textbf{\textit{3}}$: Towards Robust Multi-Modal Reasoning via Model Selection</h3>
<p><a href='https://openreview.net/forum?id=KTf4DGAzus'>https://openreview.net/forum?id=KTf4DGAzus</a></p>
<p><b>Compressor summary</b>: The text discusses challenges in model selection for multi-modal agents in complex tasks and proposes a framework called M3 to improve it while maintaining low runtime overhead.</p><hr><h3>Memorization in Self-Supervised Learning Improves Downstream Generalization</h3>
<p><a href='https://openreview.net/forum?id=KSjPaXtxP8'>https://openreview.net/forum?id=KSjPaXtxP8</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework to define memorization in self-supervised learning and shows that it is essential for high performance of encoders on various tasks.</p><hr><h3>Proving Test Set Contamination for Black-Box Language Models</h3>
<p><a href='https://openreview.net/forum?id=KS8mIvetg2'>https://openreview.net/forum?id=KS8mIvetg2</a></p>
<p><b>Compressor summary</b>: Our procedure detects test set contamination in language models by comparing the likelihood of canonical orderings versus shuffled orderings, without needing access to pretraining data or model weights.</p><hr><h3>Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation</h3>
<p><a href='https://openreview.net/forum?id=KQe9tHd0k8'>https://openreview.net/forum?id=KQe9tHd0k8</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel algorithm that iteratively pseudo-labels instances based on aggregated labels and bag information, then refines embeddings using these pseudo-labels, and finally trains a classifier with the final pseudo-labels, achieving significant improvements in learning from label proportions problems for both tabular and image datasets.</p><hr><h3>Learning Implicit Representation for Reconstructing Articulated Objects</h3>
<p><a href='https://openreview.net/forum?id=KQ2i6jazVK'>https://openreview.net/forum?id=KQ2i6jazVK</a></p>
<p><b>Compressor summary</b>: The paper presents a method to reconstruct 3D articulated objects from motion cues in videos without using category-specific skeletons and achieves better results than existing methods.</p><hr><h3>LEAP: Liberate Sparse-View 3D Modeling from Camera Poses</h3>
<p><a href='https://openreview.net/forum?id=KPmajBxEaF'>https://openreview.net/forum?id=KPmajBxEaF</a></p>
<p><b>Compressor summary</b>: LEAP is a novel pose-free approach for sparse-view 3D modeling that uses a neural volume to encode geometry and texture priors, enabling fast and accurate novel view synthesis without camera poses.</p><hr><h3>Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization</h3>
<p><a href='https://openreview.net/forum?id=KOZu91CzbK'>https://openreview.net/forum?id=KOZu91CzbK</a></p>
<p><b>Compressor summary</b>: This paper presents a framework for enhancing large language models with autonomous language agents that learn from environment feedback using policy gradient and retrospective summaries.</p><hr><h3>HiGen: Hierarchical Graph Generative Networks</h3>
<p><a href='https://openreview.net/forum?id=KNvubydSB5'>https://openreview.net/forum?id=KNvubydSB5</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel graph generative network that captures the hierarchical structure of graphs, generates communities and cross-edges in parallel, models the output distribution with a multinomial factorization, and achieves high quality results on complex graphs.</p><hr><h3>At Which Training Stage Does Code Data Help LLMs Reasoning?</h3>
<p><a href='https://openreview.net/forum?id=KIPJKST4gw'>https://openreview.net/forum?id=KIPJKST4gw</a></p>
<p><b>Compressor summary</b>: The paper investigates how introducing code data in different stages of training large language models can improve their reasoning capabilities for various applications.</p><hr><h3>ReMasker: Imputing Tabular Data with Masked Autoencoding</h3>
<p><a href='https://openreview.net/forum?id=KI9NqjLVDT'>https://openreview.net/forum?id=KI9NqjLVDT</a></p>
<p><b>Compressor summary</b>: ReMasker is a simple and effective method for imputing missing values in tabular data using masked autoencoding, which learns representations invariant to missingness.</p><hr><h3>A Cognitive Model for Learning Abstract Relational Structures from Memory-based Decision-Making Tasks</h3>
<p><a href='https://openreview.net/forum?id=KC58bVmxyN'>https://openreview.net/forum?id=KC58bVmxyN</a></p>
<p><b>Compressor summary</b>: The authors propose a novel cognitive model that learns abstract relational structures across different domains and shows its similarity to human cognitive maps in the hippocampus.</p><hr><h3>Diving Segmentation Model into Pixels</h3>
<p><a href='https://openreview.net/forum?id=KBo7Z5aTV0'>https://openreview.net/forum?id=KBo7Z5aTV0</a></p>
<p><b>Compressor summary</b>: The paper introduces PiXL, a framework for semantic segmentation that focuses on tailored learning of pixels, handling pixel-level variance, and enhancing per-pixel recognition capabilities.</p><hr><h3>Data Filtering Networks</h3>
<p><a href='https://openreview.net/forum?id=KAk6ngZ09F'>https://openreview.net/forum?id=KAk6ngZ09F</a></p>
<p><b>Compressor summary</b>: The paper introduces data filtering networks (DFNs) to create high-quality image-text datasets from large uncurated pools, and releases two new state-of-the-art datasets and a method to train DFNs from public data.</p><hr><h3>VersVideo: Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation</h3>
<p><a href='https://openreview.net/forum?id=K9sVJ17zvB'>https://openreview.net/forum?id=K9sVJ17zvB</a></p>
<p><b>Compressor summary</b>: VersVideo is a versatile video generation model that enhances spatial-temporal performance using multi-excitation paths, spatial-temporal attention blocks, and temporal consistency modules, while maintaining controllability across various conditions.</p><hr><h3>Robust Similarity Learning with Difference Alignment Regularization</h3>
<p><a href='https://openreview.net/forum?id=K9V7ugVuUz'>https://openreview.net/forum?id=K9V7ugVuUz</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method, DAR, to improve similarity-based representation learning by aligning differences between classes, reducing overfitting and tightening error bounds.</p><hr><h3>CAMBranch: Contrastive Learning with Augmented MILPs for Branching</h3>
<p><a href='https://openreview.net/forum?id=K6kt50zAiG'>https://openreview.net/forum?id=K6kt50zAiG</a></p>
<p><b>Compressor summary</b>: CAMBranch uses contrastive learning and variable shifting to improve MILP branching policies with limited expert data.</p><hr><h3>Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair Mining Approach</h3>
<p><a href='https://openreview.net/forum?id=K2c04ulKXn'>https://openreview.net/forum?id=K2c04ulKXn</a></p>
<p><b>Compressor summary</b>: The paper proposes DBPM, a method to identify and suppress noisy and faulty positive pairs in time series contrastive learning, improving representation quality.</p><hr><h3>Conditional Information Bottleneck Approach for Time Series Imputation</h3>
<p><a href='https://openreview.net/forum?id=K1mcPiDdOJ'>https://openreview.net/forum?id=K1mcPiDdOJ</a></p>
<p><b>Compressor summary</b>: The paper proposes a conditional information bottleneck approach for time series imputation that preserves temporal context, reducing redundancy and improving performance in interpolation and extrapolation, as well as classification tasks.</p><hr><h3>Generative Adversarial Inverse Multiagent Learning</h3>
<p><a href='https://openreview.net/forum?id=JzvIWvC9MG'>https://openreview.net/forum?id=JzvIWvC9MG</a></p>
<p><b>Compressor summary</b>: The paper presents algorithms to find game parameters that result in expected equilibrium behavior using generative adversarial optimization and shows their effectiveness in predicting electricity market prices.</p><hr><h3>Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=JzG7kSpjJk'>https://openreview.net/forum?id=JzG7kSpjJk</a></p>
<p><b>Compressor summary</b>: Per-IC quantization and Adaptive Dimensions are proposed methods that improve the efficiency of large language models by mitigating outliers and adapting to weight sensitivity patterns.</p><hr><h3>Improving Natural Language Understanding with Computation-Efficient Retrieval Augmentation</h3>
<p><a href='https://openreview.net/forum?id=JtKGkz9fAe'>https://openreview.net/forum?id=JtKGkz9fAe</a></p>
<p><b>Compressor summary</b>: ReFusion is a method that fuses retrieval representations into language models for non-knowledge-intensive tasks, improving efficiency and performance.</p><hr><h3>Exploring Weight Balancing on Long-Tailed Recognition Problem</h3>
<p><a href='https://openreview.net/forum?id=JsnR0YO4Fq'>https://openreview.net/forum?id=JsnR0YO4Fq</a></p>
<p><b>Compressor summary</b>: The study analyzes weight balancing, a simple but effective method for long-tailed data, and shows how it can be simplified by understanding its underlying mechanisms.</p><hr><h3>A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation</h3>
<p><a href='https://openreview.net/forum?id=Js5PJPHDyY'>https://openreview.net/forum?id=Js5PJPHDyY</a></p>
<p><b>Compressor summary</b>: The paper proposes a method that improves CLIP's zero-shot classification performance by using Gaussian Discriminant Analysis, which requires less training time and computational resources than existing methods.</p><hr><h3>A Mutual Information Perspective on Federated Contrastive Learning</h3>
<p><a href='https://openreview.net/forum?id=JrmPG9ufKg'>https://openreview.net/forum?id=JrmPG9ufKg</a></p>
<p><b>Compressor summary</b>: The paper proposes federated SimCLR variants that incorporate user verification and semi-supervised learning, and studies how different non-i.i.d.-ness sources affect their performance.</p><hr><h3>LabelDP-Pro: Learning with Label Differential Privacy via Projections</h3>
<p><a href='https://openreview.net/forum?id=JnYaF3vv3G'>https://openreview.net/forum?id=JnYaF3vv3G</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm for private training of machine learning models that combines gradient projection and stochastic gradient descent, improving both privacy and utility.</p><hr><h3>Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning</h3>
<p><a href='https://openreview.net/forum?id=JnRStoIuTe'>https://openreview.net/forum?id=JnRStoIuTe</a></p>
<p><b>Compressor summary</b>: RSRS or RS2 is an efficient data pruning method for deep neural networks that reduces time-to-accuracy by learning from repeatedly sampled data subsets throughout training, outperforming many state-of-the-art methods.</p><hr><h3>Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators</h3>
<p><a href='https://openreview.net/forum?id=JiTVtCUOpS'>https://openreview.net/forum?id=JiTVtCUOpS</a></p>
<p><b>Compressor summary</b>: LIFT is a new method for multivariate time series forecasting that leverages local lead-lag relationships to improve predictions by using advance information from leading indicators.</p><hr><h3>FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing</h3>
<p><a href='https://openreview.net/forum?id=JgqftqZQZ7'>https://openreview.net/forum?id=JgqftqZQZ7</a></p>
<p><b>Compressor summary</b>: FLATTEN is a training-free method that improves visual consistency in text-to-video editing by using optical flow in the attention module of diffusion models' U-Nets.</p><hr><h3>The optimality of kernel classifiers in Sobolev space</h3>
<p><a href='https://openreview.net/forum?id=JfqN3gu0i7'>https://openreview.net/forum?id=JfqN3gu0i7</a></p>
<p><b>Compressor summary</b>: This paper analyzes the performance of kernel classification methods using statistical theory and proposes a way to estimate interpolation smoothness for practical applications.</p><hr><h3>Skill-Mix: a Flexible and Expandable Family of Evaluations for AI Models</h3>
<p><a href='https://openreview.net/forum?id=Jf5gplvglq'>https://openreview.net/forum?id=Jf5gplvglq</a></p>
<p><b>Compressor summary</b>: The paper introduces Skill-Mix, a new evaluation method to measure how well LLMs can combine different skills they have learned, which reveals differences in model capabilities not captured by existing evaluations.</p><hr><h3>When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations</h3>
<p><a href='https://openreview.net/forum?id=JewzobRhay'>https://openreview.net/forum?id=JewzobRhay</a></p>
<p><b>Compressor summary</b>: Context-based fine-tuning methods are less expressive than full fine-tuning and cannot change the relative attention pattern over the content.</p><hr><h3>iTransformer: Inverted Transformers Are Effective for Time Series Forecasting</h3>
<p><a href='https://openreview.net/forum?id=JePfAI8fah'>https://openreview.net/forum?id=JePfAI8fah</a></p>
<p><b>Compressor summary</b>: The iTransformer model improves Transformer-based forecasters by inverting the roles of attention and feed-forward networks for better representation of multivariate time series.</p><hr><h3>Multimodal Patient Representation Learning with Missing Modalities and Labels</h3>
<p><a href='https://openreview.net/forum?id=Je5SHCKpPa'>https://openreview.net/forum?id=Je5SHCKpPa</a></p>
<p><b>Compressor summary</b>: MUSE is a graph contrastive learning method for multimodal patient representation learning that handles missing modalities and labels and reduces modality collapse.</p><hr><h3>Optimal criterion for feature learning of two-layer linear neural network in high dimensional interpolation regime</h3>
<p><a href='https://openreview.net/forum?id=Jc0FssXh2R'>https://openreview.net/forum?id=Jc0FssXh2R</a></p>
<p><b>Compressor summary</b>: This paper analyzes when two-layer linear neural networks with feature learning outperform normal ridge regression in high-dimensional settings, using a new criterion that acts as an upper bound on the predictive risk and achieves optimal Bayes risk.</p><hr><h3>SEA: Sparse Linear Attention with Estimated Attention Mask</h3>
<p><a href='https://openreview.net/forum?id=JbcwfmYrob'>https://openreview.net/forum?id=JbcwfmYrob</a></p>
<p><b>Compressor summary</b>: SEA is a method that combines sparse and linear attention to achieve better performance and interpretability than previous sparse and linear approaches for natural language understanding tasks while using less memory.</p><hr><h3>Towards Codable Text Watermarking for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=JYu5Flqm9D'>https://openreview.net/forum?id=JYu5Flqm9D</a></p>
<p><b>Compressor summary</b>: The paper introduces and evaluates Codable Text Watermarking for LLMs (CTWL), a technique to embed hidden patterns in generated texts, carrying customizable information while preserving text quality.</p><hr><h3>The Update Equivalence Framework for Decision-Time Planning</h3>
<p><a href='https://openreview.net/forum?id=JXGph215fL'>https://openreview.net/forum?id=JXGph215fL</a></p>
<p><b>Compressor summary</b>: The paper proposes a new framework for decision-time planning in imperfect-information games that does not rely on subgames, improves performance in games with large amounts of non-public information, and is based on the notion of update equivalence.</p><hr><h3>ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning</h3>
<p><a href='https://openreview.net/forum?id=JWpwDdVbaM'>https://openreview.net/forum?id=JWpwDdVbaM</a></p>
<p><b>Compressor summary</b>: ARM is a new multivariate temporal-contextual learning method that improves long-term time series forecasting by adapting to individual series patterns and inter-series dependencies, without increasing computational costs.</p><hr><h3>AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction</h3>
<p><a href='https://openreview.net/forum?id=JW3jTjaaAB'>https://openreview.net/forum?id=JW3jTjaaAB</a></p>
<p><b>Compressor summary</b>: AirPhyNet is a novel approach for air quality prediction that uses physics-based neural networks to overcome the limitations of traditional models and achieve better accuracy and interpretability.</p><hr><h3>On the Role of General Function Approximation in Offline Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=JSS9rKHySk'>https://openreview.net/forum?id=JSS9rKHySk</a></p>
<p><b>Compressor summary</b>: The paper analyzes the challenges and applications of general function approximation in offline reinforcement learning, and introduces new lower bounds based on model-realizability.</p><hr><h3>On the Stability of Iterative Retraining of Generative Models on their own Data</h3>
<p><a href='https://openreview.net/forum?id=JORAfH2xFd'>https://openreview.net/forum?id=JORAfH2xFd</a></p>
<p><b>Compressor summary</b>: The paper develops a framework to study the impact of training generative models on mixed datasets of real and synthetic data, proving their stability under certain conditions and quantifying error and radius stability.</p><hr><h3>Scaling Laws of RoPE-based Extrapolation</h3>
<p><a href='https://openreview.net/forum?id=JO7k0SJ5V6'>https://openreview.net/forum?id=JO7k0SJ5V6</a></p>
<p><b>Compressor summary</b>: This paper explores the effects of modifying Rotary Position Embedding in Large Language Models to improve their extrapolation capabilities, and proposes a unified framework to describe the relationship between base value and tuning context length.</p><hr><h3>Koopman-based generalization bound: New aspect for full-rank weights</h3>
<p><a href='https://openreview.net/forum?id=JN7TcCm9LF'>https://openreview.net/forum?id=JN7TcCm9LF</a></p>
<p><b>Compressor summary</b>: The paper proposes a new generalization bound for neural networks using Koopman operators that works for full-rank weight matrices and is independent of the network width if the weights are orthogonal.</p><hr><h3>Sample-Efficient Quality-Diversity by Cooperative Coevolution</h3>
<p><a href='https://openreview.net/forum?id=JDud6zbpFv'>https://openreview.net/forum?id=JDud6zbpFv</a></p>
<p><b>Compressor summary</b>: CCQD is a new optimization framework that simplifies the QD problem by dividing a policy network into representation and decision layers, which coevolve together to improve sample efficiency.</p><hr><h3>Rethinking Label Poisoning for GNNs: Pitfalls and Attacks</h3>
<p><a href='https://openreview.net/forum?id=J7ioefqDPw'>https://openreview.net/forum?id=J7ioefqDPw</a></p>
<p><b>Compressor summary</b>: The paper studies how graph neural networks (GNNs) can be attacked by manipulating node labels, reveals evaluation issues in the existing literature, and proposes new and stronger attacks.</p><hr><h3>Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning</h3>
<p><a href='https://openreview.net/forum?id=J44HfH4JCg'>https://openreview.net/forum?id=J44HfH4JCg</a></p>
<p><b>Compressor summary</b>: The paper presents a new dataset and evaluation method for visual instruction tuning, and demonstrates how to reduce hallucination in multi-modal models by using them.</p><hr><h3>Toward Optimal Policy Population Growth in Two-Player Zero-Sum Games</h3>
<p><a href='https://openreview.net/forum?id=J2TZgj3Tac'>https://openreview.net/forum?id=J2TZgj3Tac</a></p>
<p><b>Compressor summary</b>: The paper introduces new algorithms for deep RL in two-agent environments that reduce exploitability and improve convergence, especially Self-Play PSRO which incorporates an optimal stochastic policy into the population.</p><hr><h3>Efficient Score Matching with Deep Equilibrium Layers</h3>
<p><a href='https://openreview.net/forum?id=J1djqLAa6N'>https://openreview.net/forum?id=J1djqLAa6N</a></p>
<p><b>Compressor summary</b>: The study improves score matching methods in deep learning by using deep equilibrium models, which reduce memory costs and enable better modeling.</p><hr><h3>Private Zeroth-Order Nonsmooth Nonconvex Optimization</h3>
<p><a href='https://openreview.net/forum?id=IzqZbNMZ0M'>https://openreview.net/forum?id=IzqZbNMZ0M</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm for private optimization that has the same optimal complexity as its non-private counterpart and works on nonconvex and nonsmooth objectives.</p><hr><h3>Bellman Optimal Step-size Straightening of Flow-Matching Models</h3>
<p><a href='https://openreview.net/forum?id=Iyve2ycvGZ'>https://openreview.net/forum?id=Iyve2ycvGZ</a></p>
<p><b>Compressor summary</b>: BOSS is a technique to improve efficiency and image quality of flow-matching generative models by optimizing step sizes and refining the velocity network.</p><hr><h3>VQ-TR: Vector Quantized Attention for Time Series Forecasting</h3>
<p><a href='https://openreview.net/forum?id=IxpTsFS7mh'>https://openreview.net/forum?id=IxpTsFS7mh</a></p>
<p><b>Compressor summary</b>: VQ-TR is a new method for probabilistic time series forecasting that uses discrete latent representations and Attention module to achieve accuracy and efficiency, outperforming current Transformer-based methods.</p><hr><h3>A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation</h3>
<p><a href='https://openreview.net/forum?id=Ixi4j6LtdX'>https://openreview.net/forum?id=Ixi4j6LtdX</a></p>
<p><b>Compressor summary</b>: MPDistil is a meta-policy distillation technique that improves knowledge transfer from larger to smaller language models using collaboration, competition, and curriculum learning.</p><hr><h3>Talk like a Graph: Encoding Graphs for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=IuXR1CCrSi'>https://openreview.net/forum?id=IuXR1CCrSi</a></p>
<p><b>Compressor summary</b>: The paper explores how different ways of writing graphs as text affect the ability of large language models to reason with them, and shows that choosing the right encoding method can significantly improve performance.</p><hr><h3>Consistent Video-to-Video Transfer Using Synthetic Dataset</h3>
<p><a href='https://openreview.net/forum?id=IoKRezZMxF'>https://openreview.net/forum?id=IoKRezZMxF</a></p>
<p><b>Compressor summary</b>: The paper presents an efficient method for editing videos using text instructions, which generates paired samples of input and edited videos with consistent long video lengths across batches.</p><hr><h3>Large Language Models Cannot Self-Correct Reasoning Yet</h3>
<p><a href='https://openreview.net/forum?id=IkmD3fKBPQ'>https://openreview.net/forum?id=IkmD3fKBPQ</a></p>
<p><b>Compressor summary</b>: The paper explores how well large language models can correct themselves without external feedback and suggests improvements for their reasoning abilities.</p><hr><h3>GraphChef: Decision-Tree Recipes to Explain Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=IjMUGuUmBI'>https://openreview.net/forum?id=IjMUGuUmBI</a></p>
<p><b>Compressor summary</b>: GraphChef is a new GNN model that creates human understandable rules (recipes) to explain the classes in a dataset using decision trees.</p><hr><h3>DP-OPT: Make Large Language Model Your Differentially-Private Prompt Engineer</h3>
<p><a href='https://openreview.net/forum?id=Ifz3IgsEPX'>https://openreview.net/forum?id=Ifz3IgsEPX</a></p>
<p><b>Compressor summary</b>: DP-OPT is a method to create privacy-friendly prompts for large language models without compromising their performance, by tuning them on the client side and applying them to cloud models.</p><hr><h3>Vanishing Gradients in Reinforcement Finetuning of Language Models</h3>
<p><a href='https://openreview.net/forum?id=IcVNBR7qZi'>https://openreview.net/forum?id=IcVNBR7qZi</a></p>
<p><b>Compressor summary</b>: Reinforcement finetuning (RFT) for language models suffers from vanishing gradients due to small reward standard deviation, which harms optimization and performance compared to supervised finetuning (SFT).</p><hr><h3>Towards Generative Abstract Reasoning: Completing Ravens Progressive Matrix via Rule Abstraction and Selection</h3>
<p><a href='https://openreview.net/forum?id=IcR1OOFzxm'>https://openreview.net/forum?id=IcR1OOFzxm</a></p>
<p><b>Compressor summary</b>: The paper proposes a conditional generative model (RAISE) that encodes image attributes as latent concepts and decomposes rules into atomic rules, enabling better abstract visual reasoning in machine intelligence for Raven's Progressive Matrix problems.</p><hr><h3>HIFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance</h3>
<p><a href='https://openreview.net/forum?id=IZMPWmcS3H'>https://openreview.net/forum?id=IZMPWmcS3H</a></p>
<p><b>Compressor summary</b>: The text describes a novel method for generating high-quality 3D models from text using a single-stage optimization, addressing artifacts, inconsistencies, and flickering issues in the previous methods.</p><hr><h3>Improved Active Learning via Dependent Leverage Score Sampling</h3>
<p><a href='https://openreview.net/forum?id=IYxDy2jDFL'>https://openreview.net/forum?id=IYxDy2jDFL</a></p>
<p><b>Compressor summary</b>: The paper proposes a method called pivotal sampling to improve active learning by reducing sample size and improving coverage, and provides two theoretical results supporting its effectiveness.</p><hr><h3>Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation</h3>
<p><a href='https://openreview.net/forum?id=ITq4ZRUT4a'>https://openreview.net/forum?id=ITq4ZRUT4a</a></p>
<p><b>Compressor summary</b>: The text proposes DSG, a graph-based QG/A framework for evaluating text-to-image models that addresses reliability challenges in existing methods.</p><hr><h3>Learning Hierarchical Image Segmentation For Recognition and By Recognition</h3>
<p><a href='https://openreview.net/forum?id=IRcv4yFX6z'>https://openreview.net/forum?id=IRcv4yFX6z</a></p>
<p><b>Compressor summary</b>: CAST is a novel learning framework that combines image segmentation and recognition using adaptive segment tokens inspired by vision transformers.</p><hr><h3>Window Attention is Bugged: How not to Interpolate Position Embeddings</h3>
<p><a href='https://openreview.net/forum?id=IPhm01y9a9'>https://openreview.net/forum?id=IPhm01y9a9</a></p>
<p><b>Compressor summary</b>: The paper proposes a simple bug fix called "absolute win" that improves the performance of two computer vision models, Hiera and ViTDet, by correctly handling window attention and position embeddings.</p><hr><h3>Adversarial Feature Map Pruning for Backdoor</h3>
<p><a href='https://openreview.net/forum?id=IOEEDkla96'>https://openreview.net/forum?id=IOEEDkla96</a></p>
<p><b>Compressor summary</b>: The paper proposes a defense strategy for deep neural networks against backdoor attacks by pruning the feature maps that extract the attack information, achieving better performance and coverage than existing methods.</p><hr><h3>Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model</h3>
<p><a href='https://openreview.net/forum?id=ILqA09Oeq2'>https://openreview.net/forum?id=ILqA09Oeq2</a></p>
<p><b>Compressor summary</b>: The paper compares two methods for detecting a hidden signal in multi-view clustering data: one using tensor decomposition and another using matrix approximation, finding that the latter performs worse and has an optimal threshold.</p><hr><h3>Uncertainty-aware Constraint Inference in Inverse Constrained Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=ILYjDvUM6U'>https://openreview.net/forum?id=ILYjDvUM6U</a></p>
<p><b>Compressor summary</b>: UAICRL is a robust ICRL algorithm that models and mitigates aleatoric and epistemic uncertainties to infer and adhere to expert constraints more accurately.</p><hr><h3>Fleet Policy Learning via Weight Merging and An Application to Robotic Tool-Use</h3>
<p><a href='https://openreview.net/forum?id=IL71c1z7et'>https://openreview.net/forum?id=IL71c1z7et</a></p>
<p><b>Compressor summary</b>: The paper proposes fleet-merge, a method for distributed learning of policies among robots that can handle symmetries and improve performance on various tasks, including a new tool-use benchmark.</p><hr><h3>GraphGuard: Provably Robust Graph Classification against Adversarial Attacks</h3>
<p><a href='https://openreview.net/forum?id=IGzaH538fz'>https://openreview.net/forum?id=IGzaH538fz</a></p>
<p><b>Compressor summary</b>: GraphGuard is a certified defense that robustly predicts labels for graphs under perturbations, overcoming limitations of existing provable defenses and empirical defenses in graph classification.</p><hr><h3>Eureka: Human-Level Reward Design via Coding Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=IEduRUO55F'>https://openreview.net/forum?id=IEduRUO55F</a></p>
<p><b>Compressor summary</b>: Eureka is an algorithm that uses large language models to generate human-level rewards for complex low-level manipulation tasks, outperforming expert-designed rewards and enabling in-context learning.</p><hr><h3>DiffusionSat: A Generative Foundation Model for Satellite Imagery</h3>
<p><a href='https://openreview.net/forum?id=I5webNFDgQ'>https://openreview.net/forum?id=I5webNFDgQ</a></p>
<p><b>Compressor summary</b>: DiffusionSat is a large generative model that produces realistic satellite images using metadata and outperforms previous methods.</p><hr><h3>Domain-Inspired Sharpness Aware Minimization Under Domain Shifts</h3>
<p><a href='https://openreview.net/forum?id=I4wB3HA3dJ'>https://openreview.net/forum?id=I4wB3HA3dJ</a></p>
<p><b>Compressor summary</b>: DISAM is an algorithm that optimizes under domain shifts and improves convergence and generalization by adjusting gradient perturbations based on domain losses.</p><hr><h3>Understanding Domain Generalization: A Noise Robustness Perspective</h3>
<p><a href='https://openreview.net/forum?id=I2mIxuXA72'>https://openreview.net/forum?id=I2mIxuXA72</a></p>
<p><b>Compressor summary</b>: This paper examines how machine learning algorithms for domain generalization handle label noise and spurious correlations, finding that they have some benefits but not necessarily better performance than classic empirical risk minimization.</p><hr><h3>Energy-Based Concept Bottleneck Models</h3>
<p><a href='https://openreview.net/forum?id=I1quoTXZzc'>https://openreview.net/forum?id=I1quoTXZzc</a></p>
<p><b>Compressor summary</b>: ECBMs improve black-box deep learning models by capturing high-order interactions between concepts and quantifying conditional dependencies, leading to higher accuracy and richer interpretations.</p><hr><h3>Oracle Efficient Algorithms for Groupwise Regret</h3>
<p><a href='https://openreview.net/forum?id=HrRKc9ei7h'>https://openreview.net/forum?id=HrRKc9ei7h</a></p>
<p><b>Compressor summary</b>: The paper proposes an efficient algorithm for online prediction with group-wise regret guarantees, based on a modification of the sleeping experts technique.</p><hr><h3>Chain-of-Experts: When LLMs Meet Complex Operations Research Problems</h3>
<p><a href='https://openreview.net/forum?id=HobyL1B9CZ'>https://openreview.net/forum?id=HobyL1B9CZ</a></p>
<p><b>Compressor summary</b>: The paper proposes Chain-of-Experts, a multi-agent framework using large language models to solve complex operation research problems, which also introduces a new benchmark dataset.</p><hr><h3>Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning</h3>
<p><a href='https://openreview.net/forum?id=HiYMiZYwkw'>https://openreview.net/forum?id=HiYMiZYwkw</a></p>
<p><b>Compressor summary</b>: SMA is a domain-agnostic masked modeling method that learns masks to sample data without domain-specific assumptions, achieving state-of-the-art performance in self-supervised learning across protein biology, chemical property prediction, and particle physics.</p><hr><h3>Dynamic Neural Response Tuning</h3>
<p><a href='https://openreview.net/forum?id=HiTg16qhxp'>https://openreview.net/forum?id=HiTg16qhxp</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel mechanism called Dynamic Neural Response Tuning (DNRT) for artificial neural networks that mimics biological neurons' dynamic response conditions and improves their performance in various tasks and domains.</p><hr><h3>Towards a statistical theory of data selection under weak supervision</h3>
<p><a href='https://openreview.net/forum?id=HhfcNgQn6p'>https://openreview.net/forum?id=HhfcNgQn6p</a></p>
<p><b>Compressor summary</b>: The paper studies how to select a subset of unlabeled data samples to reduce labeling and computational costs, and shows that data selection can improve performance over training on the full sample.</p><hr><h3>Can Transformers Capture Spatial Relations between Objects?</h3>
<p><a href='https://openreview.net/forum?id=HgZUcwFhjr'>https://openreview.net/forum?id=HgZUcwFhjr</a></p>
<p><b>Compressor summary</b>: The paper presents RelatiViT, a new method based on transformers, to recognize spatial relations between objects in images more accurately than current approaches.</p><hr><h3>SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem</h3>
<p><a href='https://openreview.net/forum?id=HgOJlxzB16'>https://openreview.net/forum?id=HgOJlxzB16</a></p>
<p><b>Compressor summary</b>: This paper shows how to train a 2-layer neural network with ReLU activations and standard SGD on XOR function data using a signal-finding phase and a signal-heavy phase, achieving low population error with polylogarithmic samples.</p><hr><h3>Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted Activations</h3>
<p><a href='https://openreview.net/forum?id=HfXDrAzFvG'>https://openreview.net/forum?id=HfXDrAzFvG</a></p>
<p><b>Compressor summary</b>: The paper extends LipSDP to estimate Lipschitz bounds for neural networks with general activation functions like GroupSort, MaxMin, and Householder by using novel quadratic constraints derived from their properties.</p><hr><h3>Towards Best Practices of Activation Patching in Language Models: Metrics and Methods</h3>
<p><a href='https://openreview.net/forum?id=Hf17y6u9BC'>https://openreview.net/forum?id=Hf17y6u9BC</a></p>
<p><b>Compressor summary</b>: The text discusses the impact of methodological details in activation patching, a technique to understand machine learning models' internal mechanisms, and provides recommendations for its best practices.</p><hr><h3>Topic modeling as multi-objective optimization with Setwise Contrastive Learning</h3>
<p><a href='https://openreview.net/forum?id=HdAoLSBYXj'>https://openreview.net/forum?id=HdAoLSBYXj</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel contrastive learning method for neural topic models that balances the trade-off between reconstruction and generalization objectives using sets of topic vectors.</p><hr><h3>Principled Architecture-aware Scaling of Hyperparameters</h3>
<p><a href='https://openreview.net/forum?id=HZndRcfyNI'>https://openreview.net/forum?id=HZndRcfyNI</a></p>
<p><b>Compressor summary</b>: This paper characterizes how neural network architecture affects the choice of hyperparameters and proposes a method to generalize across different architectures for better AutoML performance.</p><hr><h3>Set Learning for Accurate and Calibrated Models</h3>
<p><a href='https://openreview.net/forum?id=HZ3S17EI0o'>https://openreview.net/forum?id=HZ3S17EI0o</a></p>
<p><b>Compressor summary</b>: OKO learning improves accuracy and calibration of machine learning models by considering sets instead of individual examples.</p><hr><h3>Object-Centric Semantic Vector Quantization</h3>
<p><a href='https://openreview.net/forum?id=HYyRwm367m'>https://openreview.net/forum?id=HYyRwm367m</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel model called SVQ that learns semantic neural discrete representations by quantizing scenes hierarchically and training a prior over them, achieving better generation performance and scene understanding compared to existing methods.</p><hr><h3>FairVLM: Mitigating Bias In Pre-Trained Vision-Language Models</h3>
<p><a href='https://openreview.net/forum?id=HXoq9EqR9e'>https://openreview.net/forum?id=HXoq9EqR9e</a></p>
<p><b>Compressor summary</b>: FairVLM is a method to make vision-language models more fair and robust by jointly debiasing their image and text representations in RKHSs, improving zero-shot prediction accuracy and addressing societal biases and spurious features.</p><hr><h3>Diffusion Sampling with Momentum for Mitigating Divergence Artifacts</h3>
<p><a href='https://openreview.net/forum?id=HXc5aXeoc8'>https://openreview.net/forum?id=HXc5aXeoc8</a></p>
<p><b>Compressor summary</b>: The paper proposes two techniques to reduce artifacts in image generation using diffusion models, one based on Heavy Ball momentum and another on a variable trade-off between accuracy and artifact suppression.</p><hr><h3>Label-Noise Robust Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=HXWTXXtHNl'>https://openreview.net/forum?id=HXWTXXtHNl</a></p>
<p><b>Compressor summary</b>: TDSM is a new method to train conditional diffusion models with noisy labels by estimating transition probabilities from clean and noisy labels and improving sample quality.</p><hr><h3>In-Context Learning through the Bayesian Prism</h3>
<p><a href='https://openreview.net/forum?id=HX5ujdsSon'>https://openreview.net/forum?id=HX5ujdsSon</a></p>
<p><b>Compressor summary</b>: This paper explores how large language models like transformers use a Bayesian perspective to learn and generalize to new functions in different task families, as well as when they deviate from this behavior.</p><hr><h3>Learning to Compose: Improving Object Centric Learning by Injecting Compositionality</h3>
<p><a href='https://openreview.net/forum?id=HT2dAhh4uV'>https://openreview.net/forum?id=HT2dAhh4uV</a></p>
<p><b>Compressor summary</b>: Our method improves object-centric learning by explicitly encouraging compositionality in representation with a novel objective that ensures valid mixtures of object features from different images.</p><hr><h3>Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness</h3>
<p><a href='https://openreview.net/forum?id=HSKaGOi7Ar'>https://openreview.net/forum?id=HSKaGOi7Ar</a></p>
<p><b>Compressor summary</b>: The paper introduces homomorphism expressivity as a novel framework for quantitatively assessing the expressiveness of Graph Neural Networks (GNNs), addressing limitations of existing measures and providing insights, bridging gaps, and settling open questions in the GNN community.</p><hr><h3>Compositional Conservatism: A Transductive Approach in Offline Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=HRkyLbBRHI'>https://openreview.net/forum?id=HRkyLbBRHI</a></p>
<p><b>Compressor summary</b>: COCOA is a novel approach to offline reinforcement learning that achieves conservatism by reparameterizing inputs as compositions of anchors and differences, improving the performance of four state-of-the-art algorithms on the D4RL benchmark.</p><hr><h3>Efficiently Computing Similarities to Private Datasets</h3>
<p><a href='https://openreview.net/forum?id=HMe5CJv9dQ'>https://openreview.net/forum?id=HMe5CJv9dQ</a></p>
<p><b>Compressor summary</b>: The paper proposes a new differentially private data structure that efficiently approximates similarity queries between a large high-dimensional private dataset and any query point, using low-dimensional structures in the similarity function.</p><hr><h3>Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics</h3>
<p><a href='https://openreview.net/forum?id=HKgRwNhI9R'>https://openreview.net/forum?id=HKgRwNhI9R</a></p>
<p><b>Compressor summary</b>: The paper proposes a general method for continuous convolutions using separable basis functions in fluid simulations and shows that Fourier-based networks are more accurate and stable than other architectures, without needing window functions.</p><hr><h3>Frequency-Aware Transformer for Learned  Image Compression</h3>
<p><a href='https://openreview.net/forum?id=HKGQDDTuvZ'>https://openreview.net/forum?id=HKGQDDTuvZ</a></p>
<p><b>Compressor summary</b>: The FAT block is a novel method that uses multiscale directional analysis to improve learned image compression by capturing anisotropic frequency components and preserving directional details.</p><hr><h3>Looped Transformers are Better at Learning Learning Algorithms</h3>
<p><a href='https://openreview.net/forum?id=HHbRxoDTxE'>https://openreview.net/forum?id=HHbRxoDTxE</a></p>
<p><b>Compressor summary</b>: The looped transformer is a new architecture that combines iterative features with transformers, enabling them to solve data-fitting problems more efficiently and with fewer parameters.</p><hr><h3>SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation</h3>
<p><a href='https://openreview.net/forum?id=HHWlwxDeRn'>https://openreview.net/forum?id=HHWlwxDeRn</a></p>
<p><b>Compressor summary</b>: The paper presents a novel method for teaching robots to manipulate diverse objects using sparse 3D features learned from images, feature refinement, and an energy function.</p><hr><h3>Temporal Generalization Estimation in Evolving Graphs</h3>
<p><a href='https://openreview.net/forum?id=HFtrXBfNru'>https://openreview.net/forum?id=HFtrXBfNru</a></p>
<p><b>Compressor summary</b>: Smart is a method that uses self-supervised graph reconstruction to improve the representation of evolving graphs in Graph Neural Networks and reduce distortion.</p><hr><h3>"What Data Benefits My Classifier?" Enhancing Model Performance and Interpretability through Influence-Based Data Selection</h3>
<p><a href='https://openreview.net/forum?id=HE9eUQlAvo'>https://openreview.net/forum?id=HE9eUQlAvo</a></p>
<p><b>Compressor summary</b>: The paper proposes data selection methods based on influence estimation to enhance the utility, fairness, and robustness of convex learning models for a given function of interest, and tests them on diverse datasets and settings.</p><hr><h3>Forward $\chi^2$ Divergence Based Variational Importane Sampling</h3>
<p><a href='https://openreview.net/forum?id=HD5Y7M8Xdk'>https://openreview.net/forum?id=HD5Y7M8Xdk</a></p>
<p><b>Compressor summary</b>: VIS is a new method that improves latent variable model learning by maximizing the log-likelihood using optimal proposal distributions.</p><hr><h3>Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments</h3>
<p><a href='https://openreview.net/forum?id=HC0msxE3sf'>https://openreview.net/forum?id=HC0msxE3sf</a></p>
<p><b>Compressor summary</b>: The paper reinterprets a common setting in emergent communication as a different model and shows how choosing the right prior distributions can make the resulting languages more like natural ones in terms of word lengths and segmentation.</p><hr><h3>Discovering modular solutions that generalize compositionally</h3>
<p><a href='https://openreview.net/forum?id=H98CVcX1eh'>https://openreview.net/forum?id=H98CVcX1eh</a></p>
<p><b>Compressor summary</b>: The text discusses modular systems' ability to discover compositional structure in tasks, their theoretical and empirical analysis, and application to various problems.</p><hr><h3>DMV3D: Denoising Multi-view Diffusion Using 3D Large Reconstruction Model</h3>
<p><a href='https://openreview.net/forum?id=H4yQefeXhp'>https://openreview.net/forum?id=H4yQefeXhp</a></p>
<p><b>Compressor summary</b>: DMV3D is a new method that uses a transformer model to denoise and reconstruct 3D images from multi-view data, achieving high-quality results for single-image reconstruction and text-to-3D generation.</p><hr><h3>A Unified Approach for Online Continuous DR-Submodular Maximization</h3>
<p><a href='https://openreview.net/forum?id=H4A9e8HvIn'>https://openreview.net/forum?id=H4A9e8HvIn</a></p>
<p><b>Compressor summary</b>: The paper presents new projection-free algorithms for optimal non-monotone and monotone adversarial continuous DR-submodular optimization with sub-linear regret bounds and various constraints and feedback types.</p><hr><h3>On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs</h3>
<p><a href='https://openreview.net/forum?id=H3UayAQWoE'>https://openreview.net/forum?id=H3UayAQWoE</a></p>
<p><b>Compressor summary</b>: The paper introduces a framework called PPBench to evaluate psychological aspects of large language models and tests five popular models using it.</p><hr><h3>Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning</h3>
<p><a href='https://openreview.net/forum?id=H3IUunLy8s'>https://openreview.net/forum?id=H3IUunLy8s</a></p>
<p><b>Compressor summary</b>: CAPABOOST is a strategy that enhances model capacity for fine-tuning large foundation models by using low-rank updates through parallel weight modules with random masks, improving performance on various downstream tasks without extra costs.</p><hr><h3>A unique M-pattern for micro-expreesion spotting in long videos</h3>
<p><a href='https://openreview.net/forum?id=H396R79GiQ'>https://openreview.net/forum?id=H396R79GiQ</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for micro-expression spotting that uses an unfixed reference frame, a skip-$k$-frame strategy, and a block-wise main directional mean optical flow feature to accurately detect and locate micro-expressions despite their small size and susceptibility to head movements.</p><hr><h3>A Benchmark Study on Calibration</h3>
<p><a href='https://openreview.net/forum?id=GzNhzX9kVa'>https://openreview.net/forum?id=GzNhzX9kVa</a></p>
<p><b>Compressor summary</b>: The authors create a dataset to explore model calibration properties in neural architecture search (NAS) using 117,702 unique neural networks across various datasets, robustness, and bin size measurements, addressing several longstanding questions in the field.</p><hr><h3>PINNACLE: PINN Adaptive ColLocation and Experimental points selection</h3>
<p><a href='https://openreview.net/forum?id=GzNaCp6Vcg'>https://openreview.net/forum?id=GzNaCp6Vcg</a></p>
<p><b>Compressor summary</b>: PINNACLE is a novel algorithm that optimizes the selection of all training points in Physics-Informed Neural Networks, improving their performance in various learning tasks.</p><hr><h3>Universal Jailbreak Backdoors from Poisoned Human Feedback</h3>
<p><a href='https://openreview.net/forum?id=GxCGsxiAaK'>https://openreview.net/forum?id=GxCGsxiAaK</a></p>
<p><b>Compressor summary</b>: The paper explores the threat of poisoning reinforcement learning data with universal jailbreak backdoors that allow harmful responses without needing adversarial prompts.</p><hr><h3>DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing</h3>
<p><a href='https://openreview.net/forum?id=GruDNzQ4ux'>https://openreview.net/forum?id=GruDNzQ4ux</a></p>
<p><b>Compressor summary</b>: The paper proposes DreamSmooth, a reward smoothing method for model-based reinforcement learning that improves sample efficiency and performance on sparse-reward tasks.</p><hr><h3>Offline RL with Observation Histories: Analyzing and Improving Sample Complexity</h3>
<p><a href='https://openreview.net/forum?id=GnOLWS4Llt'>https://openreview.net/forum?id=GnOLWS4Llt</a></p>
<p><b>Compressor summary</b>: The text discusses challenges and solutions for offline reinforcement learning in complex applications involving partially observed states, proposing a bisimulation loss to improve sample efficiency.</p><hr><h3>Improved algorithm and bounds for successive projection</h3>
<p><a href='https://openreview.net/forum?id=GlpawHh80l'>https://openreview.net/forum?id=GlpawHh80l</a></p>
<p><b>Compressor summary</b>: The paper proposes pp-SPA, a new approach to vertex hunting that generates pseudo-points and improves the performance of SPA by reducing noise and outliers in high-dimensional spaces.</p><hr><h3>Tree-Planner: Efficient Close-loop Task Planning with Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=Glcsog6zOe'>https://openreview.net/forum?id=Glcsog6zOe</a></p>
<p><b>Compressor summary</b>: The paper introduces Tree-Planner, a method that improves efficiency and scalability of task planning with LLMs by breaking down the process into three phases: plan sampling, action tree construction, and grounded deciding.</p><hr><h3>FeatUp: A Model-Agnostic Framework for Features at Any Resolution</h3>
<p><a href='https://openreview.net/forum?id=GkJiNn2QDF'>https://openreview.net/forum?id=GkJiNn2QDF</a></p>
<p><b>Compressor summary</b>: FeatUp is a framework to restore spatial information in deep features, enabling better performance in dense prediction tasks like segmentation and depth prediction without retraining.</p><hr><h3>Orbit-Equivariant Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=GkJOCga62u'>https://openreview.net/forum?id=GkJOCga62u</a></p>
<p><b>Compressor summary</b>: This paper introduces orbit-equivariance, a relaxed form of equivariance in graph neural networks that enables functions to produce different outputs for similar nodes while preserving important structural biases.</p><hr><h3>Investigating the Benefits of Projection Head for Representation Learning</h3>
<p><a href='https://openreview.net/forum?id=GgEAdqYPNA'>https://openreview.net/forum?id=GgEAdqYPNA</a></p>
<p><b>Compressor summary</b>: MMCL achieves robust representations by contrasting intra-class features and sharing inter-class details, preventing spurious features from dominating generalizable ones.</p><hr><h3>Dictionary Contrastive Forward Learning via Adaptive Label Embeddings</h3>
<p><a href='https://openreview.net/forum?id=Gg7cXo3S8l'>https://openreview.net/forum?id=Gg7cXo3S8l</a></p>
<p><b>Compressor summary</b>: DC-FL is a novel contrastive learning approach for forward learning that improves performance and efficiency compared to existing methods and backpropagation.</p><hr><h3>REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes</h3>
<p><a href='https://openreview.net/forum?id=Gf15GsnfTy'>https://openreview.net/forum?id=Gf15GsnfTy</a></p>
<p><b>Compressor summary</b>: REValueD is a new reinforcement learning algorithm that uses value-decomposition to handle high-dimensional discrete action spaces and achieves better performance on challenging tasks.</p><hr><h3>Robust Model Based Reinforcement Learning Using $\mathcal{L}_1$ Adaptive Control</h3>
<p><a href='https://openreview.net/forum?id=GaLCLvJaoF'>https://openreview.net/forum?id=GaLCLvJaoF</a></p>
<p><b>Compressor summary</b>: The text introduces a control-theoretic technique called $\mathcal{L}_1$-MBRL that improves the robustness of Model-Based Reinforcement Learning algorithms by generating an approximate model of the transition function and adapting the control input accordingly.</p><hr><h3>MMD Graph Kernel: Effective Metric Learning for Graphs via Maximum Mean Discrepancy</h3>
<p><a href='https://openreview.net/forum?id=GZ6AcZwA8r'>https://openreview.net/forum?id=GZ6AcZwA8r</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel class of graph kernels called MMD-GK that use MMD to measure graph similarities effectively and adaptively, and demonstrate their advantages over existing methods on various graph tasks.</p><hr><h3>Domain Randomization via Entropy Maximization</h3>
<p><a href='https://openreview.net/forum?id=GXtmuiVrOM'>https://openreview.net/forum?id=GXtmuiVrOM</a></p>
<p><b>Compressor summary</b>: DORAEMON is a new method for reinforcement learning that adjusts dynamics parameters during training to maximize entropy and generalization, improving sim-to-real transfer.</p><hr><h3>Rethinking Information-theoretic Generalization: Loss Entropy Induced PAC Bounds</h3>
<p><a href='https://openreview.net/forum?id=GWSIo2MzuH'>https://openreview.net/forum?id=GWSIo2MzuH</a></p>
<p><b>Compressor summary</b>: The paper proposes new PAC information-theoretic bounds for generalization analysis that are easier to compute and do not require the bounded loss assumption, by using loss entropies.</p><hr><h3>Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment</h3>
<p><a href='https://openreview.net/forum?id=GW4j4n2cjH'>https://openreview.net/forum?id=GW4j4n2cjH</a></p>
<p><b>Compressor summary</b>: The paper introduces a new task in dance generation called dance accompaniment, where a model generates responsive movements for a follower dancer synchronized with a lead dancer and music.</p><hr><h3>DreamFlow: High-quality text-to-3D generation by Approximating Probability Flow</h3>
<p><a href='https://openreview.net/forum?id=GURqUuTebY'>https://openreview.net/forum?id=GURqUuTebY</a></p>
<p><b>Compressor summary</b>: The paper introduces DreamFlow, a fast and high-quality text-to-3D generation method that uses a novel optimization algorithm based on multi-view image-to-image translation and predetermined timesteps.</p><hr><h3>DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation</h3>
<p><a href='https://openreview.net/forum?id=GTk0AdOYLq'>https://openreview.net/forum?id=GTk0AdOYLq</a></p>
<p><b>Compressor summary</b>: The text proposes a diffusion probabilistic end-to-end model for generating raw speech waveforms, which can synthesize unlimited speech duration with high-fidelity, preserving temporal coherence and natural local acoustic behaviors.</p><hr><h3>Noisy Interpolation Learning with Shallow Univariate ReLU Networks</h3>
<p><a href='https://openreview.net/forum?id=GTUoTJXPBf'>https://openreview.net/forum?id=GTUoTJXPBf</a></p>
<p><b>Compressor summary</b>: The paper analyzes how overparameterized neural networks generalize well despite interpolating noisy data, finding that they often exhibit "tempered overfitting" but can also suffer from catastrophic overfitting in certain scenarios.</p><hr><h3>Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=GSBHKiw19c'>https://openreview.net/forum?id=GSBHKiw19c</a></p>
<p><b>Compressor summary</b>: The study introduces a new idea called dynamics reward to improve offline reinforcement learning and proposes MOREC, a method that achieves state-of-the-art performance on benchmarks.</p><hr><h3>AutoChunk: Automated Activation Chunk for Memory-Efficient Deep Learning Inference</h3>
<p><a href='https://openreview.net/forum?id=GQGNLEHmdl'>https://openreview.net/forum?id=GQGNLEHmdl</a></p>
<p><b>Compressor summary</b>: AutoChunk is an automatic compiler system that reduces activation memory for long sequence inference in deep learning models by applying chunk strategies.</p><hr><h3>The Reversal Curse: LLMs trained on A is B fail to learn B is A</h3>
<p><a href='https://openreview.net/forum?id=GPKTIktA0k'>https://openreview.net/forum?id=GPKTIktA0k</a></p>
<p><b>Compressor summary</b>: The Reversal Curse is a phenomenon where large language models fail to generalize from one direction of a statement ("A is B") to its reverse ("B is A").</p><hr><h3>RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval</h3>
<p><a href='https://openreview.net/forum?id=GN921JHCRw'>https://openreview.net/forum?id=GN921JHCRw</a></p>
<p><b>Compressor summary</b>: RAPTOR is a novel approach that recursively embeds, clusters, and summarizes text chunks into a tree, enabling better integration and understanding of information across long documents for various tasks, especially complex question-answering.</p><hr><h3>Effective Structural Encodings via Local Curvature Profiles</h3>
<p><a href='https://openreview.net/forum?id=GIUjLsDP4Z'>https://openreview.net/forum?id=GIUjLsDP4Z</a></p>
<p><b>Compressor summary</b>: The paper proposes a new structural encoding method for graph neural networks based on discrete Ricci curvature, which outperforms existing methods and improves downstream tasks more than rewiring techniques.</p><hr><h3>Grokking in Linear Estimators -- A Solvable Model that Groks without Understanding</h3>
<p><a href='https://openreview.net/forum?id=GH2LYb9XV0'>https://openreview.net/forum?id=GH2LYb9XV0</a></p>
<p><b>Compressor summary</b>: Grokking is when a linear model learns to generalize well after training on simple tasks, and it can be explained by the dynamics of the training and generalization data covariance matrix.</p><hr><h3>Identifying the Risks of LM Agents with an LM-Emulated Sandbox</h3>
<p><a href='https://openreview.net/forum?id=GEcwtMk1uA'>https://openreview.net/forum?id=GEcwtMk1uA</a></p>
<p><b>Compressor summary</b>: ToolEmu is a framework that uses an LM to emulate tool execution and enables scalable testing of LM agents against diverse tools and scenarios, helping identify risks associated with language model agents.</p><hr><h3>Empirical Likelihood for Fair Classification</h3>
<p><a href='https://openreview.net/forum?id=GACjMj1MS1'>https://openreview.net/forum?id=GACjMj1MS1</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to assess and impose fairness constraints on machine learning algorithms using uncertainty and covariance measures, achieving accurate and fair results in decision-making systems.</p><hr><h3>Hypothesis Search: Inductive Reasoning with Language Models</h3>
<p><a href='https://openreview.net/forum?id=G7UtIGQmjm'>https://openreview.net/forum?id=G7UtIGQmjm</a></p>
<p><b>Compressor summary</b>: The authors propose a method to improve large language models' inductive reasoning ability by generating hypotheses in natural language and converting them into Python programs, which outperforms the direct prompting approach and requires minimal human input.</p><hr><h3>PTaRL: Prototype-based Tabular Representation Learning via Space Calibration</h3>
<p><a href='https://openreview.net/forum?id=G32oY4Vnm8'>https://openreview.net/forum?id=G32oY4Vnm8</a></p>
<p><b>Compressor summary</b>: PTaRL is a prototype-based representation learning framework that improves prediction performance and consistency for tabular tasks by constructing disentangled representations around global data prototypes.</p><hr><h3>Image Clustering Conditioned on Text Criteria</h3>
<p><a href='https://openreview.net/forum?id=G2cG3mQqop'>https://openreview.net/forum?id=G2cG3mQqop</a></p>
<p><b>Compressor summary</b>: IC$|$TC is a new method for image clustering based on text criteria, which gives users more control and better results than traditional methods.</p><hr><h3>Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning</h3>
<p><a href='https://openreview.net/forum?id=G1Hlubz1fR'>https://openreview.net/forum?id=G1Hlubz1fR</a></p>
<p><b>Compressor summary</b>: This paper proposes a novel modular skill learning approach that categorizes and efficiently organizes skills for neural networks, improving knowledge transfer and sample efficiency in multi-task learning.</p><hr><h3>CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets</h3>
<p><a href='https://openreview.net/forum?id=G0vdDSt9XM'>https://openreview.net/forum?id=G0vdDSt9XM</a></p>
<p><b>Compressor summary</b>: CRAFT is a framework that enhances LLMs by generating and retrieving task-specific code snippets, improving their performance on various complex tasks without fine-tuning.</p><hr><h3>AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</h3>
<p><a href='https://openreview.net/forum?id=Fx2SbBgcte'>https://openreview.net/forum?id=Fx2SbBgcte</a></p>
<p><b>Compressor summary</b>: AnimateDiff is a framework for animating personalized text-to-image models by adding a plug-and-play motion module, which can be fine-tuned with MotionLoRA to adapt to new motion patterns.</p><hr><h3>GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion</h3>
<p><a href='https://openreview.net/forum?id=FvK2noilxT'>https://openreview.net/forum?id=FvK2noilxT</a></p>
<p><b>Compressor summary</b>: The paper presents GeneOH Diffusion, a novel approach to refine hand-object interactions from noisy sequences by using a contact-centric representation and a denoising via diffusion strategy.</p><hr><h3>AttEXplore: Attribution for Explanation with model parameters eXploration</h3>
<p><a href='https://openreview.net/forum?id=FsVxd9CIlb'>https://openreview.net/forum?id=FsVxd9CIlb</a></p>
<p><b>Compressor summary</b>: AttEXplore is a novel attribution method for DNN models that uses decision boundary exploration and frequency manipulation to provide enhanced explainability and outperforms other interpretability methods.</p><hr><h3>Direct Inversion: Boosting Diffusion-based Editing with 3 Lines of Code</h3>
<p><a href='https://openreview.net/forum?id=FoMZ4ljhVw'>https://openreview.net/forum?id=FoMZ4ljhVw</a></p>
<p><b>Compressor summary</b>: The paper introduces "Direct Inversion," a technique that separates source and target diffusion branches for better image editing, and presents PIE-Bench, a benchmark to evaluate it.</p><hr><h3>SOInter: A Novel Deep Energy-Based Interpretation Method for Explaining Structured Output Models</h3>
<p><a href='https://openreview.net/forum?id=Fn655mJ4bv'>https://openreview.net/forum?id=Fn655mJ4bv</a></p>
<p><b>Compressor summary</b>: The paper presents a new way to explain how structured models make decisions by finding important features in each part of the input space, considering output variable correlations, and training an energy-based interpreter function.</p><hr><h3>Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization</h3>
<p><a href='https://openreview.net/forum?id=FlvtjAB0gl'>https://openreview.net/forum?id=FlvtjAB0gl</a></p>
<p><b>Compressor summary</b>: The paper proposes LaVIT, a model that uses a visual tokenizer to translate images into tokens for LLM to understand and generate multi-modal content simultaneously, improving performance on vision-language tasks.</p><hr><h3>DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven Text-to-Image Generation</h3>
<p><a href='https://openreview.net/forum?id=FlhjUkC7vH'>https://openreview.net/forum?id=FlhjUkC7vH</a></p>
<p><b>Compressor summary</b>: DisenBooth is a framework that uses disentangled embeddings to generate images based on text descriptions while preserving the subject identity and controlling the irrelevant information.</p><hr><h3>Incentive-Aware Federated Learning with Training-Time Model Rewards</h3>
<p><a href='https://openreview.net/forum?id=FlY7WQ2hWS'>https://openreview.net/forum?id=FlY7WQ2hWS</a></p>
<p><b>Compressor summary</b>: Our method incentivizes clients in federated learning by providing differentiated training-time rewards, avoiding post-training challenges and ensuring optimal models.</p><hr><h3>Candidate Label Set Pruning: A Data-centric Perspective for Deep Partial-label Learning</h3>
<p><a href='https://openreview.net/forum?id=Fk5IzauJ7F'>https://openreview.net/forum?id=Fk5IzauJ7F</a></p>
<p><b>Compressor summary</b>: The paper proposes a candidate label set pruning method that filters out false labels in a training-free manner, improving deep partial-label learning performance.</p><hr><h3>Let's do the time-warp-attend: Learning topological invariants of dynamical systems</h3>
<p><a href='https://openreview.net/forum?id=Fj7Fzm5lWL'>https://openreview.net/forum?id=Fj7Fzm5lWL</a></p>
<p><b>Compressor summary</b>: The paper proposes a deep-learning framework to classify dynamical regimes and characterize bifurcation boundaries in various systems using topological invariants extracted from data.</p><hr><h3>A Versatile Causal Discovery Framework to Allow Causally-Related Hidden Variables</h3>
<p><a href='https://openreview.net/forum?id=FhQSGhBlqv'>https://openreview.net/forum?id=FhQSGhBlqv</a></p>
<p><b>Compressor summary</b>: The paper presents a new framework for causal discovery that can handle hidden variables in complex networks, using rank information from covariance matrices.</p><hr><h3>Adaptive Causal Balancing for Collaborative Filtering</h3>
<p><a href='https://openreview.net/forum?id=Ffjc8ApSbt'>https://openreview.net/forum?id=Ffjc8ApSbt</a></p>
<p><b>Compressor summary</b>: The paper analyzes how to balance function classes in collaborative filtering to reduce biases, and proposes a universal kernel-based method and an adaptive causal balancing method for this purpose.</p><hr><h3>Does Writing with Language Models Reduce Content Diversity?</h3>
<p><a href='https://openreview.net/forum?id=Feiz5HtCD0'>https://openreview.net/forum?id=Feiz5HtCD0</a></p>
<p><b>Compressor summary</b>: The study finds that using InstructGPT for collaborative writing reduces diversity in essays, while GPT3 does not have this effect.</p><hr><h3>On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters</h3>
<p><a href='https://openreview.net/forum?id=FddFxi08J3'>https://openreview.net/forum?id=FddFxi08J3</a></p>
<p><b>Compressor summary</b>: The paper explores the connection between graph neural networks and the Weisfeiler-Leman test for distinguishing graphs based on their patterns, and provides a method to compute the least dimension needed for this distinction using local information.</p><hr><h3>Alpagasus: Training a Better Alpaca Model with Fewer Data</h3>
<p><a href='https://openreview.net/forum?id=FdVXgSJhvz'>https://openreview.net/forum?id=FdVXgSJhvz</a></p>
<p><b>Compressor summary</b>: The paper introduces Alpagasus, a data selection strategy that uses a large language model to filter out low-quality instances from IFT datasets, resulting in faster training and improved performance on instruction-following tasks.</p><hr><h3>OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning</h3>
<p><a href='https://openreview.net/forum?id=FbuyDzZTPt'>https://openreview.net/forum?id=FbuyDzZTPt</a></p>
<p><b>Compressor summary</b>: The paper proposes a regularization method and a simplified prompt-based approach for class-incremental learning, improving accuracy and reducing computation cost compared to previous rehearsal-free methods.</p><hr><h3>Improving Non-Transferable Representation Learning by Harnessing Content and Style</h3>
<p><a href='https://openreview.net/forum?id=FYKVPOHCpE'>https://openreview.net/forum?id=FYKVPOHCpE</a></p>
<p><b>Compressor summary</b>: H-NTL is a novel method that uses causal modeling to disentangle content and style factors in learning non-transferable representations for better domain adaptation.</p><hr><h3>Neural-Symbolic Recursive Machine for Systematic Generalization</h3>
<p><a href='https://openreview.net/forum?id=FWJAmwE0xH'>https://openreview.net/forum?id=FWJAmwE0xH</a></p>
<p><b>Compressor summary</b>: NSR is a Neural-Symbolic model with a Grounded Symbol System core representation that learns perception, parsing, and reasoning modules and achieves better systematic generalization on various tasks.</p><hr><h3>Multisize Dataset Condensation</h3>
<p><a href='https://openreview.net/forum?id=FVhmnvqnsI'>https://openreview.net/forum?id=FVhmnvqnsI</a></p>
<p><b>Compressor summary</b>: MDC is a method that compresses multiple dataset condensation processes into one with adaptive subset loss to address challenges in on-device scenarios and improve training efficiency.</p><hr><h3>MVDream: Multi-view Diffusion for 3D Generation</h3>
<p><a href='https://openreview.net/forum?id=FUgrjq2pbB'>https://openreview.net/forum?id=FUgrjq2pbB</a></p>
<p><b>Compressor summary</b>: MVDream is a multi-view diffusion model that generates consistent 3D images from text prompts using both 2D and 3D data, improving on existing methods for 3D generation and concept learning.</p><hr><h3>ZeroFlow: Scalable Scene Flow via Distillation</h3>
<p><a href='https://openreview.net/forum?id=FRCHDhbxZF'>https://openreview.net/forum?id=FRCHDhbxZF</a></p>
<p><b>Compressor summary</b>: Scene Flow via Distillation is a method that uses unlabeled data to train a fast feedforward model for scene flow estimation without human labels, achieving state-of-the-art performance and enabling real-time applications.</p><hr><h3>ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate</h3>
<p><a href='https://openreview.net/forum?id=FQepisCUWu'>https://openreview.net/forum?id=FQepisCUWu</a></p>
<p><b>Compressor summary</b>: The paper introduces ChatEval, a multi-agent debating system using large language models to evaluate texts more accurately and effectively than single-agent approaches, and shows that diverse role prompts are crucial for its performance.</p><hr><h3>A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors</h3>
<p><a href='https://openreview.net/forum?id=FOSBQuXgAq'>https://openreview.net/forum?id=FOSBQuXgAq</a></p>
<p><b>Compressor summary</b>: The paper explores the complex posterior distribution of deep Bayesian Neural Networks, studying its approximation, quality, impact on uncertainty, visualization, and weight-space symmetries in various vision tasks and architectures.</p><hr><h3>SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction</h3>
<p><a href='https://openreview.net/forum?id=FNq3nIvP4F'>https://openreview.net/forum?id=FNq3nIvP4F</a></p>
<p><b>Compressor summary</b>: The paper introduces SEINE, a short-to-long video diffusion model that generates coherent and creative transitions between different scenes based on textual descriptions.</p><hr><h3>Gradual Optimization Learning for Conformational Energy Minimization</h3>
<p><a href='https://openreview.net/forum?id=FMMF1a9ifL'>https://openreview.net/forum?id=FMMF1a9ifL</a></p>
<p><b>Compressor summary</b>: GOLF is a new framework that uses neural networks and gradients to optimize molecular conformations faster and more accurately for drug discovery and materials design.</p><hr><h3>Post-hoc bias scoring is optimal for fair classification</h3>
<p><a href='https://openreview.net/forum?id=FM5xfcaR2Y'>https://openreview.net/forum?id=FM5xfcaR2Y</a></p>
<p><b>Compressor summary</b>: The paper proposes a simple modification rule for binary classifiers to satisfy different group fairness constraints, which can be applied to multiple datasets without needing sensitive attribute information during inference.</p><hr><h3>Denoising Diffusion Bridge Models</h3>
<p><a href='https://openreview.net/forum?id=FKksTayvGo'>https://openreview.net/forum?id=FKksTayvGo</a></p>
<p><b>Compressor summary</b>: Denoising Diffusion Bridge Models (DDBMs) are a new type of generative model that uses diffusion bridges, interpolating processes between two distributions, to map data without relying on random noise or cumbersome methods like guidance or projected sampling.</p><hr><h3>Bayesian low-rank adaptation for large language models</h3>
<p><a href='https://openreview.net/forum?id=FJiUyzOF1m'>https://openreview.net/forum?id=FJiUyzOF1m</a></p>
<p><b>Compressor summary</b>: Laplace-LoRA is a Bayesian method that improves calibration and reduces overconfidence in fine-tuned large language models using the Laplace approximation.</p><hr><h3>SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking</h3>
<p><a href='https://openreview.net/forum?id=FJWT0692hw'>https://openreview.net/forum?id=FJWT0692hw</a></p>
<p><b>Compressor summary</b>: SequenceMatch is a method that improves autoregressive modeling by using imitation learning to minimize divergences between generated sequences and the dataset, including out-of-distribution errors.</p><hr><h3>QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=FIplmUWdm3'>https://openreview.net/forum?id=FIplmUWdm3</a></p>
<p><b>Compressor summary</b>: QLLM is an efficient and accurate low-bitwidth post-training quantization method for large language models that uses adaptive channel reassembly and tuning to mitigate outliers and improve performance.</p><hr><h3>CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?</h3>
<p><a href='https://openreview.net/forum?id=FIGXAxr9E4'>https://openreview.net/forum?id=FIGXAxr9E4</a></p>
<p><b>Compressor summary</b>: The text investigates how data-balancing can help reduce biases in CLIP models and discusses its strengths, limitations, and impact on different tasks.</p><hr><h3>Multimarginal Generative Modeling with Stochastic Interpolants</h3>
<p><a href='https://openreview.net/forum?id=FHqAzWl2wE'>https://openreview.net/forum?id=FHqAzWl2wE</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to learn joint distributions from multiple probability densities using dynamical transport of measure, which can extract correspondences among the marginals and has applications in various tasks.</p><hr><h3>Few-shot Hybrid Domain Adaptation of Image Generator</h3>
<p><a href='https://openreview.net/forum?id=FE2e8664Sl'>https://openreview.net/forum?id=FE2e8664Sl</a></p>
<p><b>Compressor summary</b>: The paper introduces Few-shot Hybrid Domain Adaptation (HDA), where a pre-trained generator adapts to multiple target domains with different attributes while preserving the source domain's characteristics using a novel directional subspace loss.</p><hr><h3>Attention-based Iterative Decomposition for Tensor Product Representation</h3>
<p><a href='https://openreview.net/forum?id=FDb2JQZsFH'>https://openreview.net/forum?id=FDb2JQZsFH</a></p>
<p><b>Compressor summary</b>: The paper introduces an AID module that improves the performance of TPR-based models on systematic generalization tasks by enhancing the binding of structured representations to input features using attention.</p><hr><h3>LOQA: Learning with Opponent Q-Learning Awareness</h3>
<p><a href='https://openreview.net/forum?id=FDQF6A1s6M'>https://openreview.net/forum?id=FDQF6A1s6M</a></p>
<p><b>Compressor summary</b>: LOQA is a reinforcement learning algorithm that helps agents optimize their utility while cooperating in partially competitive environments and performs well in benchmark scenarios.</p><hr><h3>Explaining Kernel Clustering via Decision Trees</h3>
<p><a href='https://openreview.net/forum?id=FAGtjl7HOw'>https://openreview.net/forum?id=FAGtjl7HOw</a></p>
<p><b>Compressor summary</b>: The paper explores interpretable kernel clustering using decision trees to approximate nonlinear extensions of k-means, while maintaining flexibility and interpretability in practice.</p><hr><h3>GTMGC: Using Graph Transformer to Predict Molecules Ground-State Conformation</h3>
<p><a href='https://openreview.net/forum?id=F7QnIKlC1N'>https://openreview.net/forum?id=F7QnIKlC1N</a></p>
<p><b>Compressor summary</b>: The paper introduces GTMGC, a novel deep learning network that predicts molecular conformations from their 2D topology using a new self-attention mechanism called MSRSA, and shows it outperforms existing methods on two benchmark datasets.</p><hr><h3>Sparse Autoencoders Find Highly Interpretable Features in Language Models</h3>
<p><a href='https://openreview.net/forum?id=F76bwRSLeK'>https://openreview.net/forum?id=F76bwRSLeK</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to identify interpretable features in neural networks that can help understand their internal workings and causal effects on tasks.</p><hr><h3>Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers</h3>
<p><a href='https://openreview.net/forum?id=F5dhGCdyYh'>https://openreview.net/forum?id=F5dhGCdyYh</a></p>
<p><b>Compressor summary</b>: The paper introduces a new type of adversarial attack on reinforcement learning agents that is hard to detect by both automated methods and humans, aiming to improve robustness against attacks on sensory inputs.</p><hr><h3>Fixed Non-negative Orthogonal Classifier: Inducing Zero-mean Neural Collapse with Feature Dimension Separation</h3>
<p><a href='https://openreview.net/forum?id=F4bmOrmUwc'>https://openreview.net/forum?id=F4bmOrmUwc</a></p>
<p><b>Compressor summary</b>: This paper investigates zero-mean neural collapse for fixed non-negative orthogonal classifiers, which improve performance in classification problems by enhancing softmax masking and tackling mixup limitations.</p><hr><h3>Robotic Task Generalization via Hindsight Trajectory Sketches</h3>
<p><a href='https://openreview.net/forum?id=F1TKzG8LJO'>https://openreview.net/forum?id=F1TKzG8LJO</a></p>
<p><b>Compressor summary</b>: The RT-Trajectory method enables robots to generalize to new tasks by representing them through rough trajectory sketches that balance low-level guidance and situational context, and can be specified through simple human inputs or automated methods.</p><hr><h3>DATS: Difficulty-Aware Task Sampler for Meta-Learning Physics-Informed Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=EvyYFSxdgB'>https://openreview.net/forum?id=EvyYFSxdgB</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel difficulty-aware task sampler (DATS) for meta-learning physics-informed neural networks (PINNs) that can improve the accuracy and reduce disparity across different partial differential equations (PDEs).</p><hr><h3>Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning</h3>
<p><a href='https://openreview.net/forum?id=EvDeiLv7qc'>https://openreview.net/forum?id=EvDeiLv7qc</a></p>
<p><b>Compressor summary</b>: The paper introduces a new Mixture of Experts (MoE) architecture that is extremely parameter-efficient, generalizes to unseen tasks, and performs well with limited resources.</p><hr><h3>Dissecting Neural Network Robustness Proofs</h3>
<p><a href='https://openreview.net/forum?id=Ev10F9TWML'>https://openreview.net/forum?id=Ev10F9TWML</a></p>
<p><b>Compressor summary</b>: The paper presents new methods to make the proofs of deep neural network robustness more understandable, showing that different training methods affect how the proofs use or remove input features.</p><hr><h3>Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control</h3>
<p><a href='https://openreview.net/forum?id=EriR6Ec69a'>https://openreview.net/forum?id=EriR6Ec69a</a></p>
<p><b>Compressor summary</b>: The authors study how to make recurrent neural networks more robust to changes in the environment by adjusting their connectivity, and find that low-rank, sparse connectivity works best for closed-form continuous-time neural networks.</p><hr><h3>Self-supervised Representation Learning from Random Data Projectors</h3>
<p><a href='https://openreview.net/forum?id=EpYnZpDpsQ'>https://openreview.net/forum?id=EpYnZpDpsQ</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-supervised representation learning method that does not rely on data augmentations, but uses random projections for reconstruction. It shows superior performance on various tasks and modalities.</p><hr><h3>Privileged Sensing Scaffolds Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=EpVe8jAjdx'>https://openreview.net/forum?id=EpVe8jAjdx</a></p>
<p><b>Compressor summary</b>: Sensory scaffolding helps novice learners by providing additional observation streams, and _Scaffolder_ is a reinforcement learning approach that uses privileged sensing to improve artificial agents' performance in various simulated robotic tasks.</p><hr><h3>ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving</h3>
<p><a href='https://openreview.net/forum?id=Ep0TtjVoap'>https://openreview.net/forum?id=Ep0TtjVoap</a></p>
<p><b>Compressor summary</b>: ToRA agents use external tools to solve complex math problems, outperforming open-source models and achieving competitive results with GPT-4.</p><hr><h3>An Emulator for Fine-tuning Large Language Models using Small Language Models</h3>
<p><a href='https://openreview.net/forum?id=Eo7kv0sllr'>https://openreview.net/forum?id=Eo7kv0sllr</a></p>
<p><b>Compressor summary</b>: This paper introduces emulated fine-tuning (EFT), a method to test the impact of scaling pre-training and fine-tuning stages in language models, and shows that it improves helpfulness and factuality in various scenarios.</p><hr><h3>Building Cooperative Embodied Agents Modularly with Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=EnXJfQqy0K'>https://openreview.net/forum?id=EnXJfQqy0K</a></p>
<p><b>Compressor summary</b>: The text describes a novel framework, CoELA, that leverages large language models to enhance multi-agent cooperation with natural language communication and commonsense reasoning in embodied environments.</p><hr><h3>Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation</h3>
<p><a href='https://openreview.net/forum?id=EmQSOi1X2f'>https://openreview.net/forum?id=EmQSOi1X2f</a></p>
<p><b>Compressor summary</b>: This paper investigates self-contradictions in large language models, proposes a detection and mitigation framework using prompts, and releases a tool to help users avoid such issues.</p><hr><h3>Leveraging Previous Tasks in Optimizing Risk Measures with Gaussian Processes</h3>
<p><a href='https://openreview.net/forum?id=ElykcDu5YK'>https://openreview.net/forum?id=ElykcDu5YK</a></p>
<p><b>Compressor summary</b>: The paper proposes a new meta-Bayesian optimization algorithm for risk measures that uses previous tasks' results, has scaling and shifting invariance, and robustness to harmful tasks.</p><hr><h3>L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation</h3>
<p><a href='https://openreview.net/forum?id=EhrzQwsV4K'>https://openreview.net/forum?id=EhrzQwsV4K</a></p>
<p><b>Compressor summary</b>: L2MAC is a novel memory-augmented language model that enables long and consistent code generation by managing context and interacting with a file store, outperforming other methods on system design tasks.</p><hr><h3>HoloNets: Spectral Convolutions do extend to Directed Graphs</h3>
<p><a href='https://openreview.net/forum?id=EhmEwfavOW'>https://openreview.net/forum?id=EhmEwfavOW</a></p>
<p><b>Compressor summary</b>: The paper presents a method to extend spectral convolutions to directed graphs, enabling new state of the art results in node classification and stability to topological perturbations.</p><hr><h3>HyperAttention: Long-context Attention in Near-Linear Time</h3>
<p><a href='https://openreview.net/forum?id=Eh0Od2BJIM'>https://openreview.net/forum?id=Eh0Od2BJIM</a></p>
<p><b>Compressor summary</b>: HyperAttention is a new attention mechanism that addresses the computational challenges of large language models by using two parameters to measure the difficulty of the problem and achieving linear time sampling even with unbounded entries or large stable rank, leading to significant speed improvements.</p><hr><h3>Understanding Convergence and Generalization in Federated Learning through Feature Learning Theory</h3>
<p><a href='https://openreview.net/forum?id=EcetCr4trp'>https://openreview.net/forum?id=EcetCr4trp</a></p>
<p><b>Compressor summary</b>: This paper proposes a feature learning theory to understand convergence and generalization properties in Federated Learning, showing how communication helps achieve low test error and addressing data heterogeneity issues with a weighted method.</p><hr><h3>Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching</h3>
<p><a href='https://openreview.net/forum?id=Ebt7JgMHv1'>https://openreview.net/forum?id=Ebt7JgMHv1</a></p>
<p><b>Compressor summary</b>: This paper warns about interpretability illusions from nave subspace interventions in neural models, showing examples and suggesting more careful analysis for valid interpretations.</p><hr><h3>Cameras as Rays: Sparse-view Pose Estimation via Ray Diffusion</h3>
<p><a href='https://openreview.net/forum?id=EanCFCwAjM'>https://openreview.net/forum?id=EanCFCwAjM</a></p>
<p><b>Compressor summary</b>: The paper introduces a novel camera pose estimation method that uses rays, image features, and set-level transformers, and learns from uncertainties with a denoising diffusion model.</p><hr><h3>Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy</h3>
<p><a href='https://openreview.net/forum?id=EXitynZhYn'>https://openreview.net/forum?id=EXitynZhYn</a></p>
<p><b>Compressor summary</b>: The text proposes a novel VQA benchmark using visual classification datasets, follow-up questions, and LLM-based metrics to evaluate text-generative vision-language models more accurately.</p><hr><h3>Invariance-based Learning of Latent Dynamics</h3>
<p><a href='https://openreview.net/forum?id=EWTFMkTdkT'>https://openreview.net/forum?id=EWTFMkTdkT</a></p>
<p><b>Compressor summary</b>: The paper introduces a new model class that efficiently learns complicated dynamics from high-dimensional data by combining variational autoencoders, spatio-temporal attention, and scientifically-motivated invariances.</p><hr><h3>Minimax optimality of convolutional neural networks for infinite dimensional input-output problems and separation from kernel methods</h3>
<p><a href='https://openreview.net/forum?id=EW8ZExRZkJ'>https://openreview.net/forum?id=EW8ZExRZkJ</a></p>
<p><b>Compressor summary</b>: The paper analyzes dilated CNNs' performance in estimating maps between infinite-dimensional input and output spaces and shows they achieve minimax optimal error rates, outperforming linear estimators.</p><hr><h3>Exploring the cloud of feature interaction scores in a Rashomon set</h3>
<p><a href='https://openreview.net/forum?id=EPNEazJoAg'>https://openreview.net/forum?id=EPNEazJoAg</a></p>
<p><b>Compressor summary</b>: The feature interaction score (FIS) measures how much feature interactions vary among equally accurate predictive models, helping to understand their behavior.</p><hr><h3>Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive</h3>
<p><a href='https://openreview.net/forum?id=EJPIzl7mgc'>https://openreview.net/forum?id=EJPIzl7mgc</a></p>
<p><b>Compressor summary</b>: The paper proposes an adversarial training method to improve layout-to-image generation, enabling better alignment and editability.</p><hr><h3>Parametric Augmentation for Time Series Contrastive Learning</h3>
<p><a href='https://openreview.net/forum?id=EIPLdFy3vp'>https://openreview.net/forum?id=EIPLdFy3vp</a></p>
<p><b>Compressor summary</b>: The text discusses contrastive learning for time series data, proposing AutoTCL, an adaptive augmentation method that improves representation learning and outperforms existing approaches on forecasting and classification tasks.</p><hr><h3>ResFields: Residual Neural Fields for Spatiotemporal Signals</h3>
<p><a href='https://openreview.net/forum?id=EHrvRNs2Y0'>https://openreview.net/forum?id=EHrvRNs2Y0</a></p>
<p><b>Compressor summary</b>: ResFields are neural fields with temporal residual layers that can effectively model complex temporal signals and improve performance on various tasks involving 2D video approximation, dynamic shape modeling, and dynamic NeRF reconstruction.</p><hr><h3>AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors</h3>
<p><a href='https://openreview.net/forum?id=EHg5GDnyq1'>https://openreview.net/forum?id=EHg5GDnyq1</a></p>
<p><b>Compressor summary</b>: AgentVerse is a framework that helps AI agents cooperate and perform better on various tasks than individually.</p><hr><h3>Prompt Gradient Projection for Continual Learning</h3>
<p><a href='https://openreview.net/forum?id=EH2O3h7sBI'>https://openreview.net/forum?id=EH2O3h7sBI</a></p>
<p><b>Compressor summary</b>: The paper proposes a new approach (PGP) that combines prompt-tuning and gradient projection for continual learning, which reduces forgetting by selecting relevant prompts and using self-attention and SVD in vision-transformers.</p><hr><h3>Revisiting Data Augmentation in Deep Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=EGQBpkIEuu'>https://openreview.net/forum?id=EGQBpkIEuu</a></p>
<p><b>Compressor summary</b>: The text analyzes existing data augmentation techniques for image-based deep reinforcement learning, compares their effects, and proposes a novel regularization term called tangent prop that improves performance and sample efficiency.</p><hr><h3>Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation</h3>
<p><a href='https://openreview.net/forum?id=EG68RSznLT'>https://openreview.net/forum?id=EG68RSznLT</a></p>
<p><b>Compressor summary</b>: FTB is a method that uses generative models and preference augmentation to improve trajectories in offline PbRL without learning inaccurate rewards from preferences.</p><hr><h3>One-shot Active Learning Based on Lewis Weight Sampling for Multiple Deep Models</h3>
<p><a href='https://openreview.net/forum?id=EDXkkUAIFW'>https://openreview.net/forum?id=EDXkkUAIFW</a></p>
<p><b>Compressor summary</b>: The paper proposes a one-shot active learning method for training multiple models using different representations without repeated model training, which samples and reweights instances based on their maximum Lewis weights.</p><hr><h3>Vision-by-Language for Training-Free Compositional Image Retrieval</h3>
<p><a href='https://openreview.net/forum?id=EDPxCjXzSb'>https://openreview.net/forum?id=EDPxCjXzSb</a></p>
<p><b>Compressor summary</b>: CIReVL is a training-free method for Compositional Image Retrieval using large vision and language models to caption images and apply textual modifications, achieving competitive performance and human-understandability.</p><hr><h3>VBH-GNN: Variational Bayesian Heterogeneous Graph Neural Networks for Cross-subject Emotion Recognition</h3>
<p><a href='https://openreview.net/forum?id=EArTDUmILF'>https://openreview.net/forum?id=EArTDUmILF</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method using multi-modal physiological signals and a Variational Bayesian Heterogeneous Graph Neural Network with Relationship Distribution Adaptation to improve cross-subject emotion recognition accuracy without aligning signal features.</p><hr><h3>CAS: A Probability-Based Approach for Universal Condition Alignment Score</h3>
<p><a href='https://openreview.net/forum?id=E78OaH2s3f'>https://openreview.net/forum?id=E78OaH2s3f</a></p>
<p><b>Compressor summary</b>: The paper introduces a universal condition alignment score for diverse conditional generation tasks that works across all conditions and requires no additional models.</p><hr><h3>Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions</h3>
<p><a href='https://openreview.net/forum?id=E60SIDItyT'>https://openreview.net/forum?id=E60SIDItyT</a></p>
<p><b>Compressor summary</b>: The paper compares two loss functions for learning from aggregate data, introduces a new interpolating estimator, and analyzes the effect of bag size on risk-privacy trade-offs in linear regression.</p><hr><h3>Parallelizing non-linear sequential models over the sequence length</h3>
<p><a href='https://openreview.net/forum?id=E34AlVLN0v'>https://openreview.net/forum?id=E34AlVLN0v</a></p>
<p><b>Compressor summary</b>: The paper presents a parallel algorithm that greatly speeds up GPU evaluation and training of sequential models without sacrificing accuracy, enabling their use in long sequence problems.</p><hr><h3>Enhancing Group Fairness in Online Settings Using Oblique Decision Forests</h3>
<p><a href='https://openreview.net/forum?id=E1NxN5QMOE'>https://openreview.net/forum?id=E1NxN5QMOE</a></p>
<p><b>Compressor summary</b>: Aranyani is an ensemble of oblique decision trees for making fair decisions in online settings by using aggregate statistics of previous decisions, reducing storage and computation requirements.</p><hr><h3>Adversarial Imitation Learning via Boosting</h3>
<p><a href='https://openreview.net/forum?id=DuQkqSe9en'>https://openreview.net/forum?id=DuQkqSe9en</a></p>
<p><b>Compressor summary</b>: AILBoost is a novel and principled off-policy adversarial imitation learning algorithm that uses an ensemble of weighted weak learners and a discriminator to improve sample efficiency and scalability in various environments.</p><hr><h3>Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems</h3>
<p><a href='https://openreview.net/forum?id=DsEhqQtfAG'>https://openreview.net/forum?id=DsEhqQtfAG</a></p>
<p><b>Compressor summary</b>: The paper proposes a diffusion sampling strategy that combines Krylov subspace methods with modern diffusion models for efficient and high-quality solutions to inverse problems, such as MRI and CT reconstruction.</p><hr><h3>Single Motion Diffusion</h3>
<p><a href='https://openreview.net/forum?id=DrhZneqz4n'>https://openreview.net/forum?id=DrhZneqz4n</a></p>
<p><b>Compressor summary</b>: SinMDM is a single motion diffusion model that learns from a single input motion and synthesizes various motions of different lengths while maintaining the original motion's essence, applicable to multiple contexts like spatial and temporal in-betweening, motion expansion, style transfer, and crowd animation.</p><hr><h3>Point2SSM: Learning Morphological Variations of Anatomies from Point Clouds</h3>
<p><a href='https://openreview.net/forum?id=DqziS8DG4M'>https://openreview.net/forum?id=DqziS8DG4M</a></p>
<p><b>Compressor summary</b>: Point2SSM is a novel unsupervised learning approach to construct statistical shape models of anatomy from noisy point clouds using attention-based correspondence mappings.</p><hr><h3>Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework</h3>
<p><a href='https://openreview.net/forum?id=DqD59dQP37'>https://openreview.net/forum?id=DqD59dQP37</a></p>
<p><b>Compressor summary</b>: This paper analyzes how unobserved confounding affects causal fairness in machine learning predictions and proposes a neural framework to ensure fairness even under observed confounding.</p><hr><h3>Group Preference Optimization: Few-Shot Alignment of Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=DpFeMH4l8Q'>https://openreview.net/forum?id=DpFeMH4l8Q</a></p>
<p><b>Compressor summary</b>: GPO is a framework to align large language models with the preferences of different groups using few-shot learning and meta-learning.</p><hr><h3>DeepSPF: Spherical SO(3)-Equivariant Patches for Scan-to-CAD Estimation</h3>
<p><a href='https://openreview.net/forum?id=Dnc3paMqDE'>https://openreview.net/forum?id=Dnc3paMqDE</a></p>
<p><b>Compressor summary</b>: The paper introduces Spherical Patch Fields, a technique for SO(3)-equivariant 3D point clouds, and presents DeepSPF, which improves Scan-to-CAD performance in registration, retrieval, and completion tasks.</p><hr><h3>BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction</h3>
<p><a href='https://openreview.net/forum?id=DmD1wboID9'>https://openreview.net/forum?id=DmD1wboID9</a></p>
<p><b>Compressor summary</b>: BayesPrompt is a novel approach that learns prompts with domain discriminative information to improve few-shot learning using pre-trained language models.</p><hr><h3>CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction</h3>
<p><a href='https://openreview.net/forum?id=DjzvJCRsVf'>https://openreview.net/forum?id=DjzvJCRsVf</a></p>
<p><b>Compressor summary</b>: CLIPSelf is a method that improves CLIP ViTs for local image region tasks like object detection and segmentation by aligning their representations without needing extra data.</p><hr><h3>Simplicial Representation Learning with Neural $k$-Forms</h3>
<p><a href='https://openreview.net/forum?id=Djw0XhjHZb'>https://openreview.net/forum?id=Djw0XhjHZb</a></p>
<p><b>Compressor summary</b>: The paper proposes a new geometric deep learning method using differential forms to represent simplices and embed them in $\mathbb{R}^n$, achieving universal approximation, interpretability, and efficiency without message passing.</p><hr><h3>Robustifying State-space Models for Long Sequences via Approximate Diagonalization</h3>
<p><a href='https://openreview.net/forum?id=DjeQ39QoLQ'>https://openreview.net/forum?id=DjeQ39QoLQ</a></p>
<p><b>Compressor summary</b>: The paper proposes a perturb-then-diagonalize method for initializing state-space models, improving their robustness and performance compared to previous diagonal-based approaches.</p><hr><h3>Scalable Monotonic Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=DjIsNDEOYX'>https://openreview.net/forum?id=DjIsNDEOYX</a></p>
<p><b>Compressor summary</b>: The research proposes SMNN, a partially connected network with a scalable monotonic hidden layer, which preserves monotonicity and is easy to implement and train.</p><hr><h3>Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform</h3>
<p><a href='https://openreview.net/forum?id=Diq6urt3lS'>https://openreview.net/forum?id=Diq6urt3lS</a></p>
<p><b>Compressor summary</b>: Cleanba is a new platform for distributed reinforcement learning that improves reproducibility and efficiency over existing methods.</p><hr><h3>MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation</h3>
<p><a href='https://openreview.net/forum?id=DiWRG9JTWZ'>https://openreview.net/forum?id=DiWRG9JTWZ</a></p>
<p><b>Compressor summary</b>: The text discusses OOD problems in few-shot classification, focusing on two types (CD-FSC and SC-FSC), introduces a new benchmark (MetaCoCo) for SC-FSC, and evaluates existing methods on it.</p><hr><h3>Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks</h3>
<p><a href='https://openreview.net/forum?id=DfPtC8uSot'>https://openreview.net/forum?id=DfPtC8uSot</a></p>
<p><b>Compressor summary</b>: The paper defines and analyzes expected robustness in GNNs, proposes a new robust variant called GCORNs, and evaluates it on real-world datasets.</p><hr><h3>The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing</h3>
<p><a href='https://openreview.net/forum?id=DesYwmUG00'>https://openreview.net/forum?id=DesYwmUG00</a></p>
<p><b>Compressor summary</b>: The text presents a new probabilistic method for image editing using stochastic differential equations (SDEs) that improves upon existing ordinary differential equation (ODE) methods and shows promising results in various tasks.</p><hr><h3>Improving Out-of-Domain Generalization with Domain Relations</h3>
<p><a href='https://openreview.net/forum?id=Dc4rXq3HIA'>https://openreview.net/forum?id=Dc4rXq3HIA</a></p>
<p><b>Compressor summary</b>: The paper proposes DG, a new approach to handle domain shifts in machine learning, which learns domain-specific models and reweights them based on domain metadata for better out-of-domain generalization.</p><hr><h3>Constrained Decoding for Cross-lingual Label Projection</h3>
<p><a href='https://openreview.net/forum?id=DayPQKXaQk'>https://openreview.net/forum?id=DayPQKXaQk</a></p>
<p><b>Compressor summary</b>: The authors propose a constrained decoding method for label projection in zero-shot cross-lingual transfer, which improves translation quality and can be applied to both training and test data strategies.</p><hr><h3>Graphpulse: Topological representations for temporal graph property prediction</h3>
<p><a href='https://openreview.net/forum?id=DZqic2sPTY'>https://openreview.net/forum?id=DZqic2sPTY</a></p>
<p><b>Compressor summary</b>: GraphPulse is a framework that combines Mapper and RNN techniques to analyze temporal graphs and improve prediction accuracy.</p><hr><h3>HypeBoy: Generative Self-Supervised Representation Learning on Hypergraphs</h3>
<p><a href='https://openreview.net/forum?id=DZUzOKE6og'>https://openreview.net/forum?id=DZUzOKE6og</a></p>
<p><b>Compressor summary</b>: The paper introduces hyperedge filling, a novel generative self-supervised learning task for hypergraphs, and HYPEBOY, a hypergraph neural network method that learns effective hypergraph representations using this task.</p><hr><h3>The LLM Surgeon</h3>
<p><a href='https://openreview.net/forum?id=DYIIRgwg2i'>https://openreview.net/forum?id=DYIIRgwg2i</a></p>
<p><b>Compressor summary</b>: The paper presents a data-driven compression method for large Transformer models that can reduce their size by 20%-30% with minimal performance loss and achieve state-of-the-art results in different types of pruning.</p><hr><h3>FedTrans: Client-Transparent Utility Estimation for Robust Federated Learning</h3>
<p><a href='https://openreview.net/forum?id=DRu8PMHgCh'>https://openreview.net/forum?id=DRu8PMHgCh</a></p>
<p><b>Compressor summary</b>: FedTrans is a method to improve federated learning by selecting clients based on their noise level and utility, using Bayesian inference with auxiliary data.</p><hr><h3>PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=DO2WFXU1Be'>https://openreview.net/forum?id=DO2WFXU1Be</a></p>
<p><b>Compressor summary</b>: PINNsFormer is a new framework that uses Transformers to improve the accuracy of numerical solutions to PDEs by capturing temporal dependencies and incorporating a wavelet activation function.</p><hr><h3>ReLoRA: High-Rank Training Through Low-Rank Updates</h3>
<p><a href='https://openreview.net/forum?id=DLJznSp6X3'>https://openreview.net/forum?id=DLJznSp6X3</a></p>
<p><b>Compressor summary</b>: ReLoRA is a new method that trains large neural networks more efficiently, saving memory and speeding up training.</p><hr><h3>Prediction Error-based Classification for Class-Incremental Learning</h3>
<p><a href='https://openreview.net/forum?id=DJZDgMOLXQ'>https://openreview.net/forum?id=DJZDgMOLXQ</a></p>
<p><b>Compressor summary</b>: PEC is a novel approach for class-incremental learning that uses prediction error to estimate class scores, reducing forgetting and improving performance compared to existing methods.</p><hr><h3>A Plug-and-Play Image Registration Network</h3>
<p><a href='https://openreview.net/forum?id=DGez4B2a6Y'>https://openreview.net/forum?id=DGez4B2a6Y</a></p>
<p><b>Compressor summary</b>: PIRATE is a novel deep learning method for biomedical image registration that uses a pre-trained CNN denoiser and an explicit data-fidelity penalty to improve registration accuracy.</p><hr><h3>Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies</h3>
<p><a href='https://openreview.net/forum?id=DFTHW0MyiW'>https://openreview.net/forum?id=DFTHW0MyiW</a></p>
<p><b>Compressor summary</b>: The authors propose a method to improve reinforcement learning policies' robustness against state-adversarial attacks by refining the policy class before test time and using an adversarial bandit subroutine.</p><hr><h3>On the Reliability of Watermarks for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=DEJIDCmWOz'>https://openreview.net/forum?id=DEJIDCmWOz</a></p>
<p><b>Compressor summary</b>: The study evaluates the reliability of watermarking as a way to identify machine-generated text in different scenarios, finding that watermarks remain detectable even after human and machine paraphrasing.</p><hr><h3>DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior</h3>
<p><a href='https://openreview.net/forum?id=DDX1u29Gqr'>https://openreview.net/forum?id=DDX1u29Gqr</a></p>
<p><b>Compressor summary</b>: DreamCraft3D is a method that creates high-quality 3D objects by using a 2D reference image and a hierarchical process involving geometry sculpting and texture boosting guided by view-dependent diffusion models.</p><hr><h3>Boosting the Adversarial Robustness of Graph Neural Networks: An OOD Perspective</h3>
<p><a href='https://openreview.net/forum?id=DCDT918ZkI'>https://openreview.net/forum?id=DCDT918ZkI</a></p>
<p><b>Compressor summary</b>: The paper proposes a new adversarial training paradigm for Graph Neural Networks that uses out-of-distribution detection to enhance robustness without relying on prior knowledge, but shows a trade-off between graph attack efficacy and defensibility.</p><hr><h3>Plugin estimators for selective classification with out-of-distribution detection</h3>
<p><a href='https://openreview.net/forum?id=DASh78rJ7g'>https://openreview.net/forum?id=DASh78rJ7g</a></p>
<p><b>Compressor summary</b>: The paper proposes new plugin estimators for selective classification with out-of-distribution detection, which are theoretically grounded and improve over existing heuristic methods.</p><hr><h3>SpaCE: The Spatial Confounding Environment</h3>
<p><a href='https://openreview.net/forum?id=D9rJdtmIG6'>https://openreview.net/forum?id=D9rJdtmIG6</a></p>
<p><b>Compressor summary</b>: SpaCE is a toolkit that helps scientists evaluate causal inference methods for dealing with spatial confounding in studies involving spatial data.</p><hr><h3>Model Merging by Uncertainty-Based Gradient Matching</h3>
<p><a href='https://openreview.net/forum?id=D7KJmfEDQP'>https://openreview.net/forum?id=D7KJmfEDQP</a></p>
<p><b>Compressor summary</b>: The paper proposes a new uncertainty-based method to improve weighted-averaging of merged models by addressing gradient mismatches and outperforms other schemes on language and vision tasks.</p><hr><h3>Finite-State Autoregressive Entropy Coding for Efficient Learned Lossless Compression</h3>
<p><a href='https://openreview.net/forum?id=D5mJSNtUtv'>https://openreview.net/forum?id=D5mJSNtUtv</a></p>
<p><b>Compressor summary</b>: The paper proposes a new system for improving learned lossless data compression by using an efficient entropy coder and a discrete latent space optimization scheme, achieving better compression ratios with minimal extra computational time.</p><hr><h3>Optimistic Bayesian Optimization with Unknown Constraints</h3>
<p><a href='https://openreview.net/forum?id=D4NJFfrqoq'>https://openreview.net/forum?id=D4NJFfrqoq</a></p>
<p><b>Compressor summary</b>: The paper proposes a new algorithm for constrained Bayesian optimization that balances exploration and exploitation in the decoupled setting, with a theoretical performance guarantee and connections to active learning.</p><hr><h3>Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=D2eOVqPX9g'>https://openreview.net/forum?id=D2eOVqPX9g</a></p>
<p><b>Compressor summary</b>: FedSARSA is a novel on-policy FRL scheme that converges to near-optimal policies and achieves linear speedup with agent collaboration.</p><hr><h3>Idempotence and Perceptual Image Compression</h3>
<p><a href='https://openreview.net/forum?id=Cy5v64DqEF'>https://openreview.net/forum?id=Cy5v64DqEF</a></p>
<p><b>Compressor summary</b>: Idempotence in image codecs is related to perceptual compression, and a new method based on unconditional generative models with idempotence constraints improves image quality without training new models.</p><hr><h3>On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation</h3>
<p><a href='https://openreview.net/forum?id=CvYBvgEUK9'>https://openreview.net/forum?id=CvYBvgEUK9</a></p>
<p><b>Compressor summary</b>: The paper studies first-order algorithms for solving Bilevel Optimization problems with smooth but possibly nonconvex objectives, using penalty methods to connect the objective functions and hyper-objective, and proposes efficient algorithms with provable convergence guarantees.</p><hr><h3>LLCP: Learning Latent Causal Processes for Reasoning-based Video Question Answer</h3>
<p><a href='https://openreview.net/forum?id=Cu5wJa5LGO'>https://openreview.net/forum?id=Cu5wJa5LGO</a></p>
<p><b>Compressor summary</b>: LLCP is a causal framework for video reasoning that uses self-supervised local auto-regression to answer accident attribution and counterfactual prediction questions without needing annotated data.</p><hr><h3>Effective pruning of web-scale datasets based on complexity of concept clusters</h3>
<p><a href='https://openreview.net/forum?id=CtOA9aN8fr'>https://openreview.net/forum?id=CtOA9aN8fr</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to prune large-scale multimodal datasets for training CLIP-style models, improving data and training efficiency while achieving better results on ImageNet zero-shot accuracy and DataComp Medium benchmark.</p><hr><h3>Efficient Multi-agent Reinforcement Learning by Planning</h3>
<p><a href='https://openreview.net/forum?id=CpnKq3UJwp'>https://openreview.net/forum?id=CpnKq3UJwp</a></p>
<p><b>Compressor summary</b>: MAZero is a model-based multi-agent reinforcement learning algorithm with improved efficiency, which combines a centralized model with Monte Carlo Tree Search and uses novel techniques for action spaces and policy optimization.</p><hr><h3>Translating Labels to Solve Annotation Mismatches Across Object Detection Datasets</h3>
<p><a href='https://openreview.net/forum?id=ChHx5ORqF0'>https://openreview.net/forum?id=ChHx5ORqF0</a></p>
<p><b>Compressor summary</b>: The paper introduces a method called Label-Guided Pseudo-Labeling (LGPL) to translate bounding boxes and class labels across different object detection datasets, improving downstream detection performance.</p><hr><h3>Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants</h3>
<p><a href='https://openreview.net/forum?id=Ch7WqGcGmb'>https://openreview.net/forum?id=Ch7WqGcGmb</a></p>
<p><b>Compressor summary</b>: This paper explores a modern error feedback method called EF21, which has better theoretical guarantees and communication complexity than previous methods, and shows its effectiveness in practice with experiments.</p><hr><h3>Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression</h3>
<p><a href='https://openreview.net/forum?id=CgPs04l9TO'>https://openreview.net/forum?id=CgPs04l9TO</a></p>
<p><b>Compressor summary</b>: This paper investigates oscillations in behavior cloning with neural networks caused by minibatch SGD updates, which lead to large errors over long horizons, and shows that exponential moving average (EMA) can effectively reduce this issue.</p><hr><h3>WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions</h3>
<p><a href='https://openreview.net/forum?id=CfXh93NDgH'>https://openreview.net/forum?id=CfXh93NDgH</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to create complex instruction data for training large language models using an evolved set of instructions, which improves their performance compared to baselines.</p><hr><h3>PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images</h3>
<p><a href='https://openreview.net/forum?id=Cf4FJGmHRQ'>https://openreview.net/forum?id=Cf4FJGmHRQ</a></p>
<p><b>Compressor summary</b>: The text proposes a new neural network model, PAC-FNO, that can handle varying image resolutions and natural variations using the frequency domain approach and improve image recognition performance.</p><hr><h3>Generative Learning for Financial Time Series with Irregular and Scale-Invariant Patterns</h3>
<p><a href='https://openreview.net/forum?id=CdjnzWsQax'>https://openreview.net/forum?id=CdjnzWsQax</a></p>
<p><b>Compressor summary</b>: The authors propose FTS-Diffusion, a framework that generates realistic financial time series by extracting and synthesizing recurring patterns and modeling their temporal transitions, improving prediction accuracy.</p><hr><h3>Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning</h3>
<p><a href='https://openreview.net/forum?id=Cc0qk6r4Nd'>https://openreview.net/forum?id=Cc0qk6r4Nd</a></p>
<p><b>Compressor summary</b>: The paper proposes InCo Aggregation, a method that uses internal cross-layer gradients to improve model performance in federated learning with system heterogeneity.</p><hr><h3>Boosting Graph Anomaly Detection with Adaptive Message Passing</h3>
<p><a href='https://openreview.net/forum?id=CanomFZssu'>https://openreview.net/forum?id=CanomFZssu</a></p>
<p><b>Compressor summary</b>: GADAM is a novel framework for unsupervised graph anomaly detection that combines efficient MLP-based local inconsistency mining with hybrid attention based adaptive message passing to capture global anomaly signals, achieving superior performance and efficiency on nine benchmark datasets.</p><hr><h3>MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process</h3>
<p><a href='https://openreview.net/forum?id=CZiY6OLktd'>https://openreview.net/forum?id=CZiY6OLktd</a></p>
<p><b>Compressor summary</b>: The MG-TSD model improves time series forecasting by using multiple granularity levels within the data to guide the learning process of diffusion models, without relying on external data.</p><hr><h3>FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets</h3>
<p><a href='https://openreview.net/forum?id=CYmF38ysDa'>https://openreview.net/forum?id=CYmF38ysDa</a></p>
<p><b>Compressor summary</b>: FLASK is a fine-grained evaluation protocol for assessing large language models based on skill sets, which improves interpretability and reliability of model performance.</p><hr><h3>Fast Updating of Truncated SVD for Representation Learning in Sparse Matrix</h3>
<p><a href='https://openreview.net/forum?id=CX2RgsS29V'>https://openreview.net/forum?id=CX2RgsS29V</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for efficiently updating truncated SVD of large, sparse, and evolving data matrices by exploiting sparsity and using an extended decomposition.</p><hr><h3>Long-range Neural Atom Learning for Molecular Graphs</h3>
<p><a href='https://openreview.net/forum?id=CUfSCwcgqm'>https://openreview.net/forum?id=CUfSCwcgqm</a></p>
<p><b>Compressor summary</b>: The paper proposes Neural Atoms, a method that abstracts atomic groups in molecules and improves the ability of Graph Neural Networks (GNNs) to capture long-range interactions for drug discovery.</p><hr><h3>What Matters to You? Towards Visual Representation Alignment for Robot Learning</h3>
<p><a href='https://openreview.net/forum?id=CTlUHIKF71'>https://openreview.net/forum?id=CTlUHIKF71</a></p>
<p><b>Compressor summary</b>: RAPL is a new method that helps robots learn visual rewards that match human preferences by using feedback and optimal transport, resulting in better and more efficient robot behaviors.</p><hr><h3>AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval</h3>
<p><a href='https://openreview.net/forum?id=COYDmKkQH4'>https://openreview.net/forum?id=COYDmKkQH4</a></p>
<p><b>Compressor summary</b>: AutoCast++ is a system that uses news articles to predict real-world events and improves forecasting accuracy by ranking and summarizing relevant passages.</p><hr><h3>Synergistic Patch Pruning for Vision Transformer: Unifying Intra- & Inter-Layer Patch Importance</h3>
<p><a href='https://openreview.net/forum?id=COO51g41Q4'>https://openreview.net/forum?id=COO51g41Q4</a></p>
<p><b>Compressor summary</b>: The paper proposes STAR, a pruning method for Vision Transformers that combines intra-layer and inter-layer patch importance scoring to reduce computational costs without much performance degradation.</p><hr><h3>Test-Time Training on Nearest Neighbors for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=CNL2bku4ra'>https://openreview.net/forum?id=CNL2bku4ra</a></p>
<p><b>Compressor summary</b>: The authors propose a method to fine-tune language models on query neighbors retrieved from a text embedding index at test time, improving performance across various language modeling tasks.</p><hr><h3>Early Stopping Against Label Noise Without Validation Data</h3>
<p><a href='https://openreview.net/forum?id=CMzF2aOfqp'>https://openreview.net/forum?id=CMzF2aOfqp</a></p>
<p><b>Compressor summary</b>: Label Wave is a novel early stopping method that works without validation data by tracking model predictions on training data and halting training before mislabeled data is excessively fitted.</p><hr><h3>Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words</h3>
<p><a href='https://openreview.net/forum?id=CK5Hfb5hBG'>https://openreview.net/forum?id=CK5Hfb5hBG</a></p>
<p><b>Compressor summary</b>: The paper proposes ChannelViT, a modified ViT architecture for multi-channel images, which improves reasoning across input channels and uses Hierarchical Channel Sampling as a regularization technique for robustness.</p><hr><h3>Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise</h3>
<p><a href='https://openreview.net/forum?id=CIqjp9yTDq'>https://openreview.net/forum?id=CIqjp9yTDq</a></p>
<p><b>Compressor summary</b>: This paper provides a rigorous theoretical analysis of heavy-ball momentum with decaying learning rates for quadratic regression problems under anisotropic gradient noise, showing accelerated convergence and usefulness in large-batch settings.</p><hr><h3>Online Stabilization of Spiking Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=CIj1CVbkpr'>https://openreview.net/forum?id=CIj1CVbkpr</a></p>
<p><b>Compressor summary</b>: The paper proposes Online Spiking Renormalization (OSR) and Online Threshold Stabilizer (OTS) to improve online training of spiking neural networks, achieving better performance with less memory consumption.</p><hr><h3>Energy-based Automated Model Evaluation</h3>
<p><a href='https://openreview.net/forum?id=CHGcP6lVWd'>https://openreview.net/forum?id=CHGcP6lVWd</a></p>
<p><b>Compressor summary</b>: The paper introduces Meta-Distribution Energy (MDE), a novel measure for automated model evaluation that improves efficiency and effectiveness by using energy-based learning and providing better predictions without ground-truth labels.</p><hr><h3>SEAL: A Framework for Systematic Evaluation of Real-World Super-Resolution</h3>
<p><a href='https://openreview.net/forum?id=CGlczSBBSj'>https://openreview.net/forum?id=CGlczSBBSj</a></p>
<p><b>Compressor summary</b>: SEAL is a framework for systematically evaluating super-resolution methods on diverse real-world images using clustered degradation cases and coarse-to-fine metrics.</p><hr><h3>The Generative AI Paradox: What It Can Create, It May Not Understand</h3>
<p><a href='https://openreview.net/forum?id=CF8H8MS5P8'>https://openreview.net/forum?id=CF8H8MS5P8</a></p>
<p><b>Compressor summary</b>: The text discusses a paradox in generative AI, where models can produce outputs beyond human expertise but still make basic errors, suggesting that their generation capability is not based on understanding like humans.</p><hr><h3>Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space</h3>
<p><a href='https://openreview.net/forum?id=CEkIyshNbC'>https://openreview.net/forum?id=CEkIyshNbC</a></p>
<p><b>Compressor summary</b>: Double descent occurs when imperfect deep learning models learn noisy data and then use over-parameterization to separate signal from noise, but not in well-regularized models.</p><hr><h3>Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations</h3>
<p><a href='https://openreview.net/forum?id=CAqdG2dy5s'>https://openreview.net/forum?id=CAqdG2dy5s</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel graph deep learning framework called Graph-graph Network (GgNet) that can infer signals at unmonitored locations by exploiting correlations between variables and locations using spatio-temporal measurements from physical sensors.</p><hr><h3>InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning</h3>
<p><a href='https://openreview.net/forum?id=C61sk5LsK6'>https://openreview.net/forum?id=C61sk5LsK6</a></p>
<p><b>Compressor summary</b>: InfoBatch is a framework that prunes less informative samples from the data and rescales gradients to achieve lossless training acceleration without gradient bias.</p><hr><h3>Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX</h3>
<p><a href='https://openreview.net/forum?id=C4CxQmp9wc'>https://openreview.net/forum?id=C4CxQmp9wc</a></p>
<p><b>Compressor summary</b>: Jumanji is a fast, flexible, and scalable open-source RL environment suite designed for combinatorial problems and general decision-making tasks, with customizable initial states and problem complexities.</p><hr><h3>Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling</h3>
<p><a href='https://openreview.net/forum?id=C4BikKsgmK'>https://openreview.net/forum?id=C4BikKsgmK</a></p>
<p><b>Compressor summary</b>: Str2Str is a new method to sample protein conformations using structure-to-structure translation, which is fast and does not need simulation data for training or inference.</p><hr><h3>The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing</h3>
<p><a href='https://openreview.net/forum?id=C36v8541Ns'>https://openreview.net/forum?id=C36v8541Ns</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve the robustness of deep neural networks using randomized smoothing and shows significant improvement in certified accuracy compared to existing methods.</p><hr><h3>Mitigating the Curse of Dimensionality for Certified Robustness via Dual Randomized Smoothing</h3>
<p><a href='https://openreview.net/forum?id=C1sQBG6Sqp'>https://openreview.net/forum?id=C1sQBG6Sqp</a></p>
<p><b>Compressor summary</b>: Dual Randomized Smoothing (DRS) improves the robustness of image classifiers by down-sampling and smoothing images in lower dimensions, overcoming the curse of dimensionality issue in Randomized Smoothing.</p><hr><h3>PARL: A Unified Framework for Policy Alignment in Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=ByR3NdDSZB'>https://openreview.net/forum?id=ByR3NdDSZB</a></p>
<p><b>Compressor summary</b>: The text proposes a new framework, PARL, that addresses policy alignment issues in reinforcement learning using preference-based feedback, leading to significant improvements in sample complexity.</p><hr><h3>Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data</h3>
<p><a href='https://openreview.net/forum?id=BxHgpC6FNv'>https://openreview.net/forum?id=BxHgpC6FNv</a></p>
<p><b>Compressor summary</b>: The paper shows that two-layer ReLU networks trained by gradient descent on noisy XOR data can overfit to training labels and then suddenly generalize well, demonstrating benign overfitting and the grokking phenomenon.</p><hr><h3>Decoupled Marked Temporal Point Process using Neural Ordinary Differential Equations</h3>
<p><a href='https://openreview.net/forum?id=BuFNoKBiMs'>https://openreview.net/forum?id=BuFNoKBiMs</a></p>
<p><b>Compressor summary</b>: The Decoupled MTPP framework uses Neural ODEs to learn continuous dynamics of event influences and disentangle inter-event dependencies in complex temporal data.</p><hr><h3>Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images</h3>
<p><a href='https://openreview.net/forum?id=BteuUysuXX'>https://openreview.net/forum?id=BteuUysuXX</a></p>
<p><b>Compressor summary</b>: The paper proposes verbose images that manipulate VLMs to generate long and inefficient sentences by using three losses and a temporal weight algorithm.</p><hr><h3>Causal-StoNet: Causal Inference for High-Dimensional Complex Data</h3>
<p><a href='https://openreview.net/forum?id=BtZ7vCt5QY'>https://openreview.net/forum?id=BtZ7vCt5QY</a></p>
<p><b>Compressor summary</b>: The paper introduces a new stochastic deep learning method for causal inference with complex high-dimensional data that uses sparse deep learning theory and stochastic neural networks to overcome challenges of high dimensionality, unknown data generation process, and missing values.</p><hr><h3>Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution</h3>
<p><a href='https://openreview.net/forum?id=BtT6o5tfHu'>https://openreview.net/forum?id=BtT6o5tfHu</a></p>
<p><b>Compressor summary</b>: The paper proposes a new sampling method for diffusion-based image super-resolution models that leverages and optimizes the randomness of the reverse process, improving the quality of results with fewer steps.</p><hr><h3>Quantifying Interactions in Semi-supervised Multimodal Learning: Guarantees and Applications</h3>
<p><a href='https://openreview.net/forum?id=BrjLHbqiYs'>https://openreview.net/forum?id=BrjLHbqiYs</a></p>
<p><b>Compressor summary</b>: The paper proposes methods to quantify multimodal interactions in semi-supervised learning using information theory and shows their usefulness for task performance and data selection.</p><hr><h3>LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation</h3>
<p><a href='https://openreview.net/forum?id=BqHaLnans2'>https://openreview.net/forum?id=BqHaLnans2</a></p>
<p><b>Compressor summary</b>: This paper proposes a method to train an LLM for vision-language capabilities on medical images by instructing it to answer questions about image inputs and generate both text and image responses.</p><hr><h3>Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain</h3>
<p><a href='https://openreview.net/forum?id=BqEvdOS1Hs'>https://openreview.net/forum?id=BqEvdOS1Hs</a></p>
<p><b>Compressor summary</b>: The paper proposes a "human-centered" approach for improving game AI agents' collaboration with humans by enhancing their goal achievement, and evaluates it in a MOBA game.</p><hr><h3>Stochastic Modified Equations and Dynamics of Dropout Algorithm</h3>
<p><a href='https://openreview.net/forum?id=Bpkhu2ExxU'>https://openreview.net/forum?id=Bpkhu2ExxU</a></p>
<p><b>Compressor summary</b>: The paper explores dropout's mechanism in neural networks training by deriving its stochastic equations theoretically and analyzing its impact on loss landscape through empirical studies.</p><hr><h3>Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature</h3>
<p><a href='https://openreview.net/forum?id=Bpcgcr8E8Z'>https://openreview.net/forum?id=Bpcgcr8E8Z</a></p>
<p><b>Compressor summary</b>: The paper introduces Fast-DetectGPT, an efficient zero-shot detector that distinguishes between human and machine-generated text using conditional probability curvature, which improves on DetectGPT's performance and speed.</p><hr><h3>Out-Of-Domain Unlabeled Data Improves Generalization</h3>
<p><a href='https://openreview.net/forum?id=Bo6GpQ3B9a'>https://openreview.net/forum?id=Bo6GpQ3B9a</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to use unlabeled data in semi-supervised classification, combining robust optimization with self-supervision, and shows improved generalization error bounds in simulations and real data.</p><hr><h3>Complete and Efficient Graph Transformers for Crystal Material Property Prediction</h3>
<p><a href='https://openreview.net/forum?id=BnQY9XiRAS'>https://openreview.net/forum?id=BnQY9XiRAS</a></p>
<p><b>Compressor summary</b>: This paper presents a new method to represent crystals as graphs using their periodic patterns and introduces ComFormer, a transformer model for crystalline materials with two variants that outperform existing methods.</p><hr><h3>Continual Momentum Filtering on Parameter Space for Online Test-time Adaptation</h3>
<p><a href='https://openreview.net/forum?id=BllUWdpIOA'>https://openreview.net/forum?id=BllUWdpIOA</a></p>
<p><b>Compressor summary</b>: The paper proposes a continual momentum filtering (CMF) framework that uses the Kalman filter to improve online test-time adaptation of deep neural networks by balancing adaptability and information retention, achieving better results than existing methods in various scenarios.</p><hr><h3>A Fast and Provable Algorithm for Sparse Phase Retrieval</h3>
<p><a href='https://openreview.net/forum?id=BlkxbI6vzl'>https://openreview.net/forum?id=BlkxbI6vzl</a></p>
<p><b>Compressor summary</b>: The paper proposes a new second-order algorithm for sparse phase retrieval that converges faster than existing first-order methods, given enough measurements.</p><hr><h3>(InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild</h3>
<p><a href='https://openreview.net/forum?id=Bl8u7ZRlbM'>https://openreview.net/forum?id=Bl8u7ZRlbM</a></p>
<p><b>Compressor summary</b>: WildChat is a large and diverse dataset of user-ChatGPT interactions that reveals how people use the chatbot in practice, including potentially unsafe scenarios, and can be used to improve instruction following models.</p><hr><h3>PB-LLM: Partially Binarized Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=BifeBRhikU'>https://openreview.net/forum?id=BifeBRhikU</a></p>
<p><b>Compressor summary</b>: This paper introduces a novel approach, Partially-Binarized LLM (PB-LLM), that compresses Large Language Models to single bit using salient weights and improves their linguistic reasoning capacity through post-training quantization and quantization-aware training.</p><hr><h3>An improved analysis of per-sample and per-update clipping in federated learning</h3>
<p><a href='https://openreview.net/forum?id=BdPvGRvoBC'>https://openreview.net/forum?id=BdPvGRvoBC</a></p>
<p><b>Compressor summary</b>: This paper analyzes the effects of two gradient clipping methods on the convergence of FedAvg, a federated learning algorithm, and provides convergence guarantees under different assumptions.</p><hr><h3>Large Language Models as Optimizers</h3>
<p><a href='https://openreview.net/forum?id=Bb4VGOWELI'>https://openreview.net/forum?id=Bb4VGOWELI</a></p>
<p><b>Compressor summary</b>: OPRO is a method that uses large language models to generate solutions for optimization problems described in natural language, achieving better performance than human-designed prompts.</p><hr><h3>AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?</h3>
<p><a href='https://openreview.net/forum?id=Bb21JPnhhr'>https://openreview.net/forum?id=Bb21JPnhhr</a></p>
<p><b>Compressor summary</b>: The text proposes AntGPT, which uses large language models to predict an actor's future behavior from video observations by inferring goals and modeling temporal dynamics.</p><hr><h3>Manifold Diffusion Fields</h3>
<p><a href='https://openreview.net/forum?id=BZtEthuXRF'>https://openreview.net/forum?id=BZtEthuXRF</a></p>
<p><b>Compressor summary</b>: MDF is a method for learning diffusion models on non-euclidean data using an intrinsic coordinate system, which improves diversity and accuracy on various datasets and problems.</p><hr><h3>Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions</h3>
<p><a href='https://openreview.net/forum?id=BXY6fe7q31'>https://openreview.net/forum?id=BXY6fe7q31</a></p>
<p><b>Compressor summary</b>: The paper introduces a new module, VPG-C, to improve multimodal language models' understanding of demonstrative instructions by completing missing visual details using synthetic discriminative training.</p><hr><h3>Provably Robust Conformal Prediction with Improved Efficiency</h3>
<p><a href='https://openreview.net/forum?id=BWAhEjXjeG'>https://openreview.net/forum?id=BWAhEjXjeG</a></p>
<p><b>Compressor summary</b>: The paper proposes RSCP+, a framework to improve conformal prediction methods' robustness against adversarial examples, and introduces two new methods, PTT and RCT, that reduce uncertainty sets while maintaining efficiency.</p><hr><h3>Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks</h3>
<p><a href='https://openreview.net/forum?id=BV1PHbTJzd'>https://openreview.net/forum?id=BV1PHbTJzd</a></p>
<p><b>Compressor summary</b>: The paper proposes a new distributed stochastic optimization algorithm using a non-linear Markov chain, the Self-Repellent Random Walk (SRRW), which improves convergence and reduces variance compared to existing methods.</p><hr><h3>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</h3>
<p><a href='https://openreview.net/forum?id=BTKAeLqLMw'>https://openreview.net/forum?id=BTKAeLqLMw</a></p>
<p><b>Compressor summary</b>: The authors study how to automatically select data for instruction tuning of language models, proposing a simple strategy and a new model (Deita) that performs well with minimal data.</p><hr><h3>DAFA: Distance-Aware Fair Adversarial Training</h3>
<p><a href='https://openreview.net/forum?id=BRdEBlwUW6'>https://openreview.net/forum?id=BRdEBlwUW6</a></p>
<p><b>Compressor summary</b>: The DAFA method addresses the robust fairness problem by adjusting adversarial margins and loss weights for different class similarities, improving the worst class's performance.</p><hr><h3>FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators</h3>
<p><a href='https://openreview.net/forum?id=BPb5AhT2Vf'>https://openreview.net/forum?id=BPb5AhT2Vf</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to register images with point clouds by matching diffusion features from pretrained models and geometric features from monocular depth estimators, achieving significant improvements over existing methods on indoor and outdoor benchmarks.</p><hr><h3>Demystifying Poisoning Backdoor Attacks from a Statistical Perspective</h3>
<p><a href='https://openreview.net/forum?id=BPHcEpGvF8'>https://openreview.net/forum?id=BPHcEpGvF8</a></p>
<p><b>Compressor summary</b>: This paper analyzes backdoor attacks in machine learning, which are stealthy ways to manipulate model behavior with hidden triggers, and proposes a theoretical framework to measure their effectiveness on clean and compromised data.</p><hr><h3>RealChat-1M: A Large-Scale Real-World LLM Conversation Dataset</h3>
<p><a href='https://openreview.net/forum?id=BOfDKxfwt0'>https://openreview.net/forum?id=BOfDKxfwt0</a></p>
<p><b>Compressor summary</b>: RealChat-1M is a large-scale dataset of real-world conversations with language models for various applications, including content moderation, safety benchmarking, instruction-following, and challenging questions.</p><hr><h3>LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints</h3>
<p><a href='https://openreview.net/forum?id=BLGQ3oqldb'>https://openreview.net/forum?id=BLGQ3oqldb</a></p>
<p><b>Compressor summary</b>: The paper introduces LogicMP, a new neural layer that enables efficient and modular integration of first-order logic constraints with neural networks using mean-field variational inference over an MLN.</p><hr><h3>CellPLM: Pre-training of Cell Language Model Beyond Single Cells</h3>
<p><a href='https://openreview.net/forum?id=BKXvPDekud'>https://openreview.net/forum?id=BKXvPDekud</a></p>
<p><b>Compressor summary</b>: The authors propose a new pre-trained model, CellPLM, for single-cell data that better captures cell-cell relationships and accounts for noisy data compared to existing models inspired by natural language processing.</p><hr><h3>Learning Scalar Fields for Molecular Docking with Fast Fourier Transforms</h3>
<p><a href='https://openreview.net/forum?id=BIveOmD1Nh'>https://openreview.net/forum?id=BIveOmD1Nh</a></p>
<p><b>Compressor summary</b>: Machine learning can speed up molecular docking by learning a fast scoring function from cross-correlation of ligand and protein fields using graph neural networks and Fourier transforms.</p><hr><h3>A Multi-Level Framework for Accelerating Training Transformer Models</h3>
<p><a href='https://openreview.net/forum?id=BI1N3lTWtn'>https://openreview.net/forum?id=BI1N3lTWtn</a></p>
<p><b>Compressor summary</b>: The paper proposes a multi-level framework for efficient training of large-scale deep learning models, using operators like coalescing, de-coalescing and interpolation, which reduces computational cost while maintaining performance.</p><hr><h3>A 2-Dimensional State Space Layer for Spatial Inductive Bias</h3>
<p><a href='https://openreview.net/forum?id=BGkqypmGvm'>https://openreview.net/forum?id=BGkqypmGvm</a></p>
<p><b>Compressor summary</b>: Our approach enhances computer vision models by adding a multidimensional State Space Model layer that improves 2-D inductive bias and performance, especially for Vision Transformers.</p><hr><h3>DP-SGD Without Clipping: The Lipschitz Neural Network Way</h3>
<p><a href='https://openreview.net/forum?id=BEyEziZ4R6'>https://openreview.net/forum?id=BEyEziZ4R6</a></p>
<p><b>Compressor summary</b>: The paper proposes using Lipschitz constrained networks to train DP-DNNs with tighter sensitivity bounds and without gradient clipping, and provides a Python package for implementing them.</p><hr><h3>Pre-training Sequence, Structure, and Surface Features for Comprehensive Protein Representation Learning</h3>
<p><a href='https://openreview.net/forum?id=BEH4mGo7zP'>https://openreview.net/forum?id=BEH4mGo7zP</a></p>
<p><b>Compressor summary</b>: The paper proposes a pre-training strategy called ProteinINR that incorporates information from protein sequences, 3D structures, and surfaces to improve protein representation learning and performance in downstream tasks.</p><hr><h3>A Unified and General Framework for Continual Learning</h3>
<p><a href='https://openreview.net/forum?id=BE5aK0ETbp'>https://openreview.net/forum?id=BE5aK0ETbp</a></p>
<p><b>Compressor summary</b>: The paper introduces a unified framework for continual learning methods that reconciles different approaches and reveals their shared mathematical structures, as well as a new concept called refresh learning that enhances performance by unlearning and relearning data.</p><hr><h3>Hybrid Directional Graph Neural Network for Molecules</h3>
<p><a href='https://openreview.net/forum?id=BBD6KXIGJL'>https://openreview.net/forum?id=BBD6KXIGJL</a></p>
<p><b>Compressor summary</b>: The Hybrid Directional Graph Neural Network (HDGNN) combines equivariant operations with learnable modules to predict chemical properties of molecules, achieving state-of-the-art performance on several tasks.</p><hr><h3>Compressing LLMs: The Truth is Rarely Pure and Never Simple</h3>
<p><a href='https://openreview.net/forum?id=B9klVS7Ddk'>https://openreview.net/forum?id=B9klVS7Ddk</a></p>
<p><b>Compressor summary</b>: LLM-KICK is a benchmark to evaluate compressed large language models based on various tasks, revealing strengths and weaknesses of existing compression methods.</p><hr><h3>ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search</h3>
<p><a href='https://openreview.net/forum?id=B6pQxqUcT8'>https://openreview.net/forum?id=B6pQxqUcT8</a></p>
<p><b>Compressor summary</b>: ToolChain* is a tree search-based algorithm that helps LLMs efficiently navigate large action spaces to solve complex problems using APIs.</p><hr><h3>P2Seg: Pointly-supervised Segmentation via Mutual Distillation</h3>
<p><a href='https://openreview.net/forum?id=B4vzu2aokv'>https://openreview.net/forum?id=B4vzu2aokv</a></p>
<p><b>Compressor summary</b>: This paper proposes a Mutual Distillation Module for point-level supervised instance segmentation that leverages both instance position and semantic information to enhance object perception and achieve better results than existing methods.</p><hr><h3>Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood</h3>
<p><a href='https://openreview.net/forum?id=AyzkDpuqcl'>https://openreview.net/forum?id=AyzkDpuqcl</a></p>
<p><b>Compressor summary</b>: CDRL is an approach that improves energy-based model training by learning from noisy versions of a dataset and refining samples with an initializer model, resulting in better sample quality and faster sampling time.</p><hr><h3>Symmetric Neural-Collapse Representations with Supervised Contrastive Loss: The Impact of ReLU and Batching</h3>
<p><a href='https://openreview.net/forum?id=AyXIDfvYg8'>https://openreview.net/forum?id=AyXIDfvYg8</a></p>
<p><b>Compressor summary</b>: The paper shows that adding a ReLU activation to the final layer of a model using supervised contrastive loss can restore symmetry in learned representations under class imbalances, without affecting test accuracy.</p><hr><h3>Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation and Regression</h3>
<p><a href='https://openreview.net/forum?id=Ax2yRhCQr1'>https://openreview.net/forum?id=Ax2yRhCQr1</a></p>
<p><b>Compressor summary</b>: This paper analyzes the role of augmentation in self-supervised representation learning, providing generalization bounds and a quantitative measure of augmentation complexity.</p><hr><h3>LLMs Represent Contextual Tasks as Compact Function Vectors</h3>
<p><a href='https://openreview.net/forum?id=AwyxtyMwaG'>https://openreview.net/forum?id=AwyxtyMwaG</a></p>
<p><b>Compressor summary</b>: The study reveals how transformer models use function vectors (FV) to represent and execute tasks across different contexts.</p><hr><h3>Understanding Length Generalization by Thinking Like Transformers</h3>
<p><a href='https://openreview.net/forum?id=AssIuHnmHX'>https://openreview.net/forum?id=AssIuHnmHX</a></p>
<p><b>Compressor summary</b>: The paper proposes a framework to understand when and how Transformers can length generalize, using RASP programming language and showing a correlation between RASP-simplicity and generalization.</p><hr><h3>FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis</h3>
<p><a href='https://openreview.net/forum?id=ArpwmicoYW'>https://openreview.net/forum?id=ArpwmicoYW</a></p>
<p><b>Compressor summary</b>: FairTune is a framework to optimize fairness in AI models by adjusting fine-tuning parameters based on validation data.</p><hr><h3>KoLA: Carefully Benchmarking World Knowledge of Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=AqN23oqraW'>https://openreview.net/forum?id=AqN23oqraW</a></p>
<p><b>Compressor summary</b>: KoLA is a benchmark that evaluates large language models based on their knowledge abilities using carefully designed tasks, data, and metrics.</p><hr><h3>Critical Learning Periods Emerge Even in Deep Linear Networks</h3>
<p><a href='https://openreview.net/forum?id=Aq35gl2c1k'>https://openreview.net/forum?id=Aq35gl2c1k</a></p>
<p><b>Compressor summary</b>: Critical learning periods are observed in both biological and artificial networks, and depend on network depth and data structure, as well as feature competition and task relationship in deep linear models.</p><hr><h3>How Well Do Supervised Models Transfer to 3D Image Segmentation?</h3>
<p><a href='https://openreview.net/forum?id=AhizIPytk4'>https://openreview.net/forum?id=AhizIPytk4</a></p>
<p><b>Compressor summary</b>: The text introduces ImageNetCT-9K, a large 3D CT dataset for pre-training models, which improves transfer learning for 3D image segmentation tasks.</p><hr><h3>BatchPrompt: Accomplish more with less</h3>
<p><a href='https://openreview.net/forum?id=Agyicd577r'>https://openreview.net/forum?id=Agyicd577r</a></p>
<p><b>Compressor summary</b>: The paper proposes BPE + SEAS to improve BatchPrompt, a prompting strategy that batches data points into one input for LLMs. This increases token utilization and efficiency, while enhancing quality through permutation, voting, and early stopping. The approach outperforms or matches SinglePrompt on several NLP tasks with less LLM resources.</p><hr><h3>OMNI: Open-endedness via Models of human Notions of Interestingness</h3>
<p><a href='https://openreview.net/forum?id=AgM3MzT99c'>https://openreview.net/forum?id=AgM3MzT99c</a></p>
<p><b>Compressor summary</b>: The text proposes using large language models to prioritize tasks for open-ended algorithms based on human notions of interestingness, improving the selection of learnable and novel tasks.</p><hr><h3>Large Language Models as Analogical Reasoners</h3>
<p><a href='https://openreview.net/forum?id=AgDICX1h50'>https://openreview.net/forum?id=AgDICX1h50</a></p>
<p><b>Compressor summary</b>: Analogical Prompting is a new approach that guides language models to generate relevant knowledge from past experiences and apply it to solve reasoning tasks, improving their performance.</p><hr><h3>Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs</h3>
<p><a href='https://openreview.net/forum?id=AfnsTnYphT'>https://openreview.net/forum?id=AfnsTnYphT</a></p>
<p><b>Compressor summary</b>: The Dynamic Signal Distribution model shows that CNNs have statistical advantages over LCNs and FCNs in capturing locality and translation invariance in real-world images.</p><hr><h3>General Stability Analysis for Zeroth-Order Optimization Algorithms</h3>
<p><a href='https://openreview.net/forum?id=AfhNyr73Ma'>https://openreview.net/forum?id=AfhNyr73Ma</a></p>
<p><b>Compressor summary</b>: The paper presents a general proof framework for stability analysis of zeroth-order optimization algorithms, which improves generalization bounds for SGD, GD, and SVRG using coordinate estimation.</p><hr><h3>Ghost on the Shell: An Expressive Representation of General 3D Shapes</h3>
<p><a href='https://openreview.net/forum?id=Ad87VjRqUw'>https://openreview.net/forum?id=Ad87VjRqUw</a></p>
<p><b>Compressor summary</b>: G-Shell is a parametrization method that enables fast reconstruction and generative modeling of both watertight and non-watertight 3D meshes from multiview images.</p><hr><h3>Rotation has two sides: Evaluating Data Augmentation for Deep One-class Classification</h3>
<p><a href='https://openreview.net/forum?id=Ad81awoBVS'>https://openreview.net/forum?id=Ad81awoBVS</a></p>
<p><b>Compressor summary</b>: The study finds a strong linear relationship between rotation prediction accuracy and one-class classification (OCC) performance, suggesting that estimating the transformation distribution in the dataset is crucial for self-supervised representation learning.</p><hr><h3>Risk Bounds of Accelerated SGD for Overparameterized Linear Regression</h3>
<p><a href='https://openreview.net/forum?id=AcoXPIPh4A'>https://openreview.net/forum?id=AcoXPIPh4A</a></p>
<p><b>Compressor summary</b>: ASGD can perform better generalization than SGD in overparameterized linear regression by adapting to different eigenvalue subspaces of the data covariance matrix.</p><hr><h3>Transformers vs. Message Passing GNNs: Distinguished in Uniform</h3>
<p><a href='https://openreview.net/forum?id=AcSChDWL6V'>https://openreview.net/forum?id=AcSChDWL6V</a></p>
<p><b>Compressor summary</b>: Graph Transformers and Message Passing Graph Neural Networks with virtual nodes have different universal expressivity, but both can effectively learn from global information on graphs.</p><hr><h3>Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness</h3>
<p><a href='https://openreview.net/forum?id=AcRfzLS6se'>https://openreview.net/forum?id=AcRfzLS6se</a></p>
<p><b>Compressor summary</b>: BLOOD is a method for detecting out-of-distribution (OOD) data in deep neural networks based on the smoothness of transformation between intermediate layers, which works well on pre-trained models without training data access and outperforms other methods.</p><hr><h3>Rethinking Model Ensemble in Transfer-based Adversarial Attacks</h3>
<p><a href='https://openreview.net/forum?id=AcJrSoArlh'>https://openreview.net/forum?id=AcJrSoArlh</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to improve the transferability of adversarial examples by targeting the common weaknesses of model ensemble, which are flatness and closeness to local optimum. The method works well on various tasks and models.</p><hr><h3>Forward Learning of Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=Abr7dU98ME'>https://openreview.net/forum?id=Abr7dU98ME</a></p>
<p><b>Compressor summary</b>: ForwardGNN is a new forward learning procedure for graph neural networks that avoids the constraints of backpropagation by performing layer-wise local forward training on graph data.</p><hr><h3>Implicit regularization of deep residual networks towards neural ODEs</h3>
<p><a href='https://openreview.net/forum?id=AbXGwqb5Ht'>https://openreview.net/forum?id=AbXGwqb5Ht</a></p>
<p><b>Compressor summary</b>: The article establishes a regularization method for deep residual networks trained with gradient flow, showing that they converge to neural ODEs when initialized as discretizations of them.</p><hr><h3>Enhancing Instance-Level Image Classification with Set-Level Labels</h3>
<p><a href='https://openreview.net/forum?id=AZW3qlCGTe'>https://openreview.net/forum?id=AZW3qlCGTe</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to improve image classification by using set-level labels that capture relationships among instances and provides theoretical analysis and experimental results showing its effectiveness.</p><hr><h3>Towards Cross Domain Generalization of Hamiltonian Representation via Meta Learning</h3>
<p><a href='https://openreview.net/forum?id=AZGIwqCyYY'>https://openreview.net/forum?id=AZGIwqCyYY</a></p>
<p><b>Compressor summary</b>: The authors propose a meta learning method to train a graph neural network for Hamiltonian dynamics, enabling it to adapt to new physical systems and capture a generalized representation across domains.</p><hr><h3>Adaptive Regret for Bandits Made Possible: Two Queries Suffice</h3>
<p><a href='https://openreview.net/forum?id=AY9KyTGcnk'>https://openreview.net/forum?id=AY9KyTGcnk</a></p>
<p><b>Compressor summary</b>: The paper presents StABL, an optimal online learning algorithm that adapts quickly to changing environments with minimal queries per round.</p><hr><h3>Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=AY6aM13gGF'>https://openreview.net/forum?id=AY6aM13gGF</a></p>
<p><b>Compressor summary</b>: The paper presents LaMo, a framework that uses Decision Transformers and pre-trained Language Models to achieve state-of-the-art performance in offline reinforcement learning tasks, especially with limited data samples.</p><hr><h3>Protein-ligand binding representation learning from fine-grained interactions</h3>
<p><a href='https://openreview.net/forum?id=AXbN2qMNiW'>https://openreview.net/forum?id=AXbN2qMNiW</a></p>
<p><b>Compressor summary</b>: The paper proposes a self-supervised learning method for protein-ligand binding representation using Transformer interactions and two pre-training tasks, achieving better generalization across various binding tasks.</p><hr><h3>M3C: A Framework towards Convergent, Flexible, and Unsupervised Learning of Mixture Graph Matching and Clustering</h3>
<p><a href='https://openreview.net/forum?id=AXC9KydyZq'>https://openreview.net/forum?id=AXC9KydyZq</a></p>
<p><b>Compressor summary</b>: M3C is a learning-free algorithm for graph matching and clustering that guarantees convergence, offers flexibility, and outperforms existing methods.</p><hr><h3>BrainPy: a differentiable brain simulator bridging brain simulation and brain-inspired computing</h3>
<p><a href='https://openreview.net/forum?id=AU2gS9ut61'>https://openreview.net/forum?id=AU2gS9ut61</a></p>
<p><b>Compressor summary</b>: BrainPy is a differentiable brain simulator that bridges the gap between brain simulation and brain-inspired computing by providing efficient and scalable brain modeling capabilities.</p><hr><h3>GAIA: Data-driven Zero-shot Talking Avatar Generation</h3>
<p><a href='https://openreview.net/forum?id=ATEawsFUj4'>https://openreview.net/forum?id=ATEawsFUj4</a></p>
<p><b>Compressor summary</b>: GAIA is a data-driven approach that generates diverse, natural, and high-quality talking avatars using speech-to-motion and appearance disentanglement.</p><hr><h3>On the hardness of learning under symmetries</h3>
<p><a href='https://openreview.net/forum?id=ARPrtuzAnQ'>https://openreview.net/forum?id=ARPrtuzAnQ</a></p>
<p><b>Compressor summary</b>: The paper investigates if incorporating symmetries into neural networks can reduce the difficulty of learning them with gradient descent, and finds that it does not for several types of networks.</p><hr><h3>OpenChat: Advancing Open-source Language Models with Mixed-Quality Data</h3>
<p><a href='https://openreview.net/forum?id=AOJyfhWYHf'>https://openreview.net/forum?id=AOJyfhWYHf</a></p>
<p><b>Compressor summary</b>: OpenChat is a novel framework that improves open-source language models with mixed-quality data using a class-conditioned policy in reinforcement learning, achieving the highest performance among 13b models.</p><hr><h3>Generalization in diffusion models arises from geometry-adaptive harmonic representation</h3>
<p><a href='https://openreview.net/forum?id=ANvmVS2Yr0'>https://openreview.net/forum?id=ANvmVS2Yr0</a></p>
<p><b>Compressor summary</b>: DNNs trained for denoising can learn high-dimensional densities and perform shrinkage operations in adapted bases, despite memorization concerns.</p><hr><h3>Coeditor: Leveraging Repo-level Diffs for Code Auto-editing</h3>
<p><a href='https://openreview.net/forum?id=ALVwQjZRS8'>https://openreview.net/forum?id=ALVwQjZRS8</a></p>
<p><b>Compressor summary</b>: The authors propose Coeditor, a fine-tuned language model that predicts multiple edits to a code region based on recent changes within the same codebase, outperforming existing models in single and multi-round code completion tasks.</p><hr><h3>One Forward is Enough for Neural Network Training via Likelihood Ratio Method</h3>
<p><a href='https://openreview.net/forum?id=ALGFFPXWSi'>https://openreview.net/forum?id=ALGFFPXWSi</a></p>
<p><b>Compressor summary</b>: The unified likelihood ratio (ULR) method is a flexible and efficient gradient estimation technique for neural network training that reduces computation, improves device adaptation, and enhances robustness.</p><hr><h3>Causality-Inspired Spatial-Temporal Explanations for Dynamic Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=AJBkfwXh3u'>https://openreview.net/forum?id=AJBkfwXh3u</a></p>
<p><b>Compressor summary</b>: The paper proposes a causality-inspired generative model to improve the interpretability of dynamic graph neural networks by identifying static, dynamic, and causal relationships in complex spatial-temporal correlations.</p><hr><h3>Backdoor Federated Learning by Poisoning Backdoor-Critical Layers</h3>
<p><a href='https://openreview.net/forum?id=AJBGSVSTT2'>https://openreview.net/forum?id=AJBGSVSTT2</a></p>
<p><b>Compressor summary</b>: The paper proposes a new backdoor attack method for federated learning that targets crucial layers of the model, improving stealthiness and effectiveness under various defense strategies.</p><hr><h3>MuSc : Zero-Shot Anomaly Classification and Segmentation by Mutual Scoring of the Unlabeled Images</h3>
<p><a href='https://openreview.net/forum?id=AHgc5SMdtd'>https://openreview.net/forum?id=AHgc5SMdtd</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel zero-shot anomaly classification and segmentation method for industrial vision that uses mutual scoring of unlabeled test images and achieves superior performance on MVTec AD and VisA datasets.</p><hr><h3>SpeechTokenizer: Unified Speech Tokenizer for Speech Language Models</h3>
<p><a href='https://openreview.net/forum?id=AF9Q8Vip84'>https://openreview.net/forum?id=AF9Q8Vip84</a></p>
<p><b>Compressor summary</b>: The paper introduces SpeechTokenizer, a unified speech tokenizer for speech large language models that uses Encoder-Decoder architecture with residual vector quantization (RVQ) and proposes Unified Speech Language Model (USLM), which performs well on benchmarks and text-to-speech tasks.</p><hr><h3>LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents</h3>
<p><a href='https://openreview.net/forum?id=ADSxCpCu9s'>https://openreview.net/forum?id=ADSxCpCu9s</a></p>
<p><b>Compressor summary</b>: The text presents a benchmark system that evaluates how well language models can plan tasks for home-service robots using different datasets, simulators, and prompts.</p><hr><h3>Manipulating dropout reveals an optimal balance of efficiency and robustness in biological and machine visual systems</h3>
<p><a href='https://openreview.net/forum?id=ADDCErFzev'>https://openreview.net/forum?id=ADDCErFzev</a></p>
<p><b>Compressor summary</b>: The text discusses how varying dropout proportion in object recognition models can reveal an optimal balance between high-dimensional efficient codes and low-dimensional robust codes in hierarchical vision systems, as observed in human occipitotemporal cortex.</p><hr><h3>Hyper Evidential Deep Learning to Quantify Composite Classification Uncertainty</h3>
<p><a href='https://openreview.net/forum?id=A7t7z6g6tM'>https://openreview.net/forum?id=A7t7z6g6tM</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework called HENN that models predictive uncertainty caused by composite set labels in DNNs using Subjective Logic, and shows improved performance over existing methods.</p><hr><h3>Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning</h3>
<p><a href='https://openreview.net/forum?id=A4mJuFRMN8'>https://openreview.net/forum?id=A4mJuFRMN8</a></p>
<p><b>Compressor summary</b>: The paper proposes RENT, a resampling method for learning with noisy labels using the transition matrix, and shows it performs better than existing methods.</p><hr><h3>Semantic Flow: Learning Semantic Fields of Dynamic Scenes from Monocular Videos</h3>
<p><a href='https://openreview.net/forum?id=A2mRcRyGdl'>https://openreview.net/forum?id=A2mRcRyGdl</a></p>
<p><b>Compressor summary</b>: Semantic Flow learns 3D motion information from monocular videos using neural networks and semantic labels to enable various tasks like editing, completion, tracking, and adaptation.</p><hr><h3>Course Correcting Koopman Representations</h3>
<p><a href='https://openreview.net/forum?id=A18gWgc5mi'>https://openreview.net/forum?id=A18gWgc5mi</a></p>
<p><b>Compressor summary</b>: The paper explores autoencoders for modeling nonlinear dynamical systems and proposes a new method called Periodic Reencoding to improve future state prediction.</p><hr><h3>What happens when you fine-tuning your model? Mechanistic analysis of procedurally generated tasks.</h3>
<p><a href='https://openreview.net/forum?id=A0HKeKl4Nl'>https://openreview.net/forum?id=A0HKeKl4Nl</a></p>
<p><b>Compressor summary</b>: Fine-tuning changes machine learning models' capabilities in subtle ways, which could affect their safety and performance.</p><hr><h3>Leveraging Generative Models for Unsupervised Alignment of Neural Time Series Data</h3>
<p><a href='https://openreview.net/forum?id=9zhHVyLY4K'>https://openreview.net/forum?id=9zhHVyLY4K</a></p>
<p><b>Compressor summary</b>: The authors propose an unsupervised method to align pre-trained generative models with new neural datasets for neuroscience applications.</p><hr><h3>Provably Efficient CVaR RL in Low-rank MDPs</h3>
<p><a href='https://openreview.net/forum?id=9x6yrFAPnx'>https://openreview.net/forum?id=9x6yrFAPnx</a></p>
<p><b>Compressor summary</b>: The paper presents a new CVaR RL algorithm that combines UCB bonus, exploration, and representation learning in low-rank MDPs with nonlinear functions. It shows the sample complexity and efficiency of the algorithm.</p><hr><h3>Entropy is not Enough for Test-time Adaptation: From the Perspective of Disentangled Factors</h3>
<p><a href='https://openreview.net/forum?id=9w3iw8wDuE'>https://openreview.net/forum?id=9w3iw8wDuE</a></p>
<p><b>Compressor summary</b>: DeYO is a novel test-time adaptation method that uses a new confidence metric, Pseudo-Label Probability Difference (PLPD), to select and weight samples based on the influence of object shape on predictions.</p><hr><h3>Video Language Planning</h3>
<p><a href='https://openreview.net/forum?id=9pKtcJcMP3'>https://openreview.net/forum?id=9pKtcJcMP3</a></p>
<p><b>Compressor summary</b>: Video language planning (VLP) is a novel algorithm that uses generative models to create multimodal plans for complex tasks, which can then be executed by real robots with higher success rates.</p><hr><h3>Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=9nsNyN0vox'>https://openreview.net/forum?id=9nsNyN0vox</a></p>
<p><b>Compressor summary</b>: The text proposes a method called "Neural Comprehension" that incorporates compiled neural networks into language models to improve their rule-based reasoning and symbolic comprehension abilities.</p><hr><h3>DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=9m02ib92Wz'>https://openreview.net/forum?id=9m02ib92Wz</a></p>
<p><b>Compressor summary</b>: DataInf is a fast and accurate method for approximating influence scores in large generative AI models, enabling efficient data attribution and improving model transparency.</p><hr><h3>Time Fairness in Online Knapsack Problems</h3>
<p><a href='https://openreview.net/forum?id=9kG7TwgLYu'>https://openreview.net/forum?id=9kG7TwgLYu</a></p>
<p><b>Compressor summary</b>: The paper proposes fair and efficient algorithms for the online knapsack problem with applications to cloud resource allocation.</p><hr><h3>On the Learnability of Watermarks for Language Models</h3>
<p><a href='https://openreview.net/forum?id=9k0krNzvlV'>https://openreview.net/forum?id=9k0krNzvlV</a></p>
<p><b>Compressor summary</b>: Watermark distillation trains a student model to generate watermarked text like a teacher model that uses decoding-based watermarking, with varying results depending on the watermarking strategy and conditions.</p><hr><h3>Bayesian Optimization through Gaussian Cox Process Models for Spatio-temporal Data</h3>
<p><a href='https://openreview.net/forum?id=9j1RD9LlWH'>https://openreview.net/forum?id=9j1RD9LlWH</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to efficiently optimize expensive functions using Gaussian Cox processes and shows its effectiveness in various datasets.</p><hr><h3>Machine Unlearning for Image-to-Image Generative Models</h3>
<p><a href='https://openreview.net/forum?id=9hjVoPWPnh'>https://openreview.net/forum?id=9hjVoPWPnh</a></p>
<p><b>Compressor summary</b>: The paper introduces a framework and algorithm for machine unlearning image-to-image generative models without compromising performance or violating data retention policies.</p><hr><h3>AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation</h3>
<p><a href='https://openreview.net/forum?id=9cQtXpRshE'>https://openreview.net/forum?id=9cQtXpRshE</a></p>
<p><b>Compressor summary</b>: AGILE3D is an attention-based model that enables simultaneous and more accurate 3D object segmentation with fewer user clicks and faster inference by leveraging spatial-temporal queries and click attention.</p><hr><h3>Bootstrapping Variational Information Pursuit with Foundation Models for Interpretable Image Classification</h3>
<p><a href='https://openreview.net/forum?id=9bmTbVaA2A'>https://openreview.net/forum?id=9bmTbVaA2A</a></p>
<p><b>Compressor summary</b>: Concept-QA+V-IP is a method that combines a language model (GPT) to propose semantic concepts as queries for image classification, a question-answering network (Concept-QA) to answer the queries, and Variational Information Pursuit (V-IP) to select informative query chains that serve as explanations for predictions.</p><hr><h3>TOSS: High-quality Text-guided Novel View Synthesis from a Single Image</h3>
<p><a href='https://openreview.net/forum?id=9ZUYJpvIys'>https://openreview.net/forum?id=9ZUYJpvIys</a></p>
<p><b>Compressor summary</b>: TOSS uses text to guide the generation of novel views from a single image, improving quality and control over existing methods like Zero123.</p><hr><h3>Zipformer: A faster and better encoder for automatic speech recognition</h3>
<p><a href='https://openreview.net/forum?id=9WD9KwssyT'>https://openreview.net/forum?id=9WD9KwssyT</a></p>
<p><b>Compressor summary</b>: Zipformer is a faster, more memory-efficient, and better-performing transformer model for automatic speech recognition that uses a U-Net-like encoder structure, reorganized block structure, BiasNorm, new activation functions, and ScaledAdam optimizer.</p><hr><h3>Maximally discriminative stimuli for functional cell type identification</h3>
<p><a href='https://openreview.net/forum?id=9W6KaAcYlr'>https://openreview.net/forum?id=9W6KaAcYlr</a></p>
<p><b>Compressor summary</b>: The authors propose an optimization-based clustering approach using deep predictive models to identify functional cell types in retina and visual cortex using Maximally Discriminative Stimuli (MDS), which are interpretable and can be used for on-the-fly assignment of cell types during data acquisition.</p><hr><h3>De novo Protein Design Using Geometric Vector Field Networks</h3>
<p><a href='https://openreview.net/forum?id=9UIGyJJpay'>https://openreview.net/forum?id=9UIGyJJpay</a></p>
<p><b>Compressor summary</b>: The Vector Field Network (VFN) is a new encoder for protein design that can model both frames and atoms using learnable vector computations, improving the performance of de novo protein design methods.</p><hr><h3>NECO: NEural Collapse Based Out-of-distribution detection</h3>
<p><a href='https://openreview.net/forum?id=9ROuKblmi7'>https://openreview.net/forum?id=9ROuKblmi7</a></p>
<p><b>Compressor summary</b>: NECO is a new method that uses neural collapse and principal component spaces to detect out-of-distribution data in machine learning models.</p><hr><h3>Leave-one-out Distinguishability in Machine Learning</h3>
<p><a href='https://openreview.net/forum?id=9RNfX0ah0K'>https://openreview.net/forum?id=9RNfX0ah0K</a></p>
<p><b>Compressor summary</b>: The paper introduces LOOD, a framework to measure data memorization and information leakage in machine learning algorithms by analyzing the changes in output distribution when leaving out one data point at a time.</p><hr><h3>On Double-Descent in Reinforcement Learning with LSTD and Random Features</h3>
<p><a href='https://openreview.net/forum?id=9RIbNmx984'>https://openreview.net/forum?id=9RIbNmx984</a></p>
<p><b>Compressor summary</b>: The paper analyzes how network size, regularization, and the ratio between parameters and visited states affect TD algorithms in deep RL, finding a double-descent pattern in performance.</p><hr><h3>Do Large Language Models Know about Facts?</h3>
<p><a href='https://openreview.net/forum?id=9OevMUdods'>https://openreview.net/forum?id=9OevMUdods</a></p>
<p><b>Compressor summary</b>: Pinocchio is a benchmark to evaluate the factual knowledge in large language models, revealing their limitations and challenges for trustworthy AI.</p><hr><h3>Neural Field Classifiers via Target Encoding and Classification Loss</h3>
<p><a href='https://openreview.net/forum?id=9NqC72m31m'>https://openreview.net/forum?id=9NqC72m31m</a></p>
<p><b>Compressor summary</b>: The authors propose a Neural Field Classifier framework that transforms regression tasks in neural fields into multi-label classification tasks, improving performance and robustness with minimal additional cost.</p><hr><h3>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</h3>
<p><a href='https://openreview.net/forum?id=9JQtrumvg8'>https://openreview.net/forum?id=9JQtrumvg8</a></p>
<p><b>Compressor summary</b>: The paper introduces WebAgent, a large language model-driven agent that learns from self-experience to complete tasks on real websites using natural language instructions, planning, summarization, and code generation.</p><hr><h3>ED-NeRF: Efficient Text-Guided Editing of 3D Scene With Latent Space NeRF</h3>
<p><a href='https://openreview.net/forum?id=9DvDRTTdlu'>https://openreview.net/forum?id=9DvDRTTdlu</a></p>
<p><b>Compressor summary</b>: ED-NeRF is a novel approach for fast and effective 3D NeRF editing using a refinement layer in the latent space of a diffusion model and an improved loss function tailored for editing.</p><hr><h3>Elucidating the design space of classifier-guided diffusion generation</h3>
<p><a href='https://openreview.net/forum?id=9DXXMXnIGm'>https://openreview.net/forum?id=9DXXMXnIGm</a></p>
<p><b>Compressor summary</b>: The paper proposes a training-free method to improve diffusion models by using off-the-shelf classifiers for guidance, achieving significant performance improvements with minimal extra cost.</p><hr><h3>Multi-granularity Correspondence Learning from Noisy Instructional Videos</h3>
<p><a href='https://openreview.net/forum?id=9Cu8MRmhq2'>https://openreview.net/forum?id=9Cu8MRmhq2</a></p>
<p><b>Compressor summary</b>: Norton is a method that uses optimal transport to learn correspondence between video clips and captions, addressing misalignment issues in temporal learning for video-language tasks.</p><hr><h3>Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior</h3>
<p><a href='https://openreview.net/forum?id=99tKiMVJhY'>https://openreview.net/forum?id=99tKiMVJhY</a></p>
<p><b>Compressor summary</b>: The paper proposes Dec-POMFC, a method for multi-agent reinforcement learning that considers decentralization and partial observability, and shows its effectiveness on swarming tasks.</p><hr><h3>Alt-Text with Context: Improving Accessibility for Images on Twitter</h3>
<p><a href='https://openreview.net/forum?id=97Dl82avFs'>https://openreview.net/forum?id=97Dl82avFs</a></p>
<p><b>Compressor summary</b>: The paper proposes an alt-text generation method for Twitter images that uses both image and text inputs and shows superior performance over previous methods.</p><hr><h3>The HIM Solution for Legged Locomotion: Minimal Sensors, Efficient Learning, and Substantial Agility</h3>
<p><a href='https://openreview.net/forum?id=93LoCyww8o'>https://openreview.net/forum?id=93LoCyww8o</a></p>
<p><b>Compressor summary</b>: The paper proposes a Hybrid Internal Model method for quadruped robots' locomotion control using joint encoders and an IMU, which improves agility, robustness, and adaptability on various terrains.</p><hr><h3>SPDER: Semiperiodic Damping-Enabled Object Representation</h3>
<p><a href='https://openreview.net/forum?id=92btneN9Wm'>https://openreview.net/forum?id=92btneN9Wm</a></p>
<p><b>Compressor summary</b>: The proposed SPDER neural network architecture learns a positional embedding and overcomes spectral bias, resulting in faster training, lower losses, and better performance on various tasks.</p><hr><h3>On gauge freedom, conservativity and intrinsic dimensionality estimation in diffusion models</h3>
<p><a href='https://openreview.net/forum?id=92KV9xAMhF'>https://openreview.net/forum?id=92KV9xAMhF</a></p>
<p><b>Compressor summary</b>: The text discusses how diffusion models use generative processes and vector fields to estimate densities and sample data in high dimensions, and explores the importance of constraining these vector fields to be conservative for accurate inference and dimensionality estimation.</p><hr><h3>Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer</h3>
<p><a href='https://openreview.net/forum?id=90yw2uM6J5'>https://openreview.net/forum?id=90yw2uM6J5</a></p>
<p><b>Compressor summary</b>: The paper presents Hierarchical Contact Mesh Transformer (HCMT), a GNN model that uses hierarchical mesh structures to learn long-range dependencies and quickly propagate collision effects in flexible body dynamics, achieving better performance than existing methods.</p><hr><h3>MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data</h3>
<p><a href='https://openreview.net/forum?id=8xliOUg9EW'>https://openreview.net/forum?id=8xliOUg9EW</a></p>
<p><b>Compressor summary</b>: MUSTARD is a data generation framework that creates high-quality theorem and proof datasets for training large language models in mathematical reasoning and theorem proving tasks.</p><hr><h3>What does automatic differentiation compute for neural networks?</h3>
<p><a href='https://openreview.net/forum?id=8vKknbgXxf'>https://openreview.net/forum?id=8vKknbgXxf</a></p>
<p><b>Compressor summary</b>: AD can compute a generalized derivative called a Clarke subderivative for neural networks with non-differentiable functions, under certain conditions and minibatch size.</p><hr><h3>Learning Nash equilibria in Rank-1 games: Going beyond the Minty Property</h3>
<p><a href='https://openreview.net/forum?id=8utTlmhw8v'>https://openreview.net/forum?id=8utTlmhw8v</a></p>
<p><b>Compressor summary</b>: The paper shows how to efficiently learn Nash equilibria in rank-1 bimatrix games using optimistic gradient descent/ascent, without requiring the Minty property.</p><hr><h3>Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking</h3>
<p><a href='https://openreview.net/forum?id=8sKcAWOf2D'>https://openreview.net/forum?id=8sKcAWOf2D</a></p>
<p><b>Compressor summary</b>: Fine-tuning language models on generalized tasks improves their entity tracking ability by enhancing positional information handling.</p><hr><h3>One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention</h3>
<p><a href='https://openreview.net/forum?id=8p3fu56lKc'>https://openreview.net/forum?id=8p3fu56lKc</a></p>
<p><b>Compressor summary</b>: The paper studies how one-layer transformers with linear self-attention learn to implement gradient descent on synthetic noisy linear regression data and finds that the learned algorithm depends on the distribution of covariates, weight vector, and responses.</p><hr><h3>DiffEnc: Variational Diffusion with a Learned Encoder</h3>
<p><a href='https://openreview.net/forum?id=8nxy1bQWTG'>https://openreview.net/forum?id=8nxy1bQWTG</a></p>
<p><b>Compressor summary</b>: Diffusion models are improved hierarchical variational autoencoders with flexible mean functions and weighted noise variances that achieve state of the art results on CIFAR-10.</p><hr><h3>Combining Axes Preconditioners through Kronecker Approximation for Deep Learning</h3>
<p><a href='https://openreview.net/forum?id=8j9hz8DVi8'>https://openreview.net/forum?id=8j9hz8DVi8</a></p>
<p><b>Compressor summary</b>: CASPR is a technique that improves the speed and performance of training deep neural networks by using different preconditioners for each parameter axis and combining them with Kronecker-sum approximation.</p><hr><h3>Finite Scalar Quantization: VQ-VAE Made Simple</h3>
<p><a href='https://openreview.net/forum?id=8ishA3LxN8'>https://openreview.net/forum?id=8ishA3LxN8</a></p>
<p><b>Compressor summary</b>: The authors propose a simpler method called finite scalar quantization (FSQ) for representing latent variables in variational autoencoders (VAE), which can perform well on various computer vision tasks without the drawbacks of vector quantization (VQ).</p><hr><h3>Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection</h3>
<p><a href='https://openreview.net/forum?id=8iTpB4RNvP'>https://openreview.net/forum?id=8iTpB4RNvP</a></p>
<p><b>Compressor summary</b>: This paper introduces a new backdoor attack on face forgery detection methods that uses specific trigger patterns to deceive detectors and embeds them with backdoors, outperforming existing attacks.</p><hr><h3>Neural Auto-designer for Enhanced Quantum Kernels</h3>
<p><a href='https://openreview.net/forum?id=8htNAnMSyP'>https://openreview.net/forum?id=8htNAnMSyP</a></p>
<p><b>Compressor summary</b>: The study proposes a data-driven approach to design effective quantum feature maps for real-world datasets using feature-selection techniques and a deep neural predictor.</p><hr><h3>Amortized Network Intervention to Steer the Excitatory Point Processes</h3>
<p><a href='https://openreview.net/forum?id=8g26Yv1EOu'>https://openreview.net/forum?id=8g26Yv1EOu</a></p>
<p><b>Compressor summary</b>: The text presents a model-based reinforcement learning approach with neural ODEs and GD-MPC to intervene in large-scale networked excitatory point processes, using an ANI framework for knowledge transfer and sharing across diverse contexts.</p><hr><h3>Prometheus: Inducing Evaluation Capability in Language Models</h3>
<p><a href='https://openreview.net/forum?id=8euJaTveKw'>https://openreview.net/forum?id=8euJaTveKw</a></p>
<p><b>Compressor summary</b>: PROMETHEUS is an open-source language model that can evaluate text as well as GPT-4 when given appropriate reference materials, trained on a new dataset called FEEDBACK COLLECTION that includes score rubrics and feedback generated by GPT-4.</p><hr><h3>Uncertainty-aware Graph-based Hyperspectral Image Classification</h3>
<p><a href='https://openreview.net/forum?id=8dN7gApKm3'>https://openreview.net/forum?id=8dN7gApKm3</a></p>
<p><b>Compressor summary</b>: The paper proposes uncertainty quantification models for hyperspectral image classification, addressing epistemic and aleatoric uncertainties, and improving out-of-distribution and misclassification detection with regularization terms.</p><hr><h3>OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=8Wuvhh0LYW'>https://openreview.net/forum?id=8Wuvhh0LYW</a></p>
<p><b>Compressor summary</b>: OmniQuant is a technique that optimizes quantization parameters for large language models to improve efficiency and performance across various settings.</p><hr><h3>Context is Environment</h3>
<p><a href='https://openreview.net/forum?id=8VPWfqtQMX'>https://openreview.net/forum?id=8VPWfqtQMX</a></p>
<p><b>Compressor summary</b>: The paper proposes In-Context Risk Minimization (ICRM), an algorithm that uses contextual information to improve domain generalization and out-of-distribution performance.</p><hr><h3>Revisiting Link Prediction: a data perspective</h3>
<p><a href='https://openreview.net/forum?id=8Ur2xmuw7w'>https://openreview.net/forum?id=8Ur2xmuw7w</a></p>
<p><b>Compressor summary</b>: The paper explores link prediction principles across diverse datasets, identifying three critical factors and their relationships, and providing instructions for GNN4LP model design and dataset selection.</p><hr><h3>Class Incremental Learning via Likelihood Ratio Based Task Prediction</h3>
<p><a href='https://openreview.net/forum?id=8QfK9Dq4q0'>https://openreview.net/forum?id=8QfK9Dq4q0</a></p>
<p><b>Compressor summary</b>: The paper proposes TPLR, a novel method for predicting task IDs in class incremental learning using likelihood ratios and replay data, which significantly improves performance over existing approaches.</p><hr><h3>R&B: Region and Boundary Aware Zero-shot Grounded Text-to-image Generation</h3>
<p><a href='https://openreview.net/forum?id=8Q4uVOJ5bX'>https://openreview.net/forum?id=8Q4uVOJ5bX</a></p>
<p><b>Compressor summary</b>: The paper presents a novel zero-shot grounded T2I generation method with cross-attention guidance, discrete sampling, and region/boundary-aware losses that improves image quality and layout fidelity.</p><hr><h3>MgNO: Efficient Parameterization of Linear Operators via Multigrid</h3>
<p><a href='https://openreview.net/forum?id=8OxL034uEr'>https://openreview.net/forum?id=8OxL034uEr</a></p>
<p><b>Compressor summary</b>: The paper presents a new neural operator architecture using multigrid structures for learning nonlinear operators, which improves training ease and reduces overfitting compared to existing methods.</p><hr><h3>Fast-ELECTRA for Efficient Pre-training</h3>
<p><a href='https://openreview.net/forum?id=8OBuqbLb8h'>https://openreview.net/forum?id=8OBuqbLb8h</a></p>
<p><b>Compressor summary</b>: Fast-ELECTRA is a more efficient ELECTRA-style pre-training method that uses an existing language model as the auxiliary model, smooths the main model's output distribution with temperature scaling, and reduces training cost.</p><hr><h3>Sample Relationship from Learning Dynamics Matters for Generalisation</h3>
<p><a href='https://openreview.net/forum?id=8Ju0VmvMCW'>https://openreview.net/forum?id=8Ju0VmvMCW</a></p>
<p><b>Compressor summary</b>: The authors propose a new kernel that considers label information to measure interactions between samples in artificial neural networks and demonstrate its usefulness for understanding learning phenomena and detecting poisoning samples.</p><hr><h3>CoBIT: A Contrastive Bi-directional Image-Text Generation Model</h3>
<p><a href='https://openreview.net/forum?id=8ISRqgtjPc'>https://openreview.net/forum?id=8ISRqgtjPc</a></p>
<p><b>Compressor summary</b>: CoBIT is a novel framework that unifies three pre-training objectives for vision-and-language tasks using a unicoder-decoder structure with image and text encoders and a cross-modal decoder, achieving improved performance in various tasks.</p><hr><h3>Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction</h3>
<p><a href='https://openreview.net/forum?id=8HCARN2hhw'>https://openreview.net/forum?id=8HCARN2hhw</a></p>
<p><b>Compressor summary</b>: The text proposes learning an actionable representation of 3D scenes for agents without explicit scene reconstruction, using a blind auxiliary agent to optimize navigation and improve robustness to distribution changes.</p><hr><h3>Learning Personalized Causally Invariant Representations for Heterogeneous Federated Clients</h3>
<p><a href='https://openreview.net/forum?id=8FHWkY0SwF'>https://openreview.net/forum?id=8FHWkY0SwF</a></p>
<p><b>Compressor summary</b>: The paper proposes FedSDR, a method for personalized federated learning that discovers and removes shortcut features to improve out-of-distribution generalization using structural causal models.</p><hr><h3>Language-Interfaced Tabular Oversampling via Progressive Imputation and Self-Authentication</h3>
<p><a href='https://openreview.net/forum?id=8F6bws5JBy'>https://openreview.net/forum?id=8F6bws5JBy</a></p>
<p><b>Compressor summary</b>: The paper proposes a new oversampling framework using language models to generate synthetic minority samples for balancing class distribution and improving machine learning performance.</p><hr><h3>CLAP: Collaborative Adaptation for Checkerboard Learning</h3>
<p><a href='https://openreview.net/forum?id=8EyRkd3Qj2'>https://openreview.net/forum?id=8EyRkd3Qj2</a></p>
<p><b>Compressor summary</b>: The paper proposes checkerboard learning, a novel imputation framework for fragmented multimodal data in distributed clients, which adapts transformations between modalities using a Pareto min-max framework.</p><hr><h3>A Policy Gradient Method for Confounded POMDPs</h3>
<p><a href='https://openreview.net/forum?id=8BAkNCqpGW'>https://openreview.net/forum?id=8BAkNCqpGW</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method to estimate policy gradients for confounded POMDPs with continuous state and observation spaces in the offline setting using non-parametric identification, moment restrictions, and min-max learning.</p><hr><h3>Local Graph Clustering with Noisy Labels</h3>
<p><a href='https://openreview.net/forum?id=89A5c6enfc'>https://openreview.net/forum?id=89A5c6enfc</a></p>
<p><b>Compressor summary</b>: The paper proposes using noisy node labels as a proxy for additional information to improve fast local graph clustering methods.</p><hr><h3>CPPO: Continual Learning for Reinforcement Learning with Human Feedback</h3>
<p><a href='https://openreview.net/forum?id=86zAUE80pP'>https://openreview.net/forum?id=86zAUE80pP</a></p>
<p><b>Compressor summary</b>: CPPO is a method for continually aligning language models with human preferences using reinforcement learning, without needing complete retraining.</p><hr><h3>CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models</h3>
<p><a href='https://openreview.net/forum?id=86NGO8qeWs'>https://openreview.net/forum?id=86NGO8qeWs</a></p>
<p><b>Compressor summary</b>: The paper proposes CompA, a collection of benchmarks to evaluate compositional reasoning in audio-language models (ALMs), and presents CompA-CLAP, an improved version of CLAP that performs better on these benchmarks.</p><hr><h3>Detecting, Explaining, and Mitigating Memorization in Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=84n3UwkH7b'>https://openreview.net/forum?id=84n3UwkH7b</a></p>
<p><b>Compressor summary</b>: The authors present a method to detect and prevent image generation models from memorizing and replicating training data, which can help avoid legal issues and improve creativity.</p><hr><h3>FreeDyG: Frequency Enhanced Continuous-Time Dynamic Graph Model for Link Prediction</h3>
<p><a href='https://openreview.net/forum?id=82Mc5ilInM'>https://openreview.net/forum?id=82Mc5ilInM</a></p>
<p><b>Compressor summary</b>: FreeDyG is a novel method for link prediction in dynamic graphs that uses frequency domain analysis to capture node interaction patterns, improving performance over existing methods.</p><hr><h3>Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces</h3>
<p><a href='https://openreview.net/forum?id=80wh3jjCZf'>https://openreview.net/forum?id=80wh3jjCZf</a></p>
<p><b>Compressor summary</b>: The paper proposes Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm for handling large and structured discrete action spaces in reinforcement learning, which is more efficient and scalable than existing approaches.</p><hr><h3>Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning</h3>
<p><a href='https://openreview.net/forum?id=7zY781bMDO'>https://openreview.net/forum?id=7zY781bMDO</a></p>
<p><b>Compressor summary</b>: The paper proposes return-conditioned supervised learning (RCSL) as an improved off-policy technique for sequential decision-making, addressing convergence issues and enabling learning from sub-optimal datasets using trajectory stitching.</p><hr><h3>Cauchy-Schwarz Divergence Information Bottleneck for Regression</h3>
<p><a href='https://openreview.net/forum?id=7wY67ZDQTE'>https://openreview.net/forum?id=7wY67ZDQTE</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for improving deep neural networks using the information bottleneck principle, which leverages the Cauchy-Schwarz divergence to avoid MSE-based regression and achieve better generalization, robustness, and trade-off between prediction accuracy and compression.</p><hr><h3>Rethinking and Extending the Probabilistic Inference Capacity of GNNs</h3>
<p><a href='https://openreview.net/forum?id=7vVWiCrFnd'>https://openreview.net/forum?id=7vVWiCrFnd</a></p>
<p><b>Compressor summary</b>: This paper explores how graph neural networks (GNNs) learn from probabilistic graphical models, introduces a hierarchical framework to analyze their performance, and proposes novel methods to improve their expressive power.</p><hr><h3>Heterogeneous Personalized Federated Learning by Local-Global Updates Mixing via Convergence Rate</h3>
<p><a href='https://openreview.net/forum?id=7pWRLDBAtc'>https://openreview.net/forum?id=7pWRLDBAtc</a></p>
<p><b>Compressor summary</b>: LG-Mix uses NTK-based convergence to mix local and global updates based on their importance, improving personalized federated learning performance across various datasets.</p><hr><h3>TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting</h3>
<p><a href='https://openreview.net/forum?id=7oLshfEIC2'>https://openreview.net/forum?id=7oLshfEIC2</a></p>
<p><b>Compressor summary</b>: TimeMixer uses a novel multiscale-mixing approach to disentangle complex temporal variations in time series, achieving state-of-the-art forecasting performance with efficient run-time.</p><hr><h3>Continuous-Multiple Image Outpainting in One-Step via Positional Query and A Diffusion-based Approach</h3>
<p><a href='https://openreview.net/forum?id=7hxoYxKDTV'>https://openreview.net/forum?id=7hxoYxKDTV</a></p>
<p><b>Compressor summary</b>: The paper proposes PQDiff, a novel image outpainting method that can generate content beyond the input boundaries with arbitrary and continuous multiples in a single step, without relying on a pre-trained backbone network.</p><hr><h3>EQA-MX: Embodied Question Answering using Multimodal Expression</h3>
<p><a href='https://openreview.net/forum?id=7gUrYE50Rb'>https://openreview.net/forum?id=7gUrYE50Rb</a></p>
<p><b>Compressor summary</b>: The paper introduces new embodied question answering tasks and a multimodal representation learning model that improves performance on them.</p><hr><h3>Proper Laplacian Representation Learning</h3>
<p><a href='https://openreview.net/forum?id=7gLfQT52Nn'>https://openreview.net/forum?id=7gLfQT52Nn</a></p>
<p><b>Compressor summary</b>: The paper introduces a new objective and algorithm for approximating the Laplacian representation in reinforcement learning, which improves exploration, generalization, and transfer by accurately recovering true eigenvectors and eigenvalues.</p><hr><h3>Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations</h3>
<p><a href='https://openreview.net/forum?id=7gDENzTzw1'>https://openreview.net/forum?id=7gDENzTzw1</a></p>
<p><b>Compressor summary</b>: The text introduces a new robust reinforcement learning algorithm that protects against state perturbations by deriving a pessimistic policy, inferring belief states, and purifying state distributions.</p><hr><h3>SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition</h3>
<p><a href='https://openreview.net/forum?id=7etoNfU9uF'>https://openreview.net/forum?id=7etoNfU9uF</a></p>
<p><b>Compressor summary</b>: SpikePoint is a novel point-based SNN architecture that effectively processes sparse event cloud data, achieving state-of-the-art performance in event-based action recognition with low power consumption and few parameters.</p><hr><h3>Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks</h3>
<p><a href='https://openreview.net/forum?id=7erlRDoaV8'>https://openreview.net/forum?id=7erlRDoaV8</a></p>
<p><b>Compressor summary</b>: The authors propose an attack-and-defense framework for deleting sensitive information directly from pretrained language model weights, but find that existing methods are vulnerable to extraction attacks.</p><hr><h3>Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization</h3>
<p><a href='https://openreview.net/forum?id=7avlrpzWqo'>https://openreview.net/forum?id=7avlrpzWqo</a></p>
<p><b>Compressor summary</b>: The paper presents a new aggregation system for distributed deep learning that uses Beta densities and optimization techniques to handle failures and data augmentation, and shows improved performance in various tasks.</p><hr><h3>You Only Query Once: An Efficient Label-Only Membership Inference Attack</h3>
<p><a href='https://openreview.net/forum?id=7WsivwyHrS'>https://openreview.net/forum?id=7WsivwyHrS</a></p>
<p><b>Compressor summary</b>: YOQO is a new label-only attack method for membership inference that can reduce the query budget to just one and achieve high accuracy while being resistant to common defenses.</p><hr><h3>Human Feedback is not Gold Standard</h3>
<p><a href='https://openreview.net/forum?id=7W3GLNImfS'>https://openreview.net/forum?id=7W3GLNImfS</a></p>
<p><b>Compressor summary</b>: The text discusses the limitations and biases of using human feedback for evaluating and training large language models, especially regarding factuality errors and assertiveness.</p><hr><h3>Provable Compositional Generalization for Object-Centric Learning</h3>
<p><a href='https://openreview.net/forum?id=7VPTUWkiDQ'>https://openreview.net/forum?id=7VPTUWkiDQ</a></p>
<p><b>Compressor summary</b>: The paper studies when object-centric representations, learned by specific autoencoders with consistent encoder-decoder structures, can generalize to novel compositions of known concepts, using identifiability theory and experiments.</p><hr><h3>Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design</h3>
<p><a href='https://openreview.net/forum?id=7UhxsmbdaQ'>https://openreview.net/forum?id=7UhxsmbdaQ</a></p>
<p><b>Compressor summary</b>: The paper proposes Beam Enumeration, which enhances generative molecular design by explaining substructures and improving sample efficiency using reinforcement learning.</p><hr><h3>BooookScore: A systematic exploration of book-length summarization in the era of LLMs</h3>
<p><a href='https://openreview.net/forum?id=7Ttk3RzDeu'>https://openreview.net/forum?id=7Ttk3RzDeu</a></p>
<p><b>Compressor summary</b>: The paper investigates how to improve book summarization by LLMs using human and automatic evaluation methods, introducing BooookScore as a metric for coherence errors, and comparing different prompting workflows and LLM models.</p><hr><h3>Removing Biases from Molecular Representations via Information Maximization</h3>
<p><a href='https://openreview.net/forum?id=7TOs9gjAg1'>https://openreview.net/forum?id=7TOs9gjAg1</a></p>
<p><b>Compressor summary</b>: InfoCORE is a method to remove batch effects and improve drug screening data analysis by maximizing information and adaptively reweighting samples.</p><hr><h3>Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs</h3>
<p><a href='https://openreview.net/forum?id=7QI7tVrh2c'>https://openreview.net/forum?id=7QI7tVrh2c</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to improve the neural network approximation of partial differential equations by optimizing both the model and the random samples in the training set using a deep generative model.</p><hr><h3>Data-independent Module-aware Pruning for Hierarchical Vision Transformers</h3>
<p><a href='https://openreview.net/forum?id=7Ol6foUi1G'>https://openreview.net/forum?id=7Ol6foUi1G</a></p>
<p><b>Compressor summary</b>: DIMAP is a pruning method for hierarchical vision transformers that considers local self-attention and weight distribution, achieving efficient compression without sacrificing performance.</p><hr><h3>Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization</h3>
<p><a href='https://openreview.net/forum?id=7NzgkEdGyr'>https://openreview.net/forum?id=7NzgkEdGyr</a></p>
<p><b>Compressor summary</b>: The paper introduces Orthogonal Butterfly, a parameter-efficient way to adapt large models to downstream tasks using orthogonal matrices and butterfly structures.</p><hr><h3>Online Continual Learning for Interactive Instruction Following Agents</h3>
<p><a href='https://openreview.net/forum?id=7M0EzjugaN'>https://openreview.net/forum?id=7M0EzjugaN</a></p>
<p><b>Compressor summary</b>: The text proposes realistic continuous learning scenarios for embodied agents and introduces CAMA, a new method that updates information based on confidence scores without task boundary information, which performs better than previous methods in the proposed setups.</p><hr><h3>A Foundation Model for Error Correction Codes</h3>
<p><a href='https://openreview.net/forum?id=7KDuQPrAF3'>https://openreview.net/forum?id=7KDuQPrAF3</a></p>
<p><b>Compressor summary</b>: The paper introduces a foundation model for Error Correction Codes that uses a modified Transformer architecture to learn from multiple codes and adapt to unseen ones, achieving similar or better results than specialized code-specific models.</p><hr><h3>Generating Stealthy Jailbreak Prompts on Aligned Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=7Jwpw4qKkb'>https://openreview.net/forum?id=7Jwpw4qKkb</a></p>
<p><b>Compressor summary</b>: AutoDAN is a novel jailbreak attack against aligned LLMs that generates stealthy prompts using a hierarchical genetic algorithm, overcoming scalability and stealthiness issues in existing techniques.</p><hr><h3>STREAM: Spatio-TempoRal Evaluation and  Analysis Metric for Video Generative Models</h3>
<p><a href='https://openreview.net/forum?id=7JfKCZQPxJ'>https://openreview.net/forum?id=7JfKCZQPxJ</a></p>
<p><b>Compressor summary</b>: The paper introduces a new video evaluation metric called STREAM, which independently assesses the spatial and temporal aspects of videos, addressing the limitations of existing metrics like Frechet Video Distance (FVD) and providing insights for improving video generative models.</p><hr><h3>SLiMe: Segment Like Me</h3>
<p><a href='https://openreview.net/forum?id=7FeIRqCedv'>https://openreview.net/forum?id=7FeIRqCedv</a></p>
<p><b>Compressor summary</b>: SLiMe is a method to segment images using vision-language models like Stable Diffusion, requiring only one annotated sample and achieving better performance than other methods.</p><hr><h3>Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis</h3>
<p><a href='https://openreview.net/forum?id=7ERQPyR2eb'>https://openreview.net/forum?id=7ERQPyR2eb</a></p>
<p><b>Compressor summary</b>: Real3D-Portrait is a framework for generating realistic talking portrait videos from unseen images, using 3D reconstruction, motion-conditioned animation, and natural torso and background synthesis.</p><hr><h3>Elastic Feature Consolidation For Cold Start Exemplar-Free Incremental Learning</h3>
<p><a href='https://openreview.net/forum?id=7D9X2cFnt1'>https://openreview.net/forum?id=7D9X2cFnt1</a></p>
<p><b>Compressor summary</b>: Elastic Feature Consolidation (EFC) is a method for learning from sequential tasks without previous data that reduces feature drift and improves performance on image classification tasks.</p><hr><h3>Designing Skill-Compatible AI: Methodologies and Frameworks in Chess</h3>
<p><a href='https://openreview.net/forum?id=79rfgv3jw4'>https://openreview.net/forum?id=79rfgv3jw4</a></p>
<p><b>Compressor summary</b>: The paper presents a framework for creating and evaluating AI agents that can collaborate with less-skilled partners in complex settings, using chess variants as models. It shows that skill-compatible AI agents outperform conventional chess AI despite being weaker individually.</p><hr><h3>A Characterization Theorem for Equivariant Networks with Point-wise Activations</h3>
<p><a href='https://openreview.net/forum?id=79FVDdfoSR'>https://openreview.net/forum?id=79FVDdfoSR</a></p>
<p><b>Compressor summary</b>: Equivariant neural networks can be improved by using specific point-wise activations and coordinate systems, which this paper characterizes for general symmetries and applications to graph and image analysis.</p><hr><h3>Mirage: Model-agnostic Graph Distillation for Graph Classification</h3>
<p><a href='https://openreview.net/forum?id=78iGZdqxYY'>https://openreview.net/forum?id=78iGZdqxYY</a></p>
<p><b>Compressor summary</b>: MIRAGE is a novel graph distillation algorithm that compresses computation data in message-passing GNNs to create a smaller synthetic training set without compromising model performance or requiring full dataset training.</p><hr><h3>Exploring the Common Appearance-Boundary Adaptation for Nighttime Optical Flow</h3>
<p><a href='https://openreview.net/forum?id=776lhoaulC'>https://openreview.net/forum?id=776lhoaulC</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method for nighttime optical flow that adapts appearance and boundary features from daytime and event domains using a common latent space.</p><hr><h3>RetroBridge: Modeling Retrosynthesis with Markov Bridges</h3>
<p><a href='https://openreview.net/forum?id=770DetV8He'>https://openreview.net/forum?id=770DetV8He</a></p>
<p><b>Compressor summary</b>: The Markov Bridge Model is a new generative framework for single-step retrosynthesis planning in chemistry that uses Markov processes to approximate intractable discrete distributions and does not require tractable noise distributions as sampling proxies.</p><hr><h3>Continuous Invariance Learning</h3>
<p><a href='https://openreview.net/forum?id=70IgE3tRbu'>https://openreview.net/forum?id=70IgE3tRbu</a></p>
<p><b>Compressor summary</b>: The paper introduces CIL, a new method for learning invariant features in continuous domain problems, which improves generalization and performance over existing techniques.</p><hr><h3>Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback</h3>
<p><a href='https://openreview.net/forum?id=6yv8UHVJn4'>https://openreview.net/forum?id=6yv8UHVJn4</a></p>
<p><b>Compressor summary</b>: The paper introduces two algorithms for online reinforcement learning in adversarial settings that outperform existing methods in terms of regret performance.</p><hr><h3>Chain of Hindsight aligns Language Models with Feedback</h3>
<p><a href='https://openreview.net/forum?id=6xfe4IVcOu'>https://openreview.net/forum?id=6xfe4IVcOu</a></p>
<p><b>Compressor summary</b>: The paper proposes a new technique, Chain of Hindsight, that enables language models to learn from any feedback by converting it into sentences and fine-tuning the model on sequences of model generations paired with feedback, improving alignment with human preferences and performance on summarization and dialogue tasks.</p><hr><h3>ImplicitSLIM and How it Improves Embedding-based Collaborative Filtering</h3>
<p><a href='https://openreview.net/forum?id=6vF0ZJGor4'>https://openreview.net/forum?id=6vF0ZJGor4</a></p>
<p><b>Compressor summary</b>: ImplicitSLIM is an unsupervised learning method for sparse high-dimensional data that enhances embedding-based models by extracting embeddings from SLIM-like models, improving performance and speeding up convergence in collaborative filtering applications.</p><hr><h3>Towards Establishing Guaranteed Error for Learned Database Operations</h3>
<p><a href='https://openreview.net/forum?id=6tqgL8VluV'>https://openreview.net/forum?id=6tqgL8VluV</a></p>
<p><b>Compressor summary</b>: The paper studies the necessary conditions for providing error guarantees in machine learning-based database operations, such as indexing, cardinality estimation, and range-sum estimation, by presenting lower bounds on model size.</p><hr><h3>Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations</h3>
<p><a href='https://openreview.net/forum?id=6pPYRXKPpw'>https://openreview.net/forum?id=6pPYRXKPpw</a></p>
<p><b>Compressor summary</b>: The text introduces simulation environments and datasets to evaluate imitation learning models' ability to learn diverse multi-modal human behaviors using tractable metrics and benchmarks existing methods.</p><hr><h3>A Semantic Invariant Robust Watermark for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=6p8lpe4MNf'>https://openreview.net/forum?id=6p8lpe4MNf</a></p>
<p><b>Compressor summary</b>: The paper proposes a semantic invariant watermarking method for large language models that can resist attacks based on synonym substitution and text paraphrasing while maintaining security robustness.</p><hr><h3>Large Language Model Cascades with Mixture of Thought Representations for Cost-Efficient Reasoning</h3>
<p><a href='https://openreview.net/forum?id=6okaSfANzh'>https://openreview.net/forum?id=6okaSfANzh</a></p>
<p><b>Compressor summary</b>: The paper presents a cost-saving cascade pipeline for reasoning tasks using LLMs, which switches between cheaper and more expensive models based on answer consistency.</p><hr><h3>Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=6mLjDwYte5'>https://openreview.net/forum?id=6mLjDwYte5</a></p>
<p><b>Compressor summary</b>: The paper shows that combining sparse mixture-of-experts and instruction tuning improves the performance of large language models with fewer parameters and computational cost.</p><hr><h3>STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction</h3>
<p><a href='https://openreview.net/forum?id=6iwg437CZs'>https://openreview.net/forum?id=6iwg437CZs</a></p>
<p><b>Compressor summary</b>: STanHop-Net is a novel neural network that uses sparse temporal and cross-series representations learned by tandem Hopfield layers and enhanced with external memory modules for fast response to events in multivariate time series prediction.</p><hr><h3>Learning to solve Class-Constrained Bin Packing Problems via Encoder-Decoder Model</h3>
<p><a href='https://openreview.net/forum?id=6hvtSLkKeZ'>https://openreview.net/forum?id=6hvtSLkKeZ</a></p>
<p><b>Compressor summary</b>: The paper introduces CCBPP, a vector Bin Packing Problem variant that considers item classes and sizes, and proposes an Encoder-Decoder Model with GCN and Cluster Decode to solve it efficiently.</p><hr><h3>Local Search GFlowNets</h3>
<p><a href='https://openreview.net/forum?id=6cFcw1Rxww'>https://openreview.net/forum?id=6cFcw1Rxww</a></p>
<p><b>Compressor summary</b>: This paper introduces local search for GFlowNets, a method that improves diverse sample generation by exploiting high rewarded solutions using destruction and reconstruction guided by backward and forward policies.</p><hr><h3>Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models</h3>
<p><a href='https://openreview.net/forum?id=6bcAD6g688'>https://openreview.net/forum?id=6bcAD6g688</a></p>
<p><b>Compressor summary</b>: The study introduces a framework to evaluate and fix label errors in datasets used for training harmless language models, improving their performance and safety.</p><hr><h3>IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs</h3>
<p><a href='https://openreview.net/forum?id=6RR3wU4mSZ'>https://openreview.net/forum?id=6RR3wU4mSZ</a></p>
<p><b>Compressor summary</b>: Our method speeds up self-attention in pretrained transformer models for long sequences without retraining or hardware changes.</p><hr><h3>LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=6PmJoRfdaK'>https://openreview.net/forum?id=6PmJoRfdaK</a></p>
<p><b>Compressor summary</b>: LongLoRA is a method to efficiently extend the context sizes of pre-trained language models by using sparse local attention during fine-tuning and parameter-efficient context expansion.</p><hr><h3>Dynamic Discounted Counterfactual Regret Minimization</h3>
<p><a href='https://openreview.net/forum?id=6PbvbLyqT6'>https://openreview.net/forum?id=6PbvbLyqT6</a></p>
<p><b>Compressor summary</b>: DDCFR is a new algorithm for solving imperfect-information games that learns how to weight each iteration dynamically, improving convergence rate and performance.</p><hr><h3>Beyond Linear Spherical Interpolation: Noise Correction for Image Interpolation with Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=6O3Q6AFUTu'>https://openreview.net/forum?id=6O3Q6AFUTu</a></p>
<p><b>Compressor summary</b>: The paper proposes a new interpolation method that combines noise addition and diffusion models to create better image results and handle various types of images.</p><hr><h3>Graphical Multioutput Gaussian Process with Attention</h3>
<p><a href='https://openreview.net/forum?id=6N8TW504aa'>https://openreview.net/forum?id=6N8TW504aa</a></p>
<p><b>Compressor summary</b>: The paper introduces GMOGP, a novel framework for multi-output regression that improves flexibility, optimality, and scalability over existing MOGP methods.</p><hr><h3>SaProt: Protein Language Modeling with Structure-aware Vocabulary</h3>
<p><a href='https://openreview.net/forum?id=6MRm3G4NiU'>https://openreview.net/forum?id=6MRm3G4NiU</a></p>
<p><b>Compressor summary</b>: The authors introduce SaProt, a protein language model that incorporates 3D structure information, improving performance on various downstream tasks.</p><hr><h3>UniTabE: A Universal Pretraining Protocol for Tabular Foundation  Model in Data Science</h3>
<p><a href='https://openreview.net/forum?id=6LLho5X6xV'>https://openreview.net/forum?id=6LLho5X6xV</a></p>
<p><b>Compressor summary</b>: UniTabE is a new method that pretrains models on tables and uses Transformer encoders to improve their performance in data science tasks.</p><hr><h3>Principled Federated Domain Adaptation: Gradient Projection and Auto-Weighting</h3>
<p><a href='https://openreview.net/forum?id=6J3ehSUrMU'>https://openreview.net/forum?id=6J3ehSUrMU</a></p>
<p><b>Compressor summary</b>: The text discusses Federated Domain Adaptation, a challenging problem in federated learning where source and target clients have different domains and limited data, and proposes a new aggregation rule ($	exttt{FedGP}$) and an auto-weighting scheme to improve performance.</p><hr><h3>Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training</h3>
<p><a href='https://openreview.net/forum?id=6IjN7oxjXt'>https://openreview.net/forum?id=6IjN7oxjXt</a></p>
<p><b>Compressor summary</b>: Adversarial training improves neural network robustness but can cause trade-offs; CURE is a new framework that selectively updates layers to balance robustness and generalization while mitigating "robust overfitting".</p><hr><h3>Personalize Segment Anything Model with One Shot</h3>
<p><a href='https://openreview.net/forum?id=6Gzkhoc6YS'>https://openreview.net/forum?id=6Gzkhoc6YS</a></p>
<p><b>Compressor summary</b>: PerSAM is a method to customize SAM, a powerful segmentation framework, for specific visual concepts using one-shot data and without any training, improving performance on various benchmarks and enabling personalized text-to-image synthesis.</p><hr><h3>Learning Reusable Dense Rewards for Multi-Stage Tasks</h3>
<p><a href='https://openreview.net/forum?id=6CZ50WgfCG'>https://openreview.net/forum?id=6CZ50WgfCG</a></p>
<p><b>Compressor summary</b>: DrS is a method for learning dense rewards from stages that can be reused in unseen tasks, improving RL performance and sample efficiency on physical robot manipulation tasks.</p><hr><h3>Two-timescale Extragradient for Finding Local Minimax Points</h3>
<p><a href='https://openreview.net/forum?id=6CIGhcJYJH'>https://openreview.net/forum?id=6CIGhcJYJH</a></p>
<p><b>Compressor summary</b>: Two-timescale extragradient efficiently optimizes minimax problems without assuming nondegenerate Hessians.</p><hr><h3>Enhancing Tail Performance in Extreme Classifiers by Label Variance Reduction</h3>
<p><a href='https://openreview.net/forum?id=6ARlSgun7J'>https://openreview.net/forum?id=6ARlSgun7J</a></p>
<p><b>Compressor summary</b>: This paper proposes a method to reduce label variance in extreme classification using a tail-robust teacher model and knowledge distillation, improving tail performance on challenging label sets.</p><hr><h3>DreamClean: Restoring Clean Image Using Deep Diffusion Prior</h3>
<p><a href='https://openreview.net/forum?id=6ALuy19mPa'>https://openreview.net/forum?id=6ALuy19mPa</a></p>
<p><b>Compressor summary</b>: DreamClean is a training-free method that restores high-quality images from diverse degradations by embedding the degraded image in the latent of pre-trained diffusion models and re-sampling it through a novel Variance Preservation Sampling technique.</p><hr><h3>Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models</h3>
<p><a href='https://openreview.net/forum?id=66arKkGiFy'>https://openreview.net/forum?id=66arKkGiFy</a></p>
<p><b>Compressor summary</b>: The paper analyzes the relationship between PnP-ULA, a Bayesian method for imaging inverse problems, and its data-fidelity and denoiser, providing an error bound and validating it numerically on several examples.</p><hr><h3>Compressed Context Memory for Online Language Model Interaction</h3>
<p><a href='https://openreview.net/forum?id=64kSvC4iPg'>https://openreview.net/forum?id=64kSvC4iPg</a></p>
<p><b>Compressor summary</b>: The paper proposes a compression method for online language models that uses a lightweight adapter to reduce memory and attention needs, enabling similar performance to full context models with much smaller memory spaces.</p><hr><h3>In-Context Learning Dynamics with Random Binary Sequences</h3>
<p><a href='https://openreview.net/forum?id=62K7mALO2q'>https://openreview.net/forum?id=62K7mALO2q</a></p>
<p><b>Compressor summary</b>: The study analyzes how large language models learn new tasks through in-context learning, revealing emergent abilities and underlying concepts using random binary sequences as context.</p><hr><h3>NeurRev: Train Better Sparse Neural Network Practically via Neuron Revitalization</h3>
<p><a href='https://openreview.net/forum?id=60lNoatp7u'>https://openreview.net/forum?id=60lNoatp7u</a></p>
<p><b>Compressor summary</b>: NeurRev is a framework for dynamic sparse training that awakens dormant neurons by pruning and reduces memory consumption on resource-constrained platforms.</p><hr><h3>Scale-Adaptive Diffusion Model for Complex Sketch Synthesis</h3>
<p><a href='https://openreview.net/forum?id=5xadJmgwix'>https://openreview.net/forum?id=5xadJmgwix</a></p>
<p><b>Compressor summary</b>: The paper presents a new way to use diffusion models for generating realistic and complex sketches from pixels, using guidance from classifiers and a special sampling strategy.</p><hr><h3>Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=5tGGWOijvq'>https://openreview.net/forum?id=5tGGWOijvq</a></p>
<p><b>Compressor summary</b>: The paper proposes Prompt Risk Control, a lightweight framework for selecting language model prompts based on rigorous risk bounds, to avoid high loss in deployment.</p><hr><h3>Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances</h3>
<p><a href='https://openreview.net/forum?id=5t57omGVMw'>https://openreview.net/forum?id=5t57omGVMw</a></p>
<p><b>Compressor summary</b>: The paper proposes a bandit algorithm that can learn optimal parameters for Successive Over-Relaxation (SOR), a linear system solver, without extra matrix computations and shows its effectiveness in speeding up numerical simulations.</p><hr><h3>Pose Modulated Avatars from Video</h3>
<p><a href='https://openreview.net/forum?id=5t44vPlv9x'>https://openreview.net/forum?id=5t44vPlv9x</a></p>
<p><b>Compressor summary</b>: The paper proposes a two-branch neural network that adapts and encodes body part correlations and global frequencies to reconstruct dynamic human motion and shape with high detail and generalization.</p><hr><h3>Robust Angular Synchronization via Directed Graph Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=5sjxMwWmk8'>https://openreview.net/forum?id=5sjxMwWmk8</a></p>
<p><b>Compressor summary</b>: The paper proposes GNNSync, a framework using neural networks to estimate unknown angles from noisy measurements in applications like sensor network localization and clock synchronization.</p><hr><h3>Whittle Index with Multiple Actions and State Constraint for Inventory Management</h3>
<p><a href='https://openreview.net/forum?id=5sixirvG0I'>https://openreview.net/forum?id=5sixirvG0I</a></p>
<p><b>Compressor summary</b>: The paper proposes a deep reinforcement learning algorithm based on the Whittle index for multi-agent inventory management, which performs better than existing methods on large-scale problems.</p><hr><h3>Goodhart's Law in Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=5o9G4XF1LI'>https://openreview.net/forum?id=5o9G4XF1LI</a></p>
<p><b>Compressor summary</b>: The paper discusses how imperfect reward functions in reinforcement learning can lead to suboptimal outcomes and proposes methods to address this issue, including optimal early stopping and worst-case reward maximization.</p><hr><h3>Linear Log-Normal Attention with Unbiased Concentration</h3>
<p><a href='https://openreview.net/forum?id=5nM2AHzqUj'>https://openreview.net/forum?id=5nM2AHzqUj</a></p>
<p><b>Compressor summary</b>: The authors propose and test a new self-attention mechanism called Linear Log-Normal Attention that improves the scalability of transformer models by emulating the original attention's distribution and concentration properties.</p><hr><h3>Time-Efficient Reinforcement Learning with Stochastic Stateful Policies</h3>
<p><a href='https://openreview.net/forum?id=5liV2xUdJL'>https://openreview.net/forum?id=5liV2xUdJL</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for training stateful policies in reinforcement learning using a stochastic internal state kernel and a stateless policy, which improves speed, simplicity, and scalability over Backpropagation Through Time.</p><hr><h3>Jointly Training Large Autoregressive Multimodal Models</h3>
<p><a href='https://openreview.net/forum?id=5jcav5RcKw'>https://openreview.net/forum?id=5jcav5RcKw</a></p>
<p><b>Compressor summary</b>: The JAM framework combines text and image models to produce seamless multimodal outputs using a data-efficient instruction-tuning strategy.</p><hr><h3>Some Intriguing Aspects about Lipschitz Continuity of Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=5jWsW08zUh'>https://openreview.net/forum?id=5jWsW08zUh</a></p>
<p><b>Compressor summary</b>: This paper investigates how different factors affect the Lipschitz continuity of neural networks, which is important for their robustness, generalization, and adversarial resistance.</p><hr><h3>Interpreting and Controlling Vision Foundation Models via Text Explanations</h3>
<p><a href='https://openreview.net/forum?id=5iENGLEJKG'>https://openreview.net/forum?id=5iENGLEJKG</a></p>
<p><b>Compressor summary</b>: The authors propose a method to interpret and edit vision transformers using natural language explanations of their latent tokens.</p><hr><h3>Towards Robust Offline Reinforcement Learning under Diverse Data Corruption</h3>
<p><a href='https://openreview.net/forum?id=5hAMmCU0bK'>https://openreview.net/forum?id=5hAMmCU0bK</a></p>
<p><b>Compressor summary</b>: Robust IQL is a new offline reinforcement learning approach that uses Huber loss and quantile estimators to handle various types of data corruption, improving upon the resilience of implicit Q-learning.</p><hr><h3>MiniLLM: Knowledge Distillation of Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=5h0qf7IBZZ'>https://openreview.net/forum?id=5h0qf7IBZZ</a></p>
<p><b>Compressor summary</b>: The paper proposes MiniLLM, a technique that distills smaller language models from generative larger ones using reverse KLD objective and improves their performance in instruction-following tasks.</p><hr><h3>Contextual Bandits with Online Neural Regression</h3>
<p><a href='https://openreview.net/forum?id=5ep85sakT3'>https://openreview.net/forum?id=5ep85sakT3</a></p>
<p><b>Compressor summary</b>: The paper proposes a neural network approach for Neural Contextual Bandits and shows improved regret bounds for online regression with almost convex losses that satisfy the Quadratic Growth condition.</p><hr><h3>Structured Video-Language Modeling with Temporal Grouping and Spatial Grounding</h3>
<p><a href='https://openreview.net/forum?id=5dlfiJIXoh'>https://openreview.net/forum?id=5dlfiJIXoh</a></p>
<p><b>Compressor summary</b>: S-ViLM is a video-language modeling framework that enhances fine-grained information in videos and captions, improving performance on various downstream tasks.</p><hr><h3>Certified Adversarial Robustness for Rate Encoded Spiking Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=5bNYf0CqxY'>https://openreview.net/forum?id=5bNYf0CqxY</a></p>
<p><b>Compressor summary</b>: The text introduces a new technique to improve the robustness of spiking neural networks against adversarial perturbation of inputs by using rate-encoded models and randomized smoothing.</p><hr><h3>ControlVideo: Training-free Controllable Text-to-video Generation</h3>
<p><a href='https://openreview.net/forum?id=5a79AqFr0c'>https://openreview.net/forum?id=5a79AqFr0c</a></p>
<p><b>Compressor summary</b>: ControlVideo is a text-to-video model that uses a pre-trained image generator and frame smoothing techniques to produce high-quality, controllable videos from text prompts and motion sequences, without the need for extensive training.</p><hr><h3>Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network</h3>
<p><a href='https://openreview.net/forum?id=5RielfrDkP'>https://openreview.net/forum?id=5RielfrDkP</a></p>
<p><b>Compressor summary</b>: The Multiresolution Meta-Framelet-based Graph Convolutional Network (MM-FGCN) is a new method to adaptively learn multiresolution representations of graphs for different tasks, using meta-learning and framelets.</p><hr><h3>TEDDY: Trimming Edges with Degree-based Graph Diffusion Strategy</h3>
<p><a href='https://openreview.net/forum?id=5RUf9nEdyC'>https://openreview.net/forum?id=5RUf9nEdyC</a></p>
<p><b>Compressor summary</b>: TEDDY is a framework for finding sparse graph lottery tickets by exploiting edge-degree information and achieving better performance than iterative approaches.</p><hr><h3>PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization</h3>
<p><a href='https://openreview.net/forum?id=5Nn2BLV7SB'>https://openreview.net/forum?id=5Nn2BLV7SB</a></p>
<p><b>Compressor summary</b>: PandaLM is a large language model that can judge other LLMs based on various factors like conciseness and clarity, ensuring fairer and cost-effective hyperparameter tuning without privacy risks.</p><hr><h3>Multiscale Positive-Unlabeled Detection of AI-Generated Texts</h3>
<p><a href='https://openreview.net/forum?id=5Lp6qU9hzV'>https://openreview.net/forum?id=5Lp6qU9hzV</a></p>
<p><b>Compressor summary</b>: The paper proposes a Multiscale Positive-Unlabeled (MPU) training framework to improve the detection of AI-generated texts, especially on short texts like SMSes and Tweets.</p><hr><h3>MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning</h3>
<p><a href='https://openreview.net/forum?id=5KojubHBr8'>https://openreview.net/forum?id=5KojubHBr8</a></p>
<p><b>Compressor summary</b>: MMICL is a new approach that enhances vision-language models' ability to understand complex multi-modal prompts using in-context learning and a large dataset, improving their performance on various tasks.</p><hr><h3>Implicit Neural Representation Inference for Low-Dimensional Bayesian Deep Learning</h3>
<p><a href='https://openreview.net/forum?id=5KUiMKRebi'>https://openreview.net/forum?id=5KUiMKRebi</a></p>
<p><b>Compressor summary</b>: Our approach combines deterministic and probabilistic components in deep learning models to achieve accurate predictions with well-calibrated uncertainty estimates, outperforming existing methods.</p><hr><h3>An Intuitive Multi-Frequency Feature Representation for SO(3)-Equivariant Networks</h3>
<p><a href='https://openreview.net/forum?id=5JWAOLBxwp'>https://openreview.net/forum?id=5JWAOLBxwp</a></p>
<p><b>Compressor summary</b>: The authors propose a new feature representation for 3D vision algorithms that improves the performance of Vector Neuron by capturing more details in 3D data.</p><hr><h3>When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method</h3>
<p><a href='https://openreview.net/forum?id=5HCnKDeTws'>https://openreview.net/forum?id=5HCnKDeTws</a></p>
<p><b>Compressor summary</b>: The paper studies how different factors like model size, data size, and tuning methods affect the performance of large language models in various tasks, finding that they follow a power-based scaling law and have task- and data-dependent optimal methods.</p><hr><h3>Tailoring Retrieval Representations to Long-term Visual Localization</h3>
<p><a href='https://openreview.net/forum?id=5EniAcsO7f'>https://openreview.net/forum?id=5EniAcsO7f</a></p>
<p><b>Compressor summary</b>: The paper proposes improving visual localization by expanding the training set with synthetic images generated from text-to-image models and using a tailored training approach.</p><hr><h3>A Theoretical Explanation of Deep RL Performance in Stochastic Environments</h3>
<p><a href='https://openreview.net/forum?id=5ES5Hdlbxw'>https://openreview.net/forum?id=5ES5Hdlbxw</a></p>
<p><b>Compressor summary</b>: The paper explains deep RL's success using a new algorithm (SQIRL) that separates exploration and learning, showing that random exploration works well and deep function approximators generalize effectively in many stochastic MDPs.</p><hr><h3>Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings</h3>
<p><a href='https://openreview.net/forum?id=5Dwqu5urzs'>https://openreview.net/forum?id=5Dwqu5urzs</a></p>
<p><b>Compressor summary</b>: The paper introduces a physics-regulated deep reinforcement learning framework for safe autonomous systems that integrates data-driven and model-based methods with invariant principles.</p><hr><h3>Interpreting CLIP's Image Representation via Text-Based Decomposition</h3>
<p><a href='https://openreview.net/forum?id=5Ca9sSzuDp'>https://openreview.net/forum?id=5Ca9sSzuDp</a></p>
<p><b>Compressor summary</b>: The paper analyzes how CLIP's image encoder works by breaking down its components, finding text representations for attention heads, and using this knowledge to improve a zero-shot segmentation model.</p><hr><h3>Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval</h3>
<p><a href='https://openreview.net/forum?id=5BXAXOpaWu'>https://openreview.net/forum?id=5BXAXOpaWu</a></p>
<p><b>Compressor summary</b>: ISA is a method to retrieve images based on a query image and text, using an adaptive token learner to map the image to a sentence, and an asymmetric structure for flexible deployment of large and small models.</p><hr><h3>Demystifying CLIP Data</h3>
<p><a href='https://openreview.net/forum?id=5BCFlnfE1g'>https://openreview.net/forum?id=5BCFlnfE1g</a></p>
<p><b>Compressor summary</b>: MetaCLIP is a method to create balanced datasets for language-image pre-training by using metadata derived from CLIP's concepts, outperforming CLIP's data on multiple benchmarks.</p><hr><h3>How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions</h3>
<p><a href='https://openreview.net/forum?id=567BjxgaTp'>https://openreview.net/forum?id=567BjxgaTp</a></p>
<p><b>Compressor summary</b>: The paper presents a simple and accurate lie detector for large language models that works without access to their internal workings or factual knowledge.</p><hr><h3>Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach</h3>
<p><a href='https://openreview.net/forum?id=55uj7mU7Cv'>https://openreview.net/forum?id=55uj7mU7Cv</a></p>
<p><b>Compressor summary</b>: The text discusses a study that introduces a theory to eliminate multiple translation functions and improve unsupervised domain translation by aligning conditional distributions over auxiliary variable-induced subsets of domains.</p><hr><h3>Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference for Recommendation</h3>
<p><a href='https://openreview.net/forum?id=52fz5sUAy2'>https://openreview.net/forum?id=52fz5sUAy2</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to address selection bias and neighborhood effects in recommender systems by formulating the problem as an interference issue and introducing a treatment representation for causal inference.</p><hr><h3>Habitat 3.0: A Co-Habitat for Humans, Avatars, and Robots</h3>
<p><a href='https://openreview.net/forum?id=4znwzG92CE'>https://openreview.net/forum?id=4znwzG92CE</a></p>
<p><b>Compressor summary</b>: Habitat 3.0 is a simulation platform that studies human-robot collaboration in home environments using accurate humanoid simulation, human-in-the-loop infrastructure, and collaborative tasks such as Social Navigation and Social Rearrangement.</p><hr><h3>Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical Variational Autoencoders</h3>
<p><a href='https://openreview.net/forum?id=4zZFGliCl9'>https://openreview.net/forum?id=4zZFGliCl9</a></p>
<p><b>Compressor summary</b>: The paper investigates the posterior collapse phenomenon in variational autoencoders (VAEs) and advances the theory for two less studied VAE variants, conditional VAEs and hierarchical VAEs, revealing their causes of posterior collapse.</p><hr><h3>Space and time continuous physics simulation from partial observations</h3>
<p><a href='https://openreview.net/forum?id=4yaFQ7181M'>https://openreview.net/forum?id=4yaFQ7181M</a></p>
<p><b>Compressor summary</b>: The authors propose a novel data-driven method for computational fluid dynamics that can predict solutions at arbitrary spatial and temporal locations using recurrent GNNs and spatio-temporal attention.</p><hr><h3>PAC Prediction Sets Under Label Shift</h3>
<p><a href='https://openreview.net/forum?id=4vPVBh3fhz'>https://openreview.net/forum?id=4vPVBh3fhz</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method for constructing prediction sets with guaranteed uncertainty quantification in the presence of label shift by estimating importance weights and propagating them through Gaussian elimination.</p><hr><h3>Learning 3D Particle-based Simulators from RGB-D Videos</h3>
<p><a href='https://openreview.net/forum?id=4rBEgZCubP'>https://openreview.net/forum?id=4rBEgZCubP</a></p>
<p><b>Compressor summary</b>: VPD is a method that learns a particle-based representation of 3D scenes, a neural simulator, and a renderer from RGB-D videos without privileged information, enabling scene editing and long-term predictions for various applications.</p><hr><h3>Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings</h3>
<p><a href='https://openreview.net/forum?id=4r2ybzJnmN'>https://openreview.net/forum?id=4r2ybzJnmN</a></p>
<p><b>Compressor summary</b>: The text proposes a new algorithm to learn delays in spiking neural networks, which improves their performance for temporal data processing tasks such as speech recognition.</p><hr><h3>DREAM: Dual Structured Exploration with Mixup for Open-set Graph Domain Adaption</h3>
<p><a href='https://openreview.net/forum?id=4olqbTBt1Y'>https://openreview.net/forum?id=4olqbTBt1Y</a></p>
<p><b>Compressor summary</b>: DREAM is a novel method that uses graph-level and subgraph-enhanced branches to explore both global and local structures in graphs for open-set domain adaptation, incorporating posterior regularization, mixup, and k nearest neighbor-based graph-of-graphs.</p><hr><h3>Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=4kLVvIh8cp'>https://openreview.net/forum?id=4kLVvIh8cp</a></p>
<p><b>Compressor summary</b>: The paper proposes PNLSVI, an oracle-efficient algorithm for offline RL with non-linear function approximation that has minimax optimal instance-dependent regret and a wide range of applicability.</p><hr><h3>Scalable Neural Network Kernels</h3>
<p><a href='https://openreview.net/forum?id=4iPw1klFWa'>https://openreview.net/forum?id=4iPw1klFWa</a></p>
<p><b>Compressor summary</b>: SNNKs are scalable neural network kernels that can replace regular feedforward layers, leading to more efficient and expressive models with reduced parameters and computational cost.</p><hr><h3>Diffusion-TS: Interpretable Diffusion for General Time Series Generation</h3>
<p><a href='https://openreview.net/forum?id=4h1apFjO99'>https://openreview.net/forum?id=4h1apFjO99</a></p>
<p><b>Compressor summary</b>: Diffusion-TS is a new DDPM-based framework that generates high-quality multivariate time series using an encoder-decoder Transformer with disentangled temporal representations, enabling conditional generation tasks and achieving state-of-the-art results.</p><hr><h3>The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry</h3>
<p><a href='https://openreview.net/forum?id=4g02l2N2Nx'>https://openreview.net/forum?id=4g02l2N2Nx</a></p>
<p><b>Compressor summary</b>: Hedgehog is a learnable linear attention that mimics softmax attention's properties and achieves near-linear complexity, improving Transformer efficiency and performance in various settings.</p><hr><h3>Language Model Detectors Are Easily Optimized Against</h3>
<p><a href='https://openreview.net/forum?id=4eJDMjYZZG'>https://openreview.net/forum?id=4eJDMjYZZG</a></p>
<p><b>Compressor summary</b>: The paper shows how to use reinforcement learning to fine-tune language models to evade existing detectors that try to identify if a text was written by a model or not.</p><hr><h3>Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory</h3>
<p><a href='https://openreview.net/forum?id=4bSQ3lsfEV'>https://openreview.net/forum?id=4bSQ3lsfEV</a></p>
<p><b>Compressor summary</b>: The paper presents a new concept of functionally equivalent features in neural networks, which are features that output the same value under certain transformations, and a metric for measuring their complexity. It also proposes an algorithm to prune networks based on FEFs, which can improve performance and reduce parameters. The experiments confirm the existence and effects of FEFs across different networks and tasks.</p><hr><h3>EMO: EARTH MOVER DISTANCE OPTIMIZATION FOR AUTO-REGRESSIVE LANGUAGE MODELING</h3>
<p><a href='https://openreview.net/forum?id=4bLXfRd0CX'>https://openreview.net/forum?id=4bLXfRd0CX</a></p>
<p><b>Compressor summary</b>: Earth Mover Distance Optimization (EMO) is a new method for improving language models by addressing issues with existing cross-entropy based approaches, leading to better performance and easier fine-tuning.</p><hr><h3>Tackling the Data Heterogeneity in Asynchronous Federated Learning with Cached Update Calibration</h3>
<p><a href='https://openreview.net/forum?id=4aywmeb97I'>https://openreview.net/forum?id=4aywmeb97I</a></p>
<p><b>Compressor summary</b>: Asynchronous federated learning can be slowed down by data heterogeneity, but a new method that caches and reuses client updates can improve its convergence under non-i.i.d. data.</p><hr><h3>Adaptive Instrument Design for Indirect Experiments</h3>
<p><a href='https://openreview.net/forum?id=4Zz5UELkIt'>https://openreview.net/forum?id=4Zz5UELkIt</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to adaptively design data collection policies for indirect experiments, using influence functions to minimize mean-squared error and improve sample efficiency.</p><hr><h3>NfgTransformer: Equivariant Representation Learning for Normal-form Games</h3>
<p><a href='https://openreview.net/forum?id=4YESQqIys7'>https://openreview.net/forum?id=4YESQqIys7</a></p>
<p><b>Compressor summary</b>: The paper introduces NfgTransformer, a neural network architecture for representing normal-form games and enabling deep learning systems to reason about strategic interaction concepts.</p><hr><h3>Llemma: An Open Language Model for Mathematics</h3>
<p><a href='https://openreview.net/forum?id=4WnqRR915j'>https://openreview.net/forum?id=4WnqRR915j</a></p>
<p><b>Compressor summary</b>: Llemma is a powerful pretrained language model for mathematics that surpasses existing models on the MATH benchmark and can perform theorem proving without finetuning.</p><hr><h3>Learning from Sparse Offline Datasets via Conservative Density Estimation</h3>
<p><a href='https://openreview.net/forum?id=4WM0OogPTx'>https://openreview.net/forum?id=4WM0OogPTx</a></p>
<p><b>Compressor summary</b>: The paper proposes Conservative Density Estimation (CDE), a novel algorithm for offline reinforcement learning that handles out-of-distribution errors by constraining state-action occupancy distributions, achieving state-of-the-art performance on the D4RL benchmark.</p><hr><h3>Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels</h3>
<p><a href='https://openreview.net/forum?id=4VgBjsOC8k'>https://openreview.net/forum?id=4VgBjsOC8k</a></p>
<p><b>Compressor summary</b>: This paper shows that depthwise-separable convolutional neural networks have trained filters that cluster into a few main patterns resembling biological vision systems, suggesting potential for more interpretable and biologically-inspired designs.</p><hr><h3>Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data</h3>
<p><a href='https://openreview.net/forum?id=4VIgNuQ1pY'>https://openreview.net/forum?id=4VIgNuQ1pY</a></p>
<p><b>Compressor summary</b>: The paper introduces stable Neural SDEs classes for handling irregular and incomplete time series data, and evaluates their performance on various tasks and datasets.</p><hr><h3>Towards Non-Asymptotic Convergence for Diffusion-Based Generative Models</h3>
<p><a href='https://openreview.net/forum?id=4VGEeER6W9'>https://openreview.net/forum?id=4VGEeER6W9</a></p>
<p><b>Compressor summary</b>: This paper develops non-asymptotic theory to analyze the data generation process of diffusion models in discrete time, with or without score function estimates.</p><hr><h3>Coordinate-Aware Modulation for Neural Fields</h3>
<p><a href='https://openreview.net/forum?id=4UiLqimGm5'>https://openreview.net/forum?id=4UiLqimGm5</a></p>
<p><b>Compressor summary</b>: The paper proposes Coordinate-Aware Modulation (CAM), which improves neural field representations by combining MLPs and grid methods, enhancing performance and stability across various signals.</p><hr><h3>Locality-Aware Graph Rewiring in GNNs</h3>
<p><a href='https://openreview.net/forum?id=4Ua4hKiAJX'>https://openreview.net/forum?id=4Ua4hKiAJX</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel graph rewiring framework for GNNs that balances over-squashing, locality, and sparsity, and shows its effectiveness on various benchmarks.</p><hr><h3>Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection</h3>
<p><a href='https://openreview.net/forum?id=4UIBysXjVq'>https://openreview.net/forum?id=4UIBysXjVq</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel spectral graph neural network (RQGNN) for detecting anomalies in graphs by using Rayleigh Quotient and Chebyshev Wavelet GNN with RQ-pooling, achieving better performance than existing methods.</p><hr><h3>DAM: A Foundation Model for Forecasting</h3>
<p><a href='https://openreview.net/forum?id=4NhMhElWqP'>https://openreview.net/forum?id=4NhMhElWqP</a></p>
<p><b>Compressor summary</b>: The DAM is a neural model that uses randomly sampled histories to produce an adjustable basis composition for universal forecasting of time series across domains and datasets, outperforming existing models on various metrics.</p><hr><h3>Weakly-supervised Audio Separation via Bi-modal Semantic Similarity</h3>
<p><a href='https://openreview.net/forum?id=4N97bz1sP6'>https://openreview.net/forum?id=4N97bz1sP6</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to separate single-source sounds in audio using language descriptions, improving unsupervised and supervised learning for audio separation.</p><hr><h3>Generative Adversarial Policy Network for Modelling Protein Complexes</h3>
<p><a href='https://openreview.net/forum?id=4MsfQ2H0lP'>https://openreview.net/forum?id=4MsfQ2H0lP</a></p>
<p><b>Compressor summary</b>: GAPN is a novel method that uses a generative adversarial policy network to predict the structure of large protein complexes efficiently and accurately.</p><hr><h3>Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding</h3>
<p><a href='https://openreview.net/forum?id=4L0xnS4GQM'>https://openreview.net/forum?id=4L0xnS4GQM</a></p>
<p><b>Compressor summary</b>: The Chain-of-Table framework uses tabular data as a proxy for intermediate thoughts in the reasoning chain to improve table understanding tasks with large language models.</p><hr><h3>Curiosity-driven Red-teaming for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=4KqkizXgXU'>https://openreview.net/forum?id=4KqkizXgXU</a></p>
<p><b>Compressor summary</b>: The paper introduces curiosity-driven exploration to train red team models, which generates diverse and effective test cases for large language models without relying solely on human testers.</p><hr><h3>Neurosymbolic Grounding for Compositional Generalization</h3>
<p><a href='https://openreview.net/forum?id=4KZpDGD4Nh'>https://openreview.net/forum?id=4KZpDGD4Nh</a></p>
<p><b>Compressor summary</b>: Cosmos is a framework that uses neurosymbolic grounding to create object-centric world models for compositional generalization tasks, achieving state-of-the-art results.</p><hr><h3>The Devil is in the Object Boundary: Towards Annotation-free Instance Segmentation using Foundation Models</h3>
<p><a href='https://openreview.net/forum?id=4JbrdrHxYy'>https://openreview.net/forum?id=4JbrdrHxYy</a></p>
<p><b>Compressor summary</b>: Zip combines CLIP's boundary prior with SAM to enable annotation-free object detection and instance segmentation, improving performance significantly in various settings.</p><hr><h3>One For All: Towards Training One Graph Model For All Classification Tasks</h3>
<p><a href='https://openreview.net/forum?id=4IT2pgc9v6'>https://openreview.net/forum?id=4IT2pgc9v6</a></p>
<p><b>Compressor summary</b>: One for All (OFA) is a framework that uses text-attributed graphs and a single model to handle various tasks on graphs, such as node, link, and graph tasks, without fine-tuning.</p><hr><h3>Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning</h3>
<p><a href='https://openreview.net/forum?id=4DoSULcfG6'>https://openreview.net/forum?id=4DoSULcfG6</a></p>
<p><b>Compressor summary</b>: The paper proposes a new attack called Chameleon that can effectively infer membership in machine learning models using only the predicted labels by using data poisoning and query selection.</p><hr><h3>Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space</h3>
<p><a href='https://openreview.net/forum?id=4Ay23yeuz0'>https://openreview.net/forum?id=4Ay23yeuz0</a></p>
<p><b>Compressor summary</b>: TabSYN is a method that uses diffusion models within a VAE to synthesize high-quality tabular data with generality, quality, and speed advantages over existing methods.</p><hr><h3>LCOT: Linear Circular Optimal Transport</h3>
<p><a href='https://openreview.net/forum?id=49z97Y9lMq'>https://openreview.net/forum?id=49z97Y9lMq</a></p>
<p><b>Compressor summary</b>: The paper introduces a new efficient metric (LCOT) for circular probability measures that allows applying ML algorithms and improves representation learning.</p><hr><h3>Language Model Decoding as Direct Metrics Optimization</h3>
<p><a href='https://openreview.net/forum?id=488A64eOf6'>https://openreview.net/forum?id=488A64eOf6</a></p>
<p><b>Compressor summary</b>: The paper proposes a decoding method for language models that optimizes multiple aspects of text quality and shows its effectiveness in aligning with human texts.</p><hr><h3>OPTIMAL ROBUST MEMORIZATION WITH RELU NEURAL NETWORKS</h3>
<p><a href='https://openreview.net/forum?id=47hDbAMLbc'>https://openreview.net/forum?id=47hDbAMLbc</a></p>
<p><b>Compressor summary</b>: The paper studies the challenges and solutions of robust memorization in neural networks, showing that it is NP-hard, providing upper and lower bounds for network width, and constructing optimal networks for different data sets.</p><hr><h3>Understanding Expressivity of Neural KG Reasoning from Rule Structure Learning</h3>
<p><a href='https://openreview.net/forum?id=43cYe4oogi'>https://openreview.net/forum?id=43cYe4oogi</a></p>
<p><b>Compressor summary</b>: The paper analyzes the expressivity of Graph Neural Networks (GNNs) for Knowledge Graph (KG) reasoning, formalizes the rule structures they can learn, and proposes a novel labeling strategy to improve their performance.</p><hr><h3>Neural Rate Control for Learned Video Compression</h3>
<p><a href='https://openreview.net/forum?id=42lcaojZug'>https://openreview.net/forum?id=42lcaojZug</a></p>
<p><b>Compressor summary</b>: The paper proposes a neural network-based rate control system for learning-based video compression, which assigns optimal bitrates to frames and predicts coding parameters for a given rate, enhancing the compression performance.</p><hr><h3>DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$</h3>
<p><a href='https://openreview.net/forum?id=3zvB14IF6D'>https://openreview.net/forum?id=3zvB14IF6D</a></p>
<p><b>Compressor summary</b>: The paper proposes DORSal, a video diffusion model for high-fidelity 3D scene generation from object-centric representations, enabling scalable neural rendering with object-level editing.</p><hr><h3>Retrieval-Based Reconstruction For Time-series Contrastive Learning</h3>
<p><a href='https://openreview.net/forum?id=3zQo5oUvia'>https://openreview.net/forum?id=3zQo5oUvia</a></p>
<p><b>Compressor summary</b>: REBAR is a novel approach for self-supervised contrastive learning in time-series, which uses cross-attention to reconstruct subsequences and labels them with REBAR error, achieving state-of-the-art results on downstream tasks.</p><hr><h3>Generalized Knowledge Distillation for Auto-regressive Language Models</h3>
<p><a href='https://openreview.net/forum?id=3zKtaqxLhW'>https://openreview.net/forum?id=3zKtaqxLhW</a></p>
<p><b>Compressor summary</b>: Generalized Knowledge Distillation (GKD) is a new method that trains smaller models on their own outputs with feedback from larger models to overcome distribution mismatch issues in compressing sequence models.</p><hr><h3>Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks</h3>
<p><a href='https://openreview.net/forum?id=3z60EWfh1p'>https://openreview.net/forum?id=3z60EWfh1p</a></p>
<p><b>Compressor summary</b>: The paper introduces ${\it GATE}$, a novel transfer learning technique for regression tasks using differential geometry to align latent vectors between tasks and enable knowledge transfer.</p><hr><h3>T-Rep: Representation Learning for Time Series using Time-Embeddings</h3>
<p><a href='https://openreview.net/forum?id=3y2TfP966N'>https://openreview.net/forum?id=3y2TfP966N</a></p>
<p><b>Compressor summary</b>: T-Rep is a self-supervised method that learns vector embeddings of time and uses them to extract temporal features for robust time series representation learning, outperforming existing algorithms on classification, forecasting, and anomaly detection tasks.</p><hr><h3>Brain decoding: toward real-time reconstruction of visual perception</h3>
<p><a href='https://openreview.net/forum?id=3y1K6buO8c'>https://openreview.net/forum?id=3y1K6buO8c</a></p>
<p><b>Compressor summary</b>: The authors propose a new MEG decoding model that improves image retrieval and reveals high-level visual features of brain activity with high temporal resolution.</p><hr><h3>Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training</h3>
<p><a href='https://openreview.net/forum?id=3xHDeA8Noi'>https://openreview.net/forum?id=3xHDeA8Noi</a></p>
<p><b>Compressor summary</b>: Sophia is a scalable second-order optimizer that speeds up language model training with GPT models by using a simple estimate of the diagonal Hessian as the pre-conditioner.</p><hr><h3>Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning</h3>
<p><a href='https://openreview.net/forum?id=3xDaj4pRna'>https://openreview.net/forum?id=3xDaj4pRna</a></p>
<p><b>Compressor summary</b>: SAM, a neural network training method, can improve feature quality and diversity by adaptively suppressing well-learned features in datasets with redundant or spurious features.</p><hr><h3>A Study of Generalization in Offline Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=3w6xuXDOdY'>https://openreview.net/forum?id=3w6xuXDOdY</a></p>
<p><b>Compressor summary</b>: The paper studies the generalization abilities of offline RL algorithms and introduces a benchmark with diverse datasets to evaluate them, finding that existing methods struggle to generalize.</p><hr><h3>Generative Learning for Solving Non-Convex Problem with Multi-Valued Input-Solution Mapping</h3>
<p><a href='https://openreview.net/forum?id=3tM1l5tSbv'>https://openreview.net/forum?id=3tM1l5tSbv</a></p>
<p><b>Compressor summary</b>: The paper proposes a new generative learning approach using a rectified flow model to tackle multi-valued input-solution mappings in non-convex problems, outperforming existing methods in stability, complexity, and solution optimality.</p><hr><h3>Boosting Vanilla Lightweight Vision Transformers via Re-parameterization</h3>
<p><a href='https://openreview.net/forum?id=3rmpixOjPS'>https://openreview.net/forum?id=3rmpixOjPS</a></p>
<p><b>Compressor summary</b>: This paper proposes a novel module for lightweight Vision Transformers that improves their performance by adapting re-parameterization technology, achieving state-of-the-art results on various vision tasks.</p><hr><h3>LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks</h3>
<p><a href='https://openreview.net/forum?id=3qo1pJHabg'>https://openreview.net/forum?id=3qo1pJHabg</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to defend against adversarial perturbations in visual object tracking using a spatial-temporal continuous representation and semantic text guidance.</p><hr><h3>Rethinking the Uniformity Metric in Self-Supervised Learning</h3>
<p><a href='https://openreview.net/forum?id=3pf2hEdu8B'>https://openreview.net/forum?id=3pf2hEdu8B</a></p>
<p><b>Compressor summary</b>: The paper proposes a new uniformity metric for self-supervised learning that is sensitive to dimensional collapse and improves the performance of downstream tasks.</p><hr><h3>Where We Have Arrived in Proving the Emergence of Sparse Interaction Primitives in AI Models</h3>
<p><a href='https://openreview.net/forum?id=3pWSL8My6B'>https://openreview.net/forum?id=3pWSL8My6B</a></p>
<p><b>Compressor summary</b>: The study shows how AI models learn sparse, symbolic concepts from occluded input samples and proves conditions for their emergence.</p><hr><h3>Improving Generalization in Equivariant Graph Neural Networks with Physical Inductive Biases</h3>
<p><a href='https://openreview.net/forum?id=3oTPsORaDH'>https://openreview.net/forum?id=3oTPsORaDH</a></p>
<p><b>Compressor summary</b>: SEGNO is a new GNN model that incorporates second-order continuity and velocity information to improve generalization for complex physical systems.</p><hr><h3>Towards Principled Representation Learning from Videos for Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=3mnWvUZIXt'>https://openreview.net/forum?id=3mnWvUZIXt</a></p>
<p><b>Compressor summary</b>: The paper investigates how to learn good representations for decision-making using video data, and shows that some common methods work well with iid noise but struggle with exogenous, non-iid noise.</p><hr><h3>Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy</h3>
<p><a href='https://openreview.net/forum?id=3fEKavFsnv'>https://openreview.net/forum?id=3fEKavFsnv</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel optimization method (MMD-MP) to detect machine-generated texts using maximum mean discrepancy, addressing the challenges posed by multiple text populations from different large language models.</p><hr><h3>LEGO-Prover: Neural Theorem Proving with Growing Libraries</h3>
<p><a href='https://openreview.net/forum?id=3f5PALef5B'>https://openreview.net/forum?id=3f5PALef5B</a></p>
<p><b>Compressor summary</b>: LEGO-Prover uses a growing skill library with verified lemmas to enhance LLMs for theorem proving, enabling modular proofs, skill creation, and evolution, improving pass rates on math problems.</p><hr><h3>Efficient-3Dim: Learning a Generalizable Single-image Novel-view Synthesizer in One Day</h3>
<p><a href='https://openreview.net/forum?id=3eFMnZ3N4J'>https://openreview.net/forum?id=3eFMnZ3N4J</a></p>
<p><b>Compressor summary</b>: Efficient-3DiM is a framework that simplifies and speeds up the process of generating novel views from single images using diffusion models.</p><hr><h3>Privately Aligning Language Models with Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=3d0OmYTNui'>https://openreview.net/forum?id=3d0OmYTNui</a></p>
<p><b>Compressor summary</b>: The authors propose a differential privacy framework for aligning large language models through reinforcement learning, with applications in text generation and summarization.</p><hr><h3>Identifying Representations for Intervention Extrapolation</h3>
<p><a href='https://openreview.net/forum?id=3cuJwmPxXj'>https://openreview.net/forum?id=3cuJwmPxXj</a></p>
<p><b>Compressor summary</b>: The paper proposes Rep4Ex, a method to learn identifiable representations that enable non-linear extrapolation for intervention prediction tasks using autoencoders with linearity constraints on latent features.</p><hr><h3>Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=3bq3jsvcQ1'>https://openreview.net/forum?id=3bq3jsvcQ1</a></p>
<p><b>Compressor summary</b>: Step-Back Prompting is a technique that helps LLMs do abstractions and improve their reasoning skills, leading to significant performance gains on various tasks.</p><hr><h3>Why is SAM Robust to Label Noise?</h3>
<p><a href='https://openreview.net/forum?id=3aZCPl3ZvR'>https://openreview.net/forum?id=3aZCPl3ZvR</a></p>
<p><b>Compressor summary</b>: Sharpness-Aware Minimization (SAM) enhances performance, especially under label noise, by altering the gradient contribution from clean examples and modifying the network Jacobian, which can be replicated with cheaper alternatives.</p><hr><h3>Evaluating Language Models Through Negotiations</h3>
<p><a href='https://openreview.net/forum?id=3ZqKxMHcAg'>https://openreview.net/forum?id=3ZqKxMHcAg</a></p>
<p><b>Compressor summary</b>: The authors propose evaluating language models' performance and alignment in negotiation games, which reveal their ability to plan and negotiate, and show that open-source models are weak, cooperation is challenging, and the best models do not always prevail.</p><hr><h3>PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training</h3>
<p><a href='https://openreview.net/forum?id=3Z1gxuAQrA'>https://openreview.net/forum?id=3Z1gxuAQrA</a></p>
<p><b>Compressor summary</b>: PoSE is a training method that simulates long inputs using a fixed context window, allowing large language models to adapt to different input lengths efficiently and with minimal performance impact.</p><hr><h3>Continual Learning in the Presence of Spurious Correlations: Analyses and a Simple Baseline</h3>
<p><a href='https://openreview.net/forum?id=3Y7r6xueJJ'>https://openreview.net/forum?id=3Y7r6xueJJ</a></p>
<p><b>Compressor summary</b>: The text discusses how dataset bias in continual learning algorithms can lead to undesirable transfer and accumulation of bias, and proposes a baseline method to address this issue.</p><hr><h3>LEMON: Lossless model expansion</h3>
<p><a href='https://openreview.net/forum?id=3Vw7DQqq7U'>https://openreview.net/forum?id=3Vw7DQqq7U</a></p>
<p><b>Compressor summary</b>: LEMON is a method to initialize and train scaled Transformers using the weights of their smaller pre-trained counterparts, reducing computational costs by up to 56.7%.</p><hr><h3>Fusion is Not Enough: Single Modal Attack on Fusion Models for 3D Object Detection</h3>
<p><a href='https://openreview.net/forum?id=3VD4PNEt5q'>https://openreview.net/forum?id=3VD4PNEt5q</a></p>
<p><b>Compressor summary</b>: The paper proposes an adversarial attack on camera-LiDAR fusion models for 3D object detection by exploiting their weakness in the camera modality.</p><hr><h3>Learning Planning Abstractions from Language</h3>
<p><a href='https://openreview.net/forum?id=3UWuFoksGb'>https://openreview.net/forum?id=3UWuFoksGb</a></p>
<p><b>Compressor summary</b>: The paper introduces PARL, a framework for learning abstract actions and states from language-annotated demonstrations, which can generalize to new scenarios and tasks.</p><hr><h3>BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models</h3>
<p><a href='https://openreview.net/forum?id=3TO3TtnOFl'>https://openreview.net/forum?id=3TO3TtnOFl</a></p>
<p><b>Compressor summary</b>: The text introduces binary token representations (BTR) for retrieval augmentation in language models, which significantly reduce computation, storage, and improve inference speed on knowledge-intensive NLP tasks.</p><hr><h3>Generalization error of spectral algorithms</h3>
<p><a href='https://openreview.net/forum?id=3SJE1WLB4M'>https://openreview.net/forum?id=3SJE1WLB4M</a></p>
<p><b>Compressor summary</b>: The paper studies how kernel methods like neural networks are trained with different algorithms, and shows how their generalization error depends on the learning profile of these algorithms for various data models.</p><hr><h3>Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks</h3>
<p><a href='https://openreview.net/forum?id=3ROGsTX3IR'>https://openreview.net/forum?id=3ROGsTX3IR</a></p>
<p><b>Compressor summary</b>: The paper explores how deep neural networks learn new features and experience Grokking phenomena by applying the adaptive kernel approach to teacher-student models and comparing it to phase transitions in physics.</p><hr><h3>Universal Backdoor Attacks</h3>
<p><a href='https://openreview.net/forum?id=3QkzYBSWqL'>https://openreview.net/forum?id=3QkzYBSWqL</a></p>
<p><b>Compressor summary</b>: The authors propose a universal data poisoning attack that exploits inter-class transferability to control misclassifications in deep image classifiers with minimal poison samples.</p><hr><h3>SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning</h3>
<p><a href='https://openreview.net/forum?id=3QLkwU40EE'>https://openreview.net/forum?id=3QLkwU40EE</a></p>
<p><b>Compressor summary</b>: SPTNet uses a two-stage adaptation approach and spatial prompt tuning method to improve generalized category discovery, achieving state-of-the-art results with minimal additional parameters.</p><hr><h3>GIO: Gradient Information Optimization for Training Dataset Selection</h3>
<p><a href='https://openreview.net/forum?id=3NnfJnbJT2'>https://openreview.net/forum?id=3NnfJnbJT2</a></p>
<p><b>Compressor summary</b>: GIO is a scalable method to select informative data subsets for model training, achieving high performance with few examples, across different tasks and domains.</p><hr><h3>Don't Play Favorites: Minority Guidance for Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=3NmO9lY4Jn'>https://openreview.net/forum?id=3NmO9lY4Jn</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel framework to generate high-quality minority samples using diffusion models by introducing a uniqueness metric and a sampling technique called minority guidance.</p><hr><h3>CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping</h3>
<p><a href='https://openreview.net/forum?id=3M0GXoUEzP'>https://openreview.net/forum?id=3M0GXoUEzP</a></p>
<p><b>Compressor summary</b>: CrIBo is a new method for improving visual representation learning by bootstrapping objects at the image level, enhancing in-context learning and achieving state-of-the-art results on downstream tasks.</p><hr><h3>The Reasonableness Behind Unreasonable Translation Capability of Large Language Model</h3>
<p><a href='https://openreview.net/forum?id=3KDbIWT26J'>https://openreview.net/forum?id=3KDbIWT26J</a></p>
<p><b>Compressor summary</b>: The study explores why large language models can translate without parallel data and finds that unintentional bilingualism in pre-training corpus, especially word alignment data, helps them learn translation skills.</p><hr><h3>On Representation Complexity of Model-based and Model-free Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=3K3s9qxSn7'>https://openreview.net/forum?id=3K3s9qxSn7</a></p>
<p><b>Compressor summary</b>: The paper investigates how easily circuits can represent various aspects of reinforcement learning problems and shows that model-based algorithms often have better performance due to simpler environment rules.</p><hr><h3>The Marginal Value of Momentum for Small Learning Rate SGD</h3>
<p><a href='https://openreview.net/forum?id=3JjJezzVkT'>https://openreview.net/forum?id=3JjJezzVkT</a></p>
<p><b>Compressor summary</b>: Momentum may help reduce variance in stochastic optimization, but its benefits are limited for practical deep learning training when the optimal learning rate is not very large.</p><hr><h3>On Harmonizing Implicit Subpopulations</h3>
<p><a href='https://openreview.net/forum?id=3GurO0kRue'>https://openreview.net/forum?id=3GurO0kRue</a></p>
<p><b>Compressor summary</b>: The paper proposes a method called SHE to address class-level imbalance by discovering hidden subpopulations and adjusting predictions for better generalization.</p><hr><h3>Self-supervised Heterogeneous Graph Learning:  a Homogeneity and Heterogeneity Perspective</h3>
<p><a href='https://openreview.net/forum?id=3FJOKjooIj'>https://openreview.net/forum?id=3FJOKjooIj</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel self-supervised learning method for heterogeneous graphs that captures both homogeneity and heterogeneity without pre-defined meta-paths, using a self-expressive matrix and two losses to improve task performance.</p><hr><h3>Chain of Thought Empowers Transformers to Solve Inherently Serial Problems</h3>
<p><a href='https://openreview.net/forum?id=3EWTEy9MTM'>https://openreview.net/forum?id=3EWTEy9MTM</a></p>
<p><b>Compressor summary</b>: The text explains how a sequence of intermediate steps (chain of thought) helps large language models solve complex arithmetic and symbolic reasoning tasks by enabling serial computation that they otherwise lack.</p><hr><h3>Training Unbiased Diffusion Models From Biased Dataset</h3>
<p><a href='https://openreview.net/forum?id=39cPKijBed'>https://openreview.net/forum?id=39cPKijBed</a></p>
<p><b>Compressor summary</b>: The paper introduces a new technique called time-dependent importance reweighting to reduce dataset bias in diffusion models and improve sample quality.</p><hr><h3>Language Model Self-improvement by Reinforcement Learning Contemplation</h3>
<p><a href='https://openreview.net/forum?id=38E4yUbrgr'>https://openreview.net/forum?id=38E4yUbrgr</a></p>
<p><b>Compressor summary</b>: RLC is a novel LMSI method that uses evaluation scores from language models to improve their performance on various tasks without external supervision.</p><hr><h3>Beating Price of Anarchy and Gradient Descent without Regret in Potential Games</h3>
<p><a href='https://openreview.net/forum?id=36L7W3ri4U'>https://openreview.net/forum?id=36L7W3ri4U</a></p>
<p><b>Compressor summary</b>: Q-Replicator Dynamics (QRD) is a class of continuous-time no-regret learning dynamics in potential games, which includes gradient descent, log-barrier and replicator dynamics as special cases. The paper studies QRD's ability to select socially optimal equilibria and shows that QRD dynamics have unbounded gaps between average case performance and Price of Anarchy analysis even in larger settings.</p><hr><h3>Polynomial Width is Sufficient for Set Representation with High-dimensional Features</h3>
<p><a href='https://openreview.net/forum?id=34STseLBrQ'>https://openreview.net/forum?id=34STseLBrQ</a></p>
<p><b>Compressor summary</b>: The paper studies how the dimension of the latent space affects the performance of DeepSets, a neural network architecture for set representation, and proposes new embeddings that require less dimension.</p><hr><h3>Conformal Risk Control</h3>
<p><a href='https://openreview.net/forum?id=33XGfHLtZg'>https://openreview.net/forum?id=33XGfHLtZg</a></p>
<p><b>Compressor summary</b>: The paper presents an improved conformal prediction method that can control the expected value of any monotone loss function and is applicable to various tasks and settings.</p><hr><h3>Domain-agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations</h3>
<p><a href='https://openreview.net/forum?id=327tbF3S65'>https://openreview.net/forum?id=327tbF3S65</a></p>
<p><b>Compressor summary</b>: The paper proposes a new model that generates adaptive positional embeddings for synthesizing implicit neural representations (INRs) across various domains, improving the quality of generation over existing methods.</p><hr><h3>Repelling Random Walks</h3>
<p><a href='https://openreview.net/forum?id=31IOmrnoP4'>https://openreview.net/forum?id=31IOmrnoP4</a></p>
<p><b>Compressor summary</b>: Repelling random walks is a new method that helps explore graphs more efficiently by inducing correlations between walker trajectories, improving statistical estimators without bias.</p><hr><h3>Matrix Manifold Neural Networks++</h3>
<p><a href='https://openreview.net/forum?id=30aSE3FB3L'>https://openreview.net/forum?id=30aSE3FB3L</a></p>
<p><b>Compressor summary</b>: The paper presents new neural network architectures on Riemannian manifolds, such as SPD and Grassmann manifolds, for computer vision tasks like human action recognition and node classification.</p><hr><h3>Separating common from salient patterns with Contrastive Representation Learning</h3>
<p><a href='https://openreview.net/forum?id=30N3bNAiw3'>https://openreview.net/forum?id=30N3bNAiw3</a></p>
<p><b>Compressor summary</b>: The authors propose a new method for contrastive analysis using mutual information terms to learn semantically expressive representations and assess its performance on various datasets.</p><hr><h3>Enable Lanuguage Models to Implicitly Learn Self-Improvement From Data</h3>
<p><a href='https://openreview.net/forum?id=2tVHNRZuCs'>https://openreview.net/forum?id=2tVHNRZuCs</a></p>
<p><b>Compressor summary</b>: The paper proposes an implicit self-improvement framework for large language models that learns from human preference data to improve response quality without relying on explicit rubrics.</p><hr><h3>SparseFormer: Sparse Visual Recognition via Limited Latent Tokens</h3>
<p><a href='https://openreview.net/forum?id=2pvECsmld3'>https://openreview.net/forum?id=2pvECsmld3</a></p>
<p><b>Compressor summary</b>: SparseFormer is a new method that mimics human's sparse visual recognition by using few tokens to represent images, achieving performance similar to existing models with lower computational costs.</p><hr><h3>Light-MILPopt: Solving Large-scale Mixed Integer Linear Programs with Small-scale Optimizer and Small Training Dataset</h3>
<p><a href='https://openreview.net/forum?id=2oWRumm67L'>https://openreview.net/forum?id=2oWRumm67L</a></p>
<p><b>Compressor summary</b>: Light-MILPopt is a lightweight optimization framework that uses a small-scale optimizer and dataset to solve large-scale MILPs more efficiently than existing methods.</p><hr><h3>Ferret: Refer and Ground Anything Anywhere at Any Granularity</h3>
<p><a href='https://openreview.net/forum?id=2msbbX3ydD'>https://openreview.net/forum?id=2msbbX3ydD</a></p>
<p><b>Compressor summary</b>: Ferret is a new language model that can understand and describe different shapes and regions in images, using a novel hybrid representation and a large dataset called GRIT.</p><hr><h3>Instant3D: Fast Text-to-3D with Sparse-view Generation and Large Reconstruction Model</h3>
<p><a href='https://openreview.net/forum?id=2lDQLiH1W4'>https://openreview.net/forum?id=2lDQLiH1W4</a></p>
<p><b>Compressor summary</b>: Instant3D is a novel text-to-3D method that uses a diffusion model for fast and high-quality 3D asset generation from text prompts in two stages: text-to-image and image-to-NeRF.</p><hr><h3>Deep SE(3)-Equivariant Geometric Reasoning for Precise Placement Tasks</h3>
<p><a href='https://openreview.net/forum?id=2inBuwTyL2'>https://openreview.net/forum?id=2inBuwTyL2</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for precise relative pose prediction in robot manipulation tasks that is provably SE(3)-equivariant, learned from few demonstrations, and generalizes across object variations.</p><hr><h3>BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics</h3>
<p><a href='https://openreview.net/forum?id=2iGiSHmeAN'>https://openreview.net/forum?id=2iGiSHmeAN</a></p>
<p><b>Compressor summary</b>: BroGNet combines SDEs and GNNs to learn Brownian dynamics from trajectories, preserving linear momentum conservation and outperforming baselines on various systems.</p><hr><h3>Vision Transformers Need Registers</h3>
<p><a href='https://openreview.net/forum?id=2dnO3LLiJ1'>https://openreview.net/forum?id=2dnO3LLiJ1</a></p>
<p><b>Compressor summary</b>: The paper identifies and solves artifacts in ViT networks' feature maps, improving their performance for self-supervised learning and visual processing.</p><hr><h3>Function-space Parameterization of Neural Networks for Sequential Learning</h3>
<p><a href='https://openreview.net/forum?id=2dhxxIKhqz'>https://openreview.net/forum?id=2dhxxIKhqz</a></p>
<p><b>Compressor summary</b>: The paper proposes a technique to convert neural networks into function space, enabling scalability, retention of prior knowledge, and efficient incorporation of new data in sequential learning paradigms.</p><hr><h3>Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints</h3>
<p><a href='https://openreview.net/forum?id=2cRzmWXK9N'>https://openreview.net/forum?id=2cRzmWXK9N</a></p>
<p><b>Compressor summary</b>: This paper introduces $f$-DPO, a generalized method to optimize large language models for human preference alignment using various divergence constraints, which improves alignment performance and generation diversity.</p><hr><h3>MEND: Meta Demonstration Distillation for Efficient and Effective In-Context Learning</h3>
<p><a href='https://openreview.net/forum?id=2Y5kBPtU0o'>https://openreview.net/forum?id=2Y5kBPtU0o</a></p>
<p><b>Compressor summary</b>: MEND is a method that learns to condense demonstrations without retraining, improving in-context learning performance and reducing computational overhead for large language models.</p><hr><h3>Efficient and Scalable Graph Generation by Spectrum Preserving Local Expansion</h3>
<p><a href='https://openreview.net/forum?id=2XkTz7gdpc'>https://openreview.net/forum?id=2XkTz7gdpc</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for generating large graphs by progressively expanding a single node, using denoising diffusion to add nodes and edges in a localized manner while achieving high expressivity and computational efficiency.</p><hr><h3>sRGB Real Noise Modeling via Noise-Aware Sampling with Normalizing Flows</h3>
<p><a href='https://openreview.net/forum?id=2XBBumBGeP'>https://openreview.net/forum?id=2XBBumBGeP</a></p>
<p><b>Compressor summary</b>: This paper proposes a new normalizing flows framework for generating realistic noise images using estimated camera settings, which improves noise modeling and enhances image denoising performance.</p><hr><h3>On the Over-Memorization During Natural, Robust and Catastrophic Overfitting</h3>
<p><a href='https://openreview.net/forum?id=2V1Z0Jdmss'>https://openreview.net/forum?id=2V1Z0Jdmss</a></p>
<p><b>Compressor summary</b>: The text explores over-memorization, a type of overfitting that affects deep neural networks' generalization capacity, and proposes a framework to prevent it by distracting the network from high-confidence natural patterns.</p><hr><h3>Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation</h3>
<p><a href='https://openreview.net/forum?id=2UnCj3jeao'>https://openreview.net/forum?id=2UnCj3jeao</a></p>
<p><b>Compressor summary</b>: The text introduces a new method to improve neural Monge map estimators by incorporating unbalanced optimal transport, enhancing their performance in unpaired domain translation tasks like image and cellular data.</p><hr><h3>Towards Offline Opponent Modeling with In-context Learning</h3>
<p><a href='https://openreview.net/forum?id=2SwHngthig'>https://openreview.net/forum?id=2SwHngthig</a></p>
<p><b>Compressor summary</b>: TAO is a general approach that uses pre-trained Transformers to learn offline opponent models and adapt to unknown fixed policies in competitive environments.</p><hr><h3>Time Travel in LLMs: Tracing Data Contamination in Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=2Rwq6c3tvr'>https://openreview.net/forum?id=2Rwq6c3tvr</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to identify data contamination in large language models using guided instruction and classification techniques.</p><hr><h3>GOAt: Explaining Graph Neural Networks via Graph Output Attribution</h3>
<p><a href='https://openreview.net/forum?id=2Q8TZWAHv4'>https://openreview.net/forum?id=2Q8TZWAHv4</a></p>
<p><b>Compressor summary</b>: The paper proposes Graph Output Attribution (GOAt), a method to explain Graph Neural Networks (GNNs) by attributing graph outputs to input features using scalar products, achieving faithful, discriminative, and stable explanations.</p><hr><h3>More Context, Less Distraction: Zero-shot Visual Classification by Inferring and Conditioning on Contextual Attributes</h3>
<p><a href='https://openreview.net/forum?id=2Oiee202rd'>https://openreview.net/forum?id=2Oiee202rd</a></p>
<p><b>Compressor summary</b>: The paper proposes a two-step zero-shot image classification method called PerceptionCLIP that infers contextual attributes from an image and uses them for object classification, improving performance and reducing reliance on spurious features.</p><hr><h3>Neural Neighborhood Search for Multi-agent Path Finding</h3>
<p><a href='https://openreview.net/forum?id=2NpAw2QJBY'>https://openreview.net/forum?id=2NpAw2QJBY</a></p>
<p><b>Compressor summary</b>: The paper presents a novel neural architecture that enhances large neighborhood search for multi-agent path finding by using convolution and attention to represent subproblems.</p><hr><h3>Pooling Image Datasets with Multiple Covariate Shift and Imbalance</h3>
<p><a href='https://openreview.net/forum?id=2Mo7v69otj'>https://openreview.net/forum?id=2Mo7v69otj</a></p>
<p><b>Compressor summary</b>: The paper proposes a Category theory approach to handle shifts and imbalances in covariates for overparameterized DNN models in data pooling tasks.</p><hr><h3>Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance</h3>
<p><a href='https://openreview.net/forum?id=2JF8mJRJ7M'>https://openreview.net/forum?id=2JF8mJRJ7M</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method, Lipsum-FT, for fine-tuning pre-trained vision-language models that preserves their robustness against distribution shifts while improving performance on image classification tasks.</p><hr><h3>What does the Knowledge Neuron Thesis Have to do with Knowledge?</h3>
<p><a href='https://openreview.net/forum?id=2HJRwwbV3G'>https://openreview.net/forum?id=2HJRwwbV3G</a></p>
<p><b>Compressor summary</b>: The authors question the Knowledge Neuron Thesis, which claims that large language models recall facts from a training corpus through MLP weights resembling key-value memory, and suggest looking into other aspects of the models to better understand knowledge representation.</p><hr><h3>Neural Spectral Methods</h3>
<p><a href='https://openreview.net/forum?id=2DbVeuoa6a'>https://openreview.net/forum?id=2DbVeuoa6a</a></p>
<p><b>Compressor summary</b>: Neural Spectral Methods is a technique for solving PDEs using neural networks that learns mappings between spectral coefficients and achieves faster and more accurate results than previous methods.</p><hr><h3>Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=28L2FCtMWq'>https://openreview.net/forum?id=28L2FCtMWq</a></p>
<p><b>Compressor summary</b>: Ground-A-Video is a training-free video editing method that uses cross-frame gated attention and other techniques to accurately edit multiple attributes of videos in a consistent way.</p><hr><h3>Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models</h3>
<p><a href='https://openreview.net/forum?id=26XphugOcS'>https://openreview.net/forum?id=26XphugOcS</a></p>
<p><b>Compressor summary</b>: The paper proposes a zero-shot method to transfer continuous prompts between NLP models by encoding them into relative space and searching for target prompts.</p><hr><h3>RETSim: Resilient and Efficient Text Similarity</h3>
<p><a href='https://openreview.net/forum?id=23b9KSNQTX'>https://openreview.net/forum?id=23b9KSNQTX</a></p>
<p><b>Compressor summary</b>: RETSim is a robust deep learning model for finding similar texts, outperforming existing methods on several tasks and introducing a new benchmark for evaluation.</p><hr><h3>PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization</h3>
<p><a href='https://openreview.net/forum?id=22pyNMuIoa'>https://openreview.net/forum?id=22pyNMuIoa</a></p>
<p><b>Compressor summary</b>: PromptAgent is an automated system that uses strategic planning and error feedback to discover expert-level prompts for large language models, outperforming existing methods on 12 tasks across three domains.</p><hr><h3>RA-DIT: Retrieval-Augmented Dual Instruction Tuning</h3>
<p><a href='https://openreview.net/forum?id=22OTbutug9'>https://openreview.net/forum?id=22OTbutug9</a></p>
<p><b>Compressor summary</b>: RA-DIT is a method that fine-tunes language models to use external data for better performance on knowledge-intensive tasks, improving over existing approaches by a significant margin.</p><hr><h3>MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction Following</h3>
<p><a href='https://openreview.net/forum?id=1vrS1zwekw'>https://openreview.net/forum?id=1vrS1zwekw</a></p>
<p><b>Compressor summary</b>: MUFFIN is a new instruction-following dataset curation scheme for large language models that diversifies tasks with input facets, improving their performance across various benchmarks.</p><hr><h3>Directly Fine-Tuning Diffusion Models on Differentiable Rewards</h3>
<p><a href='https://openreview.net/forum?id=1vmSEVL19f'>https://openreview.net/forum?id=1vmSEVL19f</a></p>
<p><b>Compressor summary</b>: DRaFT is a method to improve diffusion models using reward gradients that works well for various reward functions and enhances image quality.</p><hr><h3>Mastering Memory Tasks with World Models</h3>
<p><a href='https://openreview.net/forum?id=1vDArHJ68h'>https://openreview.net/forum?id=1vDArHJ68h</a></p>
<p><b>Compressor summary</b>: The Recall to Imagine (R2I) method improves the performance and speed of model-based reinforcement learning agents by integrating state space models for better long-term memory and credit assignment.</p><hr><h3>Unified Human-Scene Interaction via Prompted Chain-of-Contacts</h3>
<p><a href='https://openreview.net/forum?id=1vCnDyQkjg'>https://openreview.net/forum?id=1vCnDyQkjg</a></p>
<p><b>Compressor summary</b>: UniHSI is a framework for human-scene interaction using language commands, which translates tasks into contact sequences and executes them uniformly.</p><hr><h3>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=1tZbq88f27'>https://openreview.net/forum?id=1tZbq88f27</a></p>
<p><b>Compressor summary</b>: MiniGPT-4 shows that aligning visual features with an advanced language model can achieve multi-modal abilities similar to GPT-4, like image description and website creation from sketches.</p><hr><h3>ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection</h3>
<p><a href='https://openreview.net/forum?id=1pSL2cXWoz'>https://openreview.net/forum?id=1pSL2cXWoz</a></p>
<p><b>Compressor summary</b>: The paper proposes a new OOD detection method based on Bregman divergence that uses an optimal norm coefficient to measure the distance between data distributions and achieves state-of-the-art performance.</p><hr><h3>Convolutional Deep Kernel Machines</h3>
<p><a href='https://openreview.net/forum?id=1oqedRt6Z7'>https://openreview.net/forum?id=1oqedRt6Z7</a></p>
<p><b>Compressor summary</b>: The paper introduces convolutional deep kernel machines with novel techniques, achieving state-of-the-art performance for kernel methods on image datasets.</p><hr><h3>Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach</h3>
<p><a href='https://openreview.net/forum?id=1op5YGZu8X'>https://openreview.net/forum?id=1op5YGZu8X</a></p>
<p><b>Compressor summary</b>: The paper explains why adversarial training can harm the robustness of deep neural networks and proposes a new method to improve it for infinite-width DNNs.</p><hr><h3>Self-Alignment with Instruction Backtranslation</h3>
<p><a href='https://openreview.net/forum?id=1oijHJBRsT'>https://openreview.net/forum?id=1oijHJBRsT</a></p>
<p><b>Compressor summary</b>: The paper introduces instruction backtranslation, a method to improve language models by generating and selecting instruction prompts for web documents and finetuning them.</p><hr><h3>Dynamic Sparse No Training:  Training-Free Fine-tuning for Sparse LLMs</h3>
<p><a href='https://openreview.net/forum?id=1ndDmZdT4g'>https://openreview.net/forum?id=1ndDmZdT4g</a></p>
<p><b>Compressor summary</b>: Dynamic Sparse No Training (DSNT) is a training-free approach for fine-tuning large language models that updates sparse models without backpropagation or weight updates, improving performance at high sparsity levels.</p><hr><h3>Unsupervised Fact Verification by Language Model Distillation</h3>
<p><a href='https://openreview.net/forum?id=1mjsP8RYAw'>https://openreview.net/forum?id=1mjsP8RYAw</a></p>
<p><b>Compressor summary</b>: The paper proposes SFAVEL, an unsupervised framework for fact verification using pre-trained language models and a novel contrastive loss function that improves alignment quality and semantic relationships.</p><hr><h3>Domain constraints improve risk prediction when outcome data is missing</h3>
<p><a href='https://openreview.net/forum?id=1mNFsbvo2P'>https://openreview.net/forum?id=1mNFsbvo2P</a></p>
<p><b>Compressor summary</b>: The paper proposes a Bayesian model to estimate risk for patients who were tested or not tested for disease, incorporating domain constraints from health settings to improve parameter inference and cancer risk prediction.</p><hr><h3>InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation</h3>
<p><a href='https://openreview.net/forum?id=1k4yZbbDqX'>https://openreview.net/forum?id=1k4yZbbDqX</a></p>
<p><b>Compressor summary</b>: The paper presents InstaFlow, a one-step diffusion-based text-to-image generator using Rectified Flow and reflow to achieve high image quality at ultra-fast inference speed with low computational cost.</p><hr><h3>Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning</h3>
<p><a href='https://openreview.net/forum?id=1jbh2e0b2K'>https://openreview.net/forum?id=1jbh2e0b2K</a></p>
<p><b>Compressor summary</b>: This paper analyzes why multitask finetuning improves the performance of foundation models on target tasks with limited labeled samples, and proposes a practical task selection algorithm.</p><hr><h3>Simple Minimax Optimal Byzantine Robust Algorithm for Nonconvex Objectives with Uniform Gradient Heterogeneity</h3>
<p><a href='https://openreview.net/forum?id=1ii8idH4tH'>https://openreview.net/forum?id=1ii8idH4tH</a></p>
<p><b>Compressor summary</b>: The study proposes a new Byzantine robust algorithm called Momentum Screening for nonconvex federated learning problems with adaptive hyperparameters and improved optimization error rate in certain conditions.</p><hr><h3>Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight</h3>
<p><a href='https://openreview.net/forum?id=1hsVvgW0rU'>https://openreview.net/forum?id=1hsVvgW0rU</a></p>
<p><b>Compressor summary</b>: The paper proposes a new feedback model for learning in hard reinforcement learning problems, which allows collecting additional observations from latent states without observing them directly.</p><hr><h3>ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation</h3>
<p><a href='https://openreview.net/forum?id=1d2cLKeNgY'>https://openreview.net/forum?id=1d2cLKeNgY</a></p>
<p><b>Compressor summary</b>: The paper presents a ReSimAD method that converts domain knowledge into invariant representations and simulates target domains using 3D meshes, enabling zero-shot perception in autonomous driving across different domains.</p><hr><h3>I-PHYRE: Interactive Physical Reasoning</h3>
<p><a href='https://openreview.net/forum?id=1bbPQShCT2'>https://openreview.net/forum?id=1bbPQShCT2</a></p>
<p><b>Compressor summary</b>: I-PHYRE is a framework that tests agents' abilities to reason about physics, plan multiple steps, and intervene in real time during dynamic events.</p><hr><h3>DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation</h3>
<p><a href='https://openreview.net/forum?id=1bAUywYJTU'>https://openreview.net/forum?id=1bAUywYJTU</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to improve 3D content creation by aligning the 3D optimization process with the sampling process of diffusion models using prioritized timestep sampling.</p><hr><h3>Defining Expertise: Applications to Treatment Effect Estimation</h3>
<p><a href='https://openreview.net/forum?id=1YPfmglNRU'>https://openreview.net/forum?id=1YPfmglNRU</a></p>
<p><b>Compressor summary</b>: This paper argues that considering the type of expertise in decision-makers can help design and select methods for estimating treatment effects, as it influences their performance and can be predicted from data.</p><hr><h3>A Variational Perspective on Solving Inverse Problems with Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=1YO4EE3SPB'>https://openreview.net/forum?id=1YO4EE3SPB</a></p>
<p><b>Compressor summary</b>: The paper proposes a variational approach to approximate the posterior distribution in diffusion models for inverse problems, using denoisers with different structural constraints and SNR-based weights, resulting in improved image restoration tasks.</p><hr><h3>Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes</h3>
<p><a href='https://openreview.net/forum?id=1Wi0Ys33Nm'>https://openreview.net/forum?id=1Wi0Ys33Nm</a></p>
<p><b>Compressor summary</b>: The paper extends a proof about neural network convergence to different weight distributions and shows how to find the Edge of Chaos for better training.</p><hr><h3>Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods</h3>
<p><a href='https://openreview.net/forum?id=1VeQ6VBbev'>https://openreview.net/forum?id=1VeQ6VBbev</a></p>
<p><b>Compressor summary</b>: Dynamical policy gradient combines dynamic programming and policy gradient methods to improve convergence bounds and exploit structure in finite time horizon problems, such as optimal stopping or language model training.</p><hr><h3>Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks</h3>
<p><a href='https://openreview.net/forum?id=1SbkubNdbW'>https://openreview.net/forum?id=1SbkubNdbW</a></p>
<p><b>Compressor summary</b>: Label smoothing can increase or decrease model privacy depending on the method used; negative factor smoothing enhances resilience against information extraction attacks.</p><hr><h3>Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips</h3>
<p><a href='https://openreview.net/forum?id=1SIBN5Xyw7'>https://openreview.net/forum?id=1SIBN5Xyw7</a></p>
<p><b>Compressor summary</b>: The paper introduces Meta-SpikeFormer, a versatile and energy-efficient Transformer-based spiking neural network that outperforms existing CNN-based spiking neural networks on various vision tasks.</p><hr><h3>Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings</h3>
<p><a href='https://openreview.net/forum?id=1RrOtCmuKr'>https://openreview.net/forum?id=1RrOtCmuKr</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to reduce the memory footprint of deep neural networks by jointly learning codebooks and weight mappings, enabling efficient compression and loading on commodity devices.</p><hr><h3>MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning</h3>
<p><a href='https://openreview.net/forum?id=1RE0H6mU7M'>https://openreview.net/forum?id=1RE0H6mU7M</a></p>
<p><b>Compressor summary</b>: The paper proposes a new model-based meta-RL method that achieves better sample efficiency and returns on common and challenging high-dimensional domains.</p><hr><h3>Bespoke Solvers for Generative Flow Models</h3>
<p><a href='https://openreview.net/forum?id=1PXEY7ofFX'>https://openreview.net/forum?id=1PXEY7ofFX</a></p>
<p><b>Compressor summary</b>: Bespoke solvers improve sampling quality for diffusion models by optimizing custom ODE solvers tailored to the model's equations.</p><hr><h3>Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency</h3>
<p><a href='https://openreview.net/forum?id=1OfAO2mes1'>https://openreview.net/forum?id=1OfAO2mes1</a></p>
<p><b>Compressor summary</b>: The paper proposes a novel method to automatically identify and separate backdoored data in machine learning systems without needing clean data or manual thresholds.</p><hr><h3>Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality</h3>
<p><a href='https://openreview.net/forum?id=1NHgmKqOzZ'>https://openreview.net/forum?id=1NHgmKqOzZ</a></p>
<p><b>Compressor summary</b>: Progressive dataset distillation creates multiple small sets of synthetic images at different phases of training to reduce the time and memory requirement of training deep networks on large datasets.</p><hr><h3>Neural Architecture Retrieval</h3>
<p><a href='https://openreview.net/forum?id=1JtTPYBKqt'>https://openreview.net/forum?id=1JtTPYBKqt</a></p>
<p><b>Compressor summary</b>: The authors propose a new problem called Neural Architecture Retrieval that uses motif-based graph representation learning to find similar neural architectures efficiently and automatically.</p><hr><h3>Denoising Diffusion via Image-Based Rendering</h3>
<p><a href='https://openreview.net/forum?id=1JbsdayvhO'>https://openreview.net/forum?id=1JbsdayvhO</a></p>
<p><b>Compressor summary</b>: The paper presents a diffusion model that can reconstruct and generate realistic 3D scenes from 2D images using a novel neural scene representation, without needing additional supervision or regularizers.</p><hr><h3>Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders</h3>
<p><a href='https://openreview.net/forum?id=1CPta0bfN2'>https://openreview.net/forum?id=1CPta0bfN2</a></p>
<p><b>Compressor summary</b>: The text proposes a method that uses sparse matrix factorization to approximate cross-encoder similarity scores and perform $k$-nearest neighbor search, achieving better recall than existing methods while being more efficient.</p><hr><h3>Unsupervised Order Learning</h3>
<p><a href='https://openreview.net/forum?id=1CK45cqkEh'>https://openreview.net/forum?id=1CK45cqkEh</a></p>
<p><b>Compressor summary</b>: The paper proposes UOL, a novel clustering algorithm for ordered data, which groups objects into ordered clusters, constructs an embedding space, and estimates ranks via nearest neighbor search.</p><hr><h3>Enhancing Transferable Adversarial Attacks on Vision Transformers through Gradient Normalization Scaling and High-Frequency Adaptation</h3>
<p><a href='https://openreview.net/forum?id=1BuWv9poWz'>https://openreview.net/forum?id=1BuWv9poWz</a></p>
<p><b>Compressor summary</b>: The paper proposes a gradient normalization scaling method to improve adversarial attacks on Vision Transformers by exploiting their attention points in the frequency domain.</p><hr><h3>Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors</h3>
<p><a href='https://openreview.net/forum?id=1BmveEMNbG'>https://openreview.net/forum?id=1BmveEMNbG</a></p>
<p><b>Compressor summary</b>: The paper discusses challenges in reasoning on knowledge graphs, proposes a new dataset and method using fuzzy logic to improve reasoning capabilities for complex queries.</p><hr><h3>Feature Learning in Infinite Depth Neural Networks</h3>
<p><a href='https://openreview.net/forum?id=17pVDnpwwl'>https://openreview.net/forum?id=17pVDnpwwl</a></p>
<p><b>Compressor summary</b>: Depth-muP is a method for scaling deep neural networks that improves feature learning and diversity while preventing performance degradation.</p><hr><h3>SALMONN: Towards Generic Hearing Abilities for Large Language Models</h3>
<p><a href='https://openreview.net/forum?id=14rn7HpKVk'>https://openreview.net/forum?id=14rn7HpKVk</a></p>
<p><b>Compressor summary</b>: SALMONN is a multimodal AI model that combines text and audio processing to achieve competitive performance on various speech and audio tasks and discover new emergent abilities.</p><hr><h3>LipSim: A Provably Robust Perceptual Similarity Metric</h3>
<p><a href='https://openreview.net/forum?id=0w42S2Gp70'>https://openreview.net/forum?id=0w42S2Gp70</a></p>
<p><b>Compressor summary</b>: This paper proposes a robust perceptual similarity metric called LipSim that uses 1-Lipschitz neural networks to protect against adversarial attacks and provide guarantees.</p><hr><h3>Linear attention is (maybe) all you need (to understand Transformer optimization)</h3>
<p><a href='https://openreview.net/forum?id=0uI5415ry7'>https://openreview.net/forum?id=0uI5415ry7</a></p>
<p><b>Compressor summary</b>: The paper studies a simple linearized shallow Transformer model to better understand and improve Transformer training.</p><hr><h3>Understanding Hidden Context in Preference Learning: Consequences for RLHF</h3>
<p><a href='https://openreview.net/forum?id=0tWTxYYPnW'>https://openreview.net/forum?id=0tWTxYYPnW</a></p>
<p><b>Compressor summary</b>: The text discusses how human feedback in preference learning contains hidden context, which can lead to counter-intuitive results and vulnerabilities in methods like reinforcement learning from human feedback (RLHF). It introduces distributional preference learning (DPL) as a solution to address these issues.</p><hr><h3>Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization</h3>
<p><a href='https://openreview.net/forum?id=0t1O8ziRZp'>https://openreview.net/forum?id=0t1O8ziRZp</a></p>
<p><b>Compressor summary</b>: This study introduces RGLS, a parameter that adapts pre-trained agents' recommendations for logic synthesis, improving circuit quality and reducing runtime significantly.</p><hr><h3>MINDE: Mutual Information Neural Diffusion Estimation</h3>
<p><a href='https://openreview.net/forum?id=0kWd8SJq8d'>https://openreview.net/forum?id=0kWd8SJq8d</a></p>
<p><b>Compressor summary</b>: The paper proposes a new method to estimate Mutual Information using score-based diffusion models that outperforms existing methods in accuracy and passes self-consistency tests.</p><hr><h3>Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN</h3>
<p><a href='https://openreview.net/forum?id=0jsfesDZDq'>https://openreview.net/forum?id=0jsfesDZDq</a></p>
<p><b>Compressor summary</b>: The paper proposes a method for creating sparse recurrent spiking neural networks (RSNNs) from untrained models, using graph sparsification and Lyapunov exponents, which improves computational efficiency and task performance compared to traditional pruning methods.</p><hr><h3>Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors</h3>
<p><a href='https://openreview.net/forum?id=0jHkUDyEO9'>https://openreview.net/forum?id=0jHkUDyEO9</a></p>
<p><b>Compressor summary</b>: Magic123 is a method that generates high-quality 3D meshes from single images using neural networks and 2D and 3D priors.</p><hr><h3>UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models</h3>
<p><a href='https://openreview.net/forum?id=0j9ZDzMPqr'>https://openreview.net/forum?id=0j9ZDzMPqr</a></p>
<p><b>Compressor summary</b>: The paper proposes a method to generate counterfactual explanations for unsupervised node representation learning, helping to understand the underlying subgraphs that affect the node's behavior in downstream tasks.</p><hr><h3>Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions</h3>
<p><a href='https://openreview.net/forum?id=0i6Z9N5MLY'>https://openreview.net/forum?id=0i6Z9N5MLY</a></p>
<p><b>Compressor summary</b>: The paper proposes variance reduction methods for solving finite-sum monotone inclusion problems with improved oracle complexity and provides a near-optimal complexity bound.</p><hr><h3>TopoMLP: An Simple yet Strong Pipeline for Driving Topology Reasoning</h3>
<p><a href='https://openreview.net/forum?id=0gTW5JUFTW'>https://openreview.net/forum?id=0gTW5JUFTW</a></p>
<p><b>Compressor summary</b>: The paper introduces a new method for understanding road scenes and planning driving routes, using improved detectors for lanes and traffic elements and a simple neural network pipeline for reasoning their topology relationships.</p><hr><h3>LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection</h3>
<p><a href='https://openreview.net/forum?id=0d1gQI114C'>https://openreview.net/forum?id=0d1gQI114C</a></p>
<p><b>Compressor summary</b>: LiDAR-PTQ is a novel post-training quantization method for 3D lidar detection that achieves state-of-the-art performance and speedup while being cost-effective compared to other methods.</p><hr><h3>NEFTune: Noisy Embeddings Improve Instruction Finetuning</h3>
<p><a href='https://openreview.net/forum?id=0bMmZ3fkCk'>https://openreview.net/forum?id=0bMmZ3fkCk</a></p>
<p><b>Compressor summary</b>: NEFTune is a simple augmentation that improves language model finetuning on various tasks and datasets.</p><hr><h3>Contrastive Difference Predictive Coding</h3>
<p><a href='https://openreview.net/forum?id=0akLDTFR9x'>https://openreview.net/forum?id=0akLDTFR9x</a></p>
<p><b>Compressor summary</b>: The paper proposes a new time series representation learning method based on temporal difference contrastive predictive coding, which improves goal-conditioned reinforcement learning performance in less data and more noisy settings.</p><hr><h3>Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages</h3>
<p><a href='https://openreview.net/forum?id=0aR1s9YxoL'>https://openreview.net/forum?id=0aR1s9YxoL</a></p>
<p><b>Compressor summary</b>: This paper explores how to maintain plasticity in visual reinforcement learning and proposes a new method called Adaptive RR that adjusts reuse frequency based on the critic's plasticity level.</p><hr><h3>MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design</h3>
<p><a href='https://openreview.net/forum?id=0VBsoluxR2'>https://openreview.net/forum?id=0VBsoluxR2</a></p>
<p><b>Compressor summary</b>: The text introduces MOFDiff, a model that generates new metal-organic frameworks (MOFs) with unique structures and improved performance for carbon capture using diffusion and graph neural networks.</p><hr><h3>Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision</h3>
<p><a href='https://openreview.net/forum?id=0V5TVt9bk0'>https://openreview.net/forum?id=0V5TVt9bk0</a></p>
<p><b>Compressor summary</b>: Q-Bench is a benchmark to assess MLLMs' low-level visual perception, description, and quality assessment abilities using LLVisionQA, LLDescribe, and quantifiable quality scores datasets.</p><hr><h3>Inner Classifier-Free Guidance and Its Taylor Expansion for Diffusion Models</h3>
<p><a href='https://openreview.net/forum?id=0QAzIMq32X'>https://openreview.net/forum?id=0QAzIMq32X</a></p>
<p><b>Compressor summary</b>: Inner classifier-free guidance (ICFG) improves the balance between fidelity and diversity in conditional diffusion models when the condition is continuous and has a specific structure, by providing an alternative perspective on classifier-free guidance.</p><hr><h3>Planting a SEED of Vision in Large Language Model</h3>
<p><a href='https://openreview.net/forum?id=0Nui91LBQS'>https://openreview.net/forum?id=0Nui91LBQS</a></p>
<p><b>Compressor summary</b>: SEED is an image tokenizer that enables text and images to be processed interchangeably within a unified autoregressive Transformer, improving multimodal comprehension and generation in large language models.</p><hr><h3>From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module</h3>
<p><a href='https://openreview.net/forum?id=0JsRZEGZ7L'>https://openreview.net/forum?id=0JsRZEGZ7L</a></p>
<p><b>Compressor summary</b>: Latent Topology Inference (LTI) with Differentiable Cell Complex Module (DCM) improves Graph Neural Networks (GNNs) by learning higher-order cell complexes for multi-way data interactions, achieving better results especially when no input graph is given.</p><hr><h3>Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks</h3>
<p><a href='https://openreview.net/forum?id=0H6DFoZZXZ'>https://openreview.net/forum?id=0H6DFoZZXZ</a></p>
<p><b>Compressor summary</b>: The paper proposes a Language to Control Diffusion model (LCD) that uses natural language instructions to plan and control generalist agents efficiently and effectively across high-dimensional inputs, long horizons, and novel tasks.</p><hr><h3>One-shot Empirical Privacy Estimation for Federated Learning</h3>
<p><a href='https://openreview.net/forum?id=0BqyZSWfzo'>https://openreview.net/forum?id=0BqyZSWfzo</a></p>
<p><b>Compressor summary</b>: The paper proposes a "one-shot" approach to audit or estimate differentially private algorithms' privacy loss efficiently during model training without additional assumptions or knowledge.</p><hr><h3>AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework</h3>
<p><a href='https://openreview.net/forum?id=09xFexjhqE'>https://openreview.net/forum?id=09xFexjhqE</a></p>
<p><b>Compressor summary</b>: This paper proposes AutoLoRa, a method that disentangles and automates Robust Fine-Tuning to improve adversarial robustness in downstream applications using a low-rank branch and heuristic strategies for learning rate scheduling.</p><hr><h3>Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning</h3>
<p><a href='https://openreview.net/forum?id=09iOdaeOzp'>https://openreview.net/forum?id=09iOdaeOzp</a></p>
<p><b>Compressor summary</b>: Structured pruning helps create smaller yet powerful large language models (LLMs) from pre-trained, larger models by removing layers and dynamically adjusting data composition, outperforming state-of-the-art open-source models with much less compute cost.</p><hr><h3>Dropout Enhanced Bilevel Training</h3>
<p><a href='https://openreview.net/forum?id=06lrITXVAx'>https://openreview.net/forum?id=06lrITXVAx</a></p>
<p><b>Compressor summary</b>: The paper proposes a dropout-based bilevel optimization model to address overfitting in machine learning tasks with limited datasets and analyzes its optimization properties.</p><hr><h3>Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing</h3>
<p><a href='https://openreview.net/forum?id=02f3mUtqnM'>https://openreview.net/forum?id=02f3mUtqnM</a></p>
<p><b>Compressor summary</b>: The paper proposes a hybrid inference approach using a router that assigns queries to small or large NLP models based on difficulty and desired quality, saving cost without sacrificing quality.</p>